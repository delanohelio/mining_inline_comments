{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY2MTkwNDM3", "number": 9162, "title": "MINOR: refactor Log to get rid of \"return\" in nested anonymous function", "bodyText": "scala throws and then catches NonLocalReturnException to implement the control flow of returning from a nested anonymous function. That is anti-pattern and we should avoid using it in the hot methods.\nI run the producer benchmark and the following screen snapshot show the improvement by this patch. The NonLocalReturnException is gone.\nBEFORE\n\nAFTER\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-08-11T15:43:36Z", "url": "https://github.com/apache/kafka/pull/9162", "merged": true, "mergeCommit": {"oid": "c2737ee9ebe1f9f0a1aa7860fd212ce1b0ca59ea"}, "closed": true, "closedAt": "2020-10-27T09:45:12Z", "author": {"login": "chia7712"}, "timelineItems": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc997z2AFqTQ2NTQ1MDc0OA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdWjMLggH2gAyNDY2MTkwNDM3OmMyNjA1OWFlMTA1YmY0MDYyYjMwOTc4ZDkxMzY0NGVkNTBiOTIwN2I=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1NDUwNzQ4", "url": "https://github.com/apache/kafka/pull/9162#pullrequestreview-465450748", "createdAt": "2020-08-11T21:45:00Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMTo0NTowMFrOG_KUdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMTo0NTowMFrOG_KUdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg4MjU1MQ==", "bodyText": "Since this code is pretty critical, I'd keep the previous while loop and add a boolean to indicate a break or something like that.", "url": "https://github.com/apache/kafka/pull/9162#discussion_r468882551", "createdAt": "2020-08-11T21:45:00Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1501,43 +1501,36 @@ class Log(@volatile private var _dir: File,\n         case FetchTxnCommitted => fetchLastStableOffsetMetadata\n       }\n \n-      if (startOffset == maxOffsetMetadata.messageOffset) {\n-        return emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n-      } else if (startOffset > maxOffsetMetadata.messageOffset) {\n-        val startOffsetMetadata = convertToOffsetMetadataOrThrow(startOffset)\n-        return emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n-      }\n-\n-      // Do the read on the segment with a base offset less than the target offset\n-      // but if that segment doesn't contain any messages with an offset greater than that\n-      // continue to read from successive segments until we get some messages or we reach the end of the log\n-      while (segmentEntry != null) {\n-        val segment = segmentEntry.getValue\n-\n-        val maxPosition = {\n-          // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.\n-          if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {\n-            maxOffsetMetadata.relativePositionInSegment\n-          } else {\n-            segment.size\n-          }\n-        }\n-\n-        val fetchInfo = segment.read(startOffset, maxLength, maxPosition, minOneMessage)\n-        if (fetchInfo == null) {\n-          segmentEntry = segments.higherEntry(segmentEntry.getKey)\n-        } else {\n-          return if (includeAbortedTxns)\n-            addAbortedTransactions(startOffset, segmentEntry, fetchInfo)\n-          else\n-            fetchInfo\n-        }\n+      if (startOffset == maxOffsetMetadata.messageOffset)\n+        emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n+      else if (startOffset > maxOffsetMetadata.messageOffset)\n+        emptyFetchDataInfo(convertToOffsetMetadataOrThrow(startOffset), includeAbortedTxns)\n+      else {\n+        // Do the read on the segment with a base offset less than the target offset\n+        // but if that segment doesn't contain any messages with an offset greater than that\n+        // continue to read from successive segments until we get some messages or we reach the end of the log\n+        var currentEntry = segmentFloorEntry\n+        Iterator.continually(currentEntry)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2Mzc0ODYw", "url": "https://github.com/apache/kafka/pull/9162#pullrequestreview-516374860", "createdAt": "2020-10-25T16:20:23Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNjoyMDoyM1rOHn6qtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNjozODozNlrOHn6zIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYxNzcxOA==", "bodyText": "Nit: we can remove the braces from the if/else.", "url": "https://github.com/apache/kafka/pull/9162#discussion_r511617718", "createdAt": "2020-10-25T16:20:23Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1535,42 +1533,48 @@ class Log(@volatile private var _dir: File,\n       }\n \n       if (startOffset == maxOffsetMetadata.messageOffset) {\n-        return emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n+        emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n       } else if (startOffset > maxOffsetMetadata.messageOffset) {\n         val startOffsetMetadata = convertToOffsetMetadataOrThrow(startOffset)\n-        return emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n-      }\n+        emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n+      } else {\n \n-      // Do the read on the segment with a base offset less than the target offset\n-      // but if that segment doesn't contain any messages with an offset greater than that\n-      // continue to read from successive segments until we get some messages or we reach the end of the log\n-      while (segmentEntry != null) {\n-        val segment = segmentEntry.getValue\n+        // Do the read on the segment with a base offset less than the target offset\n+        // but if that segment doesn't contain any messages with an offset greater than that\n+        // continue to read from successive segments until we get some messages or we reach the end of the log\n+        var done = segmentEntry == null\n+        var fetchDataInfo: FetchDataInfo = null\n+        while (!done) {\n+          val segment = segmentEntry.getValue\n+\n+          val maxPosition = {\n+            // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.\n+            if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {\n+              maxOffsetMetadata.relativePositionInSegment\n+            } else {\n+              segment.size\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 349}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYxNzkxMg==", "bodyText": "The formatting here is a bit weird (the else is aligned the same as the variable. I think it's easier to understand if the if/else are aligned and indented.", "url": "https://github.com/apache/kafka/pull/9162#discussion_r511617912", "createdAt": "2020-10-25T16:22:21Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1535,42 +1533,48 @@ class Log(@volatile private var _dir: File,\n       }\n \n       if (startOffset == maxOffsetMetadata.messageOffset) {\n-        return emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n+        emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n       } else if (startOffset > maxOffsetMetadata.messageOffset) {\n         val startOffsetMetadata = convertToOffsetMetadataOrThrow(startOffset)\n-        return emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n-      }\n+        emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n+      } else {\n \n-      // Do the read on the segment with a base offset less than the target offset\n-      // but if that segment doesn't contain any messages with an offset greater than that\n-      // continue to read from successive segments until we get some messages or we reach the end of the log\n-      while (segmentEntry != null) {\n-        val segment = segmentEntry.getValue\n+        // Do the read on the segment with a base offset less than the target offset\n+        // but if that segment doesn't contain any messages with an offset greater than that\n+        // continue to read from successive segments until we get some messages or we reach the end of the log\n+        var done = segmentEntry == null\n+        var fetchDataInfo: FetchDataInfo = null\n+        while (!done) {\n+          val segment = segmentEntry.getValue\n+\n+          val maxPosition = {\n+            // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.\n+            if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {\n+              maxOffsetMetadata.relativePositionInSegment\n+            } else {\n+              segment.size\n+            }\n+          }\n \n-        val maxPosition = {\n-          // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.\n-          if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {\n-            maxOffsetMetadata.relativePositionInSegment\n+          val fetchInfo = segment.read(startOffset, maxLength, maxPosition, minOneMessage)\n+          if (fetchInfo == null) {\n+            segmentEntry = segments.higherEntry(segmentEntry.getKey)\n+            done = segmentEntry == null\n           } else {\n-            segment.size\n+            fetchDataInfo = if (includeAbortedTxns) addAbortedTransactions(startOffset, segmentEntry, fetchInfo)\n+            else fetchInfo", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 363}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYxOTg3NQ==", "bodyText": "Maybe something like the following may be clearer:\nfetchDataInfo = segment.read(startOffset, maxLength, maxPosition, minOneMessage)\nif (fetchDataInfo != null) {\n  if (includeAbortedTxns)\n    fetchDataInfo = addAbortedTransactions(startOffset, segmentEntry, fetchDataInfo)\n} else segmentEntry = segments.higherEntry(segmentEntry.getKey)\n\ndone = fetchDataInfo != null || segmentEntry == null\nWhat do you think?", "url": "https://github.com/apache/kafka/pull/9162#discussion_r511619875", "createdAt": "2020-10-25T16:38:36Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1535,42 +1533,48 @@ class Log(@volatile private var _dir: File,\n       }\n \n       if (startOffset == maxOffsetMetadata.messageOffset) {\n-        return emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n+        emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n       } else if (startOffset > maxOffsetMetadata.messageOffset) {\n         val startOffsetMetadata = convertToOffsetMetadataOrThrow(startOffset)\n-        return emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n-      }\n+        emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n+      } else {\n \n-      // Do the read on the segment with a base offset less than the target offset\n-      // but if that segment doesn't contain any messages with an offset greater than that\n-      // continue to read from successive segments until we get some messages or we reach the end of the log\n-      while (segmentEntry != null) {\n-        val segment = segmentEntry.getValue\n+        // Do the read on the segment with a base offset less than the target offset\n+        // but if that segment doesn't contain any messages with an offset greater than that\n+        // continue to read from successive segments until we get some messages or we reach the end of the log\n+        var done = segmentEntry == null\n+        var fetchDataInfo: FetchDataInfo = null\n+        while (!done) {\n+          val segment = segmentEntry.getValue\n+\n+          val maxPosition = {\n+            // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.\n+            if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {\n+              maxOffsetMetadata.relativePositionInSegment\n+            } else {\n+              segment.size\n+            }\n+          }\n \n-        val maxPosition = {\n-          // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.\n-          if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {\n-            maxOffsetMetadata.relativePositionInSegment\n+          val fetchInfo = segment.read(startOffset, maxLength, maxPosition, minOneMessage)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 356}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2Mzc3MDg5", "url": "https://github.com/apache/kafka/pull/9162#pullrequestreview-516377089", "createdAt": "2020-10-25T16:45:26Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNjo0NToyNlrOHn62dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNjo0NToyNlrOHn62dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyMDcyNw==", "bodyText": "Seems like we should use a pattern match here instead of isDefined and get.", "url": "https://github.com/apache/kafka/pull/9162#discussion_r511620727", "createdAt": "2020-10-25T16:45:26Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1099,171 +1099,169 @@ class Log(@volatile private var _dir: File,\n       val appendInfo = analyzeAndValidateRecords(records, origin, ignoreRecordSize)\n \n       // return if we have no valid messages or if this is a duplicate of the last appended entry\n-      if (appendInfo.shallowCount == 0)\n-        return appendInfo\n+      if (appendInfo.shallowCount == 0) appendInfo\n+      else {\n \n-      // trim any invalid bytes or partial messages before appending it to the on-disk log\n-      var validRecords = trimInvalidBytes(records, appendInfo)\n+        // trim any invalid bytes or partial messages before appending it to the on-disk log\n+        var validRecords = trimInvalidBytes(records, appendInfo)\n \n-      // they are valid, insert them in the log\n-      lock synchronized {\n-        checkIfMemoryMappedBufferClosed()\n-        if (assignOffsets) {\n-          // assign offsets to the message set\n-          val offset = new LongRef(nextOffsetMetadata.messageOffset)\n-          appendInfo.firstOffset = Some(offset.value)\n-          val now = time.milliseconds\n-          val validateAndOffsetAssignResult = try {\n-            LogValidator.validateMessagesAndAssignOffsets(validRecords,\n-              topicPartition,\n-              offset,\n-              time,\n-              now,\n-              appendInfo.sourceCodec,\n-              appendInfo.targetCodec,\n-              config.compact,\n-              config.messageFormatVersion.recordVersion.value,\n-              config.messageTimestampType,\n-              config.messageTimestampDifferenceMaxMs,\n-              leaderEpoch,\n-              origin,\n-              interBrokerProtocolVersion,\n-              brokerTopicStats)\n-          } catch {\n-            case e: IOException =>\n-              throw new KafkaException(s\"Error validating messages while appending to log $name\", e)\n-          }\n-          validRecords = validateAndOffsetAssignResult.validatedRecords\n-          appendInfo.maxTimestamp = validateAndOffsetAssignResult.maxTimestamp\n-          appendInfo.offsetOfMaxTimestamp = validateAndOffsetAssignResult.shallowOffsetOfMaxTimestamp\n-          appendInfo.lastOffset = offset.value - 1\n-          appendInfo.recordConversionStats = validateAndOffsetAssignResult.recordConversionStats\n-          if (config.messageTimestampType == TimestampType.LOG_APPEND_TIME)\n-            appendInfo.logAppendTime = now\n-\n-          // re-validate message sizes if there's a possibility that they have changed (due to re-compression or message\n-          // format conversion)\n-          if (!ignoreRecordSize && validateAndOffsetAssignResult.messageSizeMaybeChanged) {\n-            for (batch <- validRecords.batches.asScala) {\n-              if (batch.sizeInBytes > config.maxMessageSize) {\n-                // we record the original message set size instead of the trimmed size\n-                // to be consistent with pre-compression bytesRejectedRate recording\n-                brokerTopicStats.topicStats(topicPartition.topic).bytesRejectedRate.mark(records.sizeInBytes)\n-                brokerTopicStats.allTopicsStats.bytesRejectedRate.mark(records.sizeInBytes)\n-                throw new RecordTooLargeException(s\"Message batch size is ${batch.sizeInBytes} bytes in append to\" +\n-                  s\"partition $topicPartition which exceeds the maximum configured size of ${config.maxMessageSize}.\")\n-              }\n+        // they are valid, insert them in the log\n+        lock synchronized {\n+          checkIfMemoryMappedBufferClosed()\n+          if (assignOffsets) {\n+            // assign offsets to the message set\n+            val offset = new LongRef(nextOffsetMetadata.messageOffset)\n+            appendInfo.firstOffset = Some(offset.value)\n+            val now = time.milliseconds\n+            val validateAndOffsetAssignResult = try {\n+              LogValidator.validateMessagesAndAssignOffsets(validRecords,\n+                topicPartition,\n+                offset,\n+                time,\n+                now,\n+                appendInfo.sourceCodec,\n+                appendInfo.targetCodec,\n+                config.compact,\n+                config.messageFormatVersion.recordVersion.value,\n+                config.messageTimestampType,\n+                config.messageTimestampDifferenceMaxMs,\n+                leaderEpoch,\n+                origin,\n+                interBrokerProtocolVersion,\n+                brokerTopicStats)\n+            } catch {\n+              case e: IOException =>\n+                throw new KafkaException(s\"Error validating messages while appending to log $name\", e)\n             }\n-          }\n-        } else {\n-          // we are taking the offsets we are given\n-          if (!appendInfo.offsetsMonotonic)\n-            throw new OffsetsOutOfOrderException(s\"Out of order offsets found in append to $topicPartition: \" +\n-                                                 records.records.asScala.map(_.offset))\n-\n-          if (appendInfo.firstOrLastOffsetOfFirstBatch < nextOffsetMetadata.messageOffset) {\n-            // we may still be able to recover if the log is empty\n-            // one example: fetching from log start offset on the leader which is not batch aligned,\n-            // which may happen as a result of AdminClient#deleteRecords()\n-            val firstOffset = appendInfo.firstOffset match {\n-              case Some(offset) => offset\n-              case None => records.batches.asScala.head.baseOffset()\n+            validRecords = validateAndOffsetAssignResult.validatedRecords\n+            appendInfo.maxTimestamp = validateAndOffsetAssignResult.maxTimestamp\n+            appendInfo.offsetOfMaxTimestamp = validateAndOffsetAssignResult.shallowOffsetOfMaxTimestamp\n+            appendInfo.lastOffset = offset.value - 1\n+            appendInfo.recordConversionStats = validateAndOffsetAssignResult.recordConversionStats\n+            if (config.messageTimestampType == TimestampType.LOG_APPEND_TIME)\n+              appendInfo.logAppendTime = now\n+\n+            // re-validate message sizes if there's a possibility that they have changed (due to re-compression or message\n+            // format conversion)\n+            if (!ignoreRecordSize && validateAndOffsetAssignResult.messageSizeMaybeChanged) {\n+              for (batch <- validRecords.batches.asScala) {\n+                if (batch.sizeInBytes > config.maxMessageSize) {\n+                  // we record the original message set size instead of the trimmed size\n+                  // to be consistent with pre-compression bytesRejectedRate recording\n+                  brokerTopicStats.topicStats(topicPartition.topic).bytesRejectedRate.mark(records.sizeInBytes)\n+                  brokerTopicStats.allTopicsStats.bytesRejectedRate.mark(records.sizeInBytes)\n+                  throw new RecordTooLargeException(s\"Message batch size is ${batch.sizeInBytes} bytes in append to\" +\n+                    s\"partition $topicPartition which exceeds the maximum configured size of ${config.maxMessageSize}.\")\n+                }\n+              }\n             }\n+          } else {\n+            // we are taking the offsets we are given\n+            if (!appendInfo.offsetsMonotonic)\n+              throw new OffsetsOutOfOrderException(s\"Out of order offsets found in append to $topicPartition: \" +\n+                records.records.asScala.map(_.offset))\n+\n+            if (appendInfo.firstOrLastOffsetOfFirstBatch < nextOffsetMetadata.messageOffset) {\n+              // we may still be able to recover if the log is empty\n+              // one example: fetching from log start offset on the leader which is not batch aligned,\n+              // which may happen as a result of AdminClient#deleteRecords()\n+              val firstOffset = appendInfo.firstOffset match {\n+                case Some(offset) => offset\n+                case None => records.batches.asScala.head.baseOffset()\n+              }\n \n-            val firstOrLast = if (appendInfo.firstOffset.isDefined) \"First offset\" else \"Last offset of the first batch\"\n-            throw new UnexpectedAppendOffsetException(\n-              s\"Unexpected offset in append to $topicPartition. $firstOrLast \" +\n-              s\"${appendInfo.firstOrLastOffsetOfFirstBatch} is less than the next offset ${nextOffsetMetadata.messageOffset}. \" +\n-              s\"First 10 offsets in append: ${records.records.asScala.take(10).map(_.offset)}, last offset in\" +\n-              s\" append: ${appendInfo.lastOffset}. Log start offset = $logStartOffset\",\n-              firstOffset, appendInfo.lastOffset)\n+              val firstOrLast = if (appendInfo.firstOffset.isDefined) \"First offset\" else \"Last offset of the first batch\"\n+              throw new UnexpectedAppendOffsetException(\n+                s\"Unexpected offset in append to $topicPartition. $firstOrLast \" +\n+                  s\"${appendInfo.firstOrLastOffsetOfFirstBatch} is less than the next offset ${nextOffsetMetadata.messageOffset}. \" +\n+                  s\"First 10 offsets in append: ${records.records.asScala.take(10).map(_.offset)}, last offset in\" +\n+                  s\" append: ${appendInfo.lastOffset}. Log start offset = $logStartOffset\",\n+                firstOffset, appendInfo.lastOffset)\n+            }\n           }\n-        }\n \n-        // update the epoch cache with the epoch stamped onto the message by the leader\n-        validRecords.batches.forEach { batch =>\n-          if (batch.magic >= RecordBatch.MAGIC_VALUE_V2) {\n-            maybeAssignEpochStartOffset(batch.partitionLeaderEpoch, batch.baseOffset)\n-          } else {\n-            // In partial upgrade scenarios, we may get a temporary regression to the message format. In\n-            // order to ensure the safety of leader election, we clear the epoch cache so that we revert\n-            // to truncation by high watermark after the next leader election.\n-            leaderEpochCache.filter(_.nonEmpty).foreach { cache =>\n-              warn(s\"Clearing leader epoch cache after unexpected append with message format v${batch.magic}\")\n-              cache.clearAndFlush()\n+          // update the epoch cache with the epoch stamped onto the message by the leader\n+          validRecords.batches.forEach { batch =>\n+            if (batch.magic >= RecordBatch.MAGIC_VALUE_V2) {\n+              maybeAssignEpochStartOffset(batch.partitionLeaderEpoch, batch.baseOffset)\n+            } else {\n+              // In partial upgrade scenarios, we may get a temporary regression to the message format. In\n+              // order to ensure the safety of leader election, we clear the epoch cache so that we revert\n+              // to truncation by high watermark after the next leader election.\n+              leaderEpochCache.filter(_.nonEmpty).foreach { cache =>\n+                warn(s\"Clearing leader epoch cache after unexpected append with message format v${batch.magic}\")\n+                cache.clearAndFlush()\n+              }\n             }\n           }\n-        }\n \n-        // check messages set size may be exceed config.segmentSize\n-        if (validRecords.sizeInBytes > config.segmentSize) {\n-          throw new RecordBatchTooLargeException(s\"Message batch size is ${validRecords.sizeInBytes} bytes in append \" +\n-            s\"to partition $topicPartition, which exceeds the maximum configured segment size of ${config.segmentSize}.\")\n-        }\n+          // check messages set size may be exceed config.segmentSize\n+          if (validRecords.sizeInBytes > config.segmentSize) {\n+            throw new RecordBatchTooLargeException(s\"Message batch size is ${validRecords.sizeInBytes} bytes in append \" +\n+              s\"to partition $topicPartition, which exceeds the maximum configured segment size of ${config.segmentSize}.\")\n+          }\n \n-        // maybe roll the log if this segment is full\n-        val segment = maybeRoll(validRecords.sizeInBytes, appendInfo)\n-\n-        val logOffsetMetadata = LogOffsetMetadata(\n-          messageOffset = appendInfo.firstOrLastOffsetOfFirstBatch,\n-          segmentBaseOffset = segment.baseOffset,\n-          relativePositionInSegment = segment.size)\n-\n-        // now that we have valid records, offsets assigned, and timestamps updated, we need to\n-        // validate the idempotent/transactional state of the producers and collect some metadata\n-        val (updatedProducers, completedTxns, maybeDuplicate) = analyzeAndValidateProducerState(\n-          logOffsetMetadata, validRecords, origin)\n-\n-        maybeDuplicate.foreach { duplicate =>\n-          appendInfo.firstOffset = Some(duplicate.firstOffset)\n-          appendInfo.lastOffset = duplicate.lastOffset\n-          appendInfo.logAppendTime = duplicate.timestamp\n-          appendInfo.logStartOffset = logStartOffset\n-          return appendInfo\n-        }\n+          // maybe roll the log if this segment is full\n+          val segment = maybeRoll(validRecords.sizeInBytes, appendInfo)\n \n-        segment.append(largestOffset = appendInfo.lastOffset,\n-          largestTimestamp = appendInfo.maxTimestamp,\n-          shallowOffsetOfMaxTimestamp = appendInfo.offsetOfMaxTimestamp,\n-          records = validRecords)\n-\n-        // Increment the log end offset. We do this immediately after the append because a\n-        // write to the transaction index below may fail and we want to ensure that the offsets\n-        // of future appends still grow monotonically. The resulting transaction index inconsistency\n-        // will be cleaned up after the log directory is recovered. Note that the end offset of the\n-        // ProducerStateManager will not be updated and the last stable offset will not advance\n-        // if the append to the transaction index fails.\n-        updateLogEndOffset(appendInfo.lastOffset + 1)\n-\n-        // update the producer state\n-        for (producerAppendInfo <- updatedProducers.values) {\n-          producerStateManager.update(producerAppendInfo)\n-        }\n+          val logOffsetMetadata = LogOffsetMetadata(\n+            messageOffset = appendInfo.firstOrLastOffsetOfFirstBatch,\n+            segmentBaseOffset = segment.baseOffset,\n+            relativePositionInSegment = segment.size)\n \n-        // update the transaction index with the true last stable offset. The last offset visible\n-        // to consumers using READ_COMMITTED will be limited by this value and the high watermark.\n-        for (completedTxn <- completedTxns) {\n-          val lastStableOffset = producerStateManager.lastStableOffset(completedTxn)\n-          segment.updateTxnIndex(completedTxn, lastStableOffset)\n-          producerStateManager.completeTxn(completedTxn)\n-        }\n+          // now that we have valid records, offsets assigned, and timestamps updated, we need to\n+          // validate the idempotent/transactional state of the producers and collect some metadata\n+          val (updatedProducers, completedTxns, maybeDuplicate) = analyzeAndValidateProducerState(\n+            logOffsetMetadata, validRecords, origin)\n \n-        // always update the last producer id map offset so that the snapshot reflects the current offset\n-        // even if there isn't any idempotent data being written\n-        producerStateManager.updateMapEndOffset(appendInfo.lastOffset + 1)\n+          if (maybeDuplicate.isDefined) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 257}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2ODQ0NTcz", "url": "https://github.com/apache/kafka/pull/9162#pullrequestreview-516844573", "createdAt": "2020-10-26T14:38:47Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "68819c1252a5da8bcd60219e1fbd26c2927fbb89", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/68819c1252a5da8bcd60219e1fbd26c2927fbb89", "committedDate": "2020-10-27T01:52:45Z", "message": "MINOR: refactor Log to get rid of \"return\" in nested anonymous function"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5f4c4518fe43ae6865843665626fd977fa8f94cc", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/5f4c4518fe43ae6865843665626fd977fa8f94cc", "committedDate": "2020-10-27T01:52:45Z", "message": "fix code style and address review comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac83d4a08827280098c265246584af37baa025c5", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/ac83d4a08827280098c265246584af37baa025c5", "committedDate": "2020-10-27T01:53:36Z", "message": "code review"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "ac83d4a08827280098c265246584af37baa025c5", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/ac83d4a08827280098c265246584af37baa025c5", "committedDate": "2020-10-27T01:53:36Z", "message": "code review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c26059ae105bf4062b30978d913644ed50b9207b", "author": {"user": {"login": "chia7712", "name": "Chia-Ping Tsai"}}, "url": "https://github.com/apache/kafka/commit/c26059ae105bf4062b30978d913644ed50b9207b", "committedDate": "2020-10-27T06:43:33Z", "message": "add trivial change to trigger QA"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1127, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}