{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA1NTgxMDM1", "number": 8514, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOVQxODozOToyMFrODzcccQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxMTozMjozOVrODzrlNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1MjcwMDAxOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOVQxODozOToyMFrOGH7S9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOVQxOTozNjo1OVrOGH78PQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk2NDcyNw==", "bodyText": "Why do we write into STREAM_INPUT_TWO with 3 calls instead of just one call passing in all 3 records at once?", "url": "https://github.com/apache/kafka/pull/8514#discussion_r410964727", "createdAt": "2020-04-19T18:39:20Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java", "diffHunk": "@@ -214,70 +176,55 @@ private StreamsBuilder builderForSegmentedStateStore() {\n                     .withRetention(WINDOW_SIZE))\n             .toStream()\n             .map((key, value) -> KeyValue.pair(value, value))\n-            .to(STREAM_OUTPUT, Produced.with(Serdes.Long(), Serdes.Long()));\n+            .to(STREAM_OUTPUT_TWO, Produced.with(Serdes.Long(), Serdes.Long()));\n         return builder;\n     }\n \n     private void cleanUpStateRunVerifyAndClose(final StreamsBuilder builder,\n                                                final Properties streamsConfiguration,\n-                                               final Class outputKeyDeserializer,\n-                                               final Class outputValueDeserializer,\n-                                               final MetricsVerifier metricsVerifier,\n-                                               final String metricsScope) throws Exception {\n+                                               final MetricsVerifier metricsVerifier) throws Exception {\n         final KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);\n         kafkaStreams.cleanUp();\n         produceRecords();\n \n         StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams, TIMEOUT);\n \n-        IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(\n-            TestUtils.consumerConfig(\n-                CLUSTER.bootstrapServers(),\n-                \"consumerApp\",\n-                outputKeyDeserializer,\n-                outputValueDeserializer,\n-                new Properties()\n-            ),\n-            STREAM_OUTPUT,\n-            1\n-        );\n-        metricsVerifier.verify(kafkaStreams, metricsScope);\n+        metricsVerifier.verify(kafkaStreams, \"rocksdb-state-id\");\n+        metricsVerifier.verify(kafkaStreams, \"rocksdb-window-state-id\");\n         kafkaStreams.close();\n     }\n \n     private void produceRecords() throws Exception {\n         final MockTime mockTime = new MockTime(WINDOW_SIZE.toMillis());\n+        final Properties prop = TestUtils.producerConfig(\n+            CLUSTER.bootstrapServers(),\n+            IntegerSerializer.class,\n+            StringSerializer.class,\n+            new Properties()\n+        );\n+        // non-segmented store do not need records with different timestamps\n         IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n-            STREAM_INPUT,\n-            Collections.singletonList(new KeyValue<>(1, \"A\")),\n-            TestUtils.producerConfig(\n-                CLUSTER.bootstrapServers(),\n-                IntegerSerializer.class,\n-                StringSerializer.class,\n-                new Properties()\n-            ),\n+            STREAM_INPUT_ONE,\n+            Utils.mkSet(new KeyValue<>(1, \"A\"), new KeyValue<>(1, \"B\"), new KeyValue<>(1, \"C\")),\n+            prop,\n             mockTime.milliseconds()\n         );\n         IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n-            STREAM_INPUT,\n-            Collections.singletonList(new KeyValue<>(1, \"B\")),\n-            TestUtils.producerConfig(\n-                CLUSTER.bootstrapServers(),\n-                IntegerSerializer.class,\n-                StringSerializer.class,\n-                new Properties()\n-            ),\n+            STREAM_INPUT_TWO,\n+            Collections.singleton(new KeyValue<>(1, \"A\")),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27fefff2508444c2d58e94e3b936660d62b340e0"}, "originalPosition": 204}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk3NTI5Mw==", "bodyText": "Note we use a mocktime with auto tick WINDOW_SIZE.toMillis(): and each time its milliseconds() is called it would auto-advance, we need the produced records with those advanced timestamps.", "url": "https://github.com/apache/kafka/pull/8514#discussion_r410975293", "createdAt": "2020-04-19T19:36:59Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java", "diffHunk": "@@ -214,70 +176,55 @@ private StreamsBuilder builderForSegmentedStateStore() {\n                     .withRetention(WINDOW_SIZE))\n             .toStream()\n             .map((key, value) -> KeyValue.pair(value, value))\n-            .to(STREAM_OUTPUT, Produced.with(Serdes.Long(), Serdes.Long()));\n+            .to(STREAM_OUTPUT_TWO, Produced.with(Serdes.Long(), Serdes.Long()));\n         return builder;\n     }\n \n     private void cleanUpStateRunVerifyAndClose(final StreamsBuilder builder,\n                                                final Properties streamsConfiguration,\n-                                               final Class outputKeyDeserializer,\n-                                               final Class outputValueDeserializer,\n-                                               final MetricsVerifier metricsVerifier,\n-                                               final String metricsScope) throws Exception {\n+                                               final MetricsVerifier metricsVerifier) throws Exception {\n         final KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);\n         kafkaStreams.cleanUp();\n         produceRecords();\n \n         StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams, TIMEOUT);\n \n-        IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(\n-            TestUtils.consumerConfig(\n-                CLUSTER.bootstrapServers(),\n-                \"consumerApp\",\n-                outputKeyDeserializer,\n-                outputValueDeserializer,\n-                new Properties()\n-            ),\n-            STREAM_OUTPUT,\n-            1\n-        );\n-        metricsVerifier.verify(kafkaStreams, metricsScope);\n+        metricsVerifier.verify(kafkaStreams, \"rocksdb-state-id\");\n+        metricsVerifier.verify(kafkaStreams, \"rocksdb-window-state-id\");\n         kafkaStreams.close();\n     }\n \n     private void produceRecords() throws Exception {\n         final MockTime mockTime = new MockTime(WINDOW_SIZE.toMillis());\n+        final Properties prop = TestUtils.producerConfig(\n+            CLUSTER.bootstrapServers(),\n+            IntegerSerializer.class,\n+            StringSerializer.class,\n+            new Properties()\n+        );\n+        // non-segmented store do not need records with different timestamps\n         IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n-            STREAM_INPUT,\n-            Collections.singletonList(new KeyValue<>(1, \"A\")),\n-            TestUtils.producerConfig(\n-                CLUSTER.bootstrapServers(),\n-                IntegerSerializer.class,\n-                StringSerializer.class,\n-                new Properties()\n-            ),\n+            STREAM_INPUT_ONE,\n+            Utils.mkSet(new KeyValue<>(1, \"A\"), new KeyValue<>(1, \"B\"), new KeyValue<>(1, \"C\")),\n+            prop,\n             mockTime.milliseconds()\n         );\n         IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n-            STREAM_INPUT,\n-            Collections.singletonList(new KeyValue<>(1, \"B\")),\n-            TestUtils.producerConfig(\n-                CLUSTER.bootstrapServers(),\n-                IntegerSerializer.class,\n-                StringSerializer.class,\n-                new Properties()\n-            ),\n+            STREAM_INPUT_TWO,\n+            Collections.singleton(new KeyValue<>(1, \"A\")),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk2NDcyNw=="}, "originalCommit": {"oid": "27fefff2508444c2d58e94e3b936660d62b340e0"}, "originalPosition": 204}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU1NTE4MDA0OnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxMTozMjozOVrOGIQCBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxMTozMjozOVrOGIQCBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTMwNDQ1Mg==", "bodyText": "Thank you! I forgot to delete this in my refactoring.", "url": "https://github.com/apache/kafka/pull/8514#discussion_r411304452", "createdAt": "2020-04-20T11:32:39Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java", "diffHunk": "@@ -319,18 +266,6 @@ private void checkMetricByName(final List<Metric> listMetric,\n         }\n     }\n \n-    private void verifyThatBytesWrittenTotalIncreases(final KafkaStreams kafkaStreams,\n-                                                      final String metricsScope) throws InterruptedException {\n-        final List<Metric> metric = getRocksDBMetrics(kafkaStreams, metricsScope).stream()\n-            .filter(m -> BYTES_WRITTEN_TOTAL.equals(m.metricName().name()))\n-            .collect(Collectors.toList());\n-        TestUtils.waitForCondition(\n-            () -> (double) metric.get(0).metricValue() > 0,\n-            TIMEOUT,\n-            () -> \"RocksDB metric bytes.written.total did not increase in \" + TIMEOUT + \" ms\"\n-        );\n-    }\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "897ff76770290ecca0b2344a7ffb9a975f0309e8"}, "originalPosition": 244}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3042, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}