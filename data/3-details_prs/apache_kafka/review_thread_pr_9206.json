{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDcxOTU5NDA3", "number": 9206, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQxNToxNjo0N1rOEmIwlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQxNjoxNTo0N1rOEmKdCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NDI0ODUyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogValidator.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQxNToxNjo0N1rOHV_aaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQxNTozMzo0NVrOHWAPrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjgyMTA5Nw==", "bodyText": "Have we benchmarked this path? It seems doubtful that these micro optimizations help given that we are converting.", "url": "https://github.com/apache/kafka/pull/9206#discussion_r492821097", "createdAt": "2020-09-22T15:16:47Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/LogValidator.scala", "diffHunk": "@@ -234,16 +234,17 @@ private[log] object LogValidator extends Logging {\n \n     val firstBatch = getFirstBatchAndMaybeValidateNoMoreBatches(records, NoCompressionCodec)\n \n-    for (batch <- records.batches.asScala) {\n+    records.batches.forEach { batch =>\n       validateBatch(topicPartition, firstBatch, batch, origin, toMagicValue, brokerTopicStats)\n \n       val recordErrors = new ArrayBuffer[ApiRecordError](0)\n-      for ((record, batchIndex) <- batch.asScala.view.zipWithIndex) {\n+      var batchIndex = 0\n+      batch.forEach { record =>\n         validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,\n           timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)\n         // we fail the batch if any record fails, so we stop appending if any record fails\n-        if (recordErrors.isEmpty)\n-          builder.appendWithOffset(offsetCounter.getAndIncrement(), record)\n+        if (recordErrors.isEmpty) builder.appendWithOffset(offsetCounter.getAndIncrement(), record)\n+        batchIndex += 1", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjgzNDczNQ==", "bodyText": "Have we benchmarked this path?\n\nI didn't benchmark this path and you are right that optimization is small as we have to convert data in this path. I will revert it to make small patch.", "url": "https://github.com/apache/kafka/pull/9206#discussion_r492834735", "createdAt": "2020-09-22T15:33:45Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/log/LogValidator.scala", "diffHunk": "@@ -234,16 +234,17 @@ private[log] object LogValidator extends Logging {\n \n     val firstBatch = getFirstBatchAndMaybeValidateNoMoreBatches(records, NoCompressionCodec)\n \n-    for (batch <- records.batches.asScala) {\n+    records.batches.forEach { batch =>\n       validateBatch(topicPartition, firstBatch, batch, origin, toMagicValue, brokerTopicStats)\n \n       val recordErrors = new ArrayBuffer[ApiRecordError](0)\n-      for ((record, batchIndex) <- batch.asScala.view.zipWithIndex) {\n+      var batchIndex = 0\n+      batch.forEach { record =>\n         validateRecord(batch, topicPartition, record, batchIndex, now, timestampType,\n           timestampDiffMaxMs, compactedTopic, brokerTopicStats).foreach(recordError => recordErrors += recordError)\n         // we fail the batch if any record fails, so we stop appending if any record fails\n-        if (recordErrors.isEmpty)\n-          builder.appendWithOffset(offsetCounter.getAndIncrement(), record)\n+        if (recordErrors.isEmpty) builder.appendWithOffset(offsetCounter.getAndIncrement(), record)\n+        batchIndex += 1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjgyMTA5Nw=="}, "originalCommit": null, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NDI2MTcxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogValidator.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQxNToxOTozNFrOHV_i7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQxNTozMzo1MVrOHWAQAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjgyMzI3OQ==", "bodyText": "Worth adding a comment here that this is a hot path and we want to avoid any unnecessary allocations.", "url": "https://github.com/apache/kafka/pull/9206#discussion_r492823279", "createdAt": "2020-09-22T15:19:34Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/LogValidator.scala", "diffHunk": "@@ -279,14 +280,15 @@ private[log] object LogValidator extends Logging {\n \n     val firstBatch = getFirstBatchAndMaybeValidateNoMoreBatches(records, NoCompressionCodec)\n \n-    for (batch <- records.batches.asScala) {\n+    records.batches.forEach { batch =>\n       validateBatch(topicPartition, firstBatch, batch, origin, magic, brokerTopicStats)\n \n       var maxBatchTimestamp = RecordBatch.NO_TIMESTAMP\n       var offsetOfMaxBatchTimestamp = -1L\n \n       val recordErrors = new ArrayBuffer[ApiRecordError](0)\n-      for ((record, batchIndex) <- batch.asScala.view.zipWithIndex) {\n+      var batchIndex = 0", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjgzNDgxOA==", "bodyText": "copy that", "url": "https://github.com/apache/kafka/pull/9206#discussion_r492834818", "createdAt": "2020-09-22T15:33:51Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/log/LogValidator.scala", "diffHunk": "@@ -279,14 +280,15 @@ private[log] object LogValidator extends Logging {\n \n     val firstBatch = getFirstBatchAndMaybeValidateNoMoreBatches(records, NoCompressionCodec)\n \n-    for (batch <- records.batches.asScala) {\n+    records.batches.forEach { batch =>\n       validateBatch(topicPartition, firstBatch, batch, origin, magic, brokerTopicStats)\n \n       var maxBatchTimestamp = RecordBatch.NO_TIMESTAMP\n       var offsetOfMaxBatchTimestamp = -1L\n \n       val recordErrors = new ArrayBuffer[ApiRecordError](0)\n-      for ((record, batchIndex) <- batch.asScala.view.zipWithIndex) {\n+      var batchIndex = 0", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjgyMzI3OQ=="}, "originalCommit": null, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4NDUyNjE4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogValidator.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQxNjoxNTo0N1rOHWCGzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQxNjoyNjoyMVrOHWCi2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjg2NTIzMA==", "bodyText": "I liked your changes to make the code more concise, I'd keep them.", "url": "https://github.com/apache/kafka/pull/9206#discussion_r492865230", "createdAt": "2020-09-22T16:15:47Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/LogValidator.scala", "diffHunk": "@@ -234,17 +234,16 @@ private[log] object LogValidator extends Logging {\n \n     val firstBatch = getFirstBatchAndMaybeValidateNoMoreBatches(records, NoCompressionCodec)\n \n-    records.batches.forEach { batch =>\n+    for (batch <- records.batches.asScala) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjg3MjQwOA==", "bodyText": "ok~", "url": "https://github.com/apache/kafka/pull/9206#discussion_r492872408", "createdAt": "2020-09-22T16:26:21Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/log/LogValidator.scala", "diffHunk": "@@ -234,17 +234,16 @@ private[log] object LogValidator extends Logging {\n \n     val firstBatch = getFirstBatchAndMaybeValidateNoMoreBatches(records, NoCompressionCodec)\n \n-    records.batches.forEach { batch =>\n+    for (batch <- records.batches.asScala) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mjg2NTIzMA=="}, "originalCommit": null, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1944, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}