{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA4MjQ4Njcx", "number": 8541, "title": "KAFKA-6145: KIP-441: Add TaskAssignor class config", "bodyText": "add a config to set the TaskAssignor\nset the default assignor to HighAvailabilityTaskAssignor\nfix broken tests\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-04-23T23:21:42Z", "url": "https://github.com/apache/kafka/pull/8541", "merged": true, "mergeCommit": {"oid": "5bb3415c77cc61b7d1591ccfe028d10bbf9f2a7a"}, "closed": true, "closedAt": "2020-04-28T20:57:12Z", "author": {"login": "vvcephei"}, "timelineItems": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcamQXhgH2gAyNDA4MjQ4NjcxOjk3MjI4YTVmYmRiOWI4ZDY0YWE2MDlkNTljMGI2NGU0ZmU3YmJlMGM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABccIXxXAFqTQwMjExMTk4MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "97228a5fbdb9b8d64aa609d59c0b64e4fe7bbe0c", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/97228a5fbdb9b8d64aa609d59c0b64e4fe7bbe0c", "committedDate": "2020-04-24T00:22:23Z", "message": "KAFKA-6145: KIP-441: Add TaskAssignor class config\n\n* add a config to set the TaskAssignor\n* set the default assignor to HighAvailabilityTaskAssignor\n* fix broken tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk5NTQzNDU2", "url": "https://github.com/apache/kafka/pull/8541#pullrequestreview-399543456", "createdAt": "2020-04-23T23:23:56Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QyMzoyMzo1N1rOGLADsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMDoyNTozMVrOGLBcJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE4ODQ2NQ==", "bodyText": "Necessary because the test name that JUnit generates for the parameterized StreamsPartitionAssignorTest is slightly too long. I have no way to shorten it because the thing that pushes it over is the fact that there are two package names in the parameterized method name, and there's no control over the format of the test name itself. So, I decided just to truncate the file name instead, which is almost certainly still unique for pretty much any test.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414188465", "createdAt": "2020-04-23T23:23:57Z", "author": {"login": "vvcephei"}, "path": "build.gradle", "diffHunk": "@@ -236,8 +236,10 @@ subprojects {\n     def logStreams = new HashMap<String, FileOutputStream>()\n     beforeTest { TestDescriptor td ->\n       def tid = testId(td)\n+      // truncate the file name if it's too long\n       def logFile = new File(\n-          \"${projectDir}/build/reports/testOutput/${tid}.test.stdout\")\n+              \"${projectDir}/build/reports/testOutput/${tid.substring(0, Math.min(tid.size(),240))}.test.stdout\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE4ODU5Mg==", "bodyText": "I've been wanting this for a while, so I just decided to add it.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414188592", "createdAt": "2020-04-23T23:24:17Z", "author": {"login": "vvcephei"}, "path": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java", "diffHunk": "@@ -1146,4 +1146,13 @@ private static byte checkRange(final byte i) {\n             }\n         };\n     }\n+\n+    @SafeVarargs\n+    public static <E> Set<E> union(final Supplier<Set<E>> constructor, final Set<E>... set) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE4ODcyMw==", "bodyText": "This is pointless unless we evaluate it inside the lambda.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414188723", "createdAt": "2020-04-23T23:24:40Z", "author": {"login": "vvcephei"}, "path": "clients/src/test/java/org/apache/kafka/test/TestUtils.java", "diffHunk": "@@ -361,9 +361,9 @@ public static void waitForCondition(final TestCondition testCondition, final lon\n      * avoid transient failures due to slow or overloaded machines.\n      */\n     public static void waitForCondition(final TestCondition testCondition, final long maxWaitMs, Supplier<String> conditionDetailsSupplier) throws InterruptedException {\n-        String conditionDetailsSupplied = conditionDetailsSupplier != null ? conditionDetailsSupplier.get() : null;\n-        String conditionDetails = conditionDetailsSupplied != null ? conditionDetailsSupplied : \"\";\n         retryOnExceptionWithTimeout(maxWaitMs, () -> {\n+            String conditionDetailsSupplied = conditionDetailsSupplier != null ? conditionDetailsSupplier.get() : null;\n+            String conditionDetails = conditionDetailsSupplied != null ? conditionDetailsSupplied : \"\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTU0Mw==", "bodyText": "This is a change. I decided that from the perspective of the TaskAssignor API, the lags are one of the inputs, so it doesn't make sense to invoke the assignor if the lags aren't present.\nThis potentially harms assignors that don't care about the lag (like the StickyTaskAssignor), but it also seems like if we can't compute the lags, then we probably can't do lots of other stuff that Streams needs to do anyway, so maybe it's not the worst thing in the world to schedule a \"retry\" on the assignment, even if it's not strictly necessary.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414191543", "createdAt": "2020-04-23T23:31:46Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTgzOA==", "bodyText": "Just to clarify everyone's roles, I added a new assignor whose only behavior is to return all previously owned tasks, and then assign any unowned tasks.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414191838", "createdAt": "2020-04-23T23:32:44Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {\n+            log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n+                         + \"trigger another rebalance to retry.\");\n+            setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n+            taskAssignor = new PriorTaskAssignor();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MjI3Ng==", "bodyText": "This constructor is currently only used in tests, but I'm planning a follow-on refactor that would actually use it from the StickyTaskAssignor as well.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414192276", "createdAt": "2020-04-23T23:33:50Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -86,6 +90,22 @@ private ClientState(final Set<TaskId> activeTasks,\n         this.capacity = capacity;\n     }\n \n+    public ClientState(final Set<TaskId> previousActiveTasks,\n+                       final Set<TaskId> previousStandbyTasks,\n+                       final Map<TaskId, Long> taskLagTotals,\n+                       final int capacity) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MjcwOA==", "bodyText": "All these fields have to be non-final now, because we're setting them in assign instead of the constructor.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414192708", "createdAt": "2020-04-23T23:34:59Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -16,49 +16,50 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClientOrNoCaughtUpClientsExist;\n-import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n-import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n-import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import java.util.Collection;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n import java.util.TreeSet;\n import java.util.UUID;\n import java.util.stream.Collectors;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n-import java.util.Map;\n-import java.util.Set;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClientOrNoCaughtUpClientsExist;\n+import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n+import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n \n public class HighAvailabilityTaskAssignor implements TaskAssignor {\n     private static final Logger log = LoggerFactory.getLogger(HighAvailabilityTaskAssignor.class);\n \n-    private final Map<UUID, ClientState> clientStates;\n-    private final Map<UUID, Integer> clientsToNumberOfThreads;\n-    private final SortedSet<UUID> sortedClients;\n+    private Map<UUID, ClientState> clientStates;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5Mjg1NQ==", "bodyText": "I found this log useful while debugging the integration tests. WDYT about keeping it?", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414192855", "createdAt": "2020-04-23T23:35:25Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -95,6 +94,11 @@ public boolean assign() {\n \n         assignStatelessActiveTasks();\n \n+        log.info(\"Decided on assignment: \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MzkwMQ==", "bodyText": "The StickyTaskAssignor is capable of satisfying the PriorTaskAssignor's contract, so we can just delegate to it. The important thing is that we now have two separately defined contracts:\n\nreturn all previous tasks and assign the rest (PriorTaskAssignor)\nstrike a balance between stickiness and balance (StickyTaskAssignor)\n\nThe fact that the implementation is shared is an ... implementation detail.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414193901", "createdAt": "2020-04-23T23:38:07Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/PriorTaskAssignor.java", "diffHunk": "@@ -0,0 +1,40 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+\n+public class PriorTaskAssignor implements TaskAssignor {\n+    private final StickyTaskAssignor delegate;\n+\n+    public PriorTaskAssignor() {\n+        delegate = new StickyTaskAssignor(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5NDg1Mg==", "bodyText": "While debugging this test, I found the \"garbage collection\" nomenclature confusing (because it is possible to invoke the JVM GC, but that's not what we're doing here), so I transitioned to calling it a \"stall\", which was also a term used in the test comments.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414194852", "createdAt": "2020-04-23T23:40:14Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -111,8 +115,9 @@\n     private final String storeName = \"store\";\n \n     private AtomicBoolean errorInjected;\n-    private AtomicBoolean gcInjected;\n-    private volatile boolean doGC = true;\n+    private AtomicBoolean stallInjected;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5NTY4MA==", "bodyText": "I added an argument to the KafkaStreams builder to set the dummy host name. Previously, it was always \"dummy\" even though we had two instances, which resulted in the metadata map only containing one entry, even though there were two nodes in the cluster. I'm not sure if this was a cause of flakiness (since it seems it would be non-deterministic), but it's definitely not right.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414195680", "createdAt": "2020-04-23T23:42:23Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -515,84 +520,114 @@ public void shouldNotViolateEosIfOneTaskGetsFencedUsingIsolatedAppInstances() th\n         // the app is supposed to copy all 60 records into the output topic\n         // the app commits after each 10 records per partition, and thus will have 2*5 uncommitted writes\n         //\n-        // a GC pause gets inject after 20 committed and 30 uncommitted records got received\n-        // -> the GC pause only affects one thread and should trigger a rebalance\n+        // a stall gets injected after 20 committed and 30 uncommitted records got received\n+        // -> the stall only affects one thread and should trigger a rebalance\n         // after rebalancing, we should read 40 committed records (even if 50 record got written)\n         //\n         // afterwards, the \"stalling\" thread resumes, and another rebalance should get triggered\n         // we write the remaining 20 records and verify to read 60 result records\n \n         try (\n-            final KafkaStreams streams1 = getKafkaStreams(false, \"appDir1\", 1, eosConfig);\n-            final KafkaStreams streams2 = getKafkaStreams(false, \"appDir2\", 1, eosConfig)\n+            final KafkaStreams streams1 = getKafkaStreams(\"streams1\", false, \"appDir1\", 1, eosConfig);\n+            final KafkaStreams streams2 = getKafkaStreams(\"streams2\", false, \"appDir2\", 1, eosConfig)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5NzU2NQ==", "bodyText": "The previous test was seemingly dependent on the non-stalling instance being the one to \"win\" and be present in the metadata map, which is why the metadatas for both instances were 1 before.\nNow, we're being a little more explicit, by actually finding out which instance is the stalled one. Then we can assert that the instance that isn't stalled (the only one still in the group) doesn't see the stalled instance anymore (since it has dropped out), and that it is now assigned both partitions.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414197565", "createdAt": "2020-04-23T23:47:22Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -515,84 +520,114 @@ public void shouldNotViolateEosIfOneTaskGetsFencedUsingIsolatedAppInstances() th\n         // the app is supposed to copy all 60 records into the output topic\n         // the app commits after each 10 records per partition, and thus will have 2*5 uncommitted writes\n         //\n-        // a GC pause gets inject after 20 committed and 30 uncommitted records got received\n-        // -> the GC pause only affects one thread and should trigger a rebalance\n+        // a stall gets injected after 20 committed and 30 uncommitted records got received\n+        // -> the stall only affects one thread and should trigger a rebalance\n         // after rebalancing, we should read 40 committed records (even if 50 record got written)\n         //\n         // afterwards, the \"stalling\" thread resumes, and another rebalance should get triggered\n         // we write the remaining 20 records and verify to read 60 result records\n \n         try (\n-            final KafkaStreams streams1 = getKafkaStreams(false, \"appDir1\", 1, eosConfig);\n-            final KafkaStreams streams2 = getKafkaStreams(false, \"appDir2\", 1, eosConfig)\n+            final KafkaStreams streams1 = getKafkaStreams(\"streams1\", false, \"appDir1\", 1, eosConfig);\n+            final KafkaStreams streams2 = getKafkaStreams(\"streams2\", false, \"appDir2\", 1, eosConfig)\n         ) {\n             startKafkaStreamsAndWaitForRunningState(streams1, MAX_WAIT_TIME_MS);\n             startKafkaStreamsAndWaitForRunningState(streams2, MAX_WAIT_TIME_MS);\n \n-            final List<KeyValue<Long, Long>> committedDataBeforeGC = prepareData(0L, 10L, 0L, 1L);\n-            final List<KeyValue<Long, Long>> uncommittedDataBeforeGC = prepareData(10L, 15L, 0L, 1L);\n+            final List<KeyValue<Long, Long>> committedDataBeforeStall = prepareData(0L, 10L, 0L, 1L);\n+            final List<KeyValue<Long, Long>> uncommittedDataBeforeStall = prepareData(10L, 15L, 0L, 1L);\n \n-            final List<KeyValue<Long, Long>> dataBeforeGC = new ArrayList<>();\n-            dataBeforeGC.addAll(committedDataBeforeGC);\n-            dataBeforeGC.addAll(uncommittedDataBeforeGC);\n+            final List<KeyValue<Long, Long>> dataBeforeStall = new ArrayList<>();\n+            dataBeforeStall.addAll(committedDataBeforeStall);\n+            dataBeforeStall.addAll(uncommittedDataBeforeStall);\n \n             final List<KeyValue<Long, Long>> dataToTriggerFirstRebalance = prepareData(15L, 20L, 0L, 1L);\n \n             final List<KeyValue<Long, Long>> dataAfterSecondRebalance = prepareData(20L, 30L, 0L, 1L);\n \n-            writeInputData(committedDataBeforeGC);\n+            writeInputData(committedDataBeforeStall);\n \n             waitForCondition(\n                 () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n                 \"SteamsTasks did not request commit.\");\n \n-            writeInputData(uncommittedDataBeforeGC);\n+            writeInputData(uncommittedDataBeforeStall);\n \n-            final List<KeyValue<Long, Long>> uncommittedRecords = readResult(dataBeforeGC.size(), null);\n-            final List<KeyValue<Long, Long>> committedRecords = readResult(committedDataBeforeGC.size(), CONSUMER_GROUP_ID);\n+            final List<KeyValue<Long, Long>> uncommittedRecords = readResult(dataBeforeStall.size(), null);\n+            final List<KeyValue<Long, Long>> committedRecords = readResult(committedDataBeforeStall.size(), CONSUMER_GROUP_ID);\n \n-            checkResultPerKey(committedRecords, committedDataBeforeGC);\n-            checkResultPerKey(uncommittedRecords, dataBeforeGC);\n+            checkResultPerKey(committedRecords, committedDataBeforeStall);\n+            checkResultPerKey(uncommittedRecords, dataBeforeStall);\n \n-            gcInjected.set(true);\n+            LOG.info(\"Injecting Stall\");\n+            stallInjected.set(true);\n             writeInputData(dataToTriggerFirstRebalance);\n+            LOG.info(\"Input Data Written\");\n+            waitForCondition(\n+                () -> stallingHost.get() != null,\n+                MAX_WAIT_TIME_MS,\n+                \"Expected a host to start stalling\"\n+            );\n+            final String observedStallingHost = stallingHost.get();\n+            final KafkaStreams stallingInstance;\n+            final KafkaStreams remainingInstance;\n+            if (\"streams1\".equals(observedStallingHost)) {\n+                stallingInstance = streams1;\n+                remainingInstance = streams2;\n+            } else if (\"streams2\".equals(observedStallingHost)) {\n+                stallingInstance = streams2;\n+                remainingInstance = streams1;\n+            } else {\n+                throw new IllegalArgumentException(\"unexpected host name: \" + observedStallingHost);\n+            }\n \n+            // the stalling instance won't have an updated view, and it doesn't matter what it thinks\n+            // the assignment is. We only really care that the remaining instance only sees one host\n+            // that owns both partitions.\n             waitForCondition(\n-                () -> streams1.allMetadata().size() == 1\n-                    && streams2.allMetadata().size() == 1\n-                    && (streams1.allMetadata().iterator().next().topicPartitions().size() == 2\n-                        || streams2.allMetadata().iterator().next().topicPartitions().size() == 2),\n-                MAX_WAIT_TIME_MS, \"Should have rebalanced.\");\n+                () -> stallingInstance.allMetadata().size() == 2\n+                    && remainingInstance.allMetadata().size() == 1\n+                    && remainingInstance.allMetadata().iterator().next().topicPartitions().size() == 2,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5Nzg5Nw==", "bodyText": "Again, 2 was always the right answer, we were just accidentally overwriting one instance's metadata with the other.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414197897", "createdAt": "2020-04-23T23:48:17Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -515,84 +520,114 @@ public void shouldNotViolateEosIfOneTaskGetsFencedUsingIsolatedAppInstances() th\n         // the app is supposed to copy all 60 records into the output topic\n         // the app commits after each 10 records per partition, and thus will have 2*5 uncommitted writes\n         //\n-        // a GC pause gets inject after 20 committed and 30 uncommitted records got received\n-        // -> the GC pause only affects one thread and should trigger a rebalance\n+        // a stall gets injected after 20 committed and 30 uncommitted records got received\n+        // -> the stall only affects one thread and should trigger a rebalance\n         // after rebalancing, we should read 40 committed records (even if 50 record got written)\n         //\n         // afterwards, the \"stalling\" thread resumes, and another rebalance should get triggered\n         // we write the remaining 20 records and verify to read 60 result records\n \n         try (\n-            final KafkaStreams streams1 = getKafkaStreams(false, \"appDir1\", 1, eosConfig);\n-            final KafkaStreams streams2 = getKafkaStreams(false, \"appDir2\", 1, eosConfig)\n+            final KafkaStreams streams1 = getKafkaStreams(\"streams1\", false, \"appDir1\", 1, eosConfig);\n+            final KafkaStreams streams2 = getKafkaStreams(\"streams2\", false, \"appDir2\", 1, eosConfig)\n         ) {\n             startKafkaStreamsAndWaitForRunningState(streams1, MAX_WAIT_TIME_MS);\n             startKafkaStreamsAndWaitForRunningState(streams2, MAX_WAIT_TIME_MS);\n \n-            final List<KeyValue<Long, Long>> committedDataBeforeGC = prepareData(0L, 10L, 0L, 1L);\n-            final List<KeyValue<Long, Long>> uncommittedDataBeforeGC = prepareData(10L, 15L, 0L, 1L);\n+            final List<KeyValue<Long, Long>> committedDataBeforeStall = prepareData(0L, 10L, 0L, 1L);\n+            final List<KeyValue<Long, Long>> uncommittedDataBeforeStall = prepareData(10L, 15L, 0L, 1L);\n \n-            final List<KeyValue<Long, Long>> dataBeforeGC = new ArrayList<>();\n-            dataBeforeGC.addAll(committedDataBeforeGC);\n-            dataBeforeGC.addAll(uncommittedDataBeforeGC);\n+            final List<KeyValue<Long, Long>> dataBeforeStall = new ArrayList<>();\n+            dataBeforeStall.addAll(committedDataBeforeStall);\n+            dataBeforeStall.addAll(uncommittedDataBeforeStall);\n \n             final List<KeyValue<Long, Long>> dataToTriggerFirstRebalance = prepareData(15L, 20L, 0L, 1L);\n \n             final List<KeyValue<Long, Long>> dataAfterSecondRebalance = prepareData(20L, 30L, 0L, 1L);\n \n-            writeInputData(committedDataBeforeGC);\n+            writeInputData(committedDataBeforeStall);\n \n             waitForCondition(\n                 () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n                 \"SteamsTasks did not request commit.\");\n \n-            writeInputData(uncommittedDataBeforeGC);\n+            writeInputData(uncommittedDataBeforeStall);\n \n-            final List<KeyValue<Long, Long>> uncommittedRecords = readResult(dataBeforeGC.size(), null);\n-            final List<KeyValue<Long, Long>> committedRecords = readResult(committedDataBeforeGC.size(), CONSUMER_GROUP_ID);\n+            final List<KeyValue<Long, Long>> uncommittedRecords = readResult(dataBeforeStall.size(), null);\n+            final List<KeyValue<Long, Long>> committedRecords = readResult(committedDataBeforeStall.size(), CONSUMER_GROUP_ID);\n \n-            checkResultPerKey(committedRecords, committedDataBeforeGC);\n-            checkResultPerKey(uncommittedRecords, dataBeforeGC);\n+            checkResultPerKey(committedRecords, committedDataBeforeStall);\n+            checkResultPerKey(uncommittedRecords, dataBeforeStall);\n \n-            gcInjected.set(true);\n+            LOG.info(\"Injecting Stall\");\n+            stallInjected.set(true);\n             writeInputData(dataToTriggerFirstRebalance);\n+            LOG.info(\"Input Data Written\");\n+            waitForCondition(\n+                () -> stallingHost.get() != null,\n+                MAX_WAIT_TIME_MS,\n+                \"Expected a host to start stalling\"\n+            );\n+            final String observedStallingHost = stallingHost.get();\n+            final KafkaStreams stallingInstance;\n+            final KafkaStreams remainingInstance;\n+            if (\"streams1\".equals(observedStallingHost)) {\n+                stallingInstance = streams1;\n+                remainingInstance = streams2;\n+            } else if (\"streams2\".equals(observedStallingHost)) {\n+                stallingInstance = streams2;\n+                remainingInstance = streams1;\n+            } else {\n+                throw new IllegalArgumentException(\"unexpected host name: \" + observedStallingHost);\n+            }\n \n+            // the stalling instance won't have an updated view, and it doesn't matter what it thinks\n+            // the assignment is. We only really care that the remaining instance only sees one host\n+            // that owns both partitions.\n             waitForCondition(\n-                () -> streams1.allMetadata().size() == 1\n-                    && streams2.allMetadata().size() == 1\n-                    && (streams1.allMetadata().iterator().next().topicPartitions().size() == 2\n-                        || streams2.allMetadata().iterator().next().topicPartitions().size() == 2),\n-                MAX_WAIT_TIME_MS, \"Should have rebalanced.\");\n+                () -> stallingInstance.allMetadata().size() == 2\n+                    && remainingInstance.allMetadata().size() == 1\n+                    && remainingInstance.allMetadata().iterator().next().topicPartitions().size() == 2,\n+                MAX_WAIT_TIME_MS,\n+                () -> \"Should have rebalanced.\\n\" +\n+                    \"Streams1[\" + streams1.allMetadata() + \"]\\n\" +\n+                    \"Streams2[\" + streams2.allMetadata() + \"]\");\n \n             final List<KeyValue<Long, Long>> committedRecordsAfterRebalance = readResult(\n-                uncommittedDataBeforeGC.size() + dataToTriggerFirstRebalance.size(),\n+                uncommittedDataBeforeStall.size() + dataToTriggerFirstRebalance.size(),\n                 CONSUMER_GROUP_ID);\n \n             final List<KeyValue<Long, Long>> expectedCommittedRecordsAfterRebalance = new ArrayList<>();\n-            expectedCommittedRecordsAfterRebalance.addAll(uncommittedDataBeforeGC);\n+            expectedCommittedRecordsAfterRebalance.addAll(uncommittedDataBeforeStall);\n             expectedCommittedRecordsAfterRebalance.addAll(dataToTriggerFirstRebalance);\n \n             checkResultPerKey(committedRecordsAfterRebalance, expectedCommittedRecordsAfterRebalance);\n \n-            doGC = false;\n+            LOG.info(\"Releasing Stall\");\n+            doStall = false;\n+            // Once the stalling host rejoins the group, we expect both instances to see both instances.\n+            // It doesn't really matter what the assignment is, but we might as well also assert that they\n+            // both see both partitions assigned exactly once\n             waitForCondition(\n-                () -> streams1.allMetadata().size() == 1\n-                    && streams2.allMetadata().size() == 1\n-                    && streams1.allMetadata().iterator().next().topicPartitions().size() == 1\n-                    && streams2.allMetadata().iterator().next().topicPartitions().size() == 1,\n+                () -> streams1.allMetadata().size() == 2\n+                    && streams2.allMetadata().size() == 2", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5ODI2OQ==", "bodyText": "The prior expectation was dependent on the rebalance algorithm's behavior. Now, we relax it and just ensure that all the partitions are assigned.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414198269", "createdAt": "2020-04-23T23:49:10Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -515,84 +520,114 @@ public void shouldNotViolateEosIfOneTaskGetsFencedUsingIsolatedAppInstances() th\n         // the app is supposed to copy all 60 records into the output topic\n         // the app commits after each 10 records per partition, and thus will have 2*5 uncommitted writes\n         //\n-        // a GC pause gets inject after 20 committed and 30 uncommitted records got received\n-        // -> the GC pause only affects one thread and should trigger a rebalance\n+        // a stall gets injected after 20 committed and 30 uncommitted records got received\n+        // -> the stall only affects one thread and should trigger a rebalance\n         // after rebalancing, we should read 40 committed records (even if 50 record got written)\n         //\n         // afterwards, the \"stalling\" thread resumes, and another rebalance should get triggered\n         // we write the remaining 20 records and verify to read 60 result records\n \n         try (\n-            final KafkaStreams streams1 = getKafkaStreams(false, \"appDir1\", 1, eosConfig);\n-            final KafkaStreams streams2 = getKafkaStreams(false, \"appDir2\", 1, eosConfig)\n+            final KafkaStreams streams1 = getKafkaStreams(\"streams1\", false, \"appDir1\", 1, eosConfig);\n+            final KafkaStreams streams2 = getKafkaStreams(\"streams2\", false, \"appDir2\", 1, eosConfig)\n         ) {\n             startKafkaStreamsAndWaitForRunningState(streams1, MAX_WAIT_TIME_MS);\n             startKafkaStreamsAndWaitForRunningState(streams2, MAX_WAIT_TIME_MS);\n \n-            final List<KeyValue<Long, Long>> committedDataBeforeGC = prepareData(0L, 10L, 0L, 1L);\n-            final List<KeyValue<Long, Long>> uncommittedDataBeforeGC = prepareData(10L, 15L, 0L, 1L);\n+            final List<KeyValue<Long, Long>> committedDataBeforeStall = prepareData(0L, 10L, 0L, 1L);\n+            final List<KeyValue<Long, Long>> uncommittedDataBeforeStall = prepareData(10L, 15L, 0L, 1L);\n \n-            final List<KeyValue<Long, Long>> dataBeforeGC = new ArrayList<>();\n-            dataBeforeGC.addAll(committedDataBeforeGC);\n-            dataBeforeGC.addAll(uncommittedDataBeforeGC);\n+            final List<KeyValue<Long, Long>> dataBeforeStall = new ArrayList<>();\n+            dataBeforeStall.addAll(committedDataBeforeStall);\n+            dataBeforeStall.addAll(uncommittedDataBeforeStall);\n \n             final List<KeyValue<Long, Long>> dataToTriggerFirstRebalance = prepareData(15L, 20L, 0L, 1L);\n \n             final List<KeyValue<Long, Long>> dataAfterSecondRebalance = prepareData(20L, 30L, 0L, 1L);\n \n-            writeInputData(committedDataBeforeGC);\n+            writeInputData(committedDataBeforeStall);\n \n             waitForCondition(\n                 () -> commitRequested.get() == 2, MAX_WAIT_TIME_MS,\n                 \"SteamsTasks did not request commit.\");\n \n-            writeInputData(uncommittedDataBeforeGC);\n+            writeInputData(uncommittedDataBeforeStall);\n \n-            final List<KeyValue<Long, Long>> uncommittedRecords = readResult(dataBeforeGC.size(), null);\n-            final List<KeyValue<Long, Long>> committedRecords = readResult(committedDataBeforeGC.size(), CONSUMER_GROUP_ID);\n+            final List<KeyValue<Long, Long>> uncommittedRecords = readResult(dataBeforeStall.size(), null);\n+            final List<KeyValue<Long, Long>> committedRecords = readResult(committedDataBeforeStall.size(), CONSUMER_GROUP_ID);\n \n-            checkResultPerKey(committedRecords, committedDataBeforeGC);\n-            checkResultPerKey(uncommittedRecords, dataBeforeGC);\n+            checkResultPerKey(committedRecords, committedDataBeforeStall);\n+            checkResultPerKey(uncommittedRecords, dataBeforeStall);\n \n-            gcInjected.set(true);\n+            LOG.info(\"Injecting Stall\");\n+            stallInjected.set(true);\n             writeInputData(dataToTriggerFirstRebalance);\n+            LOG.info(\"Input Data Written\");\n+            waitForCondition(\n+                () -> stallingHost.get() != null,\n+                MAX_WAIT_TIME_MS,\n+                \"Expected a host to start stalling\"\n+            );\n+            final String observedStallingHost = stallingHost.get();\n+            final KafkaStreams stallingInstance;\n+            final KafkaStreams remainingInstance;\n+            if (\"streams1\".equals(observedStallingHost)) {\n+                stallingInstance = streams1;\n+                remainingInstance = streams2;\n+            } else if (\"streams2\".equals(observedStallingHost)) {\n+                stallingInstance = streams2;\n+                remainingInstance = streams1;\n+            } else {\n+                throw new IllegalArgumentException(\"unexpected host name: \" + observedStallingHost);\n+            }\n \n+            // the stalling instance won't have an updated view, and it doesn't matter what it thinks\n+            // the assignment is. We only really care that the remaining instance only sees one host\n+            // that owns both partitions.\n             waitForCondition(\n-                () -> streams1.allMetadata().size() == 1\n-                    && streams2.allMetadata().size() == 1\n-                    && (streams1.allMetadata().iterator().next().topicPartitions().size() == 2\n-                        || streams2.allMetadata().iterator().next().topicPartitions().size() == 2),\n-                MAX_WAIT_TIME_MS, \"Should have rebalanced.\");\n+                () -> stallingInstance.allMetadata().size() == 2\n+                    && remainingInstance.allMetadata().size() == 1\n+                    && remainingInstance.allMetadata().iterator().next().topicPartitions().size() == 2,\n+                MAX_WAIT_TIME_MS,\n+                () -> \"Should have rebalanced.\\n\" +\n+                    \"Streams1[\" + streams1.allMetadata() + \"]\\n\" +\n+                    \"Streams2[\" + streams2.allMetadata() + \"]\");\n \n             final List<KeyValue<Long, Long>> committedRecordsAfterRebalance = readResult(\n-                uncommittedDataBeforeGC.size() + dataToTriggerFirstRebalance.size(),\n+                uncommittedDataBeforeStall.size() + dataToTriggerFirstRebalance.size(),\n                 CONSUMER_GROUP_ID);\n \n             final List<KeyValue<Long, Long>> expectedCommittedRecordsAfterRebalance = new ArrayList<>();\n-            expectedCommittedRecordsAfterRebalance.addAll(uncommittedDataBeforeGC);\n+            expectedCommittedRecordsAfterRebalance.addAll(uncommittedDataBeforeStall);\n             expectedCommittedRecordsAfterRebalance.addAll(dataToTriggerFirstRebalance);\n \n             checkResultPerKey(committedRecordsAfterRebalance, expectedCommittedRecordsAfterRebalance);\n \n-            doGC = false;\n+            LOG.info(\"Releasing Stall\");\n+            doStall = false;\n+            // Once the stalling host rejoins the group, we expect both instances to see both instances.\n+            // It doesn't really matter what the assignment is, but we might as well also assert that they\n+            // both see both partitions assigned exactly once\n             waitForCondition(\n-                () -> streams1.allMetadata().size() == 1\n-                    && streams2.allMetadata().size() == 1\n-                    && streams1.allMetadata().iterator().next().topicPartitions().size() == 1\n-                    && streams2.allMetadata().iterator().next().topicPartitions().size() == 1,\n+                () -> streams1.allMetadata().size() == 2\n+                    && streams2.allMetadata().size() == 2\n+                    && streams1.allMetadata().stream().mapToLong(meta -> meta.topicPartitions().size()).sum() == 2\n+                    && streams2.allMetadata().stream().mapToLong(meta -> meta.topicPartitions().size()).sum() == 2,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIwNTIxNQ==", "bodyText": "I'm not sure if we should really delete this test. The restore test is fine, but the rebalance test fails 50% of the time. Here's the situation:\n\nThe test relies on the second instance being the one to get the standby replica, which just happens to be the behavior of the StickyTaskAssignor.\nMaking this test agnostic to the assignor's choice is extremely difficult because different barriers and latches need to be passed into the \"standby\" instance than the \"active\" one.\nI looked into the allLocalStorePartitionLags() implementation, and it doesn't look like there's any way in which the rebalance lifecycle could affect users' ability to query the lags. I'm not sure if a prior version of the code made this more plausible.\n\nReally, the rationale to delete the test is (3). I figured that out because I was investigating the possibility of writing a unit test instead, to make sure you could query the lags in each phase of the rebalance, but I came up blank because there seems to be no relationship at all between lag computation and rebalancing.\nIf we don't want to delete the test, then we could consider setting the TaskAssignor to the (new) PriorTaskAssignor, which would actually guarantee in a reliable way that the second instance gets the standby task.\nBut before just \"fixing\" the test, I wanted to double-check that we really need this test at all.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414205215", "createdAt": "2020-04-24T00:08:53Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java", "diffHunk": "@@ -1,349 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements. See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License. You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.kafka.streams.integration;\n-\n-import static org.apache.kafka.common.utils.Utils.mkSet;\n-import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.startApplicationAndWaitUntilRunning;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.core.IsEqual.equalTo;\n-import static org.junit.Assert.assertTrue;\n-\n-import java.io.File;\n-import java.nio.file.Files;\n-import java.nio.file.Path;\n-import java.time.Duration;\n-import java.util.ArrayList;\n-import java.util.Collections;\n-import java.util.Comparator;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Properties;\n-import java.util.concurrent.CountDownLatch;\n-import java.util.concurrent.CyclicBarrier;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicReference;\n-import kafka.utils.MockTime;\n-import org.apache.kafka.clients.consumer.ConsumerConfig;\n-import org.apache.kafka.common.TopicPartition;\n-import org.apache.kafka.common.serialization.LongDeserializer;\n-import org.apache.kafka.common.serialization.LongSerializer;\n-import org.apache.kafka.common.serialization.Serdes;\n-import org.apache.kafka.common.serialization.StringDeserializer;\n-import org.apache.kafka.common.serialization.StringSerializer;\n-import org.apache.kafka.streams.KafkaStreams;\n-import org.apache.kafka.streams.KafkaStreamsWrapper;\n-import org.apache.kafka.streams.KeyValue;\n-import org.apache.kafka.streams.LagInfo;\n-import org.apache.kafka.streams.StreamsBuilder;\n-import org.apache.kafka.streams.StreamsConfig;\n-import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n-import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n-import org.apache.kafka.streams.kstream.KTable;\n-import org.apache.kafka.streams.kstream.Materialized;\n-import org.apache.kafka.streams.processor.StateRestoreListener;\n-import org.apache.kafka.streams.processor.internals.StreamThread;\n-import org.apache.kafka.test.IntegrationTest;\n-import org.apache.kafka.test.TestUtils;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.ClassRule;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.experimental.categories.Category;\n-import org.junit.rules.TestName;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-@Category({IntegrationTest.class})\n-public class LagFetchIntegrationTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIwNTYxOQ==", "bodyText": "I moved this and other tests from StreamsPartitionAssignorTest that had been guarded to only actually run when parameterized with \"high availability\"", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414205619", "createdAt": "2020-04-24T00:10:04Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/HighAvailabilityStreamsPartitionAssignorTest.java", "diffHunk": "@@ -0,0 +1,326 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.admin.Admin;\n+import org.apache.kafka.clients.admin.AdminClient;\n+import org.apache.kafka.clients.admin.ListOffsetsResult;\n+import org.apache.kafka.clients.admin.ListOffsetsResult.ListOffsetsResultInfo;\n+import org.apache.kafka.clients.consumer.ConsumerPartitionAssignor.Assignment;\n+import org.apache.kafka.clients.consumer.ConsumerPartitionAssignor.GroupSubscription;\n+import org.apache.kafka.clients.consumer.ConsumerPartitionAssignor.Subscription;\n+import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.internals.KafkaFutureImpl;\n+import org.apache.kafka.common.utils.MockTime;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.StreamsConfig.InternalConfig;\n+import org.apache.kafka.streams.errors.StreamsException;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignmentInfo;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorError;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo;\n+import org.apache.kafka.test.MockClientSupplier;\n+import org.apache.kafka.test.MockInternalTopicManager;\n+import org.apache.kafka.test.MockKeyValueStoreBuilder;\n+import org.apache.kafka.test.MockProcessorSupplier;\n+import org.easymock.EasyMock;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+import static java.util.Arrays.asList;\n+import static java.util.Collections.emptyMap;\n+import static java.util.Collections.emptySet;\n+import static java.util.Collections.singletonList;\n+import static java.util.Collections.singletonMap;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.EMPTY_CHANGELOG_END_OFFSETS;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.EMPTY_TASKS;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_0;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_1;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_2;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_1;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n+import static org.apache.kafka.streams.processor.internals.assignment.StreamsAssignmentProtocolVersions.LATEST_SUPPORTED_VERSION;\n+import static org.easymock.EasyMock.anyObject;\n+import static org.easymock.EasyMock.expect;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertTrue;\n+\n+public class HighAvailabilityStreamsPartitionAssignorTest {\n+\n+    private final List<PartitionInfo> infos = asList(\n+        new PartitionInfo(\"topic1\", 0, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic1\", 1, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic1\", 2, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic2\", 0, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic2\", 1, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic2\", 2, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic3\", 0, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic3\", 1, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic3\", 2, Node.noNode(), new Node[0], new Node[0]),\n+        new PartitionInfo(\"topic3\", 3, Node.noNode(), new Node[0], new Node[0])\n+    );\n+\n+    private final Cluster metadata = new Cluster(\n+        \"cluster\",\n+        singletonList(Node.noNode()),\n+        infos,\n+        emptySet(),\n+        emptySet());\n+\n+    private final StreamsPartitionAssignor partitionAssignor = new StreamsPartitionAssignor();\n+    private final MockClientSupplier mockClientSupplier = new MockClientSupplier();\n+    private static final String USER_END_POINT = \"localhost:8080\";\n+    private static final String APPLICATION_ID = \"stream-partition-assignor-test\";\n+\n+    private TaskManager taskManager;\n+    private Admin adminClient;\n+    private StreamsConfig streamsConfig = new StreamsConfig(configProps());\n+    private final InternalTopologyBuilder builder = new InternalTopologyBuilder();\n+    private final StreamsMetadataState streamsMetadataState = EasyMock.createNiceMock(StreamsMetadataState.class);\n+    private final Map<String, Subscription> subscriptions = new HashMap<>();\n+\n+    private final AtomicInteger assignmentError = new AtomicInteger();\n+    private final AtomicLong nextProbingRebalanceMs = new AtomicLong(Long.MAX_VALUE);\n+    private final MockTime time = new MockTime();\n+\n+    private Map<String, Object> configProps() {\n+        final Map<String, Object> configurationMap = new HashMap<>();\n+        configurationMap.put(StreamsConfig.APPLICATION_ID_CONFIG, APPLICATION_ID);\n+        configurationMap.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, USER_END_POINT);\n+        configurationMap.put(InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR, taskManager);\n+        configurationMap.put(InternalConfig.STREAMS_METADATA_STATE_FOR_PARTITION_ASSIGNOR, streamsMetadataState);\n+        configurationMap.put(InternalConfig.STREAMS_ADMIN_CLIENT, adminClient);\n+        configurationMap.put(InternalConfig.ASSIGNMENT_ERROR_CODE, assignmentError);\n+        configurationMap.put(InternalConfig.NEXT_PROBING_REBALANCE_MS, nextProbingRebalanceMs);\n+        configurationMap.put(InternalConfig.TIME, time);\n+        configurationMap.put(AssignorConfiguration.INTERNAL_TASK_ASSIGNOR_CLASS, HighAvailabilityTaskAssignor.class.getName());\n+        return configurationMap;\n+    }\n+\n+    // Make sure to complete setting up any mocks (such as TaskManager or AdminClient) before configuring the assignor\n+    private void configureDefaultPartitionAssignor() {\n+        configurePartitionAssignorWith(emptyMap());\n+    }\n+\n+    // Make sure to complete setting up any mocks (such as TaskManager or AdminClient) before configuring the assignor\n+    private void configurePartitionAssignorWith(final Map<String, Object> props) {\n+        final Map<String, Object> configMap = configProps();\n+        configMap.putAll(props);\n+\n+        streamsConfig = new StreamsConfig(configMap);\n+        partitionAssignor.configure(configMap);\n+        EasyMock.replay(taskManager, adminClient);\n+\n+        overwriteInternalTopicManagerWithMock();\n+    }\n+\n+    // Useful for tests that don't care about the task offset sums\n+    private void createMockTaskManager(final Set<TaskId> activeTasks) {\n+        createMockTaskManager(getTaskOffsetSums(activeTasks));\n+    }\n+\n+    private void createMockTaskManager(final Map<TaskId, Long> taskOffsetSums) {\n+        taskManager = EasyMock.createNiceMock(TaskManager.class);\n+        expect(taskManager.builder()).andReturn(builder).anyTimes();\n+        expect(taskManager.getTaskOffsetSums()).andReturn(taskOffsetSums).anyTimes();\n+        expect(taskManager.processId()).andReturn(UUID_1).anyTimes();\n+        builder.setApplicationId(APPLICATION_ID);\n+        builder.buildTopology();\n+    }\n+\n+    // If you don't care about setting the end offsets for each specific topic partition, the helper method\n+    // getTopicPartitionOffsetMap is useful for building this input map for all partitions\n+    private void createMockAdminClient(final Map<TopicPartition, Long> changelogEndOffsets) {\n+        adminClient = EasyMock.createMock(AdminClient.class);\n+\n+        final ListOffsetsResult result = EasyMock.createNiceMock(ListOffsetsResult.class);\n+        final KafkaFutureImpl<Map<TopicPartition, ListOffsetsResultInfo>> allFuture = new KafkaFutureImpl<>();\n+        allFuture.complete(changelogEndOffsets.entrySet().stream().collect(Collectors.toMap(\n+            Entry::getKey,\n+            t -> {\n+                final ListOffsetsResultInfo info = EasyMock.createNiceMock(ListOffsetsResultInfo.class);\n+                expect(info.offset()).andStubReturn(t.getValue());\n+                EasyMock.replay(info);\n+                return info;\n+            }))\n+        );\n+\n+        expect(adminClient.listOffsets(anyObject())).andStubReturn(result);\n+        expect(result.all()).andReturn(allFuture);\n+\n+        EasyMock.replay(result);\n+    }\n+\n+    private void overwriteInternalTopicManagerWithMock() {\n+        final MockInternalTopicManager mockInternalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);\n+        partitionAssignor.setInternalTopicManager(mockInternalTopicManager);\n+    }\n+\n+    @Before\n+    public void setUp() {\n+        createMockAdminClient(EMPTY_CHANGELOG_END_OFFSETS);\n+    }\n+\n+\n+    @Test\n+    public void shouldReturnAllActiveTasksToPreviousOwnerRegardlessOfBalanceAndTriggerRebalanceIfEndOffsetFetchFailsAndHighAvailabilityEnabled() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIwNjA1Mg==", "bodyText": "This is why I moved these methods to their own test, so that this class can focus on verifying behavior that is invariant with respect to the parameterized assignor.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414206052", "createdAt": "2020-04-24T00:11:13Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java", "diffHunk": "@@ -1832,102 +1834,6 @@ public void shouldThrowIllegalStateExceptionIfAnyTopicsMissingFromChangelogEndOf\n         assertThrows(IllegalStateException.class, () -> partitionAssignor.assign(metadata, new GroupSubscription(subscriptions)));\n     }\n \n-    @Test\n-    public void shouldReturnAllActiveTasksToPreviousOwnerRegardlessOfBalanceAndTriggerRebalanceIfEndOffsetFetchFailsAndHighAvailabilityEnabled() {\n-        if (highAvailabilityEnabled) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIwODY3Mw==", "bodyText": "I made a bunch of changes to this test, because it was pretty brittle with respect to changes in the HighAvailabilityTaskAssignor. For context, this is the second time I've touched the assignment code since we introduced the HATA, and it's the second time I've had to deal with irrelevant test failures in this class.\nFirst, I replaced the ClientState mocks with \"real\" ClientStates, constructed to represent the desired scenario for each test. Mocks are really more appropriate for isolating a component from external components (like mocking a remote service). Mocking data types leads to verifying that a specific set of queries happens against the data type, which is likely to break any time the logic under test changes in any way. Another problem with data-type mocks is that they can violate the invariants of the data type itself. For example, you can mock a list that both isEmpty and contains items. In our case, we threw NPEs in the assignor that could never happen in production when the mocked assigned/standby tasks didn't agree with the assigned tasks or the stateful assigned tasks weren't mocked to agree with the lags. Now, we just construct a ClientState for each client, representing the desired scenario and make assertions on the resulting assignment.\nSecond, the tests as written rely heavily on shared mutable fields inserted into shared mutable collections to build the assignor. This can be a good way to minimize the text inside the test method, which lets readers focus on the proper logic of the test itself. However, it makes it harder to understand the full context of a test, and it also raises the possibility of tests polluting each others' environments. Since in this particular case, localizing all the setup code is about as compact as factoring it out, I went ahead and minimized the shared fields, and eliminated the mutability, the tests are self-contained.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414208673", "createdAt": "2020-04-24T00:18:44Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -41,132 +54,107 @@\n import static org.easymock.EasyMock.replay;\n import static org.hamcrest.CoreMatchers.equalTo;\n import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertTrue;\n-\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.UUID;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n-import org.easymock.EasyMock;\n-import org.junit.Test;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n \n public class HighAvailabilityTaskAssignorTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIxMDY1Mw==", "bodyText": "These first tests are really the only ones to change. They're still asserting the same basic fact, but previousAssignmentIsValid is now an internal method, so we instead make assertions about the black-box semantics of the assignor, instead of a specific \"visible for testing\" method.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414210653", "createdAt": "2020-04-24T00:24:04Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -41,132 +54,107 @@\n import static org.easymock.EasyMock.replay;\n import static org.hamcrest.CoreMatchers.equalTo;\n import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertTrue;\n-\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.UUID;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n-import org.easymock.EasyMock;\n-import org.junit.Test;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n \n public class HighAvailabilityTaskAssignorTest {\n-    private long acceptableRecoveryLag = 100L;\n-    private int balanceFactor = 1;\n-    private int maxWarmupReplicas = 2;\n-    private int numStandbyReplicas = 0;\n-    private long probingRebalanceInterval = 60 * 1000L;\n-\n-    private Map<UUID, ClientState> clientStates = new HashMap<>();\n-    private Set<TaskId> allTasks = new HashSet<>();\n-    private Set<TaskId> statefulTasks = new HashSet<>();\n-\n-    private ClientState client1;\n-    private ClientState client2;\n-    private ClientState client3;\n-    \n-    private HighAvailabilityTaskAssignor taskAssignor;\n-\n-    private void createTaskAssignor() {\n-        final AssignmentConfigs configs = new AssignmentConfigs(\n-            acceptableRecoveryLag,\n-            balanceFactor,\n-            maxWarmupReplicas,\n-            numStandbyReplicas,\n-            probingRebalanceInterval\n-        );\n-        taskAssignor = new HighAvailabilityTaskAssignor(\n-            clientStates,\n-            allTasks,\n-            statefulTasks,\n-            configs);\n-    }\n+    private final AssignmentConfigs configWithoutStandbys = new AssignmentConfigs(\n+        /*acceptableRecoveryLag*/ 100L,\n+        /*balanceFactor*/ 1,\n+        /*maxWarmupReplicas*/ 2,\n+        /*numStandbyReplicas*/ 0,\n+        /*probingRebalanceIntervalMs*/ 60 * 1000L\n+    );\n+\n+    private final AssignmentConfigs configWithStandbys = new AssignmentConfigs(\n+        /*acceptableRecoveryLag*/ 100L,\n+        /*balanceFactor*/ 1,\n+        /*maxWarmupReplicas*/ 2,\n+        /*numStandbyReplicas*/ 1,\n+        /*probingRebalanceIntervalMs*/ 60 * 1000L\n+    );\n \n-    @Test\n-    public void shouldDecidePreviousAssignmentIsInvalidIfThereAreUnassignedActiveTasks() {\n-        client1 = EasyMock.createNiceMock(ClientState.class);\n-        expect(client1.prevActiveTasks()).andReturn(singleton(TASK_0_0));\n-        expect(client1.prevStandbyTasks()).andStubReturn(EMPTY_TASKS);\n-        replay(client1);\n-        allTasks =  mkSet(TASK_0_0, TASK_0_1);\n-        clientStates = singletonMap(UUID_1, client1);\n-        createTaskAssignor();\n \n-        assertFalse(taskAssignor.previousAssignmentIsValid());\n+    @Test\n+    public void shouldComputeNewAssignmentIfThereAreUnassignedActiveTasks() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97228a5fbdb9b8d64aa609d59c0b64e4fe7bbe0c"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIxMTExMA==", "bodyText": "Since I added the new assignor, I added a regression test to verify its most important special function. Most of its validity is verified by the parameterized StreamsPartitionAssignorTest.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414211110", "createdAt": "2020-04-24T00:25:31Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/PriorTaskAssignorTest.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.junit.Test;\n+\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TreeMap;\n+import java.util.UUID;\n+\n+import static java.util.Arrays.asList;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_0;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_1;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.TASK_0_2;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_1;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentTestUtils.UUID_2;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.is;\n+\n+public class PriorTaskAssignorTest {\n+\n+    private final Map<UUID, ClientState> clients = new TreeMap<>();\n+\n+    @Test\n+    public void shouldViolateBalanceToPreserveActiveTaskStickiness() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97228a5fbdb9b8d64aa609d59c0b64e4fe7bbe0c"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk5NTgwMzQ0", "url": "https://github.com/apache/kafka/pull/8541#pullrequestreview-399580344", "createdAt": "2020-04-24T01:21:18Z", "commit": {"oid": "97228a5fbdb9b8d64aa609d59c0b64e4fe7bbe0c"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMToyMToxOFrOGLCcRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQwMTozNDo1MVrOGLCrVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIyNzUyNQ==", "bodyText": "Fine with me (although it does slightly detract from the opt-out possibility). WDYT about adding a retry backoff though? I'm a bit concerned we might just end up stuck in a loop of useless rebalancing, and waiting the full probing.rebalance.interval doesn't feel right either", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414227525", "createdAt": "2020-04-24T01:21:18Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTU0Mw=="}, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIzMDM1OQ==", "bodyText": "Should we put this with the other Streams internal configs? And/or follow the pattern of prefix+suffixing with __ ?", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414230359", "createdAt": "2020-04-24T01:31:25Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/AssignorConfiguration.java", "diffHunk": "@@ -41,8 +42,8 @@\n import static org.apache.kafka.streams.processor.internals.assignment.StreamsAssignmentProtocolVersions.LATEST_SUPPORTED_VERSION;\n \n public final class AssignorConfiguration {\n-    public static final String HIGH_AVAILABILITY_ENABLED_CONFIG = \"internal.high.availability.enabled\";\n-    private final boolean highAvailabilityEnabled;\n+    public static final String INTERNAL_TASK_ASSIGNOR_CLASS = \"internal.task.assignor.class\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97228a5fbdb9b8d64aa609d59c0b64e4fe7bbe0c"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIzMTAxMQ==", "bodyText": "I'm all for useful logging \ud83d\udc4d", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414231011", "createdAt": "2020-04-24T01:33:37Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -95,6 +94,11 @@ public boolean assign() {\n \n         assignStatelessActiveTasks();\n \n+        log.info(\"Decided on assignment: \" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5Mjg1NQ=="}, "originalCommit": null, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDIzMTM4Mw==", "bodyText": "Thanks for the improvement, this feels a lot nicer", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414231383", "createdAt": "2020-04-24T01:34:51Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/PriorTaskAssignor.java", "diffHunk": "@@ -0,0 +1,40 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+\n+public class PriorTaskAssignor implements TaskAssignor {\n+    private final StickyTaskAssignor delegate;\n+\n+    public PriorTaskAssignor() {\n+        delegate = new StickyTaskAssignor(true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MzkwMQ=="}, "originalCommit": null, "originalPosition": 30}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fed2506de71a1a661b7ddaa07592dd8599e868e6", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/fed2506de71a1a661b7ddaa07592dd8599e868e6", "committedDate": "2020-04-24T02:41:05Z", "message": "restore LagFetchIntegrationTest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b7373de4dd29699868a6c7317b8e6ec71d2220ae", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/b7373de4dd29699868a6c7317b8e6ec71d2220ae", "committedDate": "2020-04-24T15:25:47Z", "message": "cr feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "deefb582a3a8c3f3c15d1a403b064dc77fa5dc1b", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/deefb582a3a8c3f3c15d1a403b064dc77fa5dc1b", "committedDate": "2020-04-24T15:39:49Z", "message": "cr comments and fixing system tests in progress"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "77fbb4193766c72eaf2a8b2ab34c96a4482de14f", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/77fbb4193766c72eaf2a8b2ab34c96a4482de14f", "committedDate": "2020-04-24T19:41:10Z", "message": "fix system tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9015be8cb3ed6cbd3865758660d4023da350a702", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/9015be8cb3ed6cbd3865758660d4023da350a702", "committedDate": "2020-04-24T21:25:37Z", "message": "fix tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/126afd1f2249cb70d7f23c57965d1fdf01a4d957", "committedDate": "2020-04-24T22:27:58Z", "message": "revert unnecessary change"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwMzA2OTU4", "url": "https://github.com/apache/kafka/pull/8541#pullrequestreview-400306958", "createdAt": "2020-04-24T22:32:14Z", "commit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMjozMjoxNFrOGLrcAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQyMjozNzo0NFrOGLrjOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDg5OTIwMw==", "bodyText": "This is not a TODO. I'm planning to leave the test like this. (Just opening the floor for objections)", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414899203", "createdAt": "2020-04-24T22:32:14Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java", "diffHunk": "@@ -147,6 +149,9 @@ private void shouldFetchLagsDuringRebalancing(final String optimization) throws\n         // create stream threads\n         for (int i = 0; i < 2; i++) {\n             final Properties props = (Properties) streamsConfiguration.clone();\n+            // this test relies on the second instance getting the standby, so we specify\n+            // an assignor with this contract.\n+            props.put(StreamsConfig.InternalConfig.INTERNAL_TASK_ASSIGNOR_CLASS, PriorTaskAssignor.class.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDkwMDE1OQ==", "bodyText": "I've added this as a general mechanism in a couple of places to pass specific configs into Streams, so we don't have to make new constructors for every different parameterization.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414900159", "createdAt": "2020-04-24T22:35:09Z", "author": {"login": "vvcephei"}, "path": "tests/kafkatest/services/streams.py", "diffHunk": "@@ -477,6 +477,10 @@ def __init__(self, test_context, kafka):\n                                                                  \"\")\n         self.UPGRADE_FROM = None\n         self.UPGRADE_TO = None\n+        self.extra_properties = {}\n+\n+    def set_config(self, key, value):\n+        self.extra_properties[key] = value", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDkwMDQ0Nw==", "bodyText": "These will become follow-on tasks to fix each test. Thankfully, there aren't many.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414900447", "createdAt": "2020-04-24T22:36:01Z", "author": {"login": "vvcephei"}, "path": "tests/kafkatest/services/streams.py", "diffHunk": "@@ -562,6 +568,8 @@ def prop_file(self):\n                       consumer_property.SESSION_TIMEOUT_MS: 60000}\n \n         properties['input.topic'] = self.INPUT_TOPIC\n+        # TODO KIP-441: consider rewriting the test for HighAvailabilityTaskAssignor\n+        properties['internal.task.assignor.class'] = \"org.apache.kafka.streams.processor.internals.assignment.StickyTaskAssignor\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDkwMDg2MQ==", "bodyText": "This accounted for most of the test failures, and it's already fixed on trunk.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414900861", "createdAt": "2020-04-24T22:37:06Z", "author": {"login": "vvcephei"}, "path": "tests/kafkatest/tests/streams/streams_broker_bounce_test.py", "diffHunk": "@@ -164,7 +164,7 @@ def setup_system(self, start_processor=True, num_threads=3):\n \n         # Start test harness\n         self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)\n-        self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka, num_threads)\n+        self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka, \"at_least_once\", num_threads)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDkwMTA1MA==", "bodyText": "This one already had a different mechanism to add more configs, so I just left it alone.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r414901050", "createdAt": "2020-04-24T22:37:44Z", "author": {"login": "vvcephei"}, "path": "tests/kafkatest/tests/streams/streams_broker_down_resilience_test.py", "diffHunk": "@@ -144,7 +144,11 @@ def test_streams_runs_with_broker_down_initially(self):\n     def test_streams_should_scale_in_while_brokers_down(self):\n         self.kafka.start()\n \n-        configs = self.get_configs(extra_configs=\",application.id=shutdown_with_broker_down\")\n+        # TODO KIP-441: consider rewriting the test for HighAvailabilityTaskAssignor\n+        configs = self.get_configs(\n+            extra_configs=\",application.id=shutdown_with_broker_down\" +\n+                          \",internal.task.assignor.class=org.apache.kafka.streams.processor.internals.assignment.StickyTaskAssignor\"\n+        )", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 9}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwOTEzMTY4", "url": "https://github.com/apache/kafka/pull/8541#pullrequestreview-400913168", "createdAt": "2020-04-27T12:43:21Z", "commit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxMjo0MzoyMVrOGMhJtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QxNDo0MDo1NFrOGMmxFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTc3OTI1Mg==", "bodyText": "req: Please add unit tests for this method", "url": "https://github.com/apache/kafka/pull/8541#discussion_r415779252", "createdAt": "2020-04-27T12:43:21Z", "author": {"login": "cadonna"}, "path": "clients/src/main/java/org/apache/kafka/common/utils/Utils.java", "diffHunk": "@@ -1146,4 +1146,13 @@ private static byte checkRange(final byte i) {\n             }\n         };\n     }\n+\n+    @SafeVarargs\n+    public static <E> Set<E> union(final Supplier<Set<E>> constructor, final Set<E>... set) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE4ODU5Mg=="}, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTc5NzA3MQ==", "bodyText": "prop:\nCould we package this logic into a factory method to make the code more readable?\nfinal TaskAssignor taskAssignor = createTaskAssignor(boolean lagComputationSuccessful);", "url": "https://github.com/apache/kafka/pull/8541#discussion_r415797071", "createdAt": "2020-04-27T13:08:32Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {\n+            log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n+                         + \"trigger another rebalance to retry.\");\n+            setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n+            taskAssignor = new PriorTaskAssignor();\n         } else {\n-            taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, false);\n+            taskAssignor = this.taskAssignor.get();\n         }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgxMTk2Mw==", "bodyText": "I see your point. What I do not like so much is that it is not very intuitive to require successful lag computation for sticky assignor. I understand that if lag computation is not successful other parts of Streams will fail, but it is not the responsibility of this class to avoid that. I think what I am trying to say is that the verifications should be done where they are required to make the code easily comprehensible. I am just imagining me coming back to this code and trying to understand why the lag computation must be successful for the sticky assignor.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r415811963", "createdAt": "2020-04-27T13:27:51Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -713,23 +713,18 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n             allTasks, clientStates, numStandbyReplicas());\n \n         final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n+        if (!lagComputationSuccessful) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDE5MTU0Mw=="}, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgxNzM0OA==", "bodyText": "req: Please add a unit test.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r415817348", "createdAt": "2020-04-27T13:35:04Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/ClientState.java", "diffHunk": "@@ -86,6 +90,22 @@ private ClientState(final Set<TaskId> activeTasks,\n         this.capacity = capacity;\n     }\n \n+    public ClientState(final Set<TaskId> previousActiveTasks,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTgxODk3OQ==", "bodyText": "You are an exemplary boy scout!", "url": "https://github.com/apache/kafka/pull/8541#discussion_r415818979", "createdAt": "2020-04-27T13:37:22Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignor.java", "diffHunk": "@@ -16,49 +16,50 @@\n  */\n package org.apache.kafka.streams.processor.internals.assignment;\n \n-import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClientOrNoCaughtUpClientsExist;\n-import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n-import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n-import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import java.util.Collection;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n import java.util.SortedMap;\n import java.util.SortedSet;\n import java.util.TreeSet;\n import java.util.UUID;\n import java.util.stream.Collectors;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n-import java.util.Map;\n-import java.util.Set;\n+import static org.apache.kafka.streams.processor.internals.assignment.AssignmentUtils.taskIsCaughtUpOnClientOrNoCaughtUpClientsExist;\n+import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.buildClientRankingsByTask;\n+import static org.apache.kafka.streams.processor.internals.assignment.RankedClient.tasksToCaughtUpClients;\n+import static org.apache.kafka.streams.processor.internals.assignment.TaskMovement.assignTaskMovements;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg3MDQ5NA==", "bodyText": "prop:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        rebalancePending = new HighAvailabilityTaskAssignor().assign(harness.clientStates,\n          \n          \n            \n                                                                                     allTasks,\n          \n          \n            \n                                                                                     harness.statefulTaskEndOffsetSums.keySet(),\n          \n          \n            \n                                                                                     configs);\n          \n          \n            \n                        rebalancePending = new HighAvailabilityTaskAssignor().assign(\n          \n          \n            \n                            harness.clientStates,\n          \n          \n            \n                            allTasks,\n          \n          \n            \n                            harness.statefulTaskEndOffsetSums.keySet(),\n          \n          \n            \n                            configs\n          \n          \n            \n                        );", "url": "https://github.com/apache/kafka/pull/8541#discussion_r415870494", "createdAt": "2020-04-27T14:40:01Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorConvergenceTest.java", "diffHunk": "@@ -416,11 +416,10 @@ private static void testForConvergence(final Harness harness,\n             iteration++;\n             harness.prepareForNextRebalance();\n             harness.recordBefore(iteration);\n-            rebalancePending = new HighAvailabilityTaskAssignor(\n-                harness.clientStates, allTasks,\n-                harness.statefulTaskEndOffsetSums.keySet(),\n-                configs\n-            ).assign();\n+            rebalancePending = new HighAvailabilityTaskAssignor().assign(harness.clientStates,\n+                                                                         allTasks,\n+                                                                         harness.statefulTaskEndOffsetSums.keySet(),\n+                                                                         configs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTg3MTI1Mw==", "bodyText": "prop:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    final boolean followupRebalanceNeeded = assign(TASK_0_0,\n          \n          \n            \n                                                                   TASK_0_1,\n          \n          \n            \n                                                                   TASK_0_2,\n          \n          \n            \n                                                                   new TaskId(1, 0),\n          \n          \n            \n                                                                   new TaskId(1, 1),\n          \n          \n            \n                                                                   new TaskId(1, 2),\n          \n          \n            \n                                                                   new TaskId(2, 0),\n          \n          \n            \n                                                                   new TaskId(2, 1),\n          \n          \n            \n                                                                   new TaskId(2, 2),\n          \n          \n            \n                                                                   new TaskId(3, 0),\n          \n          \n            \n                                                                   new TaskId(3, 1),\n          \n          \n            \n                                                                   new TaskId(3, 2));\n          \n          \n            \n                    final boolean followupRebalanceNeeded = assign(\n          \n          \n            \n                        TASK_0_0,\n          \n          \n            \n                        TASK_0_1,\n          \n          \n            \n                        TASK_0_2,\n          \n          \n            \n                        new TaskId(1, 0),\n          \n          \n            \n                        new TaskId(1, 1),\n          \n          \n            \n                        new TaskId(1, 2),\n          \n          \n            \n                        new TaskId(2, 0),\n          \n          \n            \n                        new TaskId(2, 1),\n          \n          \n            \n                        new TaskId(2, 2),\n          \n          \n            \n                        new TaskId(3, 0),\n          \n          \n            \n                        new TaskId(3, 1),\n          \n          \n            \n                        new TaskId(3, 2)\n          \n          \n            \n                    );", "url": "https://github.com/apache/kafka/pull/8541#discussion_r415871253", "createdAt": "2020-04-27T14:40:54Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/StickyTaskAssignorTest.java", "diffHunk": "@@ -350,20 +350,20 @@ public void shouldAssignMoreTasksToClientWithMoreCapacity() {\n         createClient(UUID_2, 2);\n         createClient(UUID_1, 1);\n \n-        final StickyTaskAssignor taskAssignor = createTaskAssignor(TASK_0_0,\n-                                                                            TASK_0_1,\n-                                                                            TASK_0_2,\n-                                                                            new TaskId(1, 0),\n-                                                                            new TaskId(1, 1),\n-                                                                            new TaskId(1, 2),\n-                                                                            new TaskId(2, 0),\n-                                                                            new TaskId(2, 1),\n-                                                                            new TaskId(2, 2),\n-                                                                            new TaskId(3, 0),\n-                                                                            new TaskId(3, 1),\n-                                                                            new TaskId(3, 2));\n-\n-        taskAssignor.assign();\n+        final boolean followupRebalanceNeeded = assign(TASK_0_0,\n+                                                       TASK_0_1,\n+                                                       TASK_0_2,\n+                                                       new TaskId(1, 0),\n+                                                       new TaskId(1, 1),\n+                                                       new TaskId(1, 2),\n+                                                       new TaskId(2, 0),\n+                                                       new TaskId(2, 1),\n+                                                       new TaskId(2, 2),\n+                                                       new TaskId(3, 0),\n+                                                       new TaskId(3, 1),\n+                                                       new TaskId(3, 2));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "126afd1f2249cb70d7f23c57965d1fdf01a4d957"}, "originalPosition": 371}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "98e317e93a9b961f32a8c6187dc2324748ab0749", "author": {"user": {"login": "vvcephei", "name": "John Roesler"}}, "url": "https://github.com/apache/kafka/commit/98e317e93a9b961f32a8c6187dc2324748ab0749", "committedDate": "2020-04-27T21:26:19Z", "message": "Update streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/StickyTaskAssignorTest.java\n\nCo-Authored-By: Bruno Cadonna <bruno@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e3bf615ecd9842630013745174bafb98a9fa8a3e", "author": {"user": {"login": "vvcephei", "name": "John Roesler"}}, "url": "https://github.com/apache/kafka/commit/e3bf615ecd9842630013745174bafb98a9fa8a3e", "committedDate": "2020-04-27T21:26:43Z", "message": "Update streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/TaskAssignorConvergenceTest.java\n\nCo-Authored-By: Bruno Cadonna <bruno@confluent.io>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e077be2947f06b9a8dc4797c01687aaa4620ddf6", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/e077be2947f06b9a8dc4797c01687aaa4620ddf6", "committedDate": "2020-04-27T23:24:36Z", "message": "cr feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAxNDEyNTUy", "url": "https://github.com/apache/kafka/pull/8541#pullrequestreview-401412552", "createdAt": "2020-04-28T00:12:06Z", "commit": {"oid": "e077be2947f06b9a8dc4797c01687aaa4620ddf6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMDoxMjowNlrOGM87uQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMDoxMjowNlrOGM87uQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjIzNDQyNQ==", "bodyText": "I renamed the PriorTaskAssignor and added a Javadoc to make its role clear.\nNote that \"PriorTaskAssignor\" would be an appropriate behavioral name, except that it also always returns \"true\", and that it must ignore the lags, which is what makes it a \"fallback\" assignor here.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416234425", "createdAt": "2020-04-28T00:12:06Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/FallbackPriorTaskAssignor.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+\n+/**\n+ * A special task assignor implementation to be used as a fallback in case the\n+ * configured assignor couldn't be invoked.\n+ *\n+ * Specifically, this assignor must:\n+ * 1. ignore the task lags in the ClientState map\n+ * 2. always return true, indicating that a follow-up rebalance is needed\n+ */\n+public class FallbackPriorTaskAssignor implements TaskAssignor {\n+    private final StickyTaskAssignor delegate;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e077be2947f06b9a8dc4797c01687aaa4620ddf6"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAxNDE1NzAy", "url": "https://github.com/apache/kafka/pull/8541#pullrequestreview-401415702", "createdAt": "2020-04-28T00:21:32Z", "commit": {"oid": "e077be2947f06b9a8dc4797c01687aaa4620ddf6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMDoyMTozMlrOGM9JBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMDoyMTozMlrOGM9JBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjIzNzgyOQ==", "bodyText": "We should probably rename this to probingRebalanceRequired or so on, see comment on FallbackPriorTaskAssignor", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416237829", "createdAt": "2020-04-28T00:21:32Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -712,31 +712,32 @@ private boolean assignTasksToClients(final Set<String> allSourceTopics,\n         log.debug(\"Assigning tasks {} to clients {} with number of replicas {}\",\n             allTasks, clientStates, numStandbyReplicas());\n \n-        final TaskAssignor taskAssignor;\n-        if (highAvailabilityEnabled) {\n-            if (lagComputationSuccessful) {\n-                taskAssignor = new HighAvailabilityTaskAssignor(\n-                    clientStates,\n-                    allTasks,\n-                    statefulTasks,\n-                    assignmentConfigs);\n-            } else {\n-                log.info(\"Failed to fetch end offsets for changelogs, will return previous assignment to clients and \"\n-                             + \"trigger another rebalance to retry.\");\n-                setAssignmentErrorCode(AssignorError.REBALANCE_NEEDED.code());\n-                taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, true);\n-            }\n-        } else {\n-            taskAssignor = new StickyTaskAssignor(clientStates, allTasks, statefulTasks, assignmentConfigs, false);\n-        }\n-        final boolean followupRebalanceNeeded = taskAssignor.assign();\n+        final TaskAssignor taskAssignor = createTaskAssignor(lagComputationSuccessful);\n+\n+        final boolean followupRebalanceNeeded = taskAssignor.assign(clientStates,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e077be2947f06b9a8dc4797c01687aaa4620ddf6"}, "originalPosition": 60}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAxNDIyNzEz", "url": "https://github.com/apache/kafka/pull/8541#pullrequestreview-401422713", "createdAt": "2020-04-28T00:41:24Z", "commit": {"oid": "e077be2947f06b9a8dc4797c01687aaa4620ddf6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMDo0MToyNVrOGM9mfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwMDo0MToyNVrOGM9mfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjI0NTM3NA==", "bodyText": "Returning true here will schedule a followup rebalance at the probing interval, but we also schedule a followup rebalance immediately before instantiating this assignor (line 735). Is this intentional? IIUC your proposal was to trigger a followup rebalance right away, which we do by means of the assignment error code.\nOf course, this is in memory so if the instance crashes and restarts we lose this information. I think we should actually avoid using the REBALANCE_NEEDED error code inside the assign method, and only allow. it during onAssignment. If we know that a followup rebalance is needed during assign we should just encode the nextScheduledRebalance with the current time", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416245374", "createdAt": "2020-04-28T00:41:25Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/FallbackPriorTaskAssignor.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals.assignment;\n+\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n+\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+\n+/**\n+ * A special task assignor implementation to be used as a fallback in case the\n+ * configured assignor couldn't be invoked.\n+ *\n+ * Specifically, this assignor must:\n+ * 1. ignore the task lags in the ClientState map\n+ * 2. always return true, indicating that a follow-up rebalance is needed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e077be2947f06b9a8dc4797c01687aaa4620ddf6"}, "originalPosition": 32}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d22de81d1e873c13b208ddedbb8eec1bd44628a9", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/d22de81d1e873c13b208ddedbb8eec1bd44628a9", "committedDate": "2020-04-28T02:58:29Z", "message": "cr feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d58f62dc73dc3f4832cb89b5be6a8c8ce2f32e60", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/d58f62dc73dc3f4832cb89b5be6a8c8ce2f32e60", "committedDate": "2020-04-28T03:21:14Z", "message": "rename 'followup' to 'probing'"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAyMDE4OTkx", "url": "https://github.com/apache/kafka/pull/8541#pullrequestreview-402018991", "createdAt": "2020-04-28T16:39:58Z", "commit": {"oid": "d58f62dc73dc3f4832cb89b5be6a8c8ce2f32e60"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxNjozOTo1OFrOGNdHdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxNjozOTo1OFrOGNdHdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjc2MTcxOQ==", "bodyText": "req: I think, you can now restrict access to previousAssignmentIsValid() to private.", "url": "https://github.com/apache/kafka/pull/8541#discussion_r416761719", "createdAt": "2020-04-28T16:39:58Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/HighAvailabilityTaskAssignorTest.java", "diffHunk": "@@ -41,132 +54,107 @@\n import static org.easymock.EasyMock.replay;\n import static org.hamcrest.CoreMatchers.equalTo;\n import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertTrue;\n-\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.UUID;\n-import org.apache.kafka.streams.processor.TaskId;\n-import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentConfigs;\n-import org.easymock.EasyMock;\n-import org.junit.Test;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.is;\n+import static org.hamcrest.Matchers.not;\n \n public class HighAvailabilityTaskAssignorTest {\n-    private long acceptableRecoveryLag = 100L;\n-    private int balanceFactor = 1;\n-    private int maxWarmupReplicas = 2;\n-    private int numStandbyReplicas = 0;\n-    private long probingRebalanceInterval = 60 * 1000L;\n-\n-    private Map<UUID, ClientState> clientStates = new HashMap<>();\n-    private Set<TaskId> allTasks = new HashSet<>();\n-    private Set<TaskId> statefulTasks = new HashSet<>();\n-\n-    private ClientState client1;\n-    private ClientState client2;\n-    private ClientState client3;\n-    \n-    private HighAvailabilityTaskAssignor taskAssignor;\n-\n-    private void createTaskAssignor() {\n-        final AssignmentConfigs configs = new AssignmentConfigs(\n-            acceptableRecoveryLag,\n-            balanceFactor,\n-            maxWarmupReplicas,\n-            numStandbyReplicas,\n-            probingRebalanceInterval\n-        );\n-        taskAssignor = new HighAvailabilityTaskAssignor(\n-            clientStates,\n-            allTasks,\n-            statefulTasks,\n-            configs);\n-    }\n+    private final AssignmentConfigs configWithoutStandbys = new AssignmentConfigs(\n+        /*acceptableRecoveryLag*/ 100L,\n+        /*balanceFactor*/ 1,\n+        /*maxWarmupReplicas*/ 2,\n+        /*numStandbyReplicas*/ 0,\n+        /*probingRebalanceIntervalMs*/ 60 * 1000L\n+    );\n+\n+    private final AssignmentConfigs configWithStandbys = new AssignmentConfigs(\n+        /*acceptableRecoveryLag*/ 100L,\n+        /*balanceFactor*/ 1,\n+        /*maxWarmupReplicas*/ 2,\n+        /*numStandbyReplicas*/ 1,\n+        /*probingRebalanceIntervalMs*/ 60 * 1000L\n+    );\n \n-    @Test\n-    public void shouldDecidePreviousAssignmentIsInvalidIfThereAreUnassignedActiveTasks() {\n-        client1 = EasyMock.createNiceMock(ClientState.class);\n-        expect(client1.prevActiveTasks()).andReturn(singleton(TASK_0_0));\n-        expect(client1.prevStandbyTasks()).andStubReturn(EMPTY_TASKS);\n-        replay(client1);\n-        allTasks =  mkSet(TASK_0_0, TASK_0_1);\n-        clientStates = singletonMap(UUID_1, client1);\n-        createTaskAssignor();\n \n-        assertFalse(taskAssignor.previousAssignmentIsValid());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d58f62dc73dc3f4832cb89b5be6a8c8ce2f32e60"}, "originalPosition": 97}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAyMTExOTgx", "url": "https://github.com/apache/kafka/pull/8541#pullrequestreview-402111981", "createdAt": "2020-04-28T18:41:10Z", "commit": {"oid": "d58f62dc73dc3f4832cb89b5be6a8c8ce2f32e60"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1200, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}