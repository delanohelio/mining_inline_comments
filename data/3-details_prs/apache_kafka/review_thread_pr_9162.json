{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY2MTkwNDM3", "number": 9162, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMTo0NTowMFrOEXZIGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNjo0NToyNlrOExraDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyOTY0Mzc3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMTo0NTowMFrOG_KUdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQwNDozNDoyOVrOG_RdQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg4MjU1MQ==", "bodyText": "Since this code is pretty critical, I'd keep the previous while loop and add a boolean to indicate a break or something like that.", "url": "https://github.com/apache/kafka/pull/9162#discussion_r468882551", "createdAt": "2020-08-11T21:45:00Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1501,43 +1501,36 @@ class Log(@volatile private var _dir: File,\n         case FetchTxnCommitted => fetchLastStableOffsetMetadata\n       }\n \n-      if (startOffset == maxOffsetMetadata.messageOffset) {\n-        return emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n-      } else if (startOffset > maxOffsetMetadata.messageOffset) {\n-        val startOffsetMetadata = convertToOffsetMetadataOrThrow(startOffset)\n-        return emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n-      }\n-\n-      // Do the read on the segment with a base offset less than the target offset\n-      // but if that segment doesn't contain any messages with an offset greater than that\n-      // continue to read from successive segments until we get some messages or we reach the end of the log\n-      while (segmentEntry != null) {\n-        val segment = segmentEntry.getValue\n-\n-        val maxPosition = {\n-          // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.\n-          if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {\n-            maxOffsetMetadata.relativePositionInSegment\n-          } else {\n-            segment.size\n-          }\n-        }\n-\n-        val fetchInfo = segment.read(startOffset, maxLength, maxPosition, minOneMessage)\n-        if (fetchInfo == null) {\n-          segmentEntry = segments.higherEntry(segmentEntry.getKey)\n-        } else {\n-          return if (includeAbortedTxns)\n-            addAbortedTransactions(startOffset, segmentEntry, fetchInfo)\n-          else\n-            fetchInfo\n-        }\n+      if (startOffset == maxOffsetMetadata.messageOffset)\n+        emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n+      else if (startOffset > maxOffsetMetadata.messageOffset)\n+        emptyFetchDataInfo(convertToOffsetMetadataOrThrow(startOffset), includeAbortedTxns)\n+      else {\n+        // Do the read on the segment with a base offset less than the target offset\n+        // but if that segment doesn't contain any messages with an offset greater than that\n+        // continue to read from successive segments until we get some messages or we reach the end of the log\n+        var currentEntry = segmentFloorEntry\n+        Iterator.continually(currentEntry)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODk5OTQ5MA==", "bodyText": "your are right!", "url": "https://github.com/apache/kafka/pull/9162#discussion_r468999490", "createdAt": "2020-08-12T04:34:29Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1501,43 +1501,36 @@ class Log(@volatile private var _dir: File,\n         case FetchTxnCommitted => fetchLastStableOffsetMetadata\n       }\n \n-      if (startOffset == maxOffsetMetadata.messageOffset) {\n-        return emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n-      } else if (startOffset > maxOffsetMetadata.messageOffset) {\n-        val startOffsetMetadata = convertToOffsetMetadataOrThrow(startOffset)\n-        return emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n-      }\n-\n-      // Do the read on the segment with a base offset less than the target offset\n-      // but if that segment doesn't contain any messages with an offset greater than that\n-      // continue to read from successive segments until we get some messages or we reach the end of the log\n-      while (segmentEntry != null) {\n-        val segment = segmentEntry.getValue\n-\n-        val maxPosition = {\n-          // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.\n-          if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {\n-            maxOffsetMetadata.relativePositionInSegment\n-          } else {\n-            segment.size\n-          }\n-        }\n-\n-        val fetchInfo = segment.read(startOffset, maxLength, maxPosition, minOneMessage)\n-        if (fetchInfo == null) {\n-          segmentEntry = segments.higherEntry(segmentEntry.getKey)\n-        } else {\n-          return if (includeAbortedTxns)\n-            addAbortedTransactions(startOffset, segmentEntry, fetchInfo)\n-          else\n-            fetchInfo\n-        }\n+      if (startOffset == maxOffsetMetadata.messageOffset)\n+        emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n+      else if (startOffset > maxOffsetMetadata.messageOffset)\n+        emptyFetchDataInfo(convertToOffsetMetadataOrThrow(startOffset), includeAbortedTxns)\n+      else {\n+        // Do the read on the segment with a base offset less than the target offset\n+        // but if that segment doesn't contain any messages with an offset greater than that\n+        // continue to read from successive segments until we get some messages or we reach the end of the log\n+        var currentEntry = segmentFloorEntry\n+        Iterator.continually(currentEntry)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg4MjU1MQ=="}, "originalCommit": null, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTI0NDI5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNjoyMDoyM1rOHn6qtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNjoyMDoyM1rOHn6qtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYxNzcxOA==", "bodyText": "Nit: we can remove the braces from the if/else.", "url": "https://github.com/apache/kafka/pull/9162#discussion_r511617718", "createdAt": "2020-10-25T16:20:23Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1535,42 +1533,48 @@ class Log(@volatile private var _dir: File,\n       }\n \n       if (startOffset == maxOffsetMetadata.messageOffset) {\n-        return emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n+        emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n       } else if (startOffset > maxOffsetMetadata.messageOffset) {\n         val startOffsetMetadata = convertToOffsetMetadataOrThrow(startOffset)\n-        return emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n-      }\n+        emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n+      } else {\n \n-      // Do the read on the segment with a base offset less than the target offset\n-      // but if that segment doesn't contain any messages with an offset greater than that\n-      // continue to read from successive segments until we get some messages or we reach the end of the log\n-      while (segmentEntry != null) {\n-        val segment = segmentEntry.getValue\n+        // Do the read on the segment with a base offset less than the target offset\n+        // but if that segment doesn't contain any messages with an offset greater than that\n+        // continue to read from successive segments until we get some messages or we reach the end of the log\n+        var done = segmentEntry == null\n+        var fetchDataInfo: FetchDataInfo = null\n+        while (!done) {\n+          val segment = segmentEntry.getValue\n+\n+          val maxPosition = {\n+            // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.\n+            if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {\n+              maxOffsetMetadata.relativePositionInSegment\n+            } else {\n+              segment.size\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 349}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTI0NTgwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNjoyMjoyMVrOHn6reA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNjoyMjoyMVrOHn6reA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYxNzkxMg==", "bodyText": "The formatting here is a bit weird (the else is aligned the same as the variable. I think it's easier to understand if the if/else are aligned and indented.", "url": "https://github.com/apache/kafka/pull/9162#discussion_r511617912", "createdAt": "2020-10-25T16:22:21Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1535,42 +1533,48 @@ class Log(@volatile private var _dir: File,\n       }\n \n       if (startOffset == maxOffsetMetadata.messageOffset) {\n-        return emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n+        emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n       } else if (startOffset > maxOffsetMetadata.messageOffset) {\n         val startOffsetMetadata = convertToOffsetMetadataOrThrow(startOffset)\n-        return emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n-      }\n+        emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n+      } else {\n \n-      // Do the read on the segment with a base offset less than the target offset\n-      // but if that segment doesn't contain any messages with an offset greater than that\n-      // continue to read from successive segments until we get some messages or we reach the end of the log\n-      while (segmentEntry != null) {\n-        val segment = segmentEntry.getValue\n+        // Do the read on the segment with a base offset less than the target offset\n+        // but if that segment doesn't contain any messages with an offset greater than that\n+        // continue to read from successive segments until we get some messages or we reach the end of the log\n+        var done = segmentEntry == null\n+        var fetchDataInfo: FetchDataInfo = null\n+        while (!done) {\n+          val segment = segmentEntry.getValue\n+\n+          val maxPosition = {\n+            // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.\n+            if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {\n+              maxOffsetMetadata.relativePositionInSegment\n+            } else {\n+              segment.size\n+            }\n+          }\n \n-        val maxPosition = {\n-          // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.\n-          if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {\n-            maxOffsetMetadata.relativePositionInSegment\n+          val fetchInfo = segment.read(startOffset, maxLength, maxPosition, minOneMessage)\n+          if (fetchInfo == null) {\n+            segmentEntry = segments.higherEntry(segmentEntry.getKey)\n+            done = segmentEntry == null\n           } else {\n-            segment.size\n+            fetchDataInfo = if (includeAbortedTxns) addAbortedTransactions(startOffset, segmentEntry, fetchInfo)\n+            else fetchInfo", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 363}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTI2MTc4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNjozODozNlrOHn6zIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwNTozOTowM1rOHoBbNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYxOTg3NQ==", "bodyText": "Maybe something like the following may be clearer:\nfetchDataInfo = segment.read(startOffset, maxLength, maxPosition, minOneMessage)\nif (fetchDataInfo != null) {\n  if (includeAbortedTxns)\n    fetchDataInfo = addAbortedTransactions(startOffset, segmentEntry, fetchDataInfo)\n} else segmentEntry = segments.higherEntry(segmentEntry.getKey)\n\ndone = fetchDataInfo != null || segmentEntry == null\nWhat do you think?", "url": "https://github.com/apache/kafka/pull/9162#discussion_r511619875", "createdAt": "2020-10-25T16:38:36Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1535,42 +1533,48 @@ class Log(@volatile private var _dir: File,\n       }\n \n       if (startOffset == maxOffsetMetadata.messageOffset) {\n-        return emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n+        emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n       } else if (startOffset > maxOffsetMetadata.messageOffset) {\n         val startOffsetMetadata = convertToOffsetMetadataOrThrow(startOffset)\n-        return emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n-      }\n+        emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n+      } else {\n \n-      // Do the read on the segment with a base offset less than the target offset\n-      // but if that segment doesn't contain any messages with an offset greater than that\n-      // continue to read from successive segments until we get some messages or we reach the end of the log\n-      while (segmentEntry != null) {\n-        val segment = segmentEntry.getValue\n+        // Do the read on the segment with a base offset less than the target offset\n+        // but if that segment doesn't contain any messages with an offset greater than that\n+        // continue to read from successive segments until we get some messages or we reach the end of the log\n+        var done = segmentEntry == null\n+        var fetchDataInfo: FetchDataInfo = null\n+        while (!done) {\n+          val segment = segmentEntry.getValue\n+\n+          val maxPosition = {\n+            // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.\n+            if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {\n+              maxOffsetMetadata.relativePositionInSegment\n+            } else {\n+              segment.size\n+            }\n+          }\n \n-        val maxPosition = {\n-          // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.\n-          if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {\n-            maxOffsetMetadata.relativePositionInSegment\n+          val fetchInfo = segment.read(startOffset, maxLength, maxPosition, minOneMessage)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 356}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTcyODQzNw==", "bodyText": "good style. Will copy that.", "url": "https://github.com/apache/kafka/pull/9162#discussion_r511728437", "createdAt": "2020-10-26T05:39:03Z", "author": {"login": "chia7712"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1535,42 +1533,48 @@ class Log(@volatile private var _dir: File,\n       }\n \n       if (startOffset == maxOffsetMetadata.messageOffset) {\n-        return emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n+        emptyFetchDataInfo(maxOffsetMetadata, includeAbortedTxns)\n       } else if (startOffset > maxOffsetMetadata.messageOffset) {\n         val startOffsetMetadata = convertToOffsetMetadataOrThrow(startOffset)\n-        return emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n-      }\n+        emptyFetchDataInfo(startOffsetMetadata, includeAbortedTxns)\n+      } else {\n \n-      // Do the read on the segment with a base offset less than the target offset\n-      // but if that segment doesn't contain any messages with an offset greater than that\n-      // continue to read from successive segments until we get some messages or we reach the end of the log\n-      while (segmentEntry != null) {\n-        val segment = segmentEntry.getValue\n+        // Do the read on the segment with a base offset less than the target offset\n+        // but if that segment doesn't contain any messages with an offset greater than that\n+        // continue to read from successive segments until we get some messages or we reach the end of the log\n+        var done = segmentEntry == null\n+        var fetchDataInfo: FetchDataInfo = null\n+        while (!done) {\n+          val segment = segmentEntry.getValue\n+\n+          val maxPosition = {\n+            // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.\n+            if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {\n+              maxOffsetMetadata.relativePositionInSegment\n+            } else {\n+              segment.size\n+            }\n+          }\n \n-        val maxPosition = {\n-          // Use the max offset position if it is on this segment; otherwise, the segment size is the limit.\n-          if (maxOffsetMetadata.segmentBaseOffset == segment.baseOffset) {\n-            maxOffsetMetadata.relativePositionInSegment\n+          val fetchInfo = segment.read(startOffset, maxLength, maxPosition, minOneMessage)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYxOTg3NQ=="}, "originalCommit": null, "originalPosition": 356}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTI2ODYyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNjo0NToyNlrOHn62dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNVQxNjo0NToyNlrOHn62dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTYyMDcyNw==", "bodyText": "Seems like we should use a pattern match here instead of isDefined and get.", "url": "https://github.com/apache/kafka/pull/9162#discussion_r511620727", "createdAt": "2020-10-25T16:45:26Z", "author": {"login": "ijuma"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1099,171 +1099,169 @@ class Log(@volatile private var _dir: File,\n       val appendInfo = analyzeAndValidateRecords(records, origin, ignoreRecordSize)\n \n       // return if we have no valid messages or if this is a duplicate of the last appended entry\n-      if (appendInfo.shallowCount == 0)\n-        return appendInfo\n+      if (appendInfo.shallowCount == 0) appendInfo\n+      else {\n \n-      // trim any invalid bytes or partial messages before appending it to the on-disk log\n-      var validRecords = trimInvalidBytes(records, appendInfo)\n+        // trim any invalid bytes or partial messages before appending it to the on-disk log\n+        var validRecords = trimInvalidBytes(records, appendInfo)\n \n-      // they are valid, insert them in the log\n-      lock synchronized {\n-        checkIfMemoryMappedBufferClosed()\n-        if (assignOffsets) {\n-          // assign offsets to the message set\n-          val offset = new LongRef(nextOffsetMetadata.messageOffset)\n-          appendInfo.firstOffset = Some(offset.value)\n-          val now = time.milliseconds\n-          val validateAndOffsetAssignResult = try {\n-            LogValidator.validateMessagesAndAssignOffsets(validRecords,\n-              topicPartition,\n-              offset,\n-              time,\n-              now,\n-              appendInfo.sourceCodec,\n-              appendInfo.targetCodec,\n-              config.compact,\n-              config.messageFormatVersion.recordVersion.value,\n-              config.messageTimestampType,\n-              config.messageTimestampDifferenceMaxMs,\n-              leaderEpoch,\n-              origin,\n-              interBrokerProtocolVersion,\n-              brokerTopicStats)\n-          } catch {\n-            case e: IOException =>\n-              throw new KafkaException(s\"Error validating messages while appending to log $name\", e)\n-          }\n-          validRecords = validateAndOffsetAssignResult.validatedRecords\n-          appendInfo.maxTimestamp = validateAndOffsetAssignResult.maxTimestamp\n-          appendInfo.offsetOfMaxTimestamp = validateAndOffsetAssignResult.shallowOffsetOfMaxTimestamp\n-          appendInfo.lastOffset = offset.value - 1\n-          appendInfo.recordConversionStats = validateAndOffsetAssignResult.recordConversionStats\n-          if (config.messageTimestampType == TimestampType.LOG_APPEND_TIME)\n-            appendInfo.logAppendTime = now\n-\n-          // re-validate message sizes if there's a possibility that they have changed (due to re-compression or message\n-          // format conversion)\n-          if (!ignoreRecordSize && validateAndOffsetAssignResult.messageSizeMaybeChanged) {\n-            for (batch <- validRecords.batches.asScala) {\n-              if (batch.sizeInBytes > config.maxMessageSize) {\n-                // we record the original message set size instead of the trimmed size\n-                // to be consistent with pre-compression bytesRejectedRate recording\n-                brokerTopicStats.topicStats(topicPartition.topic).bytesRejectedRate.mark(records.sizeInBytes)\n-                brokerTopicStats.allTopicsStats.bytesRejectedRate.mark(records.sizeInBytes)\n-                throw new RecordTooLargeException(s\"Message batch size is ${batch.sizeInBytes} bytes in append to\" +\n-                  s\"partition $topicPartition which exceeds the maximum configured size of ${config.maxMessageSize}.\")\n-              }\n+        // they are valid, insert them in the log\n+        lock synchronized {\n+          checkIfMemoryMappedBufferClosed()\n+          if (assignOffsets) {\n+            // assign offsets to the message set\n+            val offset = new LongRef(nextOffsetMetadata.messageOffset)\n+            appendInfo.firstOffset = Some(offset.value)\n+            val now = time.milliseconds\n+            val validateAndOffsetAssignResult = try {\n+              LogValidator.validateMessagesAndAssignOffsets(validRecords,\n+                topicPartition,\n+                offset,\n+                time,\n+                now,\n+                appendInfo.sourceCodec,\n+                appendInfo.targetCodec,\n+                config.compact,\n+                config.messageFormatVersion.recordVersion.value,\n+                config.messageTimestampType,\n+                config.messageTimestampDifferenceMaxMs,\n+                leaderEpoch,\n+                origin,\n+                interBrokerProtocolVersion,\n+                brokerTopicStats)\n+            } catch {\n+              case e: IOException =>\n+                throw new KafkaException(s\"Error validating messages while appending to log $name\", e)\n             }\n-          }\n-        } else {\n-          // we are taking the offsets we are given\n-          if (!appendInfo.offsetsMonotonic)\n-            throw new OffsetsOutOfOrderException(s\"Out of order offsets found in append to $topicPartition: \" +\n-                                                 records.records.asScala.map(_.offset))\n-\n-          if (appendInfo.firstOrLastOffsetOfFirstBatch < nextOffsetMetadata.messageOffset) {\n-            // we may still be able to recover if the log is empty\n-            // one example: fetching from log start offset on the leader which is not batch aligned,\n-            // which may happen as a result of AdminClient#deleteRecords()\n-            val firstOffset = appendInfo.firstOffset match {\n-              case Some(offset) => offset\n-              case None => records.batches.asScala.head.baseOffset()\n+            validRecords = validateAndOffsetAssignResult.validatedRecords\n+            appendInfo.maxTimestamp = validateAndOffsetAssignResult.maxTimestamp\n+            appendInfo.offsetOfMaxTimestamp = validateAndOffsetAssignResult.shallowOffsetOfMaxTimestamp\n+            appendInfo.lastOffset = offset.value - 1\n+            appendInfo.recordConversionStats = validateAndOffsetAssignResult.recordConversionStats\n+            if (config.messageTimestampType == TimestampType.LOG_APPEND_TIME)\n+              appendInfo.logAppendTime = now\n+\n+            // re-validate message sizes if there's a possibility that they have changed (due to re-compression or message\n+            // format conversion)\n+            if (!ignoreRecordSize && validateAndOffsetAssignResult.messageSizeMaybeChanged) {\n+              for (batch <- validRecords.batches.asScala) {\n+                if (batch.sizeInBytes > config.maxMessageSize) {\n+                  // we record the original message set size instead of the trimmed size\n+                  // to be consistent with pre-compression bytesRejectedRate recording\n+                  brokerTopicStats.topicStats(topicPartition.topic).bytesRejectedRate.mark(records.sizeInBytes)\n+                  brokerTopicStats.allTopicsStats.bytesRejectedRate.mark(records.sizeInBytes)\n+                  throw new RecordTooLargeException(s\"Message batch size is ${batch.sizeInBytes} bytes in append to\" +\n+                    s\"partition $topicPartition which exceeds the maximum configured size of ${config.maxMessageSize}.\")\n+                }\n+              }\n             }\n+          } else {\n+            // we are taking the offsets we are given\n+            if (!appendInfo.offsetsMonotonic)\n+              throw new OffsetsOutOfOrderException(s\"Out of order offsets found in append to $topicPartition: \" +\n+                records.records.asScala.map(_.offset))\n+\n+            if (appendInfo.firstOrLastOffsetOfFirstBatch < nextOffsetMetadata.messageOffset) {\n+              // we may still be able to recover if the log is empty\n+              // one example: fetching from log start offset on the leader which is not batch aligned,\n+              // which may happen as a result of AdminClient#deleteRecords()\n+              val firstOffset = appendInfo.firstOffset match {\n+                case Some(offset) => offset\n+                case None => records.batches.asScala.head.baseOffset()\n+              }\n \n-            val firstOrLast = if (appendInfo.firstOffset.isDefined) \"First offset\" else \"Last offset of the first batch\"\n-            throw new UnexpectedAppendOffsetException(\n-              s\"Unexpected offset in append to $topicPartition. $firstOrLast \" +\n-              s\"${appendInfo.firstOrLastOffsetOfFirstBatch} is less than the next offset ${nextOffsetMetadata.messageOffset}. \" +\n-              s\"First 10 offsets in append: ${records.records.asScala.take(10).map(_.offset)}, last offset in\" +\n-              s\" append: ${appendInfo.lastOffset}. Log start offset = $logStartOffset\",\n-              firstOffset, appendInfo.lastOffset)\n+              val firstOrLast = if (appendInfo.firstOffset.isDefined) \"First offset\" else \"Last offset of the first batch\"\n+              throw new UnexpectedAppendOffsetException(\n+                s\"Unexpected offset in append to $topicPartition. $firstOrLast \" +\n+                  s\"${appendInfo.firstOrLastOffsetOfFirstBatch} is less than the next offset ${nextOffsetMetadata.messageOffset}. \" +\n+                  s\"First 10 offsets in append: ${records.records.asScala.take(10).map(_.offset)}, last offset in\" +\n+                  s\" append: ${appendInfo.lastOffset}. Log start offset = $logStartOffset\",\n+                firstOffset, appendInfo.lastOffset)\n+            }\n           }\n-        }\n \n-        // update the epoch cache with the epoch stamped onto the message by the leader\n-        validRecords.batches.forEach { batch =>\n-          if (batch.magic >= RecordBatch.MAGIC_VALUE_V2) {\n-            maybeAssignEpochStartOffset(batch.partitionLeaderEpoch, batch.baseOffset)\n-          } else {\n-            // In partial upgrade scenarios, we may get a temporary regression to the message format. In\n-            // order to ensure the safety of leader election, we clear the epoch cache so that we revert\n-            // to truncation by high watermark after the next leader election.\n-            leaderEpochCache.filter(_.nonEmpty).foreach { cache =>\n-              warn(s\"Clearing leader epoch cache after unexpected append with message format v${batch.magic}\")\n-              cache.clearAndFlush()\n+          // update the epoch cache with the epoch stamped onto the message by the leader\n+          validRecords.batches.forEach { batch =>\n+            if (batch.magic >= RecordBatch.MAGIC_VALUE_V2) {\n+              maybeAssignEpochStartOffset(batch.partitionLeaderEpoch, batch.baseOffset)\n+            } else {\n+              // In partial upgrade scenarios, we may get a temporary regression to the message format. In\n+              // order to ensure the safety of leader election, we clear the epoch cache so that we revert\n+              // to truncation by high watermark after the next leader election.\n+              leaderEpochCache.filter(_.nonEmpty).foreach { cache =>\n+                warn(s\"Clearing leader epoch cache after unexpected append with message format v${batch.magic}\")\n+                cache.clearAndFlush()\n+              }\n             }\n           }\n-        }\n \n-        // check messages set size may be exceed config.segmentSize\n-        if (validRecords.sizeInBytes > config.segmentSize) {\n-          throw new RecordBatchTooLargeException(s\"Message batch size is ${validRecords.sizeInBytes} bytes in append \" +\n-            s\"to partition $topicPartition, which exceeds the maximum configured segment size of ${config.segmentSize}.\")\n-        }\n+          // check messages set size may be exceed config.segmentSize\n+          if (validRecords.sizeInBytes > config.segmentSize) {\n+            throw new RecordBatchTooLargeException(s\"Message batch size is ${validRecords.sizeInBytes} bytes in append \" +\n+              s\"to partition $topicPartition, which exceeds the maximum configured segment size of ${config.segmentSize}.\")\n+          }\n \n-        // maybe roll the log if this segment is full\n-        val segment = maybeRoll(validRecords.sizeInBytes, appendInfo)\n-\n-        val logOffsetMetadata = LogOffsetMetadata(\n-          messageOffset = appendInfo.firstOrLastOffsetOfFirstBatch,\n-          segmentBaseOffset = segment.baseOffset,\n-          relativePositionInSegment = segment.size)\n-\n-        // now that we have valid records, offsets assigned, and timestamps updated, we need to\n-        // validate the idempotent/transactional state of the producers and collect some metadata\n-        val (updatedProducers, completedTxns, maybeDuplicate) = analyzeAndValidateProducerState(\n-          logOffsetMetadata, validRecords, origin)\n-\n-        maybeDuplicate.foreach { duplicate =>\n-          appendInfo.firstOffset = Some(duplicate.firstOffset)\n-          appendInfo.lastOffset = duplicate.lastOffset\n-          appendInfo.logAppendTime = duplicate.timestamp\n-          appendInfo.logStartOffset = logStartOffset\n-          return appendInfo\n-        }\n+          // maybe roll the log if this segment is full\n+          val segment = maybeRoll(validRecords.sizeInBytes, appendInfo)\n \n-        segment.append(largestOffset = appendInfo.lastOffset,\n-          largestTimestamp = appendInfo.maxTimestamp,\n-          shallowOffsetOfMaxTimestamp = appendInfo.offsetOfMaxTimestamp,\n-          records = validRecords)\n-\n-        // Increment the log end offset. We do this immediately after the append because a\n-        // write to the transaction index below may fail and we want to ensure that the offsets\n-        // of future appends still grow monotonically. The resulting transaction index inconsistency\n-        // will be cleaned up after the log directory is recovered. Note that the end offset of the\n-        // ProducerStateManager will not be updated and the last stable offset will not advance\n-        // if the append to the transaction index fails.\n-        updateLogEndOffset(appendInfo.lastOffset + 1)\n-\n-        // update the producer state\n-        for (producerAppendInfo <- updatedProducers.values) {\n-          producerStateManager.update(producerAppendInfo)\n-        }\n+          val logOffsetMetadata = LogOffsetMetadata(\n+            messageOffset = appendInfo.firstOrLastOffsetOfFirstBatch,\n+            segmentBaseOffset = segment.baseOffset,\n+            relativePositionInSegment = segment.size)\n \n-        // update the transaction index with the true last stable offset. The last offset visible\n-        // to consumers using READ_COMMITTED will be limited by this value and the high watermark.\n-        for (completedTxn <- completedTxns) {\n-          val lastStableOffset = producerStateManager.lastStableOffset(completedTxn)\n-          segment.updateTxnIndex(completedTxn, lastStableOffset)\n-          producerStateManager.completeTxn(completedTxn)\n-        }\n+          // now that we have valid records, offsets assigned, and timestamps updated, we need to\n+          // validate the idempotent/transactional state of the producers and collect some metadata\n+          val (updatedProducers, completedTxns, maybeDuplicate) = analyzeAndValidateProducerState(\n+            logOffsetMetadata, validRecords, origin)\n \n-        // always update the last producer id map offset so that the snapshot reflects the current offset\n-        // even if there isn't any idempotent data being written\n-        producerStateManager.updateMapEndOffset(appendInfo.lastOffset + 1)\n+          if (maybeDuplicate.isDefined) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 257}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2158, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}