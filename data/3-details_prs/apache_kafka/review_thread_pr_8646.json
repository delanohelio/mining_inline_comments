{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE1OTYwMDEx", "number": 8646, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQyMjo1MTo0M1rOD72iFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQyMjo1MTo0M1rOD72iFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY0MDg2MDM3OnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQyMjo1MTo0M1rOGUbsOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxOTozMDowOVrOGiDV2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3ODM5NA==", "bodyText": "I am not sure if this is the right fix. Note that \"restoring\" and \"standby task update\" are two different things, and a standby task should. never use the \"restore\" code path what this assertion verifies.\nWhat could explain a restore is the migration of the active task from one instance to the other. However, this should actually not happen either. Could you reproduce the issue locally? We recently worked on rebalancing so maybe the issue is with regard to that.", "url": "https://github.com/apache/kafka/pull/8646#discussion_r424078394", "createdAt": "2020-05-12T22:51:43Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java", "diffHunk": "@@ -133,11 +133,6 @@ public void shouldApplyUpdatesToStandbyStore() throws Exception {\n             kafkaStreams1WasFirstActive = false;\n         }\n \n-        // Assert that no restore has occurred, ensures that when we check later that the restore\n-        // notification actually came from after the rebalance.\n-        assertThat(listener.startOffset, is(equalTo(0L)));\n-        assertThat(listener.totalNumRestored, is(equalTo(0L)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95ed0d7fb1202024a66864ce888f750a664f2315"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODUxOTEwMw==", "bodyText": "Sorry @mjsax , I tried to trace the code to find out why that would happen, but I still can't figure it out. Do you have any suggestion for it? If not, I think we can at least have more clear message output when this assert failure happen again. I'm not sure if the info is enough if the error happened again. And not sure if you have any thoughts about it?\nIt'll look like this when assert failure.\n\nThanks.", "url": "https://github.com/apache/kafka/pull/8646#discussion_r428519103", "createdAt": "2020-05-21T08:33:10Z", "author": {"login": "showuon"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java", "diffHunk": "@@ -133,11 +133,6 @@ public void shouldApplyUpdatesToStandbyStore() throws Exception {\n             kafkaStreams1WasFirstActive = false;\n         }\n \n-        // Assert that no restore has occurred, ensures that when we check later that the restore\n-        // notification actually came from after the rebalance.\n-        assertThat(listener.startOffset, is(equalTo(0L)));\n-        assertThat(listener.totalNumRestored, is(equalTo(0L)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3ODM5NA=="}, "originalCommit": {"oid": "95ed0d7fb1202024a66864ce888f750a664f2315"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUwMTc1OQ==", "bodyText": "I am not 100% sure about the root cause of the issue... If you have a good suggestion for a better error message, that would be great. I have not idea how we could improve it atm.", "url": "https://github.com/apache/kafka/pull/8646#discussion_r429501759", "createdAt": "2020-05-23T01:24:42Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java", "diffHunk": "@@ -133,11 +133,6 @@ public void shouldApplyUpdatesToStandbyStore() throws Exception {\n             kafkaStreams1WasFirstActive = false;\n         }\n \n-        // Assert that no restore has occurred, ensures that when we check later that the restore\n-        // notification actually came from after the rebalance.\n-        assertThat(listener.startOffset, is(equalTo(0L)));\n-        assertThat(listener.totalNumRestored, is(equalTo(0L)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3ODM5NA=="}, "originalCommit": {"oid": "95ed0d7fb1202024a66864ce888f750a664f2315"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM1OTAwMA==", "bodyText": "I think one possible explanation is that when we start the two instances, the rebalances took not just once but twice (once for the first instance, and another time for the second). In between the task may already be processed a bit on the first instance, and then after the second rebalance it was migrated and hence was restored a bit causing the listener to be triggered.\nI'd agree with @showuon here that we do not need to strictly forbids restoration not happening when starting the first two instances, just making sure after closing one instance we can restore the other up to the first batch is enough (which is already validated). So I'm fine with removing this listener all together.", "url": "https://github.com/apache/kafka/pull/8646#discussion_r438359000", "createdAt": "2020-06-10T19:29:05Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java", "diffHunk": "@@ -133,11 +133,6 @@ public void shouldApplyUpdatesToStandbyStore() throws Exception {\n             kafkaStreams1WasFirstActive = false;\n         }\n \n-        // Assert that no restore has occurred, ensures that when we check later that the restore\n-        // notification actually came from after the rebalance.\n-        assertThat(listener.startOffset, is(equalTo(0L)));\n-        assertThat(listener.totalNumRestored, is(equalTo(0L)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3ODM5NA=="}, "originalCommit": {"oid": "95ed0d7fb1202024a66864ce888f750a664f2315"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM1OTUxNA==", "bodyText": "BTW there's another report that line 159 can also fail:\njava.lang.AssertionError: Expected: is <true> but: was <false> at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20) at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6) at org.apache.kafka.streams.integration.OptimizedKTableIntegrationTest.shouldApplyUpdatesToStandbyStore(OptimizedKTableIntegrationTest.java:159)\n\nWhich is a bit mystery to me, since I cannot really think of a way how that could happen.\nANyways, for now removing that listener all together seems good to me.", "url": "https://github.com/apache/kafka/pull/8646#discussion_r438359514", "createdAt": "2020-06-10T19:30:09Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/OptimizedKTableIntegrationTest.java", "diffHunk": "@@ -133,11 +133,6 @@ public void shouldApplyUpdatesToStandbyStore() throws Exception {\n             kafkaStreams1WasFirstActive = false;\n         }\n \n-        // Assert that no restore has occurred, ensures that when we check later that the restore\n-        // notification actually came from after the rebalance.\n-        assertThat(listener.startOffset, is(equalTo(0L)));\n-        assertThat(listener.totalNumRestored, is(equalTo(0L)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA3ODM5NA=="}, "originalCommit": {"oid": "95ed0d7fb1202024a66864ce888f750a664f2315"}, "originalPosition": 7}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2563, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}