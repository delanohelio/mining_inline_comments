{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU4MTQ3NTE4", "number": 9094, "title": "KAFKA-10054: KIP-613, add TRACE-level e2e latency metrics", "bodyText": "Adds avg, min, and max e2e latency metrics at the new TRACE level. Also adds the missing avg task-level metric at the INFO level.\nI think where we left off with the KIP, the TRACE-level metrics were still defined to be \"stateful-processor-level\". I realized this doesn't really make sense and would be pretty much impossible to define given the DFS processing approach of Streams, and felt that store-level metrics made more sense to begin with. I haven't updated the KIP yet so I could get some initial feedback on this", "createdAt": "2020-07-29T01:59:58Z", "url": "https://github.com/apache/kafka/pull/9094", "merged": true, "mergeCommit": {"oid": "22bcd9fac3c988c15862d0b6c01930814b676253"}, "closed": true, "closedAt": "2020-08-25T00:37:50Z", "author": {"login": "ableegoldman"}, "timelineItems": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc5hN9mgFqTQ1NzE0NDQ0Ng==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc-q1tiABqjM2NTQ0NzMyMTg=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3MTQ0NDQ2", "url": "https://github.com/apache/kafka/pull/9094#pullrequestreview-457144446", "createdAt": "2020-07-29T02:01:53Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjowMTo1M1rOG4l22A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjowMTo1M1rOG4l22A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk5MzY4OA==", "bodyText": "It's kind of a bummer that we can't just add the new TRACE level for Streams only; we have to add it to all the clients that Streams passes its configs down to. We could check for the new TRACE level and strip it off before passing the configs on to the clients, but that just seems like asking for trouble.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r461993688", "createdAt": "2020-07-29T02:01:53Z", "author": {"login": "ableegoldman"}, "path": "clients/src/main/java/org/apache/kafka/clients/admin/AdminClientConfig.java", "diffHunk": "@@ -198,7 +198,7 @@\n                                 .define(METRICS_RECORDING_LEVEL_CONFIG,\n                                         Type.STRING,\n                                         Sensor.RecordingLevel.INFO.toString(),\n-                                        in(Sensor.RecordingLevel.INFO.toString(), Sensor.RecordingLevel.DEBUG.toString()),\n+                                        in(Sensor.RecordingLevel.INFO.toString(), Sensor.RecordingLevel.DEBUG.toString(), Sensor.RecordingLevel.TRACE.toString()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3MTQ0NzY5", "url": "https://github.com/apache/kafka/pull/9094#pullrequestreview-457144769", "createdAt": "2020-07-29T02:03:02Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjowMzowM1rOG4l4Ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjowMzowM1rOG4l4Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk5Mzk4Ng==", "bodyText": "Moved the common descriptions to StreamsMetricsImpl", "url": "https://github.com/apache/kafka/pull/9094#discussion_r461993986", "createdAt": "2020-07-29T02:03:03Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/TaskMetrics.java", "diffHunk": "@@ -88,13 +92,6 @@ private TaskMetrics() {}\n     private static final String NUM_BUFFERED_RECORDS_DESCRIPTION = \"The count of buffered records that are polled \" +\n         \"from consumer and not yet processed for this active task\";\n \n-    private static final String RECORD_E2E_LATENCY = \"record-e2e-latency\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3MTQ1Mzgy", "url": "https://github.com/apache/kafka/pull/9094#pullrequestreview-457145382", "createdAt": "2020-07-29T02:05:08Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjowNTowOVrOG4l6Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjowNTowOVrOG4l6Zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk5NDU5OQ==", "bodyText": "For KV stores, we just compare the current time with the current record's timestamp", "url": "https://github.com/apache/kafka/pull/9094#discussion_r461994599", "createdAt": "2020-07-29T02:05:09Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -227,6 +234,14 @@ protected Bytes keyBytes(final K key) {\n         return byteEntries;\n     }\n \n+    private void maybeRecordE2ELatency() {\n+        if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3MTUwMTM5", "url": "https://github.com/apache/kafka/pull/9094#pullrequestreview-457150139", "createdAt": "2020-07-29T02:20:38Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjoyMDozOFrOG4mK3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjoyMDozOFrOG4mK3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk5ODgxMg==", "bodyText": "For session and window stores, we also just compare the current time with the current record's timestamp when put is called. This can mean the e2e latency is measured several times on the same record, for example in a windowed aggregation.\nAt first I thought that didn't make sense, but now I think it's actually exactly what we want. First of all, it means we can actually account for the latency between calls to put within a processor. For simple point inserts this might not be a huge increase on the scale of ms, but more complex processing may benefit from seeing this granularity of information. If they don't want it, well, that's why we introduced TRACE\nSecond, while it might seem like we're over-weighting some records by measuring the e2e latency on them more than others, I'm starting to think this actually makes more sense than not: the big picture benefit/use case for the e2e latency metric is less \"how long for this record to get sent downstream\" and more \"how long for this record to be reflected in the state store/IQ results\". Given that, each record should be weighted by its actual proportion of the state store. You aren't querying individual records (in a window store), you're querying the windows themselves\nI toyed around with the idea of measuring the e2e latency relative to the window time, instead of the record timestamp, but ultimately couldn't find any sense in that.\nThoughts?", "url": "https://github.com/apache/kafka/pull/9094#discussion_r461998812", "createdAt": "2020-07-29T02:20:38Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java", "diffHunk": "@@ -248,4 +253,12 @@ public void close() {\n     private Bytes keyBytes(final K key) {\n         return Bytes.wrap(serdes.rawKey(key));\n     }\n+\n+    private void maybeRecordE2ELatency() {\n+        if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3MTUyNjQ2", "url": "https://github.com/apache/kafka/pull/9094#pullrequestreview-457152646", "createdAt": "2020-07-29T02:29:20Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjoyOToyMFrOG4mTVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwMjoyOToyMFrOG4mTVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAwMDk4MA==", "bodyText": "Ok there's something I'm not understanding about this test and/or the built-in metrics version. For some reason, the KV-store metrics are 0 when METRICS_0100_TO_24 is used, and 1 (as expected) when the latest version in used. I feel like this is wrong, and it should always be 1, but I need some clarify on how this config is supposed to be used\nWhat makes me pretty sure there's something actually wrong here is that for the window/session store metrics, they are actually always at 1. But I can't figure out why the KV store metrics would be any different than the others. Any ideas @cadonna ?", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462000980", "createdAt": "2020-07-29T02:29:20Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java", "diffHunk": "@@ -668,6 +671,9 @@ private void checkKeyValueStoreMetrics(final String group0100To24,\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_CURRENT, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_AVG, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_MAX, 0);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_AVG, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MIN, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MAX, expectedNumberofE2ELatencyMetrics);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU3NTU4NjQ5", "url": "https://github.com/apache/kafka/pull/9094#pullrequestreview-457558649", "createdAt": "2020-07-29T14:08:34Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDowODozNFrOG46NCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNToxMzoyM1rOG49Raw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMyNzA1MA==", "bodyText": "I think, you do not need to check for metrics with e2eLatencySensor.hasMetrics(). There should always be metrics within this sensor.\nhasMetrics() is used in StreamsMetricsImpl#maybeMeasureLatency() because some sensors may not contain any metrics due to the built-in metrics version. For instance, the destroy sensor exists for built-in metrics version 0.10.0-2.4 but not for latest. To avoid version checks in the record processing code, we just create an empty sensor and call record on it effectively not recording any metrics for this sensor for version latest.\nWe do not hide newly added metrics if the built-in version is set to an older version.\nSame applies to the other uses of hasMetrics() introduced in this PR.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462327050", "createdAt": "2020-07-29T14:08:34Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStore.java", "diffHunk": "@@ -227,6 +234,14 @@ protected Bytes keyBytes(final K key) {\n         return byteEntries;\n     }\n \n+    private void maybeRecordE2ELatency() {\n+        if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMzNjQ5Mw==", "bodyText": "Your approach makes sense to me. I agree that the latency should refer to the update in the state store and not to record itself. If a record updates the state more than once then latency should be measured each time.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462336493", "createdAt": "2020-07-29T14:20:23Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java", "diffHunk": "@@ -248,4 +253,12 @@ public void close() {\n     private Bytes keyBytes(final K key) {\n         return Bytes.wrap(serdes.rawKey(key));\n     }\n+\n+    private void maybeRecordE2ELatency() {\n+        if (e2eLatencySensor.shouldRecord() && e2eLatencySensor.hasMetrics()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTk5ODgxMg=="}, "originalCommit": null, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM0NTgxMQ==", "bodyText": "Could you test maybeRecordE2ELatency() through process() and forward()? Although you test maybeRecordE2ELatency(), you do not test if the recording is done during processing, but that is the crucial thing, IMO.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462345811", "createdAt": "2020-07-29T14:32:04Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "diffHunk": "@@ -468,38 +468,44 @@ public void shouldRecordE2ELatencyOnProcessForSourceNodes() {\n     }\n \n     @Test\n-    public void shouldRecordE2ELatencyMinAndMax() {\n+    public void shouldRecordE2ELatencyAvgAndMinAndMax() {\n         time = new MockTime(0L, 0L, 0L);\n         metrics = new Metrics(new MetricConfig().recordLevel(Sensor.RecordingLevel.INFO), time);\n         task = createStatelessTask(createConfig(false, \"0\"), StreamsConfig.METRICS_LATEST);\n \n         final String sourceNode = source1.name();\n \n-        final Metric maxMetric = getProcessorMetric(\"record-e2e-latency\", \"%s-max\", task.id().toString(), sourceNode, StreamsConfig.METRICS_LATEST);\n+        final Metric avgMetric = getProcessorMetric(\"record-e2e-latency\", \"%s-avg\", task.id().toString(), sourceNode, StreamsConfig.METRICS_LATEST);\n         final Metric minMetric = getProcessorMetric(\"record-e2e-latency\", \"%s-min\", task.id().toString(), sourceNode, StreamsConfig.METRICS_LATEST);\n+        final Metric maxMetric = getProcessorMetric(\"record-e2e-latency\", \"%s-max\", task.id().toString(), sourceNode, StreamsConfig.METRICS_LATEST);\n \n+        assertThat(avgMetric.metricValue(), equalTo(Double.NaN));\n         assertThat(minMetric.metricValue(), equalTo(Double.NaN));\n         assertThat(maxMetric.metricValue(), equalTo(Double.NaN));\n \n         // e2e latency = 10\n         task.maybeRecordE2ELatency(0L, 10L, sourceNode);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM3MDM0MA==", "bodyText": "You need to use the stateStoreLevelGroup() here instead of STATE_STORE_LEVEL_GROUP because the group name depends on the version and the store type.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462370340", "createdAt": "2020-07-29T15:04:00Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetrics.java", "diffHunk": "@@ -443,6 +447,25 @@ public static Sensor suppressionBufferSizeSensor(final String threadId,\n         );\n     }\n \n+    public static Sensor e2ELatencySensor(final String threadId,\n+                                          final String taskId,\n+                                          final String storeType,\n+                                          final String storeName,\n+                                          final StreamsMetricsImpl streamsMetrics) {\n+        final Sensor sensor = streamsMetrics.storeLevelSensor(threadId, taskId, storeName, RECORD_E2E_LATENCY, RecordingLevel.TRACE);\n+        final Map<String, String> tagMap = streamsMetrics.storeLevelTagMap(threadId, taskId, storeType, storeName);\n+        addAvgAndMinAndMaxToSensor(\n+            sensor,\n+            STATE_STORE_LEVEL_GROUP,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM3MzY5NQ==", "bodyText": "I agree with you, it should always be 1. It is the group of the metrics. See my comment in StateStoreMetrics. I am glad this test served its purpose, because I did not notice this in the unit tests!", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462373695", "createdAt": "2020-07-29T15:08:31Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/MetricsIntegrationTest.java", "diffHunk": "@@ -668,6 +671,9 @@ private void checkKeyValueStoreMetrics(final String group0100To24,\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_CURRENT, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_AVG, 0);\n         checkMetricByName(listMetricStore, SUPPRESSION_BUFFER_SIZE_MAX, 0);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_AVG, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MIN, expectedNumberofE2ELatencyMetrics);\n+        checkMetricByName(listMetricStore, RECORD_E2E_LATENCY_MAX, expectedNumberofE2ELatencyMetrics);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAwMDk4MA=="}, "originalCommit": null, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM3NzMyMw==", "bodyText": "You need to make this dependent on the built-in metrics version by using instance variable storeLevelGroup.", "url": "https://github.com/apache/kafka/pull/9094#discussion_r462377323", "createdAt": "2020-07-29T15:13:23Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/metrics/StateStoreMetricsTest.java", "diffHunk": "@@ -327,6 +327,38 @@ public void shouldGetExpiredWindowRecordDropSensor() {\n         assertThat(sensor, is(expectedSensor));\n     }\n \n+    @Test\n+    public void shouldGetRecordE2ELatencySensor() {\n+        final String metricName = \"record-e2e-latency\";\n+\n+        final String e2eLatencyDescription =\n+            \"end-to-end latency of a record, measuring by comparing the record timestamp with the \"\n+                + \"system time when it has been fully processed by the node\";\n+        final String descriptionOfAvg = \"The average \" + e2eLatencyDescription;\n+        final String descriptionOfMin = \"The minimum \" + e2eLatencyDescription;\n+        final String descriptionOfMax = \"The maximum \" + e2eLatencyDescription;\n+\n+        expect(streamsMetrics.storeLevelSensor(THREAD_ID, TASK_ID, STORE_NAME, metricName, RecordingLevel.TRACE))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.storeLevelTagMap(THREAD_ID, TASK_ID, STORE_TYPE, STORE_NAME)).andReturn(storeTagMap);\n+        StreamsMetricsImpl.addAvgAndMinAndMaxToSensor(\n+            expectedSensor,\n+            STORE_LEVEL_GROUP,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyMTczNzM3", "url": "https://github.com/apache/kafka/pull/9094#pullrequestreview-462173737", "createdAt": "2020-08-06T04:09:50Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNDowOTo1MVrOG8ikqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNDowOTo1MVrOG8ikqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjEzNDE4Ng==", "bodyText": "This is not introduced by this PR, but I'm wondering why we record the maybeRecordE2ELatency twice, once before the record is being processed and once when it reaches the sink node?", "url": "https://github.com/apache/kafka/pull/9094#discussion_r466134186", "createdAt": "2020-08-06T04:09:51Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -154,15 +153,15 @@ public StreamTask(final TaskId id,\n         for (final String terminalNodeName : topology.terminalNodes()) {\n             e2eLatencySensors.put(\n                 terminalNodeName,\n-                TaskMetrics.e2ELatencySensor(threadId, taskId, terminalNodeName, RecordingLevel.INFO, streamsMetrics)\n+                TaskMetrics.e2ELatencySensor(threadId, taskId, terminalNodeName, streamsMetrics)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1NTY4NTY5", "url": "https://github.com/apache/kafka/pull/9094#pullrequestreview-465568569", "createdAt": "2020-08-12T03:35:50Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MzIwMjQ0", "url": "https://github.com/apache/kafka/pull/9094#pullrequestreview-466320244", "createdAt": "2020-08-12T22:29:08Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMjoyOTowOFrOG_1BzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMjoyOTowOFrOG_1BzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU4MjI4NA==", "bodyText": "There's a slight hiccup with moving the INFO metrics from task to node level:\nWe get the current sensor from StreamsMetrics#taskLevelSensor which computes the fullSensorName with the #taskSensorPrefix\nIf we instead use StreamsMetrics#nodeLevelSensor then the fullSensorName is constructed from the #nodeSensorPrefix, which is obviously different. So moving this to a \u201ctrue\u201d node level sensor would be a breaking change, IIUC\nI think the best we can do is just move this from TaskMetrics to ProcessorNodeMetrics, but still leave it as a taskLevelSensor. Let me know if I'm missing something though cc @guozhangwang @cadonna", "url": "https://github.com/apache/kafka/pull/9094#discussion_r469582284", "createdAt": "2020-08-12T22:29:08Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ProcessorNodeMetrics.java", "diffHunk": "@@ -289,6 +294,25 @@ public static Sensor processorAtSourceSensorOrForwardSensor(final String threadI\n         return processAtSourceSensor(threadId, taskId, processorNodeId, streamsMetrics);\n     }\n \n+    public static Sensor e2ELatencySensor(final String threadId,\n+                                          final String taskId,\n+                                          final String processorNodeId,\n+                                          final StreamsMetricsImpl streamsMetrics) {\n+        final String sensorName = processorNodeId + \"-\" + RECORD_E2E_LATENCY;\n+        final Sensor sensor = streamsMetrics.taskLevelSensor(threadId, taskId, sensorName, RecordingLevel.INFO);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2NTQ2NjM2", "url": "https://github.com/apache/kafka/pull/9094#pullrequestreview-466546636", "createdAt": "2020-08-13T08:24:26Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aaaebb18cce6c43baed320a66a13e1c139078172", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/aaaebb18cce6c43baed320a66a13e1c139078172", "committedDate": "2020-08-14T01:48:24Z", "message": "WIP"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "df2b264644c7d4fe1807c5feff68c35876d4bdb0", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/df2b264644c7d4fe1807c5feff68c35876d4bdb0", "committedDate": "2020-08-14T01:48:26Z", "message": "add TRACE level metrics and avg"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c56ab0bac404f5722675a4202e46dac333a33e40", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/c56ab0bac404f5722675a4202e46dac333a33e40", "committedDate": "2020-08-14T01:48:26Z", "message": "add tests for new recording level"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6eb754aa3413a9f437ce1bd4e00f74570d93f13a", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/6eb754aa3413a9f437ce1bd4e00f74570d93f13a", "committedDate": "2020-08-14T01:48:26Z", "message": "checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b4197f357357cb654003470983e634c2bc57cc9d", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/b4197f357357cb654003470983e634c2bc57cc9d", "committedDate": "2020-08-14T01:48:26Z", "message": "fix metrics integration test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "800a95e5806cbfbd7c36c96b2d006ef5c275c3b9", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/800a95e5806cbfbd7c36c96b2d006ef5c275c3b9", "committedDate": "2020-08-14T01:48:26Z", "message": "review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cec034545c4809282bab363236bfeacdc9f8f36e", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/cec034545c4809282bab363236bfeacdc9f8f36e", "committedDate": "2020-08-14T01:48:27Z", "message": "improve StreamTaskTest"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "75a0ed2b73dfc5f7a2e8ac058924105149e7285f", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/75a0ed2b73dfc5f7a2e8ac058924105149e7285f", "committedDate": "2020-08-14T01:48:27Z", "message": "fix metrics integraiton test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "18fcd37b09645a687fe1e58dae183364b0973a39", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/18fcd37b09645a687fe1e58dae183364b0973a39", "committedDate": "2020-08-14T01:48:27Z", "message": "move to ProcessorNodeMetrics"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ee543643e9b5f656032211f0f609a9a3e841e028", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/ee543643e9b5f656032211f0f609a9a3e841e028", "committedDate": "2020-08-14T01:48:27Z", "message": "use nodeLevelSensor"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5469984a49edcd4e05ea016c10d51d828d5b7930", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/5469984a49edcd4e05ea016c10d51d828d5b7930", "committedDate": "2020-08-14T02:03:34Z", "message": "remove sensors in task"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "5469984a49edcd4e05ea016c10d51d828d5b7930", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/5469984a49edcd4e05ea016c10d51d828d5b7930", "committedDate": "2020-08-14T02:03:34Z", "message": "remove sensors in task"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 959, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}