{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQyMzMwNTE4", "number": 8962, "title": "KAFKA-10166: checkpoint recycled standbys and ignore empty rocksdb base directory", "bodyText": "Two more edge cases I found producing extra TaskcorruptedException while playing around with the failing eos-beta upgrade test (sadly these are unrelated problems, as the test still fails with these fixes in place).\n\nNeed to write the checkpoint when recycling a standby: although we do preserve the changelog offsets when recycling a task, and should therefore write the offsets when the new task is itself closed, we do NOT write the checkpoint for uninitialized tasks. So if the new task is ultimately closed before it gets out of the CREATED state, the offsets will not be written and we can get a TaskCorruptedException\nWith the change in task locking to address some Windows-related nonsense (am I remembering that correctly?), we don't delete entire task directories but just clear the inner state. With EOS, during initialization we check if the state directory is non-empty and the checkpoint is missing, and throw a TaskCorrupted if so. But just opening a rocksdb store creates a rocksdb base dir in the task directory, so the taskDirIsEmpty check always fails and we always throw TaskCorrupted even if there's nothing there. We can fix this for rocksdb (and custom stores) by searching through the task directory for any actual contents: we just do a BFS looking for any file that isn't itself a directory or sst file specifically if rocksdb\n\nNote: fix 2 is not perfect but it helps. It's not a correctness issue, just an annoyance.", "createdAt": "2020-06-30T22:30:47Z", "url": "https://github.com/apache/kafka/pull/8962", "merged": true, "mergeCommit": {"oid": "cdf68a4dae284ef021ae4ed26c5e0128c0cd7224"}, "closed": true, "closedAt": "2020-07-07T00:16:12Z", "author": {"login": "ableegoldman"}, "timelineItems": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcwb9bngH2gAyNDQyMzMwNTE4OjE0MjkwZmY5MjI1NTcxMDI4ZDcwYjljYjU5NmE5NDMzMDRlNzZiM2M=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcyYw44gFqTQ0MzQ0MDI3OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "14290ff9225571028d70b9cb596a943304e76b3c", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/14290ff9225571028d70b9cb596a943304e76b3c", "committedDate": "2020-06-30T20:48:59Z", "message": "write checkpoint for recycled standbys, clean up task producer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e8791a4bcd89c4e63b564006780594dbb727d5b4", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/e8791a4bcd89c4e63b564006780594dbb727d5b4", "committedDate": "2020-06-30T21:57:38Z", "message": "fix stateDirEmpty condition"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "47e1f4c86e44730416652ea5631f2da4e54e3c25", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/47e1f4c86e44730416652ea5631f2da4e54e3c25", "committedDate": "2020-06-30T21:59:21Z", "message": "remove unused import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2c7a2b6e4ac2b94ca4192960dbfef71d8258c5ea", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/2c7a2b6e4ac2b94ca4192960dbfef71d8258c5ea", "committedDate": "2020-06-30T22:12:43Z", "message": "add unit test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "25e4429f11281c2cd24e7e6750c101ce1026a30d", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/25e4429f11281c2cd24e7e6750c101ce1026a30d", "committedDate": "2020-06-30T22:20:44Z", "message": "move cleanupTaskProducer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "89dd4bd12427cae711fce736053d56da66213b22", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/89dd4bd12427cae711fce736053d56da66213b22", "committedDate": "2020-06-30T22:56:45Z", "message": "improve emptiness check"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "101aa1e304a3e13f09f9e60f741f37f0adafda2c", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/101aa1e304a3e13f09f9e60f741f37f0adafda2c", "committedDate": "2020-06-30T23:51:34Z", "message": "hack for rocksdb"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwNDY0MDE1", "url": "https://github.com/apache/kafka/pull/8962#pullrequestreview-440464015", "createdAt": "2020-07-01T00:03:40Z", "commit": {"oid": "101aa1e304a3e13f09f9e60f741f37f0adafda2c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwMDowMzo0MVrOGrSdKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwMDowMzo0MVrOGrSdKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA0NDMyOQ==", "bodyText": "This is admittedly quite hacky, and of course does not solve the problem for custom state stores that might write some non-data files upon open. A \"better\" fix would probably be to write some sentinel value in the checkpoint ala OFFSET_UNKNOWN, so we do have an entry in there if the store was opened but does not yet have any data.\nBut, I wanted to keep things simple (a very relative term here, I know) and low-risk before the 2.6 release. We can discuss better solutions once we're not at the doorstep of the release (and blocking the door, I might add)", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448044329", "createdAt": "2020-07-01T00:03:41Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java", "diffHunk": "@@ -136,7 +143,59 @@ private boolean taskDirEmpty(final File taskDir) {\n                 !pathname.getName().equals(CHECKPOINT_FILE_NAME));\n \n         // if the task is stateless, storeDirs would be null\n-        return storeDirs == null || storeDirs.length == 0;\n+        if (storeDirs == null || storeDirs.length == 0) {\n+            return true;\n+        }\n+\n+        final List<File> baseSubDirectories = new LinkedList<>();\n+        for (final File file : storeDirs) {\n+            if (file.isDirectory()) {\n+                baseSubDirectories.add(file);\n+            } else {\n+                return false;\n+            }\n+        }\n+\n+        for (final File dir : baseSubDirectories) {\n+            final boolean isEmpty;\n+            if (dir.getName().equals(ROCKSDB_DIRECTORY_NAME)) {\n+                isEmpty = taskSubDirectoriesEmpty(dir, true);\n+            } else {\n+                isEmpty =  taskSubDirectoriesEmpty(dir, false);\n+            }\n+            if (!isEmpty) {\n+                return false;\n+            }\n+        }\n+        return true;\n+    }\n+\n+    // BFS through the task directory to look for any files that are not more subdirectories\n+    private boolean taskSubDirectoriesEmpty(final File baseDir, final boolean sstOnly) {\n+        final Queue<File> subDirectories = new LinkedList<>();\n+        subDirectories.offer(baseDir);\n+\n+        final Set<File> visited = new HashSet<>();\n+        while (!subDirectories.isEmpty()) {\n+            final File dir = subDirectories.poll();\n+            if (!visited.contains(dir)) {\n+                final  File[] files = dir.listFiles();\n+                if (files == null) {\n+                    continue;\n+                }\n+                for (final File file : files) {\n+                    if (file.isDirectory()) {\n+                        subDirectories.offer(file);\n+                    } else if (sstOnly && file.getName().endsWith(ROCKSDB_SST_SUFFIX)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "101aa1e304a3e13f09f9e60f741f37f0adafda2c"}, "originalPosition": 69}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eb890804d38b163fb162a5ce5e44e293d5472065", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/eb890804d38b163fb162a5ce5e44e293d5472065", "committedDate": "2020-07-01T17:05:11Z", "message": "fix StateDirectory tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67ae92ff41b7ebb9e4bf65062ed0a02a0f5dfd69", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/67ae92ff41b7ebb9e4bf65062ed0a02a0f5dfd69", "committedDate": "2020-07-01T17:18:13Z", "message": "add realistic RocksDB test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0e4e213414b0b406210ecc3bffa58f348c74d2d7", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/0e4e213414b0b406210ecc3bffa58f348c74d2d7", "committedDate": "2020-07-01T17:22:10Z", "message": "unused import"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMTczMjY4", "url": "https://github.com/apache/kafka/pull/8962#pullrequestreview-441173268", "createdAt": "2020-07-01T20:25:29Z", "commit": {"oid": "0e4e213414b0b406210ecc3bffa58f348c74d2d7"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQyMDoyNToyOVrOGr0Oug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQyMDoyODowMVrOGr0TRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5NzY5MA==", "bodyText": "Thanks for this. What's the impact of the exception that we're avoiding here? It might be better to just not do anything right now than to introduce assumptions about the implementation of the default persistent store implementation here. If those assumptions become false later, it could be pretty bad.", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448597690", "createdAt": "2020-07-01T20:25:29Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java", "diffHunk": "@@ -136,7 +143,59 @@ private boolean taskDirEmpty(final File taskDir) {\n                 !pathname.getName().equals(CHECKPOINT_FILE_NAME));\n \n         // if the task is stateless, storeDirs would be null\n-        return storeDirs == null || storeDirs.length == 0;\n+        if (storeDirs == null || storeDirs.length == 0) {\n+            return true;\n+        }\n+\n+        final List<File> baseSubDirectories = new LinkedList<>();\n+        for (final File file : storeDirs) {\n+            if (file.isDirectory()) {\n+                baseSubDirectories.add(file);\n+            } else {\n+                return false;\n+            }\n+        }\n+\n+        for (final File dir : baseSubDirectories) {\n+            final boolean isEmpty;\n+            if (dir.getName().equals(ROCKSDB_DIRECTORY_NAME)) {\n+                isEmpty = taskSubDirectoriesEmpty(dir, true);\n+            } else {\n+                isEmpty =  taskSubDirectoriesEmpty(dir, false);\n+            }\n+            if (!isEmpty) {\n+                return false;\n+            }\n+        }\n+        return true;\n+    }\n+\n+    // BFS through the task directory to look for any files that are not more subdirectories\n+    private boolean taskSubDirectoriesEmpty(final File baseDir, final boolean sstOnly) {\n+        final Queue<File> subDirectories = new LinkedList<>();\n+        subDirectories.offer(baseDir);\n+\n+        final Set<File> visited = new HashSet<>();\n+        while (!subDirectories.isEmpty()) {\n+            final File dir = subDirectories.poll();\n+            if (!visited.contains(dir)) {\n+                final  File[] files = dir.listFiles();\n+                if (files == null) {\n+                    continue;\n+                }\n+                for (final File file : files) {\n+                    if (file.isDirectory()) {\n+                        subDirectories.offer(file);\n+                    } else if (sstOnly && file.getName().endsWith(ROCKSDB_SST_SUFFIX)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA0NDMyOQ=="}, "originalCommit": {"oid": "101aa1e304a3e13f09f9e60f741f37f0adafda2c"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5ODg1NA==", "bodyText": "This comment makes me a bit twitchy. Can we assert it instead? I mention it because I assume that active tasks also don't need to be committed because it should have happened already. Can we assert that as well?", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448598854", "createdAt": "2020-07-01T20:28:01Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -270,8 +270,11 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 if (oldTask.isActive()) {\n                     final Set<TopicPartition> partitions = standbyTasksToCreate.remove(oldTask.id());\n                     newTask = standbyTaskCreator.createStandbyTaskFromActive((StreamTask) oldTask, partitions);\n+                    cleanUpTaskProducer(oldTask, taskCloseExceptions);\n                 } else {\n                     oldTask.suspend(); // Only need to suspend transitioning standbys, actives should be suspended already", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0e4e213414b0b406210ecc3bffa58f348c74d2d7"}, "originalPosition": 6}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f314c79d3902c57e1ddf2e40c00deaa3c6254c03", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/f314c79d3902c57e1ddf2e40c00deaa3c6254c03", "committedDate": "2020-07-02T02:33:27Z", "message": "go with different approach; sentinel values in checkpoint"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMzEwOTQ1", "url": "https://github.com/apache/kafka/pull/8962#pullrequestreview-441310945", "createdAt": "2020-07-02T02:35:51Z", "commit": {"oid": "f314c79d3902c57e1ddf2e40c00deaa3c6254c03"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwMjozNTo1MVrOGr7Z-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwMjozNTo1MVrOGr7Z-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcxNTI1OQ==", "bodyText": "This is a little awkward, and it might be cleaner to just replace the use of null with this OFFSET_UNKNOWN sentinel throughout the ProcessorStateManager/StoreChangelogReader -- but, I wanted to keep the changes as short and simple as possible for now", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448715259", "createdAt": "2020-07-02T02:35:51Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -578,4 +580,14 @@ private StateStoreMetadata findStore(final TopicPartition changelogPartition) {\n \n         return found.isEmpty() ? null : found.get(0);\n     }\n+\n+    // Pass in a sentinel value to checkpoint when the changelog offset is not yet initialized/known\n+    private long checkpointableOffsetFromChangelogOffset(final Long offset) {\n+        return offset != null ? offset : OFFSET_UNKNOWN;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f314c79d3902c57e1ddf2e40c00deaa3c6254c03"}, "originalPosition": 38}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "402b91b74351a259d76c015978b97a3871065bb6", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/402b91b74351a259d76c015978b97a3871065bb6", "committedDate": "2020-07-02T02:36:13Z", "message": "remove extra space"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxOTY5NzAx", "url": "https://github.com/apache/kafka/pull/8962#pullrequestreview-441969701", "createdAt": "2020-07-02T19:34:03Z", "commit": {"oid": "402b91b74351a259d76c015978b97a3871065bb6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxOTozNDowM1rOGsar1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxOTozNDowM1rOGsar1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIyNzczMg==", "bodyText": "This is kind of awkward, we forgot actually read the checkpoint here \ud83e\udd26\u200d\u2640\ufe0f", "url": "https://github.com/apache/kafka/pull/8962#discussion_r449227732", "createdAt": "2020-07-02T19:34:03Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/OffsetCheckpointTest.java", "diffHunk": "@@ -91,20 +94,38 @@ public void shouldSkipNegativeOffsetsDuringRead() throws IOException {\n             offsets.put(new TopicPartition(topic, 0), -1L);\n \n             writeVersion0(offsets, file);\n+            assertTrue(checkpoint.read().isEmpty());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "402b91b74351a259d76c015978b97a3871065bb6"}, "originalPosition": 26}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNjI3Mzg1", "url": "https://github.com/apache/kafka/pull/8962#pullrequestreview-442627385", "createdAt": "2020-07-04T19:27:45Z", "commit": {"oid": "402b91b74351a259d76c015978b97a3871065bb6"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNFQxOToyNzo0NlrOGs9jxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNFQxOToyNzo0NlrOGs9jxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTc5OTExMA==", "bodyText": "I think @vvcephei 's confusion comes from a change that we now require all tasks to be transited to suspended before transiting to close: previously, we allow e.g. a running task to be closed immediately and inside the task itself the logic actually did the \"running -> suspend -> close\" logic, i.e. it is totally agnostic to the TM. In a refactoring with eos-beta we changed it. So now the responsibility is kinda split between the two: TM needs to make sure the transition is valid and the task verifies it. By doing this we avoided the \"pre-/post-\" functions.", "url": "https://github.com/apache/kafka/pull/8962#discussion_r449799110", "createdAt": "2020-07-04T19:27:46Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -270,8 +270,11 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 if (oldTask.isActive()) {\n                     final Set<TopicPartition> partitions = standbyTasksToCreate.remove(oldTask.id());\n                     newTask = standbyTaskCreator.createStandbyTaskFromActive((StreamTask) oldTask, partitions);\n+                    cleanUpTaskProducer(oldTask, taskCloseExceptions);\n                 } else {\n                     oldTask.suspend(); // Only need to suspend transitioning standbys, actives should be suspended already", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5ODg1NA=="}, "originalCommit": {"oid": "0e4e213414b0b406210ecc3bffa58f348c74d2d7"}, "originalPosition": 6}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ca3274ffc9a1973ed7a4977889e257cc8fc8b67e", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/ca3274ffc9a1973ed7a4977889e257cc8fc8b67e", "committedDate": "2020-07-06T19:48:24Z", "message": "add check"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQzMzY2Nzg1", "url": "https://github.com/apache/kafka/pull/8962#pullrequestreview-443366785", "createdAt": "2020-07-06T19:56:15Z", "commit": {"oid": "ca3274ffc9a1973ed7a4977889e257cc8fc8b67e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQzNDQwMjc5", "url": "https://github.com/apache/kafka/pull/8962#pullrequestreview-443440279", "createdAt": "2020-07-06T22:13:25Z", "commit": {"oid": "ca3274ffc9a1973ed7a4977889e257cc8fc8b67e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1416, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}