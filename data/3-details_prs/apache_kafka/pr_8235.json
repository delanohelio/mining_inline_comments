{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg0NTQyMzcx", "number": 8235, "title": "KAFKA-9176: Do not update limit offset if we are in RESTORE_ACTIVE mode", "bodyText": "Previously we may be updating the standby's limit offset as committed offsets to those source changelogs, and then inside the inner method we check if the state is in RESTORE_ACTIVE or not, which is a bug.\nWe should, instead, just check on the caller that we can skip restoring if:\n\nwe are in RESTORE_ACTIVE mode.\nthere're no source changelog partitions.\nthose partitions do not have any buffered records.\n\nAlso updated the unit test for this coverage.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-03-05T22:58:01Z", "url": "https://github.com/apache/kafka/pull/8235", "merged": true, "mergeCommit": {"oid": "dcfb641add650073f46fe8d8370c72f0284e62d7"}, "closed": true, "closedAt": "2020-03-10T00:48:45Z", "author": {"login": "guozhangwang"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcKzPMqgH2gAyMzg0NTQyMzcxOmZiNTBlYjhhNmZlN2ZjNTk1NmFiM2MxYjU1Y2U0OTBiMjk0YzA1Yzk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcMHB1pgFqTM3MTYwNDYyNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "fb50eb8a6fe7fc5956ab3c1b55ce490b294c05c9", "author": {"user": {"login": "guozhangwang", "name": "Guozhang Wang"}}, "url": "https://github.com/apache/kafka/commit/fb50eb8a6fe7fc5956ab3c1b55ce490b294c05c9", "committedDate": "2020-03-05T22:27:05Z", "message": "fix the unnecessary check"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "39da56bcae9df1d3640329fd145c31d2d15739bf", "author": {"user": {"login": "guozhangwang", "name": "Guozhang Wang"}}, "url": "https://github.com/apache/kafka/commit/39da56bcae9df1d3640329fd145c31d2d15739bf", "committedDate": "2020-03-05T22:42:27Z", "message": "update unit tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcxNDA5ODQz", "url": "https://github.com/apache/kafka/pull/8235#pullrequestreview-371409843", "createdAt": "2020-03-09T18:03:10Z", "commit": {"oid": "39da56bcae9df1d3640329fd145c31d2d15739bf"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxODowMzoxMVrOFzzkvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxODoxMjoxNlrOFzz3_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg2NjY4NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n          \n          \n            \n                        // we can try to update the limit offset as either committed offset for source changelog partitions;\n          \n          \n            \n                        // when the interval has elapsed we should try to update the limit offset for standbys reading from\n          \n          \n            \n                        // a source changelog with the new committed offset, unless there are no buffered records since \n          \n          \n            \n                        // we only need the limit when processing new records", "url": "https://github.com/apache/kafka/pull/8235#discussion_r389866684", "createdAt": "2020-03-09T18:03:11Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -451,15 +451,23 @@ public void restore() {\n     }\n \n     private void maybeUpdateLimitOffsetsForStandbyChangelogs() {\n-        // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n-        // we can try to update the limit offset next time.\n-        if (updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n-            final Set<ChangelogMetadata> standbyChangelogs = changelogs.values().stream()\n-                .filter(metadata -> metadata.stateManager.taskType() == Task.TaskType.STANDBY)\n-                .collect(Collectors.toSet());\n-            for (final ChangelogMetadata metadata : standbyChangelogs) {\n-                if (!metadata.bufferedRecords().isEmpty()) {\n-                    updateLimitOffsets();\n+        // we only consider updating the limit offset for standbys if we are not restoring active tasks\n+        if (state == ChangelogReaderState.STANDBY_UPDATING &&\n+            updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n+\n+            // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n+            // we can try to update the limit offset as either committed offset for source changelog partitions;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39da56bcae9df1d3640329fd145c31d2d15739bf"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg3MTYxMg==", "bodyText": "Q: if we skip updating the limit offsets because the buffer is empty, and then fetch new records immediately after, we'll still have to wait another commit interval to update the limit offsets. This means the standbys are potentially  lagging behind by up to the commit interval. Do we think that's worth the optimization of skipping the limit offset update?. Note I've seen a number of users report setting the commit interval quite large for various reasons (eg avoid flushing the memtables prematurely).\nMaybe we can find some middle ground by updating the limits before the commit interval if we do get some buffered records and skipped updating previously?", "url": "https://github.com/apache/kafka/pull/8235#discussion_r389871612", "createdAt": "2020-03-09T18:12:16Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -451,15 +451,23 @@ public void restore() {\n     }\n \n     private void maybeUpdateLimitOffsetsForStandbyChangelogs() {\n-        // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n-        // we can try to update the limit offset next time.\n-        if (updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n-            final Set<ChangelogMetadata> standbyChangelogs = changelogs.values().stream()\n-                .filter(metadata -> metadata.stateManager.taskType() == Task.TaskType.STANDBY)\n-                .collect(Collectors.toSet());\n-            for (final ChangelogMetadata metadata : standbyChangelogs) {\n-                if (!metadata.bufferedRecords().isEmpty()) {\n-                    updateLimitOffsets();\n+        // we only consider updating the limit offset for standbys if we are not restoring active tasks\n+        if (state == ChangelogReaderState.STANDBY_UPDATING &&\n+            updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n+\n+            // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n+            // we can try to update the limit offset as either committed offset for source changelog partitions;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg2NjY4NA=="}, "originalCommit": {"oid": "39da56bcae9df1d3640329fd145c31d2d15739bf"}, "originalPosition": 18}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcxNDcwNDA5", "url": "https://github.com/apache/kafka/pull/8235#pullrequestreview-371470409", "createdAt": "2020-03-09T19:33:54Z", "commit": {"oid": "39da56bcae9df1d3640329fd145c31d2d15739bf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxOTozMzo1NFrOFz2gog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxOTozMzo1NFrOFz2gog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTkxNDc4Ng==", "bodyText": "Could we just add one more boolean condition into the filter and check whether changelogsWithLimitOffsets is empty or not.", "url": "https://github.com/apache/kafka/pull/8235#discussion_r389914786", "createdAt": "2020-03-09T19:33:54Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -451,15 +451,23 @@ public void restore() {\n     }\n \n     private void maybeUpdateLimitOffsetsForStandbyChangelogs() {\n-        // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n-        // we can try to update the limit offset next time.\n-        if (updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n-            final Set<ChangelogMetadata> standbyChangelogs = changelogs.values().stream()\n-                .filter(metadata -> metadata.stateManager.taskType() == Task.TaskType.STANDBY)\n-                .collect(Collectors.toSet());\n-            for (final ChangelogMetadata metadata : standbyChangelogs) {\n-                if (!metadata.bufferedRecords().isEmpty()) {\n-                    updateLimitOffsets();\n+        // we only consider updating the limit offset for standbys if we are not restoring active tasks\n+        if (state == ChangelogReaderState.STANDBY_UPDATING &&\n+            updateOffsetIntervalMs < time.milliseconds() - lastUpdateOffsetTime) {\n+\n+            // for standby changelogs, if the interval has elapsed and there are buffered records not applicable,\n+            // we can try to update the limit offset as either committed offset for source changelog partitions;\n+            // for other changelog partitions we do not need to update limit offset at all since we never need to\n+            // check when it completes based on limit offset anyways: the end offset would keep increasing and the\n+            // standby never need to stop\n+            final Set<TopicPartition> changelogsWithLimitOffsets = changelogs.entrySet().stream()\n+                .filter(entry -> entry.getValue().stateManager.taskType() == Task.TaskType.STANDBY &&\n+                    entry.getValue().stateManager.changelogAsSource(entry.getKey()))\n+                .map(Map.Entry::getKey).collect(Collectors.toSet());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "39da56bcae9df1d3640329fd145c31d2d15739bf"}, "originalPosition": 25}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "00390129ed6c84be7b74018e60f4e6f764efeab9", "author": {"user": {"login": "guozhangwang", "name": "Guozhang Wang"}}, "url": "https://github.com/apache/kafka/commit/00390129ed6c84be7b74018e60f4e6f764efeab9", "committedDate": "2020-03-09T21:35:04Z", "message": "Update streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java\n\nCo-Authored-By: A. Sophie Blee-Goldman <ableegoldman@gmail.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcxNTU4NTM3", "url": "https://github.com/apache/kafka/pull/8235#pullrequestreview-371558537", "createdAt": "2020-03-09T22:00:26Z", "commit": {"oid": "00390129ed6c84be7b74018e60f4e6f764efeab9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcxNTkzMTk5", "url": "https://github.com/apache/kafka/pull/8235#pullrequestreview-371593199", "createdAt": "2020-03-09T23:29:21Z", "commit": {"oid": "00390129ed6c84be7b74018e60f4e6f764efeab9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQyMzoyOToyMVrOFz8tCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQyMzoyOToyMVrOFz8tCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDAxNjI2NQ==", "bodyText": "Is this really sufficient? We did call restore() once above with no available records and thus we don't expect that offset limits are updated. Now we call restore() again but would potentially update the offset limits at the very end -- hence would we not need one more call to restore() that the offset limits did no change?", "url": "https://github.com/apache/kafka/pull/8235#discussion_r390016265", "createdAt": "2020-03-09T23:29:21Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StoreChangelogReaderTest.java", "diffHunk": "@@ -627,6 +627,58 @@ public void shouldOnlyRestoreStandbyChangelogInUpdateStandbyState() {\n         assertTrue(changelogReader.changelogMetadata(tp).bufferedRecords().isEmpty());\n     }\n \n+    @Test\n+    public void shouldNotUpdateLimitForNonSourceStandbyChangelog() {\n+        EasyMock.expect(standbyStateManager.changelogAsSource(tp)).andReturn(false).anyTimes();\n+        EasyMock.replay(standbyStateManager, storeMetadata, store);\n+\n+        final MockConsumer<byte[], byte[]> consumer = new MockConsumer<byte[], byte[]>(OffsetResetStrategy.EARLIEST) {\n+            @Override\n+            public Map<TopicPartition, OffsetAndMetadata> committed(final Set<TopicPartition> partitions) {\n+                throw new AssertionError(\"Should not try to fetch committed offsets\");\n+            }\n+        };\n+\n+        final Properties properties = new Properties();\n+        properties.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 100L);\n+        final StreamsConfig config = new StreamsConfig(StreamsTestUtils.getStreamsConfig(\"test-reader\", properties));\n+        final StoreChangelogReader changelogReader = new StoreChangelogReader(time, config, logContext, consumer, callback);\n+        changelogReader.setMainConsumer(consumer);\n+        changelogReader.transitToUpdateStandby();\n+\n+        consumer.updateBeginningOffsets(Collections.singletonMap(tp, 5L));\n+        changelogReader.register(tp, standbyStateManager);\n+        assertNull(changelogReader.changelogMetadata(tp).endOffset());\n+        assertEquals(0L, changelogReader.changelogMetadata(tp).totalRestored());\n+\n+        // if there's no records fetchable, nothings gets restored\n+        changelogReader.restore();\n+        assertNull(callback.restoreTopicPartition);\n+        assertNull(callback.storeNameCalledStates.get(RESTORE_START));\n+        assertEquals(StoreChangelogReader.ChangelogState.RESTORING, changelogReader.changelogMetadata(tp).state());\n+        assertNull(changelogReader.changelogMetadata(tp).endOffset());\n+        assertEquals(0L, changelogReader.changelogMetadata(tp).totalRestored());\n+\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 5L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 6L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 7L, \"key\".getBytes(), \"value\".getBytes()));\n+        // null key should be ignored\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 8L, null, \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 9L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 10L, \"key\".getBytes(), \"value\".getBytes()));\n+        consumer.addRecord(new ConsumerRecord<>(topicName, 0, 11L, \"key\".getBytes(), \"value\".getBytes()));\n+\n+        // we should be able to restore to the log end offsets since there's no limit\n+        changelogReader.restore();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00390129ed6c84be7b74018e60f4e6f764efeab9"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcxNTk0MDUw", "url": "https://github.com/apache/kafka/pull/8235#pullrequestreview-371594050", "createdAt": "2020-03-09T23:31:53Z", "commit": {"oid": "00390129ed6c84be7b74018e60f4e6f764efeab9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcxNjA0NjI2", "url": "https://github.com/apache/kafka/pull/8235#pullrequestreview-371604626", "createdAt": "2020-03-10T00:04:31Z", "commit": {"oid": "00390129ed6c84be7b74018e60f4e6f764efeab9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 113, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}