{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYyODgxOTkx", "number": 7961, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQwMTo0MDoxOFrODYCo7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMzo1NjowNVrODYrY2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2NTM1NjYxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQwMTo0MDoxOFrOFdrJ4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQxODo1MDozMVrOFej3iA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY2MDA2NQ==", "bodyText": "@vvcephei @brary I actually have a concern with this overestimation.. Let's take a typical streams deployment with num_standby_replicas =1 ... During the time period, where the lag is fetched when a standby changelog partition is assigned but has not yet processed any events, we will report a really high value that may not lead to IQs being failed (since the store is deemed laggy).. Given a caller that periodically fetches the positions every few seconds would have a non-zero past value already, wonder if it makes sense to not report this partition instead.. i.e we only report lags on store partitions which have begun consumption..  At least for ksqlDB, I am thinking servers can track easily when lag was reported for a given store partition, given server and decide on using that value based on how recently it was updated..  But the values we receive won't be 'jumpy'. End of the day, I am also theorizing how this will behave.. So this is more about which way we want to bias.\nAcross the cluster, this time period will not in sync with each other and can happen differently. So not sure if this will in-fact provide correct relative order..", "url": "https://github.com/apache/kafka/pull/7961#discussion_r366660065", "createdAt": "2020-01-15T01:40:18Z", "author": {"login": "vinothchandar"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -1144,4 +1154,74 @@ public void cleanUp() {\n         }\n         return threadMetadata;\n     }\n+\n+    /**\n+     * Returns {@link LagInfo}, for all store partitions (active or standby) local to this Streams instance. Note that the\n+     * values returned are just estimates and meant to be used for making soft decisions on whether the data in the store\n+     * partition is fresh enough for querying.\n+     *\n+     * Note: Each invocation of this method issues a call to the Kafka brokers. Thus its advisable to limit the frequency\n+     * of invocation to once every few seconds.\n+     *\n+     * @return map of store names to another map of partition to {@link LagInfo}s\n+     */\n+    public Map<String, Map<Integer, LagInfo>> allLocalStorePartitionLags() {\n+        final long latestSentinel = -2L;\n+        final Map<String, Map<Integer, LagInfo>> localStorePartitionLags = new TreeMap<>();\n+\n+        final Collection<TopicPartition> allPartitions = new LinkedList<>();\n+        final Map<TopicPartition, Long> changelogPositions = new HashMap<>();\n+\n+        // Obtain the current positions, of all the active-restoring and standby tasks\n+        for (final StreamThread streamThread : threads) {\n+            for (final StandbyTask standbyTask : streamThread.allStandbyTasks()) {\n+                allPartitions.addAll(standbyTask.changelogPartitions());\n+                // Note that not all changelog partitions, will have positions; since some may not have started\n+                changelogPositions.putAll(standbyTask.getChangelogPositions());\n+            }\n+\n+            final Set<TaskId> restoringTaskIds = streamThread.restoringTaskIds();\n+            for (final StreamTask activeTask : streamThread.allStreamsTasks()) {\n+                final Collection<TopicPartition> taskChangelogPartitions = activeTask.changelogPartitions();\n+                allPartitions.addAll(taskChangelogPartitions);\n+\n+                final boolean isRestoring = restoringTaskIds.contains(activeTask.id());\n+                final Map<TopicPartition, Long> restoredOffsets = activeTask.restoredOffsets();\n+                for (final TopicPartition topicPartition : taskChangelogPartitions) {\n+                    if (isRestoring && restoredOffsets.containsKey(topicPartition)) {\n+                        changelogPositions.put(topicPartition, restoredOffsets.get(topicPartition));\n+                    } else {\n+                        changelogPositions.put(topicPartition, latestSentinel);\n+                    }\n+                }\n+            }\n+        }\n+\n+        log.debug(\"Current changelog positions: {}\", changelogPositions);\n+        final Map<TopicPartition, ListOffsetsResultInfo> allEndOffsets;\n+        try {\n+            allEndOffsets = adminClient.listOffsets(\n+                allPartitions.stream()\n+                    .collect(Collectors.toMap(Function.identity(), tp -> OffsetSpec.latest()))\n+            ).all().get();\n+        } catch (final RuntimeException | InterruptedException | ExecutionException e) {\n+            throw new StreamsException(\"Unable to obtain end offsets from kafka\", e);\n+        }\n+        log.debug(\"Current end offsets :{}\", allEndOffsets);\n+        for (final Map.Entry<TopicPartition, ListOffsetsResultInfo> entry : allEndOffsets.entrySet()) {\n+            // Avoiding an extra admin API lookup by computing lags for not-yet-started restorations", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzIxODMwOQ==", "bodyText": "Thanks for bringing this up. I'm not totally sure I follow, though...\nLet's say the partition in question has an end offset of 100 and a true \"earliest\" offset of 90 (due to compaction). If an instance has this partition assigned but has not yet recorded any position, we'd report its lag as 100. Another instance that has started processing but only processed one record would be reported with a lag of 9. This doesn't accurately represent the amount-of-work difference between the two instances (which is actually only one record, though the difference in lags is 91), but it does correctly state that the second instance is ahead of the first in freshness.\nI also seems reasonable to not report a lag at all for not-yet-started stores, but then you have to handle the special case in client-side code. I.e., the metadata API would tell you that this instance owns a store, but then there's no lag reported, so maybe there was a rebalance and we lost the store, or maybe we do own the store, but haven't started processing yet... It still seems simpler to me to say \"woah, that lag is really big, like it's lagging by 100% of the topic, maybe I won't query this store\" if I don't want to query really stale data, versus handling the special case to reach the same conclusion.\nRealistically, even if we are still restoring, but the store is still behind by a year (for example), you still wouldn't want to return the results, but rather would have some heuristic of how stale is ok and how stale is too much, which would be able to handle this math just as easily either way.\nOne other thought is that offset values can still be jumpy, due to compaction. So there's nothing to say that there are any records at all between offset 5 and offset 9999. Maybe this is contributing to my feeling that it's ok to just gloss over this case.", "url": "https://github.com/apache/kafka/pull/7961#discussion_r367218309", "createdAt": "2020-01-16T03:44:46Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -1144,4 +1154,74 @@ public void cleanUp() {\n         }\n         return threadMetadata;\n     }\n+\n+    /**\n+     * Returns {@link LagInfo}, for all store partitions (active or standby) local to this Streams instance. Note that the\n+     * values returned are just estimates and meant to be used for making soft decisions on whether the data in the store\n+     * partition is fresh enough for querying.\n+     *\n+     * Note: Each invocation of this method issues a call to the Kafka brokers. Thus its advisable to limit the frequency\n+     * of invocation to once every few seconds.\n+     *\n+     * @return map of store names to another map of partition to {@link LagInfo}s\n+     */\n+    public Map<String, Map<Integer, LagInfo>> allLocalStorePartitionLags() {\n+        final long latestSentinel = -2L;\n+        final Map<String, Map<Integer, LagInfo>> localStorePartitionLags = new TreeMap<>();\n+\n+        final Collection<TopicPartition> allPartitions = new LinkedList<>();\n+        final Map<TopicPartition, Long> changelogPositions = new HashMap<>();\n+\n+        // Obtain the current positions, of all the active-restoring and standby tasks\n+        for (final StreamThread streamThread : threads) {\n+            for (final StandbyTask standbyTask : streamThread.allStandbyTasks()) {\n+                allPartitions.addAll(standbyTask.changelogPartitions());\n+                // Note that not all changelog partitions, will have positions; since some may not have started\n+                changelogPositions.putAll(standbyTask.getChangelogPositions());\n+            }\n+\n+            final Set<TaskId> restoringTaskIds = streamThread.restoringTaskIds();\n+            for (final StreamTask activeTask : streamThread.allStreamsTasks()) {\n+                final Collection<TopicPartition> taskChangelogPartitions = activeTask.changelogPartitions();\n+                allPartitions.addAll(taskChangelogPartitions);\n+\n+                final boolean isRestoring = restoringTaskIds.contains(activeTask.id());\n+                final Map<TopicPartition, Long> restoredOffsets = activeTask.restoredOffsets();\n+                for (final TopicPartition topicPartition : taskChangelogPartitions) {\n+                    if (isRestoring && restoredOffsets.containsKey(topicPartition)) {\n+                        changelogPositions.put(topicPartition, restoredOffsets.get(topicPartition));\n+                    } else {\n+                        changelogPositions.put(topicPartition, latestSentinel);\n+                    }\n+                }\n+            }\n+        }\n+\n+        log.debug(\"Current changelog positions: {}\", changelogPositions);\n+        final Map<TopicPartition, ListOffsetsResultInfo> allEndOffsets;\n+        try {\n+            allEndOffsets = adminClient.listOffsets(\n+                allPartitions.stream()\n+                    .collect(Collectors.toMap(Function.identity(), tp -> OffsetSpec.latest()))\n+            ).all().get();\n+        } catch (final RuntimeException | InterruptedException | ExecutionException e) {\n+            throw new StreamsException(\"Unable to obtain end offsets from kafka\", e);\n+        }\n+        log.debug(\"Current end offsets :{}\", allEndOffsets);\n+        for (final Map.Entry<TopicPartition, ListOffsetsResultInfo> entry : allEndOffsets.entrySet()) {\n+            // Avoiding an extra admin API lookup by computing lags for not-yet-started restorations", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY2MDA2NQ=="}, "originalCommit": null, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzU4OTI1Ng==", "bodyText": "but it does correctly state that the second instance is ahead of the first in freshness.\n\nFair. Your argument is if you have not begun work, then you would be behind another instance that has begun work..\n\nreasonable to not report a lag at all for not-yet-started stores, but then you have to handle the special case in client-side code.\n\nthe client code is going to have to use some mechanism to \"fence\" really bad lag.. i.e full restorations.. My concern was that if we hit this race condition, you would suddenly see a jump to \"oh this replica needs to fully restore\" and then back to the actual lag..\n\nOne other thought is that offset values can still be jumpy, due to compaction.\n\nI did think about this a bit.. I think what we have here would work fairly well in the common case, where the active and standby are apart by lag within the uncompacted portion of the changelog topic..\n\nSo there's nothing to say that there are any records at all between offset 5 and offset 9999.\nOnce beyond that, since we just rely on the latest lag value, this is okay.. if there are no records, the restoration will be faster than expected.. but as a relative metric, it's still good..\n\nIn all, I am okay sticking with this behavior for now.. We can revisit based on real world experience.. and really build a solid solid implementation in the next month or so..", "url": "https://github.com/apache/kafka/pull/7961#discussion_r367589256", "createdAt": "2020-01-16T18:50:31Z", "author": {"login": "vinothchandar"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -1144,4 +1154,74 @@ public void cleanUp() {\n         }\n         return threadMetadata;\n     }\n+\n+    /**\n+     * Returns {@link LagInfo}, for all store partitions (active or standby) local to this Streams instance. Note that the\n+     * values returned are just estimates and meant to be used for making soft decisions on whether the data in the store\n+     * partition is fresh enough for querying.\n+     *\n+     * Note: Each invocation of this method issues a call to the Kafka brokers. Thus its advisable to limit the frequency\n+     * of invocation to once every few seconds.\n+     *\n+     * @return map of store names to another map of partition to {@link LagInfo}s\n+     */\n+    public Map<String, Map<Integer, LagInfo>> allLocalStorePartitionLags() {\n+        final long latestSentinel = -2L;\n+        final Map<String, Map<Integer, LagInfo>> localStorePartitionLags = new TreeMap<>();\n+\n+        final Collection<TopicPartition> allPartitions = new LinkedList<>();\n+        final Map<TopicPartition, Long> changelogPositions = new HashMap<>();\n+\n+        // Obtain the current positions, of all the active-restoring and standby tasks\n+        for (final StreamThread streamThread : threads) {\n+            for (final StandbyTask standbyTask : streamThread.allStandbyTasks()) {\n+                allPartitions.addAll(standbyTask.changelogPartitions());\n+                // Note that not all changelog partitions, will have positions; since some may not have started\n+                changelogPositions.putAll(standbyTask.getChangelogPositions());\n+            }\n+\n+            final Set<TaskId> restoringTaskIds = streamThread.restoringTaskIds();\n+            for (final StreamTask activeTask : streamThread.allStreamsTasks()) {\n+                final Collection<TopicPartition> taskChangelogPartitions = activeTask.changelogPartitions();\n+                allPartitions.addAll(taskChangelogPartitions);\n+\n+                final boolean isRestoring = restoringTaskIds.contains(activeTask.id());\n+                final Map<TopicPartition, Long> restoredOffsets = activeTask.restoredOffsets();\n+                for (final TopicPartition topicPartition : taskChangelogPartitions) {\n+                    if (isRestoring && restoredOffsets.containsKey(topicPartition)) {\n+                        changelogPositions.put(topicPartition, restoredOffsets.get(topicPartition));\n+                    } else {\n+                        changelogPositions.put(topicPartition, latestSentinel);\n+                    }\n+                }\n+            }\n+        }\n+\n+        log.debug(\"Current changelog positions: {}\", changelogPositions);\n+        final Map<TopicPartition, ListOffsetsResultInfo> allEndOffsets;\n+        try {\n+            allEndOffsets = adminClient.listOffsets(\n+                allPartitions.stream()\n+                    .collect(Collectors.toMap(Function.identity(), tp -> OffsetSpec.latest()))\n+            ).all().get();\n+        } catch (final RuntimeException | InterruptedException | ExecutionException e) {\n+            throw new StreamsException(\"Unable to obtain end offsets from kafka\", e);\n+        }\n+        log.debug(\"Current end offsets :{}\", allEndOffsets);\n+        for (final Map.Entry<TopicPartition, ListOffsetsResultInfo> entry : allEndOffsets.entrySet()) {\n+            // Avoiding an extra admin API lookup by computing lags for not-yet-started restorations", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjY2MDA2NQ=="}, "originalCommit": null, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2NzI2MjI3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNVQxNTo1NjowNFrOFd9PVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQxNToyNTowNFrOFedO-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Njk1NjM3Mw==", "bodyText": "is there a notion of experimental APIs in streams? (like some other apache projects like Spark have) I think we can mark this experimental if so, setting the expectations with the user for the next release.", "url": "https://github.com/apache/kafka/pull/7961#discussion_r366956373", "createdAt": "2020-01-15T15:56:04Z", "author": {"login": "vinothchandar"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -1144,4 +1154,74 @@ public void cleanUp() {\n         }\n         return threadMetadata;\n     }\n+\n+    /**\n+     * Returns {@link LagInfo}, for all store partitions (active or standby) local to this Streams instance. Note that the\n+     * values returned are just estimates and meant to be used for making soft decisions on whether the data in the store\n+     * partition is fresh enough for querying.\n+     *\n+     * Note: Each invocation of this method issues a call to the Kafka brokers. Thus its advisable to limit the frequency\n+     * of invocation to once every few seconds.\n+     *\n+     * @return map of store names to another map of partition to {@link LagInfo}s\n+     */\n+    public Map<String, Map<Integer, LagInfo>> allLocalStorePartitionLags() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzE0MDMzNQ==", "bodyText": "There's not. There's \"interface: evolving\", but I think it's only for... interfaces. We already say in the javadoc that the returned values are estimates, so I think we've created enough wiggle room in the correctness of the returned lags. As to whether the method signature itself might change in the future, we'll just have to go the normal route of deprecating and replacing if we want to change it.", "url": "https://github.com/apache/kafka/pull/7961#discussion_r367140335", "createdAt": "2020-01-15T22:26:18Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -1144,4 +1154,74 @@ public void cleanUp() {\n         }\n         return threadMetadata;\n     }\n+\n+    /**\n+     * Returns {@link LagInfo}, for all store partitions (active or standby) local to this Streams instance. Note that the\n+     * values returned are just estimates and meant to be used for making soft decisions on whether the data in the store\n+     * partition is fresh enough for querying.\n+     *\n+     * Note: Each invocation of this method issues a call to the Kafka brokers. Thus its advisable to limit the frequency\n+     * of invocation to once every few seconds.\n+     *\n+     * @return map of store names to another map of partition to {@link LagInfo}s\n+     */\n+    public Map<String, Map<Integer, LagInfo>> allLocalStorePartitionLags() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Njk1NjM3Mw=="}, "originalCommit": null, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzQ4MDU3MQ==", "bodyText": "Okay.. lets stick to what we have.", "url": "https://github.com/apache/kafka/pull/7961#discussion_r367480571", "createdAt": "2020-01-16T15:25:04Z", "author": {"login": "vinothchandar"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -1144,4 +1154,74 @@ public void cleanUp() {\n         }\n         return threadMetadata;\n     }\n+\n+    /**\n+     * Returns {@link LagInfo}, for all store partitions (active or standby) local to this Streams instance. Note that the\n+     * values returned are just estimates and meant to be used for making soft decisions on whether the data in the store\n+     * partition is fresh enough for querying.\n+     *\n+     * Note: Each invocation of this method issues a call to the Kafka brokers. Thus its advisable to limit the frequency\n+     * of invocation to once every few seconds.\n+     *\n+     * @return map of store names to another map of partition to {@link LagInfo}s\n+     */\n+    public Map<String, Map<Integer, LagInfo>> allLocalStorePartitionLags() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Njk1NjM3Mw=="}, "originalCommit": null, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2ODg3NzQ1OnYy", "diffSide": "RIGHT", "path": "clients/src/test/java/org/apache/kafka/clients/admin/MockAdminClient.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQwMzoyMjoxMFrOFeM-bQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQxNTo0NzoxNFrOFeeDCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzIxNDE4OQ==", "bodyText": "What is the purpose of this change? It looks unrelated, but I'm guessing it's related somehow to your tests?", "url": "https://github.com/apache/kafka/pull/7961#discussion_r367214189", "createdAt": "2020-01-16T03:22:10Z", "author": {"login": "vvcephei"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/MockAdminClient.java", "diffHunk": "@@ -450,12 +450,17 @@ public ListPartitionReassignmentsResult listPartitionReassignments(Optional<Set<\n \n     @Override\n     public AlterConsumerGroupOffsetsResult alterConsumerGroupOffsets(String groupId, Map<TopicPartition, OffsetAndMetadata> offsets, AlterConsumerGroupOffsetsOptions options) {\n-        throw new UnsupportedOperationException(\"Not implement yet\");\n+        throw new UnsupportedOperationException(\"Not implemented yet\");\n     }\n \n     @Override\n     public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets, ListOffsetsOptions options) {\n-        throw new UnsupportedOperationException(\"Not implement yet\");\n+        throw new UnsupportedOperationException(\"Not implemented yet\");\n+    }\n+\n+    @Override\n+    public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets) {\n+        throw new UnsupportedOperationException(\"Not implemented yet\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eb33dba59f443d56b860fc3b80dcbafc00c597af"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzQ4MTM3NQ==", "bodyText": "Thats the variant I am using in allLocalOffsetLags.. I swear, at some point I needed it for the KafkaStreamsTest to pass .. I took another look though and test currently with a partial mock on MockAdminClient seems to pass without this change.. So will back this out. Thanks for the call out", "url": "https://github.com/apache/kafka/pull/7961#discussion_r367481375", "createdAt": "2020-01-16T15:26:26Z", "author": {"login": "vinothchandar"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/MockAdminClient.java", "diffHunk": "@@ -450,12 +450,17 @@ public ListPartitionReassignmentsResult listPartitionReassignments(Optional<Set<\n \n     @Override\n     public AlterConsumerGroupOffsetsResult alterConsumerGroupOffsets(String groupId, Map<TopicPartition, OffsetAndMetadata> offsets, AlterConsumerGroupOffsetsOptions options) {\n-        throw new UnsupportedOperationException(\"Not implement yet\");\n+        throw new UnsupportedOperationException(\"Not implemented yet\");\n     }\n \n     @Override\n     public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets, ListOffsetsOptions options) {\n-        throw new UnsupportedOperationException(\"Not implement yet\");\n+        throw new UnsupportedOperationException(\"Not implemented yet\");\n+    }\n+\n+    @Override\n+    public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets) {\n+        throw new UnsupportedOperationException(\"Not implemented yet\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzIxNDE4OQ=="}, "originalCommit": {"oid": "eb33dba59f443d56b860fc3b80dcbafc00c597af"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzQ5Mzg5Ng==", "bodyText": "Thanks!", "url": "https://github.com/apache/kafka/pull/7961#discussion_r367493896", "createdAt": "2020-01-16T15:47:14Z", "author": {"login": "vvcephei"}, "path": "clients/src/test/java/org/apache/kafka/clients/admin/MockAdminClient.java", "diffHunk": "@@ -450,12 +450,17 @@ public ListPartitionReassignmentsResult listPartitionReassignments(Optional<Set<\n \n     @Override\n     public AlterConsumerGroupOffsetsResult alterConsumerGroupOffsets(String groupId, Map<TopicPartition, OffsetAndMetadata> offsets, AlterConsumerGroupOffsetsOptions options) {\n-        throw new UnsupportedOperationException(\"Not implement yet\");\n+        throw new UnsupportedOperationException(\"Not implemented yet\");\n     }\n \n     @Override\n     public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets, ListOffsetsOptions options) {\n-        throw new UnsupportedOperationException(\"Not implement yet\");\n+        throw new UnsupportedOperationException(\"Not implemented yet\");\n+    }\n+\n+    @Override\n+    public ListOffsetsResult listOffsets(Map<TopicPartition, OffsetSpec> topicPartitionOffsets) {\n+        throw new UnsupportedOperationException(\"Not implemented yet\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzIxNDE4OQ=="}, "originalCommit": {"oid": "eb33dba59f443d56b860fc3b80dcbafc00c597af"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI2ODkxOTQyOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQwMzo1Njo0N1rOFeNW4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQxNTo1NDowOVrOFeeUnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzIyMDQ1MQ==", "bodyText": "This is neat, but we shouldn't use it. There's an IntegrationTestUtil for getting a temporary folder, which is hooked in to support for different testing environments to set their desired temporary file location.", "url": "https://github.com/apache/kafka/pull/7961#discussion_r367220451", "createdAt": "2020-01-16T03:56:47Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.startApplicationAndWaitUntilRunning;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.core.IsEqual.equalTo;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.CyclicBarrier;\n+import java.util.concurrent.TimeUnit;\n+import kafka.utils.MockTime;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KafkaStreamsWrapper;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.LagInfo;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.StreamThread;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.rules.TestName;\n+\n+@Category({IntegrationTest.class})\n+public class LagFetchIntegrationTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    private static final long CONSUMER_TIMEOUT_MS = 60000;\n+\n+    @Rule\n+    public TemporaryFolder folder = new TemporaryFolder();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf7522ecf1a7d5506c771279de064855319f4cde"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzQ4NDAzNA==", "bodyText": "I see a TestUtils#temporaryFolder which basically uses java Files.createTempDirectory/deleteOnExit route.. This is what you switched to in the QueryableStateIntegrationTest as I see ..\nI ll play by house rules.. but I don't see a IntegrationTestUtil or a related method in IntegrationTestUtils .. I see one where it purges local state dir from streams config.. Is that what you are referring to..\nOnce you respond, I will clean up both tests", "url": "https://github.com/apache/kafka/pull/7961#discussion_r367484034", "createdAt": "2020-01-16T15:30:56Z", "author": {"login": "vinothchandar"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.startApplicationAndWaitUntilRunning;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.core.IsEqual.equalTo;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.CyclicBarrier;\n+import java.util.concurrent.TimeUnit;\n+import kafka.utils.MockTime;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KafkaStreamsWrapper;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.LagInfo;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.StreamThread;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.rules.TestName;\n+\n+@Category({IntegrationTest.class})\n+public class LagFetchIntegrationTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    private static final long CONSUMER_TIMEOUT_MS = 60000;\n+\n+    @Rule\n+    public TemporaryFolder folder = new TemporaryFolder();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzIyMDQ1MQ=="}, "originalCommit": {"oid": "cf7522ecf1a7d5506c771279de064855319f4cde"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzQ5ODM5Ng==", "bodyText": "Ah, yes, it's org.apache.kafka.test.TestUtils#tempDirectory(). My mistake. The protocol is for all temporary state in Kafka tests to use that method.\nThe change I made in QueryableStateIntegrationTest is basically what we should do here as well.", "url": "https://github.com/apache/kafka/pull/7961#discussion_r367498396", "createdAt": "2020-01-16T15:54:09Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/LagFetchIntegrationTest.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.startApplicationAndWaitUntilRunning;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.core.IsEqual.equalTo;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.CyclicBarrier;\n+import java.util.concurrent.TimeUnit;\n+import kafka.utils.MockTime;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.LongDeserializer;\n+import org.apache.kafka.common.serialization.LongSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KafkaStreamsWrapper;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.LagInfo;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.KTable;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.StreamThread;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.rules.TestName;\n+\n+@Category({IntegrationTest.class})\n+public class LagFetchIntegrationTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    private static final long CONSUMER_TIMEOUT_MS = 60000;\n+\n+    @Rule\n+    public TemporaryFolder folder = new TemporaryFolder();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzIyMDQ1MQ=="}, "originalCommit": {"oid": "cf7522ecf1a7d5506c771279de064855319f4cde"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MjAwNzQwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/LagInfo.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMzo0MToyNVrOFeq-Og==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMzo0MToyNVrOFeq-Og==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcwNTY1OA==", "bodyText": "Since offsetLag is computed from the other two fields, this comparison is not necessary.", "url": "https://github.com/apache/kafka/pull/7961#discussion_r367705658", "createdAt": "2020-01-16T23:41:25Z", "author": {"login": "tedyu"}, "path": "streams/src/main/java/org/apache/kafka/streams/LagInfo.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams;\n+\n+import java.util.Objects;\n+\n+/**\n+ * Encapsulates information about lag, at a store partition replica (active or standby). This information is constantly changing as the\n+ * tasks process records and thus, they should be treated as simply instantaenous measure of lag.\n+ */\n+public class LagInfo {\n+\n+    private final long currentOffsetPosition;\n+\n+    private final long endOffsetPosition;\n+\n+    private final long offsetLag;\n+\n+    LagInfo(final long currentOffsetPosition, final long endOffsetPosition) {\n+        this.currentOffsetPosition = currentOffsetPosition;\n+        this.endOffsetPosition = endOffsetPosition;\n+        this.offsetLag = Math.max(0, endOffsetPosition - currentOffsetPosition);\n+    }\n+\n+    /**\n+     * Get the current maximum offset on the store partition's changelog topic, that has been successfully written into\n+     * the store partition's state store.\n+     *\n+     * @return current consume offset for standby/restoring store partitions & simply endoffset for active store partition replicas\n+     */\n+    public long currentOffsetPosition() {\n+        return this.currentOffsetPosition;\n+    }\n+\n+    /**\n+     * Get the end offset position for this store partition's changelog topic on the Kafka brokers.\n+     *\n+     * @return last offset written to the changelog topic partition\n+     */\n+    public long endOffsetPosition() {\n+        return this.endOffsetPosition;\n+    }\n+\n+    /**\n+     * Get the measured lag between current and end offset positions, for this store partition replica\n+     *\n+     * @return lag as measured by message offsets\n+     */\n+    public long offsetLag() {\n+        return this.offsetLag;\n+    }\n+\n+    @Override\n+    public boolean equals(final Object obj) {\n+        if (!(obj instanceof LagInfo)) {\n+            return false;\n+        }\n+        final LagInfo other = (LagInfo) obj;\n+        return currentOffsetPosition == other.currentOffsetPosition\n+            && endOffsetPosition == other.endOffsetPosition\n+            && this.offsetLag == other.offsetLag;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "026b9dd5f36a78839a5e628cd51d2e98f91313e9"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3MjAzMjkxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMzo1NjowNVrOFerNTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QyMjoxMToxMlrOFfGizA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcwOTUxNg==", "bodyText": "I noticed some maps are changed to ConcurrentHashMap.\nMay I ask what was the selection criterion for the change ?\nthanks", "url": "https://github.com/apache/kafka/pull/7961#discussion_r367709516", "createdAt": "2020-01-16T23:56:05Z", "author": {"login": "tedyu"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -46,7 +47,7 @@\n     private final StateRestoreListener userStateRestoreListener;\n     private final Map<TopicPartition, Long> restoreToOffsets = new HashMap<>();\n     private final Map<String, List<PartitionInfo>> partitionInfo = new HashMap<>();\n-    private final Map<TopicPartition, StateRestorer> stateRestorers = new HashMap<>();\n+    private final Map<TopicPartition, StateRestorer> stateRestorers = new ConcurrentHashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "026b9dd5f36a78839a5e628cd51d2e98f91313e9"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODE1NzM4OA==", "bodyText": "This is for safe iteration from the thread calling the lag fetch API", "url": "https://github.com/apache/kafka/pull/7961#discussion_r368157388", "createdAt": "2020-01-17T22:11:12Z", "author": {"login": "vinothchandar"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java", "diffHunk": "@@ -46,7 +47,7 @@\n     private final StateRestoreListener userStateRestoreListener;\n     private final Map<TopicPartition, Long> restoreToOffsets = new HashMap<>();\n     private final Map<String, List<PartitionInfo>> partitionInfo = new HashMap<>();\n-    private final Map<TopicPartition, StateRestorer> stateRestorers = new HashMap<>();\n+    private final Map<TopicPartition, StateRestorer> stateRestorers = new ConcurrentHashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcwOTUxNg=="}, "originalCommit": {"oid": "026b9dd5f36a78839a5e628cd51d2e98f91313e9"}, "originalPosition": 13}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4367, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}