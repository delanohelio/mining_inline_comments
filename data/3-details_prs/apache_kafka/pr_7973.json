{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYzNzA1NDg2", "number": 7973, "title": "MINOR: Handle expandIsr in PartitionLockTest and ensure read threads not blocked on write", "bodyText": "Noticed a PR build failure in the new PartitionLockTest:\nERROR Exception during updateFollowerFetchState (kafka.cluster.PartitionLockTest:76)\nscala.MatchError: null\n\tat kafka.cluster.Partition.maybeUpdateIsrAndVersion(Partition.scala:1193)\n\tat kafka.cluster.Partition.expandIsr(Partition.scala:1183)\n\tat kafka.cluster.Partition.$anonfun$maybeExpandIsr$2(Partition.scala:690)\n\tat kafka.cluster.Partition.maybeExpandIsr(Partition.scala:686)\n\tat kafka.cluster.Partition.updateFollowerFetchState(Partition.scala:609)\n\tat kafka.cluster.PartitionLockTest.$anonfun$updateFollowerFetchState$1(PartitionLockTest.scala:267)\n\tat kafka.cluster.PartitionLockTest.updateFollowerFetchState(PartitionLockTest.scala:257)\n\nThe mocked instance was not handling expandIsr, so updated the test to handle this. Also updated the test to use the offset from the batch to trigger expandIsr more often. With this change, the test may try to acquire write lock while appends are blocked on read lock, so separated out the test using write lock to make it safer.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-01-16T15:29:42Z", "url": "https://github.com/apache/kafka/pull/7973", "merged": true, "mergeCommit": {"oid": "eb8e2a8e3b3e2e7f5c097593faf2c651f92f2caf"}, "closed": true, "closedAt": "2020-01-17T19:21:02Z", "author": {"login": "rajinisivaram"}, "timelineItems": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb67pgIAH2gAyMzYzNzA1NDg2OmJhYmNlNTllODk0MDAzZDIxNDliOWZjMGNlMjEzMDcwOWQwN2NlMDY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABb7RPbfAFqTM0NDY5MDUxMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "babce59e894003d2149b9fc0ce2130709d07ce06", "author": {"user": {"login": "rajinisivaram", "name": "Rajini Sivaram"}}, "url": "https://github.com/apache/kafka/commit/babce59e894003d2149b9fc0ce2130709d07ce06", "committedDate": "2020-01-16T15:12:16Z", "message": "MINOR: Handle expandIsr in PartitionLockTest and ensure read threads not blocked on write"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0Mzk3OTYy", "url": "https://github.com/apache/kafka/pull/7973#pullrequestreview-344397962", "createdAt": "2020-01-17T07:19:13Z", "commit": {"oid": "babce59e894003d2149b9fc0ce2130709d07ce06"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwNzoxOToxNFrOFewk3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwNzozNDozMVrOFewygA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzc5NzQ2OQ==", "bodyText": "This is just ensuring there is one call to updateFollowers with every append?", "url": "https://github.com/apache/kafka/pull/7973#discussion_r367797469", "createdAt": "2020-01-17T07:19:14Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/cluster/PartitionLockTest.scala", "diffHunk": "@@ -111,62 +111,75 @@ class PartitionLockTest extends Logging {\n \n     val future = scheduleShrinkIsr(active, mockTimeSleepMs = 10000)\n     TestUtils.waitUntilTrue(() => shrinkIsrSemaphore.hasQueuedThreads, \"shrinkIsr not invoked\")\n-    concurrentProduceFetch(numProducers, numReplicaFetchers, numRecordsPerProducer, appendSemaphore, Some(shrinkIsrSemaphore))\n+    concurrentProduceFetchWithWriteLock(appendSemaphore, shrinkIsrSemaphore)\n     active.set(false)\n     future.get(15, TimeUnit.SECONDS)\n   }\n \n-  private def concurrentProduceFetch(numProducers: Int,\n-                                     numReplicaFetchers: Int,\n-                                     numRecords: Int,\n-                                     appendSemaphore: Semaphore,\n-                                     shrinkIsrSemaphore: Option[Semaphore]): Unit = {\n-    val followerQueues = (0 until numReplicaFetchers).map(_ => new ArrayBlockingQueue[MemoryRecords](2))\n+  /**\n+   * Perform concurrent appends and replica fetch requests that don't require write lock to\n+   * update follower state. Release sufficient append permits to complete all except one append.\n+   * Verify that follower state updates complete even though an append holding read lock is in progress.\n+   * Then release the permit for the final append and verify that all appends and follower updates complete.\n+   */\n+  private def concurrentProduceFetchWithReadLockOnly(appendSemaphore: Semaphore): Unit = {\n+    val appendFutures = scheduleAppends()\n+    val stateUpdateFutures = scheduleUpdateFollowers(numProducers * numRecordsPerProducer - 1)\n+\n+    appendSemaphore.release(numProducers * numRecordsPerProducer - 1)\n+    stateUpdateFutures.foreach(_.get(15, TimeUnit.SECONDS))\n+\n+    appendSemaphore.release(1)\n+    scheduleUpdateFollowers(1).foreach(_.get(15, TimeUnit.SECONDS))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "babce59e894003d2149b9fc0ce2130709d07ce06"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2Nzc5OTI2MQ==", "bodyText": "nit: would be nice to be consistent on the usage of class fields. This wouldn't work if the passed semaphore wasn't the class field since that is what we used when building the log. Maybe we can drop the parameter?", "url": "https://github.com/apache/kafka/pull/7973#discussion_r367799261", "createdAt": "2020-01-17T07:27:03Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/cluster/PartitionLockTest.scala", "diffHunk": "@@ -111,62 +111,75 @@ class PartitionLockTest extends Logging {\n \n     val future = scheduleShrinkIsr(active, mockTimeSleepMs = 10000)\n     TestUtils.waitUntilTrue(() => shrinkIsrSemaphore.hasQueuedThreads, \"shrinkIsr not invoked\")\n-    concurrentProduceFetch(numProducers, numReplicaFetchers, numRecordsPerProducer, appendSemaphore, Some(shrinkIsrSemaphore))\n+    concurrentProduceFetchWithWriteLock(appendSemaphore, shrinkIsrSemaphore)\n     active.set(false)\n     future.get(15, TimeUnit.SECONDS)\n   }\n \n-  private def concurrentProduceFetch(numProducers: Int,\n-                                     numReplicaFetchers: Int,\n-                                     numRecords: Int,\n-                                     appendSemaphore: Semaphore,\n-                                     shrinkIsrSemaphore: Option[Semaphore]): Unit = {\n-    val followerQueues = (0 until numReplicaFetchers).map(_ => new ArrayBlockingQueue[MemoryRecords](2))\n+  /**\n+   * Perform concurrent appends and replica fetch requests that don't require write lock to\n+   * update follower state. Release sufficient append permits to complete all except one append.\n+   * Verify that follower state updates complete even though an append holding read lock is in progress.\n+   * Then release the permit for the final append and verify that all appends and follower updates complete.\n+   */\n+  private def concurrentProduceFetchWithReadLockOnly(appendSemaphore: Semaphore): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "babce59e894003d2149b9fc0ce2130709d07ce06"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzgwMDk2MA==", "bodyText": "Just checking my understanding, but it seems there's no need to do this after the shrink semaphore is released. If we did it before, then we could assert that the append futures are blocked just like the update follower futures.", "url": "https://github.com/apache/kafka/pull/7973#discussion_r367800960", "createdAt": "2020-01-17T07:34:31Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/cluster/PartitionLockTest.scala", "diffHunk": "@@ -111,62 +111,75 @@ class PartitionLockTest extends Logging {\n \n     val future = scheduleShrinkIsr(active, mockTimeSleepMs = 10000)\n     TestUtils.waitUntilTrue(() => shrinkIsrSemaphore.hasQueuedThreads, \"shrinkIsr not invoked\")\n-    concurrentProduceFetch(numProducers, numReplicaFetchers, numRecordsPerProducer, appendSemaphore, Some(shrinkIsrSemaphore))\n+    concurrentProduceFetchWithWriteLock(appendSemaphore, shrinkIsrSemaphore)\n     active.set(false)\n     future.get(15, TimeUnit.SECONDS)\n   }\n \n-  private def concurrentProduceFetch(numProducers: Int,\n-                                     numReplicaFetchers: Int,\n-                                     numRecords: Int,\n-                                     appendSemaphore: Semaphore,\n-                                     shrinkIsrSemaphore: Option[Semaphore]): Unit = {\n-    val followerQueues = (0 until numReplicaFetchers).map(_ => new ArrayBlockingQueue[MemoryRecords](2))\n+  /**\n+   * Perform concurrent appends and replica fetch requests that don't require write lock to\n+   * update follower state. Release sufficient append permits to complete all except one append.\n+   * Verify that follower state updates complete even though an append holding read lock is in progress.\n+   * Then release the permit for the final append and verify that all appends and follower updates complete.\n+   */\n+  private def concurrentProduceFetchWithReadLockOnly(appendSemaphore: Semaphore): Unit = {\n+    val appendFutures = scheduleAppends()\n+    val stateUpdateFutures = scheduleUpdateFollowers(numProducers * numRecordsPerProducer - 1)\n+\n+    appendSemaphore.release(numProducers * numRecordsPerProducer - 1)\n+    stateUpdateFutures.foreach(_.get(15, TimeUnit.SECONDS))\n+\n+    appendSemaphore.release(1)\n+    scheduleUpdateFollowers(1).foreach(_.get(15, TimeUnit.SECONDS))\n+    appendFutures.foreach(_.get(15, TimeUnit.SECONDS))\n+  }\n+\n+  /**\n+   * Perform concurrent appends and replica fetch requests that may require write lock to update\n+   * follower state. Threads waiting for write lock to update follower state while append thread is\n+   * holding read lock will prevent other threads acquiring the read or write lock. So release sufficient\n+   * permits for all appends to complete before verifying state updates.\n+   */\n+  private def concurrentProduceFetchWithWriteLock(appendSemaphore: Semaphore,\n+                                                  shrinkIsrSemaphore: Semaphore): Unit = {\n+\n+    val appendFutures = scheduleAppends()\n+    val stateUpdateFutures = scheduleUpdateFollowers(numProducers * numRecordsPerProducer)\n+\n+    assertFalse(stateUpdateFutures.exists(_.isDone))\n+    shrinkIsrSemaphore.release()\n+    appendSemaphore.release(numProducers * numRecordsPerProducer)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "babce59e894003d2149b9fc0ce2130709d07ce06"}, "originalPosition": 88}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8edfe93407704a21ce7c286952e6dfe107c448a8", "author": {"user": {"login": "rajinisivaram", "name": "Rajini Sivaram"}}, "url": "https://github.com/apache/kafka/commit/8edfe93407704a21ce7c286952e6dfe107c448a8", "committedDate": "2020-01-17T09:59:30Z", "message": "Address review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0NjkwNTEx", "url": "https://github.com/apache/kafka/pull/7973#pullrequestreview-344690511", "createdAt": "2020-01-17T16:21:42Z", "commit": {"oid": "8edfe93407704a21ce7c286952e6dfe107c448a8"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2038, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}