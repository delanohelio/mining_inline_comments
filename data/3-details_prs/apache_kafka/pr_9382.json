{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk4NTY0MTky", "number": 9382, "title": "KAFKA-10554; Perform follower truncation based on diverging epochs in Fetch response", "bodyText": "For IBP 2.7 onwards, fetch responses include diverging epoch and offset in fetch responses if lastFetchedEpoch is provided in the fetch request. This PR uses that information for truncation and avoids the additional OffsetForLeaderEpoch requests in followers when lastFetchEpoch is known.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-10-06T13:53:04Z", "url": "https://github.com/apache/kafka/pull/9382", "merged": true, "mergeCommit": {"oid": "7ecc3a579a4b13e0cef4bd3129982ea3bc1a9341"}, "closed": true, "closedAt": "2020-12-03T10:12:07Z", "author": {"login": "rajinisivaram"}, "timelineItems": {"totalCount": 32, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdP6tN9gBqjM4NDYzMjY1ODM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdiYxJmAFqTU0MzM2OTMxNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAzNTMyNDUw", "url": "https://github.com/apache/kafka/pull/9382#pullrequestreview-503532450", "createdAt": "2020-10-07T05:22:03Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwNToyMjowM1rOHdi_Aw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwNToyMjowM1rOHdi_Aw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDc0MzkzOQ==", "bodyText": "NIT: Should this be the first check in the if () statement ?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r500743939", "createdAt": "2020-10-07T05:22:03Z", "author": {"login": "rite2nikhil"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -432,14 +455,22 @@ abstract class AbstractFetcherThread(name: String,\n       failedPartitions.removeAll(initialFetchStates.keySet)\n \n       initialFetchStates.forKeyValue { (tp, initialFetchState) =>\n-        // We can skip the truncation step iff the leader epoch matches the existing epoch\n+        // For IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+        // For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch\n         val currentState = partitionStates.stateValue(tp)\n-        val updatedState = if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.leaderEpoch) {\n+        val updatedState = if (initialFetchState.offset >= 0 && isTruncationOnFetchSupported && initialFetchState.lastFetchedEpoch.nonEmpty) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3OTE1NjUw", "url": "https://github.com/apache/kafka/pull/9382#pullrequestreview-507915650", "createdAt": "2020-10-14T00:25:49Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMDoyNTo0OVrOHg99Kg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMTozNDozMFrOHg_C4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzMTU2Mg==", "bodyText": "Wondering if it might be better not to change this type since it is used in contexts where lastFetchedEpoch is not relevant. Following the types through here, we first have use InitialFetchState in AbstractFetcherManager:\ndef addFetcherForPartitions(partitionAndOffsets: Map[TopicPartition, InitialFetchState])\nWe then convert to OffsetAndEpoch which gets passed down to AbstractFetcherThread:\ndef addPartitions(initialFetchStates: Map[TopicPartition, OffsetAndEpoch]): Set[TopicPartition]\nThen this gets converted to PartitionFetchState. I wonder if it's possible to skip the conversion through OffsetAndEpoch and use InitialFetchState consistently? Perhaps the only reason the current code doesn't do that is that InitialFetchState includes the broker end point which is not really relevant to the fetcher thread. Maybe that's not such a big deal?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504331562", "createdAt": "2020-10-14T00:25:49Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -813,8 +852,9 @@ case class OffsetTruncationState(offset: Long, truncationCompleted: Boolean) {\n   override def toString: String = \"offset:%d-truncationCompleted:%b\".format(offset, truncationCompleted)\n }\n \n-case class OffsetAndEpoch(offset: Long, leaderEpoch: Int) {\n+case class OffsetAndEpoch(offset: Long, leaderEpoch: Int, lastFetchedEpoch: Option[Int] = None) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzMjY2Mg==", "bodyText": "nit: unnecessary parenthesis", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504332662", "createdAt": "2020-10-14T00:29:56Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -432,14 +455,22 @@ abstract class AbstractFetcherThread(name: String,\n       failedPartitions.removeAll(initialFetchStates.keySet)\n \n       initialFetchStates.forKeyValue { (tp, initialFetchState) =>\n-        // We can skip the truncation step iff the leader epoch matches the existing epoch\n+        // For IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+        // For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch\n         val currentState = partitionStates.stateValue(tp)\n-        val updatedState = if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.leaderEpoch) {\n+        val updatedState = if (initialFetchState.offset >= 0 && isTruncationOnFetchSupported && initialFetchState.lastFetchedEpoch.nonEmpty) {\n+          if (currentState != null)\n+            currentState\n+          else\n+            PartitionFetchState(initialFetchState.offset, None, initialFetchState.leaderEpoch,\n+              state = Fetching, initialFetchState.lastFetchedEpoch)\n+        } else if (currentState != null && (currentState.currentLeaderEpoch == initialFetchState.leaderEpoch)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0MTI1Mg==", "bodyText": "Borderline overkill perhaps, but we could check if epochEndOffsets is non-empty before acquiring the lock", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504341252", "createdAt": "2020-10-14T01:02:37Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -225,6 +227,20 @@ abstract class AbstractFetcherThread(name: String,\n     }\n   }\n \n+  private def truncateOnFetchResponse(responseData: Map[TopicPartition, FetchData]): Unit = {\n+    val epochEndOffsets = responseData\n+      .filter { case (tp, fetchData) => fetchData.error == Errors.NONE && fetchData.divergingEpoch.isPresent }\n+      .map { case (tp, fetchData) =>\n+        val divergingEpoch = fetchData.divergingEpoch.get\n+        tp -> new EpochEndOffset(Errors.NONE, divergingEpoch.epoch, divergingEpoch.endOffset)\n+      }.toMap\n+    inLock(partitionMapLock) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0Mzc2NQ==", "bodyText": "Rather than doing an additional pass over the response partitions, would it be reasonable to build epochEndOffsets inline with the other error handling in processFetchRequest?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504343765", "createdAt": "2020-10-14T01:12:19Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -225,6 +227,20 @@ abstract class AbstractFetcherThread(name: String,\n     }\n   }\n \n+  private def truncateOnFetchResponse(responseData: Map[TopicPartition, FetchData]): Unit = {\n+    val epochEndOffsets = responseData", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0NTY1OA==", "bodyText": "Can be done separately, but it would be nice to figure out how to move this logic into ReplicaAlterLogDirManager since this comment seems to only make sense if we assume this is the log dir fetcher and reconciliation with the leader has already completed.\nIn fact, I wonder if it is possible to get rid of this code entirely. If the log dir fetcher is also tracking lastFetchedEpoch, then we could rely on detecting truncation dynamically through ReplicaManager.fetchMessages instead of the current somewhat clumsy coordination with the replica fetcher.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504345658", "createdAt": "2020-10-14T01:19:43Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -408,9 +428,12 @@ abstract class AbstractFetcherThread(name: String,\n   def markPartitionsForTruncation(topicPartition: TopicPartition, truncationOffset: Long): Unit = {\n     partitionMapLock.lockInterruptibly()\n     try {\n+      // It is safe to reset `lastFetchedEpoch` here since we don't expect diverging offsets", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0Nzk5MA==", "bodyText": "Below we only use currentState if the current epoch matches the initial epoch. Why is it safe to skip that check here?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504347990", "createdAt": "2020-10-14T01:28:47Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -432,14 +455,22 @@ abstract class AbstractFetcherThread(name: String,\n       failedPartitions.removeAll(initialFetchStates.keySet)\n \n       initialFetchStates.forKeyValue { (tp, initialFetchState) =>\n-        // We can skip the truncation step iff the leader epoch matches the existing epoch\n+        // For IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+        // For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch\n         val currentState = partitionStates.stateValue(tp)\n-        val updatedState = if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.leaderEpoch) {\n+        val updatedState = if (initialFetchState.offset >= 0 && isTruncationOnFetchSupported && initialFetchState.lastFetchedEpoch.nonEmpty) {\n+          if (currentState != null)\n+            currentState", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0OTQwOQ==", "bodyText": "Good catch here and in FetchSession. Do you think we should consider doing these fixes separately so that we can get them into 2.7? Otherwise it might be difficult to tie this behavior to the 2.7 IBP.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504349409", "createdAt": "2020-10-14T01:34:30Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/DelayedFetch.scala", "diffHunk": "@@ -77,6 +78,7 @@ class DelayedFetch(delayMs: Long,\n    * Case E: This broker is the leader, but the requested epoch is now fenced\n    * Case F: The fetch offset locates not on the last segment of the log\n    * Case G: The accumulated bytes from all the fetching partitions exceeds the minimum bytes\n+   * Case H: A diverging epoch was found, return response to trigger truncation", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "bee67c9057f7f5bce755cc4f8d68c9502e65a397", "author": {"user": {"login": "rajinisivaram", "name": "Rajini Sivaram"}}, "url": "https://github.com/apache/kafka/commit/bee67c9057f7f5bce755cc4f8d68c9502e65a397", "committedDate": "2020-10-14T14:41:15Z", "message": "Address review comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bee67c9057f7f5bce755cc4f8d68c9502e65a397", "author": {"user": {"login": "rajinisivaram", "name": "Rajini Sivaram"}}, "url": "https://github.com/apache/kafka/commit/bee67c9057f7f5bce755cc4f8d68c9502e65a397", "committedDate": "2020-10-14T14:41:15Z", "message": "Address review comments"}, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEzOTk1ODg0", "url": "https://github.com/apache/kafka/pull/9382#pullrequestreview-513995884", "createdAt": "2020-10-21T17:23:15Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNzoyMzoxNVrOHl3njg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNzo1MDozNVrOHl5quQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ3MDYwNg==", "bodyText": "Is it not possible that the InitialFetchState has a bump to the current leader epoch? We will still need the latest epoch in order to continue fetching.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r509470606", "createdAt": "2020-10-21T17:23:15Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -426,21 +454,42 @@ abstract class AbstractFetcherThread(name: String,\n     warn(s\"Partition $topicPartition marked as failed\")\n   }\n \n-  def addPartitions(initialFetchStates: Map[TopicPartition, OffsetAndEpoch]): Set[TopicPartition] = {\n+  /**\n+   * Returns initial partition fetch state based on current state and the provided `initialFetchState`.\n+   * From IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+   * For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch.\n+   */\n+  private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\n+    if (isTruncationOnFetchSupported && initialFetchState.initOffset >= 0 && initialFetchState.lastFetchedEpoch.nonEmpty) {\n+      if (currentState == null) {\n+        return PartitionFetchState(initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\n+          state = Fetching, initialFetchState.lastFetchedEpoch)\n+      }\n+      // If we are in `Fetching` state can continue to fetch regardless of current leader epoch and truncate\n+      // if necessary based on diverging epochs returned by the leader. If we are currently in Truncating state,\n+      // fall through and handle based on current epoch.\n+      if (currentState.state == Fetching) {\n+        return currentState", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ3Mjg1NA==", "bodyText": "Do we need to adjust this? I think we want to remain in the Fetching state if truncation detection is through Fetch.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r509472854", "createdAt": "2020-10-21T17:25:30Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -461,8 +510,9 @@ abstract class AbstractFetcherThread(name: String,\n         val maybeTruncationComplete = fetchOffsets.get(topicPartition) match {\n           case Some(offsetTruncationState) =>\n             val state = if (offsetTruncationState.truncationCompleted) Fetching else Truncating", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ4MjMzMg==", "bodyText": "This is a little unclear to me. I guess it is safe to reset lastFetchedEpoch as long as we reinitialize it after the next leader change. On the other hand, it seems safer to always retain the value.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r509482332", "createdAt": "2020-10-21T17:33:14Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -461,8 +510,9 @@ abstract class AbstractFetcherThread(name: String,\n         val maybeTruncationComplete = fetchOffsets.get(topicPartition) match {\n           case Some(offsetTruncationState) =>\n             val state = if (offsetTruncationState.truncationCompleted) Fetching else Truncating\n+            // Resetting `lastFetchedEpoch` since we are truncating and don't expect diverging epoch in the next fetch", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ4NDY3NA==", "bodyText": "Again it seems safe to keep lastFetchedEpoch in sync with the local log. If we have done a full truncation above, then lastFetchedEpoch will be None, but otherwise it seems like we should set it.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r509484674", "createdAt": "2020-10-21T17:35:34Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -629,7 +680,9 @@ abstract class AbstractFetcherThread(name: String,\n \n       val initialLag = leaderEndOffset - offsetToFetch\n       fetcherLagStats.getAndMaybePut(topicPartition).lag = initialLag\n-      PartitionFetchState(offsetToFetch, Some(initialLag), currentLeaderEpoch, state = Fetching)\n+      // We don't expect diverging epochs from the next fetch request, so resetting `lastFetchedEpoch`", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ5MzE4Nw==", "bodyText": "Do we need to initialize lastFetchedEpoch? It seems like the log may not be empty at this point.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r509493187", "createdAt": "2020-10-21T17:43:55Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -770,7 +770,7 @@ class ReplicaManager(val config: KafkaConfig,\n             logManager.abortAndPauseCleaning(topicPartition)\n \n             val initialFetchState = InitialFetchState(BrokerEndPoint(config.brokerId, \"localhost\", -1),\n-              partition.getLeaderEpoch, futureLog.highWatermark)\n+              partition.getLeaderEpoch, futureLog.highWatermark, lastFetchedEpoch = None)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTUwNDE4NQ==", "bodyText": "This doesn't seem right. The last fetched epoch is supposed to represent the epoch of the last fetched batch. The fetcher could be fetching the data from an older epoch here.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r509504185", "createdAt": "2020-10-21T17:50:35Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -341,11 +352,18 @@ abstract class AbstractFetcherThread(name: String,\n                       // ReplicaDirAlterThread may have removed topicPartition from the partitionStates after processing the partition data\n                       if (validBytes > 0 && partitionStates.contains(topicPartition)) {\n                         // Update partitionStates only if there is no exception during processPartitionData\n-                        val newFetchState = PartitionFetchState(nextOffset, Some(lag), currentFetchState.currentLeaderEpoch, state = Fetching)\n+                        val newFetchState = PartitionFetchState(nextOffset, Some(lag),\n+                          currentFetchState.currentLeaderEpoch, state = Fetching,\n+                          Some(currentFetchState.currentLeaderEpoch))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE3Mjk0MDUx", "url": "https://github.com/apache/kafka/pull/9382#pullrequestreview-517294051", "createdAt": "2020-10-27T01:55:33Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwMTo1NTozM1rOHoolew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxOToyNToyOVrOHpNDaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM3MDA0Mw==", "bodyText": "nit: not sure how much it matters, but maybe we can avoid the extra garbage and just use an integer until we're ready to build the result?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r512370043", "createdAt": "2020-10-27T01:55:33Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1388,6 +1390,7 @@ class Log(@volatile private var _dir: File,\n     var validBytesCount = 0\n     var firstOffset: Option[Long] = None\n     var lastOffset = -1L\n+    var lastLeaderEpoch: Option[Int] = None", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjg3MDEzMA==", "bodyText": "Sounds fair.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r512870130", "createdAt": "2020-10-27T17:05:18Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -770,7 +770,7 @@ class ReplicaManager(val config: KafkaConfig,\n             logManager.abortAndPauseCleaning(topicPartition)\n \n             val initialFetchState = InitialFetchState(BrokerEndPoint(config.brokerId, \"localhost\", -1),\n-              partition.getLeaderEpoch, futureLog.highWatermark)\n+              partition.getLeaderEpoch, futureLog.highWatermark, lastFetchedEpoch = None)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ5MzE4Nw=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjg4MzY2Nw==", "bodyText": "nit: line below is misaligned", "url": "https://github.com/apache/kafka/pull/9382#discussion_r512883667", "createdAt": "2020-10-27T17:24:32Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1667,8 +1667,9 @@ class ReplicaManager(val config: KafkaConfig,\n         val partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map { partition =>\n           val leader = metadataCache.getAliveBrokers.find(_.id == partition.leaderReplicaIdOpt.get).get\n             .brokerEndPoint(config.interBrokerListenerName)\n-          val fetchOffset = partition.localLogOrException.highWatermark\n-          partition.topicPartition -> InitialFetchState(leader, partition.getLeaderEpoch, fetchOffset)\n+          val log = partition.localLogOrException\n+          val (fetchOffset, lastFetchedEpoch) = initialFetchOffsetAndEpoch(log)\n+          partition.topicPartition -> InitialFetchState(leader, partition.getLeaderEpoch, fetchOffset, lastFetchedEpoch)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjg4NjY5NQ==", "bodyText": "I am wondering in what situation we would find currentState non-null here. The current logic in ReplicaManager.makeFollowers always calls removeFetcherForPartitions before adding the partition back. The reason I ask is that I wasn't sure we should be taking the last fetched epoch from the initial state or if we should keep the current one. It seems like the latter might be more current?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r512886695", "createdAt": "2020-10-27T17:27:55Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -426,21 +451,34 @@ abstract class AbstractFetcherThread(name: String,\n     warn(s\"Partition $topicPartition marked as failed\")\n   }\n \n-  def addPartitions(initialFetchStates: Map[TopicPartition, OffsetAndEpoch]): Set[TopicPartition] = {\n+  /**\n+   * Returns initial partition fetch state based on current state and the provided `initialFetchState`.\n+   * From IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+   * For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch.\n+   */\n+  private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\n+    if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) {\n+      currentState\n+    } else if (isTruncationOnFetchSupported && initialFetchState.initOffset >= 0 && initialFetchState.lastFetchedEpoch.nonEmpty &&\n+              (currentState == null || currentState.state == Fetching)) {\n+      PartitionFetchState(initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\n+          state = Fetching, initialFetchState.lastFetchedEpoch)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk2NDgxOA==", "bodyText": "It's not clear to me why we set maySkipTruncation to false here. If the truncation is not complete, wouldn't that put us in the Truncating state?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r512964818", "createdAt": "2020-10-27T19:22:24Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -221,7 +223,15 @@ abstract class AbstractFetcherThread(name: String,\n \n       val ResultWithPartitions(fetchOffsets, partitionsWithError) = maybeTruncateToEpochEndOffsets(epochEndOffsets, latestEpochsForPartitions)\n       handlePartitionsWithErrors(partitionsWithError, \"truncateToEpochEndOffsets\")\n-      updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets)\n+      updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets, isTruncationOnFetchSupported)\n+    }\n+  }\n+\n+  private def truncateOnFetchResponse(epochEndOffsets: Map[TopicPartition, EpochEndOffset]): Unit = {\n+    inLock(partitionMapLock) {\n+      val ResultWithPartitions(fetchOffsets, partitionsWithError) = maybeTruncateToEpochEndOffsets(epochEndOffsets, Map.empty)\n+      handlePartitionsWithErrors(partitionsWithError, \"truncateOnFetchResponse\")\n+      updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets, maySkipTruncation = false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk2NzUyOQ==", "bodyText": "I'm a little uncertain about this case. If we have truncated to an earlier offset, wouldn't we also need to reset last fetched epoch? I am thinking we should remove this check and modify the first one:\nval (state, lastFetchedEpoch) = if (maySkipTruncation || offsetTruncationState.truncationCompleted)\n  (Fetching, latestEpoch(topicPartition))\nI might be missing something though.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r512967529", "createdAt": "2020-10-27T19:25:29Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -454,15 +492,23 @@ abstract class AbstractFetcherThread(name: String,\n     * truncation completed if their offsetTruncationState indicates truncation completed\n     *\n     * @param fetchOffsets the partitions to update fetch offset and maybe mark truncation complete\n+    * @param maySkipTruncation true if we can stay in Fetching mode and perform truncation later based on\n+   *                           diverging epochs from fetch responses.\n     */\n-  private def updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets: Map[TopicPartition, OffsetTruncationState]): Unit = {\n+  private def updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets: Map[TopicPartition, OffsetTruncationState],\n+                                                              maySkipTruncation: Boolean): Unit = {\n     val newStates: Map[TopicPartition, PartitionFetchState] = partitionStates.partitionStateMap.asScala\n       .map { case (topicPartition, currentFetchState) =>\n         val maybeTruncationComplete = fetchOffsets.get(topicPartition) match {\n           case Some(offsetTruncationState) =>\n-            val state = if (offsetTruncationState.truncationCompleted) Fetching else Truncating\n+            val (state, lastFetchedEpoch) = if (offsetTruncationState.truncationCompleted)\n+              (Fetching, latestEpoch(topicPartition))\n+            else if (maySkipTruncation && currentFetchState.lastFetchedEpoch.nonEmpty)\n+              (Fetching, currentFetchState.lastFetchedEpoch)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 160}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyMjAzODc0", "url": "https://github.com/apache/kafka/pull/9382#pullrequestreview-542203874", "createdAt": "2020-12-01T18:57:58Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxODo1Nzo1OVrOH87VUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxOTo0NTo1OVrOH89C9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY0ODcyMg==", "bodyText": "This check is a still a little hard to follow. I think we expect that if initOffset is negative, then lastFetchedEpoch will be empty and we will hit the fetchOffsetAndTruncate case below. Is that right? On the other hand, if lastFetchedEpoch is empty, then initOffset could still be non-negative if we have an old message format, which means we need to enter Truncating so that we can truncate to the high watermark.\nOne case that is not so clear is when currentState is non-null. Then we will enter the Truncating state below regardless whether isTruncationOnFetchSupported is set or not. Is that what we want?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r533648722", "createdAt": "2020-12-01T18:57:59Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -426,21 +451,34 @@ abstract class AbstractFetcherThread(name: String,\n     warn(s\"Partition $topicPartition marked as failed\")\n   }\n \n-  def addPartitions(initialFetchStates: Map[TopicPartition, OffsetAndEpoch]): Set[TopicPartition] = {\n+  /**\n+   * Returns initial partition fetch state based on current state and the provided `initialFetchState`.\n+   * From IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+   * For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch.\n+   */\n+  private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\n+    if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) {\n+      currentState\n+    } else if (isTruncationOnFetchSupported && initialFetchState.initOffset >= 0 && initialFetchState.lastFetchedEpoch.nonEmpty &&\n+              (currentState == null || currentState.state == Fetching)) {\n+      PartitionFetchState(initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\n+          state = Fetching, initialFetchState.lastFetchedEpoch)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjg4NjY5NQ=="}, "originalCommit": null, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY3Mjg5Mg==", "bodyText": "This is probably ok. I guess an alternative would be to not take the initial last fetched epoch from InitialFetchState, but instead use latestEpoch.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r533672892", "createdAt": "2020-12-01T19:39:13Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -669,11 +714,18 @@ abstract class AbstractFetcherThread(name: String,\n     Option(partitionStates.stateValue(topicPartition))\n   }\n \n+  /**\n+   * Returns current fetch state for each partition assigned to this thread. This is used to reassign\n+   * partitions when thread pool is resized. We return `lastFetchedEpoch=None` to ensure we go through", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY3Njc5MA==", "bodyText": "nit: I don't think we need this. We can override isTruncationOnFetchSupported with a val", "url": "https://github.com/apache/kafka/pull/9382#discussion_r533676790", "createdAt": "2020-12-01T19:45:59Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/ReplicaFetcherThread.scala", "diffHunk": "@@ -102,6 +103,7 @@ class ReplicaFetcherThread(name: String,\n   private val maxBytes = brokerConfig.replicaFetchResponseMaxBytes\n   private val fetchSize = brokerConfig.replicaFetchMaxBytes\n   private val brokerSupportsLeaderEpochRequest = brokerConfig.interBrokerProtocolVersion >= KAFKA_0_11_0_IV2\n+  private val brokerSupportsTruncationOnFetch = ApiVersion.isTruncationOnFetchSupported(brokerConfig.interBrokerProtocolVersion)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cbf877f845dd975a9d6a57e3faeff10cde9eda94", "author": {"user": {"login": "rajinisivaram", "name": "Rajini Sivaram"}}, "url": "https://github.com/apache/kafka/commit/cbf877f845dd975a9d6a57e3faeff10cde9eda94", "committedDate": "2020-12-02T11:55:59Z", "message": "KAFKA-10554; Perform follower truncation based on diverging epochs in Fetch response"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "88a64810464adb5387cc1ebb3e8cac664e81b295", "author": {"user": {"login": "rajinisivaram", "name": "Rajini Sivaram"}}, "url": "https://github.com/apache/kafka/commit/88a64810464adb5387cc1ebb3e8cac664e81b295", "committedDate": "2020-12-02T11:56:00Z", "message": "Address review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bc4a933f6b7d4ddf557b6bf1b9660c8e527c6f0b", "author": {"user": {"login": "rajinisivaram", "name": "Rajini Sivaram"}}, "url": "https://github.com/apache/kafka/commit/bc4a933f6b7d4ddf557b6bf1b9660c8e527c6f0b", "committedDate": "2020-12-02T11:56:00Z", "message": "Fix initial offsets for ReplicaAlterLogDirs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "800f11726d6f2056a10ff6ddf3e396fd9395c21d", "author": {"user": {"login": "rajinisivaram", "name": "Rajini Sivaram"}}, "url": "https://github.com/apache/kafka/commit/800f11726d6f2056a10ff6ddf3e396fd9395c21d", "committedDate": "2020-12-02T11:56:00Z", "message": "Fix resizeThreadPool"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1fa0340795256d5862f79a958fa98a837447fc47", "author": {"user": {"login": "rajinisivaram", "name": "Rajini Sivaram"}}, "url": "https://github.com/apache/kafka/commit/1fa0340795256d5862f79a958fa98a837447fc47", "committedDate": "2020-12-02T11:56:01Z", "message": "Address review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "16dc12548c3261d03b6b2e02ff7010dcac331c07", "author": {"user": {"login": "rajinisivaram", "name": "Rajini Sivaram"}}, "url": "https://github.com/apache/kafka/commit/16dc12548c3261d03b6b2e02ff7010dcac331c07", "committedDate": "2020-12-02T11:56:01Z", "message": "Revert log.dir fetcher changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "42c536ebd17fb4feb8d3bd5aadc3acec0ffdc8b9", "author": {"user": {"login": "rajinisivaram", "name": "Rajini Sivaram"}}, "url": "https://github.com/apache/kafka/commit/42c536ebd17fb4feb8d3bd5aadc3acec0ffdc8b9", "committedDate": "2020-12-02T11:56:01Z", "message": "Address review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3a13fa8bad0d1a2697475513df5c6370d4572a8d", "author": {"user": {"login": "rajinisivaram", "name": "Rajini Sivaram"}}, "url": "https://github.com/apache/kafka/commit/3a13fa8bad0d1a2697475513df5c6370d4572a8d", "committedDate": "2020-12-02T11:56:01Z", "message": "Address review comment"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "bf976df9fb3201c9f996ec71f8b325fc4a13c05c", "author": {"user": {"login": "rajinisivaram", "name": "Rajini Sivaram"}}, "url": "https://github.com/apache/kafka/commit/bf976df9fb3201c9f996ec71f8b325fc4a13c05c", "committedDate": "2020-12-02T17:02:37Z", "message": "Address review comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "bf976df9fb3201c9f996ec71f8b325fc4a13c05c", "author": {"user": {"login": "rajinisivaram", "name": "Rajini Sivaram"}}, "url": "https://github.com/apache/kafka/commit/bf976df9fb3201c9f996ec71f8b325fc4a13c05c", "committedDate": "2020-12-02T17:02:37Z", "message": "Address review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQzMTY1NzA2", "url": "https://github.com/apache/kafka/pull/9382#pullrequestreview-543165706", "createdAt": "2020-12-02T19:25:16Z", "commit": {"oid": "bf976df9fb3201c9f996ec71f8b325fc4a13c05c"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxOToyNToxNlrOH9qrBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQyMDoxODo0NVrOH9siaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQyNDMyNQ==", "bodyText": "Hmm.. Do we actually return Some(EpochEndOffset.UNDEFINED_EPOCH) from latestEpoch? That seems surprising.\nMight be worth a comment here that we still go through the Truncating state here when the message format is old.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r534424325", "createdAt": "2020-12-02T19:25:16Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -426,21 +451,35 @@ abstract class AbstractFetcherThread(name: String,\n     warn(s\"Partition $topicPartition marked as failed\")\n   }\n \n-  def addPartitions(initialFetchStates: Map[TopicPartition, OffsetAndEpoch]): Set[TopicPartition] = {\n+  /**\n+   * Returns initial partition fetch state based on current state and the provided `initialFetchState`.\n+   * From IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+   * For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch.\n+   */\n+  private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\n+    if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) {\n+      currentState\n+    } else if (initialFetchState.initOffset < 0) {\n+      fetchOffsetAndTruncate(tp, initialFetchState.currentLeaderEpoch)\n+    } else if (isTruncationOnFetchSupported) {\n+      val lastFetchedEpoch = latestEpoch(tp)\n+      val state = if (lastFetchedEpoch.exists(_ != EpochEndOffset.UNDEFINED_EPOCH)) Fetching else Truncating", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf976df9fb3201c9f996ec71f8b325fc4a13c05c"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQzMjI1OQ==", "bodyText": "This reads a bit odd following removeAllPartitions. I guess what we get from removeFetcherForPartitions is the clearing of failedPartitions and de-registration from fetcherLagStats. Not super important, but wonder if it's worth trying to consolidate a little. Maybe removeFetcherForPartitions could return the initial fetch offsets or something.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r534432259", "createdAt": "2020-12-02T19:38:47Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherManager.scala", "diffHunk": "@@ -64,8 +64,8 @@ abstract class AbstractFetcherManager[T <: AbstractFetcherThread](val name: Stri\n   def resizeThreadPool(newSize: Int): Unit = {\n     def migratePartitions(newSize: Int): Unit = {\n       fetcherThreadMap.forKeyValue { (id, thread) =>\n-        val removedPartitions = thread.partitionsAndOffsets\n-        removeFetcherForPartitions(removedPartitions.keySet)\n+        val removedPartitions = thread.removeAllPartitions()\n+        removeFetcherForPartitions(removedPartitions.keySet) // clear state for removed partitions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf976df9fb3201c9f996ec71f8b325fc4a13c05c"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ0NzM3OQ==", "bodyText": "I think this could be saved for a follow-up, but I wonder if we should consider similarly letting the initial offset be determined by the fetcher thread on initialization rather than being passed in. I find it confusing that we expect this to be the high watermark in some cases. It seems a little slippery the way we rely on it in AbstractFetcherThread.truncateToHighWatermark.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r534447379", "createdAt": "2020-12-02T20:05:04Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1691,6 +1692,18 @@ class ReplicaManager(val config: KafkaConfig,\n     partitionsToMakeFollower\n   }\n \n+  /**\n+   * From IBP 2.7 onwards, we send latest fetch epoch in the request and truncate if a\n+   * diverging epoch is returned in the response, avoiding the need for a separate\n+   * OffsetForLeaderEpoch request.\n+   */\n+  private def initialFetchOffset(log: Log): Long = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf976df9fb3201c9f996ec71f8b325fc4a13c05c"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ1NDg4OA==", "bodyText": "nit: needs update?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r534454888", "createdAt": "2020-12-02T20:18:45Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala", "diffHunk": "@@ -453,6 +466,107 @@ class ReplicaFetcherThreadTest {\n                truncateToCapture.getValues.asScala.contains(101))\n   }\n \n+  @Test\n+  def shouldTruncateIfLeaderRepliesWithDivergingEpochNotKnownToFollower(): Unit = {\n+\n+    // Create a capture to track what partitions/offsets are truncated\n+    val truncateToCapture: Capture[Long] = newCapture(CaptureType.ALL)\n+\n+    val config = KafkaConfig.fromProps(TestUtils.createBrokerConfig(1, \"localhost:1234\"))\n+\n+    // Setup all dependencies\n+    val quota: ReplicationQuotaManager = createNiceMock(classOf[ReplicationQuotaManager])\n+    val logManager: LogManager = createMock(classOf[LogManager])\n+    val replicaAlterLogDirsManager: ReplicaAlterLogDirsManager = createMock(classOf[ReplicaAlterLogDirsManager])\n+    val log: Log = createNiceMock(classOf[Log])\n+    val partition: Partition = createNiceMock(classOf[Partition])\n+    val replicaManager: ReplicaManager = createMock(classOf[ReplicaManager])\n+\n+    val initialLEO = 200\n+    var latestLogEpoch: Option[Int] = Some(5)\n+\n+    // Stubs\n+    expect(partition.truncateTo(capture(truncateToCapture), anyBoolean())).anyTimes()\n+    expect(partition.localLogOrException).andReturn(log).anyTimes()\n+    expect(log.highWatermark).andReturn(115).anyTimes()\n+    expect(log.latestEpoch).andAnswer(() => latestLogEpoch).anyTimes()\n+    expect(log.endOffsetForEpoch(4)).andReturn(Some(OffsetAndEpoch(149, 4))).anyTimes()\n+    expect(log.endOffsetForEpoch(3)).andReturn(Some(OffsetAndEpoch(129, 2))).anyTimes()\n+    expect(log.endOffsetForEpoch(2)).andReturn(Some(OffsetAndEpoch(119, 1))).anyTimes()\n+    expect(log.logEndOffset).andReturn(initialLEO).anyTimes()\n+    expect(replicaManager.localLogOrException(anyObject(classOf[TopicPartition]))).andReturn(log).anyTimes()\n+    expect(replicaManager.logManager).andReturn(logManager).anyTimes()\n+    expect(replicaManager.replicaAlterLogDirsManager).andReturn(replicaAlterLogDirsManager).anyTimes()\n+    expect(replicaManager.brokerTopicStats).andReturn(mock(classOf[BrokerTopicStats]))\n+    stub(partition, replicaManager, log)\n+\n+    replay(replicaManager, logManager, quota, partition, log)\n+\n+    // Create the fetcher thread\n+    val mockNetwork = new ReplicaFetcherMockBlockingSend(Collections.emptyMap(), brokerEndPoint, new SystemTime())\n+    val thread = new ReplicaFetcherThread(\"bob\", 0, brokerEndPoint, config, failedPartitions, replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork)) {\n+      override def processPartitionData(topicPartition: TopicPartition, fetchOffset: Long, partitionData: FetchData): Option[LogAppendInfo] = None\n+    }\n+    thread.addPartitions(Map(t1p0 -> initialFetchState(initialLEO), t1p1 -> initialFetchState(initialLEO)))\n+    val partitions = Set(t1p0, t1p1)\n+\n+    // Loop 1 -- both topic partitions skip epoch fetch and send fetch request since\n+    // lastFetchedEpoch is set in initial fetch state.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf976df9fb3201c9f996ec71f8b325fc4a13c05c"}, "originalPosition": 216}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c88e3a79b4b5db49f8b81bd236314b3554454cd9", "author": {"user": {"login": "rajinisivaram", "name": "Rajini Sivaram"}}, "url": "https://github.com/apache/kafka/commit/c88e3a79b4b5db49f8b81bd236314b3554454cd9", "committedDate": "2020-12-02T23:52:49Z", "message": "Address review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "be8b884ae2034c4ce6aef17d29a9a3f3429abe82", "author": {"user": {"login": "hachikuji", "name": "Jason Gustafson"}}, "url": "https://github.com/apache/kafka/commit/be8b884ae2034c4ce6aef17d29a9a3f3429abe82", "committedDate": "2020-12-03T00:43:25Z", "message": "Trivial style tweaks"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQzMzY5MzE3", "url": "https://github.com/apache/kafka/pull/9382#pullrequestreview-543369317", "createdAt": "2020-12-03T01:22:04Z", "commit": {"oid": "c88e3a79b4b5db49f8b81bd236314b3554454cd9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 453, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}