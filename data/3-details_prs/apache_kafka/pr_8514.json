{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA1NTgxMDM1", "number": 8514, "title": "MINOR: Further reduce runtime for metrics integration tests", "bodyText": "In both RocksDBMetrics and Metrics integration tests, we do not need to wait for consumer to consume records from output topics since the sensors / metrics are registered upon task creation.\n\n\nMerged the two test cases of RocksDB with one app that creates two state stores (non-segmented and segmented).\n\n\nWith these two changes, local runtime of these two tests reduced from 2min+ and 3min+ to under a minute.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-04-19T00:24:01Z", "url": "https://github.com/apache/kafka/pull/8514", "merged": true, "mergeCommit": {"oid": "fcf45e1fac88238b1d3dbcfa1f324674939706f3"}, "closed": true, "closedAt": "2020-04-20T18:00:59Z", "author": {"login": "guozhangwang"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcY_K4RAH2gAyNDA1NTgxMDM1OjI3ZmVmZmYyNTA4NDQ0YzJkNThlOTRlM2I5MzY2NjBkNjJiMzQwZTA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcZduJ9gFqTM5NjM4MjE1OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "27fefff2508444c2d58e94e3b936660d62b340e0", "author": {"user": {"login": "guozhangwang", "name": "Guozhang Wang"}}, "url": "https://github.com/apache/kafka/commit/27fefff2508444c2d58e94e3b936660d62b340e0", "committedDate": "2020-04-19T00:16:10Z", "message": "reduce runtime further for metrics integration tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2MDQ5MDE3", "url": "https://github.com/apache/kafka/pull/8514#pullrequestreview-396049017", "createdAt": "2020-04-19T18:39:20Z", "commit": {"oid": "27fefff2508444c2d58e94e3b936660d62b340e0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOVQxODozOToyMFrOGH7S9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOVQxODozOToyMFrOGH7S9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDk2NDcyNw==", "bodyText": "Why do we write into STREAM_INPUT_TWO with 3 calls instead of just one call passing in all 3 records at once?", "url": "https://github.com/apache/kafka/pull/8514#discussion_r410964727", "createdAt": "2020-04-19T18:39:20Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java", "diffHunk": "@@ -214,70 +176,55 @@ private StreamsBuilder builderForSegmentedStateStore() {\n                     .withRetention(WINDOW_SIZE))\n             .toStream()\n             .map((key, value) -> KeyValue.pair(value, value))\n-            .to(STREAM_OUTPUT, Produced.with(Serdes.Long(), Serdes.Long()));\n+            .to(STREAM_OUTPUT_TWO, Produced.with(Serdes.Long(), Serdes.Long()));\n         return builder;\n     }\n \n     private void cleanUpStateRunVerifyAndClose(final StreamsBuilder builder,\n                                                final Properties streamsConfiguration,\n-                                               final Class outputKeyDeserializer,\n-                                               final Class outputValueDeserializer,\n-                                               final MetricsVerifier metricsVerifier,\n-                                               final String metricsScope) throws Exception {\n+                                               final MetricsVerifier metricsVerifier) throws Exception {\n         final KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), streamsConfiguration);\n         kafkaStreams.cleanUp();\n         produceRecords();\n \n         StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams, TIMEOUT);\n \n-        IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(\n-            TestUtils.consumerConfig(\n-                CLUSTER.bootstrapServers(),\n-                \"consumerApp\",\n-                outputKeyDeserializer,\n-                outputValueDeserializer,\n-                new Properties()\n-            ),\n-            STREAM_OUTPUT,\n-            1\n-        );\n-        metricsVerifier.verify(kafkaStreams, metricsScope);\n+        metricsVerifier.verify(kafkaStreams, \"rocksdb-state-id\");\n+        metricsVerifier.verify(kafkaStreams, \"rocksdb-window-state-id\");\n         kafkaStreams.close();\n     }\n \n     private void produceRecords() throws Exception {\n         final MockTime mockTime = new MockTime(WINDOW_SIZE.toMillis());\n+        final Properties prop = TestUtils.producerConfig(\n+            CLUSTER.bootstrapServers(),\n+            IntegerSerializer.class,\n+            StringSerializer.class,\n+            new Properties()\n+        );\n+        // non-segmented store do not need records with different timestamps\n         IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n-            STREAM_INPUT,\n-            Collections.singletonList(new KeyValue<>(1, \"A\")),\n-            TestUtils.producerConfig(\n-                CLUSTER.bootstrapServers(),\n-                IntegerSerializer.class,\n-                StringSerializer.class,\n-                new Properties()\n-            ),\n+            STREAM_INPUT_ONE,\n+            Utils.mkSet(new KeyValue<>(1, \"A\"), new KeyValue<>(1, \"B\"), new KeyValue<>(1, \"C\")),\n+            prop,\n             mockTime.milliseconds()\n         );\n         IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n-            STREAM_INPUT,\n-            Collections.singletonList(new KeyValue<>(1, \"B\")),\n-            TestUtils.producerConfig(\n-                CLUSTER.bootstrapServers(),\n-                IntegerSerializer.class,\n-                StringSerializer.class,\n-                new Properties()\n-            ),\n+            STREAM_INPUT_TWO,\n+            Collections.singleton(new KeyValue<>(1, \"A\")),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27fefff2508444c2d58e94e3b936660d62b340e0"}, "originalPosition": 204}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2MDQ5MTAx", "url": "https://github.com/apache/kafka/pull/8514#pullrequestreview-396049101", "createdAt": "2020-04-19T18:40:12Z", "commit": {"oid": "27fefff2508444c2d58e94e3b936660d62b340e0"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "50765792c168aca3bdb2a604f6f0172794998e7b", "author": {"user": {"login": "guozhangwang", "name": "Guozhang Wang"}}, "url": "https://github.com/apache/kafka/commit/50765792c168aca3bdb2a604f6f0172794998e7b", "committedDate": "2020-04-19T19:38:03Z", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into KMinor-reduce-metrics-integration-runtime"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "897ff76770290ecca0b2344a7ffb9a975f0309e8", "author": {"user": {"login": "guozhangwang", "name": "Guozhang Wang"}}, "url": "https://github.com/apache/kafka/commit/897ff76770290ecca0b2344a7ffb9a975f0309e8", "committedDate": "2020-04-19T19:39:46Z", "message": "fix checkstyle"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2MzcwMzI5", "url": "https://github.com/apache/kafka/pull/8514#pullrequestreview-396370329", "createdAt": "2020-04-20T11:32:39Z", "commit": {"oid": "897ff76770290ecca0b2344a7ffb9a975f0309e8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxMTozMjozOVrOGIQCBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQxMTozMjozOVrOGIQCBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTMwNDQ1Mg==", "bodyText": "Thank you! I forgot to delete this in my refactoring.", "url": "https://github.com/apache/kafka/pull/8514#discussion_r411304452", "createdAt": "2020-04-20T11:32:39Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/RocksDBMetricsIntegrationTest.java", "diffHunk": "@@ -319,18 +266,6 @@ private void checkMetricByName(final List<Metric> listMetric,\n         }\n     }\n \n-    private void verifyThatBytesWrittenTotalIncreases(final KafkaStreams kafkaStreams,\n-                                                      final String metricsScope) throws InterruptedException {\n-        final List<Metric> metric = getRocksDBMetrics(kafkaStreams, metricsScope).stream()\n-            .filter(m -> BYTES_WRITTEN_TOTAL.equals(m.metricName().name()))\n-            .collect(Collectors.toList());\n-        TestUtils.waitForCondition(\n-            () -> (double) metric.get(0).metricValue() > 0,\n-            TIMEOUT,\n-            () -> \"RocksDB metric bytes.written.total did not increase in \" + TIMEOUT + \" ms\"\n-        );\n-    }\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "897ff76770290ecca0b2344a7ffb9a975f0309e8"}, "originalPosition": 244}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2MzgyMTU4", "url": "https://github.com/apache/kafka/pull/8514#pullrequestreview-396382158", "createdAt": "2020-04-20T11:51:51Z", "commit": {"oid": "897ff76770290ecca0b2344a7ffb9a975f0309e8"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1575, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}