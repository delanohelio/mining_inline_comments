{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE0ODgzNDYx", "number": 9551, "title": "KAFKA-10679: Migrate upgrade changes from site to kafka/docs", "bodyText": "During the AK website upgrade, changes made to kafka-site weren't migrated back to kafka-docs.\nThis PR is an initial attempt at porting the changes to kafka/docs, but it does not include the streams changes. Those will come in a separate PR.\nFor the most part, the bulk of the changes in the PR are cosmetic.  Only the introduction.html has substantial changes, but it's a direct port from the live documentation.\nFor testing:\n\nI reviewed the PR diffs\nRendered the changes locally\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-11-03T17:19:53Z", "url": "https://github.com/apache/kafka/pull/9551", "merged": true, "mergeCommit": {"oid": "b4f00d7ef61ff5a53487a3930889c0b329250534"}, "closed": true, "closedAt": "2020-11-03T18:40:45Z", "author": {"login": "bbejeck"}, "timelineItems": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdY8JbOgH2gAyNTE0ODgzNDYxOmE4NjJhYjlkMjc4MGVhMWVhYzNmNmUwYzJjOWU0NTk4ZDhhNjljODA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdY9m-iAH2gAyNTE0ODgzNDYxOjcwMDlkMmViYzkwZjg1ZmVhOGVlOGE2ZDRjODFiMGVmMjc4YjVmMjY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "a862ab9d2780ea1eac3f6e0c2c9e4598d8a69c80", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/apache/kafka/commit/a862ab9d2780ea1eac3f6e0c2c9e4598d8a69c80", "committedDate": "2020-11-03T16:56:01Z", "message": "MINOR: Add back section taken out by mistake"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyNzMyOTI2", "url": "https://github.com/apache/kafka/pull/9551#pullrequestreview-522732926", "createdAt": "2020-11-03T17:38:11Z", "commit": {"oid": "a862ab9d2780ea1eac3f6e0c2c9e4598d8a69c80"}, "state": "APPROVED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNzozODoxMVrOHs5oNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNzo0NzoyNFrOHs594g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg0MzU3NA==", "bodyText": "Why was this sentence removed?", "url": "https://github.com/apache/kafka/pull/9551#discussion_r516843574", "createdAt": "2020-11-03T17:38:11Z", "author": {"login": "mjsax"}, "path": "docs/design.html", "diffHunk": "@@ -508,7 +506,7 @@ <h4><a id=\"design_compactionbasics\" href=\"#design_compactionbasics\">Log Compacti\n     the log, even if the message with that offset has been compacted away; in this case this position is indistinguishable from the next highest offset that does appear in the log. For example, in the picture above the\n     offsets 36, 37, and 38 are all equivalent positions and a read beginning at any of these offsets would return a message set beginning with 38.\n     <p>\n-    Compaction also allows for deletes. A message with a key and a null payload will be treated as a delete from the log. Such a record is sometimes referred to as a <i>tombstone</i>. This delete marker will cause any prior message with that key to be removed (as would any new\n+    Compaction also allows for deletes. A message with a key and a null payload will be treated as a delete from the log. This delete marker will cause any prior message with that key to be removed (as would any new", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a862ab9d2780ea1eac3f6e0c2c9e4598d8a69c80"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg0NDcxMA==", "bodyText": "Seems we are missing a opening <code> tag? Similar the next two changes below.", "url": "https://github.com/apache/kafka/pull/9551#discussion_r516844710", "createdAt": "2020-11-03T17:40:05Z", "author": {"login": "mjsax"}, "path": "docs/design.html", "diffHunk": "@@ -543,25 +544,25 @@ <h4><a id=\"design_compactiondetails\" href=\"#design_compactiondetails\">Log Compac\n     (assuming 1k messages).\n     </ol>\n     <p>\n-    <h4><a id=\"design_compactionconfig\" href=\"#design_compactionconfig\">Configuring The Log Cleaner</a></h4>\n+    <h4 class=\"anchor-heading\"><a id=\"design_compactionconfig\" class=\"anchor-link\"></a><a href=\"#design_compactionconfig\">Configuring The Log Cleaner</a></h4>\n \n     The log cleaner is enabled by default. This will start the pool of cleaner threads.\n     To enable log cleaning on a particular topic, add the log-specific property\n-    <pre class=\"brush: text;\"> log.cleanup.policy=compact</pre>\n+    <pre class=\"language-text\"> log.cleanup.policy=compact</code></pre>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a862ab9d2780ea1eac3f6e0c2c9e4598d8a69c80"}, "originalPosition": 222}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg0NTk1Mg==", "bodyText": "Cannot really verify those changes. But I guess, should be ok.", "url": "https://github.com/apache/kafka/pull/9551#discussion_r516845952", "createdAt": "2020-11-03T17:42:16Z", "author": {"login": "mjsax"}, "path": "docs/introduction.html", "diffHunk": "@@ -18,198 +18,203 @@\n <script><!--#include virtual=\"js/templateData.js\" --></script>\n \n <script id=\"introduction-template\" type=\"text/x-handlebars-template\">\n-  <h3> Apache Kafka&reg; is <i>a distributed streaming platform</i>. What exactly does that mean?</h3>\n-  <p>A streaming platform has three key capabilities:</p>\n-  <ul>\n-    <li>Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system.\n-    <li>Store streams of records in a fault-tolerant durable way.\n-    <li>Process streams of records as they occur.\n-  </ul>\n-  <p>Kafka is generally used for two broad classes of applications:</p>\n-  <ul>\n-    <li>Building real-time streaming data pipelines that reliably get data between systems or applications\n-    <li>Building real-time streaming applications that transform or react to the streams of data\n-  </ul>\n-  <p>To understand how Kafka does these things, let's dive in and explore Kafka's capabilities from the bottom up.</p>\n-  <p>First a few concepts:</p>\n-  <ul>\n-    <li>Kafka runs as a cluster on one or more servers that can span multiple datacenters.\n-      <li>The Kafka cluster stores streams of <i>records</i> in categories called <i>topics</i>.\n-    <li>Each record consists of a key, a value, and a timestamp.\n-  </ul>\n-  <p>Kafka has five core APIs:</p>\n-  <div style=\"overflow: hidden;\">\n-      <ul style=\"float: left; width: 40%;\">\n-      <li>The <a href=\"/documentation.html#producerapi\">Producer API</a> allows an application to publish a stream of records to one or more Kafka topics.\n-      <li>The <a href=\"/documentation.html#consumerapi\">Consumer API</a> allows an application to subscribe to one or more topics and process the stream of records produced to them.\n-    <li>The <a href=\"/documentation/streams\">Streams API</a> allows an application to act as a <i>stream processor</i>, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.\n-    <li>The <a href=\"/documentation.html#connect\">Connector API</a> allows building and running reusable producers or consumers that connect Kafka topics to existing applications or data systems. For example, a connector to a relational database might capture every change to a table.\n-    <li>The <a href=\"/documentation.html#adminapi\">Admin API</a> allows managing and inspecting topics, brokers and other Kafka objects.\n-  </ul>\n-      <img src=\"/{{version}}/images/kafka-apis.png\" style=\"float: right; width: 50%;\">\n-      </div>\n-  <p>\n-  In Kafka the communication between the clients and the servers is done with a simple, high-performance, language agnostic <a href=\"https://kafka.apache.org/protocol.html\">TCP protocol</a>. This protocol is versioned and maintains backwards compatibility with older versions. We provide a Java client for Kafka, but clients are available in <a href=\"https://cwiki.apache.org/confluence/display/KAFKA/Clients\">many languages</a>.</p>\n-\n-  <h4><a id=\"intro_topics\" href=\"#intro_topics\">Topics and Logs</a></h4>\n-  <p>Let's first dive into the core abstraction Kafka provides for a stream of records&mdash;the topic.</p>\n-  <p>A topic is a category or feed name to which records are published. Topics in Kafka are always multi-subscriber; that is, a topic can have zero, one, or many consumers that subscribe to the data written to it.</p>\n-  <p> For each topic, the Kafka cluster maintains a partitioned log that looks like this: </p>\n-  <img class=\"centered\" src=\"/{{version}}/images/log_anatomy.png\">\n-\n-  <p> Each partition is an ordered, immutable sequence of records that is continually appended to&mdash;a structured commit log. The records in the partitions are each assigned a sequential id number called the <i>offset</i> that uniquely identifies each record within the partition.\n-  </p>\n+  <h4 class=\"anchor-heading\">", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a862ab9d2780ea1eac3f6e0c2c9e4598d8a69c80"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg0NjQ1NA==", "bodyText": "Missing opening <code> tag", "url": "https://github.com/apache/kafka/pull/9551#discussion_r516846454", "createdAt": "2020-11-03T17:43:10Z", "author": {"login": "mjsax"}, "path": "docs/ops.html", "diffHunk": "@@ -78,37 +68,31 @@ <h4><a id=\"basic_ops_restarting\" href=\"#basic_ops_restarting\">Graceful shutdown<\n   </ol>\n \n   Syncing the logs will happen automatically whenever the server is stopped other than by a hard kill, but the controlled leadership migration requires using a special setting:\n-  <pre class=\"brush: text;\">\n-      controlled.shutdown.enable=true\n-  </pre>\n+  <pre class=\"line-numbers\"><code class=\"language-text\">      controlled.shutdown.enable=true</code></pre>\n   Note that controlled shutdown will only succeed if <i>all</i> the partitions hosted on the broker have replicas (i.e. the replication factor is greater than 1 <i>and</i> at least one of these replicas is alive). This is generally what you want since shutting down the last replica would make that topic partition unavailable.\n \n-  <h4><a id=\"basic_ops_leader_balancing\" href=\"#basic_ops_leader_balancing\">Balancing leadership</a></h4>\n+  <h4 class=\"anchor-heading\"><a id=\"basic_ops_leader_balancing\" class=\"anchor-link\"></a><a href=\"#basic_ops_leader_balancing\">Balancing leadership</a></h4>\n \n   Whenever a broker stops or crashes, leadership for that broker's partitions transfers to other replicas. When the broker is restarted it will only be a follower for all its partitions, meaning it will not be used for client reads and writes.\n   <p>\n   To avoid this imbalance, Kafka has a notion of preferred replicas. If the list of replicas for a partition is 1,5,9 then node 1 is preferred as the leader to either node 5 or 9 because it is earlier in the replica list. By default the Kafka cluster will try to restore leadership to the restored replicas.  This behaviour is configured with:\n \n-  <pre class=\"brush: text;\">\n-      auto.leader.rebalance.enable=true\n-  </pre>\n+  <pre class=\"line-numbers\"><code class=\"language-text\">      auto.leader.rebalance.enable=true</code></pre>\n     You can also set this to false, but you will then need to manually restore leadership to the restored replicas by running the command:\n-  <pre class=\"brush: bash;\">\n-  &gt; bin/kafka-preferred-replica-election.sh --bootstrap-server broker_host:port\n-  </pre>\n+  <pre class=\"line-numbers\"><code class=\"language-bash\">  &gt; bin/kafka-preferred-replica-election.sh --bootstrap-server broker_host:port</code></pre>\n \n-  <h4><a id=\"basic_ops_racks\" href=\"#basic_ops_racks\">Balancing Replicas Across Racks</a></h4>\n+  <h4 class=\"anchor-heading\"><a id=\"basic_ops_racks\" class=\"anchor-link\"></a><a href=\"#basic_ops_racks\">Balancing Replicas Across Racks</a></h4>\n   The rack awareness feature spreads replicas of the same partition across different racks. This extends the guarantees Kafka provides for broker-failure to cover rack-failure, limiting the risk of data loss should all the brokers on a rack fail at once. The feature can also be applied to other broker groupings such as availability zones in EC2.\n   <p></p>\n   You can specify that a broker belongs to a particular rack by adding a property to the broker config:\n-  <pre class=\"brush: text;\">   broker.rack=my-rack-id</pre>\n+  <pre class=\"language-text\">   broker.rack=my-rack-id</code></pre>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a862ab9d2780ea1eac3f6e0c2c9e4598d8a69c80"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg0NzQ2Ng==", "bodyText": "Missing opening <code> tag", "url": "https://github.com/apache/kafka/pull/9551#discussion_r516847466", "createdAt": "2020-11-03T17:44:43Z", "author": {"login": "mjsax"}, "path": "docs/ops.html", "diffHunk": "@@ -1658,9 +1564,9 @@ <h4><a id=\"kafka_streams_monitoring\" href=\"#kafka_streams_monitoring\">Streams Mo\n   Use the following configuration option to specify which metrics\n   you want collected:\n \n-<pre>metrics.recording.level=\"info\"</pre>\n+<pre>metrics.recording.level=\"info\"</code></pre>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a862ab9d2780ea1eac3f6e0c2c9e4598d8a69c80"}, "originalPosition": 826}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg0OTA2Ng==", "bodyText": "Cannot really verify this one.", "url": "https://github.com/apache/kafka/pull/9551#discussion_r516849066", "createdAt": "2020-11-03T17:47:18Z", "author": {"login": "mjsax"}, "path": "docs/quickstart-docker.html", "diffHunk": "@@ -0,0 +1,204 @@\n+<!--", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a862ab9d2780ea1eac3f6e0c2c9e4598d8a69c80"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjg0OTEyMg==", "bodyText": "Cannot really verify this one.", "url": "https://github.com/apache/kafka/pull/9551#discussion_r516849122", "createdAt": "2020-11-03T17:47:24Z", "author": {"login": "mjsax"}, "path": "docs/quickstart-zookeeper.html", "diffHunk": "@@ -0,0 +1,277 @@\n+<!--", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a862ab9d2780ea1eac3f6e0c2c9e4598d8a69c80"}, "originalPosition": 1}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7009d2ebc90f85fea8ee8a6d4c81b0ef278b5f26", "author": {"user": {"login": "bbejeck", "name": "Bill Bejeck"}}, "url": "https://github.com/apache/kafka/commit/7009d2ebc90f85fea8ee8a6d4c81b0ef278b5f26", "committedDate": "2020-11-03T18:38:12Z", "message": "MINOR: Updates per comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2672, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}