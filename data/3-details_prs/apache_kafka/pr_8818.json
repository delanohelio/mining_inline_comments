{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI4ODE2MzI1", "number": 8818, "title": "KAFKA-10086: Integration test for ensuring warmups are effective", "bodyText": "Add an integration test for the task assignor.\n\nensure we see proper scale-out behavior with warmups\nensure in-memory stores are properly recycled and not restored through the scale-out process\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-06-05T22:31:18Z", "url": "https://github.com/apache/kafka/pull/8818", "merged": true, "mergeCommit": {"oid": "e8e9fe82daa4b0604d039be6f408256b66f97072"}, "closed": true, "closedAt": "2020-06-11T13:31:21Z", "author": {"login": "vvcephei"}, "timelineItems": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcoajUrAFqTQyNTY2NDU3Ng==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcqFw1ZAH2gAyNDI4ODE2MzI1OmNhNTZiNTY0ODM3ODhmNjRjM2YxMmQ3ZWRkZmNjMGM4N2ZlMDM2YzQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI1NjY0NTc2", "url": "https://github.com/apache/kafka/pull/8818#pullrequestreview-425664576", "createdAt": "2020-06-05T22:34:27Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQyMjozNDoyN1rOGf_JGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQyMjozOTowNFrOGf_OFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5MzU2MA==", "bodyText": "Bug 1: we can't remove active tasks in the cooperative algorithm, because this causes their state to get discarded (definitely for in-memory stores, and maybe for persistent ones, depending on the state cleaner).\nInstead, we convert them to standbys so they can keep warm.", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436193560", "createdAt": "2020-06-05T22:34:27Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -1013,7 +1014,7 @@ private boolean addClientAssignments(final Map<String, Assignment> assignment,\n             final List<TopicPartition> activePartitionsList = new ArrayList<>();\n             final List<TaskId> assignedActiveList = new ArrayList<>();\n \n-            final boolean tasksRevoked = populateActiveTaskAndPartitionsLists(\n+            final Set<TaskId> activeTasksRemovedPendingRevokation = populateActiveTaskAndPartitionsLists(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5MzgzMg==", "bodyText": "just noticed this.", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436193832", "createdAt": "2020-06-05T22:35:22Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -86,7 +87,7 @@\n     private boolean rebalanceInProgress = false;  // if we are in the middle of a rebalance, it is not safe to commit\n \n     // includes assigned & initialized tasks and unassigned tasks we locked temporarily during rebalance\n-    private Set<TaskId> lockedTaskDirectories = new HashSet<>();\n+    private final Set<TaskId> lockedTaskDirectories = new HashSet<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NDA3NA==", "bodyText": "Bug 2: tasks with only in-memory stores don't get task directories, so we were skipping them here. See the comment.", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436194074", "createdAt": "2020-06-05T22:36:07Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -514,17 +515,24 @@ void handleLostAll() {\n \n     /**\n      * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n-     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryToLockAllNonEmptyTaskDirectories()}\n-     *\n-     * @return Map from task id to its total offset summed across all state stores\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryToLockAllNonEmptyTaskDirectories()}.\n+     * Does not include stateless or non-logged tasks.\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : lockedTaskDirectories) {\n+        // Not all tasks will create directories, and there may be directories for tasks we don't currently own,\n+        // so we consider all tasks that are either owned or on disk. This includes stateless tasks, which should\n+        // just have an empty changelogOffsets map.\n+        for (final TaskId id : union(HashSet::new, lockedTaskDirectories, tasks.keySet())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NDIyMQ==", "bodyText": "I independently implemented this check before Matthias's PR, and preferred this message.", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436194221", "createdAt": "2020-06-05T22:36:47Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -605,13 +613,14 @@ private long sumOfChangelogOffsets(final TaskId id, final Map<TopicPartition, Lo\n             final long offset = changelogEntry.getValue();\n \n \n-            if (offset == Task.LATEST_OFFSET) { // this condition can only be true for active tasks; never for standby\n+            if (offset == Task.LATEST_OFFSET) {\n+                // this condition can only be true for active tasks; never for standby\n                 // for this case, the offset of all partitions is set to `LATEST_OFFSET`\n                 // and we \"forward\" the sentinel value directly\n                 return Task.LATEST_OFFSET;\n             } else {\n                 if (offset < 0) {\n-                    throw new IllegalStateException(\"Offset should not be negative.\");\n+                    throw new IllegalStateException(\"Expected not to get a sentinel offset, but got: \" + changelogEntry);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NDM1Nw==", "bodyText": "Just a few cleanups in this first test.", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436194357", "createdAt": "2020-06-05T22:37:17Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java", "diffHunk": "@@ -75,21 +101,23 @@ public void shouldProperlyConfigureTheAssignor() throws NoSuchFieldException, Il\n \n         final String testId = safeUniqueTestName(getClass(), testName);\n         final String appId = \"appId_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n \n-        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, \"input\");\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, inputTopic);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NDUyMg==", "bodyText": "Here are the new tests, one for in-memory and one for persistent. We expect exactly the same behavior for both.", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436194522", "createdAt": "2020-06-05T22:37:50Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java", "diffHunk": "@@ -142,4 +169,184 @@ public void shouldProperlyConfigureTheAssignor() throws NoSuchFieldException, Il\n             assertThat(taskAssignor, instanceOf(MyTaskAssignor.class));\n         }\n     }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NDgzOQ==", "bodyText": "The fix for Bug 1 necessitated this test change.", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436194839", "createdAt": "2020-06-05T22:39:04Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignorTest.java", "diffHunk": "@@ -1634,16 +1636,15 @@ public void shouldReturnInterleavedAssignmentWithUnrevokedPartitionsRemovedWhenN\n \n         // The new consumer's assignment should be empty until c1 has the chance to revoke its partitions/tasks\n         assertThat(assignment.get(CONSUMER_2).partitions(), equalTo(emptyList()));\n-        assertThat(\n-            AssignmentInfo.decode(assignment.get(CONSUMER_2).userData()),\n-            equalTo(new AssignmentInfo(\n-                LATEST_SUPPORTED_VERSION,\n-                emptyList(),\n-                emptyMap(),\n-                emptyMap(),\n-                emptyMap(),\n-                0\n-            )));\n+\n+        final AssignmentInfo actualAssignment = AssignmentInfo.decode(assignment.get(CONSUMER_2).userData());\n+        assertThat(actualAssignment.version(), is(LATEST_SUPPORTED_VERSION));\n+        assertThat(actualAssignment.activeTasks(), empty());\n+        // Note we're not asserting anything about standbys. If the assignor gave an active task to CONSUMER_2, it would\n+        // be converted to a standby, but we don't know whether the assignor will do that.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI1Njc4Mjc2", "url": "https://github.com/apache/kafka/pull/8818#pullrequestreview-425678276", "createdAt": "2020-06-05T23:27:15Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQyMzoyNzoxNVrOGf_25g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQyMzoyNzoxNVrOGf_25g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIwNTI4Ng==", "bodyText": "By the way, this tripped me up quite a bit. Changing the clientState has absolutely no effect at this point. Might be nice to enforce that in code, but I'd like to defer it (I took a stab, but it's a lot of changes).", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436205286", "createdAt": "2020-06-05T23:27:15Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -1084,12 +1088,15 @@ private boolean populateActiveTaskAndPartitionsLists(final List<TopicPartition>\n                 // If the partition is new to this consumer but is still owned by another, remove from the assignment\n                 // until it has been revoked and can safely be reassigned according to the COOPERATIVE protocol\n                 if (newPartitionForConsumer && allOwnedPartitions.contains(partition)) {\n-                    log.info(\"Removing task {} from assignment until it is safely revoked in followup rebalance\", taskId);\n-                    clientState.unassignActive(taskId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 72}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI1NjY4MTc1", "url": "https://github.com/apache/kafka/pull/8818#pullrequestreview-425668175", "createdAt": "2020-06-05T22:46:25Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQyMjo0NjoyNVrOGf_U8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQyMzo0ODoxMFrOGgAFlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5NjU5Mw==", "bodyText": "\"apparently\" ? \ud83e\udd14", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436196593", "createdAt": "2020-06-05T22:46:25Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -514,17 +515,24 @@ void handleLostAll() {\n \n     /**\n      * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n-     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryToLockAllNonEmptyTaskDirectories()}\n-     *\n-     * @return Map from task id to its total offset summed across all state stores\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryToLockAllNonEmptyTaskDirectories()}.\n+     * Does not include stateless or non-logged tasks.\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : lockedTaskDirectories) {\n+        // Not all tasks will create directories, and there may be directories for tasks we don't currently own,\n+        // so we consider all tasks that are either owned or on disk. This includes stateless tasks, which should\n+        // just have an empty changelogOffsets map.\n+        for (final TaskId id : union(HashSet::new, lockedTaskDirectories, tasks.keySet())) {\n             final Task task = tasks.get(id);\n             if (task != null) {\n-                taskOffsetSums.put(id, sumOfChangelogOffsets(id, task.changelogOffsets()));\n+                final Map<TopicPartition, Long> changelogOffsets = task.changelogOffsets();\n+                if (changelogOffsets.isEmpty()) {\n+                    log.debug(\"Skipping to encode apparently stateless (or non-logged) offset sum for task {}\", id);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE5OTAwNA==", "bodyText": "What is NB?", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436199004", "createdAt": "2020-06-05T22:56:49Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java", "diffHunk": "@@ -142,4 +169,184 @@ public void shouldProperlyConfigureTheAssignor() throws NoSuchFieldException, Il\n             assertThat(taskAssignor, instanceOf(MyTaskAssignor.class));\n         }\n     }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIwODU4Nw==", "bodyText": "I guess it was nice to have as a sanity check that the task really was assigned to this client, and that we haven't already tried to remove it or something. But maybe we can take that for granted...? (I only mention it because it helped me find a bug in the thread-level stickiness PR)", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436208587", "createdAt": "2020-06-05T23:45:23Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java", "diffHunk": "@@ -1084,12 +1088,15 @@ private boolean populateActiveTaskAndPartitionsLists(final List<TopicPartition>\n                 // If the partition is new to this consumer but is still owned by another, remove from the assignment\n                 // until it has been revoked and can safely be reassigned according to the COOPERATIVE protocol\n                 if (newPartitionForConsumer && allOwnedPartitions.contains(partition)) {\n-                    log.info(\"Removing task {} from assignment until it is safely revoked in followup rebalance\", taskId);\n-                    clientState.unassignActive(taskId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIwNTI4Ng=="}, "originalCommit": null, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjIwOTA0NQ==", "bodyText": "Should we also add up the records restored here, since onRestoreEnd is not invoked if the task is closed during restoration?", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436209045", "createdAt": "2020-06-05T23:48:10Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java", "diffHunk": "@@ -142,4 +169,184 @@ public void shouldProperlyConfigureTheAssignor() throws NoSuchFieldException, Il\n             assertThat(taskAssignor, instanceOf(MyTaskAssignor.class));\n         }\n     }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + System.currentTimeMillis() + \"_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n+        final String storeName = \"store\" + testId;\n+        final String storeChangelog = appId + \"-store\" + testId + \"-changelog\";\n+        final Set<TopicPartition> changelogTopicPartitions = mkSet(\n+            new TopicPartition(storeChangelog, 0),\n+            new TopicPartition(storeChangelog, 1)\n+        );\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, 2, inputTopic, storeChangelog);\n+\n+        final ReentrantLock assignmentLock = new ReentrantLock();\n+        final AtomicInteger assignmentsCompleted = new AtomicInteger(0);\n+        final AtomicBoolean assignmentStable = new AtomicBoolean(false);\n+        final AssignmentListener assignmentListener =\n+            stable -> {\n+                assignmentLock.lock();\n+                try {\n+                    assignmentsCompleted.incrementAndGet();\n+                    assignmentStable.set(stable);\n+                } finally {\n+                    assignmentLock.unlock();\n+                }\n+            };\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.table(inputTopic, materializedFunction.apply(storeName));\n+        final Topology topology = builder.build();\n+\n+        final Properties producerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ProducerConfig.ACKS_CONFIG, \"all\"),\n+                mkEntry(ProducerConfig.RETRIES_CONFIG, \"0\"),\n+                mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()),\n+                mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName())\n+            )\n+        );\n+\n+        final StringBuilder kiloBuilder = new StringBuilder(1000);\n+        for (int i = 0; i < 1000; i++) {\n+            kiloBuilder.append('0');\n+        }\n+        final String kilo = kiloBuilder.toString();\n+\n+        try (final Producer<String, String> producer = new KafkaProducer<>(producerProperties)) {\n+            for (int i = 0; i < 1000; i++) {\n+                producer.send(new ProducerRecord<>(inputTopic, String.valueOf(i), kilo));\n+            }\n+        }\n+\n+        final Properties consumerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()),\n+                mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName())\n+            )\n+        );\n+\n+\n+        try (final KafkaStreams kafkaStreams0 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final KafkaStreams kafkaStreams1 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final Consumer<String, String> consumer = new KafkaConsumer<>(consumerProperties)) {\n+            kafkaStreams0.start();\n+\n+            // wait until all the input records are in the changelog\n+            TestUtils.waitForCondition(\n+                () -> getChangelogOffsetSum(changelogTopicPartitions, consumer) == 1000,\n+                120_000L,\n+                () -> \"Input records haven't all been written to the changelog: \" + getChangelogOffsetSum(changelogTopicPartitions, consumer)\n+            );\n+\n+            final AtomicLong instance1TotalRestored = new AtomicLong(-1);\n+            final CountDownLatch restoreCompleteLatch = new CountDownLatch(1);\n+            kafkaStreams1.setGlobalStateRestoreListener(new StateRestoreListener() {\n+                @Override\n+                public void onRestoreStart(final TopicPartition topicPartition,\n+                                           final String storeName,\n+                                           final long startingOffset,\n+                                           final long endingOffset) {\n+                }\n+\n+                @Override\n+                public void onBatchRestored(final TopicPartition topicPartition,\n+                                            final String storeName,\n+                                            final long batchEndOffset,\n+                                            final long numRestored) {\n+                }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 214}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2Mzk3NzI2", "url": "https://github.com/apache/kafka/pull/8818#pullrequestreview-426397726", "createdAt": "2020-06-08T16:12:13Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNjoxMjoxNFrOGglw6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNjoxMjoxNFrOGglw6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjgyNjM0NA==", "bodyText": "Cool, I was kind of hoping you would put this in a separate integration test class", "url": "https://github.com/apache/kafka/pull/8818#discussion_r436826344", "createdAt": "2020-06-08T16:12:14Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 70}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI2Mzk5NDIy", "url": "https://github.com/apache/kafka/pull/8818#pullrequestreview-426399422", "createdAt": "2020-06-08T16:14:12Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4NDA4Nzkw", "url": "https://github.com/apache/kafka/pull/8818#pullrequestreview-428408790", "createdAt": "2020-06-10T20:36:47Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMDozNjo0OFrOGiFbKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMDozNjo0OFrOGiFbKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5MzY0MA==", "bodyText": "nit: simplify -> throws Exception (also above)", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438393640", "createdAt": "2020-06-10T20:36:48Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 91}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4NDA5MTc4", "url": "https://github.com/apache/kafka/pull/8818#pullrequestreview-428409178", "createdAt": "2020-06-10T20:37:21Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMDozNzoyMVrOGiFcSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMDozNzoyMVrOGiFcSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5MzkyOA==", "bodyText": "Why set this? zero is the default anyway", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438393928", "createdAt": "2020-06-10T20:37:21Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + System.currentTimeMillis() + \"_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n+        final String storeName = \"store\" + testId;\n+        final String storeChangelog = appId + \"-store\" + testId + \"-changelog\";\n+        final Set<TopicPartition> changelogTopicPartitions = mkSet(\n+            new TopicPartition(storeChangelog, 0),\n+            new TopicPartition(storeChangelog, 1)\n+        );\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, 2, inputTopic, storeChangelog);\n+\n+        final ReentrantLock assignmentLock = new ReentrantLock();\n+        final AtomicInteger assignmentsCompleted = new AtomicInteger(0);\n+        final AtomicBoolean assignmentStable = new AtomicBoolean(false);\n+        final AssignmentListener assignmentListener =\n+            stable -> {\n+                assignmentLock.lock();\n+                try {\n+                    assignmentsCompleted.incrementAndGet();\n+                    assignmentStable.set(stable);\n+                } finally {\n+                    assignmentLock.unlock();\n+                }\n+            };\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.table(inputTopic, materializedFunction.apply(storeName));\n+        final Topology topology = builder.build();\n+\n+        final Properties producerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ProducerConfig.ACKS_CONFIG, \"all\"),\n+                mkEntry(ProducerConfig.RETRIES_CONFIG, \"0\"),\n+                mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()),\n+                mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName())\n+            )\n+        );\n+\n+        final StringBuilder kiloBuilder = new StringBuilder(1000);\n+        for (int i = 0; i < 1000; i++) {\n+            kiloBuilder.append('0');\n+        }\n+        final String kilo = kiloBuilder.toString();\n+\n+        try (final Producer<String, String> producer = new KafkaProducer<>(producerProperties)) {\n+            for (int i = 0; i < 1000; i++) {\n+                producer.send(new ProducerRecord<>(inputTopic, String.valueOf(i), kilo));\n+            }\n+        }\n+\n+        final Properties consumerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()),\n+                mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName())\n+            )\n+        );\n+\n+\n+        try (final KafkaStreams kafkaStreams0 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final KafkaStreams kafkaStreams1 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final Consumer<String, String> consumer = new KafkaConsumer<>(consumerProperties)) {\n+            kafkaStreams0.start();\n+\n+            // wait until all the input records are in the changelog\n+            TestUtils.waitForCondition(\n+                () -> getChangelogOffsetSum(changelogTopicPartitions, consumer) == 1000,\n+                120_000L,\n+                () -> \"Input records haven't all been written to the changelog: \" + getChangelogOffsetSum(changelogTopicPartitions, consumer)\n+            );\n+\n+            final AtomicLong instance1TotalRestored = new AtomicLong(-1);\n+            final AtomicLong instance1NumRestored = new AtomicLong(-1);\n+            final CountDownLatch restoreCompleteLatch = new CountDownLatch(1);\n+            kafkaStreams1.setGlobalStateRestoreListener(new StateRestoreListener() {\n+                @Override\n+                public void onRestoreStart(final TopicPartition topicPartition,\n+                                           final String storeName,\n+                                           final long startingOffset,\n+                                           final long endingOffset) {\n+                }\n+\n+                @Override\n+                public void onBatchRestored(final TopicPartition topicPartition,\n+                                            final String storeName,\n+                                            final long batchEndOffset,\n+                                            final long numRestored) {\n+                    instance1NumRestored.accumulateAndGet(\n+                        numRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                }\n+\n+                @Override\n+                public void onRestoreEnd(final TopicPartition topicPartition,\n+                                         final String storeName,\n+                                         final long totalRestored) {\n+                    instance1TotalRestored.accumulateAndGet(\n+                        totalRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                    restoreCompleteLatch.countDown();\n+                }\n+            });\n+            final int assignmentsBeforeScaleOut = assignmentsCompleted.get();\n+            final AtomicInteger assignmentsAfterScaleOut = new AtomicInteger(0);\n+            kafkaStreams1.start();\n+            TestUtils.waitForCondition(\n+                () -> {\n+                    assignmentLock.lock();\n+                    try {\n+                        assignmentsAfterScaleOut.set(assignmentsCompleted.get());\n+                        if (assignmentsBeforeScaleOut < assignmentsAfterScaleOut.get()) {\n+                            // the first assignment after adding a node should be unstable while we warm up the state.\n+                            assertThat(assignmentStable.get(), is(false));\n+                            return true;\n+                        } else {\n+                            return false;\n+                        }\n+                    } finally {\n+                        assignmentLock.unlock();\n+                    }\n+                },\n+                120_000L,\n+                \"Never saw a first assignment after scale out: \" + assignmentsCompleted.get()\n+            );\n+\n+            TestUtils.waitForCondition(\n+                assignmentStable::get,\n+                120_000L,\n+                \"Assignment hasn't become stable: \" + assignmentsCompleted.get() +\n+                    \" Note, if this does fail, check and see if the new instance just failed to catch up within\" +\n+                    \" the probing rebalance interval. A full minute should be long enough to read ~500 records\" +\n+                    \" in any test environment, but you never know...\"\n+            );\n+\n+            restoreCompleteLatch.await();\n+            // We should finalize the restoration without having restored any records (because they're already in\n+            // the store. Otherwise, we failed to properly re-use the state from the standby.\n+            assertThat(instance1TotalRestored.get(), is(0L));\n+            // Belt-and-suspenders check that we never even attempt to restore any records.\n+            assertThat(instance1NumRestored.get(), is(-1L));\n+        }\n+    }\n+\n+    private static Properties streamsProperties(final String appId,\n+                                                final AssignmentListener configuredAssignmentListener) {\n+        return mkObjectProperties(\n+            mkMap(\n+                mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                mkEntry(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, \"0\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 246}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4NDA5NzU1", "url": "https://github.com/apache/kafka/pull/8818#pullrequestreview-428409755", "createdAt": "2020-06-10T20:38:10Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMDozODoxMVrOGiFd2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMDozODoxMVrOGiFd2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5NDMzMA==", "bodyText": "The comment above say, the minimum is 60secs -- seems we control this minimum -- could we reduce it?", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438394330", "createdAt": "2020-06-10T20:38:11Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + System.currentTimeMillis() + \"_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n+        final String storeName = \"store\" + testId;\n+        final String storeChangelog = appId + \"-store\" + testId + \"-changelog\";\n+        final Set<TopicPartition> changelogTopicPartitions = mkSet(\n+            new TopicPartition(storeChangelog, 0),\n+            new TopicPartition(storeChangelog, 1)\n+        );\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, 2, inputTopic, storeChangelog);\n+\n+        final ReentrantLock assignmentLock = new ReentrantLock();\n+        final AtomicInteger assignmentsCompleted = new AtomicInteger(0);\n+        final AtomicBoolean assignmentStable = new AtomicBoolean(false);\n+        final AssignmentListener assignmentListener =\n+            stable -> {\n+                assignmentLock.lock();\n+                try {\n+                    assignmentsCompleted.incrementAndGet();\n+                    assignmentStable.set(stable);\n+                } finally {\n+                    assignmentLock.unlock();\n+                }\n+            };\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.table(inputTopic, materializedFunction.apply(storeName));\n+        final Topology topology = builder.build();\n+\n+        final Properties producerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ProducerConfig.ACKS_CONFIG, \"all\"),\n+                mkEntry(ProducerConfig.RETRIES_CONFIG, \"0\"),\n+                mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()),\n+                mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName())\n+            )\n+        );\n+\n+        final StringBuilder kiloBuilder = new StringBuilder(1000);\n+        for (int i = 0; i < 1000; i++) {\n+            kiloBuilder.append('0');\n+        }\n+        final String kilo = kiloBuilder.toString();\n+\n+        try (final Producer<String, String> producer = new KafkaProducer<>(producerProperties)) {\n+            for (int i = 0; i < 1000; i++) {\n+                producer.send(new ProducerRecord<>(inputTopic, String.valueOf(i), kilo));\n+            }\n+        }\n+\n+        final Properties consumerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()),\n+                mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName())\n+            )\n+        );\n+\n+\n+        try (final KafkaStreams kafkaStreams0 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final KafkaStreams kafkaStreams1 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final Consumer<String, String> consumer = new KafkaConsumer<>(consumerProperties)) {\n+            kafkaStreams0.start();\n+\n+            // wait until all the input records are in the changelog\n+            TestUtils.waitForCondition(\n+                () -> getChangelogOffsetSum(changelogTopicPartitions, consumer) == 1000,\n+                120_000L,\n+                () -> \"Input records haven't all been written to the changelog: \" + getChangelogOffsetSum(changelogTopicPartitions, consumer)\n+            );\n+\n+            final AtomicLong instance1TotalRestored = new AtomicLong(-1);\n+            final AtomicLong instance1NumRestored = new AtomicLong(-1);\n+            final CountDownLatch restoreCompleteLatch = new CountDownLatch(1);\n+            kafkaStreams1.setGlobalStateRestoreListener(new StateRestoreListener() {\n+                @Override\n+                public void onRestoreStart(final TopicPartition topicPartition,\n+                                           final String storeName,\n+                                           final long startingOffset,\n+                                           final long endingOffset) {\n+                }\n+\n+                @Override\n+                public void onBatchRestored(final TopicPartition topicPartition,\n+                                            final String storeName,\n+                                            final long batchEndOffset,\n+                                            final long numRestored) {\n+                    instance1NumRestored.accumulateAndGet(\n+                        numRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                }\n+\n+                @Override\n+                public void onRestoreEnd(final TopicPartition topicPartition,\n+                                         final String storeName,\n+                                         final long totalRestored) {\n+                    instance1TotalRestored.accumulateAndGet(\n+                        totalRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                    restoreCompleteLatch.countDown();\n+                }\n+            });\n+            final int assignmentsBeforeScaleOut = assignmentsCompleted.get();\n+            final AtomicInteger assignmentsAfterScaleOut = new AtomicInteger(0);\n+            kafkaStreams1.start();\n+            TestUtils.waitForCondition(\n+                () -> {\n+                    assignmentLock.lock();\n+                    try {\n+                        assignmentsAfterScaleOut.set(assignmentsCompleted.get());\n+                        if (assignmentsBeforeScaleOut < assignmentsAfterScaleOut.get()) {\n+                            // the first assignment after adding a node should be unstable while we warm up the state.\n+                            assertThat(assignmentStable.get(), is(false));\n+                            return true;\n+                        } else {\n+                            return false;\n+                        }\n+                    } finally {\n+                        assignmentLock.unlock();\n+                    }\n+                },\n+                120_000L,\n+                \"Never saw a first assignment after scale out: \" + assignmentsCompleted.get()\n+            );\n+\n+            TestUtils.waitForCondition(\n+                assignmentStable::get,\n+                120_000L,\n+                \"Assignment hasn't become stable: \" + assignmentsCompleted.get() +\n+                    \" Note, if this does fail, check and see if the new instance just failed to catch up within\" +\n+                    \" the probing rebalance interval. A full minute should be long enough to read ~500 records\" +\n+                    \" in any test environment, but you never know...\"\n+            );\n+\n+            restoreCompleteLatch.await();\n+            // We should finalize the restoration without having restored any records (because they're already in\n+            // the store. Otherwise, we failed to properly re-use the state from the standby.\n+            assertThat(instance1TotalRestored.get(), is(0L));\n+            // Belt-and-suspenders check that we never even attempt to restore any records.\n+            assertThat(instance1NumRestored.get(), is(-1L));\n+        }\n+    }\n+\n+    private static Properties streamsProperties(final String appId,\n+                                                final AssignmentListener configuredAssignmentListener) {\n+        return mkObjectProperties(\n+            mkMap(\n+                mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                mkEntry(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, \"0\"),\n+                mkEntry(StreamsConfig.ACCEPTABLE_RECOVERY_LAG_CONFIG, \"0\"), // make the warmup catch up completely\n+                mkEntry(StreamsConfig.MAX_WARMUP_REPLICAS_CONFIG, \"2\"),\n+                mkEntry(StreamsConfig.PROBING_REBALANCE_INTERVAL_MS_CONFIG, \"60000\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 249}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4NDEwOTAz", "url": "https://github.com/apache/kafka/pull/8818#pullrequestreview-428410903", "createdAt": "2020-06-10T20:39:59Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMDozOTo1OVrOGiFhJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMDozOTo1OVrOGiFhJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODM5NTE3NA==", "bodyText": "why do we set retries to zero?", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438395174", "createdAt": "2020-06-10T20:39:59Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + System.currentTimeMillis() + \"_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n+        final String storeName = \"store\" + testId;\n+        final String storeChangelog = appId + \"-store\" + testId + \"-changelog\";\n+        final Set<TopicPartition> changelogTopicPartitions = mkSet(\n+            new TopicPartition(storeChangelog, 0),\n+            new TopicPartition(storeChangelog, 1)\n+        );\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, 2, inputTopic, storeChangelog);\n+\n+        final ReentrantLock assignmentLock = new ReentrantLock();\n+        final AtomicInteger assignmentsCompleted = new AtomicInteger(0);\n+        final AtomicBoolean assignmentStable = new AtomicBoolean(false);\n+        final AssignmentListener assignmentListener =\n+            stable -> {\n+                assignmentLock.lock();\n+                try {\n+                    assignmentsCompleted.incrementAndGet();\n+                    assignmentStable.set(stable);\n+                } finally {\n+                    assignmentLock.unlock();\n+                }\n+            };\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.table(inputTopic, materializedFunction.apply(storeName));\n+        final Topology topology = builder.build();\n+\n+        final Properties producerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ProducerConfig.ACKS_CONFIG, \"all\"),\n+                mkEntry(ProducerConfig.RETRIES_CONFIG, \"0\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 126}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4NDIyODAx", "url": "https://github.com/apache/kafka/pull/8818#pullrequestreview-428422801", "createdAt": "2020-06-10T20:57:47Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMDo1Nzo0N1rOGiGEkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMDo1Nzo0N1rOGiGEkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwNDI0MA==", "bodyText": "Seems we don't need variable assignmentsAfterScaleOut and can just call assignmentsCompleted.get() here?\nAt least, we can make it a local int variable?", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438404240", "createdAt": "2020-06-10T20:57:47Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + System.currentTimeMillis() + \"_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n+        final String storeName = \"store\" + testId;\n+        final String storeChangelog = appId + \"-store\" + testId + \"-changelog\";\n+        final Set<TopicPartition> changelogTopicPartitions = mkSet(\n+            new TopicPartition(storeChangelog, 0),\n+            new TopicPartition(storeChangelog, 1)\n+        );\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, 2, inputTopic, storeChangelog);\n+\n+        final ReentrantLock assignmentLock = new ReentrantLock();\n+        final AtomicInteger assignmentsCompleted = new AtomicInteger(0);\n+        final AtomicBoolean assignmentStable = new AtomicBoolean(false);\n+        final AssignmentListener assignmentListener =\n+            stable -> {\n+                assignmentLock.lock();\n+                try {\n+                    assignmentsCompleted.incrementAndGet();\n+                    assignmentStable.set(stable);\n+                } finally {\n+                    assignmentLock.unlock();\n+                }\n+            };\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.table(inputTopic, materializedFunction.apply(storeName));\n+        final Topology topology = builder.build();\n+\n+        final Properties producerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ProducerConfig.ACKS_CONFIG, \"all\"),\n+                mkEntry(ProducerConfig.RETRIES_CONFIG, \"0\"),\n+                mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()),\n+                mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName())\n+            )\n+        );\n+\n+        final StringBuilder kiloBuilder = new StringBuilder(1000);\n+        for (int i = 0; i < 1000; i++) {\n+            kiloBuilder.append('0');\n+        }\n+        final String kilo = kiloBuilder.toString();\n+\n+        try (final Producer<String, String> producer = new KafkaProducer<>(producerProperties)) {\n+            for (int i = 0; i < 1000; i++) {\n+                producer.send(new ProducerRecord<>(inputTopic, String.valueOf(i), kilo));\n+            }\n+        }\n+\n+        final Properties consumerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()),\n+                mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName())\n+            )\n+        );\n+\n+\n+        try (final KafkaStreams kafkaStreams0 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final KafkaStreams kafkaStreams1 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final Consumer<String, String> consumer = new KafkaConsumer<>(consumerProperties)) {\n+            kafkaStreams0.start();\n+\n+            // wait until all the input records are in the changelog\n+            TestUtils.waitForCondition(\n+                () -> getChangelogOffsetSum(changelogTopicPartitions, consumer) == 1000,\n+                120_000L,\n+                () -> \"Input records haven't all been written to the changelog: \" + getChangelogOffsetSum(changelogTopicPartitions, consumer)\n+            );\n+\n+            final AtomicLong instance1TotalRestored = new AtomicLong(-1);\n+            final AtomicLong instance1NumRestored = new AtomicLong(-1);\n+            final CountDownLatch restoreCompleteLatch = new CountDownLatch(1);\n+            kafkaStreams1.setGlobalStateRestoreListener(new StateRestoreListener() {\n+                @Override\n+                public void onRestoreStart(final TopicPartition topicPartition,\n+                                           final String storeName,\n+                                           final long startingOffset,\n+                                           final long endingOffset) {\n+                }\n+\n+                @Override\n+                public void onBatchRestored(final TopicPartition topicPartition,\n+                                            final String storeName,\n+                                            final long batchEndOffset,\n+                                            final long numRestored) {\n+                    instance1NumRestored.accumulateAndGet(\n+                        numRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                }\n+\n+                @Override\n+                public void onRestoreEnd(final TopicPartition topicPartition,\n+                                         final String storeName,\n+                                         final long totalRestored) {\n+                    instance1TotalRestored.accumulateAndGet(\n+                        totalRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                    restoreCompleteLatch.countDown();\n+                }\n+            });\n+            final int assignmentsBeforeScaleOut = assignmentsCompleted.get();\n+            final AtomicInteger assignmentsAfterScaleOut = new AtomicInteger(0);\n+            kafkaStreams1.start();\n+            TestUtils.waitForCondition(\n+                () -> {\n+                    assignmentLock.lock();\n+                    try {\n+                        assignmentsAfterScaleOut.set(assignmentsCompleted.get());\n+                        if (assignmentsBeforeScaleOut < assignmentsAfterScaleOut.get()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 206}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4NDIzOTk1", "url": "https://github.com/apache/kafka/pull/8818#pullrequestreview-428423995", "createdAt": "2020-06-10T20:59:41Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMDo1OTo0MVrOGiGIEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMDo1OTo0MVrOGiGIEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwNTEzNw==", "bodyText": "Should the assignment not because stable eventually again? Ie, are we subject to a potential race condition here? (Maybe this assertions is only safe if assignmentsBeforeScaleOut + 1 == assignmentsAfterScaleOut ?)", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438405137", "createdAt": "2020-06-10T20:59:41Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + System.currentTimeMillis() + \"_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n+        final String storeName = \"store\" + testId;\n+        final String storeChangelog = appId + \"-store\" + testId + \"-changelog\";\n+        final Set<TopicPartition> changelogTopicPartitions = mkSet(\n+            new TopicPartition(storeChangelog, 0),\n+            new TopicPartition(storeChangelog, 1)\n+        );\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, 2, inputTopic, storeChangelog);\n+\n+        final ReentrantLock assignmentLock = new ReentrantLock();\n+        final AtomicInteger assignmentsCompleted = new AtomicInteger(0);\n+        final AtomicBoolean assignmentStable = new AtomicBoolean(false);\n+        final AssignmentListener assignmentListener =\n+            stable -> {\n+                assignmentLock.lock();\n+                try {\n+                    assignmentsCompleted.incrementAndGet();\n+                    assignmentStable.set(stable);\n+                } finally {\n+                    assignmentLock.unlock();\n+                }\n+            };\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.table(inputTopic, materializedFunction.apply(storeName));\n+        final Topology topology = builder.build();\n+\n+        final Properties producerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ProducerConfig.ACKS_CONFIG, \"all\"),\n+                mkEntry(ProducerConfig.RETRIES_CONFIG, \"0\"),\n+                mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()),\n+                mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName())\n+            )\n+        );\n+\n+        final StringBuilder kiloBuilder = new StringBuilder(1000);\n+        for (int i = 0; i < 1000; i++) {\n+            kiloBuilder.append('0');\n+        }\n+        final String kilo = kiloBuilder.toString();\n+\n+        try (final Producer<String, String> producer = new KafkaProducer<>(producerProperties)) {\n+            for (int i = 0; i < 1000; i++) {\n+                producer.send(new ProducerRecord<>(inputTopic, String.valueOf(i), kilo));\n+            }\n+        }\n+\n+        final Properties consumerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()),\n+                mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName())\n+            )\n+        );\n+\n+\n+        try (final KafkaStreams kafkaStreams0 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final KafkaStreams kafkaStreams1 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final Consumer<String, String> consumer = new KafkaConsumer<>(consumerProperties)) {\n+            kafkaStreams0.start();\n+\n+            // wait until all the input records are in the changelog\n+            TestUtils.waitForCondition(\n+                () -> getChangelogOffsetSum(changelogTopicPartitions, consumer) == 1000,\n+                120_000L,\n+                () -> \"Input records haven't all been written to the changelog: \" + getChangelogOffsetSum(changelogTopicPartitions, consumer)\n+            );\n+\n+            final AtomicLong instance1TotalRestored = new AtomicLong(-1);\n+            final AtomicLong instance1NumRestored = new AtomicLong(-1);\n+            final CountDownLatch restoreCompleteLatch = new CountDownLatch(1);\n+            kafkaStreams1.setGlobalStateRestoreListener(new StateRestoreListener() {\n+                @Override\n+                public void onRestoreStart(final TopicPartition topicPartition,\n+                                           final String storeName,\n+                                           final long startingOffset,\n+                                           final long endingOffset) {\n+                }\n+\n+                @Override\n+                public void onBatchRestored(final TopicPartition topicPartition,\n+                                            final String storeName,\n+                                            final long batchEndOffset,\n+                                            final long numRestored) {\n+                    instance1NumRestored.accumulateAndGet(\n+                        numRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                }\n+\n+                @Override\n+                public void onRestoreEnd(final TopicPartition topicPartition,\n+                                         final String storeName,\n+                                         final long totalRestored) {\n+                    instance1TotalRestored.accumulateAndGet(\n+                        totalRestored,\n+                        (prev, restored) -> prev == -1 ? restored : prev + restored\n+                    );\n+                    restoreCompleteLatch.countDown();\n+                }\n+            });\n+            final int assignmentsBeforeScaleOut = assignmentsCompleted.get();\n+            final AtomicInteger assignmentsAfterScaleOut = new AtomicInteger(0);\n+            kafkaStreams1.start();\n+            TestUtils.waitForCondition(\n+                () -> {\n+                    assignmentLock.lock();\n+                    try {\n+                        assignmentsAfterScaleOut.set(assignmentsCompleted.get());\n+                        if (assignmentsBeforeScaleOut < assignmentsAfterScaleOut.get()) {\n+                            // the first assignment after adding a node should be unstable while we warm up the state.\n+                            assertThat(assignmentStable.get(), is(false));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 208}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4NDI2MDkz", "url": "https://github.com/apache/kafka/pull/8818#pullrequestreview-428426093", "createdAt": "2020-06-10T21:02:50Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMTowMjo1MVrOGiGOhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMTowMjo1MVrOGiGOhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwNjc5MQ==", "bodyText": "This should never be called -- would it be simpler if the just throw an exception if and of the listener methods is called?", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438406791", "createdAt": "2020-06-10T21:02:51Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/HighAvailabilityTaskAssignorIntegrationTest.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.Topology;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.processor.StateRestoreListener;\n+import org.apache.kafka.streams.processor.internals.assignment.AssignorConfiguration.AssignmentListener;\n+import org.apache.kafka.streams.processor.internals.assignment.HighAvailabilityTaskAssignor;\n+import org.apache.kafka.streams.state.KeyValueStore;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.util.Collection;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.locks.ReentrantLock;\n+import java.util.function.Function;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.common.utils.Utils.mkProperties;\n+import static org.apache.kafka.common.utils.Utils.mkSet;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.Matchers.is;\n+\n+@Category(IntegrationTest.class)\n+public class HighAvailabilityTaskAssignorIntegrationTest {\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndInMemoryStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.inMemoryKeyValueStore(storeName)));\n+    }\n+\n+    @Test\n+    public void shouldScaleOutWithWarmupTasksAndPersistentStores() throws InterruptedException {\n+        // NB: this test takes at least a minute to run, because it needs a probing rebalance, and the minimum\n+        // value is one minute\n+        shouldScaleOutWithWarmupTasks(storeName -> Materialized.as(Stores.persistentKeyValueStore(storeName)));\n+    }\n+\n+    private void shouldScaleOutWithWarmupTasks(final Function<String, Materialized<Object, Object, KeyValueStore<Bytes, byte[]>>> materializedFunction) throws InterruptedException {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        final String appId = \"appId_\" + System.currentTimeMillis() + \"_\" + testId;\n+        final String inputTopic = \"input\" + testId;\n+        final String storeName = \"store\" + testId;\n+        final String storeChangelog = appId + \"-store\" + testId + \"-changelog\";\n+        final Set<TopicPartition> changelogTopicPartitions = mkSet(\n+            new TopicPartition(storeChangelog, 0),\n+            new TopicPartition(storeChangelog, 1)\n+        );\n+\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, 2, inputTopic, storeChangelog);\n+\n+        final ReentrantLock assignmentLock = new ReentrantLock();\n+        final AtomicInteger assignmentsCompleted = new AtomicInteger(0);\n+        final AtomicBoolean assignmentStable = new AtomicBoolean(false);\n+        final AssignmentListener assignmentListener =\n+            stable -> {\n+                assignmentLock.lock();\n+                try {\n+                    assignmentsCompleted.incrementAndGet();\n+                    assignmentStable.set(stable);\n+                } finally {\n+                    assignmentLock.unlock();\n+                }\n+            };\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.table(inputTopic, materializedFunction.apply(storeName));\n+        final Topology topology = builder.build();\n+\n+        final Properties producerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ProducerConfig.ACKS_CONFIG, \"all\"),\n+                mkEntry(ProducerConfig.RETRIES_CONFIG, \"0\"),\n+                mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()),\n+                mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName())\n+            )\n+        );\n+\n+        final StringBuilder kiloBuilder = new StringBuilder(1000);\n+        for (int i = 0; i < 1000; i++) {\n+            kiloBuilder.append('0');\n+        }\n+        final String kilo = kiloBuilder.toString();\n+\n+        try (final Producer<String, String> producer = new KafkaProducer<>(producerProperties)) {\n+            for (int i = 0; i < 1000; i++) {\n+                producer.send(new ProducerRecord<>(inputTopic, String.valueOf(i), kilo));\n+            }\n+        }\n+\n+        final Properties consumerProperties = mkProperties(\n+            mkMap(\n+                mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()),\n+                mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName())\n+            )\n+        );\n+\n+\n+        try (final KafkaStreams kafkaStreams0 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final KafkaStreams kafkaStreams1 = new KafkaStreams(topology, streamsProperties(appId, assignmentListener));\n+             final Consumer<String, String> consumer = new KafkaConsumer<>(consumerProperties)) {\n+            kafkaStreams0.start();\n+\n+            // wait until all the input records are in the changelog\n+            TestUtils.waitForCondition(\n+                () -> getChangelogOffsetSum(changelogTopicPartitions, consumer) == 1000,\n+                120_000L,\n+                () -> \"Input records haven't all been written to the changelog: \" + getChangelogOffsetSum(changelogTopicPartitions, consumer)\n+            );\n+\n+            final AtomicLong instance1TotalRestored = new AtomicLong(-1);\n+            final AtomicLong instance1NumRestored = new AtomicLong(-1);\n+            final CountDownLatch restoreCompleteLatch = new CountDownLatch(1);\n+            kafkaStreams1.setGlobalStateRestoreListener(new StateRestoreListener() {\n+                @Override\n+                public void onRestoreStart(final TopicPartition topicPartition,\n+                                           final String storeName,\n+                                           final long startingOffset,\n+                                           final long endingOffset) {\n+                }\n+\n+                @Override\n+                public void onBatchRestored(final TopicPartition topicPartition,\n+                                            final String storeName,\n+                                            final long batchEndOffset,\n+                                            final long numRestored) {\n+                    instance1NumRestored.accumulateAndGet(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 181}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4NDI2Njc3", "url": "https://github.com/apache/kafka/pull/8818#pullrequestreview-428426677", "createdAt": "2020-06-10T21:03:47Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMTowMzo0N1rOGiGQbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQyMTowMzo0N1rOGiGQbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQwNzI3OA==", "bodyText": "Ugly. Old formatting is nicer.", "url": "https://github.com/apache/kafka/pull/8818#discussion_r438407278", "createdAt": "2020-06-10T21:03:47Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/TaskAssignorIntegrationTest.java", "diffHunk": "@@ -59,7 +61,8 @@\n     public TestName testName = new TestName();\n \n     // Just a dummy implementation so we can check the config\n-    public static final class MyTaskAssignor extends HighAvailabilityTaskAssignor implements TaskAssignor { }\n+    public static final class MyTaskAssignor extends HighAvailabilityTaskAssignor implements TaskAssignor {\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ebac2fb07bf99d47de80acc81bb381d2bd6a8988", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/ebac2fb07bf99d47de80acc81bb381d2bd6a8988", "committedDate": "2020-06-11T03:02:04Z", "message": "KAFKA-10086: Integration test for ensuring warmups are effective"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "755800864fdc46257bc9c4a61b5815a17808f4b0", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/755800864fdc46257bc9c4a61b5815a17808f4b0", "committedDate": "2020-06-11T03:03:04Z", "message": "formatting"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "49fabce40cdb1ed4723cb8438c582ee12e9f3321", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/49fabce40cdb1ed4723cb8438c582ee12e9f3321", "committedDate": "2020-06-11T03:12:01Z", "message": "checkstyle"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4NTY0Mzk5", "url": "https://github.com/apache/kafka/pull/8818#pullrequestreview-428564399", "createdAt": "2020-06-11T03:15:34Z", "commit": {"oid": "49fabce40cdb1ed4723cb8438c582ee12e9f3321"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ca56b56483788f64c3f12d7eddfcc0c87fe036c4", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/ca56b56483788f64c3f12d7eddfcc0c87fe036c4", "committedDate": "2020-06-11T03:33:46Z", "message": "more style"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 892, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}