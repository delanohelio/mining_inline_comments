{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY3NzIwNzMw", "number": 9178, "reviewThreads": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMjozMzo0MlrOEh4atA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNzowMjoyMlrOEiNfew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTYyODA0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMjozMzo0MlrOHPcjJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMjozMzo0MlrOHPcjJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk1ODQzOA==", "bodyText": "typo absooute", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485958438", "createdAt": "2020-09-09T22:33:42Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1698,8 +1698,12 @@ class ReplicaManager(val config: KafkaConfig,\n     Partition.removeMetrics(tp)\n   }\n \n-  // logDir should be an absolute path\n-  // sendZkNotification is needed for unit test\n+  /**\n+   * The log directory failure handler for the replica\n+   *\n+   * @param dir                     the absooute path of the log directory", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb1ad9cb612218bb793dd294b552721bcb89dd02"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTYyOTI2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogManager.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMjozNDoxMlrOHPcj5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMjozNDoxMlrOHPcj5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk1ODYyOA==", "bodyText": "typo diretory", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485958628", "createdAt": "2020-09-09T22:34:12Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -184,7 +184,11 @@ class LogManager(logDirs: Seq[File],\n     numRecoveryThreadsPerDataDir = newSize\n   }\n \n-  // dir should be an absolute path\n+  /**\n+   * The log diretory failure handler. It'll remove all the checkpoint files located in the directory", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb1ad9cb612218bb793dd294b552721bcb89dd02"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTY1ODg2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMjo0NzozM1rOHPc1pA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxMTozNTowN1rOHPvWsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2MzE3Mg==", "bodyText": "Could we make topicPartitionToBeRemoved as Option[TopicPartition]?", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485963172", "createdAt": "2020-09-09T22:47:33Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -354,12 +354,24 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)]): Unit = {\n+  /**\n+   * Update checkpoint file, or removing topics and partitions that no longer exist\n+   *\n+   * @param dataDir                       The File object to be updated\n+   * @param update                        The [TopicPartition, Long] map data to be updated. pass \"none\" if doing remove, not add\n+   * @param topicPartitionToBeRemoved     The TopicPartition to be removed\n+   */\n+  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: TopicPartition = null): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb1ad9cb612218bb793dd294b552721bcb89dd02"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjI2NjU0Nw==", "bodyText": "Updated. Thanks.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486266547", "createdAt": "2020-09-10T11:35:07Z", "author": {"login": "showuon"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -354,12 +354,24 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)]): Unit = {\n+  /**\n+   * Update checkpoint file, or removing topics and partitions that no longer exist\n+   *\n+   * @param dataDir                       The File object to be updated\n+   * @param update                        The [TopicPartition, Long] map data to be updated. pass \"none\" if doing remove, not add\n+   * @param topicPartitionToBeRemoved     The TopicPartition to be removed\n+   */\n+  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: TopicPartition = null): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2MzE3Mg=="}, "originalCommit": {"oid": "bb1ad9cb612218bb793dd294b552721bcb89dd02"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTY2NTI0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMjo1MDoxNVrOHPc5ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxMTozNToyNFrOHPvXTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2NDEzOA==", "bodyText": "updatedOffset is not being used.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485964138", "createdAt": "2020-09-09T22:50:15Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -354,12 +354,24 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)]): Unit = {\n+  /**\n+   * Update checkpoint file, or removing topics and partitions that no longer exist\n+   *\n+   * @param dataDir                       The File object to be updated\n+   * @param update                        The [TopicPartition, Long] map data to be updated. pass \"none\" if doing remove, not add\n+   * @param topicPartitionToBeRemoved     The TopicPartition to be removed\n+   */\n+  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: TopicPartition = null): Unit = {\n     inLock(lock) {\n       val checkpoint = checkpoints(dataDir)\n       if (checkpoint != null) {\n         try {\n-          val existing = checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) } ++ update\n+          val existing = update match {\n+            case Some(updatedOffset) =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb1ad9cb612218bb793dd294b552721bcb89dd02"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjI2NjcwMQ==", "bodyText": "Nice catch. Thanks.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486266701", "createdAt": "2020-09-10T11:35:24Z", "author": {"login": "showuon"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -354,12 +354,24 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)]): Unit = {\n+  /**\n+   * Update checkpoint file, or removing topics and partitions that no longer exist\n+   *\n+   * @param dataDir                       The File object to be updated\n+   * @param update                        The [TopicPartition, Long] map data to be updated. pass \"none\" if doing remove, not add\n+   * @param topicPartitionToBeRemoved     The TopicPartition to be removed\n+   */\n+  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: TopicPartition = null): Unit = {\n     inLock(lock) {\n       val checkpoint = checkpoints(dataDir)\n       if (checkpoint != null) {\n         try {\n-          val existing = checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) } ++ update\n+          val existing = update match {\n+            case Some(updatedOffset) =>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2NDEzOA=="}, "originalCommit": {"oid": "bb1ad9cb612218bb793dd294b552721bcb89dd02"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTY2NjM4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMjo1MDo1MlrOHPc6KQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMjo1MDo1MlrOHPc6KQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2NDMyOQ==", "bodyText": "typo direcotory", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485964329", "createdAt": "2020-09-09T22:50:52Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -369,13 +381,21 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n+  /**\n+   * alter the checkpoint directory for the topicPartition, to remove the data in sourceLogDir, and add the data in destLogDir\n+   */\n   def alterCheckpointDir(topicPartition: TopicPartition, sourceLogDir: File, destLogDir: File): Unit = {\n     inLock(lock) {\n       try {\n         checkpoints.get(sourceLogDir).flatMap(_.read().get(topicPartition)) match {\n           case Some(offset) =>\n-            // Remove this partition from the checkpoint file in the source log directory\n-            updateCheckpoints(sourceLogDir, None)\n+            debug(s\"Removing the partition offset data in checkpoint file for '${topicPartition}' \" +\n+              s\"from ${sourceLogDir.getAbsoluteFile} direcotory.\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb1ad9cb612218bb793dd294b552721bcb89dd02"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTY2ODc4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMjo1MTo1OVrOHPc7ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMjo1MTo1OVrOHPc7ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2NDcwNw==", "bodyText": "removing => remove", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485964707", "createdAt": "2020-09-09T22:51:59Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -354,12 +354,24 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)]): Unit = {\n+  /**\n+   * Update checkpoint file, or removing topics and partitions that no longer exist", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb1ad9cb612218bb793dd294b552721bcb89dd02"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTY3NDQyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMjo1NDo1M1rOHPc_Og==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMjo1NDo1M1rOHPc_Og==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2NTYyNg==", "bodyText": "Stop the cleaning logs => Stop cleaning logs", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485965626", "createdAt": "2020-09-09T22:54:53Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -393,13 +413,21 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n+  /**\n+   * Stop the cleaning logs in the provided directory", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb1ad9cb612218bb793dd294b552721bcb89dd02"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTY4MTg2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/checkpoints/CheckpointFile.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMjo1ODoyNVrOHPdDqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxMTozNzoyM1rOHPvbYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2Njc2MQ==", "bodyText": "We now have 2 different formats for checkpoint files, one for OffsetCheckpointFile and another for LeaderEpochCheckpointFile. Perhaps we can add the above comment to the appropriate class.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r485966761", "createdAt": "2020-09-09T22:58:25Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/checkpoints/CheckpointFile.scala", "diffHunk": "@@ -75,6 +75,17 @@ class CheckpointReadBuffer[T](location: String,\n   }\n }\n \n+/**\n+ * This class interacts with the checkpoint file to read or write [TopicPartition, Offset] entries\n+ *\n+ * The format in the checkpoint file is like this:\n+ *  -----checkpoint file content------\n+ *  0                <- OffsetCheckpointFile.currentVersion\n+ *  2                <- following entries size\n+ *  tp1  par1  1     <- the format is: TOPIC  PARTITION  OFFSET\n+ *  tp1  par2  2\n+ *  -----checkpoint file end----------\n+ */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb1ad9cb612218bb793dd294b552721bcb89dd02"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjI2Nzc0NA==", "bodyText": "Thanks for reminding! I've moved my comments to OffsetCheckpointFile. And add some format comments in LeaderEpochCheckpointFile. Thanks.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486267744", "createdAt": "2020-09-10T11:37:23Z", "author": {"login": "showuon"}, "path": "core/src/main/scala/kafka/server/checkpoints/CheckpointFile.scala", "diffHunk": "@@ -75,6 +75,17 @@ class CheckpointReadBuffer[T](location: String,\n   }\n }\n \n+/**\n+ * This class interacts with the checkpoint file to read or write [TopicPartition, Offset] entries\n+ *\n+ * The format in the checkpoint file is like this:\n+ *  -----checkpoint file content------\n+ *  0                <- OffsetCheckpointFile.currentVersion\n+ *  2                <- following entries size\n+ *  tp1  par1  1     <- the format is: TOPIC  PARTITION  OFFSET\n+ *  tp1  par2  2\n+ *  -----checkpoint file end----------\n+ */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTk2Njc2MQ=="}, "originalCommit": {"oid": "bb1ad9cb612218bb793dd294b552721bcb89dd02"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MjgxMTgyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNTo1NTo1OFrOHP61pQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQwMzowODoyN1rOHQMrxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ1NDY5Mw==", "bodyText": "To be consistent, perhaps change update to partitionToUpdateOrAdd and topicPartitionToBeRemoved to partitionToRemove?", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486454693", "createdAt": "2020-09-10T15:55:58Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -354,12 +354,30 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)]): Unit = {\n+  /**\n+   * Update checkpoint file, or remove topics and partitions that no longer exist\n+   *\n+   * @param dataDir                       The File object to be updated\n+   * @param update                        The [TopicPartition, Long] map data to be updated. pass \"none\" if doing remove, not add\n+   * @param topicPartitionToBeRemoved     The TopicPartition to be removed\n+   */\n+  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: Option[TopicPartition] = None): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc0NzA3Nw==", "bodyText": "OK", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486747077", "createdAt": "2020-09-11T03:08:27Z", "author": {"login": "showuon"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -354,12 +354,30 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)]): Unit = {\n+  /**\n+   * Update checkpoint file, or remove topics and partitions that no longer exist\n+   *\n+   * @param dataDir                       The File object to be updated\n+   * @param update                        The [TopicPartition, Long] map data to be updated. pass \"none\" if doing remove, not add\n+   * @param topicPartitionToBeRemoved     The TopicPartition to be removed\n+   */\n+  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: Option[TopicPartition] = None): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ1NDY5Mw=="}, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0Mjk4MDExOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjozNTo1OFrOHP8f2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQwMzowODo1MVrOHQMsFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4MTg4Mg==", "bodyText": "This method assumes that only one of update and topicPartitionToBeRemoved will be set. Perhaps we could just handle the more general case that both could be set?", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486481882", "createdAt": "2020-09-10T16:35:58Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -355,22 +355,28 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n   }\n \n   /**\n-   * Update checkpoint file, or removing topics and partitions that no longer exist\n+   * Update checkpoint file, or remove topics and partitions that no longer exist\n    *\n    * @param dataDir                       The File object to be updated\n    * @param update                        The [TopicPartition, Long] map data to be updated. pass \"none\" if doing remove, not add\n    * @param topicPartitionToBeRemoved     The TopicPartition to be removed\n    */\n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: TopicPartition = null): Unit = {\n+  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: Option[TopicPartition] = None): Unit = {\n     inLock(lock) {\n       val checkpoint = checkpoints(dataDir)\n       if (checkpoint != null) {\n         try {\n           val existing = update match {\n             case Some(updatedOffset) =>\n-              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap ++ update\n+              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap + updatedOffset\n             case None =>\n-              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap - topicPartitionToBeRemoved\n+              topicPartitionToBeRemoved match {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc0NzE1OQ==", "bodyText": "Good suggestion! Updated.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486747159", "createdAt": "2020-09-11T03:08:51Z", "author": {"login": "showuon"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -355,22 +355,28 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n   }\n \n   /**\n-   * Update checkpoint file, or removing topics and partitions that no longer exist\n+   * Update checkpoint file, or remove topics and partitions that no longer exist\n    *\n    * @param dataDir                       The File object to be updated\n    * @param update                        The [TopicPartition, Long] map data to be updated. pass \"none\" if doing remove, not add\n    * @param topicPartitionToBeRemoved     The TopicPartition to be removed\n    */\n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: TopicPartition = null): Unit = {\n+  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: Option[TopicPartition] = None): Unit = {\n     inLock(lock) {\n       val checkpoint = checkpoints(dataDir)\n       if (checkpoint != null) {\n         try {\n           val existing = update match {\n             case Some(updatedOffset) =>\n-              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap ++ update\n+              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap + updatedOffset\n             case None =>\n-              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap - topicPartitionToBeRemoved\n+              topicPartitionToBeRemoved match {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4MTg4Mg=="}, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0Mjk4MTU5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjozNjoxOVrOHP8gxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQwMzowODo1OVrOHQMsRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4MjExOQ==", "bodyText": "Do we need to log this?", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486482119", "createdAt": "2020-09-10T16:36:19Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -355,22 +355,28 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n   }\n \n   /**\n-   * Update checkpoint file, or removing topics and partitions that no longer exist\n+   * Update checkpoint file, or remove topics and partitions that no longer exist\n    *\n    * @param dataDir                       The File object to be updated\n    * @param update                        The [TopicPartition, Long] map data to be updated. pass \"none\" if doing remove, not add\n    * @param topicPartitionToBeRemoved     The TopicPartition to be removed\n    */\n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: TopicPartition = null): Unit = {\n+  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: Option[TopicPartition] = None): Unit = {\n     inLock(lock) {\n       val checkpoint = checkpoints(dataDir)\n       if (checkpoint != null) {\n         try {\n           val existing = update match {\n             case Some(updatedOffset) =>\n-              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap ++ update\n+              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap + updatedOffset\n             case None =>\n-              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap - topicPartitionToBeRemoved\n+              topicPartitionToBeRemoved match {\n+                case Some(topicPartion) =>\n+                  checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap - topicPartion\n+                case None =>\n+                  info(s\"Nothing added or removed for ${dataDir.getAbsoluteFile} directory in updateCheckpoints.\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc0NzIwNA==", "bodyText": "Removed. Thanks.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486747204", "createdAt": "2020-09-11T03:08:59Z", "author": {"login": "showuon"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -355,22 +355,28 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n   }\n \n   /**\n-   * Update checkpoint file, or removing topics and partitions that no longer exist\n+   * Update checkpoint file, or remove topics and partitions that no longer exist\n    *\n    * @param dataDir                       The File object to be updated\n    * @param update                        The [TopicPartition, Long] map data to be updated. pass \"none\" if doing remove, not add\n    * @param topicPartitionToBeRemoved     The TopicPartition to be removed\n    */\n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: TopicPartition = null): Unit = {\n+  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)], topicPartitionToBeRemoved: Option[TopicPartition] = None): Unit = {\n     inLock(lock) {\n       val checkpoint = checkpoints(dataDir)\n       if (checkpoint != null) {\n         try {\n           val existing = update match {\n             case Some(updatedOffset) =>\n-              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap ++ update\n+              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap + updatedOffset\n             case None =>\n-              checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap - topicPartitionToBeRemoved\n+              topicPartitionToBeRemoved match {\n+                case Some(topicPartion) =>\n+                  checkpoint.read().filter { case (tp, _) => logs.keys.contains(tp) }.toMap - topicPartion\n+                case None =>\n+                  info(s\"Nothing added or removed for ${dataDir.getAbsoluteFile} directory in updateCheckpoints.\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4MjExOQ=="}, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0Mjk4NjY5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjozNzo0MVrOHP8j_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQwMzoxMDozMVrOHQMtyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4Mjk0Mg==", "bodyText": "Could we use the named param for update in updateCheckpoints() below and other places to make it clear?", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486482942", "createdAt": "2020-09-10T16:37:41Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -390,9 +396,9 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n         checkpoints.get(sourceLogDir).flatMap(_.read().get(topicPartition)) match {\n           case Some(offset) =>\n             debug(s\"Removing the partition offset data in checkpoint file for '${topicPartition}' \" +\n-              s\"from ${sourceLogDir.getAbsoluteFile} direcotory.\")\n+              s\"from ${sourceLogDir.getAbsoluteFile} directory.\")\n             // Remove this partition data from the checkpoint file in the source log directory\n-            updateCheckpoints(sourceLogDir, None, topicPartitionToBeRemoved = topicPartition)\n+            updateCheckpoints(sourceLogDir, None, topicPartitionToBeRemoved = Some(topicPartition))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc0NzU5Mg==", "bodyText": "Good suggestion. I make the default value of 2nd parameter partitionToUpdateOrAdd to be None, so here, I can just call with 2 params: updateCheckpoints(sourceLogDir, topicPartitionToBeRemoved = Some(topicPartition)), and other places as well. Thanks.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486747592", "createdAt": "2020-09-11T03:10:31Z", "author": {"login": "showuon"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -390,9 +396,9 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n         checkpoints.get(sourceLogDir).flatMap(_.read().get(topicPartition)) match {\n           case Some(offset) =>\n             debug(s\"Removing the partition offset data in checkpoint file for '${topicPartition}' \" +\n-              s\"from ${sourceLogDir.getAbsoluteFile} direcotory.\")\n+              s\"from ${sourceLogDir.getAbsoluteFile} directory.\")\n             // Remove this partition data from the checkpoint file in the source log directory\n-            updateCheckpoints(sourceLogDir, None, topicPartitionToBeRemoved = topicPartition)\n+            updateCheckpoints(sourceLogDir, None, topicPartitionToBeRemoved = Some(topicPartition))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4Mjk0Mg=="}, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzAwNTgyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo0MjozNlrOHP8wBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo0MjozNlrOHP8wBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4NjAyMg==", "bodyText": "Truncate the checkpoint file for the given partition => Truncate the checkpointed offset for the given partition", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486486022", "createdAt": "2020-09-10T16:42:36Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -393,13 +419,21 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n+  /**\n+   * Stop cleaning logs in the provided directory\n+   *\n+   * @param dir     the absolute path of the log dir\n+   */\n   def handleLogDirFailure(dir: String): Unit = {\n     warn(s\"Stopping cleaning logs in dir $dir\")\n     inLock(lock) {\n       checkpoints = checkpoints.filter { case (k, _) => k.getAbsolutePath != dir }\n     }\n   }\n \n+  /**\n+   * Truncate the checkpoint file for the given partition if its checkpointed offset is larger than the given offset", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzAxMjg3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/checkpoints/LeaderEpochCheckpointFile.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo0NDozNFrOHP80ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQwMzoxODozOFrOHQM1sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4NzE3OA==", "bodyText": "The map stores the first offset in each epoch.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486487178", "createdAt": "2020-09-10T16:44:34Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/checkpoints/LeaderEpochCheckpointFile.scala", "diffHunk": "@@ -52,8 +52,16 @@ object LeaderEpochCheckpointFile {\n }\n \n /**\n-  * This class persists a map of (LeaderEpoch => Offsets) to a file (for a certain replica)\n-  */\n+ * This class persists a map of (LeaderEpoch => Offsets) to a file (for a certain replica)\n+ *\n+ * The format in the LeaderEpoch checkpoint file is like this:\n+ * -----checkpoint file begin------\n+ * 0                <- LeaderEpochCheckpointFile.currentVersion\n+ * 2                <- following entries size\n+ * 0  1     <- the format is: leader_epoch(int32) end_offset(int64)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc0OTYxOA==", "bodyText": "You are right. I referenced the KIP-101 to document it. After your reminding, I found the KIP is wrong. In the description, it said it's \"Start offset\", but in the table below, it becomes \"end offset\". I confirmed this is typo. I also updated the KIP as well. Thank you.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486749618", "createdAt": "2020-09-11T03:18:38Z", "author": {"login": "showuon"}, "path": "core/src/main/scala/kafka/server/checkpoints/LeaderEpochCheckpointFile.scala", "diffHunk": "@@ -52,8 +52,16 @@ object LeaderEpochCheckpointFile {\n }\n \n /**\n-  * This class persists a map of (LeaderEpoch => Offsets) to a file (for a certain replica)\n-  */\n+ * This class persists a map of (LeaderEpoch => Offsets) to a file (for a certain replica)\n+ *\n+ * The format in the LeaderEpoch checkpoint file is like this:\n+ * -----checkpoint file begin------\n+ * 0                <- LeaderEpochCheckpointFile.currentVersion\n+ * 2                <- following entries size\n+ * 0  1     <- the format is: leader_epoch(int32) end_offset(int64)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4NzE3OA=="}, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzAxODMyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleaner.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo0NTo1M1rOHP834A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQwMzoyMjozN1rOHQM5lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4ODAzMg==", "bodyText": "Could we used named param for topicPartitionToBeRemoved?", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486488032", "createdAt": "2020-09-10T16:45:53Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleaner.scala", "diffHunk": "@@ -203,16 +203,24 @@ class LogCleaner(initialConfig: CleanerConfig,\n   }\n \n   /**\n-   * Update checkpoint file, removing topics and partitions that no longer exist\n+   * Update checkpoint file to remove topics and partitions that no longer exist\n    */\n-  def updateCheckpoints(dataDir: File): Unit = {\n-    cleanerManager.updateCheckpoints(dataDir, update=None)\n+  def updateCheckpoints(dataDir: File, topicPartitionToBeRemoved: Option[TopicPartition] = None): Unit = {\n+    cleanerManager.updateCheckpoints(dataDir, update=None, topicPartitionToBeRemoved)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc1MDYxMg==", "bodyText": "Sure. I also removed the 2nd param update=None", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486750612", "createdAt": "2020-09-11T03:22:37Z", "author": {"login": "showuon"}, "path": "core/src/main/scala/kafka/log/LogCleaner.scala", "diffHunk": "@@ -203,16 +203,24 @@ class LogCleaner(initialConfig: CleanerConfig,\n   }\n \n   /**\n-   * Update checkpoint file, removing topics and partitions that no longer exist\n+   * Update checkpoint file to remove topics and partitions that no longer exist\n    */\n-  def updateCheckpoints(dataDir: File): Unit = {\n-    cleanerManager.updateCheckpoints(dataDir, update=None)\n+  def updateCheckpoints(dataDir: File, topicPartitionToBeRemoved: Option[TopicPartition] = None): Unit = {\n+    cleanerManager.updateCheckpoints(dataDir, update=None, topicPartitionToBeRemoved)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4ODAzMg=="}, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzAyNzE0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo0ODoxNVrOHP89Ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo0ODoxNVrOHP89Ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ4OTQwMw==", "bodyText": "Perhaps tweaks the comment to \"Update checkpoint file, adding or removing partitions if necessary.\"?", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486489403", "createdAt": "2020-09-10T16:48:15Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -354,12 +354,30 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n     }\n   }\n \n-  def updateCheckpoints(dataDir: File, update: Option[(TopicPartition, Long)]): Unit = {\n+  /**\n+   * Update checkpoint file, or remove topics and partitions that no longer exist", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzA0MDk4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogManager.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo1MjowNVrOHP9GBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo1MjowNVrOHP9GBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5MTY1NA==", "bodyText": "It'll remove all the checkpoint files located in the directory  => It will stop log cleaning in that directory.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486491654", "createdAt": "2020-09-10T16:52:05Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -184,7 +184,11 @@ class LogManager(logDirs: Seq[File],\n     numRecoveryThreadsPerDataDir = newSize\n   }\n \n-  // dir should be an absolute path\n+  /**\n+   * The log directory failure handler. It'll remove all the checkpoint files located in the directory", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzA0Nzc3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo1Mzo1MVrOHP9KTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo1Mzo1MVrOHP9KTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5Mjc0OQ==", "bodyText": "typo notificiation", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486492749", "createdAt": "2020-09-10T16:53:51Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1729,8 +1729,12 @@ class ReplicaManager(val config: KafkaConfig,\n     Partition.removeMetrics(tp)\n   }\n \n-  // logDir should be an absolute path\n-  // sendZkNotification is needed for unit test\n+  /**\n+   * The log directory failure handler for the replica\n+   *\n+   * @param dir                     the absolute path of the log directory\n+   * @param sendZkNotification      check if we need to send notificiation to zookeeper node (needed for unit test)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzA1NTE5OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo1NTo0NlrOHP9PCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQwMzoyNDoyNFrOHQM7Rg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5Mzk2Mg==", "bodyText": "Should we handle topicPartitionToBeRemoved or assert it is None?", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486493962", "createdAt": "2020-09-10T16:55:46Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -55,7 +59,8 @@ class LogCleanerManagerTest extends Logging {\n       cleanerCheckpoints.toMap\n     }\n \n-    override def updateCheckpoints(dataDir: File, update: Option[(TopicPartition,Long)]): Unit = {\n+    override def updateCheckpoints(dataDir: File, update: Option[(TopicPartition,Long)],\n+                                   topicPartitionToBeRemoved: Option[TopicPartition] = None): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc1MTA0Ng==", "bodyText": "I assert it. Thanks for reminding.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486751046", "createdAt": "2020-09-11T03:24:24Z", "author": {"login": "showuon"}, "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -55,7 +59,8 @@ class LogCleanerManagerTest extends Logging {\n       cleanerCheckpoints.toMap\n     }\n \n-    override def updateCheckpoints(dataDir: File, update: Option[(TopicPartition,Long)]): Unit = {\n+    override def updateCheckpoints(dataDir: File, update: Option[(TopicPartition,Long)],\n+                                   topicPartitionToBeRemoved: Option[TopicPartition] = None): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5Mzk2Mg=="}, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzA2NzA1OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo1ODo0MlrOHP9WPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo1ODo0MlrOHP9WPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NTgwNw==", "bodyText": "Could we used named param for update? Ditto below.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486495807", "createdAt": "2020-09-10T16:58:42Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -361,6 +366,93 @@ class LogCleanerManagerTest extends Logging {\n     assertEquals(\"should have 1 logs ready to be deleted\", 1, deletableLog3.size)\n   }\n \n+  @Test\n+  def testUpdateCheckpointsShouldAddOffsetToPartition(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // expect the checkpoint offset is not the expectedOffset before doing updateCheckpoints\n+    assertNotEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).getOrElse(0))\n+\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzA2ODQ4OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo1OTowN1rOHP9XNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo1OTowN1rOHP9XNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NjA1NA==", "bodyText": "expectedOffset => expected offset", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486496054", "createdAt": "2020-09-10T16:59:07Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -361,6 +366,93 @@ class LogCleanerManagerTest extends Logging {\n     assertEquals(\"should have 1 logs ready to be deleted\", 1, deletableLog3.size)\n   }\n \n+  @Test\n+  def testUpdateCheckpointsShouldAddOffsetToPartition(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // expect the checkpoint offset is not the expectedOffset before doing updateCheckpoints\n+    assertNotEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).getOrElse(0))\n+\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    // expect the checkpoint offset is now updated to the expectedOffset after doing updateCheckpoints", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzA3MDM0OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjo1OTozNFrOHP9YXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQwMzoyNDo1MVrOHQM7ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NjM1MA==", "bodyText": "allCleanerCheckpoints.get(topicPartition).get) can just be allCleanerCheckpoints(topicPartition). Ditto below.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486496350", "createdAt": "2020-09-10T16:59:34Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -361,6 +366,93 @@ class LogCleanerManagerTest extends Logging {\n     assertEquals(\"should have 1 logs ready to be deleted\", 1, deletableLog3.size)\n   }\n \n+  @Test\n+  def testUpdateCheckpointsShouldAddOffsetToPartition(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // expect the checkpoint offset is not the expectedOffset before doing updateCheckpoints\n+    assertNotEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).getOrElse(0))\n+\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    // expect the checkpoint offset is now updated to the expectedOffset after doing updateCheckpoints\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).get)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc1MTEzNA==", "bodyText": "Nice refactor! Thanks.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486751134", "createdAt": "2020-09-11T03:24:51Z", "author": {"login": "showuon"}, "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -361,6 +366,93 @@ class LogCleanerManagerTest extends Logging {\n     assertEquals(\"should have 1 logs ready to be deleted\", 1, deletableLog3.size)\n   }\n \n+  @Test\n+  def testUpdateCheckpointsShouldAddOffsetToPartition(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // expect the checkpoint offset is not the expectedOffset before doing updateCheckpoints\n+    assertNotEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).getOrElse(0))\n+\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    // expect the checkpoint offset is now updated to the expectedOffset after doing updateCheckpoints\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).get)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5NjM1MA=="}, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0MzA4MDkxOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNzowMjoyMlrOHP9fBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQwMzoyNzoxOVrOHQM9-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5ODA1Mg==", "bodyText": "It seems the < here should be > and the > two lines below should be <?", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486498052", "createdAt": "2020-09-10T17:02:22Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -361,6 +366,93 @@ class LogCleanerManagerTest extends Logging {\n     assertEquals(\"should have 1 logs ready to be deleted\", 1, deletableLog3.size)\n   }\n \n+  @Test\n+  def testUpdateCheckpointsShouldAddOffsetToPartition(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // expect the checkpoint offset is not the expectedOffset before doing updateCheckpoints\n+    assertNotEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).getOrElse(0))\n+\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    // expect the checkpoint offset is now updated to the expectedOffset after doing updateCheckpoints\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).get)\n+  }\n+\n+  @Test\n+  def testUpdateCheckpointsShouldRemovePartitionData(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // write some data into the cleaner-offset-checkpoint file\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).get)\n+\n+    // updateCheckpoints should remove the topicPartition data in the logDir\n+    cleanerManager.updateCheckpoints(logDir, None, topicPartitionToBeRemoved = Some(topicPartition))\n+    assertTrue(cleanerManager.allCleanerCheckpoints.get(topicPartition).isEmpty)\n+  }\n+\n+  @Test\n+  def testHandleLogDirFailureShouldRemoveDirAndData(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // write some data into the cleaner-offset-checkpoint file in logDir and logDir2\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    cleanerManager.updateCheckpoints(logDir2, Option(topicPartition2, offset))\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).get)\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition2).get)\n+\n+    cleanerManager.handleLogDirFailure(logDir.getAbsolutePath)\n+    // verify the partition data in logDir is gone, and data in logDir2 is still there\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition2).get)\n+    assertTrue(cleanerManager.allCleanerCheckpoints.get(topicPartition).isEmpty)\n+  }\n+\n+  @Test\n+  def testMaybeTruncateCheckpointShouldTruncateData(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+    val lowerOffset = 1L\n+    val higherOffset = 1000L\n+\n+    // write some data into the cleaner-offset-checkpoint file in logDir\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).get)\n+\n+    // we should not truncate the checkpoint data for checkpointed offset < the given offset (higherOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Njc1MTczOA==", "bodyText": "I checked again and I think I was right. The truncate Checkpoint file will happen only when the provided offset smaller than the one the the checkpoint file. So the comment is correct. I just added an equal sign (<=) to make it more accurate. Thanks.", "url": "https://github.com/apache/kafka/pull/9178#discussion_r486751738", "createdAt": "2020-09-11T03:27:19Z", "author": {"login": "showuon"}, "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -361,6 +366,93 @@ class LogCleanerManagerTest extends Logging {\n     assertEquals(\"should have 1 logs ready to be deleted\", 1, deletableLog3.size)\n   }\n \n+  @Test\n+  def testUpdateCheckpointsShouldAddOffsetToPartition(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // expect the checkpoint offset is not the expectedOffset before doing updateCheckpoints\n+    assertNotEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).getOrElse(0))\n+\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    // expect the checkpoint offset is now updated to the expectedOffset after doing updateCheckpoints\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).get)\n+  }\n+\n+  @Test\n+  def testUpdateCheckpointsShouldRemovePartitionData(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // write some data into the cleaner-offset-checkpoint file\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).get)\n+\n+    // updateCheckpoints should remove the topicPartition data in the logDir\n+    cleanerManager.updateCheckpoints(logDir, None, topicPartitionToBeRemoved = Some(topicPartition))\n+    assertTrue(cleanerManager.allCleanerCheckpoints.get(topicPartition).isEmpty)\n+  }\n+\n+  @Test\n+  def testHandleLogDirFailureShouldRemoveDirAndData(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+\n+    // write some data into the cleaner-offset-checkpoint file in logDir and logDir2\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    cleanerManager.updateCheckpoints(logDir2, Option(topicPartition2, offset))\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).get)\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition2).get)\n+\n+    cleanerManager.handleLogDirFailure(logDir.getAbsolutePath)\n+    // verify the partition data in logDir is gone, and data in logDir2 is still there\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition2).get)\n+    assertTrue(cleanerManager.allCleanerCheckpoints.get(topicPartition).isEmpty)\n+  }\n+\n+  @Test\n+  def testMaybeTruncateCheckpointShouldTruncateData(): Unit = {\n+    val records = TestUtils.singletonRecords(\"test\".getBytes, key=\"test\".getBytes)\n+    val log: Log = createLog(records.sizeInBytes * 5, LogConfig.Compact)\n+    val cleanerManager: LogCleanerManager = createCleanerManager(log)\n+    val lowerOffset = 1L\n+    val higherOffset = 1000L\n+\n+    // write some data into the cleaner-offset-checkpoint file in logDir\n+    cleanerManager.updateCheckpoints(logDir, Option(topicPartition, offset))\n+    assertEquals(offset, cleanerManager.allCleanerCheckpoints.get(topicPartition).get)\n+\n+    // we should not truncate the checkpoint data for checkpointed offset < the given offset (higherOffset)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ5ODA1Mg=="}, "originalCommit": {"oid": "5320318eec5cdb938cafd385f45525472370d359"}, "originalPosition": 92}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1920, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}