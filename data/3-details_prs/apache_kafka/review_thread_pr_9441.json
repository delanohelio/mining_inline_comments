{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA0MDMyNzIw", "number": 9441, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0wNVQyMjozOTowMVrOFuPu7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0zMFQxOToxNzo0MFrOF499Lw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg0MDM2NTg5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0wNVQyMjozOTowMVrOJDPMPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yMlQwMTo1NTo0NlrOJNZCww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwNzM3NDM5Nw==", "bodyText": "Hmm.. Why remove the epoch after resignation? It seems like it would be useful to keep tracking it. Maybe it's useful to distinguish the case where the replica is to be deleted?", "url": "https://github.com/apache/kafka/pull/9441#discussion_r607374397", "createdAt": "2021-04-05T22:39:01Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -905,19 +907,33 @@ class GroupCoordinator(val brokerId: Int,\n    *\n    * @param offsetTopicPartitionId The partition we are now leading\n    */\n-  def onElection(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+  def onElection(offsetTopicPartitionId: Int, coordinatorEpoch: Int): Unit = {\n+    val currentEpoch = epochForPartitionId.get(offsetTopicPartitionId)\n+    if (currentEpoch.forall(currentEpoch => coordinatorEpoch > currentEpoch)) {\n+      info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId in epoch $coordinatorEpoch\")\n+      groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+      epochForPartitionId.put(offsetTopicPartitionId, coordinatorEpoch)\n+    } else {\n+      warn(s\"Ignored election as group coordinator for partition $offsetTopicPartitionId \" +\n+        s\"in epoch $coordinatorEpoch since current epoch is $currentEpoch\")\n+    }\n   }\n \n   /**\n    * Unload cached state for the given partition and stop handling requests for groups which map to it.\n    *\n    * @param offsetTopicPartitionId The partition we are no longer leading\n    */\n-  def onResignation(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Resigned as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)\n+  def onResignation(offsetTopicPartitionId: Int, coordinatorEpoch: Option[Int]): Unit = {\n+    val currentEpoch = epochForPartitionId.get(offsetTopicPartitionId)\n+    if (currentEpoch.forall(currentEpoch => currentEpoch <= coordinatorEpoch.getOrElse(Int.MaxValue))) {\n+      info(s\"Resigned as the group coordinator for partition $offsetTopicPartitionId in epoch $coordinatorEpoch\")\n+      groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)\n+      epochForPartitionId.remove(offsetTopicPartitionId)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "afab8f10f9ec0da9460860d98b2eaf0f5a5e4e3e"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNTcyMjUzOQ==", "bodyText": "I guess mostly I did it for symmetry with onElection, however you're right that is doesn't affect correctness. I don't entirely follow your point about how we could use this when the replica is being deleted though. Could you explain?", "url": "https://github.com/apache/kafka/pull/9441#discussion_r615722539", "createdAt": "2021-04-19T10:17:45Z", "author": {"login": "tombentley"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -905,19 +907,33 @@ class GroupCoordinator(val brokerId: Int,\n    *\n    * @param offsetTopicPartitionId The partition we are now leading\n    */\n-  def onElection(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+  def onElection(offsetTopicPartitionId: Int, coordinatorEpoch: Int): Unit = {\n+    val currentEpoch = epochForPartitionId.get(offsetTopicPartitionId)\n+    if (currentEpoch.forall(currentEpoch => coordinatorEpoch > currentEpoch)) {\n+      info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId in epoch $coordinatorEpoch\")\n+      groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+      epochForPartitionId.put(offsetTopicPartitionId, coordinatorEpoch)\n+    } else {\n+      warn(s\"Ignored election as group coordinator for partition $offsetTopicPartitionId \" +\n+        s\"in epoch $coordinatorEpoch since current epoch is $currentEpoch\")\n+    }\n   }\n \n   /**\n    * Unload cached state for the given partition and stop handling requests for groups which map to it.\n    *\n    * @param offsetTopicPartitionId The partition we are no longer leading\n    */\n-  def onResignation(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Resigned as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)\n+  def onResignation(offsetTopicPartitionId: Int, coordinatorEpoch: Option[Int]): Unit = {\n+    val currentEpoch = epochForPartitionId.get(offsetTopicPartitionId)\n+    if (currentEpoch.forall(currentEpoch => currentEpoch <= coordinatorEpoch.getOrElse(Int.MaxValue))) {\n+      info(s\"Resigned as the group coordinator for partition $offsetTopicPartitionId in epoch $coordinatorEpoch\")\n+      groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)\n+      epochForPartitionId.remove(offsetTopicPartitionId)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwNzM3NDM5Nw=="}, "originalCommit": {"oid": "afab8f10f9ec0da9460860d98b2eaf0f5a5e4e3e"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyMTU3MQ==", "bodyText": "The onResignation hook just means we lost leadership. By keeping track of the epoch, we are protected from all potential reorderings.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r618021571", "createdAt": "2021-04-22T01:55:46Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -905,19 +907,33 @@ class GroupCoordinator(val brokerId: Int,\n    *\n    * @param offsetTopicPartitionId The partition we are now leading\n    */\n-  def onElection(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+  def onElection(offsetTopicPartitionId: Int, coordinatorEpoch: Int): Unit = {\n+    val currentEpoch = epochForPartitionId.get(offsetTopicPartitionId)\n+    if (currentEpoch.forall(currentEpoch => coordinatorEpoch > currentEpoch)) {\n+      info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId in epoch $coordinatorEpoch\")\n+      groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+      epochForPartitionId.put(offsetTopicPartitionId, coordinatorEpoch)\n+    } else {\n+      warn(s\"Ignored election as group coordinator for partition $offsetTopicPartitionId \" +\n+        s\"in epoch $coordinatorEpoch since current epoch is $currentEpoch\")\n+    }\n   }\n \n   /**\n    * Unload cached state for the given partition and stop handling requests for groups which map to it.\n    *\n    * @param offsetTopicPartitionId The partition we are no longer leading\n    */\n-  def onResignation(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Resigned as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)\n+  def onResignation(offsetTopicPartitionId: Int, coordinatorEpoch: Option[Int]): Unit = {\n+    val currentEpoch = epochForPartitionId.get(offsetTopicPartitionId)\n+    if (currentEpoch.forall(currentEpoch => currentEpoch <= coordinatorEpoch.getOrElse(Int.MaxValue))) {\n+      info(s\"Resigned as the group coordinator for partition $offsetTopicPartitionId in epoch $coordinatorEpoch\")\n+      groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)\n+      epochForPartitionId.remove(offsetTopicPartitionId)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwNzM3NDM5Nw=="}, "originalCommit": {"oid": "afab8f10f9ec0da9460860d98b2eaf0f5a5e4e3e"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg0MDM2ODkxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0wNVQyMjo0MDoxMVrOJDPN6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xOVQxMDoxNzoyNVrOJLMtKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwNzM3NDgyNg==", "bodyText": "Does this need to be a concurrent collection? It does not look like we can count on a lock protecting onElection and onResignation.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r607374826", "createdAt": "2021-04-05T22:40:11Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -87,6 +87,8 @@ class GroupCoordinator(val brokerId: Int,\n \n   private val isActive = new AtomicBoolean(false)\n \n+  val epochForPartitionId = mutable.Map[Int, Int]()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "afab8f10f9ec0da9460860d98b2eaf0f5a5e4e3e"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNTcyMjI4Mg==", "bodyText": "Yes it does, thanks! Now fixed.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r615722282", "createdAt": "2021-04-19T10:17:25Z", "author": {"login": "tombentley"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -87,6 +87,8 @@ class GroupCoordinator(val brokerId: Int,\n \n   private val isActive = new AtomicBoolean(false)\n \n+  val epochForPartitionId = mutable.Map[Int, Int]()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwNzM3NDgyNg=="}, "originalCommit": {"oid": "afab8f10f9ec0da9460860d98b2eaf0f5a5e4e3e"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzkxMTI4NzQwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yMlQwMTo1OTo0MFrOJNZOGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yMlQwMTo1OTo0MFrOJNZOGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNDQ3NA==", "bodyText": "Can we do a CAS update? Otherwise I don't think this is safe.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r618024474", "createdAt": "2021-04-22T01:59:40Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -905,19 +908,32 @@ class GroupCoordinator(val brokerId: Int,\n    *\n    * @param offsetTopicPartitionId The partition we are now leading\n    */\n-  def onElection(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+  def onElection(offsetTopicPartitionId: Int, coordinatorEpoch: Int): Unit = {\n+    val currentEpoch = Option(epochForPartitionId.get(offsetTopicPartitionId))\n+    if (currentEpoch.forall(currentEpoch => coordinatorEpoch > currentEpoch)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzkxMTI5MTM1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yMlQwMjowMDoyOVrOJNZQQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yNlQyMjozNzo0MVrOJP8YCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNTAyNg==", "bodyText": "Similarly, we should update epochForPartitionId here with a CAS operation.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r618025026", "createdAt": "2021-04-22T02:00:29Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -905,19 +908,32 @@ class GroupCoordinator(val brokerId: Int,\n    *\n    * @param offsetTopicPartitionId The partition we are now leading\n    */\n-  def onElection(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+  def onElection(offsetTopicPartitionId: Int, coordinatorEpoch: Int): Unit = {\n+    val currentEpoch = Option(epochForPartitionId.get(offsetTopicPartitionId))\n+    if (currentEpoch.forall(currentEpoch => coordinatorEpoch > currentEpoch)) {\n+      info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId in epoch $coordinatorEpoch\")\n+      groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+      epochForPartitionId.put(offsetTopicPartitionId, coordinatorEpoch)\n+    } else {\n+      warn(s\"Ignored election as group coordinator for partition $offsetTopicPartitionId \" +\n+        s\"in epoch $coordinatorEpoch since current epoch is $currentEpoch\")\n+    }\n   }\n \n   /**\n    * Unload cached state for the given partition and stop handling requests for groups which map to it.\n    *\n    * @param offsetTopicPartitionId The partition we are no longer leading\n    */\n-  def onResignation(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Resigned as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)\n+  def onResignation(offsetTopicPartitionId: Int, coordinatorEpoch: Option[Int]): Unit = {\n+    val currentEpoch = Option(epochForPartitionId.get(offsetTopicPartitionId))\n+    if (currentEpoch.forall(currentEpoch => currentEpoch <= coordinatorEpoch.getOrElse(Int.MaxValue))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODM1ODY0Mw==", "bodyText": "This one doesn't do an update any more (following your other comment).", "url": "https://github.com/apache/kafka/pull/9441#discussion_r618358643", "createdAt": "2021-04-22T12:33:59Z", "author": {"login": "tombentley"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -905,19 +908,32 @@ class GroupCoordinator(val brokerId: Int,\n    *\n    * @param offsetTopicPartitionId The partition we are now leading\n    */\n-  def onElection(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+  def onElection(offsetTopicPartitionId: Int, coordinatorEpoch: Int): Unit = {\n+    val currentEpoch = Option(epochForPartitionId.get(offsetTopicPartitionId))\n+    if (currentEpoch.forall(currentEpoch => coordinatorEpoch > currentEpoch)) {\n+      info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId in epoch $coordinatorEpoch\")\n+      groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+      epochForPartitionId.put(offsetTopicPartitionId, coordinatorEpoch)\n+    } else {\n+      warn(s\"Ignored election as group coordinator for partition $offsetTopicPartitionId \" +\n+        s\"in epoch $coordinatorEpoch since current epoch is $currentEpoch\")\n+    }\n   }\n \n   /**\n    * Unload cached state for the given partition and stop handling requests for groups which map to it.\n    *\n    * @param offsetTopicPartitionId The partition we are no longer leading\n    */\n-  def onResignation(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Resigned as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)\n+  def onResignation(offsetTopicPartitionId: Int, coordinatorEpoch: Option[Int]): Unit = {\n+    val currentEpoch = Option(epochForPartitionId.get(offsetTopicPartitionId))\n+    if (currentEpoch.forall(currentEpoch => currentEpoch <= coordinatorEpoch.getOrElse(Int.MaxValue))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNTAyNg=="}, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMDY5NzYxMQ==", "bodyText": "I have probably not been doing a good job of being clear. It is useful to bump the epoch whenever we observe a larger value whether it is in onResignation or onElection. This protects us from all potential reorderings.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r620697611", "createdAt": "2021-04-26T22:37:41Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -905,19 +908,32 @@ class GroupCoordinator(val brokerId: Int,\n    *\n    * @param offsetTopicPartitionId The partition we are now leading\n    */\n-  def onElection(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+  def onElection(offsetTopicPartitionId: Int, coordinatorEpoch: Int): Unit = {\n+    val currentEpoch = Option(epochForPartitionId.get(offsetTopicPartitionId))\n+    if (currentEpoch.forall(currentEpoch => coordinatorEpoch > currentEpoch)) {\n+      info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId in epoch $coordinatorEpoch\")\n+      groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+      epochForPartitionId.put(offsetTopicPartitionId, coordinatorEpoch)\n+    } else {\n+      warn(s\"Ignored election as group coordinator for partition $offsetTopicPartitionId \" +\n+        s\"in epoch $coordinatorEpoch since current epoch is $currentEpoch\")\n+    }\n   }\n \n   /**\n    * Unload cached state for the given partition and stop handling requests for groups which map to it.\n    *\n    * @param offsetTopicPartitionId The partition we are no longer leading\n    */\n-  def onResignation(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Resigned as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.removeGroupsForPartition(offsetTopicPartitionId, onGroupUnloaded)\n+  def onResignation(offsetTopicPartitionId: Int, coordinatorEpoch: Option[Int]): Unit = {\n+    val currentEpoch = Option(epochForPartitionId.get(offsetTopicPartitionId))\n+    if (currentEpoch.forall(currentEpoch => currentEpoch <= coordinatorEpoch.getOrElse(Int.MaxValue))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNTAyNg=="}, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzkxMTMwMDU0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yMlQwMjowMjo0NVrOJNZVuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNi0wNFQyMTozNzo1NFrOJn80wA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNjQyNw==", "bodyText": "It's not clear to me why we moved this in ReplicaManager. Is there some reason we need the replicaStateChangeLock lock?", "url": "https://github.com/apache/kafka/pull/9441#discussion_r618026427", "createdAt": "2021-04-22T02:02:45Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -279,30 +279,33 @@ class KafkaApis(val requestChannel: RequestChannel,\n         new StopReplicaResponseData().setErrorCode(Errors.STALE_BROKER_EPOCH.code)))\n     } else {\n       val partitionStates = stopReplicaRequest.partitionStates().asScala\n-      val (result, error) = replicaManager.stopReplicas(\n-        request.context.correlationId,\n-        stopReplicaRequest.controllerId,\n-        stopReplicaRequest.controllerEpoch,\n-        stopReplicaRequest.brokerEpoch,\n-        partitionStates)\n-      // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n-      // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n-      result.forKeyValue { (topicPartition, error) =>\n-        if (error == Errors.NONE) {\n-          if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n-              && partitionStates(topicPartition).deletePartition) {\n-            groupCoordinator.onResignation(topicPartition.partition)\n-          } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n-                     && partitionStates(topicPartition).deletePartition) {\n+      def onStopReplicas(error: Errors, partitions: Map[TopicPartition, Errors]): Unit = {\n+        // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n+        // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n+        partitions.forKeyValue { (topicPartition, partitionError) =>\n+          if (partitionError == Errors.NONE) {\n             val partitionState = partitionStates(topicPartition)\n             val leaderEpoch = if (partitionState.leaderEpoch >= 0)\n-                Some(partitionState.leaderEpoch)\n+              Some(partitionState.leaderEpoch)\n             else\n               None\n-            txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              groupCoordinator.onResignation(topicPartition.partition, leaderEpoch)\n+            } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            }\n           }\n         }\n       }\n+      val (result, error) = replicaManager.stopReplicas(\n+        request.context.correlationId,\n+        stopReplicaRequest.controllerId,\n+        stopReplicaRequest.controllerEpoch,\n+        stopReplicaRequest.brokerEpoch,\n+        partitionStates,\n+        onStopReplicas)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyNDIxOTc0NA==", "bodyText": "@hachikuji I think this is related to an earlier comment: #9441 (comment) The concern is that onResignation is called from two places: handleStopReplicaRequest and handleLeaderAndIsrRequest. The latter is protected by replicaStateChangeLock but the former is not, and hence race conditions may still happen.\nThe current approach seems to be going to a slight different way: instead of always trying to synchronize under replicaStateChangeLock, we just compare and swapping the epochForPartitionId, is that right?", "url": "https://github.com/apache/kafka/pull/9441#discussion_r624219744", "createdAt": "2021-04-30T21:26:04Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -279,30 +279,33 @@ class KafkaApis(val requestChannel: RequestChannel,\n         new StopReplicaResponseData().setErrorCode(Errors.STALE_BROKER_EPOCH.code)))\n     } else {\n       val partitionStates = stopReplicaRequest.partitionStates().asScala\n-      val (result, error) = replicaManager.stopReplicas(\n-        request.context.correlationId,\n-        stopReplicaRequest.controllerId,\n-        stopReplicaRequest.controllerEpoch,\n-        stopReplicaRequest.brokerEpoch,\n-        partitionStates)\n-      // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n-      // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n-      result.forKeyValue { (topicPartition, error) =>\n-        if (error == Errors.NONE) {\n-          if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n-              && partitionStates(topicPartition).deletePartition) {\n-            groupCoordinator.onResignation(topicPartition.partition)\n-          } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n-                     && partitionStates(topicPartition).deletePartition) {\n+      def onStopReplicas(error: Errors, partitions: Map[TopicPartition, Errors]): Unit = {\n+        // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n+        // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n+        partitions.forKeyValue { (topicPartition, partitionError) =>\n+          if (partitionError == Errors.NONE) {\n             val partitionState = partitionStates(topicPartition)\n             val leaderEpoch = if (partitionState.leaderEpoch >= 0)\n-                Some(partitionState.leaderEpoch)\n+              Some(partitionState.leaderEpoch)\n             else\n               None\n-            txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              groupCoordinator.onResignation(topicPartition.partition, leaderEpoch)\n+            } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            }\n           }\n         }\n       }\n+      val (result, error) = replicaManager.stopReplicas(\n+        request.context.correlationId,\n+        stopReplicaRequest.controllerId,\n+        stopReplicaRequest.controllerEpoch,\n+        stopReplicaRequest.brokerEpoch,\n+        partitionStates,\n+        onStopReplicas)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNjQyNw=="}, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyNTgwMjk2Mw==", "bodyText": "@guozhangwang yes, that's right. I forgot about our conversation about the lock when @hachikuji asked about why we were using the callback  \ud83d\ude1e.\nI notice that the partitionLock is acquired by the addLoadingPartition call in loadGroupsAndOffsets, and is also acquired in removeGroupsAndOffsets. Wouldn't it be simpler to use that than replicaStateChangeLock at this point if we're wanting to avoid a third way of handling concurrency here, or is there some subtlety? Obviously we wouldn't hold it for the call to doLoadGroupsAndOffsets in loadGroupsAndOffsets, just for the two checks at the start\n    if (!maybeUpdateCoordinatorEpoch(topicPartition.partition, Some(coordinatorEpoch))) {\n      info(s\"Not loading offsets and group metadata for $topicPartition \" +\n        s\"in epoch $coordinatorEpoch since current epoch is ${epochForPartitionId.get(topicPartition.partition)}\")\n    } else if (!addLoadingPartition(topicPartition.partition)) {\n      info(s\"Already loading offsets and group metadata from $topicPartition\")\n    }", "url": "https://github.com/apache/kafka/pull/9441#discussion_r625802963", "createdAt": "2021-05-04T13:55:27Z", "author": {"login": "tombentley"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -279,30 +279,33 @@ class KafkaApis(val requestChannel: RequestChannel,\n         new StopReplicaResponseData().setErrorCode(Errors.STALE_BROKER_EPOCH.code)))\n     } else {\n       val partitionStates = stopReplicaRequest.partitionStates().asScala\n-      val (result, error) = replicaManager.stopReplicas(\n-        request.context.correlationId,\n-        stopReplicaRequest.controllerId,\n-        stopReplicaRequest.controllerEpoch,\n-        stopReplicaRequest.brokerEpoch,\n-        partitionStates)\n-      // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n-      // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n-      result.forKeyValue { (topicPartition, error) =>\n-        if (error == Errors.NONE) {\n-          if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n-              && partitionStates(topicPartition).deletePartition) {\n-            groupCoordinator.onResignation(topicPartition.partition)\n-          } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n-                     && partitionStates(topicPartition).deletePartition) {\n+      def onStopReplicas(error: Errors, partitions: Map[TopicPartition, Errors]): Unit = {\n+        // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n+        // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n+        partitions.forKeyValue { (topicPartition, partitionError) =>\n+          if (partitionError == Errors.NONE) {\n             val partitionState = partitionStates(topicPartition)\n             val leaderEpoch = if (partitionState.leaderEpoch >= 0)\n-                Some(partitionState.leaderEpoch)\n+              Some(partitionState.leaderEpoch)\n             else\n               None\n-            txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              groupCoordinator.onResignation(topicPartition.partition, leaderEpoch)\n+            } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            }\n           }\n         }\n       }\n+      val (result, error) = replicaManager.stopReplicas(\n+        request.context.correlationId,\n+        stopReplicaRequest.controllerId,\n+        stopReplicaRequest.controllerEpoch,\n+        stopReplicaRequest.brokerEpoch,\n+        partitionStates,\n+        onStopReplicas)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNjQyNw=="}, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyNjI1MDM0Ng==", "bodyText": "I think partitionLock and replicaStateChangeLock are for different purposes here: the latter is specifically for changing the replica state including leader, ISR, while the former is for more general access patterns? @hachikuji could you chime in here if you got some time.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r626250346", "createdAt": "2021-05-05T04:49:43Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -279,30 +279,33 @@ class KafkaApis(val requestChannel: RequestChannel,\n         new StopReplicaResponseData().setErrorCode(Errors.STALE_BROKER_EPOCH.code)))\n     } else {\n       val partitionStates = stopReplicaRequest.partitionStates().asScala\n-      val (result, error) = replicaManager.stopReplicas(\n-        request.context.correlationId,\n-        stopReplicaRequest.controllerId,\n-        stopReplicaRequest.controllerEpoch,\n-        stopReplicaRequest.brokerEpoch,\n-        partitionStates)\n-      // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n-      // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n-      result.forKeyValue { (topicPartition, error) =>\n-        if (error == Errors.NONE) {\n-          if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n-              && partitionStates(topicPartition).deletePartition) {\n-            groupCoordinator.onResignation(topicPartition.partition)\n-          } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n-                     && partitionStates(topicPartition).deletePartition) {\n+      def onStopReplicas(error: Errors, partitions: Map[TopicPartition, Errors]): Unit = {\n+        // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n+        // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n+        partitions.forKeyValue { (topicPartition, partitionError) =>\n+          if (partitionError == Errors.NONE) {\n             val partitionState = partitionStates(topicPartition)\n             val leaderEpoch = if (partitionState.leaderEpoch >= 0)\n-                Some(partitionState.leaderEpoch)\n+              Some(partitionState.leaderEpoch)\n             else\n               None\n-            txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              groupCoordinator.onResignation(topicPartition.partition, leaderEpoch)\n+            } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            }\n           }\n         }\n       }\n+      val (result, error) = replicaManager.stopReplicas(\n+        request.context.correlationId,\n+        stopReplicaRequest.controllerId,\n+        stopReplicaRequest.controllerEpoch,\n+        stopReplicaRequest.brokerEpoch,\n+        partitionStates,\n+        onStopReplicas)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNjQyNw=="}, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0MzUxMzYzMg==", "bodyText": "If I understand correctly, the original issue concerned the potential reordering of loading/unloading events. This was possible because of inconsistent locking and the fact that we relied 100% on the order that the task was submitted to the scheduler. With this patch, we are now using the leader epoch in order to ensure that loading/unloading events are handled in the correct order. This means it does not actually matter if the events get submitted to the scheduler in the wrong order. Does that make sense or am I still missing something?", "url": "https://github.com/apache/kafka/pull/9441#discussion_r643513632", "createdAt": "2021-06-01T22:04:19Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -279,30 +279,33 @@ class KafkaApis(val requestChannel: RequestChannel,\n         new StopReplicaResponseData().setErrorCode(Errors.STALE_BROKER_EPOCH.code)))\n     } else {\n       val partitionStates = stopReplicaRequest.partitionStates().asScala\n-      val (result, error) = replicaManager.stopReplicas(\n-        request.context.correlationId,\n-        stopReplicaRequest.controllerId,\n-        stopReplicaRequest.controllerEpoch,\n-        stopReplicaRequest.brokerEpoch,\n-        partitionStates)\n-      // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n-      // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n-      result.forKeyValue { (topicPartition, error) =>\n-        if (error == Errors.NONE) {\n-          if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n-              && partitionStates(topicPartition).deletePartition) {\n-            groupCoordinator.onResignation(topicPartition.partition)\n-          } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n-                     && partitionStates(topicPartition).deletePartition) {\n+      def onStopReplicas(error: Errors, partitions: Map[TopicPartition, Errors]): Unit = {\n+        // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n+        // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n+        partitions.forKeyValue { (topicPartition, partitionError) =>\n+          if (partitionError == Errors.NONE) {\n             val partitionState = partitionStates(topicPartition)\n             val leaderEpoch = if (partitionState.leaderEpoch >= 0)\n-                Some(partitionState.leaderEpoch)\n+              Some(partitionState.leaderEpoch)\n             else\n               None\n-            txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              groupCoordinator.onResignation(topicPartition.partition, leaderEpoch)\n+            } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            }\n           }\n         }\n       }\n+      val (result, error) = replicaManager.stopReplicas(\n+        request.context.correlationId,\n+        stopReplicaRequest.controllerId,\n+        stopReplicaRequest.controllerEpoch,\n+        stopReplicaRequest.brokerEpoch,\n+        partitionStates,\n+        onStopReplicas)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNjQyNw=="}, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0NDU3MTY2NQ==", "bodyText": "@hachikuji as @guozhangwang pointed out, the scheduler really is FIFO (it uses a sequence number internally to guarantee order is maintained), so assuming his theory about the racing i/o threads is correct (and I think it is, but I've never observed this problem) then his solution of holding the lock for handleStopReplicaRequest would work.\nThe current version of the PR doesn't make assumptions about how any reordering can happen (i.e. whether caused by the inconsistent locking or anything else). So I don't think you're missing anything, you've just solved the problem differently.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r644571665", "createdAt": "2021-06-03T07:56:39Z", "author": {"login": "tombentley"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -279,30 +279,33 @@ class KafkaApis(val requestChannel: RequestChannel,\n         new StopReplicaResponseData().setErrorCode(Errors.STALE_BROKER_EPOCH.code)))\n     } else {\n       val partitionStates = stopReplicaRequest.partitionStates().asScala\n-      val (result, error) = replicaManager.stopReplicas(\n-        request.context.correlationId,\n-        stopReplicaRequest.controllerId,\n-        stopReplicaRequest.controllerEpoch,\n-        stopReplicaRequest.brokerEpoch,\n-        partitionStates)\n-      // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n-      // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n-      result.forKeyValue { (topicPartition, error) =>\n-        if (error == Errors.NONE) {\n-          if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n-              && partitionStates(topicPartition).deletePartition) {\n-            groupCoordinator.onResignation(topicPartition.partition)\n-          } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n-                     && partitionStates(topicPartition).deletePartition) {\n+      def onStopReplicas(error: Errors, partitions: Map[TopicPartition, Errors]): Unit = {\n+        // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n+        // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n+        partitions.forKeyValue { (topicPartition, partitionError) =>\n+          if (partitionError == Errors.NONE) {\n             val partitionState = partitionStates(topicPartition)\n             val leaderEpoch = if (partitionState.leaderEpoch >= 0)\n-                Some(partitionState.leaderEpoch)\n+              Some(partitionState.leaderEpoch)\n             else\n               None\n-            txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              groupCoordinator.onResignation(topicPartition.partition, leaderEpoch)\n+            } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            }\n           }\n         }\n       }\n+      val (result, error) = replicaManager.stopReplicas(\n+        request.context.correlationId,\n+        stopReplicaRequest.controllerId,\n+        stopReplicaRequest.controllerEpoch,\n+        stopReplicaRequest.brokerEpoch,\n+        partitionStates,\n+        onStopReplicas)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNjQyNw=="}, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY0NTg3MDc4NA==", "bodyText": "Thanks @hachikuji , I think using epoch would be sufficient too.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r645870784", "createdAt": "2021-06-04T21:37:54Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -279,30 +279,33 @@ class KafkaApis(val requestChannel: RequestChannel,\n         new StopReplicaResponseData().setErrorCode(Errors.STALE_BROKER_EPOCH.code)))\n     } else {\n       val partitionStates = stopReplicaRequest.partitionStates().asScala\n-      val (result, error) = replicaManager.stopReplicas(\n-        request.context.correlationId,\n-        stopReplicaRequest.controllerId,\n-        stopReplicaRequest.controllerEpoch,\n-        stopReplicaRequest.brokerEpoch,\n-        partitionStates)\n-      // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n-      // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n-      result.forKeyValue { (topicPartition, error) =>\n-        if (error == Errors.NONE) {\n-          if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n-              && partitionStates(topicPartition).deletePartition) {\n-            groupCoordinator.onResignation(topicPartition.partition)\n-          } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n-                     && partitionStates(topicPartition).deletePartition) {\n+      def onStopReplicas(error: Errors, partitions: Map[TopicPartition, Errors]): Unit = {\n+        // Clear the coordinator caches in case we were the leader. In the case of a reassignment, we\n+        // cannot rely on the LeaderAndIsr API for this since it is only sent to active replicas.\n+        partitions.forKeyValue { (topicPartition, partitionError) =>\n+          if (partitionError == Errors.NONE) {\n             val partitionState = partitionStates(topicPartition)\n             val leaderEpoch = if (partitionState.leaderEpoch >= 0)\n-                Some(partitionState.leaderEpoch)\n+              Some(partitionState.leaderEpoch)\n             else\n               None\n-            txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            if (topicPartition.topic == GROUP_METADATA_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              groupCoordinator.onResignation(topicPartition.partition, leaderEpoch)\n+            } else if (topicPartition.topic == TRANSACTION_STATE_TOPIC_NAME\n+              && partitionState.deletePartition) {\n+              txnCoordinator.onResignation(topicPartition.partition, coordinatorEpoch = leaderEpoch)\n+            }\n           }\n         }\n       }\n+      val (result, error) = replicaManager.stopReplicas(\n+        request.context.correlationId,\n+        stopReplicaRequest.controllerId,\n+        stopReplicaRequest.controllerEpoch,\n+        stopReplicaRequest.brokerEpoch,\n+        partitionStates,\n+        onStopReplicas)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNjQyNw=="}, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzkxMTMwMTY4OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/KafkaApisTest.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yMlQwMjowMzowNFrOJNZWew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yMlQwMjowMzowNFrOJNZWew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxODAyNjYxOQ==", "bodyText": "Can you fix this?", "url": "https://github.com/apache/kafka/pull/9441#discussion_r618026619", "createdAt": "2021-04-22T02:03:04Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/server/KafkaApisTest.scala", "diffHunk": "@@ -2891,17 +2898,29 @@ class KafkaApisTest {\n       EasyMock.eq(controllerId),\n       EasyMock.eq(controllerEpoch),\n       EasyMock.eq(brokerEpochInRequest),\n-      EasyMock.eq(stopReplicaRequest.partitionStates().asScala)\n-    )).andStubReturn(\n-      (mutable.Map(\n+      EasyMock.eq(stopReplicaRequest.partitionStates().asScala),\n+      EasyMock.anyObject()\n+    )).andStubAnswer {() =>\n+      val result = (mutable.Map(\n         fooPartition -> Errors.NONE\n       ), Errors.NONE)\n-    )\n+//<<<<<<< HEAD", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efefe58c1072cf0ae766b6b978cfe2425e7616ac"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzkyOTYzNjQ4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yNlQyMjozMjo1MVrOJP8P3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yNlQyMjozMjo1MVrOJP8P3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMDY5NTUxNw==", "bodyText": "One final thing I was considering is whether we should push this check into GroupMetadataManager.loadGroupsAndOffsets. That would give us some protection against any assumptions about ordering in KafkaScheduler.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r620695517", "createdAt": "2021-04-26T22:32:51Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -905,19 +908,35 @@ class GroupCoordinator(val brokerId: Int,\n    *\n    * @param offsetTopicPartitionId The partition we are now leading\n    */\n-  def onElection(offsetTopicPartitionId: Int): Unit = {\n-    info(s\"Elected as the group coordinator for partition $offsetTopicPartitionId\")\n-    groupManager.scheduleLoadGroupAndOffsets(offsetTopicPartitionId, onGroupLoaded)\n+  def onElection(offsetTopicPartitionId: Int, coordinatorEpoch: Int): Unit = {\n+    epochForPartitionId.compute(offsetTopicPartitionId, (_, epoch) => {\n+      val currentEpoch = Option(epoch)\n+      if (currentEpoch.forall(currentEpoch => coordinatorEpoch > currentEpoch)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4c478d59d7a2c27cc3c2450dc3e2b711d2cbc368"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzk1Mjc5NjYzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0zMFQxOToxNzo0MFrOJTN9kQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0zMFQxOToxNzo0MFrOJTN9kQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyNDEzMTQ3Mw==", "bodyText": "One minor improvement here is to change addLoadingPartition so that it checks whether the partition is already contained in ownedPartitions. If so, we can return false.", "url": "https://github.com/apache/kafka/pull/9441#discussion_r624131473", "createdAt": "2021-04-30T19:17:40Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala", "diffHunk": "@@ -526,34 +529,42 @@ class GroupMetadataManager(brokerId: Int,\n   /**\n    * Asynchronously read the partition from the offsets topic and populate the cache\n    */\n-  def scheduleLoadGroupAndOffsets(offsetsPartition: Int, onGroupLoaded: GroupMetadata => Unit): Unit = {\n+  def scheduleLoadGroupAndOffsets(offsetsPartition: Int, coordinatorEpoch: Int, onGroupLoaded: GroupMetadata => Unit): Unit = {\n     val topicPartition = new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, offsetsPartition)\n-    if (addLoadingPartition(offsetsPartition)) {\n-      info(s\"Scheduling loading of offsets and group metadata from $topicPartition\")\n-      val startTimeMs = time.milliseconds()\n-      scheduler.schedule(topicPartition.toString, () => loadGroupsAndOffsets(topicPartition, onGroupLoaded, startTimeMs))\n-    } else {\n-      info(s\"Already loading offsets and group metadata from $topicPartition\")\n-    }\n+    info(s\"Scheduling loading of offsets and group metadata from $topicPartition for epoch $coordinatorEpoch\")\n+    val startTimeMs = time.milliseconds()\n+    scheduler.schedule(topicPartition.toString, () => loadGroupsAndOffsets(topicPartition, coordinatorEpoch, onGroupLoaded, startTimeMs))\n   }\n \n-  private[group] def loadGroupsAndOffsets(topicPartition: TopicPartition, onGroupLoaded: GroupMetadata => Unit, startTimeMs: java.lang.Long): Unit = {\n-    try {\n-      val schedulerTimeMs = time.milliseconds() - startTimeMs\n-      debug(s\"Started loading offsets and group metadata from $topicPartition\")\n-      doLoadGroupsAndOffsets(topicPartition, onGroupLoaded)\n-      val endTimeMs = time.milliseconds()\n-      val totalLoadingTimeMs = endTimeMs - startTimeMs\n-      partitionLoadSensor.record(totalLoadingTimeMs.toDouble, endTimeMs, false)\n-      info(s\"Finished loading offsets and group metadata from $topicPartition \"\n-        + s\"in $totalLoadingTimeMs milliseconds, of which $schedulerTimeMs milliseconds\"\n-        + s\" was spent in the scheduler.\")\n-    } catch {\n-      case t: Throwable => error(s\"Error loading offsets from $topicPartition\", t)\n-    } finally {\n-      inLock(partitionLock) {\n-        ownedPartitions.add(topicPartition.partition)\n-        loadingPartitions.remove(topicPartition.partition)\n+  private[group] def loadGroupsAndOffsets(\n+    topicPartition: TopicPartition,\n+    coordinatorEpoch: Int,\n+    onGroupLoaded: GroupMetadata => Unit,\n+    startTimeMs: java.lang.Long\n+  ): Unit = {\n+    if (!maybeUpdateCoordinatorEpoch(topicPartition.partition, Some(coordinatorEpoch))) {\n+      info(s\"Not loading offsets and group metadata for $topicPartition \" +\n+        s\"in epoch $coordinatorEpoch since current epoch is ${epochForPartitionId.get(topicPartition.partition)}\")\n+    } else if (!addLoadingPartition(topicPartition.partition)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "311a6f91a83aa65cec4d9c67258e47fb3cf8ab4a"}, "originalPosition": 66}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1787, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}