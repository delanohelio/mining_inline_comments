{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzgzMjE2MTQw", "number": 8213, "reviewThreads": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjoxNzo1NFrODk1jLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQxNzoyOTowOFrODlg93g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTUyNjg0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjoxNzo1NFrOFxYqCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQxNzoxODo1MVrOFycnyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMyODUyMQ==", "bodyText": "Pulled out of StreamThread (could have been done a long time ago, since it was a static class anyway). I didn't embed it in TaskManager, just to keep the file size lower.\nI also dropped AbstractTaskCreator, since the creation of Active and Standby tasks are only similar, not exactly the same. We weren't really using the abstraction for much except de-duplicating a few field declarations. On the con side, the abstraction made it hard to see that we were requiring several arguments for Standby task creation that were actually not ever used. The indirection also made it harder to read the task creation logic.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387328521", "createdAt": "2020-03-03T22:17:54Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ0MjA1OQ==", "bodyText": "Sounds good to me", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388442059", "createdAt": "2020-03-05T17:18:51Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMyODUyMQ=="}, "originalCommit": null, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTU0MTQzOnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollector.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjoyMjo1N1rOFxYzHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjoyMjo1N1rOFxYzHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzMDg0Ng==", "bodyText": "We never used it as an AutoCloseable, and having it makes it hard to trace the callers.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387330846", "createdAt": "2020-03-03T22:22:57Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollector.java", "diffHunk": "@@ -25,7 +25,7 @@\n \n import java.util.Map;\n \n-public interface RecordCollector extends AutoCloseable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTU0MzcwOnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjoyMzo0MVrOFxY0gA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjoyMzo0MVrOFxY0gA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzMTIwMA==", "bodyText": "Only the creator may close the producer, but we can go ahead and call flush() here.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387331200", "createdAt": "2020-03-03T22:23:41Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -262,7 +262,7 @@ public void close() {\n         if (eosEnabled) {\n             streamsProducer.abortTransaction();\n         }\n-        streamsProducer.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTU0Njg5OnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjoyNDo1MFrOFxY2fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjoyNDo1MFrOFxY2fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzMTcxMQ==", "bodyText": "Managed fully inside the task factory now.\nYou'll also notice a bunch of references to the producers are similarly gone in the following lines.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387331711", "createdAt": "2020-03-03T22:24:50Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -544,11 +306,7 @@ public static StreamThread create(final InternalTopologyBuilder builder,\n \n         final ThreadCache cache = new ThreadCache(logContext, cacheSizeBytes, streamsMetrics);\n \n-        final Map<TaskId, Producer<byte[], byte[]>> taskProducers = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 279}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTU1MTQ5OnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjoyNjoyOVrOFxY5aA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMzoyNDoxMlrOFxaRxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzMjQ1Ng==", "bodyText": "Sooo many switch statements are now gone.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387332456", "createdAt": "2020-03-03T22:26:29Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -1127,9 +868,7 @@ StreamThread updateThreadMetadata(final String adminClientId) {\n             this.state().name(),\n             getConsumerClientId(this.getName()),\n             getRestoreConsumerClientId(this.getName()),\n-            threadProducer == null ?\n-                Collections.emptySet() :\n-                Collections.singleton(getThreadProducerClientId(this.getName())),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 360}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM1NTA3OQ==", "bodyText": "Nice!", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387355079", "createdAt": "2020-03-03T23:24:12Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -1127,9 +868,7 @@ StreamThread updateThreadMetadata(final String adminClientId) {\n             this.state().name(),\n             getConsumerClientId(this.getName()),\n             getRestoreConsumerClientId(this.getName()),\n-            threadProducer == null ?\n-                Collections.emptySet() :\n-                Collections.singleton(getThreadProducerClientId(this.getName())),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzMjQ1Ng=="}, "originalCommit": null, "originalPosition": 360}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTU1Mzc2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjoyNzoyMlrOFxY61w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjoyNzoyMlrOFxY61w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzMjgyMw==", "bodyText": "We don't manage the producers anymore, so we can defer to the taskManager (who will defer to the active task creator, but that's none of the thread's business)", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387332823", "createdAt": "2020-03-03T22:27:22Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -1198,22 +933,7 @@ public String toString(final String indent) {\n     }\n \n     public Map<MetricName, Metric> producerMetrics() {\n-        final LinkedHashMap<MetricName, Metric> result = new LinkedHashMap<>();\n-        if (threadProducer != null) {\n-            final Map<MetricName, ? extends Metric> producerMetrics = threadProducer.metrics();\n-            if (producerMetrics != null) {\n-                result.putAll(producerMetrics);\n-            }\n-        } else {\n-            // When EOS is turned on, each task will have its own producer client\n-            // and the producer object passed in here will be null. We would then iterate through\n-            // all the active tasks and add their metrics to the output metrics map.\n-            for (final StreamTask task : taskManager.fixmeStreamTasks().values()) {\n-                final Map<MetricName, ? extends Metric> taskProducerMetrics = taskProducers.get(task.id).metrics();\n-                result.putAll(taskProducerMetrics);\n-            }\n-        }\n-        return result;\n+        return taskManager.producerMetrics();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 408}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTU1OTU2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjoyOTozM1rOFxY-dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxNTo0MDo1NFrOFxyZ0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzMzc1MQ==", "bodyText": "This is a bit on the side, but there was some wacky stuff going on in here, and afaict, the only purpose of all the nullable fields was to allow including the task id in exception messages. Do we really need to do that? If so, I'll just add the logContext to the exception message instead, since it already has the task id in it.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387333751", "createdAt": "2020-03-03T22:29:33Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "diffHunk": "@@ -52,39 +51,20 @@\n \n     private final Producer<byte[], byte[]> producer;\n     private final String applicationId;\n-    private final TaskId taskId;\n-    private final String logMessage;\n     private final boolean eosEnabled;\n \n     private boolean transactionInFlight = false;\n     private boolean transactionInitialized = false;\n \n-    public StreamsProducer(final LogContext logContext,\n-                           final Producer<byte[], byte[]> producer) {\n-        this(logContext, producer, null, null);\n-    }\n-\n-    public StreamsProducer(final LogContext logContext,\n-                           final Producer<byte[], byte[]> producer,\n-                           final String applicationId,\n-                           final TaskId taskId) {\n-        if ((applicationId != null && taskId == null) ||\n-            (applicationId == null && taskId != null)) {\n-            throw new IllegalArgumentException(\"applicationId and taskId must either be both null or both be not null\");\n-        }\n-\n+    public StreamsProducer(final Producer<byte[], byte[]> producer,\n+                           final boolean eosEnabled,\n+                           final LogContext logContext,\n+                           final String applicationId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM1NjQ3MA==", "bodyText": "Actually the logContext.logPrefix() should have the format of stream-thread [%s] task [%s] already, so all the log4j entries are good. For exception messages, we can just get the prefix and then encode that into the exception message.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387356470", "createdAt": "2020-03-03T23:28:14Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "diffHunk": "@@ -52,39 +51,20 @@\n \n     private final Producer<byte[], byte[]> producer;\n     private final String applicationId;\n-    private final TaskId taskId;\n-    private final String logMessage;\n     private final boolean eosEnabled;\n \n     private boolean transactionInFlight = false;\n     private boolean transactionInitialized = false;\n \n-    public StreamsProducer(final LogContext logContext,\n-                           final Producer<byte[], byte[]> producer) {\n-        this(logContext, producer, null, null);\n-    }\n-\n-    public StreamsProducer(final LogContext logContext,\n-                           final Producer<byte[], byte[]> producer,\n-                           final String applicationId,\n-                           final TaskId taskId) {\n-        if ((applicationId != null && taskId == null) ||\n-            (applicationId == null && taskId != null)) {\n-            throw new IllegalArgumentException(\"applicationId and taskId must either be both null or both be not null\");\n-        }\n-\n+    public StreamsProducer(final Producer<byte[], byte[]> producer,\n+                           final boolean eosEnabled,\n+                           final LogContext logContext,\n+                           final String applicationId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzMzc1MQ=="}, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzc1MDM1Mg==", "bodyText": "Yep, that's what I was thinking. I'll go ahead and do it.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387750352", "createdAt": "2020-03-04T15:40:54Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "diffHunk": "@@ -52,39 +51,20 @@\n \n     private final Producer<byte[], byte[]> producer;\n     private final String applicationId;\n-    private final TaskId taskId;\n-    private final String logMessage;\n     private final boolean eosEnabled;\n \n     private boolean transactionInFlight = false;\n     private boolean transactionInitialized = false;\n \n-    public StreamsProducer(final LogContext logContext,\n-                           final Producer<byte[], byte[]> producer) {\n-        this(logContext, producer, null, null);\n-    }\n-\n-    public StreamsProducer(final LogContext logContext,\n-                           final Producer<byte[], byte[]> producer,\n-                           final String applicationId,\n-                           final TaskId taskId) {\n-        if ((applicationId != null && taskId == null) ||\n-            (applicationId == null && taskId != null)) {\n-            throw new IllegalArgumentException(\"applicationId and taskId must either be both null or both be not null\");\n-        }\n-\n+    public StreamsProducer(final Producer<byte[], byte[]> producer,\n+                           final boolean eosEnabled,\n+                           final LogContext logContext,\n+                           final String applicationId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzMzc1MQ=="}, "originalCommit": null, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTU2MTk3OnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjozMDozM1rOFxZAFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjozMDozM1rOFxZAFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzNDE2Ng==", "bodyText": "no need to add the exception to the message, since we also pass it as the cause below.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387334166", "createdAt": "2020-03-03T22:30:33Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "diffHunk": "@@ -141,10 +120,9 @@ private void maybeBeginTransaction() throws ProducerFencedException {\n                 throw new TaskMigratedException(\"Producer cannot send records anymore since it got fenced\", uncaughtException.getCause());\n             } else {\n                 final String errorMessage = String.format(\n-                    \"Error encountered sending record to topic %s%s due to:%n%s\",\n-                    record.topic(),\n-                    taskId == null ? \"\" : \" \" + logMessage,\n-                    uncaughtException.toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTU2NDM2OnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjozMToxOFrOFxZBaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjozMToxOFrOFxZBaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzNDUwNQ==", "bodyText": "No one who has a reference to the StreamsProducer has any business calling close, so the method is gone now.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387334505", "createdAt": "2020-03-03T22:31:18Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "diffHunk": "@@ -212,17 +190,6 @@ public void flush() {\n         producer.flush();\n     }\n \n-    public void close() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTU2OTI4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjozMzoyMVrOFxZEig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjozMzoyMVrOFxZEig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzNTMwNg==", "bodyText": "Standby tasks don't need the mainConsumer (obvious, in retrospect).", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387335306", "createdAt": "2020-03-03T22:33:21Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -227,7 +227,7 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n         }\n \n         if (!standbyTasksToCreate.isEmpty()) {\n-            standbyTaskCreator.createTasks(mainConsumer, standbyTasksToCreate).forEach(this::addNewTask);\n+            standbyTaskCreator.createTasks(standbyTasksToCreate).forEach(this::addNewTask);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTY3OTE4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMzoxNjowNVrOFxaHoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMToxMTo0MVrOFx9yZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM1MjQ4MQ==", "bodyText": "nit: These two functions are not for testing only.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387352481", "createdAt": "2020-03-03T23:16:05Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -624,9 +624,11 @@ StandbyTask standbyTask(final TopicPartition partition) {\n         return null;\n     }\n \n-    // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks\n-    // Instead, we should register and record the metrics properly inside of the record collector.\n-    Map<TaskId, StreamTask> fixmeStreamTasks() {\n-        return tasks.values().stream().filter(t -> t instanceof StreamTask).map(t -> (StreamTask) t).collect(Collectors.toMap(Task::id, t -> t));\n+    Map<MetricName, Metric> producerMetrics() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzgyNTY2NQ==", "bodyText": "Thanks for the feedback; I didn't understand this particular comment, though.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387825665", "createdAt": "2020-03-04T17:38:53Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -624,9 +624,11 @@ StandbyTask standbyTask(final TopicPartition partition) {\n         return null;\n     }\n \n-    // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks\n-    // Instead, we should register and record the metrics properly inside of the record collector.\n-    Map<TaskId, StreamTask> fixmeStreamTasks() {\n-        return tasks.values().stream().filter(t -> t instanceof StreamTask).map(t -> (StreamTask) t).collect(Collectors.toMap(Task::id, t -> t));\n+    Map<MetricName, Metric> producerMetrics() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM1MjQ4MQ=="}, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg0OTU4NQ==", "bodyText": "The comment line above these two method declaration says the following functions are for test only, but these two functions are not.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387849585", "createdAt": "2020-03-04T18:23:24Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -624,9 +624,11 @@ StandbyTask standbyTask(final TopicPartition partition) {\n         return null;\n     }\n \n-    // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks\n-    // Instead, we should register and record the metrics properly inside of the record collector.\n-    Map<TaskId, StreamTask> fixmeStreamTasks() {\n-        return tasks.values().stream().filter(t -> t instanceof StreamTask).map(t -> (StreamTask) t).collect(Collectors.toMap(Task::id, t -> t));\n+    Map<MetricName, Metric> producerMetrics() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM1MjQ4MQ=="}, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzkzNjg3MA==", "bodyText": "Ah, I just found what you were talking about:\n    // below are for testing only\n\nI didn't notice that up there.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387936870", "createdAt": "2020-03-04T21:11:41Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -624,9 +624,11 @@ StandbyTask standbyTask(final TopicPartition partition) {\n         return null;\n     }\n \n-    // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks\n-    // Instead, we should register and record the metrics properly inside of the record collector.\n-    Map<TaskId, StreamTask> fixmeStreamTasks() {\n-        return tasks.values().stream().filter(t -> t instanceof StreamTask).map(t -> (StreamTask) t).collect(Collectors.toMap(Task::id, t -> t));\n+    Map<MetricName, Metric> producerMetrics() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM1MjQ4MQ=="}, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTY5MDc1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMzoyMTo1NlrOFxaOzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxODoyMjo1OVrOFx4ciw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM1NDMxOQ==", "bodyText": "In shutdown(final boolean clean) we should also release task producers as well right?", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387354319", "createdAt": "2020-03-03T23:21:56Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -81,9 +81,8 @@\n                 final UUID processId,\n                 final String logPrefix,\n                 final StreamsMetricsImpl streamsMetrics,\n-                final StreamThread.AbstractTaskCreator<? extends Task> activeTaskCreator,\n-                final StreamThread.AbstractTaskCreator<? extends Task> standbyTaskCreator,\n-                final Map<TaskId, Producer<byte[], byte[]>> taskProducers,\n+                final ActiveTaskCreator activeTaskCreator,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg0OTM1NQ==", "bodyText": "Yep. Good catch.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387849355", "createdAt": "2020-03-04T18:22:59Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -81,9 +81,8 @@\n                 final UUID processId,\n                 final String logPrefix,\n                 final StreamsMetricsImpl streamsMetrics,\n-                final StreamThread.AbstractTaskCreator<? extends Task> activeTaskCreator,\n-                final StreamThread.AbstractTaskCreator<? extends Task> standbyTaskCreator,\n-                final Map<TaskId, Producer<byte[], byte[]>> taskProducers,\n+                final ActiveTaskCreator activeTaskCreator,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM1NDMxOQ=="}, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTczNDE0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMzo0MjoyNlrOFxapug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxNTo0MjowMFrOFxyc6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2MTIxMA==", "bodyText": "I'm wondering if we are introducing a latency regression without EOS here: in the old code when closing without EOS we actually do nothing, and now we would block on flushing.\nOn the other hand, flushing maybe needed when we close a task to make sure all the tasks' records are acked already.\nIf the task is in RUNNING before shutting down, we would always commit before closing, so flush is already called; if the task is in RESTORING / SUSPENDED there's nothing written from this task, so a flush is not needed. So I think it is safe to not call flush after all.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387361210", "createdAt": "2020-03-03T23:42:26Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -262,7 +262,7 @@ public void close() {\n         if (eosEnabled) {\n             streamsProducer.abortTransaction();\n         }\n-        streamsProducer.close();\n+        streamsProducer.flush();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2OTc4NA==", "bodyText": "Sounds reasonable to me", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387369784", "createdAt": "2020-03-04T00:09:53Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -262,7 +262,7 @@ public void close() {\n         if (eosEnabled) {\n             streamsProducer.abortTransaction();\n         }\n-        streamsProducer.close();\n+        streamsProducer.flush();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2MTIxMA=="}, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzc1MTE0NA==", "bodyText": "Ok, I'll swap this out.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387751144", "createdAt": "2020-03-04T15:42:00Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -262,7 +262,7 @@ public void close() {\n         if (eosEnabled) {\n             streamsProducer.abortTransaction();\n         }\n-        streamsProducer.close();\n+        streamsProducer.flush();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2MTIxMA=="}, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTc1NjMxOnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsProducerTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMzo1MjoxOVrOFxa2dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxODoyMzo0M1rOFx4eLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NDQ3MQ==", "bodyText": "We do not created new test classes of ActiveTaskCreator / StandbyTaskCreator, but we should still have those coverage to make sure the exception thrown from producers are wrapped correctly.\nAlso it seems in the new code we no long rethrow -- is that intentional. I left a comment above.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387364471", "createdAt": "2020-03-03T23:52:19Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsProducerTest.java", "diffHunk": "@@ -612,25 +582,4 @@ public void shouldFailOnEosAbortTxFatal() {\n \n         assertThat(thrown.getMessage(), equalTo(\"KABOOM!\"));\n     }\n-\n-    @Test\n-    public void shouldFailOnCloseFatal() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg0OTc3Mw==", "bodyText": "Good catch. Actually, there's a bunch of coverage missing from TaskManager. I'll add several more tests.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387849773", "createdAt": "2020-03-04T18:23:43Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsProducerTest.java", "diffHunk": "@@ -612,25 +582,4 @@ public void shouldFailOnEosAbortTxFatal() {\n \n         assertThat(thrown.getMessage(), equalTo(\"KABOOM!\"));\n     }\n-\n-    @Test\n-    public void shouldFailOnCloseFatal() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NDQ3MQ=="}, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 217}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTc1OTE3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMzo1MzozOFrOFxa4Iw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxODoyNTozNFrOFx4h3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NDg5OQ==", "bodyText": "The old code we re-throw exceptions whereas here we just swallow the error. Is that intentional?", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387364899", "createdAt": "2020-03-03T23:53:38Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {\n+        if (threadProducer != null) {\n+            try {\n+                threadProducer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 181}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzgzNTM3NA==", "bodyText": "Ah, thanks for noticing this. I meant to ask about it in here.\nI thought it was strange that we would only re-throw when closing the task producers, not the thread producer. It seems like we should do the same thing in both cases, but which thing should we do?\nI went with an error log in both cases, but it sounds like you wanted to throw the exception instead. Should we also rethrow for the thread producer?", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387835374", "createdAt": "2020-03-04T17:56:43Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {\n+        if (threadProducer != null) {\n+            try {\n+                threadProducer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NDg5OQ=="}, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 181}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg1MDcxOQ==", "bodyText": "Yeah I think we should make them consistent: previously the closing producer is within task#close and if it is called via closeDirty we should make sure it never throws. Now since it is extracted out of the close call we should just rethrow for both cases.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387850719", "createdAt": "2020-03-04T18:25:34Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {\n+        if (threadProducer != null) {\n+            try {\n+                threadProducer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NDg5OQ=="}, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 181}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTc2NDAzOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/state/KeyValueStoreTestDriver.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMzo1NjowMVrOFxa7DQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxNzo1NzoxN1rOFx3nMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NTY0NQ==", "bodyText": "null != null?", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387365645", "createdAt": "2020-03-03T23:56:01Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/KeyValueStoreTestDriver.java", "diffHunk": "@@ -201,7 +201,7 @@ private KeyValueStoreTestDriver(final StateSerdes<K, V> serdes) {\n             logContext,\n             new TaskId(0, 0),\n             consumer,\n-            new StreamsProducer(logContext, producer),\n+            new StreamsProducer(producer, null != null, logContext, null),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM5Njc5NQ==", "bodyText": "lol", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387396795", "createdAt": "2020-03-04T01:18:53Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/KeyValueStoreTestDriver.java", "diffHunk": "@@ -201,7 +201,7 @@ private KeyValueStoreTestDriver(final StateSerdes<K, V> serdes) {\n             logContext,\n             new TaskId(0, 0),\n             consumer,\n-            new StreamsProducer(logContext, producer),\n+            new StreamsProducer(producer, null != null, logContext, null),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NTY0NQ=="}, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzgzNTY5Nw==", "bodyText": "Oops! Auto-refactoring. I already made two passes to clean these up, looks like I missed one.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387835697", "createdAt": "2020-03-04T17:57:17Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/state/KeyValueStoreTestDriver.java", "diffHunk": "@@ -201,7 +201,7 @@ private KeyValueStoreTestDriver(final StateSerdes<K, V> serdes) {\n             logContext,\n             new TaskId(0, 0),\n             consumer,\n-            new StreamsProducer(logContext, producer),\n+            new StreamsProducer(producer, null != null, logContext, null),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NTY0NQ=="}, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTc4MTc4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDowNTowMVrOFxbFvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMToyNjoyM1rOFxdASA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODM4MA==", "bodyText": "closeThreadProducer()\nThe method below is not public -- does this one need to be public?", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387368380", "createdAt": "2020-03-04T00:05:01Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM5OTc1Mg==", "bodyText": "even better maybeCloseThreadProducer", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387399752", "createdAt": "2020-03-04T01:26:23Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODM4MA=="}, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 176}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTc4MjU5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDowNTozNVrOFxbGQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDowNTozNVrOFxbGQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODUxNQ==", "bodyText": "closeProducerForTask(TaskId) ?", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387368515", "createdAt": "2020-03-04T00:05:35Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {\n+        if (threadProducer != null) {\n+            try {\n+                threadProducer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);\n+            }\n+        }\n+        if (!taskProducers.isEmpty()) {\n+            throw new IllegalStateException(\"Expected task producers to have been cleared before closing\");\n+        }\n+    }\n+\n+    void releaseProducer(final TaskId id) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 189}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTc4NDYwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDowNjoyNlrOFxbHag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxODowMzoyNFrOFx3zkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODgxMA==", "bodyText": "Does this need to be public?", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387368810", "createdAt": "2020-03-04T00:06:26Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {\n+        if (threadProducer != null) {\n+            try {\n+                threadProducer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);\n+            }\n+        }\n+        if (!taskProducers.isEmpty()) {\n+            throw new IllegalStateException(\"Expected task producers to have been cleared before closing\");\n+        }\n+    }\n+\n+    void releaseProducer(final TaskId id) {\n+        final Producer<byte[], byte[]> producer = taskProducers.remove(id);\n+        if (producer != null) {\n+            try {\n+                producer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);\n+            }\n+        }\n+    }\n+\n+    Map<MetricName, Metric> producerMetrics() {\n+        final Map<MetricName, Metric> result = new LinkedHashMap<>();\n+        if (threadProducer != null) {\n+            final Map<MetricName, ? extends Metric> producerMetrics = threadProducer.metrics();\n+            if (producerMetrics != null) {\n+                result.putAll(producerMetrics);\n+            }\n+        } else {\n+            // When EOS is turned on, each task will have its own producer client\n+            // and the producer object passed in here will be null. We would then iterate through\n+            // all the active tasks and add their metrics to the output metrics map.\n+            for (final Map.Entry<TaskId, Producer<byte[], byte[]>> entry : taskProducers.entrySet()) {\n+                final Map<MetricName, ? extends Metric> taskProducerMetrics = entry.getValue().metrics();\n+                result.putAll(taskProducerMetrics);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    Set<String> producerClientIds() {\n+        if (threadProducer != null) {\n+            return Collections.singleton(getThreadProducerClientId(threadId));\n+        } else {\n+            return taskProducers.keySet()\n+                                .stream()\n+                                .map(taskId -> getTaskProducerClientId(threadId, taskId))\n+                                .collect(Collectors.toSet());\n+        }\n+    }\n+\n+    public InternalTopologyBuilder builder() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 230}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzgzODg2Nw==", "bodyText": "Hah, doesn't need to be there at all, actually.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387838867", "createdAt": "2020-03-04T18:03:24Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {\n+        if (threadProducer != null) {\n+            try {\n+                threadProducer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);\n+            }\n+        }\n+        if (!taskProducers.isEmpty()) {\n+            throw new IllegalStateException(\"Expected task producers to have been cleared before closing\");\n+        }\n+    }\n+\n+    void releaseProducer(final TaskId id) {\n+        final Producer<byte[], byte[]> producer = taskProducers.remove(id);\n+        if (producer != null) {\n+            try {\n+                producer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);\n+            }\n+        }\n+    }\n+\n+    Map<MetricName, Metric> producerMetrics() {\n+        final Map<MetricName, Metric> result = new LinkedHashMap<>();\n+        if (threadProducer != null) {\n+            final Map<MetricName, ? extends Metric> producerMetrics = threadProducer.metrics();\n+            if (producerMetrics != null) {\n+                result.putAll(producerMetrics);\n+            }\n+        } else {\n+            // When EOS is turned on, each task will have its own producer client\n+            // and the producer object passed in here will be null. We would then iterate through\n+            // all the active tasks and add their metrics to the output metrics map.\n+            for (final Map.Entry<TaskId, Producer<byte[], byte[]>> entry : taskProducers.entrySet()) {\n+                final Map<MetricName, ? extends Metric> taskProducerMetrics = entry.getValue().metrics();\n+                result.putAll(taskProducerMetrics);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    Set<String> producerClientIds() {\n+        if (threadProducer != null) {\n+            return Collections.singleton(getThreadProducerClientId(threadId));\n+        } else {\n+            return taskProducers.keySet()\n+                                .stream()\n+                                .map(taskId -> getTaskProducerClientId(threadId, taskId))\n+                                .collect(Collectors.toSet());\n+        }\n+    }\n+\n+    public InternalTopologyBuilder builder() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODgxMA=="}, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 230}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTc4NDc2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDowNjozMFrOFxbHfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDowNjozMFrOFxbHfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODgyOQ==", "bodyText": "Does this need to be public?", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387368829", "createdAt": "2020-03-04T00:06:30Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {\n+        if (threadProducer != null) {\n+            try {\n+                threadProducer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);\n+            }\n+        }\n+        if (!taskProducers.isEmpty()) {\n+            throw new IllegalStateException(\"Expected task producers to have been cleared before closing\");\n+        }\n+    }\n+\n+    void releaseProducer(final TaskId id) {\n+        final Producer<byte[], byte[]> producer = taskProducers.remove(id);\n+        if (producer != null) {\n+            try {\n+                producer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);\n+            }\n+        }\n+    }\n+\n+    Map<MetricName, Metric> producerMetrics() {\n+        final Map<MetricName, Metric> result = new LinkedHashMap<>();\n+        if (threadProducer != null) {\n+            final Map<MetricName, ? extends Metric> producerMetrics = threadProducer.metrics();\n+            if (producerMetrics != null) {\n+                result.putAll(producerMetrics);\n+            }\n+        } else {\n+            // When EOS is turned on, each task will have its own producer client\n+            // and the producer object passed in here will be null. We would then iterate through\n+            // all the active tasks and add their metrics to the output metrics map.\n+            for (final Map.Entry<TaskId, Producer<byte[], byte[]>> entry : taskProducers.entrySet()) {\n+                final Map<MetricName, ? extends Metric> taskProducerMetrics = entry.getValue().metrics();\n+                result.putAll(taskProducerMetrics);\n+            }\n+        }\n+        return result;\n+    }\n+\n+    Set<String> producerClientIds() {\n+        if (threadProducer != null) {\n+            return Collections.singleton(getThreadProducerClientId(threadId));\n+        } else {\n+            return taskProducers.keySet()\n+                                .stream()\n+                                .map(taskId -> getTaskProducerClientId(threadId, taskId))\n+                                .collect(Collectors.toSet());\n+        }\n+    }\n+\n+    public InternalTopologyBuilder builder() {\n+        return builder;\n+    }\n+\n+    public StateDirectory stateDirectory() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 234}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTk5MzkzOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMToyMzoxN1rOFxc9Ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMToyMzoxN1rOFxc9Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM5ODkxNA==", "bodyText": "I don't think we need to keep this TODO, as only after a stream 3.0 is there, we shall remove the support for task producer.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387398914", "createdAt": "2020-03-04T01:23:17Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 135}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTk5NTkzOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMToyNDozMFrOFxc-TA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxODowNjo1OVrOFx36-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM5OTI0NA==", "bodyText": "Should we throw illegal state first, since we are already in an error state?", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387399244", "createdAt": "2020-03-04T01:24:30Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {\n+        if (threadProducer != null) {\n+            try {\n+                threadProducer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);\n+            }\n+        }\n+        if (!taskProducers.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg0MDc2Mg==", "bodyText": "If we're going to rename the method to specify that it should do exactly \"close thread producer\", then this check is no longer appropriate.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r387840762", "createdAt": "2020-03-04T18:06:59Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ActiveTaskCreator.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.producer.Producer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Metric;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.streams.KafkaClientSupplier;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.streams.processor.internals.metrics.ThreadMetrics;\n+import org.apache.kafka.streams.state.internals.ThreadCache;\n+import org.slf4j.Logger;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.streams.StreamsConfig.EXACTLY_ONCE;\n+\n+class ActiveTaskCreator {\n+    private final String applicationId;\n+    private final InternalTopologyBuilder builder;\n+    private final StreamsConfig config;\n+    private final StreamsMetricsImpl streamsMetrics;\n+    private final StateDirectory stateDirectory;\n+    private final ChangelogReader storeChangelogReader;\n+    private final Time time;\n+    private final Logger log;\n+    private final String threadId;\n+    private final ThreadCache cache;\n+    private final Producer<byte[], byte[]> threadProducer;\n+    private final KafkaClientSupplier clientSupplier;\n+    private final Map<TaskId, Producer<byte[], byte[]>> taskProducers;\n+    private final Sensor createTaskSensor;\n+\n+    private static String getThreadProducerClientId(final String threadClientId) {\n+        return threadClientId + \"-producer\";\n+    }\n+\n+    private static String getTaskProducerClientId(final String threadClientId, final TaskId taskId) {\n+        return threadClientId + \"-\" + taskId + \"-producer\";\n+    }\n+\n+    ActiveTaskCreator(final InternalTopologyBuilder builder,\n+                      final StreamsConfig config,\n+                      final StreamsMetricsImpl streamsMetrics,\n+                      final StateDirectory stateDirectory,\n+                      final ChangelogReader storeChangelogReader,\n+                      final ThreadCache cache,\n+                      final Time time,\n+                      final KafkaClientSupplier clientSupplier,\n+                      final String threadId,\n+                      final Logger log) {\n+        applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);\n+        this.builder = builder;\n+        this.config = config;\n+        this.streamsMetrics = streamsMetrics;\n+        this.stateDirectory = stateDirectory;\n+        this.storeChangelogReader = storeChangelogReader;\n+        this.time = time;\n+        this.log = log;\n+\n+        if (EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG))) {\n+            threadProducer = null;\n+            taskProducers = new HashMap<>();\n+        } else {\n+            final String threadProducerClientId = getThreadProducerClientId(threadId);\n+            final Map<String, Object> producerConfigs = config.getProducerConfigs(threadProducerClientId);\n+            log.info(\"Creating thread producer client\");\n+            threadProducer = clientSupplier.getProducer(producerConfigs);\n+            taskProducers = Collections.emptyMap();\n+        }\n+\n+\n+        this.cache = cache;\n+        this.threadId = threadId;\n+        this.clientSupplier = clientSupplier;\n+\n+        createTaskSensor = ThreadMetrics.createTaskSensor(threadId, streamsMetrics);\n+    }\n+\n+    Collection<Task> createTasks(final Consumer<byte[], byte[]> consumer,\n+                                 final Map<TaskId, Set<TopicPartition>> tasksToBeCreated) {\n+        final List<Task> createdTasks = new ArrayList<>();\n+        for (final Map.Entry<TaskId, Set<TopicPartition>> newTaskAndPartitions : tasksToBeCreated.entrySet()) {\n+            final TaskId taskId = newTaskAndPartitions.getKey();\n+            final Set<TopicPartition> partitions = newTaskAndPartitions.getValue();\n+\n+            final String threadIdPrefix = String.format(\"stream-thread [%s] \", Thread.currentThread().getName());\n+            final String logPrefix = threadIdPrefix + String.format(\"%s [%s] \", \"task\", taskId);\n+            final LogContext logContext = new LogContext(logPrefix);\n+\n+            final ProcessorTopology topology = builder.buildSubtopology(taskId.topicGroupId);\n+\n+            final ProcessorStateManager stateManager = new ProcessorStateManager(\n+                taskId,\n+                partitions,\n+                Task.TaskType.ACTIVE,\n+                stateDirectory,\n+                topology.storeToChangelogTopic(),\n+                storeChangelogReader,\n+                logContext\n+            );\n+\n+            if (threadProducer == null) {\n+                // create one producer per task for EOS\n+                // TODO: after KIP-447 this would be removed\n+                final String taskProducerClientId = getTaskProducerClientId(threadId, taskId);\n+                final Map<String, Object> producerConfigs = config.getProducerConfigs(taskProducerClientId);\n+                producerConfigs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, applicationId + \"-\" + taskId);\n+                log.info(\"Creating producer client for task {}\", taskId);\n+                taskProducers.put(taskId, clientSupplier.getProducer(producerConfigs));\n+            }\n+\n+            final RecordCollector recordCollector = new RecordCollectorImpl(\n+                logContext,\n+                taskId,\n+                consumer,\n+                threadProducer != null ?\n+                    new StreamsProducer(threadProducer, false, logContext, applicationId) :\n+                    new StreamsProducer(taskProducers.get(taskId), true, logContext, applicationId),\n+                config.defaultProductionExceptionHandler(),\n+                EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)),\n+                streamsMetrics\n+            );\n+\n+            final Task task = new StreamTask(\n+                taskId,\n+                partitions,\n+                topology,\n+                consumer,\n+                config,\n+                streamsMetrics,\n+                stateDirectory,\n+                cache,\n+                time,\n+                stateManager,\n+                recordCollector\n+            );\n+\n+            log.trace(\"Created task {} with assigned partitions {}\", taskId, partitions);\n+            createdTasks.add(task);\n+            createTaskSensor.record();\n+        }\n+        return createdTasks;\n+    }\n+\n+    public void releaseProducer() {\n+        if (threadProducer != null) {\n+            try {\n+                threadProducer.close();\n+            } catch (final RuntimeException e) {\n+                log.error(\"Failed to close producer due to the following error:\", e);\n+            }\n+        }\n+        if (!taskProducers.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM5OTI0NA=="}, "originalCommit": {"oid": "f53ef6da113430fbff915c590985658f2d6166e9"}, "originalPosition": 184}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQwMzgwNDI5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzo1MzowM1rOFyB4gA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQxNzozNTo0N1rOFydMhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAwMzk2OA==", "bodyText": "Let's make it a warn instead of a debug.\nThe error message can be more specific here: Error closing task producer for task {} while handling lostAll.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388003968", "createdAt": "2020-03-04T23:53:03Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -345,7 +359,11 @@ void handleLostAll() {\n                 cleanupTask(task);\n                 task.closeDirty();\n                 iterator.remove();\n-                taskProducers.remove(task.id());\n+                try {\n+                    activeTaskCreator.closeAndRemoveTaskProducerIfNeeded(task.id());\n+                } catch (final RuntimeException e) {\n+                    log.debug(\"Error handling lostAll\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c4a2bcb2a111c9d3b866daca3481f970bababef"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ1MTQ2MA==", "bodyText": "+1", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388451460", "createdAt": "2020-03-05T17:35:47Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -345,7 +359,11 @@ void handleLostAll() {\n                 cleanupTask(task);\n                 task.closeDirty();\n                 iterator.remove();\n-                taskProducers.remove(task.id());\n+                try {\n+                    activeTaskCreator.closeAndRemoveTaskProducerIfNeeded(task.id());\n+                } catch (final RuntimeException e) {\n+                    log.debug(\"Error handling lostAll\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAwMzk2OA=="}, "originalCommit": {"oid": "2c4a2bcb2a111c9d3b866daca3481f970bababef"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQwMzgwNTI3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzo1MzozMlrOFyB5IQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzo1MzozMlrOFyB5IQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAwNDEyOQ==", "bodyText": "Ditto here about error message", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388004129", "createdAt": "2020-03-04T23:53:32Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -419,14 +439,33 @@ void shutdown(final boolean clean) {\n             } else {\n                 task.closeDirty();\n             }\n+            if (task.isActive()) {\n+                try {\n+                    activeTaskCreator.closeAndRemoveTaskProducerIfNeeded(task.id());\n+                } catch (final RuntimeException e) {\n+                    if (clean) {\n+                        firstException.compareAndSet(null, e);\n+                    } else {\n+                        log.warn(\"Ignoring an exception while closing task producer.\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c4a2bcb2a111c9d3b866daca3481f970bababef"}, "originalPosition": 189}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQwMzgwOTUzOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzo1NTo0M1rOFyB72Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQyMzo1NTo0M1rOFyB72Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAwNDgyNQ==", "bodyText": "nit: ...WhileRebalanceInProgress", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388004825", "createdAt": "2020-03-04T23:55:43Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -440,6 +872,41 @@ public void shouldCommitActiveAndStandbyTasks() {\n         assertThat(taskManager.commitAll(), equalTo(2));\n     }\n \n+    @Test\n+    public void shouldNotCommitActiveAndStandbyTasks() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c4a2bcb2a111c9d3b866daca3481f970bababef"}, "originalPosition": 642}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQwNjYxOTcxOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsProducerTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQxNzoyMzowNVrOFycxNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQyMDoxNzozOFrOFyik0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ0NDQ2OA==", "bodyText": "I was wondering what's the standard for using assertThat vs assertTrue? Do we have a convention to follow?", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388444468", "createdAt": "2020-03-05T17:23:05Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsProducerTest.java", "diffHunk": "@@ -254,30 +224,30 @@ public void shouldNotCloseProducerIfEosDisabled() {\n \n     @Test\n     public void shouldInitTxOnEos() {\n-        assertTrue(eosMockProducer.transactionInitialized());\n+        assertThat(eosMockProducer.transactionInitialized(), is(true));\n     }\n \n     @Test\n     public void shouldBeginTxOnEosSend() {\n         eosStreamsProducer.send(record, null);\n-        assertTrue(eosMockProducer.transactionInFlight());\n+        assertThat(eosMockProducer.transactionInFlight(), is(true));\n     }\n \n     @Test\n     public void shouldContinueTxnSecondEosSend() {\n         eosStreamsProducer.send(record, null);\n         eosStreamsProducer.send(record, null);\n-        assertTrue(eosMockProducer.transactionInFlight());\n-        assertThat(eosMockProducer.uncommittedRecords().size(), equalTo(2));\n+        assertThat(eosMockProducer.transactionInFlight(), is(true));\n+        assertThat(eosMockProducer.uncommittedRecords().size(), is(2));\n     }\n \n     @Test\n     public void shouldForwardRecordButNotCommitOnEosSend() {\n         eosStreamsProducer.send(record, null);\n-        assertTrue(eosMockProducer.transactionInFlight());\n-        assertTrue(eosMockProducer.history().isEmpty());\n-        assertThat(eosMockProducer.uncommittedRecords().size(), equalTo(1));\n-        assertThat(eosMockProducer.uncommittedRecords().get(0), equalTo(record));\n+        assertThat(eosMockProducer.transactionInFlight(), is(true));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c4a2bcb2a111c9d3b866daca3481f970bababef"}, "originalPosition": 251}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODUzOTYwMg==", "bodyText": "assertThat is nicer in general, but it doesn't really matter. In this case, IDEA offered to translate, and I was already changing a lot of assertions, so I just accepted the translation.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388539602", "createdAt": "2020-03-05T20:17:38Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsProducerTest.java", "diffHunk": "@@ -254,30 +224,30 @@ public void shouldNotCloseProducerIfEosDisabled() {\n \n     @Test\n     public void shouldInitTxOnEos() {\n-        assertTrue(eosMockProducer.transactionInitialized());\n+        assertThat(eosMockProducer.transactionInitialized(), is(true));\n     }\n \n     @Test\n     public void shouldBeginTxOnEosSend() {\n         eosStreamsProducer.send(record, null);\n-        assertTrue(eosMockProducer.transactionInFlight());\n+        assertThat(eosMockProducer.transactionInFlight(), is(true));\n     }\n \n     @Test\n     public void shouldContinueTxnSecondEosSend() {\n         eosStreamsProducer.send(record, null);\n         eosStreamsProducer.send(record, null);\n-        assertTrue(eosMockProducer.transactionInFlight());\n-        assertThat(eosMockProducer.uncommittedRecords().size(), equalTo(2));\n+        assertThat(eosMockProducer.transactionInFlight(), is(true));\n+        assertThat(eosMockProducer.uncommittedRecords().size(), is(2));\n     }\n \n     @Test\n     public void shouldForwardRecordButNotCommitOnEosSend() {\n         eosStreamsProducer.send(record, null);\n-        assertTrue(eosMockProducer.transactionInFlight());\n-        assertTrue(eosMockProducer.history().isEmpty());\n-        assertThat(eosMockProducer.uncommittedRecords().size(), equalTo(1));\n-        assertThat(eosMockProducer.uncommittedRecords().get(0), equalTo(record));\n+        assertThat(eosMockProducer.transactionInFlight(), is(true));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ0NDQ2OA=="}, "originalCommit": {"oid": "2c4a2bcb2a111c9d3b866daca3481f970bababef"}, "originalPosition": 251}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQwNjY0MDMwOnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQxNzoyOTowOFrOFyc-Ow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQyMDoxNzo1MVrOFyilhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ0NzgwMw==", "bodyText": "Are the tests added in TaskManager only trying for more coverage? @vvcephei", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388447803", "createdAt": "2020-03-05T17:29:08Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -24,14 +24,20 @@\n import org.apache.kafka.clients.consumer.Consumer;\n import org.apache.kafka.clients.consumer.ConsumerRecord;\n import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.Metric;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c4a2bcb2a111c9d3b866daca3481f970bababef"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODUzOTc4MA==", "bodyText": "yep, that's right.", "url": "https://github.com/apache/kafka/pull/8213#discussion_r388539780", "createdAt": "2020-03-05T20:17:51Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -24,14 +24,20 @@\n import org.apache.kafka.clients.consumer.Consumer;\n import org.apache.kafka.clients.consumer.ConsumerRecord;\n import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.Metric;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODQ0NzgwMw=="}, "originalCommit": {"oid": "2c4a2bcb2a111c9d3b866daca3481f970bababef"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4141, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}