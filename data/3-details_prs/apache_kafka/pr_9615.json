{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIzNDAyMzg4", "number": 9615, "title": "KAFKA-10500: Add thread option", "bodyText": "Can add stream threads now\nreplace #9581 after rebase\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-11-18T18:54:55Z", "url": "https://github.com/apache/kafka/pull/9615", "merged": true, "mergeCommit": {"oid": "9ece7fe372230e848920d843af2d0d5c2fbaac4d"}, "closed": true, "closedAt": "2020-12-04T19:21:04Z", "author": {"login": "wcarlson5"}, "timelineItems": {"totalCount": 32, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdeGkpGgFqTUzNDY2NDA5Ng==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdi71cXAFqTU0NTE4ODA5NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0NjY0MDk2", "url": "https://github.com/apache/kafka/pull/9615#pullrequestreview-534664096", "createdAt": "2020-11-19T17:06:42Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQxNzowNjo0MlrOH2oyHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQxNzo1Mzo1NVrOH2qt3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzA1MzM0MQ==", "bodyText": "Assume the following thread list [t2, t3, t4], threadIdx would be 4, which is already there. You should keep the currently used threadIdxs and check those to decide on the next threadIdx.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r527053341", "createdAt": "2020-11-19T17:06:42Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,11 +904,57 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        synchronized (stateLock) {\n+            if (state == State.RUNNING || state == State.REBALANCING) {\n+                final int threadIdx = threads.size() + 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzA1NTY4OQ==", "bodyText": "This should be:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            final long cacheSizePerThread = getCacheSizePerThread(threadIdx);\n          \n          \n            \n                            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);", "url": "https://github.com/apache/kafka/pull/9615#discussion_r527055689", "createdAt": "2020-11-19T17:10:00Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,11 +904,57 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        synchronized (stateLock) {\n+            if (state == State.RUNNING || state == State.REBALANCING) {\n+                final int threadIdx = threads.size() + 1;\n+                final long cacheSizePerThread = getCacheSizePerThread(threadIdx);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzA1NjgwMA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            resizeThreadCache(threadIdx);\n          \n          \n            \n                            resizeThreadCache(threads.size() + 1);", "url": "https://github.com/apache/kafka/pull/9615#discussion_r527056800", "createdAt": "2020-11-19T17:11:37Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,11 +904,57 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        synchronized (stateLock) {\n+            if (state == State.RUNNING || state == State.REBALANCING) {\n+                final int threadIdx = threads.size() + 1;\n+                final long cacheSizePerThread = getCacheSizePerThread(threadIdx);\n+                resizeThreadCache(threadIdx);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzA4NTAyMA==", "bodyText": "Where is the stream thread started?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r527085020", "createdAt": "2020-11-19T17:53:55Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,11 +904,57 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        synchronized (stateLock) {\n+            if (state == State.RUNNING || state == State.REBALANCING) {\n+                final int threadIdx = threads.size() + 1;\n+                final long cacheSizePerThread = getCacheSizePerThread(threadIdx);\n+                resizeThreadCache(threadIdx);\n+                final StreamThread streamThread = StreamThread.create(\n+                        internalTopologyBuilder,\n+                        config,\n+                        clientSupplier,\n+                        adminClient,\n+                        processId,\n+                        clientId,\n+                        streamsMetrics,\n+                        time,\n+                        streamsMetadataState,\n+                        cacheSizePerThread,\n+                        stateDirectory,\n+                        delegatingStateRestoreListener,\n+                        threadIdx,\n+                        KafkaStreams.this::closeToError,\n+                        streamsUncaughtExceptionHandler\n+                );\n+                threads.add(streamThread);\n+                threadState.put(streamThread.getId(), streamThread.state());\n+                storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+                streamThread.setStateListener(streamStateListener);\n+                return Optional.of(streamThread.getName());\n+            } else {\n+                return Optional.empty();\n+            }\n+        }\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 177}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0NzgzMTA0", "url": "https://github.com/apache/kafka/pull/9615#pullrequestreview-534783104", "createdAt": "2020-11-19T19:33:01Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQxOTozMzowMlrOH2ucDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMDoyNjoxMFrOH2wRWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0NTk5Ng==", "bodyText": "You should not use try-catch here but just add throws InterruptedException to the method signature.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r527145996", "createdAt": "2020-11-19T19:33:02Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java", "diffHunk": "@@ -588,6 +592,29 @@ public void testCloseIsIdempotent() {\n             closeCount, MockMetricsReporter.CLOSE_COUNT.get());\n     }\n \n+    @Test\n+    public void testAddThread() {\n+        props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1);\n+        final KafkaStreams streams = new KafkaStreams(getBuilderWithSource().build(), props, supplier, time);\n+        streams.start();\n+        final int oldSize = streams.threads.size();\n+        try {\n+            TestUtils.waitForCondition(() -> streams.state() == KafkaStreams.State.RUNNING, 15L, \"wait until running\");\n+        } catch (final InterruptedException e) {\n+            e.printStackTrace();\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0OTYwNA==", "bodyText": "Why do we need to synchronize the whole method on stateLock?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r527149604", "createdAt": "2020-11-19T19:39:18Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,11 +904,72 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        synchronized (stateLock) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE1MDA5NQ==", "bodyText": "Could we also use isRunningOrRebalancing() here?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r527150095", "createdAt": "2020-11-19T19:40:01Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,11 +904,72 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        synchronized (stateLock) {\n+            if (state == State.RUNNING || state == State.REBALANCING) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE2NDgzMg==", "bodyText": "We do something really similar when we start the stream threads at startup. Could you try to extract this part to a method?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r527164832", "createdAt": "2020-11-19T20:05:42Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,11 +904,72 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        synchronized (stateLock) {\n+            if (state == State.RUNNING || state == State.REBALANCING) {\n+                final int threadIdx = getNextThreadIndex();\n+                final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+                resizeThreadCache(threads.size() + 1);\n+                final StreamThread streamThread = StreamThread.create(\n+                        internalTopologyBuilder,\n+                        config,\n+                        clientSupplier,\n+                        adminClient,\n+                        processId,\n+                        clientId,\n+                        streamsMetrics,\n+                        time,\n+                        streamsMetadataState,\n+                        cacheSizePerThread,\n+                        stateDirectory,\n+                        delegatingStateRestoreListener,\n+                        threadIdx,\n+                        KafkaStreams.this::closeToError,\n+                        streamsUncaughtExceptionHandler\n+                );\n+                threads.add(streamThread);\n+                threadState.put(streamThread.getId(), streamThread.state());\n+                storeProviders.add(new StreamThreadStateStoreProvider(streamThread));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3Mjk0Nw==", "bodyText": "I would prefer to use shouldAddThread() as name although the pattern is different for the other test methods.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r527172947", "createdAt": "2020-11-19T20:20:33Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java", "diffHunk": "@@ -588,6 +592,29 @@ public void testCloseIsIdempotent() {\n             closeCount, MockMetricsReporter.CLOSE_COUNT.get());\n     }\n \n+    @Test\n+    public void testAddThread() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3MzIwMA==", "bodyText": "I would prefer to use shouldNotAddThread() as name although the pattern is different for the other test methods.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r527173200", "createdAt": "2020-11-19T20:21:00Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java", "diffHunk": "@@ -588,6 +592,29 @@ public void testCloseIsIdempotent() {\n             closeCount, MockMetricsReporter.CLOSE_COUNT.get());\n     }\n \n+    @Test\n+    public void testAddThread() {\n+        props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1);\n+        final KafkaStreams streams = new KafkaStreams(getBuilderWithSource().build(), props, supplier, time);\n+        streams.start();\n+        final int oldSize = streams.threads.size();\n+        try {\n+            TestUtils.waitForCondition(() -> streams.state() == KafkaStreams.State.RUNNING, 15L, \"wait until running\");\n+        } catch (final InterruptedException e) {\n+            e.printStackTrace();\n+        }\n+        assertThat(streams.addStreamThread(), equalTo(Optional.of(\"newThread\")));\n+        assertThat(streams.threads.size(), equalTo(oldSize + 1));\n+    }\n+\n+    @Test\n+    public void testAddThreadNotDuringStart() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE3NjAyNQ==", "bodyText": "Additionally to the unit tests that you wrote, I think we also need integration tests.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r527176025", "createdAt": "2020-11-19T20:26:10Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,11 +904,72 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 145}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1MjIyMjYw", "url": "https://github.com/apache/kafka/pull/9615#pullrequestreview-535222260", "createdAt": "2020-11-20T08:46:22Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwODo0NjoyM1rOH3F5zA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwOToyNzozM1rOH3HvRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzUzMDQ0NA==", "bodyText": "I think it would be cleaner to pass cacheSizePerThread to resizeThreadCache() instead of the number of stream threads. We would then just call getCacheSizePerThread() once instead of once in addStreamThread() and once in resizeThreadCache(). We would also just need to compute threads.size() + 1 once.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r527530444", "createdAt": "2020-11-20T08:46:23Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,11 +885,77 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread makeThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        synchronized (stateLock) {\n+            if (isRunningOrRebalancing()) {\n+                final int threadIdx = getNextThreadIndex();\n+                final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+                resizeThreadCache(threads.size() + 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU0MzIwOQ==", "bodyText": "IMO, createStreamThread() would describe the behavior better.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r527543209", "createdAt": "2020-11-20T09:04:24Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,11 +885,77 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread makeThread(final long cacheSizePerThread, final int threadIdx) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 143}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzU2MDUxOQ==", "bodyText": "Wouldn't it be sufficent to check in the beginning if the Streams client is in RUNNING or REBALANCING, then optimistically create the stream thread, and before we start the stream thread check if the client is not in PENDING_SHUTDOWN and not in ERROR and not in NOT_RUNNING? State changes between RUNNING and REBALANCING should not affect adding a stream thread, right?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r527560519", "createdAt": "2020-11-20T09:27:33Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,11 +904,72 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        synchronized (stateLock) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0OTYwNA=="}, "originalCommit": null, "originalPosition": 146}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM3MzgyOTcw", "url": "https://github.com/apache/kafka/pull/9615#pullrequestreview-537382970", "createdAt": "2020-11-24T11:07:26Z", "commit": {"oid": "2d44bce5251dbb94c8751739b75424f9e043c0ba"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxMTowNzoyNlrOH47yLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxMjo0Njo0OVrOH4_PQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTQ2MTgwNA==", "bodyText": "Please adjust indentation:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            mkMap(\n          \n          \n            \n                                    mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n          \n          \n            \n                                    mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n          \n          \n            \n                                    mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n          \n          \n            \n                                    mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n          \n          \n            \n                                    mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n          \n          \n            \n                                    mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n          \n          \n            \n                            )\n          \n          \n            \n                        mkMap(\n          \n          \n            \n                            mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n          \n          \n            \n                            mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n          \n          \n            \n                            mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n          \n          \n            \n                            mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n          \n          \n            \n                            mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n          \n          \n            \n                            mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n          \n          \n            \n                        )", "url": "https://github.com/apache/kafka/pull/9615#discussion_r529461804", "createdAt": "2020-11-24T11:07:26Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.Properties;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@Category(IntegrationTest.class)\n+public class AdjustStreamThreadCountTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    private static String inputTopic;\n+    private static StreamsBuilder builder;\n+    private static Properties properties;\n+    private static String appId = \"\";\n+\n+    @Before\n+    public void setup() {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        appId = \"appId_\" + testId;\n+        inputTopic = \"input\" + testId;\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, inputTopic);\n+\n+        builder  = new StreamsBuilder();\n+        builder.stream(inputTopic);\n+\n+        properties  = mkObjectProperties(\n+                mkMap(\n+                        mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                        mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                        mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                        mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n+                        mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n+                        mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n+                )", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d44bce5251dbb94c8751739b75424f9e043c0ba"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTQ2MTk3NQ==", "bodyText": "wrong indentation", "url": "https://github.com/apache/kafka/pull/9615#discussion_r529461975", "createdAt": "2020-11-24T11:07:45Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.Properties;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@Category(IntegrationTest.class)\n+public class AdjustStreamThreadCountTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    private static String inputTopic;\n+    private static StreamsBuilder builder;\n+    private static Properties properties;\n+    private static String appId = \"\";\n+\n+    @Before\n+    public void setup() {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        appId = \"appId_\" + testId;\n+        inputTopic = \"input\" + testId;\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, inputTopic);\n+\n+        builder  = new StreamsBuilder();\n+        builder.stream(inputTopic);\n+\n+        properties  = mkObjectProperties(\n+                mkMap(\n+                        mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                        mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                        mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                        mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n+                        mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n+                        mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n+                )\n+        );\n+    }\n+\n+    @After\n+    public void teardown() throws IOException {\n+        purgeLocalStreamsState(properties);\n+    }\n+\n+    private void produceMessages(final long timestamp, final String streamOneInput, final String msg) {\n+        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n+                streamOneInput,\n+                Collections.singletonList(new KeyValue<>(\"1\", msg)),\n+                TestUtils.producerConfig(\n+                        CLUSTER.bootstrapServers(),\n+                        StringSerializer.class,\n+                        StringSerializer.class,\n+                        new Properties()),\n+                timestamp);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d44bce5251dbb94c8751739b75424f9e043c0ba"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTQ2NDE4Mw==", "bodyText": "We should wait for the new stream thread with a timeout, otherwise we risk that this test may become flaky.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r529464183", "createdAt": "2020-11-24T11:11:22Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.Properties;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@Category(IntegrationTest.class)\n+public class AdjustStreamThreadCountTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    private static String inputTopic;\n+    private static StreamsBuilder builder;\n+    private static Properties properties;\n+    private static String appId = \"\";\n+\n+    @Before\n+    public void setup() {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        appId = \"appId_\" + testId;\n+        inputTopic = \"input\" + testId;\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, inputTopic);\n+\n+        builder  = new StreamsBuilder();\n+        builder.stream(inputTopic);\n+\n+        properties  = mkObjectProperties(\n+                mkMap(\n+                        mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                        mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                        mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                        mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n+                        mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n+                        mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n+                )\n+        );\n+    }\n+\n+    @After\n+    public void teardown() throws IOException {\n+        purgeLocalStreamsState(properties);\n+    }\n+\n+    private void produceMessages(final long timestamp, final String streamOneInput, final String msg) {\n+        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n+                streamOneInput,\n+                Collections.singletonList(new KeyValue<>(\"1\", msg)),\n+                TestUtils.producerConfig(\n+                        CLUSTER.bootstrapServers(),\n+                        StringSerializer.class,\n+                        StringSerializer.class,\n+                        new Properties()),\n+                timestamp);\n+    }\n+\n+    @Test\n+    public void shouldAddStreamThread() throws Exception {\n+        try (final KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), properties)) {\n+            StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams);\n+            final int oldThreadCount = kafkaStreams.localThreadsMetadata().size();\n+\n+            kafkaStreams.addStreamThread();\n+            produceMessages(0L, inputTopic, \"A\");\n+\n+            assertThat(kafkaStreams.localThreadsMetadata().size(), equalTo(oldThreadCount + 1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d44bce5251dbb94c8751739b75424f9e043c0ba"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTQ2NDcxMw==", "bodyText": "Do we need this line?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r529464713", "createdAt": "2020-11-24T11:12:09Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.Properties;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@Category(IntegrationTest.class)\n+public class AdjustStreamThreadCountTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    private static String inputTopic;\n+    private static StreamsBuilder builder;\n+    private static Properties properties;\n+    private static String appId = \"\";\n+\n+    @Before\n+    public void setup() {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        appId = \"appId_\" + testId;\n+        inputTopic = \"input\" + testId;\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, inputTopic);\n+\n+        builder  = new StreamsBuilder();\n+        builder.stream(inputTopic);\n+\n+        properties  = mkObjectProperties(\n+                mkMap(\n+                        mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                        mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                        mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                        mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n+                        mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n+                        mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n+                )\n+        );\n+    }\n+\n+    @After\n+    public void teardown() throws IOException {\n+        purgeLocalStreamsState(properties);\n+    }\n+\n+    private void produceMessages(final long timestamp, final String streamOneInput, final String msg) {\n+        IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(\n+                streamOneInput,\n+                Collections.singletonList(new KeyValue<>(\"1\", msg)),\n+                TestUtils.producerConfig(\n+                        CLUSTER.bootstrapServers(),\n+                        StringSerializer.class,\n+                        StringSerializer.class,\n+                        new Properties()),\n+                timestamp);\n+    }\n+\n+    @Test\n+    public void shouldAddStreamThread() throws Exception {\n+        try (final KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), properties)) {\n+            StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams);\n+            final int oldThreadCount = kafkaStreams.localThreadsMetadata().size();\n+\n+            kafkaStreams.addStreamThread();\n+            produceMessages(0L, inputTopic, \"A\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d44bce5251dbb94c8751739b75424f9e043c0ba"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTUxODQwMw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        synchronized (stateLock) {\n          \n          \n            \n                            if (isRunningOrRebalancing()) {\n          \n          \n            \n                                streamThread.start();\n          \n          \n            \n                            } else {\n          \n          \n            \n                                return Optional.empty();\n          \n          \n            \n                            }\n          \n          \n            \n                        }\n          \n          \n            \n                        return Optional.of(streamThread.getName());\n          \n          \n            \n                        synchronized (stateLock) {\n          \n          \n            \n                            if (isRunningOrRebalancing()) {\n          \n          \n            \n                                streamThread.start();\n          \n          \n            \n                                return Optional.of(streamThread.getName());\n          \n          \n            \n                            } else {\n          \n          \n            \n                                return Optional.empty();\n          \n          \n            \n                            }\n          \n          \n            \n                        }", "url": "https://github.com/apache/kafka/pull/9615#discussion_r529518403", "createdAt": "2020-11-24T12:46:49Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {\n+                if (isRunningOrRebalancing()) {\n+                    streamThread.start();\n+                } else {\n+                    return Optional.empty();\n+                }\n+            }\n+            return Optional.of(streamThread.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d44bce5251dbb94c8751739b75424f9e043c0ba"}, "originalPosition": 194}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM3ODc5MDY3", "url": "https://github.com/apache/kafka/pull/9615#pullrequestreview-537879067", "createdAt": "2020-11-24T20:28:53Z", "commit": {"oid": "70d500aae259184538feb662047f75897bb89b4d"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMDoyODo1M1rOH5T_YQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQyMDoyODo1M1rOH5T_YQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTg1ODQwMQ==", "bodyText": "nit: If not all parameters fit on one line, we put each of them on a new line. Additionally we put also the closing parenthesis on a new line.\nnit: Lines should -- if possible -- not exceed 120 characters.\nnit: I like when tests are visually structured with one block for setup, the call under test, and one block for verifications.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams);\n          \n          \n            \n                        final int oldThreadCount = kafkaStreams.localThreadsMetadata().size();\n          \n          \n            \n            \n          \n          \n            \n                        final Optional<String> name = kafkaStreams.addStreamThread();\n          \n          \n            \n                        assertThat(name, CoreMatchers.not(Optional.empty()));\n          \n          \n            \n                        TestUtils.waitForCondition(() -> kafkaStreams.localThreadsMetadata().stream().sequential().map(ThreadMetadata::threadName).anyMatch(t -> t.equals(name.orElse(\"\"))),\n          \n          \n            \n                            \"Wait for the thread to be added\");\n          \n          \n            \n                        assertThat(kafkaStreams.localThreadsMetadata().size(), equalTo(oldThreadCount + 1));\n          \n          \n            \n                        StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams);\n          \n          \n            \n                        final int oldThreadCount = kafkaStreams.localThreadsMetadata().size();\n          \n          \n            \n            \n          \n          \n            \n                        final Optional<String> name = kafkaStreams.addStreamThread();\n          \n          \n            \n                        \n          \n          \n            \n                        assertThat(name, CoreMatchers.not(Optional.empty()));\n          \n          \n            \n                        TestUtils.waitForCondition(\n          \n          \n            \n                            () -> kafkaStreams.localThreadsMetadata().stream().sequential()\n          \n          \n            \n                                .map(ThreadMetadata::threadName).anyMatch(t -> t.equals(name.orElse(\"\"))), \n          \n          \n            \n                            \"Wait for the thread to be added\"\n          \n          \n            \n                        );\n          \n          \n            \n                        assertThat(kafkaStreams.localThreadsMetadata().size(), equalTo(oldThreadCount + 1));", "url": "https://github.com/apache/kafka/pull/9615#discussion_r529858401", "createdAt": "2020-11-24T20:28:53Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.processor.ThreadMetadata;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.hamcrest.CoreMatchers;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@Category(IntegrationTest.class)\n+public class AdjustStreamThreadCountTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    private static String inputTopic;\n+    private static StreamsBuilder builder;\n+    private static Properties properties;\n+    private static String appId = \"\";\n+\n+    @Before\n+    public void setup() {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        appId = \"appId_\" + testId;\n+        inputTopic = \"input\" + testId;\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, inputTopic);\n+\n+        builder  = new StreamsBuilder();\n+        builder.stream(inputTopic);\n+\n+        properties  = mkObjectProperties(\n+            mkMap(\n+                mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n+                mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n+                mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n+            )\n+        );\n+    }\n+\n+    @After\n+    public void teardown() throws IOException {\n+        purgeLocalStreamsState(properties);\n+    }\n+\n+    @Test\n+    public void shouldAddStreamThread() throws Exception {\n+        try (final KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), properties)) {\n+            StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams);\n+            final int oldThreadCount = kafkaStreams.localThreadsMetadata().size();\n+\n+            final Optional<String> name = kafkaStreams.addStreamThread();\n+            assertThat(name, CoreMatchers.not(Optional.empty()));\n+            TestUtils.waitForCondition(() -> kafkaStreams.localThreadsMetadata().stream().sequential().map(ThreadMetadata::threadName).anyMatch(t -> t.equals(name.orElse(\"\"))),\n+                \"Wait for the thread to be added\");\n+            assertThat(kafkaStreams.localThreadsMetadata().size(), equalTo(oldThreadCount + 1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70d500aae259184538feb662047f75897bb89b4d"}, "originalPosition": 101}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxNDM2OTQ2", "url": "https://github.com/apache/kafka/pull/9615#pullrequestreview-541436946", "createdAt": "2020-12-01T02:10:09Z", "commit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwMjoxMDowOVrOH8VVuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwMjoyODoyMVrOH8Vr_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNjIzMg==", "bodyText": "nit. remove unnecessary this.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533026232", "createdAt": "2020-12-01T02:10:09Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -398,6 +407,7 @@ public void setUncaughtExceptionHandler(final StreamsUncaughtExceptionHandler st\n         final Consumer<Throwable> handler = exception -> handleStreamsUncaughtException(exception, streamsUncaughtExceptionHandler);\n         synchronized (stateLock) {\n             if (state == State.CREATED) {\n+                this.streamsUncaughtExceptionHandler = handler;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyODE1Ng==", "bodyText": "Nit: can we change the loop to int = 1; i <= numStreamThreads and just pass in i here?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533028156", "createdAt": "2020-12-01T02:16:02Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -846,43 +856,24 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 time,\n                 globalThreadId,\n                 delegatingStateRestoreListener,\n-                this::defaultStreamsUncaughtExceptionHandler\n+                streamsUncaughtExceptionHandler\n             );\n             globalThreadState = globalStreamThread.state();\n         }\n \n         // use client id instead of thread client id since this admin client may be shared among threads\n         adminClient = clientSupplier.getAdmin(config.getAdminConfigs(ClientUtils.getSharedAdminClientId(clientId)));\n \n-        final Map<Long, StreamThread.State> threadState = new HashMap<>(numStreamThreads);\n-        final ArrayList<StreamThreadStateStoreProvider> storeProviders = new ArrayList<>();\n+        threadState = new HashMap<>(numStreamThreads);\n+        storeProviders = new ArrayList<>();\n         for (int i = 0; i < numStreamThreads; i++) {\n-            final StreamThread streamThread = StreamThread.create(\n-                internalTopologyBuilder,\n-                config,\n-                clientSupplier,\n-                adminClient,\n-                processId,\n-                clientId,\n-                streamsMetrics,\n-                time,\n-                streamsMetadataState,\n-                cacheSizePerThread,\n-                stateDirectory,\n-                delegatingStateRestoreListener,\n-                i + 1,\n-                KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+            createStreamThread(cacheSizePerThread, i + 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyODQ3NA==", "bodyText": "Add missing <p> tag", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533028474", "createdAt": "2020-12-01T02:17:06Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyODUyNg==", "bodyText": "As above.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533028526", "createdAt": "2020-12-01T02:17:15Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 175}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyODcwOA==", "bodyText": "Should we link to StreamConfig instead?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533028708", "createdAt": "2020-12-01T02:17:54Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTQ4NA==", "bodyText": "Can we create the StreamStateListener before we call createStreamThread and do setStateListener within createStreamThread ? If yes, we also don't need to call setStateListener within addStreamThread", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533029484", "createdAt": "2020-12-01T02:20:24Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -846,43 +856,24 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 time,\n                 globalThreadId,\n                 delegatingStateRestoreListener,\n-                this::defaultStreamsUncaughtExceptionHandler\n+                streamsUncaughtExceptionHandler\n             );\n             globalThreadState = globalStreamThread.state();\n         }\n \n         // use client id instead of thread client id since this admin client may be shared among threads\n         adminClient = clientSupplier.getAdmin(config.getAdminConfigs(ClientUtils.getSharedAdminClientId(clientId)));\n \n-        final Map<Long, StreamThread.State> threadState = new HashMap<>(numStreamThreads);\n-        final ArrayList<StreamThreadStateStoreProvider> storeProviders = new ArrayList<>();\n+        threadState = new HashMap<>(numStreamThreads);\n+        storeProviders = new ArrayList<>();\n         for (int i = 0; i < numStreamThreads; i++) {\n-            final StreamThread streamThread = StreamThread.create(\n-                internalTopologyBuilder,\n-                config,\n-                clientSupplier,\n-                adminClient,\n-                processId,\n-                clientId,\n-                streamsMetrics,\n-                time,\n-                streamsMetadataState,\n-                cacheSizePerThread,\n-                stateDirectory,\n-                delegatingStateRestoreListener,\n-                i + 1,\n-                KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+            createStreamThread(cacheSizePerThread, i + 1);\n         }\n \n         ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) ->\n             Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count()));\n \n-        final StreamStateListener streamStateListener = new StreamStateListener(threadState, globalThreadState);\n+        streamStateListener = new StreamStateListener(threadState, globalThreadState);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTk0Mg==", "bodyText": "Just to clarify for myself: if we don't start() the thread, no harm is done creating it? Or would we need to do some cleanup even if we don't start the thread?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533029942", "createdAt": "2020-12-01T02:21:50Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {\n+                if (isRunningOrRebalancing()) {\n+                    streamThread.start();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDY0OA==", "bodyText": "Don't we need to get the stateLock as an outer most guard (and not check isRunningOrRebalancing() twice)? It seems weird to create a thread but later not start it and throw it away -- especially because we resize the caches (but also don't undo the resizing)?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533030648", "createdAt": "2020-12-01T02:24:21Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDkxNA==", "bodyText": "Why do we compute the names from scratch, but not incrementally maintain them as member variable?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533030914", "createdAt": "2020-12-01T02:25:11Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {\n+                if (isRunningOrRebalancing()) {\n+                    streamThread.start();\n+                    return Optional.of(streamThread.getName());\n+                } else {\n+                    return Optional.empty();\n+                }\n+            }\n+        } else {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private int getNextThreadIndex() {\n+        final HashSet<String> names = new HashSet<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTIyOA==", "bodyText": "Why this check?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533031228", "createdAt": "2020-12-01T02:26:12Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java", "diffHunk": "@@ -76,6 +76,9 @@ public void resize(final long newCacheSizeBytes) {\n         final boolean shrink = newCacheSizeBytes < maxCacheSizeBytes;\n         maxCacheSizeBytes = newCacheSizeBytes;\n         if (shrink) {\n+            if (caches.values().isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTY1Mg==", "bodyText": "shouldAddThreadWhenRunning", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533031652", "createdAt": "2020-12-01T02:27:33Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java", "diffHunk": "@@ -588,6 +592,25 @@ public void testCloseIsIdempotent() {\n             closeCount, MockMetricsReporter.CLOSE_COUNT.get());\n     }\n \n+    @Test\n+    public void shouldAddThread() throws InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTkzNA==", "bodyText": "shouldNotAddThreadWhenCreated\nShould we also close() KafkaStreams and check that adding a thread is not possible and/or even put the client into ERROR state and test?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533031934", "createdAt": "2020-12-01T02:28:21Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java", "diffHunk": "@@ -588,6 +592,25 @@ public void testCloseIsIdempotent() {\n             closeCount, MockMetricsReporter.CLOSE_COUNT.get());\n     }\n \n+    @Test\n+    public void shouldAddThread() throws InterruptedException {\n+        props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1);\n+        final KafkaStreams streams = new KafkaStreams(getBuilderWithSource().build(), props, supplier, time);\n+        streams.start();\n+        final int oldSize = streams.threads.size();\n+        TestUtils.waitForCondition(() -> streams.state() == KafkaStreams.State.RUNNING, 15L, \"wait until running\");\n+        assertThat(streams.addStreamThread(), equalTo(Optional.of(\"newThread\")));\n+        assertThat(streams.threads.size(), equalTo(oldSize + 1));\n+    }\n+\n+    @Test\n+    public void shouldNotAddThread() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyMTAzNTkw", "url": "https://github.com/apache/kafka/pull/9615#pullrequestreview-542103590", "createdAt": "2020-12-01T16:55:02Z", "commit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxNjo1NTowMlrOH82cYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxNzoyNzoxNlrOH831HA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU2ODYxMQ==", "bodyText": "the this. is necessary. the parameter is the same name", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533568611", "createdAt": "2020-12-01T16:55:02Z", "author": {"login": "wcarlson5"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -398,6 +407,7 @@ public void setUncaughtExceptionHandler(final StreamsUncaughtExceptionHandler st\n         final Consumer<Throwable> handler = exception -> handleStreamsUncaughtException(exception, streamsUncaughtExceptionHandler);\n         synchronized (stateLock) {\n             if (state == State.CREATED) {\n+                this.streamsUncaughtExceptionHandler = handler;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyNjIzMg=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU3NDE3Mg==", "bodyText": "I think so", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533574172", "createdAt": "2020-12-01T17:02:31Z", "author": {"login": "wcarlson5"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -846,43 +856,24 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 time,\n                 globalThreadId,\n                 delegatingStateRestoreListener,\n-                this::defaultStreamsUncaughtExceptionHandler\n+                streamsUncaughtExceptionHandler\n             );\n             globalThreadState = globalStreamThread.state();\n         }\n \n         // use client id instead of thread client id since this admin client may be shared among threads\n         adminClient = clientSupplier.getAdmin(config.getAdminConfigs(ClientUtils.getSharedAdminClientId(clientId)));\n \n-        final Map<Long, StreamThread.State> threadState = new HashMap<>(numStreamThreads);\n-        final ArrayList<StreamThreadStateStoreProvider> storeProviders = new ArrayList<>();\n+        threadState = new HashMap<>(numStreamThreads);\n+        storeProviders = new ArrayList<>();\n         for (int i = 0; i < numStreamThreads; i++) {\n-            final StreamThread streamThread = StreamThread.create(\n-                internalTopologyBuilder,\n-                config,\n-                clientSupplier,\n-                adminClient,\n-                processId,\n-                clientId,\n-                streamsMetrics,\n-                time,\n-                streamsMetadataState,\n-                cacheSizePerThread,\n-                stateDirectory,\n-                delegatingStateRestoreListener,\n-                i + 1,\n-                KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+            createStreamThread(cacheSizePerThread, i + 1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyODE1Ng=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU3NjczNg==", "bodyText": "good idea. I don't know why the SteamStateListener is created after the stream threads are made but it seems to work.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533576736", "createdAt": "2020-12-01T17:06:07Z", "author": {"login": "wcarlson5"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -846,43 +856,24 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 time,\n                 globalThreadId,\n                 delegatingStateRestoreListener,\n-                this::defaultStreamsUncaughtExceptionHandler\n+                streamsUncaughtExceptionHandler\n             );\n             globalThreadState = globalStreamThread.state();\n         }\n \n         // use client id instead of thread client id since this admin client may be shared among threads\n         adminClient = clientSupplier.getAdmin(config.getAdminConfigs(ClientUtils.getSharedAdminClientId(clientId)));\n \n-        final Map<Long, StreamThread.State> threadState = new HashMap<>(numStreamThreads);\n-        final ArrayList<StreamThreadStateStoreProvider> storeProviders = new ArrayList<>();\n+        threadState = new HashMap<>(numStreamThreads);\n+        storeProviders = new ArrayList<>();\n         for (int i = 0; i < numStreamThreads; i++) {\n-            final StreamThread streamThread = StreamThread.create(\n-                internalTopologyBuilder,\n-                config,\n-                clientSupplier,\n-                adminClient,\n-                processId,\n-                clientId,\n-                streamsMetrics,\n-                time,\n-                streamsMetadataState,\n-                cacheSizePerThread,\n-                stateDirectory,\n-                delegatingStateRestoreListener,\n-                i + 1,\n-                KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+            createStreamThread(cacheSizePerThread, i + 1);\n         }\n \n         ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) ->\n             Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count()));\n \n-        final StreamStateListener streamStateListener = new StreamStateListener(threadState, globalThreadState);\n+        streamStateListener = new StreamStateListener(threadState, globalThreadState);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTQ4NA=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU3ODQ4Ng==", "bodyText": "I think that makes more sense.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533578486", "createdAt": "2020-12-01T17:08:46Z", "author": {"login": "wcarlson5"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyODcwOA=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU4Mjc2MA==", "bodyText": "I think I explain this above. But we can remove from the thread list.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533582760", "createdAt": "2020-12-01T17:15:03Z", "author": {"login": "wcarlson5"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {\n+                if (isRunningOrRebalancing()) {\n+                    streamThread.start();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTk0Mg=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU4NDk1Ng==", "bodyText": "If it is not running or rebalancing we after it was already running or rebalancing on line 930 we know the client has stopped. The number of threads will be reset to the config and everything will be rebuilt anyways, so changing the cache size should not matter.\nThe state lock is so that in between the second check and starting the thread the state does not change to pending shutdown or something else. I don't think its necessary to guard the whole method as the cache should be thrown out if it's not being started.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533584956", "createdAt": "2020-12-01T17:18:08Z", "author": {"login": "wcarlson5"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDY0OA=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU4NzcxOQ==", "bodyText": "As threads are removed we want to reuse those names, so incrementing would not work for us. Maybe there is away to store a next name, but then the logic would have to be spread out in a few places and I prefer to just compute a few names.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533587719", "createdAt": "2020-12-01T17:22:00Z", "author": {"login": "wcarlson5"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {\n+                if (isRunningOrRebalancing()) {\n+                    streamThread.start();\n+                    return Optional.of(streamThread.getName());\n+                } else {\n+                    return Optional.empty();\n+                }\n+            }\n+        } else {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private int getNextThreadIndex() {\n+        final HashSet<String> names = new HashSet<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDkxNA=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU4ODcxMg==", "bodyText": "Apparently CircularIterators throw an error if they are made on empty lists. And if there are no caches to resize we don't need to bother with the rest of this.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533588712", "createdAt": "2020-12-01T17:23:30Z", "author": {"login": "wcarlson5"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java", "diffHunk": "@@ -76,6 +76,9 @@ public void resize(final long newCacheSizeBytes) {\n         final boolean shrink = newCacheSizeBytes < maxCacheSizeBytes;\n         maxCacheSizeBytes = newCacheSizeBytes;\n         if (shrink) {\n+            if (caches.values().isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTIyOA=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU5MTMyNA==", "bodyText": "Good idea I'll add some tests.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533591324", "createdAt": "2020-12-01T17:27:16Z", "author": {"login": "wcarlson5"}, "path": "streams/src/test/java/org/apache/kafka/streams/KafkaStreamsTest.java", "diffHunk": "@@ -588,6 +592,25 @@ public void testCloseIsIdempotent() {\n             closeCount, MockMetricsReporter.CLOSE_COUNT.get());\n     }\n \n+    @Test\n+    public void shouldAddThread() throws InterruptedException {\n+        props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1);\n+        final KafkaStreams streams = new KafkaStreams(getBuilderWithSource().build(), props, supplier, time);\n+        streams.start();\n+        final int oldSize = streams.threads.size();\n+        TestUtils.waitForCondition(() -> streams.state() == KafkaStreams.State.RUNNING, 15L, \"wait until running\");\n+        assertThat(streams.addStreamThread(), equalTo(Optional.of(\"newThread\")));\n+        assertThat(streams.threads.size(), equalTo(oldSize + 1));\n+    }\n+\n+    @Test\n+    public void shouldNotAddThread() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTkzNA=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyMTc4OTc2", "url": "https://github.com/apache/kafka/pull/9615#pullrequestreview-542178976", "createdAt": "2020-12-01T18:24:56Z", "commit": {"oid": "4cd470a4a5883481a5bd79d9b992a717470e5413"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxODoyNDo1NlrOH86Ftg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxODozOTo1MVrOH86pSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyODM0Mg==", "bodyText": "i + 1 -> i\nWondering why this does not result in a test failure? (Or does it; Jenkins is still running.) -- Maybe we want to add a small test that verifies that we name threads correctly.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533628342", "createdAt": "2020-12-01T18:24:56Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -846,18 +856,37 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 time,\n                 globalThreadId,\n                 delegatingStateRestoreListener,\n-                this::defaultStreamsUncaughtExceptionHandler\n+                streamsUncaughtExceptionHandler\n             );\n             globalThreadState = globalStreamThread.state();\n         }\n \n         // use client id instead of thread client id since this admin client may be shared among threads\n         adminClient = clientSupplier.getAdmin(config.getAdminConfigs(ClientUtils.getSharedAdminClientId(clientId)));\n \n-        final Map<Long, StreamThread.State> threadState = new HashMap<>(numStreamThreads);\n-        final ArrayList<StreamThreadStateStoreProvider> storeProviders = new ArrayList<>();\n-        for (int i = 0; i < numStreamThreads; i++) {\n-            final StreamThread streamThread = StreamThread.create(\n+        threadState = new HashMap<>(numStreamThreads);\n+        storeProviders = new ArrayList<>();\n+        streamStateListener = new StreamStateListener(threadState, globalThreadState);\n+        if (hasGlobalTopology) {\n+            globalStreamThread.setStateListener(streamStateListener);\n+        }\n+        for (int i = 1; i <= numStreamThreads; i++) {\n+            createStreamThread(cacheSizePerThread, i + 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4cd470a4a5883481a5bd79d9b992a717470e5413"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYzMTUzMQ==", "bodyText": "That is not what I meant. But it might not matter much anyway.\nWhile we need to loop over all used names in L951 below to reuse, we don't need to compute names from scratch but would just modify names each time we add/remove a thread. But it's not perf-critical so re-doing the computation is fine, too.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533631531", "createdAt": "2020-12-01T18:30:03Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {\n+                if (isRunningOrRebalancing()) {\n+                    streamThread.start();\n+                    return Optional.of(streamThread.getName());\n+                } else {\n+                    return Optional.empty();\n+                }\n+            }\n+        } else {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private int getNextThreadIndex() {\n+        final HashSet<String> names = new HashSet<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDkxNA=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYzMjMzMg==", "bodyText": "I see -- so it's a bug fix on the side (not directly related to this RP) -- the original PR that added this method should have added this check.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533632332", "createdAt": "2020-12-01T18:31:22Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java", "diffHunk": "@@ -76,6 +76,9 @@ public void resize(final long newCacheSizeBytes) {\n         final boolean shrink = newCacheSizeBytes < maxCacheSizeBytes;\n         maxCacheSizeBytes = newCacheSizeBytes;\n         if (shrink) {\n+            if (caches.values().isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTIyOA=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYzNDA0OQ==", "bodyText": "I see. So we exploit that possible state transitions are limited. Thanks for explaining. Makes sense.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533634049", "createdAt": "2020-12-01T18:34:23Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDY0OA=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYzNzQ1MQ==", "bodyText": "Maybe. We should ensure that we do proper cleanup for all cases.\nWhat make we wonder: I don't see any code (except the remove you added below) that would remove a StreamThread from the list? Will this be done in a different PR?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533637451", "createdAt": "2020-12-01T18:39:51Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {\n+                if (isRunningOrRebalancing()) {\n+                    streamThread.start();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTk0Mg=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 189}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyMjA5NDAz", "url": "https://github.com/apache/kafka/pull/9615#pullrequestreview-542209403", "createdAt": "2020-12-01T19:05:09Z", "commit": {"oid": "4cd470a4a5883481a5bd79d9b992a717470e5413"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxOTowNTowOVrOH87mvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxOToxMzo0NlrOH875hA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1MzE4MA==", "bodyText": "good catch. I don't think we make sure the thread index starts at 1. But let me fix that", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533653180", "createdAt": "2020-12-01T19:05:09Z", "author": {"login": "wcarlson5"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -846,18 +856,37 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 time,\n                 globalThreadId,\n                 delegatingStateRestoreListener,\n-                this::defaultStreamsUncaughtExceptionHandler\n+                streamsUncaughtExceptionHandler\n             );\n             globalThreadState = globalStreamThread.state();\n         }\n \n         // use client id instead of thread client id since this admin client may be shared among threads\n         adminClient = clientSupplier.getAdmin(config.getAdminConfigs(ClientUtils.getSharedAdminClientId(clientId)));\n \n-        final Map<Long, StreamThread.State> threadState = new HashMap<>(numStreamThreads);\n-        final ArrayList<StreamThreadStateStoreProvider> storeProviders = new ArrayList<>();\n-        for (int i = 0; i < numStreamThreads; i++) {\n-            final StreamThread streamThread = StreamThread.create(\n+        threadState = new HashMap<>(numStreamThreads);\n+        storeProviders = new ArrayList<>();\n+        streamStateListener = new StreamStateListener(threadState, globalThreadState);\n+        if (hasGlobalTopology) {\n+            globalStreamThread.setStateListener(streamStateListener);\n+        }\n+        for (int i = 1; i <= numStreamThreads; i++) {\n+            createStreamThread(cacheSizePerThread, i + 1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzYyODM0Mg=="}, "originalCommit": {"oid": "4cd470a4a5883481a5bd79d9b992a717470e5413"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1NjczMg==", "bodyText": "There will be two more cases of remove. In the replace thread option and in the remove thread option.\nI'm not really convinced it is necessary but I don't see a problem with re-resizing the cache if we do not start the thread. I don't think there will be any side affects as the client should be shutting down, but if we resize there should be a little extra info in the state and store providers but it would not get used", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533656732", "createdAt": "2020-12-01T19:11:28Z", "author": {"login": "wcarlson5"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {\n+                if (isRunningOrRebalancing()) {\n+                    streamThread.start();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAyOTk0Mg=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 189}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1NzY1OQ==", "bodyText": "yeah, I didn't realize this was a problem, but when I added more test coverage it showed up", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533657659", "createdAt": "2020-12-01T19:13:11Z", "author": {"login": "wcarlson5"}, "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java", "diffHunk": "@@ -76,6 +76,9 @@ public void resize(final long newCacheSizeBytes) {\n         final boolean shrink = newCacheSizeBytes < maxCacheSizeBytes;\n         maxCacheSizeBytes = newCacheSizeBytes;\n         if (shrink) {\n+            if (caches.values().isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMTIyOA=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1Nzk4OA==", "bodyText": "I'll remove a few of the unnecessary + operations then", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533657988", "createdAt": "2020-12-01T19:13:46Z", "author": {"login": "wcarlson5"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -894,19 +885,88 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+                internalTopologyBuilder,\n+                config,\n+                clientSupplier,\n+                adminClient,\n+                processId,\n+                clientId,\n+                streamsMetrics,\n+                time,\n+                streamsMetadataState,\n+                cacheSizePerThread,\n+                stateDirectory,\n+                delegatingStateRestoreListener,\n+                threadIdx,\n+                KafkaStreams.this::closeToError,\n+                streamsUncaughtExceptionHandler\n+        );\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     *\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@code cache.max.bytes.buffering}.\n+     *\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            streamThread.setStateListener(streamStateListener);\n+            synchronized (stateLock) {\n+                if (isRunningOrRebalancing()) {\n+                    streamThread.start();\n+                    return Optional.of(streamThread.getName());\n+                } else {\n+                    return Optional.empty();\n+                }\n+            }\n+        } else {\n+            return Optional.empty();\n+        }\n+    }\n+\n+    private int getNextThreadIndex() {\n+        final HashSet<String> names = new HashSet<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzAzMDkxNA=="}, "originalCommit": {"oid": "e2130ad1193ea2de05814b1218f33f216a438b6c"}, "originalPosition": 201}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyNDc5MDUw", "url": "https://github.com/apache/kafka/pull/9615#pullrequestreview-542479050", "createdAt": "2020-12-02T03:51:03Z", "commit": {"oid": "85f5ca44aaf6a9f7b3bd4e5ba098e8a67d2219d9"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwMzo1MTowNFrOH9JX7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwMzo1NjowMVrOH9JdWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg3ODc2Nw==", "bodyText": "We should also shutdown the thread if it doesn't get started, otherwise me may leak (consumer or producer) clients. But I'm actually not sure why we don't just do everything (resize cache, create thread) inside the synchronized block? I'm guessing it would deadlock due to locking on the statelock but can't we just synchronize on something else that wouldn't interfere with the StreamThread creation?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533878767", "createdAt": "2020-12-02T03:51:04Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -870,43 +899,73 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 cacheSizePerThread,\n                 stateDirectory,\n                 delegatingStateRestoreListener,\n-                i + 1,\n+                threadIdx,\n                 KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n-        }\n+                streamsUncaughtExceptionHandler\n+        );\n+        streamThread.setStateListener(streamStateListener);\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n \n-        ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) ->\n-            Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count()));\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     * <p>\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@link StreamsConfig#CACHE_MAX_BYTES_BUFFERING_CONFIG}.\n+     * <p>\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        if (isRunningOrRebalancing()) {\n+            final int threadIdx = getNextThreadIndex();\n+            final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+            resizeThreadCache(cacheSizePerThread);\n+            final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+            synchronized (stateLock) {\n+                if (isRunningOrRebalancing()) {\n+                    streamThread.start();\n+                    return Optional.of(streamThread.getName());\n+                } else {\n+                    threads.remove(streamThread);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85f5ca44aaf6a9f7b3bd4e5ba098e8a67d2219d9"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg4MDE1NA==", "bodyText": "Can we also assert that the state gets to RUNNING after the new thread has joined", "url": "https://github.com/apache/kafka/pull/9615#discussion_r533880154", "createdAt": "2020-12-02T03:56:01Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/AdjustStreamThreadCountTest.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.StreamsConfig;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.processor.ThreadMetadata;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.hamcrest.CoreMatchers;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TestName;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import java.util.Properties;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.apache.kafka.common.utils.Utils.mkObjectProperties;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.purgeLocalStreamsState;\n+import static org.apache.kafka.streams.integration.utils.IntegrationTestUtils.safeUniqueTestName;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+\n+@Category(IntegrationTest.class)\n+public class AdjustStreamThreadCountTest {\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    private static String inputTopic;\n+    private static StreamsBuilder builder;\n+    private static Properties properties;\n+    private static String appId = \"\";\n+\n+    @Before\n+    public void setup() {\n+        final String testId = safeUniqueTestName(getClass(), testName);\n+        appId = \"appId_\" + testId;\n+        inputTopic = \"input\" + testId;\n+        IntegrationTestUtils.cleanStateBeforeTest(CLUSTER, inputTopic);\n+\n+        builder  = new StreamsBuilder();\n+        builder.stream(inputTopic);\n+\n+        properties  = mkObjectProperties(\n+            mkMap(\n+                mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),\n+                mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),\n+                mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),\n+                mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),\n+                mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),\n+                mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)\n+            )\n+        );\n+    }\n+\n+    @After\n+    public void teardown() throws IOException {\n+        purgeLocalStreamsState(properties);\n+    }\n+\n+    @Test\n+    public void shouldAddStreamThread() throws Exception {\n+        try (final KafkaStreams kafkaStreams = new KafkaStreams(builder.build(), properties)) {\n+            StreamsTestUtils.startKafkaStreamsAndWaitForRunningState(kafkaStreams);\n+            final int oldThreadCount = kafkaStreams.localThreadsMetadata().size();\n+\n+            final Optional<String> name = kafkaStreams.addStreamThread();\n+\n+            assertThat(name, CoreMatchers.not(Optional.empty()));\n+            TestUtils.waitForCondition(\n+                () -> kafkaStreams.localThreadsMetadata().stream().sequential()\n+                        .map(ThreadMetadata::threadName).anyMatch(t -> t.equals(name.orElse(\"\"))),\n+                \"Wait for the thread to be added\"\n+            );\n+            assertThat(kafkaStreams.localThreadsMetadata().size(), equalTo(oldThreadCount + 1));\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85f5ca44aaf6a9f7b3bd4e5ba098e8a67d2219d9"}, "originalPosition": 106}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0MTQwODA3", "url": "https://github.com/apache/kafka/pull/9615#pullrequestreview-544140807", "createdAt": "2020-12-03T16:26:24Z", "commit": {"oid": "6f6e589e0024878ebfd8b4657e8a04df5278bb90"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QxNjoyNjoyNFrOH-lPZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QxNjo1NDowNVrOH-mteA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM4MzkwOQ==", "bodyText": "nit: If it happens that you need to push another commit, could you fix the indentation here? Sorry that I haven't noticed this before.", "url": "https://github.com/apache/kafka/pull/9615#discussion_r535383909", "createdAt": "2020-12-03T16:26:24Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -870,43 +900,75 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 cacheSizePerThread,\n                 stateDirectory,\n                 delegatingStateRestoreListener,\n-                i + 1,\n+                threadIdx,\n                 KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n-        }\n+                streamsUncaughtExceptionHandler", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f6e589e0024878ebfd8b4657e8a04df5278bb90"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM5NjE0OQ==", "bodyText": "Sorry to bother you again with the synchronization on the stateLock, but could you explain why we still need it after we synchronize on newThread?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r535396149", "createdAt": "2020-12-03T16:41:36Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -870,43 +900,75 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 cacheSizePerThread,\n                 stateDirectory,\n                 delegatingStateRestoreListener,\n-                i + 1,\n+                threadIdx,\n                 KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n-        }\n+                streamsUncaughtExceptionHandler\n+        );\n+        streamThread.setStateListener(streamStateListener);\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n \n-        ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) ->\n-            Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count()));\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     * <p>\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@link StreamsConfig#CACHE_MAX_BYTES_BUFFERING_CONFIG}.\n+     * <p>\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        synchronized (newThread) {\n+            if (isRunningOrRebalancing()) {\n+                final int threadIdx = getNextThreadIndex();\n+                final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+                resizeThreadCache(cacheSizePerThread);\n+                final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+                synchronized (stateLock) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f6e589e0024878ebfd8b4657e8a04df5278bb90"}, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQwNzk5Mg==", "bodyText": "Shouldn't that be int i = 1; i <= threads.size(); i++? Otherwise, we would look up *-StreamThread-0\" and we would not look up \"*-StreamThread-\" + threads.size().\nCould you add some tests that check the correct naming as @mjsax suggested?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r535407992", "createdAt": "2020-12-03T16:54:05Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -870,43 +900,75 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 cacheSizePerThread,\n                 stateDirectory,\n                 delegatingStateRestoreListener,\n-                i + 1,\n+                threadIdx,\n                 KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n-        }\n+                streamsUncaughtExceptionHandler\n+        );\n+        streamThread.setStateListener(streamStateListener);\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n \n-        ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) ->\n-            Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count()));\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     * <p>\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@link StreamsConfig#CACHE_MAX_BYTES_BUFFERING_CONFIG}.\n+     * <p>\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        synchronized (newThread) {\n+            if (isRunningOrRebalancing()) {\n+                final int threadIdx = getNextThreadIndex();\n+                final long cacheSizePerThread = getCacheSizePerThread(threads.size() + 1);\n+                resizeThreadCache(cacheSizePerThread);\n+                final StreamThread streamThread = createStreamThread(cacheSizePerThread, threadIdx);\n+                synchronized (stateLock) {\n+                    if (isRunningOrRebalancing()) {\n+                        streamThread.start();\n+                        return Optional.of(streamThread.getName());\n+                    } else {\n+                        streamThread.shutdown();\n+                        threads.remove(streamThread);\n+                        resizeThreadCache(getCacheSizePerThread(threads.size()));\n+                        return Optional.empty();\n+                    }\n+                }\n+            }\n+        }\n+        return Optional.empty();\n+    }\n \n-        final StreamStateListener streamStateListener = new StreamStateListener(threadState, globalThreadState);\n-        if (hasGlobalTopology) {\n-            globalStreamThread.setStateListener(streamStateListener);\n+    private int getNextThreadIndex() {\n+        final HashSet<String> names = new HashSet<>();\n+        for (final StreamThread streamThread: threads) {\n+            names.add(streamThread.getName());\n         }\n-        for (final StreamThread thread : threads) {\n-            thread.setStateListener(streamStateListener);\n+        final String baseName = clientId + \"-StreamThread-\";\n+        for (int i = 0; i < threads.size(); i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f6e589e0024878ebfd8b4657e8a04df5278bb90"}, "originalPosition": 200}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dfbd9f47becff83cde4092e46ba180a9f6c49eb0", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/dfbd9f47becff83cde4092e46ba180a9f6c49eb0", "committedDate": "2020-12-03T19:09:25Z", "message": "ThreadCache Resizes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dc4fcb8572b7dd4bb9e99a0879a4ea0a0ad28524", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/dc4fcb8572b7dd4bb9e99a0879a4ea0a0ad28524", "committedDate": "2020-12-03T19:09:25Z", "message": "comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5b2df7e44c52b54943fb09bd860215f803a7147e", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/5b2df7e44c52b54943fb09bd860215f803a7147e", "committedDate": "2020-12-03T19:09:25Z", "message": "improved names and checks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "82f307c87ea2484c6bf75f0f376dd620255fb246", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/82f307c87ea2484c6bf75f0f376dd620255fb246", "committedDate": "2020-12-03T19:09:25Z", "message": "make helper for cache size"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "65f237dd9336ef3bacae8516b7d1b402242199f8", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/65f237dd9336ef3bacae8516b7d1b402242199f8", "committedDate": "2020-12-03T19:09:25Z", "message": "remove redundant check"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c0a94e3c006e044b96f3c00682fab938efdbea85", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/c0a94e3c006e044b96f3c00682fab938efdbea85", "committedDate": "2020-12-03T19:09:25Z", "message": "add thread"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ad38800664d81cbe0c57fabd8f6be4c980e28482", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/ad38800664d81cbe0c57fabd8f6be4c980e28482", "committedDate": "2020-12-03T19:09:25Z", "message": "add thread tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d43bb5e1eb2f0519cfa11e27198dc3531ca54115", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/d43bb5e1eb2f0519cfa11e27198dc3531ca54115", "committedDate": "2020-12-03T19:09:25Z", "message": "remove extra methods"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d8793d6115eec1445d15b03cb8bd202e098add8e", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/d8793d6115eec1445d15b03cb8bd202e098add8e", "committedDate": "2020-12-03T19:09:25Z", "message": "add line"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f77626c0185e474f491cdbcb4327525817439c1", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/9f77626c0185e474f491cdbcb4327525817439c1", "committedDate": "2020-12-03T19:09:25Z", "message": "fix tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c42551a49580dd658f141a5b294a30df66eb4d79", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/c42551a49580dd658f141a5b294a30df66eb4d79", "committedDate": "2020-12-03T19:09:25Z", "message": "add start and gets name properly"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "acd65f2cf50c3066e8b6841f4df660a577d31ddd", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/acd65f2cf50c3066e8b6841f4df660a577d31ddd", "committedDate": "2020-12-03T19:09:25Z", "message": "need to add an int test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8fdaca11be2ac8c30179a9043a236e9a78913579", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/8fdaca11be2ac8c30179a9043a236e9a78913579", "committedDate": "2020-12-03T19:09:25Z", "message": "added int test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a698a6826c5eecce1e6123fecabbebb4503197eb", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/a698a6826c5eecce1e6123fecabbebb4503197eb", "committedDate": "2020-12-03T19:09:25Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "df25f11e53608e91c29573a199656fd7ee7cf36b", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/df25f11e53608e91c29573a199656fd7ee7cf36b", "committedDate": "2020-12-03T19:09:26Z", "message": "reduce line size"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6d394a3bf204905931c42d273a8b34bab9023c41", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/6d394a3bf204905931c42d273a8b34bab9023c41", "committedDate": "2020-12-03T19:09:26Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d818144143e5faaf61916c14c6b862b7a818709d", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/d818144143e5faaf61916c14c6b862b7a818709d", "committedDate": "2020-12-03T19:09:26Z", "message": "address comments pt 2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "99649b9faf95282e0c3a0f8c533e9cc99191b20f", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/99649b9faf95282e0c3a0f8c533e9cc99191b20f", "committedDate": "2020-12-03T19:09:26Z", "message": "wait for running in test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "17d8ca4ff8e91736de49ff8499ad57cd4486bfd4", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/17d8ca4ff8e91736de49ff8499ad57cd4486bfd4", "committedDate": "2020-12-03T19:09:26Z", "message": "shutdown thread"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "af3e5674f77037796801afcd445e126c1aa7f6b0", "author": {"user": {"login": "wcarlson5", "name": "Walker Carlson"}}, "url": "https://github.com/apache/kafka/commit/af3e5674f77037796801afcd445e126c1aa7f6b0", "committedDate": "2020-12-03T19:09:26Z", "message": "Add tests for thread names"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ1MTg4MDk1", "url": "https://github.com/apache/kafka/pull/9615#pullrequestreview-545188095", "createdAt": "2020-12-04T18:11:50Z", "commit": {"oid": "af3e5674f77037796801afcd445e126c1aa7f6b0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxODoxMTo1MFrOH_cRQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxODoxMTo1MFrOH_cRQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI4NTUwNQ==", "bodyText": "Why do we need this new lock-object? Would it not be simpler to just reuse stateLock ?", "url": "https://github.com/apache/kafka/pull/9615#discussion_r536285505", "createdAt": "2020-12-04T18:11:50Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "diffHunk": "@@ -845,67 +856,118 @@ private KafkaStreams(final InternalTopologyBuilder internalTopologyBuilder,\n                 time,\n                 globalThreadId,\n                 delegatingStateRestoreListener,\n-                this::defaultStreamsUncaughtExceptionHandler\n+                streamsUncaughtExceptionHandler\n             );\n             globalThreadState = globalStreamThread.state();\n         }\n \n         // use client id instead of thread client id since this admin client may be shared among threads\n         adminClient = clientSupplier.getAdmin(config.getAdminConfigs(ClientUtils.getSharedAdminClientId(clientId)));\n \n-        final Map<Long, StreamThread.State> threadState = new HashMap<>(numStreamThreads);\n-        final ArrayList<StreamThreadStateStoreProvider> storeProviders = new ArrayList<>();\n-        for (int i = 0; i < numStreamThreads; i++) {\n-            final StreamThread streamThread = StreamThread.create(\n-                internalTopologyBuilder,\n-                config,\n-                clientSupplier,\n-                adminClient,\n-                processId,\n-                clientId,\n-                streamsMetrics,\n-                time,\n-                streamsMetadataState,\n-                cacheSizePerThread,\n-                stateDirectory,\n-                delegatingStateRestoreListener,\n-                i + 1,\n-                KafkaStreams.this::closeToError,\n-                this::defaultStreamsUncaughtExceptionHandler\n-            );\n-            threads.add(streamThread);\n-            threadState.put(streamThread.getId(), streamThread.state());\n-            storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n-        }\n-\n-        ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) ->\n-            Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count()));\n-\n-        final StreamStateListener streamStateListener = new StreamStateListener(threadState, globalThreadState);\n+        threadState = new HashMap<>(numStreamThreads);\n+        storeProviders = new ArrayList<>();\n+        streamStateListener = new StreamStateListener(threadState, globalThreadState);\n         if (hasGlobalTopology) {\n             globalStreamThread.setStateListener(streamStateListener);\n         }\n-        for (final StreamThread thread : threads) {\n-            thread.setStateListener(streamStateListener);\n+        for (int i = 1; i <= numStreamThreads; i++) {\n+            createStreamThread(cacheSizePerThread, i);\n         }\n \n+        ClientMetrics.addNumAliveStreamThreadMetric(streamsMetrics, (metricsConfig, now) ->\n+            Math.toIntExact(threads.stream().filter(thread -> thread.state().isAlive()).count()));\n+\n         final GlobalStateStoreProvider globalStateStoreProvider = new GlobalStateStoreProvider(internalTopologyBuilder.globalStateStores());\n         queryableStoreProvider = new QueryableStoreProvider(storeProviders, globalStateStoreProvider);\n \n         stateDirCleaner = setupStateDirCleaner();\n-        oldHandler = false;\n         maybeWarnAboutCodeInRocksDBConfigSetter(log, config);\n         rocksDBMetricsRecordingService = maybeCreateRocksDBMetricsRecordingService(clientId, config);\n     }\n \n+    private StreamThread createStreamThread(final long cacheSizePerThread, final int threadIdx) {\n+        final StreamThread streamThread = StreamThread.create(\n+            internalTopologyBuilder,\n+            config,\n+            clientSupplier,\n+            adminClient,\n+            processId,\n+            clientId,\n+            streamsMetrics,\n+            time,\n+            streamsMetadataState,\n+            cacheSizePerThread,\n+            stateDirectory,\n+            delegatingStateRestoreListener,\n+            threadIdx,\n+            KafkaStreams.this::closeToError,\n+            streamsUncaughtExceptionHandler\n+        );\n+        streamThread.setStateListener(streamStateListener);\n+        threads.add(streamThread);\n+        threadState.put(streamThread.getId(), streamThread.state());\n+        storeProviders.add(new StreamThreadStateStoreProvider(streamThread));\n+        return streamThread;\n+    }\n+\n+    /**\n+     * Adds and starts a stream thread in addition to the stream threads that are already running in this\n+     * Kafka Streams client.\n+     * <p>\n+     * Since the number of stream threads increases, the sizes of the caches in the new stream thread\n+     * and the existing stream threads are adapted so that the sum of the cache sizes over all stream\n+     * threads does not exceed the total cache size specified in configuration\n+     * {@link StreamsConfig#CACHE_MAX_BYTES_BUFFERING_CONFIG}.\n+     * <p>\n+     * Stream threads can only be added if this Kafka Streams client is in state RUNNING or REBALANCING.\n+     *\n+     * @return name of the added stream thread or empty if a new stream thread could not be added\n+     */\n+    public Optional<String> addStreamThread() {\n+        synchronized (changeThreadCount) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "af3e5674f77037796801afcd445e126c1aa7f6b0"}, "originalPosition": 191}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2816, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}