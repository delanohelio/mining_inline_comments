{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzczODE1Nzcx", "number": 8089, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTo1NToyOVrODe-c9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxODoxODo1NVrODgzXNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzODA3MDk1OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTo1NTozMFrOFoamlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTo1NTozMFrOFoamlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkyMzIyMQ==", "bodyText": "LogCleanerManager.updateCheckpoints() can be called with update=None. In this case, the method removes topic partitions that no longer exist from the checkpoint file. It looks like the test suite does not deal with test cases that remove partitions when testing with mock log cleaner manager. However, it's useful to handle this case explicitly here so that it is easier to debug if code changes, etc.\nI think it's enough to throw an exception here if update=None with the explicit message that this case is not handled (yet) by LogCleanerManagerMock (you can do this by calling `update.getOrElse(throw new...)). Or maybe you have other ideas.", "url": "https://github.com/apache/kafka/pull/8089#discussion_r377923221", "createdAt": "2020-02-11T21:55:30Z", "author": {"login": "apovzner"}, "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -54,6 +54,11 @@ class LogCleanerManagerTest extends Logging {\n     override def allCleanerCheckpoints: Map[TopicPartition, Long] = {\n       cleanerCheckpoints.toMap\n     }\n+\n+    override def updateCheckpoints(dataDir: File, update: Option[(TopicPartition,Long)]): Unit = {\n+      val (tp, offset) = update.get", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "daee236d53286fac06a4dbfd69a36eacd0e79062"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzODExNDQxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMjoxMToxOFrOFobCmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMjoxMToxOFrOFobCmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkzMDM5Mw==", "bodyText": "I agree with your comment on this PR that it is better to move the call to updateCheckpoints to cleanableOffsets / closer to the logic that resets the first dirty offset, since you know exactly when the offset was reset and there is no need to check again. Otherwise, we are opening more possibilities for future bugs if the logic changes but only one place gets updated.", "url": "https://github.com/apache/kafka/pull/8089#discussion_r377930393", "createdAt": "2020-02-11T22:11:18Z", "author": {"login": "apovzner"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -179,7 +179,11 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n         case (topicPartition, log) => // create a LogToClean instance for each\n           try {\n             val lastCleanOffset = lastClean.get(topicPartition)\n+            val logStartOffset = log.logStartOffset\n             val (firstDirtyOffset, firstUncleanableDirtyOffset) = cleanableOffsets(log, lastCleanOffset, now)\n+            // update checkpoint for logs with invalid checkpointed offsets", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "daee236d53286fac06a4dbfd69a36eacd0e79062"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MTExMDgzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxNzoyMjozMFrOFo3miQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxNzoyMjozMFrOFo3miQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODM5ODM0NQ==", "bodyText": "Could you use @param tag for each parameter description? When describing needUpdateCheckpoint, would be useful to mention that firstDirtyOffset is the new checkpoint offset to update to (if true).\nAlso, there may be multiple segments between firstDirtyOffset and firstUncleanableDirtyOffset offsets, so instead of \"for the cleanable segment of a log\" maybe say something like \"that represents a range of dirty offsets that can be cleaned\".", "url": "https://github.com/apache/kafka/pull/8089#discussion_r378398345", "createdAt": "2020-02-12T17:22:30Z", "author": {"login": "apovzner"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -480,6 +484,15 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n   }\n }\n \n+/**\n+ * Helper class for the cleanable segment of a log and whether to update the checkpoint associated with the log\n+ * firstDirtyOffset the lower (inclusive) and firstUncleanableDirtyOffset the upper(exclusive) offsets", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "586de577887a2e221aa02821d8dd8f39a187d74a"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MTE3MTMyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxNzo0MDozMlrOFo4NeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxNzo0MDozMlrOFo4NeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODQwODMxMg==", "bodyText": "It's better to use offsetsToClean. firstDirtyOffset instead of logStartOffset here, because that would remove the assumption in the code that offsetsToClean. firstDirtyOffset would always be log start offset when needUpdateCheckpoint is true.", "url": "https://github.com/apache/kafka/pull/8089#discussion_r378408312", "createdAt": "2020-02-12T17:40:32Z", "author": {"login": "apovzner"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -179,11 +179,15 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n         case (topicPartition, log) => // create a LogToClean instance for each\n           try {\n             val lastCleanOffset = lastClean.get(topicPartition)\n-            val (firstDirtyOffset, firstUncleanableDirtyOffset) = cleanableOffsets(log, lastCleanOffset, now)\n-            val compactionDelayMs = maxCompactionDelay(log, firstDirtyOffset, now)\n+            val logStartOffset = log.logStartOffset\n+            val offsetsToClean = cleanableOffsets(log, lastCleanOffset, now)\n+            // update checkpoint for logs with invalid checkpointed offsets\n+            if (offsetsToClean.needUpdateCheckpoint)\n+              updateCheckpoints(log.dir.getParentFile(), Option(topicPartition, logStartOffset))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "586de577887a2e221aa02821d8dd8f39a187d74a"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MTE5NDc3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxNzo0Nzo0N1rOFo4chA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxNzo0Nzo0N1rOFo4chA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODQxMjE2NA==", "bodyText": "looks like math (scala) got replaced with Math (java), probably unintentional?", "url": "https://github.com/apache/kafka/pull/8089#discussion_r378412164", "createdAt": "2020-02-12T17:47:47Z", "author": {"login": "apovzner"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -572,7 +588,7 @@ private[log] object LogCleanerManager extends Logging {\n       s\"now=$now => firstDirtyOffset=$firstDirtyOffset firstUncleanableOffset=$firstUncleanableDirtyOffset \" +\n       s\"activeSegment.baseOffset=${log.activeSegment.baseOffset}\")\n \n-    (firstDirtyOffset, math.max(firstDirtyOffset, firstUncleanableDirtyOffset))\n+    OffsetsToClean(firstDirtyOffset, Math.max(firstDirtyOffset, firstUncleanableDirtyOffset), needsUpdateCheckpoint)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "586de577887a2e221aa02821d8dd8f39a187d74a"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0MTIyMjE0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxNzo1NjoyMlrOFo4tvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QyMTo1MTo0NVrOFpk0ZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODQxNjU3NQ==", "bodyText": "I think it should be ok to also return true (update checkpoint file) for \"compact, delete\" topics as well, even though we are not printing a warning for them. @junrao What do you think? It seems consistent to update checkpoint for all the cases here that set firstDirtyOffset to log start; unless there is a reason to only update checkpoint when we print the warning about resetting first dirty offset?", "url": "https://github.com/apache/kafka/pull/8089#discussion_r378416575", "createdAt": "2020-02-12T17:56:22Z", "author": {"login": "apovzner"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -515,29 +528,32 @@ private[log] object LogCleanerManager extends Logging {\n     * @param log the log\n     * @param lastCleanOffset the last checkpointed offset\n     * @param now the current time in milliseconds of the cleaning operation\n-    * @return the lower (inclusive) and upper (exclusive) offsets\n+    * @return OffsetsToClean containing offsets for cleanable portion of log and whether the log checkpoint needs updating\n     */\n-  def cleanableOffsets(log: Log, lastCleanOffset: Option[Long], now: Long): (Long, Long) = {\n+  def cleanableOffsets(log: Log, lastCleanOffset: Option[Long], now: Long): OffsetsToClean = {\n     // If the log segments are abnormally truncated and hence the checkpointed offset is no longer valid;\n     // reset to the log starting offset and log the error\n-    val firstDirtyOffset = {\n+    val (firstDirtyOffset, needsUpdateCheckpoint) = {\n       val logStartOffset = log.logStartOffset\n       val checkpointDirtyOffset = lastCleanOffset.getOrElse(logStartOffset)\n \n       if (checkpointDirtyOffset < logStartOffset) {\n         // Don't bother with the warning if compact and delete are enabled.\n-        if (!isCompactAndDelete(log))\n+        if (!isCompactAndDelete(log)) {\n           warn(s\"Resetting first dirty offset of ${log.name} to log start offset $logStartOffset \" +\n             s\"since the checkpointed offset $checkpointDirtyOffset is invalid.\")\n-        logStartOffset\n+          (logStartOffset, true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "586de577887a2e221aa02821d8dd8f39a187d74a"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTEzOTE3Mg==", "bodyText": "The reason that we don't log a warn here is that checkpointDirtyOffset is expected to be less than logStartOffset for compact and delete topic. However, it does seem we should update dirtyOffset in this case.", "url": "https://github.com/apache/kafka/pull/8089#discussion_r379139172", "createdAt": "2020-02-13T21:51:45Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -515,29 +528,32 @@ private[log] object LogCleanerManager extends Logging {\n     * @param log the log\n     * @param lastCleanOffset the last checkpointed offset\n     * @param now the current time in milliseconds of the cleaning operation\n-    * @return the lower (inclusive) and upper (exclusive) offsets\n+    * @return OffsetsToClean containing offsets for cleanable portion of log and whether the log checkpoint needs updating\n     */\n-  def cleanableOffsets(log: Log, lastCleanOffset: Option[Long], now: Long): (Long, Long) = {\n+  def cleanableOffsets(log: Log, lastCleanOffset: Option[Long], now: Long): OffsetsToClean = {\n     // If the log segments are abnormally truncated and hence the checkpointed offset is no longer valid;\n     // reset to the log starting offset and log the error\n-    val firstDirtyOffset = {\n+    val (firstDirtyOffset, needsUpdateCheckpoint) = {\n       val logStartOffset = log.logStartOffset\n       val checkpointDirtyOffset = lastCleanOffset.getOrElse(logStartOffset)\n \n       if (checkpointDirtyOffset < logStartOffset) {\n         // Don't bother with the warning if compact and delete are enabled.\n-        if (!isCompactAndDelete(log))\n+        if (!isCompactAndDelete(log)) {\n           warn(s\"Resetting first dirty offset of ${log.name} to log start offset $logStartOffset \" +\n             s\"since the checkpointed offset $checkpointDirtyOffset is invalid.\")\n-        logStartOffset\n+          (logStartOffset, true)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODQxNjU3NQ=="}, "originalCommit": {"oid": "586de577887a2e221aa02821d8dd8f39a187d74a"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NTczNzA2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QyMTozNzo1NlrOFpkbBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QyMTozNzo1NlrOFpkbBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTEzMjY3Ng==", "bodyText": "To make it clear, perhaps needUpdateCheckpoint can be forceUpdateCheckpoint?", "url": "https://github.com/apache/kafka/pull/8089#discussion_r379132676", "createdAt": "2020-02-13T21:37:56Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -480,6 +483,20 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n   }\n }\n \n+/**\n+ * Helper class for the range of cleanable dirty offsets of a log and whether to update the checkpoint associated with\n+ * the log\n+ *\n+ * @param firstDirtyOffset the lower (inclusive) offset to begin cleaning from\n+ * @param firstUncleanableDirtyOffset the upper(exclusive) offset to clean to\n+ * @param needUpdateCheckpoint whether to update the checkpoint associated with this log. if true, checkpoint should be\n+ *                             reset to firstDirtyOffset\n+ */\n+private case class OffsetsToClean(firstDirtyOffset: Long,\n+                                  firstUncleanableDirtyOffset: Long,\n+                                  needUpdateCheckpoint: Boolean = false) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9f934550f13105385b3094049c69e3c3d56331"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NTc1ODkwOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QyMTo0NTozN1rOFpko7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QyMTo0NTozN1rOFpko7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTEzNjIzNw==", "bodyText": "Could we add some error message that explains the expectation if the test fails? Ditto below.", "url": "https://github.com/apache/kafka/pull/8089#discussion_r379136237", "createdAt": "2020-02-13T21:45:37Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -478,8 +483,30 @@ class LogCleanerManagerTest extends Logging {\n \n     val lastCleanOffset = Some(0L)\n     val cleanableOffsets = LogCleanerManager.cleanableOffsets(log, lastCleanOffset, time.milliseconds)\n-    assertEquals(\"The first cleanable offset starts at the beginning of the log.\", 0L, cleanableOffsets._1)\n-    assertEquals(\"The first uncleanable offset begins with active segment.\", log.activeSegment.baseOffset, cleanableOffsets._2)\n+    assertEquals(\"The first cleanable offset starts at the beginning of the log.\", 0L, cleanableOffsets.firstDirtyOffset)\n+    assertEquals(\"The first uncleanable offset begins with active segment.\", log.activeSegment.baseOffset, cleanableOffsets.firstUncleanableDirtyOffset)\n+  }\n+\n+  @Test\n+  def testCleanableOffsetsNeedsCheckpointReset(): Unit = {\n+    val tp = new TopicPartition(\"foo\", 0)\n+    val logs = setupIncreasinglyFilthyLogs(Seq(tp), startNumBatches = 20, batchIncrement = 5)\n+    logs.get(tp).maybeIncrementLogStartOffset(10L)\n+\n+    var lastCleanOffset = Some(15L)\n+    var cleanableOffsets = LogCleanerManager.cleanableOffsets(logs.get(tp), lastCleanOffset, time.milliseconds)\n+    assertEquals(false, cleanableOffsets.needUpdateCheckpoint)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9f934550f13105385b3094049c69e3c3d56331"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0NTc2MzA5OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QyMTo0NzowM1rOFpkrgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xM1QyMTo0NzowM1rOFpkrgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTEzNjg5Nw==", "bodyText": "Do we need // ?", "url": "https://github.com/apache/kafka/pull/8089#discussion_r379136897", "createdAt": "2020-02-13T21:47:03Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -574,6 +601,45 @@ class LogCleanerManagerTest extends Logging {\n     assertEquals(LogCleaningPaused(1), cleanerManager.cleaningState(tp).get)\n   }\n \n+  /**\n+   * Logs with invalid checkpoint offsets should update their checkpoint offset even if the log doesn't need cleaning\n+   */\n+  @Test\n+  def testCheckpointUpdatedForInvalidOffsetNoCleaning(): Unit = {\n+    val tp = new TopicPartition(\"foo\", 0)\n+    val logs = setupIncreasinglyFilthyLogs(Seq(tp), startNumBatches = 20, batchIncrement = 5)\n+\n+    logs.get(tp).maybeIncrementLogStartOffset(20L)\n+    val cleanerManager = createCleanerManagerMock(logs)\n+    cleanerCheckpoints.put(tp, 15L)\n+\n+    val filthiestLog = cleanerManager.grabFilthiestCompactedLog(time)\n+    assertEquals(None, filthiestLog)\n+    assertEquals(20L, cleanerCheckpoints.get(tp).get)\n+  }\n+\n+  /**\n+   * Logs with invalid checkpoint offsets should update their checkpoint offset even if they aren't selected\n+   * for immediate cleaning\n+//   */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e9f934550f13105385b3094049c69e3c3d56331"}, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0OTMxNjMwOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQyMzo1MzozNVrOFqGr7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQyMzo1MzozNVrOFqGr7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTY5NDA2MA==", "bodyText": "you can also use assertFalse here or assertTrue (below) when testing equality with the boolean value.", "url": "https://github.com/apache/kafka/pull/8089#discussion_r379694060", "createdAt": "2020-02-14T23:53:35Z", "author": {"login": "apovzner"}, "path": "core/src/test/scala/unit/kafka/log/LogCleanerManagerTest.scala", "diffHunk": "@@ -478,8 +483,27 @@ class LogCleanerManagerTest extends Logging {\n \n     val lastCleanOffset = Some(0L)\n     val cleanableOffsets = LogCleanerManager.cleanableOffsets(log, lastCleanOffset, time.milliseconds)\n-    assertEquals(\"The first cleanable offset starts at the beginning of the log.\", 0L, cleanableOffsets._1)\n-    assertEquals(\"The first uncleanable offset begins with active segment.\", log.activeSegment.baseOffset, cleanableOffsets._2)\n+    assertEquals(\"The first cleanable offset starts at the beginning of the log.\", 0L, cleanableOffsets.firstDirtyOffset)\n+    assertEquals(\"The first uncleanable offset begins with active segment.\", log.activeSegment.baseOffset, cleanableOffsets.firstUncleanableDirtyOffset)\n+  }\n+\n+  @Test\n+  def testCleanableOffsetsNeedsCheckpointReset(): Unit = {\n+    val tp = new TopicPartition(\"foo\", 0)\n+    val logs = setupIncreasinglyFilthyLogs(Seq(tp), startNumBatches = 20, batchIncrement = 5)\n+    logs.get(tp).maybeIncrementLogStartOffset(10L)\n+\n+    var lastCleanOffset = Some(15L)\n+    var cleanableOffsets = LogCleanerManager.cleanableOffsets(logs.get(tp), lastCleanOffset, time.milliseconds)\n+    assertEquals(\"Checkpoint offset should not be reset if valid\", false, cleanableOffsets.forceUpdateCheckpoint)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "501f8d3b6e1531d3ef0a1cf6705ffa462b45ea2f"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1NzIyNTUwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxODoxODo1NVrOFrNPow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxODoxODo1NVrOFrNPow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDg1MDA4Mw==", "bodyText": "Should we change needsUpdateCheckpoint to forceUpdateCheckpoint?", "url": "https://github.com/apache/kafka/pull/8089#discussion_r380850083", "createdAt": "2020-02-18T18:18:55Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -515,12 +532,12 @@ private[log] object LogCleanerManager extends Logging {\n     * @param log the log\n     * @param lastCleanOffset the last checkpointed offset\n     * @param now the current time in milliseconds of the cleaning operation\n-    * @return the lower (inclusive) and upper (exclusive) offsets\n+    * @return OffsetsToClean containing offsets for cleanable portion of log and whether the log checkpoint needs updating\n     */\n-  def cleanableOffsets(log: Log, lastCleanOffset: Option[Long], now: Long): (Long, Long) = {\n+  def cleanableOffsets(log: Log, lastCleanOffset: Option[Long], now: Long): OffsetsToClean = {\n     // If the log segments are abnormally truncated and hence the checkpointed offset is no longer valid;\n     // reset to the log starting offset and log the error\n-    val firstDirtyOffset = {\n+    val (firstDirtyOffset, needsUpdateCheckpoint) = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a9d3dd627db241e942dc296946d9f2b3f7a4faf"}, "originalPosition": 62}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4271, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}