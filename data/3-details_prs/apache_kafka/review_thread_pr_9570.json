{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE2NTE4OTA0", "number": 9570, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNTo1OTo0MVrOE19KHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMzowNzo1NVrOE5wUiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDExOTk4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/errors/TaskTimeoutExceptions.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNTo1OTo0MVrOHuhImA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNTo1OTo0MVrOHuhImA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODUzOTQxNg==", "bodyText": "If we commit multiple tasks individually (ie, eos-alpha), we use this class as an \"exception container\" to track the TimeoutException for each failed task individually.\nTo simplify the caller code, we also wrap a single TimeoutException if we commit all tasks at once (at-least-once, eos-beta)", "url": "https://github.com/apache/kafka/pull/9570#discussion_r518539416", "createdAt": "2020-11-06T05:59:41Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/errors/TaskTimeoutExceptions.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.errors;\n+\n+import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.streams.processor.internals.Task;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+public class TaskTimeoutExceptions extends StreamsException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDEyMDc0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNjowMDowNlrOHuhJBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNjowMDowNlrOHuhJBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODUzOTUyNw==", "bodyText": "Just fixing some naming issue.", "url": "https://github.com/apache/kafka/pull/9570#discussion_r518539527", "createdAt": "2020-11-06T06:00:06Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicManager.java", "diffHunk": "@@ -164,9 +164,9 @@ public InternalTopicManager(final Time time,\n                                 \"Error message was: {}\", topicName, cause.toString());\n                             throw new StreamsException(String.format(\"Could not create topic %s.\", topicName), cause);\n                         }\n-                    } catch (final TimeoutException retryableException) {\n+                    } catch (final TimeoutException retriableException) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDEyMjE4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNjowMDo0N1rOHuhJ1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNjowMDo0N1rOHuhJ1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODUzOTczNQ==", "bodyText": "This is an open question. Input is welcome. I would like to tackle it in a follow up PR.", "url": "https://github.com/apache/kafka/pull/9570#discussion_r518539735", "createdAt": "2020-11-06T06:00:47Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -111,6 +111,8 @@ public void initialize() {\n             try {\n                 partitions = streamsProducer.partitionsFor(topic);\n             } catch (final KafkaException e) {\n+                // TODO: KIP-572 need to handle `TimeoutException`\n+                // -> should we throw a `TaskCorruptedException` for this case to reset the task and retry (including triggering `task.timeout.ms`) ?", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDEyMjY0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNjowMTowMlrOHuhKGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNjowMTowMlrOHuhKGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODUzOTgwMA==", "bodyText": "As above: This is an open question. Input is welcome. I would like to tackle it in a follow up PR.", "url": "https://github.com/apache/kafka/pull/9570#discussion_r518539800", "createdAt": "2020-11-06T06:01:02Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -202,6 +204,10 @@ private void recordSendError(final String topic, final Exception exception, fina\n                 \"indicating the task may be migrated out\";\n             sendException.set(new TaskMigratedException(errorMessage, exception));\n         } else {\n+            // TODO: KIP-572 handle `TimeoutException extends RetriableException`", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDEyNTMwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNjowMjozM1rOHuhLpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNjowMjozM1rOHuhLpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU0MDE5OQ==", "bodyText": "If a task is revoked, we don't need to trigger task.timeout.ms but can re-throw the TimeoutException to trigger a \"dirty close\" instead.", "url": "https://github.com/apache/kafka/pull/9570#discussion_r518540199", "createdAt": "2020-11-06T06:02:33Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -570,6 +575,11 @@ void handleRevocation(final Collection<TopicPartition> revokedPartitions) {\n         // so we would capture any exception and throw\n         try {\n             commitOffsetsOrTransaction(consumedOffsetsPerTask);\n+        } catch (final TaskTimeoutExceptions taskTimeoutExceptions) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDEyNTg4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNjowMjo1N1rOHuhMBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNjowMjo1N1rOHuhMBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU0MDI5Mg==", "bodyText": "During shutdown, we don't need to trigger task.timeout.ms but can re-throw the TimeoutException to trigger a \"dirty close\" instead.", "url": "https://github.com/apache/kafka/pull/9570#discussion_r518540292", "createdAt": "2020-11-06T06:02:57Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -903,6 +913,15 @@ void shutdown(final boolean clean) {\n                         tasksToCloseClean.remove(task);\n                     }\n                 }\n+            } catch (final TaskTimeoutExceptions taskTimeoutExceptions) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MDEyNzE0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNjowMzo0MVrOHuhMwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwNjowMzo0MVrOHuhMwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU0MDQ4MA==", "bodyText": "Need to make a copy (as all production calls do, too).", "url": "https://github.com/apache/kafka/pull/9570#discussion_r518540480", "createdAt": "2020-11-06T06:03:41Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -997,7 +1016,7 @@ void shutdown(final boolean clean) {\n \n     // For testing only.\n     int commitAll() {\n-        return commit(tasks.values());\n+        return commit(new HashSet<>(tasks.values()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4OTA2ODEzOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMDo1MjoxNVrOH0Q41g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxNzoxNjoxNVrOH1AO4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU2NDY5NA==", "bodyText": "It might still be nice to see the stacktrace here (even if it also gets logged elsewhere). If you want to do it, don't forget you have to change to using String.format for the variable substitution.\nI don't feel strongly in this case, so I'll defer to you whether you want to do this or not.", "url": "https://github.com/apache/kafka/pull/9570#discussion_r524564694", "createdAt": "2020-11-16T20:52:15Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -817,13 +817,15 @@ private void initializeMetadata() {\n                     .filter(e -> e.getValue() != null)\n                     .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n             initializeTaskTime(offsetsAndMetadata);\n-        } catch (final TimeoutException e) {\n-            log.warn(\"Encountered {} while trying to fetch committed offsets, will retry initializing the metadata in the next loop.\" +\n-                            \"\\nConsider overwriting consumer config {} to a larger value to avoid timeout errors\",\n-                    e.toString(),\n-                    ConsumerConfig.DEFAULT_API_TIMEOUT_MS_CONFIG);\n-\n-            throw e;\n+        } catch (final TimeoutException timeoutException) {\n+            log.warn(\n+                \"Encountered {} while trying to fetch committed offsets, will retry initializing the metadata in the next loop.\" +\n+                    \"\\nConsider overwriting consumer config {} to a larger value to avoid timeout errors\",\n+                time.toString(),\n+                ConsumerConfig.DEFAULT_API_TIMEOUT_MS_CONFIG);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM0MDM4NQ==", "bodyText": "I guess I leave it as-is.", "url": "https://github.com/apache/kafka/pull/9570#discussion_r525340385", "createdAt": "2020-11-17T17:16:15Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -817,13 +817,15 @@ private void initializeMetadata() {\n                     .filter(e -> e.getValue() != null)\n                     .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n             initializeTaskTime(offsetsAndMetadata);\n-        } catch (final TimeoutException e) {\n-            log.warn(\"Encountered {} while trying to fetch committed offsets, will retry initializing the metadata in the next loop.\" +\n-                            \"\\nConsider overwriting consumer config {} to a larger value to avoid timeout errors\",\n-                    e.toString(),\n-                    ConsumerConfig.DEFAULT_API_TIMEOUT_MS_CONFIG);\n-\n-            throw e;\n+        } catch (final TimeoutException timeoutException) {\n+            log.warn(\n+                \"Encountered {} while trying to fetch committed offsets, will retry initializing the metadata in the next loop.\" +\n+                    \"\\nConsider overwriting consumer config {} to a larger value to avoid timeout errors\",\n+                time.toString(),\n+                ConsumerConfig.DEFAULT_API_TIMEOUT_MS_CONFIG);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU2NDY5NA=="}, "originalCommit": null, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4OTA4NjMzOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMDo1NToyNlrOH0RE9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxNzoxNjoyN1rOH1APcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU2Nzc5OQ==", "bodyText": "I'm wondering if we should make this info or warn level. It doesn't seem like it would be very verbose, and it might be nice to see by default because it will have secondary effects later on when we try to start a new transaction, but get blocked.\nBut I also don't feel strongly about it, so I leave it to your discretion.", "url": "https://github.com/apache/kafka/pull/9570#discussion_r524567799", "createdAt": "2020-11-16T20:55:26Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "diffHunk": "@@ -262,13 +263,21 @@ void commitTransaction(final Map<TopicPartition, OffsetAndMetadata> offsets,\n     /**\n      * @throws IllegalStateException if EOS is disabled\n      */\n-    void abortTransaction() throws ProducerFencedException {\n+    void abortTransaction() {\n         if (!eosEnabled()) {\n             throw new IllegalStateException(formatException(\"Exactly-once is not enabled\"));\n         }\n         if (transactionInFlight) {\n             try {\n                 producer.abortTransaction();\n+            } catch (final TimeoutException logAndSwallow) {\n+                // no need to re-throw because we abort a TX only if we close a task dirty,\n+                // and thus `task.timeout.ms` does not apply\n+                log.debug(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM0MDUyOQ==", "bodyText": "Ack. warn it is.", "url": "https://github.com/apache/kafka/pull/9570#discussion_r525340529", "createdAt": "2020-11-17T17:16:27Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsProducer.java", "diffHunk": "@@ -262,13 +263,21 @@ void commitTransaction(final Map<TopicPartition, OffsetAndMetadata> offsets,\n     /**\n      * @throws IllegalStateException if EOS is disabled\n      */\n-    void abortTransaction() throws ProducerFencedException {\n+    void abortTransaction() {\n         if (!eosEnabled()) {\n             throw new IllegalStateException(formatException(\"Exactly-once is not enabled\"));\n         }\n         if (transactionInFlight) {\n             try {\n                 producer.abortTransaction();\n+            } catch (final TimeoutException logAndSwallow) {\n+                // no need to re-throw because we abort a TX only if we close a task dirty,\n+                // and thus `task.timeout.ms` does not apply\n+                log.debug(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU2Nzc5OQ=="}, "originalCommit": null, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4OTcyOTY1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjoyODo1MlrOH0X_Pg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjoyODo1MlrOH0X_Pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDY4MTAyMg==", "bodyText": "maybe we should move clearTaskTimeout here, in case some of the tasks timed out, but not all?", "url": "https://github.com/apache/kafka/pull/9570#discussion_r524681022", "createdAt": "2020-11-16T22:28:52Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -1029,20 +1048,40 @@ int commit(final Collection<Task> tasksToCommit) {\n             return -1;\n         } else {\n             int committed = 0;\n-            final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-            for (final Task task : tasksToCommit) {\n+            final Map<Task, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n+            final Iterator<Task> it = tasksToCommit.iterator();\n+            while (it.hasNext()) {\n+                final Task task = it.next();\n                 if (task.commitNeeded()) {\n                     final Map<TopicPartition, OffsetAndMetadata> offsetAndMetadata = task.prepareCommit();\n                     if (task.isActive()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), offsetAndMetadata);\n+                        consumedOffsetsAndMetadataPerTask.put(task, offsetAndMetadata);\n                     }\n+                } else {\n+                    it.remove();\n                 }\n             }\n \n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            final Set<Task> uncommittedTasks = new HashSet<>();\n+            try {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+                tasksToCommit.forEach(Task::clearTaskTimeout);\n+            } catch (final TaskTimeoutExceptions taskTimeoutExceptions) {\n+                final TimeoutException timeoutException = taskTimeoutExceptions.timeoutException();\n+                if (timeoutException != null) {\n+                    tasksToCommit.forEach(t -> t.maybeInitTaskTimeoutOrThrow(time.milliseconds(), timeoutException));\n+                    uncommittedTasks.addAll(tasksToCommit);\n+                } else {\n+                    for (final Map.Entry<Task, TimeoutException> timeoutExceptions : taskTimeoutExceptions.exceptions().entrySet()) {\n+                        final Task task = timeoutExceptions.getKey();\n+                        task.maybeInitTaskTimeoutOrThrow(time.milliseconds(), timeoutExceptions.getValue());\n+                        uncommittedTasks.add(task);\n+                    }\n+                }\n+            }\n \n             for (final Task task : tasksToCommit) {\n-                if (task.commitNeeded()) {\n+                if (!uncommittedTasks.contains(task)) {\n                     ++committed;\n                     task.postCommit(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 164}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4OTk1OTc2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMzowNzo1NVrOH0aeKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QxNzoyMDoyNFrOH1AaWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDcyMTcwNw==", "bodyText": "I was initially worried about potential side-effects of removing from the input collection, but on second thought, maybe it's reasonable (considering the usage of this method) to assume that the input collection is always a copy already.\nStill, it looks like we could actually simplify the implementation a little by not reusing the input collection but instead only relying on the key set of consumedOffsetsAndMetadataPerTask as the list of \"tasks to commit\".\nHere's what I'm thinking:\nint committed = 0;\n            final Map<Task, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n            for (final Task task : tasksToCommit) {\n                if (task.commitNeeded()) {\n                    final Map<TopicPartition, OffsetAndMetadata> offsetAndMetadata = task.prepareCommit();\n                    if (task.isActive()) {\n                        consumedOffsetsAndMetadataPerTask.put(task, offsetAndMetadata);\n                    }\n                }\n            }\n\n            final Set<Task> uncommittedTasks = new HashSet<>();\n            try {\n                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n            } catch (final TaskTimeoutExceptions taskTimeoutExceptions) {\n                final TimeoutException timeoutException = taskTimeoutExceptions.timeoutException();\n                if (timeoutException != null) {\n                    consumedOffsetsAndMetadataPerTask.keySet().forEach(t -> t.maybeInitTaskTimeoutOrThrow(time.milliseconds(), timeoutException));\n                    uncommittedTasks.addAll(tasksToCommit);\n                } else {\n                    for (final Map.Entry<Task, TimeoutException> timeoutExceptions : taskTimeoutExceptions.exceptions().entrySet()) {\n                        final Task task = timeoutExceptions.getKey();\n                        task.maybeInitTaskTimeoutOrThrow(time.milliseconds(), timeoutExceptions.getValue());\n                        uncommittedTasks.add(task);\n                    }\n                }\n            }\n\n            for (final Task task : tasksToCommit) {\n                if (!uncommittedTasks.contains(task)) {\n                    ++committed;\n                    task.postCommit(false);\n                    task.clearTaskTimeout();\n                }\n            }\n\n            return committed;\nIt struck me as potentially incorrect to\n\nmaybe init the timeout exception for standby tasks\nnot clear the timeout for successful tasks when other tasks were timed out\n\nBut I'm also not sure whether these are big deals or not.", "url": "https://github.com/apache/kafka/pull/9570#discussion_r524721707", "createdAt": "2020-11-16T23:07:55Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -1029,20 +1048,40 @@ int commit(final Collection<Task> tasksToCommit) {\n             return -1;\n         } else {\n             int committed = 0;\n-            final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-            for (final Task task : tasksToCommit) {\n+            final Map<Task, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n+            final Iterator<Task> it = tasksToCommit.iterator();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTM0MzMyMQ==", "bodyText": "Good catch!", "url": "https://github.com/apache/kafka/pull/9570#discussion_r525343321", "createdAt": "2020-11-17T17:20:24Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -1029,20 +1048,40 @@ int commit(final Collection<Task> tasksToCommit) {\n             return -1;\n         } else {\n             int committed = 0;\n-            final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-            for (final Task task : tasksToCommit) {\n+            final Map<Task, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n+            final Iterator<Task> it = tasksToCommit.iterator();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDcyMTcwNw=="}, "originalCommit": null, "originalPosition": 127}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3820, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}