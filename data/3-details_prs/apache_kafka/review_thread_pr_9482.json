{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA4NDgxNzc0", "number": 9482, "reviewThreads": {"totalCount": 35, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxODo1MzozOFrOEw5oFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwMToyMTozOFrOEz_9XQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NzExMjUzOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxODo1MzozOFrOHmvdTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxODo1MzozOFrOHmvdTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM4NTQ4Ng==", "bodyText": "I would remove this comment. We ca file an issue and fix it if this becomes a performance issue.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510385486", "createdAt": "2020-10-22T18:53:38Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -206,30 +234,77 @@ private void updateLeaderEndOffsetAndTimestamp(\n         final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n \n         if (state.updateLocalState(currentTimeMs, endOffsetMetadata)) {\n-            updateHighWatermark(state, currentTimeMs);\n+            onUpdateLeaderHighWatermark(state, currentTimeMs);\n         }\n \n-        LogOffset endOffset = new LogOffset(endOffsetMetadata.offset, Isolation.UNCOMMITTED);\n-        fetchPurgatory.maybeComplete(endOffset, currentTimeMs);\n+        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n     }\n \n-    private void updateHighWatermark(\n-        EpochState state,\n+    private void onUpdateLeaderHighWatermark(\n+        LeaderState state,\n         long currentTimeMs\n     ) {\n         state.highWatermark().ifPresent(highWatermark -> {\n-            logger.debug(\"High watermark updated to {}\", highWatermark);\n+            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n             log.updateHighWatermark(highWatermark);\n-\n-            LogOffset offset = new LogOffset(highWatermark.offset, Isolation.COMMITTED);\n-            appendPurgatory.maybeComplete(offset, currentTimeMs);\n-            fetchPurgatory.maybeComplete(offset, currentTimeMs);\n+            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n+            maybeFireHandleCommit(highWatermark.offset);\n         });\n     }\n \n-    @Override\n-    public LeaderAndEpoch currentLeaderAndEpoch() {\n-        return quorum.leaderAndEpoch();\n+    private void maybeFireHandleCommit(long highWatermark) {\n+        maybeFireHandleCommit(listenerContexts, highWatermark);\n+    }\n+\n+    private void maybeFireHandleCommit(List<ListenerContext> listenerContexts, long highWatermark) {\n+        // TODO: When there are multiple listeners, we can cache reads to save some work", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 236}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NzEyNTcyOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxODo1NzoxNVrOHmvljQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQyMDozMDozN1rOHmyrrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM4NzU5Nw==", "bodyText": "Why a return? Did you mean to use continue? If this is suppose to be a continue then maybe we can Optional.ifPresent.\nSame comment for one of the other overloaded maybeFireHandleCommit.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510387597", "createdAt": "2020-10-22T18:57:15Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -206,30 +234,77 @@ private void updateLeaderEndOffsetAndTimestamp(\n         final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n \n         if (state.updateLocalState(currentTimeMs, endOffsetMetadata)) {\n-            updateHighWatermark(state, currentTimeMs);\n+            onUpdateLeaderHighWatermark(state, currentTimeMs);\n         }\n \n-        LogOffset endOffset = new LogOffset(endOffsetMetadata.offset, Isolation.UNCOMMITTED);\n-        fetchPurgatory.maybeComplete(endOffset, currentTimeMs);\n+        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n     }\n \n-    private void updateHighWatermark(\n-        EpochState state,\n+    private void onUpdateLeaderHighWatermark(\n+        LeaderState state,\n         long currentTimeMs\n     ) {\n         state.highWatermark().ifPresent(highWatermark -> {\n-            logger.debug(\"High watermark updated to {}\", highWatermark);\n+            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n             log.updateHighWatermark(highWatermark);\n-\n-            LogOffset offset = new LogOffset(highWatermark.offset, Isolation.COMMITTED);\n-            appendPurgatory.maybeComplete(offset, currentTimeMs);\n-            fetchPurgatory.maybeComplete(offset, currentTimeMs);\n+            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n+            maybeFireHandleCommit(highWatermark.offset);\n         });\n     }\n \n-    @Override\n-    public LeaderAndEpoch currentLeaderAndEpoch() {\n-        return quorum.leaderAndEpoch();\n+    private void maybeFireHandleCommit(long highWatermark) {\n+        maybeFireHandleCommit(listenerContexts, highWatermark);\n+    }\n+\n+    private void maybeFireHandleCommit(List<ListenerContext> listenerContexts, long highWatermark) {\n+        // TODO: When there are multiple listeners, we can cache reads to save some work\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                return;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 240}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQzODMxOA==", "bodyText": "Good catch. I think that I forgot to update this when moving from a single listener to multiple listeners.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510438318", "createdAt": "2020-10-22T20:30:37Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -206,30 +234,77 @@ private void updateLeaderEndOffsetAndTimestamp(\n         final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n \n         if (state.updateLocalState(currentTimeMs, endOffsetMetadata)) {\n-            updateHighWatermark(state, currentTimeMs);\n+            onUpdateLeaderHighWatermark(state, currentTimeMs);\n         }\n \n-        LogOffset endOffset = new LogOffset(endOffsetMetadata.offset, Isolation.UNCOMMITTED);\n-        fetchPurgatory.maybeComplete(endOffset, currentTimeMs);\n+        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n     }\n \n-    private void updateHighWatermark(\n-        EpochState state,\n+    private void onUpdateLeaderHighWatermark(\n+        LeaderState state,\n         long currentTimeMs\n     ) {\n         state.highWatermark().ifPresent(highWatermark -> {\n-            logger.debug(\"High watermark updated to {}\", highWatermark);\n+            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n             log.updateHighWatermark(highWatermark);\n-\n-            LogOffset offset = new LogOffset(highWatermark.offset, Isolation.COMMITTED);\n-            appendPurgatory.maybeComplete(offset, currentTimeMs);\n-            fetchPurgatory.maybeComplete(offset, currentTimeMs);\n+            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n+            maybeFireHandleCommit(highWatermark.offset);\n         });\n     }\n \n-    @Override\n-    public LeaderAndEpoch currentLeaderAndEpoch() {\n-        return quorum.leaderAndEpoch();\n+    private void maybeFireHandleCommit(long highWatermark) {\n+        maybeFireHandleCommit(listenerContexts, highWatermark);\n+    }\n+\n+    private void maybeFireHandleCommit(List<ListenerContext> listenerContexts, long highWatermark) {\n+        // TODO: When there are multiple listeners, we can cache reads to save some work\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                return;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM4NzU5Nw=="}, "originalCommit": null, "originalPosition": 240}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NzE4NzQ1OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxOToxMzo1OVrOHmwK-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQyMzozMzowMFrOHm3TIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM5NzE3OQ==", "bodyText": "Interesting. It is good to hide this logic from the state machine. Looking at the epoch and not at the LEO is okay because at this point we guarantee that the only records with that epoch are control records (e.g. LeaderChangedMessage).\nI am wondering if the state machine may want to know this before it can process state machine requests. Maybe this is okay because the brokers/replicas will learn about the new leader through the Fetch and BeginQuorum protocol and not from the state machine (Kafka Controller) itself.\nIt is possible that the leader will receive Kafka Controller message from replicas/broker before it knows that it is leader. Most likely the Kafka Controller will reject them but the replicas/brokers need to keep retrying. This is specially important for heartbeat messages.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510397179", "createdAt": "2020-10-22T19:13:59Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -206,30 +234,77 @@ private void updateLeaderEndOffsetAndTimestamp(\n         final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n \n         if (state.updateLocalState(currentTimeMs, endOffsetMetadata)) {\n-            updateHighWatermark(state, currentTimeMs);\n+            onUpdateLeaderHighWatermark(state, currentTimeMs);\n         }\n \n-        LogOffset endOffset = new LogOffset(endOffsetMetadata.offset, Isolation.UNCOMMITTED);\n-        fetchPurgatory.maybeComplete(endOffset, currentTimeMs);\n+        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n     }\n \n-    private void updateHighWatermark(\n-        EpochState state,\n+    private void onUpdateLeaderHighWatermark(\n+        LeaderState state,\n         long currentTimeMs\n     ) {\n         state.highWatermark().ifPresent(highWatermark -> {\n-            logger.debug(\"High watermark updated to {}\", highWatermark);\n+            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n             log.updateHighWatermark(highWatermark);\n-\n-            LogOffset offset = new LogOffset(highWatermark.offset, Isolation.COMMITTED);\n-            appendPurgatory.maybeComplete(offset, currentTimeMs);\n-            fetchPurgatory.maybeComplete(offset, currentTimeMs);\n+            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n+            maybeFireHandleCommit(highWatermark.offset);\n         });\n     }\n \n-    @Override\n-    public LeaderAndEpoch currentLeaderAndEpoch() {\n-        return quorum.leaderAndEpoch();\n+    private void maybeFireHandleCommit(long highWatermark) {\n+        maybeFireHandleCommit(listenerContexts, highWatermark);\n+    }\n+\n+    private void maybeFireHandleCommit(List<ListenerContext> listenerContexts, long highWatermark) {\n+        // TODO: When there are multiple listeners, we can cache reads to save some work\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                return;\n+            }\n+\n+            long nextExpectedOffset = nextExpectedOffsetOpt.getAsLong();\n+            if (nextExpectedOffset < highWatermark) {\n+                LogFetchInfo readInfo = log.read(nextExpectedOffset, Isolation.COMMITTED);\n+                listenerContext.fireHandleCommit(nextExpectedOffset, readInfo.records);\n+            }\n+        }\n+    }\n+\n+    private void maybeFireHandleCommit(long baseOffset, int epoch, List<T> records) {\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                return;\n+            }\n+\n+            long nextExpectedOffset = nextExpectedOffsetOpt.getAsLong();\n+            if (nextExpectedOffset == baseOffset) {\n+                listenerContext.fireHandleCommit(baseOffset, epoch, records);\n+            }\n+        }\n+    }\n+\n+    private void maybeFireHandleClaim(LeaderState state) {\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            int leaderEpoch = state.epoch();\n+\n+            // We can fire `handleClaim` as soon as the listener has caught\n+            // up to the start of the leader epoch. This guarantees that the\n+            // state machine has seen the full committed state before it becomes\n+            // leader and begins writing to the log.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 272}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1NTcxMw==", "bodyText": "I thought a little about it. Right now the state machine has just two states: 1) i am not a leader, and 2) i am a leader and have caught up with all committed data from previous epochs.  An alternative design is to fire handleClaim immediately and provide the starting offset of the leader epoch. Then the controller can wait until its state machine has caught up to that offset before starting to write data. In the end, I decided not to do it because it adds a third state and I did not expect the controller would be able to do anything useful in the additional state. The point about heartbeats is interesting, but even that seems tricky since the controller would not know if a broker had been fenced until it has caught up. I think the only thing the controller could do is hold the requests in purgatory, which might be better than letting them retry, but not sure it's worth it.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510455713", "createdAt": "2020-10-22T21:03:40Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -206,30 +234,77 @@ private void updateLeaderEndOffsetAndTimestamp(\n         final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n \n         if (state.updateLocalState(currentTimeMs, endOffsetMetadata)) {\n-            updateHighWatermark(state, currentTimeMs);\n+            onUpdateLeaderHighWatermark(state, currentTimeMs);\n         }\n \n-        LogOffset endOffset = new LogOffset(endOffsetMetadata.offset, Isolation.UNCOMMITTED);\n-        fetchPurgatory.maybeComplete(endOffset, currentTimeMs);\n+        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n     }\n \n-    private void updateHighWatermark(\n-        EpochState state,\n+    private void onUpdateLeaderHighWatermark(\n+        LeaderState state,\n         long currentTimeMs\n     ) {\n         state.highWatermark().ifPresent(highWatermark -> {\n-            logger.debug(\"High watermark updated to {}\", highWatermark);\n+            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n             log.updateHighWatermark(highWatermark);\n-\n-            LogOffset offset = new LogOffset(highWatermark.offset, Isolation.COMMITTED);\n-            appendPurgatory.maybeComplete(offset, currentTimeMs);\n-            fetchPurgatory.maybeComplete(offset, currentTimeMs);\n+            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n+            maybeFireHandleCommit(highWatermark.offset);\n         });\n     }\n \n-    @Override\n-    public LeaderAndEpoch currentLeaderAndEpoch() {\n-        return quorum.leaderAndEpoch();\n+    private void maybeFireHandleCommit(long highWatermark) {\n+        maybeFireHandleCommit(listenerContexts, highWatermark);\n+    }\n+\n+    private void maybeFireHandleCommit(List<ListenerContext> listenerContexts, long highWatermark) {\n+        // TODO: When there are multiple listeners, we can cache reads to save some work\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                return;\n+            }\n+\n+            long nextExpectedOffset = nextExpectedOffsetOpt.getAsLong();\n+            if (nextExpectedOffset < highWatermark) {\n+                LogFetchInfo readInfo = log.read(nextExpectedOffset, Isolation.COMMITTED);\n+                listenerContext.fireHandleCommit(nextExpectedOffset, readInfo.records);\n+            }\n+        }\n+    }\n+\n+    private void maybeFireHandleCommit(long baseOffset, int epoch, List<T> records) {\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                return;\n+            }\n+\n+            long nextExpectedOffset = nextExpectedOffsetOpt.getAsLong();\n+            if (nextExpectedOffset == baseOffset) {\n+                listenerContext.fireHandleCommit(baseOffset, epoch, records);\n+            }\n+        }\n+    }\n+\n+    private void maybeFireHandleClaim(LeaderState state) {\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            int leaderEpoch = state.epoch();\n+\n+            // We can fire `handleClaim` as soon as the listener has caught\n+            // up to the start of the leader epoch. This guarantees that the\n+            // state machine has seen the full committed state before it becomes\n+            // leader and begins writing to the log.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM5NzE3OQ=="}, "originalCommit": null, "originalPosition": 272}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ2OTc0Mg==", "bodyText": "Yeah. I was thinking of the same thing, \"hold the requests in purgatory\". But like you said, maybe this optimization is not worth the added complexity.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510469742", "createdAt": "2020-10-22T21:31:01Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -206,30 +234,77 @@ private void updateLeaderEndOffsetAndTimestamp(\n         final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n \n         if (state.updateLocalState(currentTimeMs, endOffsetMetadata)) {\n-            updateHighWatermark(state, currentTimeMs);\n+            onUpdateLeaderHighWatermark(state, currentTimeMs);\n         }\n \n-        LogOffset endOffset = new LogOffset(endOffsetMetadata.offset, Isolation.UNCOMMITTED);\n-        fetchPurgatory.maybeComplete(endOffset, currentTimeMs);\n+        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n     }\n \n-    private void updateHighWatermark(\n-        EpochState state,\n+    private void onUpdateLeaderHighWatermark(\n+        LeaderState state,\n         long currentTimeMs\n     ) {\n         state.highWatermark().ifPresent(highWatermark -> {\n-            logger.debug(\"High watermark updated to {}\", highWatermark);\n+            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n             log.updateHighWatermark(highWatermark);\n-\n-            LogOffset offset = new LogOffset(highWatermark.offset, Isolation.COMMITTED);\n-            appendPurgatory.maybeComplete(offset, currentTimeMs);\n-            fetchPurgatory.maybeComplete(offset, currentTimeMs);\n+            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n+            maybeFireHandleCommit(highWatermark.offset);\n         });\n     }\n \n-    @Override\n-    public LeaderAndEpoch currentLeaderAndEpoch() {\n-        return quorum.leaderAndEpoch();\n+    private void maybeFireHandleCommit(long highWatermark) {\n+        maybeFireHandleCommit(listenerContexts, highWatermark);\n+    }\n+\n+    private void maybeFireHandleCommit(List<ListenerContext> listenerContexts, long highWatermark) {\n+        // TODO: When there are multiple listeners, we can cache reads to save some work\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                return;\n+            }\n+\n+            long nextExpectedOffset = nextExpectedOffsetOpt.getAsLong();\n+            if (nextExpectedOffset < highWatermark) {\n+                LogFetchInfo readInfo = log.read(nextExpectedOffset, Isolation.COMMITTED);\n+                listenerContext.fireHandleCommit(nextExpectedOffset, readInfo.records);\n+            }\n+        }\n+    }\n+\n+    private void maybeFireHandleCommit(long baseOffset, int epoch, List<T> records) {\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                return;\n+            }\n+\n+            long nextExpectedOffset = nextExpectedOffsetOpt.getAsLong();\n+            if (nextExpectedOffset == baseOffset) {\n+                listenerContext.fireHandleCommit(baseOffset, epoch, records);\n+            }\n+        }\n+    }\n+\n+    private void maybeFireHandleClaim(LeaderState state) {\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            int leaderEpoch = state.epoch();\n+\n+            // We can fire `handleClaim` as soon as the listener has caught\n+            // up to the start of the leader epoch. This guarantees that the\n+            // state machine has seen the full committed state before it becomes\n+            // leader and begins writing to the log.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM5NzE3OQ=="}, "originalCommit": null, "originalPosition": 272}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDUxMzk1NA==", "bodyText": "I guess let's keep this option in our back pocket for now.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510513954", "createdAt": "2020-10-22T23:33:00Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -206,30 +234,77 @@ private void updateLeaderEndOffsetAndTimestamp(\n         final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n \n         if (state.updateLocalState(currentTimeMs, endOffsetMetadata)) {\n-            updateHighWatermark(state, currentTimeMs);\n+            onUpdateLeaderHighWatermark(state, currentTimeMs);\n         }\n \n-        LogOffset endOffset = new LogOffset(endOffsetMetadata.offset, Isolation.UNCOMMITTED);\n-        fetchPurgatory.maybeComplete(endOffset, currentTimeMs);\n+        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n     }\n \n-    private void updateHighWatermark(\n-        EpochState state,\n+    private void onUpdateLeaderHighWatermark(\n+        LeaderState state,\n         long currentTimeMs\n     ) {\n         state.highWatermark().ifPresent(highWatermark -> {\n-            logger.debug(\"High watermark updated to {}\", highWatermark);\n+            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n             log.updateHighWatermark(highWatermark);\n-\n-            LogOffset offset = new LogOffset(highWatermark.offset, Isolation.COMMITTED);\n-            appendPurgatory.maybeComplete(offset, currentTimeMs);\n-            fetchPurgatory.maybeComplete(offset, currentTimeMs);\n+            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n+            maybeFireHandleCommit(highWatermark.offset);\n         });\n     }\n \n-    @Override\n-    public LeaderAndEpoch currentLeaderAndEpoch() {\n-        return quorum.leaderAndEpoch();\n+    private void maybeFireHandleCommit(long highWatermark) {\n+        maybeFireHandleCommit(listenerContexts, highWatermark);\n+    }\n+\n+    private void maybeFireHandleCommit(List<ListenerContext> listenerContexts, long highWatermark) {\n+        // TODO: When there are multiple listeners, we can cache reads to save some work\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                return;\n+            }\n+\n+            long nextExpectedOffset = nextExpectedOffsetOpt.getAsLong();\n+            if (nextExpectedOffset < highWatermark) {\n+                LogFetchInfo readInfo = log.read(nextExpectedOffset, Isolation.COMMITTED);\n+                listenerContext.fireHandleCommit(nextExpectedOffset, readInfo.records);\n+            }\n+        }\n+    }\n+\n+    private void maybeFireHandleCommit(long baseOffset, int epoch, List<T> records) {\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                return;\n+            }\n+\n+            long nextExpectedOffset = nextExpectedOffsetOpt.getAsLong();\n+            if (nextExpectedOffset == baseOffset) {\n+                listenerContext.fireHandleCommit(baseOffset, epoch, records);\n+            }\n+        }\n+    }\n+\n+    private void maybeFireHandleClaim(LeaderState state) {\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            int leaderEpoch = state.epoch();\n+\n+            // We can fire `handleClaim` as soon as the listener has caught\n+            // up to the start of the leader epoch. This guarantees that the\n+            // state machine has seen the full committed state before it becomes\n+            // leader and begins writing to the log.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM5NzE3OQ=="}, "originalCommit": null, "originalPosition": 272}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NzE5ODU5OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxOToxNzoxMlrOHmwRtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxOToxNzoxMlrOHmwRtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDM5ODkwMg==", "bodyText": "Incomplete sentence.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510398902", "createdAt": "2020-10-22T19:17:12Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -975,12 +1029,9 @@ private boolean handleFetchResponse(\n                 log.truncateToEndOffset(divergingOffsetAndEpoch).ifPresent(truncationOffset -> {\n                     logger.info(\"Truncated to offset {} from Fetch response from leader {}\",\n                         truncationOffset, quorum.leaderIdOrNil());\n-\n-                    // Since the end offset has been updated, we should complete any delayed\n-                    // reads at the end offset.\n-                    fetchPurgatory.maybeComplete(\n-                        new LogOffset(Long.MAX_VALUE, Isolation.UNCOMMITTED),\n-                        currentTimeMs);\n+                    // After truncation, we complete all pending reads in order to\n+                    // ensure that fetches account for the", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 467}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NzIzNDk5OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxOToyODoxMVrOHmwn9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQyMDoyMjoxMlrOHmyafA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQwNDU5Ng==", "bodyText": "Does this mean that in practice, follower will have at most two batches in flight?\n\nThe one that they are currently processing\nIf they read the last message/record in the batch then the next batch in the log?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510404596", "createdAt": "2020-10-22T19:28:11Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1757,35 +1809,86 @@ public void complete() {\n         }\n     }\n \n-    private static class UnwrittenAppend {\n-        private final Records records;\n-        private final long createTimeMs;\n-        private final long requestTimeoutMs;\n-        private final AckMode ackMode;\n-        private final CompletableFuture<OffsetAndEpoch> future;\n+    private final class ListenerContext implements CloseListener<BatchReader<T>> {\n+        private final RaftClient.Listener<T> listener;\n+        private BatchReader<T> lastSent = null;\n+        private long lastAckedOffset = 0;\n+        private int claimedEpoch = 0;\n+\n+        private ListenerContext(Listener<T> listener) {\n+            this.listener = listener;\n+        }\n+\n+        /**\n+         * Get the last acked offset, which is one greater than the offset of the\n+         * last record which was acked by the state machine.\n+         */\n+        public synchronized long lastAckedOffset() {\n+            return lastAckedOffset;\n+        }\n+\n+        /**\n+         * Get the next expected offset, which might be larger than the last acked\n+         * offset if there are inflight batches which have not been acked yet.\n+         * Note that when fetching from disk, we may not know the last offset of\n+         * inflight data until it has been processed by the state machine. In this case,\n+         * we delay sending additional data until the state machine has read to the\n+         * end and the last offset is determined.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 754}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQzMzkxNg==", "bodyText": "When catching up from the log, yes. However, I have implemented an optimization for writes from the leader. We save the original batch in memory so that it can be sent back to the state machine after the write is committed. In this case, we know the last offset of the batch, so we can have multiple inflight batches sent to the controller. This is nice because it means the elected controller will not have to read from disk.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510433916", "createdAt": "2020-10-22T20:22:12Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1757,35 +1809,86 @@ public void complete() {\n         }\n     }\n \n-    private static class UnwrittenAppend {\n-        private final Records records;\n-        private final long createTimeMs;\n-        private final long requestTimeoutMs;\n-        private final AckMode ackMode;\n-        private final CompletableFuture<OffsetAndEpoch> future;\n+    private final class ListenerContext implements CloseListener<BatchReader<T>> {\n+        private final RaftClient.Listener<T> listener;\n+        private BatchReader<T> lastSent = null;\n+        private long lastAckedOffset = 0;\n+        private int claimedEpoch = 0;\n+\n+        private ListenerContext(Listener<T> listener) {\n+            this.listener = listener;\n+        }\n+\n+        /**\n+         * Get the last acked offset, which is one greater than the offset of the\n+         * last record which was acked by the state machine.\n+         */\n+        public synchronized long lastAckedOffset() {\n+            return lastAckedOffset;\n+        }\n+\n+        /**\n+         * Get the next expected offset, which might be larger than the last acked\n+         * offset if there are inflight batches which have not been acked yet.\n+         * Note that when fetching from disk, we may not know the last offset of\n+         * inflight data until it has been processed by the state machine. In this case,\n+         * we delay sending additional data until the state machine has read to the\n+         * end and the last offset is determined.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQwNDU5Ng=="}, "originalCommit": null, "originalPosition": 754}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NzI1ODc1OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/QuorumState.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxOTozNTowOFrOHmw2qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQyMDoyOTowOVrOHmyoZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQwODM2Mg==", "bodyText": "What is \"this\" in this sentence? epochStartOffset?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510408362", "createdAt": "2020-10-22T19:35:08Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/QuorumState.java", "diffHunk": "@@ -369,6 +379,17 @@ public void transitionToLeader(long epochStartOffset) throws IOException {\n         if (!candidateState.isVoteGranted())\n             throw new IllegalStateException(\"Cannot become leader without majority votes granted\");\n \n+        // Note that the leader does not retain the high watermark that was known\n+        // in the previous state. The purpose of this is to protect the monotonicity", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQzNzQ3Nw==", "bodyText": "It was intended to refer to the behavior of not retaining the high watermark from the previous sentence. I will attempt to clarify.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510437477", "createdAt": "2020-10-22T20:29:09Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/QuorumState.java", "diffHunk": "@@ -369,6 +379,17 @@ public void transitionToLeader(long epochStartOffset) throws IOException {\n         if (!candidateState.isVoteGranted())\n             throw new IllegalStateException(\"Cannot become leader without majority votes granted\");\n \n+        // Note that the leader does not retain the high watermark that was known\n+        // in the previous state. The purpose of this is to protect the monotonicity", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQwODM2Mg=="}, "originalCommit": null, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NzI2Mjk4OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxOTozNjoyOVrOHmw5Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxOTozNjoyOVrOHmw5Zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQwOTA2Mw==", "bodyText": "nit: \"... must be iterated and closed\".", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510409063", "createdAt": "2020-10-22T19:36:29Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "diffHunk": "@@ -16,57 +16,75 @@\n  */\n package org.apache.kafka.raft;\n \n-import org.apache.kafka.common.record.Records;\n-\n import java.io.IOException;\n+import java.util.List;\n import java.util.concurrent.CompletableFuture;\n \n-public interface RaftClient {\n+public interface RaftClient<T> {\n+\n+    interface Listener<T> {\n+        /**\n+         * Callback which is invoked for all records committed to the log.\n+         * It is the responsibility of the caller to invoke {@link BatchReader#close()}\n+         * after consuming the reader.\n+         *\n+         * Note that there is not a one-to-one correspondence between writes through\n+         * {@link #scheduleAppend(int, List)} and this callback. The Raft implementation\n+         * is free to batch together the records from multiple append calls provided\n+         * that batch boundaries are respected. This means that each batch specified\n+         * through {@link #scheduleAppend(int, List)} is guaranteed to be a subset of\n+         * a batch provided by the {@link BatchReader}.\n+         *\n+         * @param reader reader instance which must be iterated", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NzI3OTI2OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxOTo0MToxMVrOHmxDUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQyMDowOTowM1rOHmx-Wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQxMTYwMw==", "bodyText": "We should also mention that MAX_VALUE is return if the RaftClient is not the leader.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510411603", "createdAt": "2020-10-22T19:41:11Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "diffHunk": "@@ -16,57 +16,75 @@\n  */\n package org.apache.kafka.raft;\n \n-import org.apache.kafka.common.record.Records;\n-\n import java.io.IOException;\n+import java.util.List;\n import java.util.concurrent.CompletableFuture;\n \n-public interface RaftClient {\n+public interface RaftClient<T> {\n+\n+    interface Listener<T> {\n+        /**\n+         * Callback which is invoked for all records committed to the log.\n+         * It is the responsibility of the caller to invoke {@link BatchReader#close()}\n+         * after consuming the reader.\n+         *\n+         * Note that there is not a one-to-one correspondence between writes through\n+         * {@link #scheduleAppend(int, List)} and this callback. The Raft implementation\n+         * is free to batch together the records from multiple append calls provided\n+         * that batch boundaries are respected. This means that each batch specified\n+         * through {@link #scheduleAppend(int, List)} is guaranteed to be a subset of\n+         * a batch provided by the {@link BatchReader}.\n+         *\n+         * @param reader reader instance which must be iterated\n+         */\n+        void handleCommit(BatchReader<T> reader);\n+\n+        /**\n+         * Invoked after this node has become a leader. This is only called after\n+         * all commits up to the start of the leader's epoch have been sent to\n+         * {@link #handleCommit(BatchReader)}.\n+         *\n+         * After becoming a leader, the client is eligible to write to the log\n+         * using {@link #scheduleAppend(int, List)}.\n+         *\n+         * @param epoch the claimed leader epoch\n+         */\n+        default void handleClaim(int epoch) {}\n+\n+        /**\n+         * Invoked after a leader has stepped down. This callback may or may not\n+         * fire before the next leader has been elected.\n+         */\n+        default void handleResign() {}\n+    }\n \n     /**\n-     * Initialize the client. This should only be called once and it must be\n-     * called before any of the other APIs can be invoked.\n+     * Initialize the client. This should only be called once on startup.\n      *\n      * @throws IOException For any IO errors during initialization\n      */\n     void initialize() throws IOException;\n \n     /**\n-     * Append a new entry to the log. The client must be in the leader state to\n-     * accept an append: it is up to the state machine implementation\n-     * to ensure this using {@link #currentLeaderAndEpoch()}.\n-     *\n-     * TODO: One improvement we can make here is to allow the caller to specify\n-     * the current leader epoch in the record set. That would ensure that each\n-     * leader change must be \"observed\" by the state machine before new appends\n-     * are accepted.\n-     *\n-     * @param records The records to append to the log\n-     * @param timeoutMs Maximum time to wait for the append to complete\n-     * @return A future containing the last offset and epoch of the appended records (if successful)\n-     */\n-    CompletableFuture<OffsetAndEpoch> append(Records records, AckMode ackMode, long timeoutMs);\n-\n-    /**\n-     * Read a set of records from the log. Note that it is the responsibility of the state machine\n-     * to filter control records added by the Raft client itself.\n-     *\n-     * If the fetch offset is no longer valid, then the future will be completed exceptionally\n-     * with a {@link LogTruncationException}.\n+     * Register a listener to get commit/leader notifications.\n      *\n-     * @param position The position to fetch from\n-     * @param isolation The isolation level to apply to the read\n-     * @param maxWaitTimeMs The maximum time to wait for new data to become available before completion\n-     * @return The record set, which may be empty if fetching from the end of the log\n+     * @param listener the listener\n      */\n-    CompletableFuture<Records> read(OffsetAndEpoch position, Isolation isolation, long maxWaitTimeMs);\n+    void register(Listener<T> listener);\n \n     /**\n-     * Get the current leader (if known) and the current epoch.\n+     * Append a list of records to the log. The write will be scheduled for some time\n+     * in the future. There is no guarantee that appended records will be written to\n+     * the log and eventually committed. However, it is guaranteed that if any of the\n+     * records become committed, then all of them will be.\n      *\n-     * @return Current leader and epoch information\n+     * @param epoch the current leader epoch\n+     * @param records the list of records to append\n+     * @return the offset within the current epoch that the log entries will be appended,\n+     *         or null if the leader was unable to accept the write (e.g. due to memory\n+     *         being reached).", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQyNjcxNA==", "bodyText": "Agreed. I added this here: https://github.com/apache/kafka/pull/9418/files#diff-ac850e29114f9b5a03aaf3ccb8f07feda8b48e5de6912f4c527b8477aa3d6cbcR60.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510426714", "createdAt": "2020-10-22T20:09:03Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "diffHunk": "@@ -16,57 +16,75 @@\n  */\n package org.apache.kafka.raft;\n \n-import org.apache.kafka.common.record.Records;\n-\n import java.io.IOException;\n+import java.util.List;\n import java.util.concurrent.CompletableFuture;\n \n-public interface RaftClient {\n+public interface RaftClient<T> {\n+\n+    interface Listener<T> {\n+        /**\n+         * Callback which is invoked for all records committed to the log.\n+         * It is the responsibility of the caller to invoke {@link BatchReader#close()}\n+         * after consuming the reader.\n+         *\n+         * Note that there is not a one-to-one correspondence between writes through\n+         * {@link #scheduleAppend(int, List)} and this callback. The Raft implementation\n+         * is free to batch together the records from multiple append calls provided\n+         * that batch boundaries are respected. This means that each batch specified\n+         * through {@link #scheduleAppend(int, List)} is guaranteed to be a subset of\n+         * a batch provided by the {@link BatchReader}.\n+         *\n+         * @param reader reader instance which must be iterated\n+         */\n+        void handleCommit(BatchReader<T> reader);\n+\n+        /**\n+         * Invoked after this node has become a leader. This is only called after\n+         * all commits up to the start of the leader's epoch have been sent to\n+         * {@link #handleCommit(BatchReader)}.\n+         *\n+         * After becoming a leader, the client is eligible to write to the log\n+         * using {@link #scheduleAppend(int, List)}.\n+         *\n+         * @param epoch the claimed leader epoch\n+         */\n+        default void handleClaim(int epoch) {}\n+\n+        /**\n+         * Invoked after a leader has stepped down. This callback may or may not\n+         * fire before the next leader has been elected.\n+         */\n+        default void handleResign() {}\n+    }\n \n     /**\n-     * Initialize the client. This should only be called once and it must be\n-     * called before any of the other APIs can be invoked.\n+     * Initialize the client. This should only be called once on startup.\n      *\n      * @throws IOException For any IO errors during initialization\n      */\n     void initialize() throws IOException;\n \n     /**\n-     * Append a new entry to the log. The client must be in the leader state to\n-     * accept an append: it is up to the state machine implementation\n-     * to ensure this using {@link #currentLeaderAndEpoch()}.\n-     *\n-     * TODO: One improvement we can make here is to allow the caller to specify\n-     * the current leader epoch in the record set. That would ensure that each\n-     * leader change must be \"observed\" by the state machine before new appends\n-     * are accepted.\n-     *\n-     * @param records The records to append to the log\n-     * @param timeoutMs Maximum time to wait for the append to complete\n-     * @return A future containing the last offset and epoch of the appended records (if successful)\n-     */\n-    CompletableFuture<OffsetAndEpoch> append(Records records, AckMode ackMode, long timeoutMs);\n-\n-    /**\n-     * Read a set of records from the log. Note that it is the responsibility of the state machine\n-     * to filter control records added by the Raft client itself.\n-     *\n-     * If the fetch offset is no longer valid, then the future will be completed exceptionally\n-     * with a {@link LogTruncationException}.\n+     * Register a listener to get commit/leader notifications.\n      *\n-     * @param position The position to fetch from\n-     * @param isolation The isolation level to apply to the read\n-     * @param maxWaitTimeMs The maximum time to wait for new data to become available before completion\n-     * @return The record set, which may be empty if fetching from the end of the log\n+     * @param listener the listener\n      */\n-    CompletableFuture<Records> read(OffsetAndEpoch position, Isolation isolation, long maxWaitTimeMs);\n+    void register(Listener<T> listener);\n \n     /**\n-     * Get the current leader (if known) and the current epoch.\n+     * Append a list of records to the log. The write will be scheduled for some time\n+     * in the future. There is no guarantee that appended records will be written to\n+     * the log and eventually committed. However, it is guaranteed that if any of the\n+     * records become committed, then all of them will be.\n      *\n-     * @return Current leader and epoch information\n+     * @param epoch the current leader epoch\n+     * @param records the list of records to append\n+     * @return the offset within the current epoch that the log entries will be appended,\n+     *         or null if the leader was unable to accept the write (e.g. due to memory\n+     *         being reached).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQxMTYwMw=="}, "originalCommit": null, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NzI4NDY5OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxOTo0MzowMVrOHmxG2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQyMDoxOToxM1rOHmyULw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQxMjUwNQ==", "bodyText": "I think it is okay as the fix may be non-trivial but technically scheduleAppend will accept records even if no Listener has received a handleClaim.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510412505", "createdAt": "2020-10-22T19:43:01Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "diffHunk": "@@ -16,57 +16,75 @@\n  */\n package org.apache.kafka.raft;\n \n-import org.apache.kafka.common.record.Records;\n-\n import java.io.IOException;\n+import java.util.List;\n import java.util.concurrent.CompletableFuture;\n \n-public interface RaftClient {\n+public interface RaftClient<T> {\n+\n+    interface Listener<T> {\n+        /**\n+         * Callback which is invoked for all records committed to the log.\n+         * It is the responsibility of the caller to invoke {@link BatchReader#close()}\n+         * after consuming the reader.\n+         *\n+         * Note that there is not a one-to-one correspondence between writes through\n+         * {@link #scheduleAppend(int, List)} and this callback. The Raft implementation\n+         * is free to batch together the records from multiple append calls provided\n+         * that batch boundaries are respected. This means that each batch specified\n+         * through {@link #scheduleAppend(int, List)} is guaranteed to be a subset of\n+         * a batch provided by the {@link BatchReader}.\n+         *\n+         * @param reader reader instance which must be iterated\n+         */\n+        void handleCommit(BatchReader<T> reader);\n+\n+        /**\n+         * Invoked after this node has become a leader. This is only called after\n+         * all commits up to the start of the leader's epoch have been sent to\n+         * {@link #handleCommit(BatchReader)}.\n+         *\n+         * After becoming a leader, the client is eligible to write to the log\n+         * using {@link #scheduleAppend(int, List)}.\n+         *\n+         * @param epoch the claimed leader epoch\n+         */\n+        default void handleClaim(int epoch) {}\n+\n+        /**\n+         * Invoked after a leader has stepped down. This callback may or may not\n+         * fire before the next leader has been elected.\n+         */\n+        default void handleResign() {}\n+    }\n \n     /**\n-     * Initialize the client. This should only be called once and it must be\n-     * called before any of the other APIs can be invoked.\n+     * Initialize the client. This should only be called once on startup.\n      *\n      * @throws IOException For any IO errors during initialization\n      */\n     void initialize() throws IOException;\n \n     /**\n-     * Append a new entry to the log. The client must be in the leader state to\n-     * accept an append: it is up to the state machine implementation\n-     * to ensure this using {@link #currentLeaderAndEpoch()}.\n-     *\n-     * TODO: One improvement we can make here is to allow the caller to specify\n-     * the current leader epoch in the record set. That would ensure that each\n-     * leader change must be \"observed\" by the state machine before new appends\n-     * are accepted.\n-     *\n-     * @param records The records to append to the log\n-     * @param timeoutMs Maximum time to wait for the append to complete\n-     * @return A future containing the last offset and epoch of the appended records (if successful)\n-     */\n-    CompletableFuture<OffsetAndEpoch> append(Records records, AckMode ackMode, long timeoutMs);\n-\n-    /**\n-     * Read a set of records from the log. Note that it is the responsibility of the state machine\n-     * to filter control records added by the Raft client itself.\n-     *\n-     * If the fetch offset is no longer valid, then the future will be completed exceptionally\n-     * with a {@link LogTruncationException}.\n+     * Register a listener to get commit/leader notifications.\n      *\n-     * @param position The position to fetch from\n-     * @param isolation The isolation level to apply to the read\n-     * @param maxWaitTimeMs The maximum time to wait for new data to become available before completion\n-     * @return The record set, which may be empty if fetching from the end of the log\n+     * @param listener the listener\n      */\n-    CompletableFuture<Records> read(OffsetAndEpoch position, Isolation isolation, long maxWaitTimeMs);\n+    void register(Listener<T> listener);\n \n     /**\n-     * Get the current leader (if known) and the current epoch.\n+     * Append a list of records to the log. The write will be scheduled for some time\n+     * in the future. There is no guarantee that appended records will be written to\n+     * the log and eventually committed. However, it is guaranteed that if any of the\n+     * records become committed, then all of them will be.\n      *\n-     * @return Current leader and epoch information\n+     * @param epoch the current leader epoch\n+     * @param records the list of records to append\n+     * @return the offset within the current epoch that the log entries will be appended,\n+     *         or null if the leader was unable to accept the write (e.g. due to memory\n+     *         being reached).\n      */\n-    LeaderAndEpoch currentLeaderAndEpoch();\n+    Long scheduleAppend(int epoch, List<T> records);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQzMjMwMw==", "bodyText": "Yeah, I don't see a strong need to be too strict about this for now.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510432303", "createdAt": "2020-10-22T20:19:13Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "diffHunk": "@@ -16,57 +16,75 @@\n  */\n package org.apache.kafka.raft;\n \n-import org.apache.kafka.common.record.Records;\n-\n import java.io.IOException;\n+import java.util.List;\n import java.util.concurrent.CompletableFuture;\n \n-public interface RaftClient {\n+public interface RaftClient<T> {\n+\n+    interface Listener<T> {\n+        /**\n+         * Callback which is invoked for all records committed to the log.\n+         * It is the responsibility of the caller to invoke {@link BatchReader#close()}\n+         * after consuming the reader.\n+         *\n+         * Note that there is not a one-to-one correspondence between writes through\n+         * {@link #scheduleAppend(int, List)} and this callback. The Raft implementation\n+         * is free to batch together the records from multiple append calls provided\n+         * that batch boundaries are respected. This means that each batch specified\n+         * through {@link #scheduleAppend(int, List)} is guaranteed to be a subset of\n+         * a batch provided by the {@link BatchReader}.\n+         *\n+         * @param reader reader instance which must be iterated\n+         */\n+        void handleCommit(BatchReader<T> reader);\n+\n+        /**\n+         * Invoked after this node has become a leader. This is only called after\n+         * all commits up to the start of the leader's epoch have been sent to\n+         * {@link #handleCommit(BatchReader)}.\n+         *\n+         * After becoming a leader, the client is eligible to write to the log\n+         * using {@link #scheduleAppend(int, List)}.\n+         *\n+         * @param epoch the claimed leader epoch\n+         */\n+        default void handleClaim(int epoch) {}\n+\n+        /**\n+         * Invoked after a leader has stepped down. This callback may or may not\n+         * fire before the next leader has been elected.\n+         */\n+        default void handleResign() {}\n+    }\n \n     /**\n-     * Initialize the client. This should only be called once and it must be\n-     * called before any of the other APIs can be invoked.\n+     * Initialize the client. This should only be called once on startup.\n      *\n      * @throws IOException For any IO errors during initialization\n      */\n     void initialize() throws IOException;\n \n     /**\n-     * Append a new entry to the log. The client must be in the leader state to\n-     * accept an append: it is up to the state machine implementation\n-     * to ensure this using {@link #currentLeaderAndEpoch()}.\n-     *\n-     * TODO: One improvement we can make here is to allow the caller to specify\n-     * the current leader epoch in the record set. That would ensure that each\n-     * leader change must be \"observed\" by the state machine before new appends\n-     * are accepted.\n-     *\n-     * @param records The records to append to the log\n-     * @param timeoutMs Maximum time to wait for the append to complete\n-     * @return A future containing the last offset and epoch of the appended records (if successful)\n-     */\n-    CompletableFuture<OffsetAndEpoch> append(Records records, AckMode ackMode, long timeoutMs);\n-\n-    /**\n-     * Read a set of records from the log. Note that it is the responsibility of the state machine\n-     * to filter control records added by the Raft client itself.\n-     *\n-     * If the fetch offset is no longer valid, then the future will be completed exceptionally\n-     * with a {@link LogTruncationException}.\n+     * Register a listener to get commit/leader notifications.\n      *\n-     * @param position The position to fetch from\n-     * @param isolation The isolation level to apply to the read\n-     * @param maxWaitTimeMs The maximum time to wait for new data to become available before completion\n-     * @return The record set, which may be empty if fetching from the end of the log\n+     * @param listener the listener\n      */\n-    CompletableFuture<Records> read(OffsetAndEpoch position, Isolation isolation, long maxWaitTimeMs);\n+    void register(Listener<T> listener);\n \n     /**\n-     * Get the current leader (if known) and the current epoch.\n+     * Append a list of records to the log. The write will be scheduled for some time\n+     * in the future. There is no guarantee that appended records will be written to\n+     * the log and eventually committed. However, it is guaranteed that if any of the\n+     * records become committed, then all of them will be.\n      *\n-     * @return Current leader and epoch information\n+     * @param epoch the current leader epoch\n+     * @param records the list of records to append\n+     * @return the offset within the current epoch that the log entries will be appended,\n+     *         or null if the leader was unable to accept the write (e.g. due to memory\n+     *         being reached).\n      */\n-    LeaderAndEpoch currentLeaderAndEpoch();\n+    Long scheduleAppend(int epoch, List<T> records);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQxMjUwNQ=="}, "originalCommit": null, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NzI4OTU0OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/RecordSerde.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxOTo0NDoxN1rOHmxJvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQxOTo0NDoxN1rOHmxJvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQxMzI0NA==", "bodyText": "TODO: missing comments.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r510413244", "createdAt": "2020-10-22T19:44:17Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/RecordSerde.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+import org.apache.kafka.common.protocol.DataOutputStreamWritable;\n+import org.apache.kafka.common.protocol.Readable;\n+\n+public interface RecordSerde<T> {\n+    /**\n+     * Create a new context object for to be used when serializing a batch of records.\n+     * This allows for state to be shared between {@link #recordSize(Object, Object)}\n+     * and {@link #write(Object, Object, DataOutputStreamWritable)}, which is useful\n+     * in order to avoid redundant work (see e.g.\n+     * {@link org.apache.kafka.common.protocol.ObjectSerializationCache}).\n+     *\n+     * @return context object or null if none is needed\n+     */\n+    default Object newWriteContext() {\n+        return null;\n+    }\n+\n+    /**\n+     * Get the size of a record.\n+     *\n+     * @param data the record that will be serialized\n+     * @param context context object created by {@link #newWriteContext()}\n+     * @return the size in bytes of the serialized record\n+     */\n+    int recordSize(T data, Object context);\n+\n+\n+    /**\n+     * Write the record to the output stream.\n+     *\n+     * @param data the record to serialize and write\n+     * @param context context object created by {@link #newWriteContext()}\n+     * @param out the output stream to write the record to\n+     */\n+    void write(T data, Object context, DataOutputStreamWritable out);\n+\n+    /**\n+     *\n+     * @param input\n+     * @param size\n+     * @return\n+     */", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTUxMDU1OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/metrics/stats/Percentiles.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNToyNjowMFrOHpaiUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxOToyODo0MlrOHqrVVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE4ODQzMw==", "bodyText": "Why do we change these values to debug?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r513188433", "createdAt": "2020-10-28T05:26:00Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/metrics/stats/Percentiles.java", "diffHunk": "@@ -113,11 +113,11 @@ protected HistogramSample newSample(long timeMs) {\n     protected void update(Sample sample, MetricConfig config, double value, long timeMs) {\n         final double boundedValue;\n         if (value > max) {\n-            log.warn(\"Received value {} which is greater than max recordable value {}, will be pinned to the max value\",\n+            log.debug(\"Received value {} which is greater than max recordable value {}, will be pinned to the max value\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDUxMjIxMw==", "bodyText": "The warn seemed excessive for a metric update, which could be done very frequently. I looked over the code and it looks like we don't really use this outside of tests (and now the code added in this patch). I think the user should just understand the contract, which is that anything outside of the specified range gets rounded down.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514512213", "createdAt": "2020-10-29T19:28:42Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/metrics/stats/Percentiles.java", "diffHunk": "@@ -113,11 +113,11 @@ protected HistogramSample newSample(long timeMs) {\n     protected void update(Sample sample, MetricConfig config, double value, long timeMs) {\n         final double boundedValue;\n         if (value > max) {\n-            log.warn(\"Received value {} which is greater than max recordable value {}, will be pinned to the max value\",\n+            log.debug(\"Received value {} which is greater than max recordable value {}, will be pinned to the max value\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE4ODQzMw=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTU5MDUzOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/protocol/DataInputStreamReadable.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNjowNjoxOFrOHpbPig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxOTozMjowM1rOHqrcgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwMDAxMA==", "bodyText": "nit: we could refactor out the try-catch logic.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r513200010", "createdAt": "2020-10-28T06:06:18Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/DataInputStreamReadable.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.protocol;\n+\n+import org.apache.kafka.common.utils.ByteUtils;\n+\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+public class DataInputStreamReadable implements Readable, Closeable {\n+    protected final DataInputStream input;\n+\n+    public DataInputStreamReadable(DataInputStream input) {\n+        this.input = input;\n+    }\n+\n+    @Override\n+    public byte readByte() {\n+        try {\n+            return input.readByte();\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    @Override\n+    public short readShort() {\n+        try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDUxNDA1MQ==", "bodyText": "Yeah, I could introduce a helper with a lambda, but that would add some unnecessary garbage to the deserialization path. Although it is ugly, I think the duplication is not a big deal. We probably won't touch this class after it is created.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514514051", "createdAt": "2020-10-29T19:32:03Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/DataInputStreamReadable.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.protocol;\n+\n+import org.apache.kafka.common.utils.ByteUtils;\n+\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+public class DataInputStreamReadable implements Readable, Closeable {\n+    protected final DataInputStream input;\n+\n+    public DataInputStreamReadable(DataInputStream input) {\n+        this.input = input;\n+    }\n+\n+    @Override\n+    public byte readByte() {\n+        try {\n+            return input.readByte();\n+        } catch (IOException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    @Override\n+    public short readShort() {\n+        try {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwMDAxMA=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTU5MTM5OnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/common/protocol/DataInputStreamReadable.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNjowNjo0OVrOHpbQFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxOTozNToyMFrOHqrjMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwMDE0OA==", "bodyText": "Why do we wrap the IO exception here?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r513200148", "createdAt": "2020-10-28T06:06:49Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/DataInputStreamReadable.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.protocol;\n+\n+import org.apache.kafka.common.utils.ByteUtils;\n+\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+public class DataInputStreamReadable implements Readable, Closeable {\n+    protected final DataInputStream input;\n+\n+    public DataInputStreamReadable(DataInputStream input) {\n+        this.input = input;\n+    }\n+\n+    @Override\n+    public byte readByte() {\n+        try {\n+            return input.readByte();\n+        } catch (IOException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDUxNTc2Mw==", "bodyText": "IOException is checked, so we cannot raise it from the current Readable interface, so the options are to either add the exception to the Readable interface or to rethrow it as an unchecked exception. I went with the latter to reduce the impact and because I think we tend to prefer unchecked exceptions in general since checked exceptions sort of end up leaking their way through a bunch of call stacks.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514515763", "createdAt": "2020-10-29T19:35:20Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/protocol/DataInputStreamReadable.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.protocol;\n+\n+import org.apache.kafka.common.utils.ByteUtils;\n+\n+import java.io.Closeable;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+public class DataInputStreamReadable implements Readable, Closeable {\n+    protected final DataInputStream input;\n+\n+    public DataInputStreamReadable(DataInputStream input) {\n+        this.input = input;\n+    }\n+\n+    @Override\n+    public byte readByte() {\n+        try {\n+            return input.readByte();\n+        } catch (IOException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwMDE0OA=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTYzMTI1OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/BatchReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNjoyNjo1MVrOHpbncA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxOTozNjo1MlrOHqrmWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwNjEyOA==", "bodyText": "We couldn't be sure whether the records could be read as string?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r513206128", "createdAt": "2020-10-28T06:26:51Z", "author": {"login": "abbccdda"}, "path": "raft/src/main/java/org/apache/kafka/raft/BatchReader.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+import java.io.Closeable;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+\n+/**\n+ * This interface is used to send committed data from the {@link RaftClient}\n+ * down to registered {@link RaftClient.Listener} instances.\n+ *\n+ * The advantage of hiding the consumption of committed batches behind an interface\n+ * is that it allows us to push blocking operations such as reads from disk outside\n+ * of the Raft IO thread. This helps to ensure that a slow state machine will not\n+ * affect replication.\n+ *\n+ * @param <T> record type (see {@link org.apache.kafka.raft.RecordSerde})\n+ */\n+public interface BatchReader<T> extends Iterator<BatchReader.Batch<T>>, Closeable {\n+\n+    /**\n+     * Get the base offset of the readable batches. Note that this value is a constant\n+     * which is defined when the {@link BatchReader} instance is constructed. It does\n+     * not change based on reader progress.\n+     *\n+     * @return the base offset\n+     */\n+    long baseOffset();\n+\n+    /**\n+     * Get the last offset of the batch if it is known. When reading from disk, we may\n+     * not know the last offset of a set of records until it has been read from disk.\n+     * In this case, the state machine cannot advance to the next committed data until\n+     * all batches from the {@link BatchReader} instance have been consumed.\n+     *\n+     * @return optional last offset\n+     */\n+    OptionalLong lastOffset();\n+\n+    /**\n+     * Close this reader. It is the responsibility of the {@link RaftClient.Listener}\n+     * to close each reader passed to {@link RaftClient.Listener#handleCommit(BatchReader)}.\n+     */\n+    @Override\n+    void close();\n+\n+    class Batch<T> {\n+        private final long baseOffset;\n+        private final int epoch;\n+        private final List<T> records;\n+\n+        public Batch(long baseOffset, int epoch, List<T> records) {\n+            this.baseOffset = baseOffset;\n+            this.epoch = epoch;\n+            this.records = records;\n+        }\n+\n+        public long lastOffset() {\n+            return baseOffset + records.size() - 1;\n+        }\n+\n+        public long baseOffset() {\n+            return baseOffset;\n+        }\n+\n+        public List<T> records() {\n+            return records;\n+        }\n+\n+        public int epoch() {\n+            return epoch;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"Batch(\" +\n+                \"baseOffset=\" + baseOffset +\n+                \", epoch=\" + epoch +\n+                \", records=\" + records +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDUxNjU3MA==", "bodyText": "Yeah, I'm relying on the toString. I think this is only useful for debugging.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514516570", "createdAt": "2020-10-29T19:36:52Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/BatchReader.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+import java.io.Closeable;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+\n+/**\n+ * This interface is used to send committed data from the {@link RaftClient}\n+ * down to registered {@link RaftClient.Listener} instances.\n+ *\n+ * The advantage of hiding the consumption of committed batches behind an interface\n+ * is that it allows us to push blocking operations such as reads from disk outside\n+ * of the Raft IO thread. This helps to ensure that a slow state machine will not\n+ * affect replication.\n+ *\n+ * @param <T> record type (see {@link org.apache.kafka.raft.RecordSerde})\n+ */\n+public interface BatchReader<T> extends Iterator<BatchReader.Batch<T>>, Closeable {\n+\n+    /**\n+     * Get the base offset of the readable batches. Note that this value is a constant\n+     * which is defined when the {@link BatchReader} instance is constructed. It does\n+     * not change based on reader progress.\n+     *\n+     * @return the base offset\n+     */\n+    long baseOffset();\n+\n+    /**\n+     * Get the last offset of the batch if it is known. When reading from disk, we may\n+     * not know the last offset of a set of records until it has been read from disk.\n+     * In this case, the state machine cannot advance to the next committed data until\n+     * all batches from the {@link BatchReader} instance have been consumed.\n+     *\n+     * @return optional last offset\n+     */\n+    OptionalLong lastOffset();\n+\n+    /**\n+     * Close this reader. It is the responsibility of the {@link RaftClient.Listener}\n+     * to close each reader passed to {@link RaftClient.Listener#handleCommit(BatchReader)}.\n+     */\n+    @Override\n+    void close();\n+\n+    class Batch<T> {\n+        private final long baseOffset;\n+        private final int epoch;\n+        private final List<T> records;\n+\n+        public Batch(long baseOffset, int epoch, List<T> records) {\n+            this.baseOffset = baseOffset;\n+            this.epoch = epoch;\n+            this.records = records;\n+        }\n+\n+        public long lastOffset() {\n+            return baseOffset + records.size() - 1;\n+        }\n+\n+        public long baseOffset() {\n+            return baseOffset;\n+        }\n+\n+        public List<T> records() {\n+            return records;\n+        }\n+\n+        public int epoch() {\n+            return epoch;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"Batch(\" +\n+                \"baseOffset=\" + baseOffset +\n+                \", epoch=\" + epoch +\n+                \", records=\" + records +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwNjEyOA=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTYzNzE3OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/BatchReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNjoyOTozM1rOHpbqxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxOTo0MDoxM1rOHqrtWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwNjk4Mw==", "bodyText": "Do we compare the value or the address of records?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r513206983", "createdAt": "2020-10-28T06:29:33Z", "author": {"login": "abbccdda"}, "path": "raft/src/main/java/org/apache/kafka/raft/BatchReader.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+import java.io.Closeable;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+\n+/**\n+ * This interface is used to send committed data from the {@link RaftClient}\n+ * down to registered {@link RaftClient.Listener} instances.\n+ *\n+ * The advantage of hiding the consumption of committed batches behind an interface\n+ * is that it allows us to push blocking operations such as reads from disk outside\n+ * of the Raft IO thread. This helps to ensure that a slow state machine will not\n+ * affect replication.\n+ *\n+ * @param <T> record type (see {@link org.apache.kafka.raft.RecordSerde})\n+ */\n+public interface BatchReader<T> extends Iterator<BatchReader.Batch<T>>, Closeable {\n+\n+    /**\n+     * Get the base offset of the readable batches. Note that this value is a constant\n+     * which is defined when the {@link BatchReader} instance is constructed. It does\n+     * not change based on reader progress.\n+     *\n+     * @return the base offset\n+     */\n+    long baseOffset();\n+\n+    /**\n+     * Get the last offset of the batch if it is known. When reading from disk, we may\n+     * not know the last offset of a set of records until it has been read from disk.\n+     * In this case, the state machine cannot advance to the next committed data until\n+     * all batches from the {@link BatchReader} instance have been consumed.\n+     *\n+     * @return optional last offset\n+     */\n+    OptionalLong lastOffset();\n+\n+    /**\n+     * Close this reader. It is the responsibility of the {@link RaftClient.Listener}\n+     * to close each reader passed to {@link RaftClient.Listener#handleCommit(BatchReader)}.\n+     */\n+    @Override\n+    void close();\n+\n+    class Batch<T> {\n+        private final long baseOffset;\n+        private final int epoch;\n+        private final List<T> records;\n+\n+        public Batch(long baseOffset, int epoch, List<T> records) {\n+            this.baseOffset = baseOffset;\n+            this.epoch = epoch;\n+            this.records = records;\n+        }\n+\n+        public long lastOffset() {\n+            return baseOffset + records.size() - 1;\n+        }\n+\n+        public long baseOffset() {\n+            return baseOffset;\n+        }\n+\n+        public List<T> records() {\n+            return records;\n+        }\n+\n+        public int epoch() {\n+            return epoch;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"Batch(\" +\n+                \"baseOffset=\" + baseOffset +\n+                \", epoch=\" + epoch +\n+                \", records=\" + records +\n+                ')';\n+        }\n+\n+        @Override\n+        public boolean equals(Object o) {\n+            if (this == o) return true;\n+            if (o == null || getClass() != o.getClass()) return false;\n+            Batch<?> batch = (Batch<?>) o;\n+            return baseOffset == batch.baseOffset &&\n+                epoch == batch.epoch &&\n+                Objects.equals(records, batch.records);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDUxODM2MA==", "bodyText": "We are relying on the standard equals method. I think it's up to the user of the api to ensure a reasonable implementation if they expect to rely on batch equality. The raft implementation does not set any expectations on record equality, but it is useful in testing where we can control the record type.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514518360", "createdAt": "2020-10-29T19:40:13Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/BatchReader.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+import java.io.Closeable;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+\n+/**\n+ * This interface is used to send committed data from the {@link RaftClient}\n+ * down to registered {@link RaftClient.Listener} instances.\n+ *\n+ * The advantage of hiding the consumption of committed batches behind an interface\n+ * is that it allows us to push blocking operations such as reads from disk outside\n+ * of the Raft IO thread. This helps to ensure that a slow state machine will not\n+ * affect replication.\n+ *\n+ * @param <T> record type (see {@link org.apache.kafka.raft.RecordSerde})\n+ */\n+public interface BatchReader<T> extends Iterator<BatchReader.Batch<T>>, Closeable {\n+\n+    /**\n+     * Get the base offset of the readable batches. Note that this value is a constant\n+     * which is defined when the {@link BatchReader} instance is constructed. It does\n+     * not change based on reader progress.\n+     *\n+     * @return the base offset\n+     */\n+    long baseOffset();\n+\n+    /**\n+     * Get the last offset of the batch if it is known. When reading from disk, we may\n+     * not know the last offset of a set of records until it has been read from disk.\n+     * In this case, the state machine cannot advance to the next committed data until\n+     * all batches from the {@link BatchReader} instance have been consumed.\n+     *\n+     * @return optional last offset\n+     */\n+    OptionalLong lastOffset();\n+\n+    /**\n+     * Close this reader. It is the responsibility of the {@link RaftClient.Listener}\n+     * to close each reader passed to {@link RaftClient.Listener#handleCommit(BatchReader)}.\n+     */\n+    @Override\n+    void close();\n+\n+    class Batch<T> {\n+        private final long baseOffset;\n+        private final int epoch;\n+        private final List<T> records;\n+\n+        public Batch(long baseOffset, int epoch, List<T> records) {\n+            this.baseOffset = baseOffset;\n+            this.epoch = epoch;\n+            this.records = records;\n+        }\n+\n+        public long lastOffset() {\n+            return baseOffset + records.size() - 1;\n+        }\n+\n+        public long baseOffset() {\n+            return baseOffset;\n+        }\n+\n+        public List<T> records() {\n+            return records;\n+        }\n+\n+        public int epoch() {\n+            return epoch;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"Batch(\" +\n+                \"baseOffset=\" + baseOffset +\n+                \", epoch=\" + epoch +\n+                \", records=\" + records +\n+                ')';\n+        }\n+\n+        @Override\n+        public boolean equals(Object o) {\n+            if (this == o) return true;\n+            if (o == null || getClass() != o.getClass()) return false;\n+            Batch<?> batch = (Batch<?>) o;\n+            return baseOffset == batch.baseOffset &&\n+                epoch == batch.epoch &&\n+                Objects.equals(records, batch.records);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwNjk4Mw=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTY0MzI3OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/ExpirationService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNjozMjozMlrOHpbuSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNjozMjozMlrOHpbuSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIwNzg4MA==", "bodyText": "after the provided timeout passes if it is not completed/ if not completed within the provided time limit", "url": "https://github.com/apache/kafka/pull/9482#discussion_r513207880", "createdAt": "2020-10-28T06:32:32Z", "author": {"login": "abbccdda"}, "path": "raft/src/main/java/org/apache/kafka/raft/ExpirationService.java", "diffHunk": "@@ -16,11 +16,17 @@\n  */\n package org.apache.kafka.raft;\n \n-import org.apache.kafka.common.KafkaException;\n+import java.util.concurrent.CompletableFuture;\n \n-public class LogTruncationException extends KafkaException {\n-\n-    public LogTruncationException(String message) {\n-        super(message);\n-    }\n+public interface ExpirationService {\n+    /**\n+     * Get a new completable future which will automatically expire with a\n+     * {@link org.apache.kafka.common.errors.TimeoutException} after the provided\n+     * timeout passes if it is not completed before then through some other means.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNTY2MDM2OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/QuorumState.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNjo0MDoyM1rOHpb4UQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNjo0MDoyM1rOHpb4UQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzIxMDQ0OQ==", "bodyText": "it does not could be removed.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r513210449", "createdAt": "2020-10-28T06:40:23Z", "author": {"login": "abbccdda"}, "path": "raft/src/main/java/org/apache/kafka/raft/QuorumState.java", "diffHunk": "@@ -369,6 +379,17 @@ public void transitionToLeader(long epochStartOffset) throws IOException {\n         if (!candidateState.isVoteGranted())\n             throw new IllegalStateException(\"Cannot become leader without majority votes granted\");\n \n+        // Note that the leader does not retain the high watermark that was known\n+        // in the previous state. The reason it does not is to protect the monotonicity", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxODMyMzYzOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/internals/MemoryBatchReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNzoyNzoxN1rOHp1irg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNzoyNzoxN1rOHp1irg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYzMDg5NA==", "bodyText": "Should we add description to the exception?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r513630894", "createdAt": "2020-10-28T17:27:17Z", "author": {"login": "abbccdda"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/MemoryBatchReader.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.raft.BatchReader;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.OptionalLong;\n+\n+public class MemoryBatchReader<T> implements BatchReader<T> {\n+    private final CloseListener<BatchReader<T>> closeListener;\n+    private final Iterator<Batch<T>> iterator;\n+    private final long baseOffset;\n+    private final long lastOffset;\n+\n+    public MemoryBatchReader(\n+        List<Batch<T>> batches,\n+        CloseListener<BatchReader<T>> closeListener\n+    ) {\n+        if (batches.isEmpty()) {\n+            throw new IllegalArgumentException();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxODMzMjc3OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/internals/RecordsBatchReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxNzoyOToyNFrOHp1oiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQyMDowNTo0NFrOHqspUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYzMjM5Mg==", "bodyText": "How do we decide to use either record batch reader or memory batch reader?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r513632392", "createdAt": "2020-10-28T17:29:24Z", "author": {"login": "abbccdda"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/RecordsBatchReader.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.protocol.DataInputStreamReadable;\n+import org.apache.kafka.common.protocol.Readable;\n+import org.apache.kafka.common.record.BufferSupplier;\n+import org.apache.kafka.common.record.DefaultRecordBatch;\n+import org.apache.kafka.common.record.FileRecords;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.MutableRecordBatch;\n+import org.apache.kafka.common.record.Records;\n+import org.apache.kafka.raft.BatchReader;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.OptionalLong;\n+\n+public class RecordsBatchReader<T> implements BatchReader<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDUzMzcxMg==", "bodyText": "MemoryBatchReader is only used for writes from the leader. We retain the original records from the call to scheduleAppend and send them to the listener in handleCommit. This is useful because it ensures the active controller will not need to read from disk.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514533712", "createdAt": "2020-10-29T20:05:44Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/RecordsBatchReader.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.common.protocol.DataInputStreamReadable;\n+import org.apache.kafka.common.protocol.Readable;\n+import org.apache.kafka.common.record.BufferSupplier;\n+import org.apache.kafka.common.record.DefaultRecordBatch;\n+import org.apache.kafka.common.record.FileRecords;\n+import org.apache.kafka.common.record.MemoryRecords;\n+import org.apache.kafka.common.record.MutableRecordBatch;\n+import org.apache.kafka.common.record.Records;\n+import org.apache.kafka.raft.BatchReader;\n+import org.apache.kafka.raft.RecordSerde;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.NoSuchElementException;\n+import java.util.OptionalLong;\n+\n+public class RecordsBatchReader<T> implements BatchReader<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzYzMjM5Mg=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyMDE3MTYzOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/internals/ThresholdPurgatory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQwMzozNjowMVrOHqHWvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQyMDowODozN1rOHqsxUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzkyMjc1MA==", "bodyText": "Do we have other types of threshold than Long?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r513922750", "createdAt": "2020-10-29T03:36:01Z", "author": {"login": "abbccdda"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/ThresholdPurgatory.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.raft.ExpirationService;\n+\n+import java.util.NavigableMap;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentNavigableMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+public class ThresholdPurgatory<T extends Comparable<T>> implements FuturePurgatory<T> {\n+    private final AtomicLong idGenerator = new AtomicLong(0);\n+    private final ExpirationService expirationService;\n+    private final ConcurrentNavigableMap<ThresholdKey<T>, CompletableFuture<Long>> thresholdMap =\n+        new ConcurrentSkipListMap<>();\n+\n+    public ThresholdPurgatory(ExpirationService expirationService) {\n+        this.expirationService = expirationService;\n+    }\n+\n+    @Override\n+    public CompletableFuture<Long> await(T threshold, long maxWaitTimeMs) {\n+        ThresholdKey<T> key = new ThresholdKey<>(idGenerator.incrementAndGet(), threshold);\n+        CompletableFuture<Long> future = expirationService.await(maxWaitTimeMs);\n+        thresholdMap.put(key, future);\n+        future.whenComplete((timeMs, exception) -> thresholdMap.remove(key));\n+        return future;\n+    }\n+\n+    @Override\n+    public void maybeComplete(T value, long currentTimeMs) {\n+        ThresholdKey<T> maxKey = new ThresholdKey<>(Long.MAX_VALUE, value);\n+        NavigableMap<ThresholdKey<T>, CompletableFuture<Long>> submap = thresholdMap.headMap(maxKey);\n+        for (CompletableFuture<Long> completion : submap.values()) {\n+            completion.complete(currentTimeMs);\n+        }\n+    }\n+\n+    @Override\n+    public void completeAll(long currentTimeMs) {\n+        for (CompletableFuture<Long> completion : thresholdMap.values()) {\n+            completion.complete(currentTimeMs);\n+        }\n+    }\n+\n+    @Override\n+    public void completeAllExceptionally(Throwable exception) {\n+        for (CompletableFuture<Long> completion : thresholdMap.values()) {\n+            completion.completeExceptionally(exception);\n+        }\n+    }\n+\n+    @Override\n+    public int numWaiting() {\n+        return thresholdMap.size();\n+    }\n+\n+    private static class ThresholdKey<T extends Comparable<T>> implements Comparable<ThresholdKey<T>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDUzNTc2MQ==", "bodyText": "Not at the moment. I guess your point is that we might be able to drop the generics, which is fair. I think we can also drop the FuturePurgatory interface. Is it ok if we save this for a follow-up?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514535761", "createdAt": "2020-10-29T20:08:37Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/internals/ThresholdPurgatory.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft.internals;\n+\n+import org.apache.kafka.raft.ExpirationService;\n+\n+import java.util.NavigableMap;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentNavigableMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+public class ThresholdPurgatory<T extends Comparable<T>> implements FuturePurgatory<T> {\n+    private final AtomicLong idGenerator = new AtomicLong(0);\n+    private final ExpirationService expirationService;\n+    private final ConcurrentNavigableMap<ThresholdKey<T>, CompletableFuture<Long>> thresholdMap =\n+        new ConcurrentSkipListMap<>();\n+\n+    public ThresholdPurgatory(ExpirationService expirationService) {\n+        this.expirationService = expirationService;\n+    }\n+\n+    @Override\n+    public CompletableFuture<Long> await(T threshold, long maxWaitTimeMs) {\n+        ThresholdKey<T> key = new ThresholdKey<>(idGenerator.incrementAndGet(), threshold);\n+        CompletableFuture<Long> future = expirationService.await(maxWaitTimeMs);\n+        thresholdMap.put(key, future);\n+        future.whenComplete((timeMs, exception) -> thresholdMap.remove(key));\n+        return future;\n+    }\n+\n+    @Override\n+    public void maybeComplete(T value, long currentTimeMs) {\n+        ThresholdKey<T> maxKey = new ThresholdKey<>(Long.MAX_VALUE, value);\n+        NavigableMap<ThresholdKey<T>, CompletableFuture<Long>> submap = thresholdMap.headMap(maxKey);\n+        for (CompletableFuture<Long> completion : submap.values()) {\n+            completion.complete(currentTimeMs);\n+        }\n+    }\n+\n+    @Override\n+    public void completeAll(long currentTimeMs) {\n+        for (CompletableFuture<Long> completion : thresholdMap.values()) {\n+            completion.complete(currentTimeMs);\n+        }\n+    }\n+\n+    @Override\n+    public void completeAllExceptionally(Throwable exception) {\n+        for (CompletableFuture<Long> completion : thresholdMap.values()) {\n+            completion.completeExceptionally(exception);\n+        }\n+    }\n+\n+    @Override\n+    public int numWaiting() {\n+        return thresholdMap.size();\n+    }\n+\n+    private static class ThresholdKey<T extends Comparable<T>> implements Comparable<ThresholdKey<T>> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzkyMjc1MA=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyMDc4MjgyOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQwNjo0MzoxOVrOHqNuBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQyMDoxMDozNVrOHqs1fA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDAyNzAxNQ==", "bodyText": "Why do we need to repeatedly get leader epoch?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514027015", "createdAt": "2020-10-29T06:43:19Z", "author": {"login": "abbccdda"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -228,35 +234,81 @@ private void updateLeaderEndOffsetAndTimestamp(\n         final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n \n         if (state.updateLocalState(currentTimeMs, endOffsetMetadata)) {\n-            updateHighWatermark(state, currentTimeMs);\n+            onUpdateLeaderHighWatermark(state, currentTimeMs);\n         }\n \n-        LogOffset endOffset = new LogOffset(endOffsetMetadata.offset, Isolation.UNCOMMITTED);\n-        fetchPurgatory.maybeComplete(endOffset, currentTimeMs);\n+        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n     }\n \n-    private void updateHighWatermark(\n-        EpochState state,\n+    private void onUpdateLeaderHighWatermark(\n+        LeaderState state,\n         long currentTimeMs\n     ) {\n         state.highWatermark().ifPresent(highWatermark -> {\n-            logger.debug(\"High watermark updated to {}\", highWatermark);\n+            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n             log.updateHighWatermark(highWatermark);\n-\n-            LogOffset offset = new LogOffset(highWatermark.offset, Isolation.COMMITTED);\n-            appendPurgatory.maybeComplete(offset, currentTimeMs);\n-            fetchPurgatory.maybeComplete(offset, currentTimeMs);\n+            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n+            maybeFireHandleCommit(highWatermark.offset);\n         });\n     }\n \n-    @Override\n-    public LeaderAndEpoch currentLeaderAndEpoch() {\n-        return quorum.leaderAndEpoch();\n+    private void maybeFireHandleCommit(long highWatermark) {\n+        maybeFireHandleCommit(listenerContexts, highWatermark);\n+    }\n+\n+    private void maybeFireHandleCommit(List<ListenerContext> listenerContexts, long highWatermark) {\n+        // TODO: When there are multiple listeners, we can cache reads to save some work\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                continue;\n+            }\n+\n+            long nextExpectedOffset = nextExpectedOffsetOpt.getAsLong();\n+            if (nextExpectedOffset < highWatermark) {\n+                LogFetchInfo readInfo = log.read(nextExpectedOffset, Isolation.COMMITTED);\n+                listenerContext.fireHandleCommit(nextExpectedOffset, readInfo.records);\n+            }\n+        }\n+    }\n+\n+    private void maybeFireHandleCommit(long baseOffset, int epoch, List<T> records) {\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                continue;\n+            }\n+\n+            long nextExpectedOffset = nextExpectedOffsetOpt.getAsLong();\n+            if (nextExpectedOffset == baseOffset) {\n+                listenerContext.fireHandleCommit(baseOffset, epoch, records);\n+            }\n+        }\n+    }\n+\n+    private void maybeFireHandleClaim(LeaderState state) {\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            int leaderEpoch = state.epoch();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDUzNjgyOA==", "bodyText": "I will move this outside the loop.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514536828", "createdAt": "2020-10-29T20:10:35Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -228,35 +234,81 @@ private void updateLeaderEndOffsetAndTimestamp(\n         final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n \n         if (state.updateLocalState(currentTimeMs, endOffsetMetadata)) {\n-            updateHighWatermark(state, currentTimeMs);\n+            onUpdateLeaderHighWatermark(state, currentTimeMs);\n         }\n \n-        LogOffset endOffset = new LogOffset(endOffsetMetadata.offset, Isolation.UNCOMMITTED);\n-        fetchPurgatory.maybeComplete(endOffset, currentTimeMs);\n+        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n     }\n \n-    private void updateHighWatermark(\n-        EpochState state,\n+    private void onUpdateLeaderHighWatermark(\n+        LeaderState state,\n         long currentTimeMs\n     ) {\n         state.highWatermark().ifPresent(highWatermark -> {\n-            logger.debug(\"High watermark updated to {}\", highWatermark);\n+            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n             log.updateHighWatermark(highWatermark);\n-\n-            LogOffset offset = new LogOffset(highWatermark.offset, Isolation.COMMITTED);\n-            appendPurgatory.maybeComplete(offset, currentTimeMs);\n-            fetchPurgatory.maybeComplete(offset, currentTimeMs);\n+            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n+            maybeFireHandleCommit(highWatermark.offset);\n         });\n     }\n \n-    @Override\n-    public LeaderAndEpoch currentLeaderAndEpoch() {\n-        return quorum.leaderAndEpoch();\n+    private void maybeFireHandleCommit(long highWatermark) {\n+        maybeFireHandleCommit(listenerContexts, highWatermark);\n+    }\n+\n+    private void maybeFireHandleCommit(List<ListenerContext> listenerContexts, long highWatermark) {\n+        // TODO: When there are multiple listeners, we can cache reads to save some work\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                continue;\n+            }\n+\n+            long nextExpectedOffset = nextExpectedOffsetOpt.getAsLong();\n+            if (nextExpectedOffset < highWatermark) {\n+                LogFetchInfo readInfo = log.read(nextExpectedOffset, Isolation.COMMITTED);\n+                listenerContext.fireHandleCommit(nextExpectedOffset, readInfo.records);\n+            }\n+        }\n+    }\n+\n+    private void maybeFireHandleCommit(long baseOffset, int epoch, List<T> records) {\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                continue;\n+            }\n+\n+            long nextExpectedOffset = nextExpectedOffsetOpt.getAsLong();\n+            if (nextExpectedOffset == baseOffset) {\n+                listenerContext.fireHandleCommit(baseOffset, epoch, records);\n+            }\n+        }\n+    }\n+\n+    private void maybeFireHandleClaim(LeaderState state) {\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            int leaderEpoch = state.epoch();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDAyNzAxNQ=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 183}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyMzQ2MzIyOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxNzo0ODoxM1rOHqnqrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQyMDoxMjoxOFrOHqs43g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ1MjE0MA==", "bodyText": "It looks a bit weird to have two versions of fire handle commit, could we name them differently or comment about their distinctive logics for determining when to fire commit callback?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514452140", "createdAt": "2020-10-29T17:48:13Z", "author": {"login": "abbccdda"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -228,35 +234,81 @@ private void updateLeaderEndOffsetAndTimestamp(\n         final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n \n         if (state.updateLocalState(currentTimeMs, endOffsetMetadata)) {\n-            updateHighWatermark(state, currentTimeMs);\n+            onUpdateLeaderHighWatermark(state, currentTimeMs);\n         }\n \n-        LogOffset endOffset = new LogOffset(endOffsetMetadata.offset, Isolation.UNCOMMITTED);\n-        fetchPurgatory.maybeComplete(endOffset, currentTimeMs);\n+        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n     }\n \n-    private void updateHighWatermark(\n-        EpochState state,\n+    private void onUpdateLeaderHighWatermark(\n+        LeaderState state,\n         long currentTimeMs\n     ) {\n         state.highWatermark().ifPresent(highWatermark -> {\n-            logger.debug(\"High watermark updated to {}\", highWatermark);\n+            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n             log.updateHighWatermark(highWatermark);\n-\n-            LogOffset offset = new LogOffset(highWatermark.offset, Isolation.COMMITTED);\n-            appendPurgatory.maybeComplete(offset, currentTimeMs);\n-            fetchPurgatory.maybeComplete(offset, currentTimeMs);\n+            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n+            maybeFireHandleCommit(highWatermark.offset);\n         });\n     }\n \n-    @Override\n-    public LeaderAndEpoch currentLeaderAndEpoch() {\n-        return quorum.leaderAndEpoch();\n+    private void maybeFireHandleCommit(long highWatermark) {\n+        maybeFireHandleCommit(listenerContexts, highWatermark);\n+    }\n+\n+    private void maybeFireHandleCommit(List<ListenerContext> listenerContexts, long highWatermark) {\n+        // TODO: When there are multiple listeners, we can cache reads to save some work\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                continue;\n+            }\n+\n+            long nextExpectedOffset = nextExpectedOffsetOpt.getAsLong();\n+            if (nextExpectedOffset < highWatermark) {\n+                LogFetchInfo readInfo = log.read(nextExpectedOffset, Isolation.COMMITTED);\n+                listenerContext.fireHandleCommit(nextExpectedOffset, readInfo.records);\n+            }\n+        }\n+    }\n+\n+    private void maybeFireHandleCommit(long baseOffset, int epoch, List<T> records) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDUzNzY5NA==", "bodyText": "The only difference is the input. I will add some comments to try and clarify the usage.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514537694", "createdAt": "2020-10-29T20:12:18Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -228,35 +234,81 @@ private void updateLeaderEndOffsetAndTimestamp(\n         final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n \n         if (state.updateLocalState(currentTimeMs, endOffsetMetadata)) {\n-            updateHighWatermark(state, currentTimeMs);\n+            onUpdateLeaderHighWatermark(state, currentTimeMs);\n         }\n \n-        LogOffset endOffset = new LogOffset(endOffsetMetadata.offset, Isolation.UNCOMMITTED);\n-        fetchPurgatory.maybeComplete(endOffset, currentTimeMs);\n+        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n     }\n \n-    private void updateHighWatermark(\n-        EpochState state,\n+    private void onUpdateLeaderHighWatermark(\n+        LeaderState state,\n         long currentTimeMs\n     ) {\n         state.highWatermark().ifPresent(highWatermark -> {\n-            logger.debug(\"High watermark updated to {}\", highWatermark);\n+            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n             log.updateHighWatermark(highWatermark);\n-\n-            LogOffset offset = new LogOffset(highWatermark.offset, Isolation.COMMITTED);\n-            appendPurgatory.maybeComplete(offset, currentTimeMs);\n-            fetchPurgatory.maybeComplete(offset, currentTimeMs);\n+            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n+            maybeFireHandleCommit(highWatermark.offset);\n         });\n     }\n \n-    @Override\n-    public LeaderAndEpoch currentLeaderAndEpoch() {\n-        return quorum.leaderAndEpoch();\n+    private void maybeFireHandleCommit(long highWatermark) {\n+        maybeFireHandleCommit(listenerContexts, highWatermark);\n+    }\n+\n+    private void maybeFireHandleCommit(List<ListenerContext> listenerContexts, long highWatermark) {\n+        // TODO: When there are multiple listeners, we can cache reads to save some work\n+        for (ListenerContext listenerContext : listenerContexts) {\n+            OptionalLong nextExpectedOffsetOpt = listenerContext.nextExpectedOffset();\n+            if (!nextExpectedOffsetOpt.isPresent()) {\n+                continue;\n+            }\n+\n+            long nextExpectedOffset = nextExpectedOffsetOpt.getAsLong();\n+            if (nextExpectedOffset < highWatermark) {\n+                LogFetchInfo readInfo = log.read(nextExpectedOffset, Isolation.COMMITTED);\n+                listenerContext.fireHandleCommit(nextExpectedOffset, readInfo.records);\n+            }\n+        }\n+    }\n+\n+    private void maybeFireHandleCommit(long baseOffset, int epoch, List<T> records) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ1MjE0MA=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 167}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyMzQ4OTEzOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxNzo1NDozMFrOHqn7Cw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQyMzozMTozNlrOHri8CQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ1NjMzMQ==", "bodyText": "Why would this work? If the flush wasn't successful, could the fetched records be invalidated later?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514456331", "createdAt": "2020-10-29T17:54:30Z", "author": {"login": "abbccdda"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -329,8 +387,9 @@ private void appendLeaderChangeMessage(LeaderState state, long currentTimeMs) {\n     }\n \n     private void flushLeaderLog(LeaderState state, long currentTimeMs) {\n-        log.flush();\n+        // We update the end offset before flushing so that parked fetches can return sooner", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 227}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDUzOTczNw==", "bodyText": "Yeah, it's ok for followers to see uncommitted or even unflushed data. The main thing is that we avoid advancing the high watermark until the fsync completes. Note that this is the main reason that we had to do KAFKA-10527. Without this fix, it was possible for the leader to continue in the same epoch after a start, which means that it could lose and overwrite unflushed data.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514539737", "createdAt": "2020-10-29T20:16:16Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -329,8 +387,9 @@ private void appendLeaderChangeMessage(LeaderState state, long currentTimeMs) {\n     }\n \n     private void flushLeaderLog(LeaderState state, long currentTimeMs) {\n-        log.flush();\n+        // We update the end offset before flushing so that parked fetches can return sooner", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ1NjMzMQ=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 227}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQyMzI0MQ==", "bodyText": "Yeah. You want to force an epoch change in the case that the old leader stays leader and partially replicated data was lost. This would force followers to truncate to the new leader's log state.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r515423241", "createdAt": "2020-10-30T23:31:36Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -329,8 +387,9 @@ private void appendLeaderChangeMessage(LeaderState state, long currentTimeMs) {\n     }\n \n     private void flushLeaderLog(LeaderState state, long currentTimeMs) {\n-        log.flush();\n+        // We update the end offset before flushing so that parked fetches can return sooner", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ1NjMzMQ=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 227}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyMzUwODkzOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxNzo1OTozMFrOHqoHjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxOToxNDoxMFrOHqq3kQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ1OTUzNA==", "bodyText": "Do we anticipate use cases to add listeners on the fly? Right now I could only see one case in static context from test raft server.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514459534", "createdAt": "2020-10-29T17:59:30Z", "author": {"login": "abbccdda"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1688,14 +1694,39 @@ private long pollCurrentState(long currentTimeMs) throws IOException {\n         }\n     }\n \n+    private void pollListeners() {\n+        // Register any listeners added since the last poll\n+        while (!pendingListeners.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 422}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDUwNDU5Mw==", "bodyText": "I doubt we would use it in practice, though I guess it would open the door to changing roles dynamically, which might be interesting in the future. That said, it was simple to add and useful in testing since it gave me an easy way to initialize a state where a listener had not caught up.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514504593", "createdAt": "2020-10-29T19:14:10Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1688,14 +1694,39 @@ private long pollCurrentState(long currentTimeMs) throws IOException {\n         }\n     }\n \n+    private void pollListeners() {\n+        // Register any listeners added since the last poll\n+        while (!pendingListeners.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ1OTUzNA=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 422}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyMzUzMjc0OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxODowNToxN1rOHqoV_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQwMzoxNjoyN1rOHrr2HA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ2MzIzMQ==", "bodyText": "Should we call pollListeners after pollCurrentState to get more recent updates quicker?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514463231", "createdAt": "2020-10-29T18:05:17Z", "author": {"login": "abbccdda"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1688,14 +1694,39 @@ private long pollCurrentState(long currentTimeMs) throws IOException {\n         }\n     }\n \n+    private void pollListeners() {\n+        // Register any listeners added since the last poll\n+        while (!pendingListeners.isEmpty()) {\n+            Listener<T> listener = pendingListeners.poll();\n+            listenerContexts.add(new ListenerContext(listener));\n+        }\n+\n+        // Check listener progress to see if reads are expected\n+        quorum.highWatermark().ifPresent(highWatermarkMetadata -> {\n+            long highWatermark = highWatermarkMetadata.offset;\n+\n+            List<ListenerContext> listenersToUpdate = listenerContexts.stream()\n+                .filter(listenerContext -> {\n+                    OptionalLong nextExpectedOffset = listenerContext.nextExpectedOffset();\n+                    return nextExpectedOffset.isPresent() && nextExpectedOffset.getAsLong() < highWatermark;\n+                })\n+                .collect(Collectors.toList());\n+\n+            maybeFireHandleCommit(listenersToUpdate, highWatermarkMetadata.offset);\n+        });\n+    }\n+\n     public void poll() throws IOException {\n         GracefulShutdown gracefulShutdown = shutdown.get();\n         if (gracefulShutdown != null) {\n             pollShutdown(gracefulShutdown);\n         } else {\n+            pollListeners();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 447}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU4NDQ2OQ==", "bodyText": "Hmm, that's a fair question. I think the listeners will tend to get new data in two cases: 1) high watermark advanced, or 2) a previous read completes. In the first case, the high watermark only advances in response to a request, so there should be no delay. In the second case, we call wakeup() to take us out of the network poll, so I think there also should be no delay. Can you think of a case where there would be a delay?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514584469", "createdAt": "2020-10-29T21:41:03Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1688,14 +1694,39 @@ private long pollCurrentState(long currentTimeMs) throws IOException {\n         }\n     }\n \n+    private void pollListeners() {\n+        // Register any listeners added since the last poll\n+        while (!pendingListeners.isEmpty()) {\n+            Listener<T> listener = pendingListeners.poll();\n+            listenerContexts.add(new ListenerContext(listener));\n+        }\n+\n+        // Check listener progress to see if reads are expected\n+        quorum.highWatermark().ifPresent(highWatermarkMetadata -> {\n+            long highWatermark = highWatermarkMetadata.offset;\n+\n+            List<ListenerContext> listenersToUpdate = listenerContexts.stream()\n+                .filter(listenerContext -> {\n+                    OptionalLong nextExpectedOffset = listenerContext.nextExpectedOffset();\n+                    return nextExpectedOffset.isPresent() && nextExpectedOffset.getAsLong() < highWatermark;\n+                })\n+                .collect(Collectors.toList());\n+\n+            maybeFireHandleCommit(listenersToUpdate, highWatermarkMetadata.offset);\n+        });\n+    }\n+\n     public void poll() throws IOException {\n         GracefulShutdown gracefulShutdown = shutdown.get();\n         if (gracefulShutdown != null) {\n             pollShutdown(gracefulShutdown);\n         } else {\n+            pollListeners();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ2MzIzMQ=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 447}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQzMzY4Ng==", "bodyText": "That looks correct to me with the clarification that \"in response to a request\" has two cases:\n\nThe leader handles a fetch request. This implementation calls \"update high watermark\nThe follower handle a fetch response. This implementation calls \"update high watermark\"\n\nI think that pollListeners should only fire a Listener::handleCommit for new listeners in pendingListeners.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r515433686", "createdAt": "2020-10-31T00:36:35Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1688,14 +1694,39 @@ private long pollCurrentState(long currentTimeMs) throws IOException {\n         }\n     }\n \n+    private void pollListeners() {\n+        // Register any listeners added since the last poll\n+        while (!pendingListeners.isEmpty()) {\n+            Listener<T> listener = pendingListeners.poll();\n+            listenerContexts.add(new ListenerContext(listener));\n+        }\n+\n+        // Check listener progress to see if reads are expected\n+        quorum.highWatermark().ifPresent(highWatermarkMetadata -> {\n+            long highWatermark = highWatermarkMetadata.offset;\n+\n+            List<ListenerContext> listenersToUpdate = listenerContexts.stream()\n+                .filter(listenerContext -> {\n+                    OptionalLong nextExpectedOffset = listenerContext.nextExpectedOffset();\n+                    return nextExpectedOffset.isPresent() && nextExpectedOffset.getAsLong() < highWatermark;\n+                })\n+                .collect(Collectors.toList());\n+\n+            maybeFireHandleCommit(listenersToUpdate, highWatermarkMetadata.offset);\n+        });\n+    }\n+\n     public void poll() throws IOException {\n         GracefulShutdown gracefulShutdown = shutdown.get();\n         if (gracefulShutdown != null) {\n             pollShutdown(gracefulShutdown);\n         } else {\n+            pollListeners();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ2MzIzMQ=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 447}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTU2OTE4MA==", "bodyText": "Sounds fair.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r515569180", "createdAt": "2020-11-01T03:16:27Z", "author": {"login": "abbccdda"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1688,14 +1694,39 @@ private long pollCurrentState(long currentTimeMs) throws IOException {\n         }\n     }\n \n+    private void pollListeners() {\n+        // Register any listeners added since the last poll\n+        while (!pendingListeners.isEmpty()) {\n+            Listener<T> listener = pendingListeners.poll();\n+            listenerContexts.add(new ListenerContext(listener));\n+        }\n+\n+        // Check listener progress to see if reads are expected\n+        quorum.highWatermark().ifPresent(highWatermarkMetadata -> {\n+            long highWatermark = highWatermarkMetadata.offset;\n+\n+            List<ListenerContext> listenersToUpdate = listenerContexts.stream()\n+                .filter(listenerContext -> {\n+                    OptionalLong nextExpectedOffset = listenerContext.nextExpectedOffset();\n+                    return nextExpectedOffset.isPresent() && nextExpectedOffset.getAsLong() < highWatermark;\n+                })\n+                .collect(Collectors.toList());\n+\n+            maybeFireHandleCommit(listenersToUpdate, highWatermarkMetadata.offset);\n+        });\n+    }\n+\n     public void poll() throws IOException {\n         GracefulShutdown gracefulShutdown = shutdown.get();\n         if (gracefulShutdown != null) {\n             pollShutdown(gracefulShutdown);\n         } else {\n+            pollListeners();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ2MzIzMQ=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 447}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyMzUzNjc2OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxODowNjoxOFrOHqoYjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQyMTo0MzowNVrOHqvzCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ2Mzg4NQ==", "bodyText": "What about just name as handleBecomeLeader?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514463885", "createdAt": "2020-10-29T18:06:18Z", "author": {"login": "abbccdda"}, "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "diffHunk": "@@ -26,30 +24,53 @@\n \n     interface Listener<T> {\n         /**\n-         * Callback which is invoked when records written through {@link #scheduleAppend(int, List)}\n-         * become committed.\n+         * Callback which is invoked for all records committed to the log.\n+         * It is the responsibility of the caller to invoke {@link BatchReader#close()}\n+         * after consuming the reader.\n          *\n          * Note that there is not a one-to-one correspondence between writes through\n          * {@link #scheduleAppend(int, List)} and this callback. The Raft implementation\n          * is free to batch together the records from multiple append calls provided\n          * that batch boundaries are respected. This means that each batch specified\n          * through {@link #scheduleAppend(int, List)} is guaranteed to be a subset of\n-         * a batch passed to {@link #handleCommit(int, long, List)}.\n+         * a batch provided by the {@link BatchReader}.\n+         *\n+         * @param reader reader instance which must be iterated and closed\n+         */\n+        void handleCommit(BatchReader<T> reader);\n+\n+        /**\n+         * Invoked after this node has become a leader. This is only called after\n+         * all commits up to the start of the leader's epoch have been sent to\n+         * {@link #handleCommit(BatchReader)}.\n+         *\n+         * After becoming a leader, the client is eligible to write to the log\n+         * using {@link #scheduleAppend(int, List)}.\n          *\n-         * @param epoch the epoch in which the write was accepted\n-         * @param lastOffset the offset of the last record in the record list\n-         * @param records the set of records that were committed\n+         * @param epoch the claimed leader epoch\n          */\n-        void handleCommit(int epoch, long lastOffset, List<T> records);\n+        default void handleClaim(int epoch) {}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU4NTM1Mw==", "bodyText": "Yeah, I considered using handleBecomeLeader and handleResignLeader. In the end, I decided to use the more concise handleClaim and handleResign names which are used in the kip-500 branch.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514585353", "createdAt": "2020-10-29T21:43:05Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/RaftClient.java", "diffHunk": "@@ -26,30 +24,53 @@\n \n     interface Listener<T> {\n         /**\n-         * Callback which is invoked when records written through {@link #scheduleAppend(int, List)}\n-         * become committed.\n+         * Callback which is invoked for all records committed to the log.\n+         * It is the responsibility of the caller to invoke {@link BatchReader#close()}\n+         * after consuming the reader.\n          *\n          * Note that there is not a one-to-one correspondence between writes through\n          * {@link #scheduleAppend(int, List)} and this callback. The Raft implementation\n          * is free to batch together the records from multiple append calls provided\n          * that batch boundaries are respected. This means that each batch specified\n          * through {@link #scheduleAppend(int, List)} is guaranteed to be a subset of\n-         * a batch passed to {@link #handleCommit(int, long, List)}.\n+         * a batch provided by the {@link BatchReader}.\n+         *\n+         * @param reader reader instance which must be iterated and closed\n+         */\n+        void handleCommit(BatchReader<T> reader);\n+\n+        /**\n+         * Invoked after this node has become a leader. This is only called after\n+         * all commits up to the start of the leader's epoch have been sent to\n+         * {@link #handleCommit(BatchReader)}.\n+         *\n+         * After becoming a leader, the client is eligible to write to the log\n+         * using {@link #scheduleAppend(int, List)}.\n          *\n-         * @param epoch the epoch in which the write was accepted\n-         * @param lastOffset the offset of the last record in the record list\n-         * @param records the set of records that were committed\n+         * @param epoch the claimed leader epoch\n          */\n-        void handleCommit(int epoch, long lastOffset, List<T> records);\n+        default void handleClaim(int epoch) {}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ2Mzg4NQ=="}, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyMzY1Mzc5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/raft/TimingWheelExpirationService.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxODozNjoxOVrOHqphdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxODozNjoxOVrOHqphdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ4MjU1MQ==", "bodyText": "Could we make 200L a constant?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r514482551", "createdAt": "2020-10-29T18:36:19Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/raft/TimingWheelExpirationService.scala", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.raft\n+\n+import java.util.concurrent.CompletableFuture\n+\n+import kafka.raft.TimingWheelExpirationService.TimerTaskCompletableFuture\n+import kafka.utils.ShutdownableThread\n+import kafka.utils.timer.{Timer, TimerTask}\n+import org.apache.kafka.common.errors.TimeoutException\n+import org.apache.kafka.raft.ExpirationService\n+\n+object TimingWheelExpirationService {\n+  class TimerTaskCompletableFuture[T](override val delayMs: Long) extends CompletableFuture[T] with TimerTask {\n+    override def run(): Unit = {\n+      completeExceptionally(new TimeoutException(\n+        s\"Future failed to be completed before timeout of $delayMs ms was reached\"))\n+    }\n+  }\n+}\n+\n+class TimingWheelExpirationService(timer: Timer) extends ExpirationService {\n+  private val expirationReaper = new ExpiredOperationReaper()\n+\n+  expirationReaper.start()\n+\n+  override def await[T](timeoutMs: Long): CompletableFuture[T] = {\n+    val future = new TimerTaskCompletableFuture[T](timeoutMs)\n+    future.whenComplete { (_, _) =>\n+      future.cancel()\n+    }\n+    timer.add(future)\n+    future\n+  }\n+\n+  private class ExpiredOperationReaper extends ShutdownableThread(\n+    name = \"raft-expiration-reaper\", isInterruptible = false) {\n+\n+    override def doWork(): Unit = {\n+      timer.advanceClock(200L)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1345f566ff9265c4072f2129bf6c477946c4407c"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTQ1MDg3OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/BatchReader.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQyMzowMTo0NVrOHrij8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMDowMTo1MlrOHsTuHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxNzA3Mg==", "bodyText": "How about extending java.lang.Iterable<...> instead of Iterator? This will allow the user to use BatchReader::forEach.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r515417072", "createdAt": "2020-10-30T23:01:45Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/BatchReader.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+import java.io.Closeable;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+\n+/**\n+ * This interface is used to send committed data from the {@link RaftClient}\n+ * down to registered {@link RaftClient.Listener} instances.\n+ *\n+ * The advantage of hiding the consumption of committed batches behind an interface\n+ * is that it allows us to push blocking operations such as reads from disk outside\n+ * of the Raft IO thread. This helps to ensure that a slow state machine will not\n+ * affect replication.\n+ *\n+ * @param <T> record type (see {@link org.apache.kafka.raft.RecordSerde})\n+ */\n+public interface BatchReader<T> extends Iterator<BatchReader.Batch<T>>, Closeable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE4NzAxOQ==", "bodyText": "I was thinking about this and having some trouble putting my hesitation into words. I guess there are two main reasons why I preferred the Iterator:\n\nWe need some way to communicate iteration progress back to the IO thread. It is probably still possible to do this with a layer of indirection through Iterable, but it seemed more natural if the IO thread had access to the Iterator object that was used by the listener.\nIteration is not necessarily cheap since it might involve reading from disk. I thought we may as well enforce a single-read pattern.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r516187019", "createdAt": "2020-11-02T18:52:25Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/BatchReader.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+import java.io.Closeable;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+\n+/**\n+ * This interface is used to send committed data from the {@link RaftClient}\n+ * down to registered {@link RaftClient.Listener} instances.\n+ *\n+ * The advantage of hiding the consumption of committed batches behind an interface\n+ * is that it allows us to push blocking operations such as reads from disk outside\n+ * of the Raft IO thread. This helps to ensure that a slow state machine will not\n+ * affect replication.\n+ *\n+ * @param <T> record type (see {@link org.apache.kafka.raft.RecordSerde})\n+ */\n+public interface BatchReader<T> extends Iterator<BatchReader.Batch<T>>, Closeable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxNzA3Mg=="}, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE5NjI0Ng==", "bodyText": "We need some way to communicate iteration progress back to the IO thread. It is probably still possible to do this with a layer of indirection through Iterable, but it seemed more natural if the IO thread had access to the Iterator object that was used by the listener.\n\nOkay. Specifically, you are saying that this would be difficult to implement with an Iterable:\nlastReturnedOffset = res.lastOffset();\nhttps://github.com/apache/kafka/pull/9482/files/867650fa2344497ac3f3505bd5058f2eae0cc0c4#diff-37728c07e52382d38a9db6f655c3921d274c4277a8909d7613fc433d6bf69636R140", "url": "https://github.com/apache/kafka/pull/9482#discussion_r516196246", "createdAt": "2020-11-02T19:10:26Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/BatchReader.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+import java.io.Closeable;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+\n+/**\n+ * This interface is used to send committed data from the {@link RaftClient}\n+ * down to registered {@link RaftClient.Listener} instances.\n+ *\n+ * The advantage of hiding the consumption of committed batches behind an interface\n+ * is that it allows us to push blocking operations such as reads from disk outside\n+ * of the Raft IO thread. This helps to ensure that a slow state machine will not\n+ * affect replication.\n+ *\n+ * @param <T> record type (see {@link org.apache.kafka.raft.RecordSerde})\n+ */\n+public interface BatchReader<T> extends Iterator<BatchReader.Batch<T>>, Closeable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxNzA3Mg=="}, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjIyMjQ5NQ==", "bodyText": "Yeah. Not so much difficult, but awkward because of the one-to-many association between the Iterable and Iterator.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r516222495", "createdAt": "2020-11-02T20:01:52Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/BatchReader.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+import java.io.Closeable;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+\n+/**\n+ * This interface is used to send committed data from the {@link RaftClient}\n+ * down to registered {@link RaftClient.Listener} instances.\n+ *\n+ * The advantage of hiding the consumption of committed batches behind an interface\n+ * is that it allows us to push blocking operations such as reads from disk outside\n+ * of the Raft IO thread. This helps to ensure that a slow state machine will not\n+ * affect replication.\n+ *\n+ * @param <T> record type (see {@link org.apache.kafka.raft.RecordSerde})\n+ */\n+public interface BatchReader<T> extends Iterator<BatchReader.Batch<T>>, Closeable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxNzA3Mg=="}, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTQ1NDE3OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/BatchReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQyMzowNDowN1rOHril4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQyMzowNDowN1rOHril4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQxNzU3MA==", "bodyText": "If we are overriding close to not throw an IOException, then maybe extending java.lang.AutoCloseable instead of java.io.Closeable is better.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r515417570", "createdAt": "2020-10-30T23:04:07Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/BatchReader.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.raft;\n+\n+import java.io.Closeable;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+\n+/**\n+ * This interface is used to send committed data from the {@link RaftClient}\n+ * down to registered {@link RaftClient.Listener} instances.\n+ *\n+ * The advantage of hiding the consumption of committed batches behind an interface\n+ * is that it allows us to push blocking operations such as reads from disk outside\n+ * of the Raft IO thread. This helps to ensure that a slow state machine will not\n+ * affect replication.\n+ *\n+ * @param <T> record type (see {@link org.apache.kafka.raft.RecordSerde})\n+ */\n+public interface BatchReader<T> extends Iterator<BatchReader.Batch<T>>, Closeable {\n+\n+    /**\n+     * Get the base offset of the readable batches. Note that this value is a constant\n+     * which is defined when the {@link BatchReader} instance is constructed. It does\n+     * not change based on reader progress.\n+     *\n+     * @return the base offset\n+     */\n+    long baseOffset();\n+\n+    /**\n+     * Get the last offset of the batch if it is known. When reading from disk, we may\n+     * not know the last offset of a set of records until it has been read from disk.\n+     * In this case, the state machine cannot advance to the next committed data until\n+     * all batches from the {@link BatchReader} instance have been consumed.\n+     *\n+     * @return optional last offset\n+     */\n+    OptionalLong lastOffset();\n+\n+    /**\n+     * Close this reader. It is the responsibility of the {@link RaftClient.Listener}\n+     * to close each reader passed to {@link RaftClient.Listener#handleCommit(BatchReader)}.\n+     */\n+    @Override\n+    void close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTUxOTA4OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQyMzo1MjoxNVrOHrjKyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMzo0NTowMlrOHsay4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQyNzAxOQ==", "bodyText": "With the new Listener when is this not a noop? Looking at the code, we only add entries to fetchPurgatorywhen the replica is a leader and it receives a Fetch request.\nIn previous read based implementation we needed this because I think the fetchPurgatory contained both reads and Fetch.\nI think the part that is missing is that the old leader should fetchPurgatory.completeAll when it loses leadership.\nWhat do you think?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r515427019", "createdAt": "2020-10-30T23:52:15Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1017,12 +1028,9 @@ private boolean handleFetchResponse(\n                 log.truncateToEndOffset(divergingOffsetAndEpoch).ifPresent(truncationOffset -> {\n                     logger.info(\"Truncated to offset {} from Fetch response from leader {}\",\n                         truncationOffset, quorum.leaderIdOrNil());\n-\n-                    // Since the end offset has been updated, we should complete any delayed\n-                    // reads at the end offset.\n-                    fetchPurgatory.maybeComplete(\n-                        new LogOffset(Long.MAX_VALUE, Isolation.UNCOMMITTED),\n-                        currentTimeMs);\n+                    // After truncation, we complete all pending reads in order to\n+                    // ensure that fetches account for the updated log end offset\n+                    fetchPurgatory.completeAll(currentTimeMs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 368}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE1MTY0NQ==", "bodyText": "With the new Listener when is this not a noop? Looking at the code, we only add entries to fetchPurgatorywhen the replica is a leader and it receives a Fetch request.\n\nYeah, that's fair. I don't think we can truncate unless we are a follower and that implies we already cleared the purgatory in onBecomeFollower. So I think you are right that we are safe to remove this, though we'll probably need to add it back once we have follower fetching.\n\nI think the part that is missing is that the old leader should fetchPurgatory.completeAll when it loses leadership.\n\nI had considered this previously and decided to leave the fetches in purgatory while the election was in progress to prevent unnecessary retries since that is all the client can do while waiting for the outcome. On the other hand, some of the fetches in purgatory might be from other voters. It might be better to respond more quickly so that there are not any unnecessary election delays. I'd suggest we open a separate issue to consider this.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r516151645", "createdAt": "2020-11-02T17:48:23Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1017,12 +1028,9 @@ private boolean handleFetchResponse(\n                 log.truncateToEndOffset(divergingOffsetAndEpoch).ifPresent(truncationOffset -> {\n                     logger.info(\"Truncated to offset {} from Fetch response from leader {}\",\n                         truncationOffset, quorum.leaderIdOrNil());\n-\n-                    // Since the end offset has been updated, we should complete any delayed\n-                    // reads at the end offset.\n-                    fetchPurgatory.maybeComplete(\n-                        new LogOffset(Long.MAX_VALUE, Isolation.UNCOMMITTED),\n-                        currentTimeMs);\n+                    // After truncation, we complete all pending reads in order to\n+                    // ensure that fetches account for the updated log end offset\n+                    fetchPurgatory.completeAll(currentTimeMs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQyNzAxOQ=="}, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 368}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE3MjU3MQ==", "bodyText": "I had considered this previously and decided to leave the fetches in purgatory while the election was in progress to prevent unnecessary retries since that is all the client can do while waiting for the outcome. On the other hand, some of the fetches in purgatory might be from other voters. It might be better to respond more quickly so that there are not any unnecessary election delays. I'd suggest we open a separate issue to consider this.\n\nSounds good to create a Jira for this.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r516172571", "createdAt": "2020-11-02T18:25:26Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1017,12 +1028,9 @@ private boolean handleFetchResponse(\n                 log.truncateToEndOffset(divergingOffsetAndEpoch).ifPresent(truncationOffset -> {\n                     logger.info(\"Truncated to offset {} from Fetch response from leader {}\",\n                         truncationOffset, quorum.leaderIdOrNil());\n-\n-                    // Since the end offset has been updated, we should complete any delayed\n-                    // reads at the end offset.\n-                    fetchPurgatory.maybeComplete(\n-                        new LogOffset(Long.MAX_VALUE, Isolation.UNCOMMITTED),\n-                        currentTimeMs);\n+                    // After truncation, we complete all pending reads in order to\n+                    // ensure that fetches account for the updated log end offset\n+                    fetchPurgatory.completeAll(currentTimeMs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQyNzAxOQ=="}, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 368}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjMzODQwMQ==", "bodyText": "Filed https://issues.apache.org/jira/browse/KAFKA-10677.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r516338401", "createdAt": "2020-11-02T23:45:02Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1017,12 +1028,9 @@ private boolean handleFetchResponse(\n                 log.truncateToEndOffset(divergingOffsetAndEpoch).ifPresent(truncationOffset -> {\n                     logger.info(\"Truncated to offset {} from Fetch response from leader {}\",\n                         truncationOffset, quorum.leaderIdOrNil());\n-\n-                    // Since the end offset has been updated, we should complete any delayed\n-                    // reads at the end offset.\n-                    fetchPurgatory.maybeComplete(\n-                        new LogOffset(Long.MAX_VALUE, Isolation.UNCOMMITTED),\n-                        currentTimeMs);\n+                    // After truncation, we complete all pending reads in order to\n+                    // ensure that fetches account for the updated log end offset\n+                    fetchPurgatory.completeAll(currentTimeMs);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQyNzAxOQ=="}, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 368}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTU0MDA0OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwMDoxMDo1MFrOHrjWjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNzo1MTo1MFrOHsPhlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQzMDAyOQ==", "bodyText": "Is this needed because users of KafkaRaftClient can call ::register before ::initizalize? When else would this result on a call to Listener::handleCommit?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r515430029", "createdAt": "2020-10-31T00:10:50Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -228,35 +234,80 @@ private void updateLeaderEndOffsetAndTimestamp(\n         final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n \n         if (state.updateLocalState(currentTimeMs, endOffsetMetadata)) {\n-            updateHighWatermark(state, currentTimeMs);\n+            onUpdateLeaderHighWatermark(state, currentTimeMs);\n         }\n \n-        LogOffset endOffset = new LogOffset(endOffsetMetadata.offset, Isolation.UNCOMMITTED);\n-        fetchPurgatory.maybeComplete(endOffset, currentTimeMs);\n+        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n     }\n \n-    private void updateHighWatermark(\n-        EpochState state,\n+    private void onUpdateLeaderHighWatermark(\n+        LeaderState state,\n         long currentTimeMs\n     ) {\n         state.highWatermark().ifPresent(highWatermark -> {\n-            logger.debug(\"High watermark updated to {}\", highWatermark);\n+            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n             log.updateHighWatermark(highWatermark);\n-\n-            LogOffset offset = new LogOffset(highWatermark.offset, Isolation.COMMITTED);\n-            appendPurgatory.maybeComplete(offset, currentTimeMs);\n-            fetchPurgatory.maybeComplete(offset, currentTimeMs);\n+            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n+            maybeFireHandleCommit(highWatermark.offset);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 140}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQzMTMyOQ==", "bodyText": "Never mind. I think this can happens when the replica changes state from follower to leader.\nI was having an issue if doing this would cause both appendPurgatory.maybeComplete and maybeFireHandleCommit to fire Listener.handleCommit for the same listener.\nI don't think this can happened based on how ListenerContext is managing the nextExpectedOffset. If appendPurgator.maybeComplete fires then that means that nextExpectedOffset is greater that the high watermark. Since the nextExpectedOffset is greater than the high watermark then maybeFireHandleCommit will not fire.\nI actually think that this order is important. Should we write a comment on the code explaining this if you agree with my analysis?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r515431329", "createdAt": "2020-10-31T00:19:44Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -228,35 +234,80 @@ private void updateLeaderEndOffsetAndTimestamp(\n         final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n \n         if (state.updateLocalState(currentTimeMs, endOffsetMetadata)) {\n-            updateHighWatermark(state, currentTimeMs);\n+            onUpdateLeaderHighWatermark(state, currentTimeMs);\n         }\n \n-        LogOffset endOffset = new LogOffset(endOffsetMetadata.offset, Isolation.UNCOMMITTED);\n-        fetchPurgatory.maybeComplete(endOffset, currentTimeMs);\n+        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n     }\n \n-    private void updateHighWatermark(\n-        EpochState state,\n+    private void onUpdateLeaderHighWatermark(\n+        LeaderState state,\n         long currentTimeMs\n     ) {\n         state.highWatermark().ifPresent(highWatermark -> {\n-            logger.debug(\"High watermark updated to {}\", highWatermark);\n+            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n             log.updateHighWatermark(highWatermark);\n-\n-            LogOffset offset = new LogOffset(highWatermark.offset, Isolation.COMMITTED);\n-            appendPurgatory.maybeComplete(offset, currentTimeMs);\n-            fetchPurgatory.maybeComplete(offset, currentTimeMs);\n+            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n+            maybeFireHandleCommit(highWatermark.offset);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQzMDAyOQ=="}, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 140}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE1Mzc0OQ==", "bodyText": "I will add a comment. I agree it is a subtle point.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r516153749", "createdAt": "2020-11-02T17:51:50Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -228,35 +234,80 @@ private void updateLeaderEndOffsetAndTimestamp(\n         final LogOffsetMetadata endOffsetMetadata = log.endOffset();\n \n         if (state.updateLocalState(currentTimeMs, endOffsetMetadata)) {\n-            updateHighWatermark(state, currentTimeMs);\n+            onUpdateLeaderHighWatermark(state, currentTimeMs);\n         }\n \n-        LogOffset endOffset = new LogOffset(endOffsetMetadata.offset, Isolation.UNCOMMITTED);\n-        fetchPurgatory.maybeComplete(endOffset, currentTimeMs);\n+        fetchPurgatory.maybeComplete(endOffsetMetadata.offset, currentTimeMs);\n     }\n \n-    private void updateHighWatermark(\n-        EpochState state,\n+    private void onUpdateLeaderHighWatermark(\n+        LeaderState state,\n         long currentTimeMs\n     ) {\n         state.highWatermark().ifPresent(highWatermark -> {\n-            logger.debug(\"High watermark updated to {}\", highWatermark);\n+            logger.debug(\"Leader high watermark updated to {}\", highWatermark);\n             log.updateHighWatermark(highWatermark);\n-\n-            LogOffset offset = new LogOffset(highWatermark.offset, Isolation.COMMITTED);\n-            appendPurgatory.maybeComplete(offset, currentTimeMs);\n-            fetchPurgatory.maybeComplete(offset, currentTimeMs);\n+            appendPurgatory.maybeComplete(highWatermark.offset, currentTimeMs);\n+            maybeFireHandleCommit(highWatermark.offset);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQzMDAyOQ=="}, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 140}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTU3NDIzOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwMDo0NDoxM1rOHrjosg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwMDo0NDoxM1rOHrjosg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQzNDY3NA==", "bodyText": "I would document that synchronized is protecting lastSent and lastAckedOffset.\nclaimedEpoch is okay because it is only used by the thread calling RaftClient::poll.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r515434674", "createdAt": "2020-10-31T00:44:13Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1778,4 +1808,98 @@ public void complete() {\n         }\n     }\n \n+    private final class ListenerContext implements CloseListener<BatchReader<T>> {\n+        private final RaftClient.Listener<T> listener;\n+        private BatchReader<T> lastSent = null;\n+        private long lastAckedOffset = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 462}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTU3ODM0OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwMDo0ODoxNlrOHrjquA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxODowMjoxMFrOHsP5XQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQzNTE5Mg==", "bodyText": "Interesting that Java let's the outer class (KafkaRaftClient) access this private method. This makes reasoning about the concurrency non-trivial.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r515435192", "createdAt": "2020-10-31T00:48:16Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1778,4 +1808,98 @@ public void complete() {\n         }\n     }\n \n+    private final class ListenerContext implements CloseListener<BatchReader<T>> {\n+        private final RaftClient.Listener<T> listener;\n+        private BatchReader<T> lastSent = null;\n+        private long lastAckedOffset = 0;\n+        private int claimedEpoch = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 463}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE1OTgzNw==", "bodyText": "Let me add a helper to ListenerContext so that we can keep the field encapsulated.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r516159837", "createdAt": "2020-11-02T18:02:10Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1778,4 +1808,98 @@ public void complete() {\n         }\n     }\n \n+    private final class ListenerContext implements CloseListener<BatchReader<T>> {\n+        private final RaftClient.Listener<T> listener;\n+        private BatchReader<T> lastSent = null;\n+        private long lastAckedOffset = 0;\n+        private int claimedEpoch = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQzNTE5Mg=="}, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 463}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTU5MTk2OnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/ExpirationService.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwMTowMzo0MFrOHrjxrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwMTowMzo0MFrOHrjxrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQzNjk3Mg==", "bodyText": "How about failAfter?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r515436972", "createdAt": "2020-10-31T01:03:40Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/ExpirationService.java", "diffHunk": "@@ -16,11 +16,17 @@\n  */\n package org.apache.kafka.raft;\n \n-import org.apache.kafka.common.KafkaException;\n+import java.util.concurrent.CompletableFuture;\n \n-public class LogTruncationException extends KafkaException {\n-\n-    public LogTruncationException(String message) {\n-        super(message);\n-    }\n+public interface ExpirationService {\n+    /**\n+     * Get a new completable future which will automatically fail exceptionally with a\n+     * {@link org.apache.kafka.common.errors.TimeoutException} if not completed before\n+     * the provided time limit expires.\n+     *\n+     * @param timeoutMs the duration in milliseconds before the future is completed exceptionally\n+     * @param <T> arbitrary future type (the service must set no expectation on the this type)\n+     * @return the completable future\n+     */\n+    <T> CompletableFuture<T> await(long timeoutMs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyOTYwNzMzOnYy", "diffSide": "RIGHT", "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwMToyMTozOFrOHrj5JA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQyMzo0NToyNlrOHsazwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQzODg4NA==", "bodyText": "This applies to all of the listener.handle... on this file.\nWhat are your thoughts on the Listener throwing an exception? I think with this implementation it will unwind all the way until KafakRaftClient::poll. Should we catch all non-fatal exceptions here and log an error instead?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r515438884", "createdAt": "2020-10-31T01:21:38Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1778,4 +1808,98 @@ public void complete() {\n         }\n     }\n \n+    private final class ListenerContext implements CloseListener<BatchReader<T>> {\n+        private final RaftClient.Listener<T> listener;\n+        private BatchReader<T> lastSent = null;\n+        private long lastAckedOffset = 0;\n+        private int claimedEpoch = 0;\n+\n+        private ListenerContext(Listener<T> listener) {\n+            this.listener = listener;\n+        }\n+\n+        /**\n+         * Get the last acked offset, which is one greater than the offset of the\n+         * last record which was acked by the state machine.\n+         */\n+        public synchronized long lastAckedOffset() {\n+            return lastAckedOffset;\n+        }\n+\n+        /**\n+         * Get the next expected offset, which might be larger than the last acked\n+         * offset if there are inflight batches which have not been acked yet.\n+         * Note that when fetching from disk, we may not know the last offset of\n+         * inflight data until it has been processed by the state machine. In this case,\n+         * we delay sending additional data until the state machine has read to the\n+         * end and the last offset is determined.\n+         */\n+        public synchronized OptionalLong nextExpectedOffset() {\n+            if (lastSent != null) {\n+                OptionalLong lastSentOffset = lastSent.lastOffset();\n+                if (lastSentOffset.isPresent()) {\n+                    return OptionalLong.of(lastSentOffset.getAsLong() + 1);\n+                } else {\n+                    return OptionalLong.empty();\n+                }\n+            } else {\n+                return OptionalLong.of(lastAckedOffset);\n+            }\n+        }\n+\n+        /**\n+         * This API is used for committed records that have been received through\n+         * replication. In general, followers will write new data to disk before they\n+         * know whether it has been committed. Rather than retaining the uncommitted\n+         * data in memory, we let the state machine read the records from disk.\n+         */\n+        public void fireHandleCommit(long baseOffset, Records records) {\n+            BufferSupplier bufferSupplier = BufferSupplier.create();\n+            RecordsBatchReader<T> reader = new RecordsBatchReader<>(baseOffset, records,\n+                serde, bufferSupplier, this);\n+            fireHandleCommit(reader);\n+        }\n+\n+        /**\n+         * This API is used for committed records originating from {@link #scheduleAppend(int, List)}\n+         * on this instance. In this case, we are able to save the original record objects,\n+         * which saves the need to read them back from disk. This is a nice optimization\n+         * for the leader which is typically doing more work than all of the followers.\n+         */\n+        public void fireHandleCommit(long baseOffset, int epoch, List<T> records) {\n+            BatchReader.Batch<T> batch = new BatchReader.Batch<>(baseOffset, epoch, records);\n+            MemoryBatchReader<T> reader = new MemoryBatchReader<>(Collections.singletonList(batch), this);\n+            fireHandleCommit(reader);\n+        }\n+\n+        private void fireHandleCommit(BatchReader<T> reader) {\n+            synchronized (this) {\n+                this.lastSent = reader;\n+            }\n+            listener.handleCommit(reader);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 527}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE2NzgwNw==", "bodyText": "Hmm... That's a good question. I guess the issue is that the listener will then be in an unknown state. Should the IO thread keep sending it updates or should it mark it as failed? This comes back to something I have been wondering in the KIP-500 world. Do we want the process to stay active if either the controller or broker listeners have failed or would it be better to shutdown? At the moment, I am leaning toward the latter. In any case, I suggest we let the errors propagate for now and file a jira to reconsider once we are closer to integration. Does that sound fair?", "url": "https://github.com/apache/kafka/pull/9482#discussion_r516167807", "createdAt": "2020-11-02T18:16:45Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1778,4 +1808,98 @@ public void complete() {\n         }\n     }\n \n+    private final class ListenerContext implements CloseListener<BatchReader<T>> {\n+        private final RaftClient.Listener<T> listener;\n+        private BatchReader<T> lastSent = null;\n+        private long lastAckedOffset = 0;\n+        private int claimedEpoch = 0;\n+\n+        private ListenerContext(Listener<T> listener) {\n+            this.listener = listener;\n+        }\n+\n+        /**\n+         * Get the last acked offset, which is one greater than the offset of the\n+         * last record which was acked by the state machine.\n+         */\n+        public synchronized long lastAckedOffset() {\n+            return lastAckedOffset;\n+        }\n+\n+        /**\n+         * Get the next expected offset, which might be larger than the last acked\n+         * offset if there are inflight batches which have not been acked yet.\n+         * Note that when fetching from disk, we may not know the last offset of\n+         * inflight data until it has been processed by the state machine. In this case,\n+         * we delay sending additional data until the state machine has read to the\n+         * end and the last offset is determined.\n+         */\n+        public synchronized OptionalLong nextExpectedOffset() {\n+            if (lastSent != null) {\n+                OptionalLong lastSentOffset = lastSent.lastOffset();\n+                if (lastSentOffset.isPresent()) {\n+                    return OptionalLong.of(lastSentOffset.getAsLong() + 1);\n+                } else {\n+                    return OptionalLong.empty();\n+                }\n+            } else {\n+                return OptionalLong.of(lastAckedOffset);\n+            }\n+        }\n+\n+        /**\n+         * This API is used for committed records that have been received through\n+         * replication. In general, followers will write new data to disk before they\n+         * know whether it has been committed. Rather than retaining the uncommitted\n+         * data in memory, we let the state machine read the records from disk.\n+         */\n+        public void fireHandleCommit(long baseOffset, Records records) {\n+            BufferSupplier bufferSupplier = BufferSupplier.create();\n+            RecordsBatchReader<T> reader = new RecordsBatchReader<>(baseOffset, records,\n+                serde, bufferSupplier, this);\n+            fireHandleCommit(reader);\n+        }\n+\n+        /**\n+         * This API is used for committed records originating from {@link #scheduleAppend(int, List)}\n+         * on this instance. In this case, we are able to save the original record objects,\n+         * which saves the need to read them back from disk. This is a nice optimization\n+         * for the leader which is typically doing more work than all of the followers.\n+         */\n+        public void fireHandleCommit(long baseOffset, int epoch, List<T> records) {\n+            BatchReader.Batch<T> batch = new BatchReader.Batch<>(baseOffset, epoch, records);\n+            MemoryBatchReader<T> reader = new MemoryBatchReader<>(Collections.singletonList(batch), this);\n+            fireHandleCommit(reader);\n+        }\n+\n+        private void fireHandleCommit(BatchReader<T> reader) {\n+            synchronized (this) {\n+                this.lastSent = reader;\n+            }\n+            listener.handleCommit(reader);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQzODg4NA=="}, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 527}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE3NjIwNQ==", "bodyText": "At the moment, I am leaning toward the latter. In any case, I suggest we let the errors propagate for now and file a jira to reconsider once we are closer to integration. Does that sound fair?\n\nSounds fair to create a Jira and revisit this later.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r516176205", "createdAt": "2020-11-02T18:31:50Z", "author": {"login": "jsancio"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1778,4 +1808,98 @@ public void complete() {\n         }\n     }\n \n+    private final class ListenerContext implements CloseListener<BatchReader<T>> {\n+        private final RaftClient.Listener<T> listener;\n+        private BatchReader<T> lastSent = null;\n+        private long lastAckedOffset = 0;\n+        private int claimedEpoch = 0;\n+\n+        private ListenerContext(Listener<T> listener) {\n+            this.listener = listener;\n+        }\n+\n+        /**\n+         * Get the last acked offset, which is one greater than the offset of the\n+         * last record which was acked by the state machine.\n+         */\n+        public synchronized long lastAckedOffset() {\n+            return lastAckedOffset;\n+        }\n+\n+        /**\n+         * Get the next expected offset, which might be larger than the last acked\n+         * offset if there are inflight batches which have not been acked yet.\n+         * Note that when fetching from disk, we may not know the last offset of\n+         * inflight data until it has been processed by the state machine. In this case,\n+         * we delay sending additional data until the state machine has read to the\n+         * end and the last offset is determined.\n+         */\n+        public synchronized OptionalLong nextExpectedOffset() {\n+            if (lastSent != null) {\n+                OptionalLong lastSentOffset = lastSent.lastOffset();\n+                if (lastSentOffset.isPresent()) {\n+                    return OptionalLong.of(lastSentOffset.getAsLong() + 1);\n+                } else {\n+                    return OptionalLong.empty();\n+                }\n+            } else {\n+                return OptionalLong.of(lastAckedOffset);\n+            }\n+        }\n+\n+        /**\n+         * This API is used for committed records that have been received through\n+         * replication. In general, followers will write new data to disk before they\n+         * know whether it has been committed. Rather than retaining the uncommitted\n+         * data in memory, we let the state machine read the records from disk.\n+         */\n+        public void fireHandleCommit(long baseOffset, Records records) {\n+            BufferSupplier bufferSupplier = BufferSupplier.create();\n+            RecordsBatchReader<T> reader = new RecordsBatchReader<>(baseOffset, records,\n+                serde, bufferSupplier, this);\n+            fireHandleCommit(reader);\n+        }\n+\n+        /**\n+         * This API is used for committed records originating from {@link #scheduleAppend(int, List)}\n+         * on this instance. In this case, we are able to save the original record objects,\n+         * which saves the need to read them back from disk. This is a nice optimization\n+         * for the leader which is typically doing more work than all of the followers.\n+         */\n+        public void fireHandleCommit(long baseOffset, int epoch, List<T> records) {\n+            BatchReader.Batch<T> batch = new BatchReader.Batch<>(baseOffset, epoch, records);\n+            MemoryBatchReader<T> reader = new MemoryBatchReader<>(Collections.singletonList(batch), this);\n+            fireHandleCommit(reader);\n+        }\n+\n+        private void fireHandleCommit(BatchReader<T> reader) {\n+            synchronized (this) {\n+                this.lastSent = reader;\n+            }\n+            listener.handleCommit(reader);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQzODg4NA=="}, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 527}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjMzODYyNQ==", "bodyText": "Filed https://issues.apache.org/jira/browse/KAFKA-10676.", "url": "https://github.com/apache/kafka/pull/9482#discussion_r516338625", "createdAt": "2020-11-02T23:45:26Z", "author": {"login": "hachikuji"}, "path": "raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java", "diffHunk": "@@ -1778,4 +1808,98 @@ public void complete() {\n         }\n     }\n \n+    private final class ListenerContext implements CloseListener<BatchReader<T>> {\n+        private final RaftClient.Listener<T> listener;\n+        private BatchReader<T> lastSent = null;\n+        private long lastAckedOffset = 0;\n+        private int claimedEpoch = 0;\n+\n+        private ListenerContext(Listener<T> listener) {\n+            this.listener = listener;\n+        }\n+\n+        /**\n+         * Get the last acked offset, which is one greater than the offset of the\n+         * last record which was acked by the state machine.\n+         */\n+        public synchronized long lastAckedOffset() {\n+            return lastAckedOffset;\n+        }\n+\n+        /**\n+         * Get the next expected offset, which might be larger than the last acked\n+         * offset if there are inflight batches which have not been acked yet.\n+         * Note that when fetching from disk, we may not know the last offset of\n+         * inflight data until it has been processed by the state machine. In this case,\n+         * we delay sending additional data until the state machine has read to the\n+         * end and the last offset is determined.\n+         */\n+        public synchronized OptionalLong nextExpectedOffset() {\n+            if (lastSent != null) {\n+                OptionalLong lastSentOffset = lastSent.lastOffset();\n+                if (lastSentOffset.isPresent()) {\n+                    return OptionalLong.of(lastSentOffset.getAsLong() + 1);\n+                } else {\n+                    return OptionalLong.empty();\n+                }\n+            } else {\n+                return OptionalLong.of(lastAckedOffset);\n+            }\n+        }\n+\n+        /**\n+         * This API is used for committed records that have been received through\n+         * replication. In general, followers will write new data to disk before they\n+         * know whether it has been committed. Rather than retaining the uncommitted\n+         * data in memory, we let the state machine read the records from disk.\n+         */\n+        public void fireHandleCommit(long baseOffset, Records records) {\n+            BufferSupplier bufferSupplier = BufferSupplier.create();\n+            RecordsBatchReader<T> reader = new RecordsBatchReader<>(baseOffset, records,\n+                serde, bufferSupplier, this);\n+            fireHandleCommit(reader);\n+        }\n+\n+        /**\n+         * This API is used for committed records originating from {@link #scheduleAppend(int, List)}\n+         * on this instance. In this case, we are able to save the original record objects,\n+         * which saves the need to read them back from disk. This is a nice optimization\n+         * for the leader which is typically doing more work than all of the followers.\n+         */\n+        public void fireHandleCommit(long baseOffset, int epoch, List<T> records) {\n+            BatchReader.Batch<T> batch = new BatchReader.Batch<>(baseOffset, epoch, records);\n+            MemoryBatchReader<T> reader = new MemoryBatchReader<>(Collections.singletonList(batch), this);\n+            fireHandleCommit(reader);\n+        }\n+\n+        private void fireHandleCommit(BatchReader<T> reader) {\n+            synchronized (this) {\n+                this.lastSent = reader;\n+            }\n+            listener.handleCommit(reader);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQzODg4NA=="}, "originalCommit": {"oid": "867650fa2344497ac3f3505bd5058f2eae0cc0c4"}, "originalPosition": 527}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3928, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}