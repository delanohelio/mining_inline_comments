{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk2NzgxMTgw", "number": 9364, "title": "KAFKA-10471 Mark broker crash during log loading as unclean shutdown", "bodyText": "LogManager writes a clean shutdown file when the broker shuts down. The\npresence of this file indicates that the broker had a clean shutdown and\nlog recovery is not needed upon the next boot up.\nEarlier, LogManager would check for this file at the start of log loading workflow,\nand delete it after the log has been loaded. If the broker were to crash\nwhile loading logs, the file would not be deleted and mislead LogManager when it\ntries to load logs upon next boot up. Hence, a crash during log loading\nwill not be considered a hard reset of broker.\nAs part of this fix, we delete the clean shutdown file as soon as we\nlook it up, at the start of log loading workflow. Thereafter, we maintain a boolean\nflag to indicate if broker underwent clean shutdown or not. So, if the\nbroker were to crash while logs are being loaded, LogManager will be\nable to detect this as a hard reset.\nMore detailed description of your change,\nif necessary. The PR title and PR message become\nthe squashed commit message, so use a separate\ncomment to ping reviewers.\nSummary of testing strategy (including rationale)\nfor the feature or bug fix. Unit and/or integration\ntests are expected for any behaviour change and\nsystem tests should be considered for larger changes.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-10-02T09:05:28Z", "url": "https://github.com/apache/kafka/pull/9364", "merged": true, "mergeCommit": {"oid": "be4c45286945ea91d703660e86811bd24f561aaa"}, "closed": true, "closedAt": "2020-11-02T21:07:15Z", "author": {"login": "RamanVerma"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdOtLdOgFqTUwMTQ0ODkwNA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdYoL0oAFqTUyMTg1NzE2Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNDQ4OTA0", "url": "https://github.com/apache/kafka/pull/9364#pullrequestreview-501448904", "createdAt": "2020-10-02T21:08:27Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQyMTowODoyN1rOHb7q5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQyMToyOTo1MlrOHb8IAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1MTIzOA==", "bodyText": "Could we add the new param to the javadoc above?", "url": "https://github.com/apache/kafka/pull/9364#discussion_r499051238", "createdAt": "2020-10-02T21:08:27Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -244,7 +244,8 @@ class Log(@volatile private var _dir: File,\n           val producerIdExpirationCheckIntervalMs: Int,\n           val topicPartition: TopicPartition,\n           val producerStateManager: ProducerStateManager,\n-          logDirFailureChannel: LogDirFailureChannel) extends Logging with KafkaMetricsGroup {\n+          logDirFailureChannel: LogDirFailureChannel,\n+          val hadCleanShutdown: Boolean = true) extends Logging with KafkaMetricsGroup {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1Mjg0OA==", "bodyText": "It seems that all callers set expectDeletedFiles to false. So, do we need this param?", "url": "https://github.com/apache/kafka/pull/9364#discussion_r499052848", "createdAt": "2020-10-02T21:13:12Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -4447,9 +4504,10 @@ class LogTest {\n \n   private def recoverAndCheck(config: LogConfig,\n                               expectedKeys: Iterable[Long],\n-                              expectDeletedFiles: Boolean = true): Log = {\n+                              expectDeletedFiles: Boolean = true,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 368}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1NTIyOA==", "bodyText": "The callers of createLog() in line 652 and 2205 seem to need lastShutdownClean to be false.", "url": "https://github.com/apache/kafka/pull/9364#discussion_r499055228", "createdAt": "2020-10-02T21:20:09Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -4429,9 +4485,10 @@ class LogTest {\n                         scheduler: Scheduler = mockTime.scheduler,\n                         time: Time = mockTime,\n                         maxProducerIdExpirationMs: Int = 60 * 60 * 1000,\n-                        producerIdExpirationCheckIntervalMs: Int = LogManager.ProducerIdExpirationCheckIntervalMs): Log = {\n+                        producerIdExpirationCheckIntervalMs: Int = LogManager.ProducerIdExpirationCheckIntervalMs,\n+                        lastShutdownClean: Boolean = true): Log = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 356}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1ODM2MA==", "bodyText": "The change is unnecessary.", "url": "https://github.com/apache/kafka/pull/9364#discussion_r499058360", "createdAt": "2020-10-02T21:28:56Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -2882,11 +2953,8 @@ class LogTest {\n     records.foreach(segment.append _)\n     segment.close()\n \n-    // Create clean shutdown file so that we do not split during the load\n-    createCleanShutdownFile()\n-\n     val logConfig = LogTest.createLogConfig(indexIntervalBytes = 1, fileDeleteDelayMs = 1000)\n-    val log = createLog(logDir, logConfig, recoveryPoint = Long.MaxValue)\n+    val log = createLog(logDir, logConfig, recoveryPoint = Long.MaxValue, lastShutdownClean = true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 238}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA1ODY5MQ==", "bodyText": "The change is unnecessary.", "url": "https://github.com/apache/kafka/pull/9364#discussion_r499058691", "createdAt": "2020-10-02T21:29:52Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -3073,9 +3139,8 @@ class LogTest {\n     // check if recovery was attempted. Even if the recovery point is 0L, recovery should not be attempted as the\n     // clean shutdown file exists.\n     recoveryPoint = log.logEndOffset\n-    log = createLog(logDir, logConfig)\n+    log = createLog(logDir, logConfig, lastShutdownClean = true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 310}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE1MjAxMTg5", "url": "https://github.com/apache/kafka/pull/9364#pullrequestreview-515201189", "createdAt": "2020-10-23T00:10:42Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QwMDoxMDo0MlrOHm4IOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNjozNjoyM1rOHnVTkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDUyNzU0NA==", "bodyText": "This is an existing issue, but cleanShutdownFile.delete() doesn't seem to throw IOException.", "url": "https://github.com/apache/kafka/pull/9364#discussion_r510527544", "createdAt": "2020-10-23T00:10:42Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -298,26 +300,38 @@ class LogManager(logDirs: Seq[File],\n   /**\n    * Recover and load all logs in the given data directories\n    */\n-  private def loadLogs(): Unit = {\n+  private[log] def loadLogs(): Unit = {\n     info(s\"Loading logs from log dirs $liveLogDirs\")\n     val startMs = time.hiResClockMs()\n     val threadPools = ArrayBuffer.empty[ExecutorService]\n     val offlineDirs = mutable.Set.empty[(String, IOException)]\n-    val jobs = mutable.Map.empty[File, Seq[Future[_]]]\n+    val jobs = ArrayBuffer.empty[Seq[Future[_]]]\n     var numTotalLogs = 0\n \n     for (dir <- liveLogDirs) {\n       val logDirAbsolutePath = dir.getAbsolutePath\n+      var hadCleanShutdown: Boolean = false\n       try {\n         val pool = Executors.newFixedThreadPool(numRecoveryThreadsPerDataDir)\n         threadPools.append(pool)\n \n         val cleanShutdownFile = new File(dir, Log.CleanShutdownFile)\n         if (cleanShutdownFile.exists) {\n           info(s\"Skipping recovery for all logs in $logDirAbsolutePath since clean shutdown file was found\")\n+          // Cache the clean shutdown status and use that for rest of log loading workflow. Delete the CleanShutdownFile\n+          // so that if broker crashes while loading the log, it is considered hard shutdown during the next boot up. KAFKA-10471\n+          try {\n+            cleanShutdownFile.delete()\n+          } catch {\n+            case e: IOException =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDUzMDc3Nw==", "bodyText": "Hmm, it seems that this tests expects a clean shutdown.", "url": "https://github.com/apache/kafka/pull/9364#discussion_r510530777", "createdAt": "2020-10-23T00:23:28Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -1257,7 +1328,7 @@ class LogTest {\n     log.close()\n \n     // After reloading log, producer state should not be regenerated\n-    val reloadedLog = createLog(logDir, logConfig, logStartOffset = 1L)\n+    val reloadedLog = createLog(logDir, logConfig, logStartOffset = 1L, lastShutdownClean = false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk5MTE4OQ==", "bodyText": "It seems that this expects a clean shutdown.", "url": "https://github.com/apache/kafka/pull/9364#discussion_r510991189", "createdAt": "2020-10-23T16:10:39Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -2131,12 +2202,12 @@ class LogTest {\n       assertEquals(\"Should have same number of time index entries as before.\", numTimeIndexEntries, log.activeSegment.timeIndex.entries)\n     }\n \n-    log = createLog(logDir, logConfig, recoveryPoint = lastOffset)\n+    log = createLog(logDir, logConfig, recoveryPoint = lastOffset, lastShutdownClean = false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 225}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDk5NTU4Nw==", "bodyText": "It seem the createLog() call on line 3976 inside testRecoverOnlyLastSegment() needs to have lastShutdownClean = false.", "url": "https://github.com/apache/kafka/pull/9364#discussion_r510995587", "createdAt": "2020-10-23T16:18:11Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -3623,7 +3690,7 @@ class LogTest {\n     log.close()\n \n     // reopen the log and recover from the beginning\n-    val recoveredLog = createLog(logDir, LogConfig())\n+    val recoveredLog = createLog(logDir, LogConfig(), lastShutdownClean = false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 292}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTAwNTU4NA==", "bodyText": "This is an existing problem. Since no callers are explicitly setting expectDeletedFiles, could we just remove this param?", "url": "https://github.com/apache/kafka/pull/9364#discussion_r511005584", "createdAt": "2020-10-23T16:36:23Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -4445,11 +4504,9 @@ class LogTest {\n     (log, segmentWithOverflow)\n   }\n \n-  private def recoverAndCheck(config: LogConfig,\n-                              expectedKeys: Iterable[Long],\n-                              expectDeletedFiles: Boolean = true): Log = {\n-    LogTest.recoverAndCheck(logDir, config, expectedKeys, brokerTopicStats, mockTime, mockTime.scheduler,\n-      expectDeletedFiles)\n+  private def recoverAndCheck(config: LogConfig, expectedKeys: Iterable[Long], expectDeletedFiles: Boolean = true) = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 343}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE3MjMwODY4", "url": "https://github.com/apache/kafka/pull/9364#pullrequestreview-517230868", "createdAt": "2020-10-26T22:48:15Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQyMjo0ODoxNVrOHolM9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQyMjo1NTo0NVrOHolYNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjMxNDYxMw==", "bodyText": "This line seems unnecessary since hadCleanShutdown is initialized to false.", "url": "https://github.com/apache/kafka/pull/9364#discussion_r512314613", "createdAt": "2020-10-26T22:48:15Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -298,26 +300,32 @@ class LogManager(logDirs: Seq[File],\n   /**\n    * Recover and load all logs in the given data directories\n    */\n-  private def loadLogs(): Unit = {\n+  private[log] def loadLogs(): Unit = {\n     info(s\"Loading logs from log dirs $liveLogDirs\")\n     val startMs = time.hiResClockMs()\n     val threadPools = ArrayBuffer.empty[ExecutorService]\n     val offlineDirs = mutable.Set.empty[(String, IOException)]\n-    val jobs = mutable.Map.empty[File, Seq[Future[_]]]\n+    val jobs = ArrayBuffer.empty[Seq[Future[_]]]\n     var numTotalLogs = 0\n \n     for (dir <- liveLogDirs) {\n       val logDirAbsolutePath = dir.getAbsolutePath\n+      var hadCleanShutdown: Boolean = false\n       try {\n         val pool = Executors.newFixedThreadPool(numRecoveryThreadsPerDataDir)\n         threadPools.append(pool)\n \n         val cleanShutdownFile = new File(dir, Log.CleanShutdownFile)\n         if (cleanShutdownFile.exists) {\n           info(s\"Skipping recovery for all logs in $logDirAbsolutePath since clean shutdown file was found\")\n+          // Cache the clean shutdown status and use that for rest of log loading workflow. Delete the CleanShutdownFile\n+          // so that if broker crashes while loading the log, it is considered hard shutdown during the next boot up. KAFKA-10471\n+          cleanShutdownFile.delete()\n+          hadCleanShutdown = true\n         } else {\n           // log recovery itself is being performed by `Log` class during initialization\n           info(s\"Attempting recovery for all logs in $logDirAbsolutePath since no clean shutdown file was found\")\n+          hadCleanShutdown = false", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjMxNzI4OA==", "bodyText": "It seems we don't need to set hadCleanShutdown to true.", "url": "https://github.com/apache/kafka/pull/9364#discussion_r512317288", "createdAt": "2020-10-26T22:55:08Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -978,10 +1051,10 @@ class LogTest {\n       producerIdExpirationCheckIntervalMs = 30000,\n       topicPartition = Log.parseTopicPartitionName(logDir),\n       producerStateManager = stateManager,\n-      logDirFailureChannel = null)\n+      logDirFailureChannel = null,\n+      hadCleanShutdown = true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjMxNzQ5Mw==", "bodyText": "It seems we don't need to set hadCleanShutdown to true.", "url": "https://github.com/apache/kafka/pull/9364#discussion_r512317493", "createdAt": "2020-10-26T22:55:45Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -1021,10 +1092,10 @@ class LogTest {\n       producerIdExpirationCheckIntervalMs = 30000,\n       topicPartition = Log.parseTopicPartitionName(logDir),\n       producerStateManager = stateManager,\n-      logDirFailureChannel = null)\n+      logDirFailureChannel = null,\n+      hadCleanShutdown = true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 204}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3b3eccfdd6d33fbb5227abb324b599cf482a639b", "author": {"user": {"login": "RamanVerma", "name": "Raman Verma"}}, "url": "https://github.com/apache/kafka/commit/3b3eccfdd6d33fbb5227abb324b599cf482a639b", "committedDate": "2020-10-30T07:09:35Z", "message": "KAFKA-10471 - Mark broker crash during log loading as unclean shutdown\n\nLogManager writes a clean shutdown file when the broker shuts down. The\npresence of this file indicates that the broker had a clean shutdown and\nlog recovery is not needed upon the next boot up.\n\nEarlier, LogManager would check for this file at the start of log loading workflow,\nand delete it after the log has been loaded. If the broker were to crash\nwhile loading logs, the file would not be deleted and mislead LogManager when it\ntries to load logs upon next boot up. Hence, a crash during log loading\nwill not be considered a hard reset of broker.\n\nAs part of this fix, we delete the clean shutdown file as soon as we\nlook it up, at the start of log loading workflow. Thereafter, we maintain a boolean\nflag to indicate if broker underwent clean shutdown or not. So, if the\nbroker were to crash while logs are being loaded, LogManager will be\nable to detect this as a hard reset."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "784591db20b3a7908db3392d093599126c374651", "author": {"user": {"login": "RamanVerma", "name": "Raman Verma"}}, "url": "https://github.com/apache/kafka/commit/784591db20b3a7908db3392d093599126c374651", "committedDate": "2020-10-30T07:09:35Z", "message": "Addressed review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c93b076c05e96ed3442c1b82870ba9df3d2f793", "author": {"user": {"login": "RamanVerma", "name": "Raman Verma"}}, "url": "https://github.com/apache/kafka/commit/0c93b076c05e96ed3442c1b82870ba9df3d2f793", "committedDate": "2020-10-30T07:09:35Z", "message": "Addressed review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "248b78e4a15f3e95d5f2eebe897577d791f5de4f", "author": {"user": {"login": "RamanVerma", "name": "Raman Verma"}}, "url": "https://github.com/apache/kafka/commit/248b78e4a15f3e95d5f2eebe897577d791f5de4f", "committedDate": "2020-10-30T07:09:35Z", "message": "Review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e6a6d55c7a8cd5141b9373ffc0e2fe545b9d8afe", "author": {"user": {"login": "RamanVerma", "name": "Raman Verma"}}, "url": "https://github.com/apache/kafka/commit/e6a6d55c7a8cd5141b9373ffc0e2fe545b9d8afe", "committedDate": "2020-10-30T07:09:35Z", "message": "Addressed review comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "e6a6d55c7a8cd5141b9373ffc0e2fe545b9d8afe", "author": {"user": {"login": "RamanVerma", "name": "Raman Verma"}}, "url": "https://github.com/apache/kafka/commit/e6a6d55c7a8cd5141b9373ffc0e2fe545b9d8afe", "committedDate": "2020-10-30T07:09:35Z", "message": "Addressed review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cdfd934e70a527d15275716581950938ec80a2b0", "author": {"user": {"login": "RamanVerma", "name": "Raman Verma"}}, "url": "https://github.com/apache/kafka/commit/cdfd934e70a527d15275716581950938ec80a2b0", "committedDate": "2020-10-30T07:41:56Z", "message": "Minor log message change"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIwODkwMzE1", "url": "https://github.com/apache/kafka/pull/9364#pullrequestreview-520890315", "createdAt": "2020-10-30T17:32:31Z", "commit": {"oid": "cdfd934e70a527d15275716581950938ec80a2b0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNzozMjozMlrOHrZUHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNzozMjozMlrOHrZUHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTI2NTU2Ng==", "bodyText": "It seems that hadCleanShutdown doesn't need to be exposed as a public val?", "url": "https://github.com/apache/kafka/pull/9364#discussion_r515265566", "createdAt": "2020-10-30T17:32:32Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -244,7 +246,8 @@ class Log(@volatile private var _dir: File,\n           val producerIdExpirationCheckIntervalMs: Int,\n           val topicPartition: TopicPartition,\n           val producerStateManager: ProducerStateManager,\n-          logDirFailureChannel: LogDirFailureChannel) extends Logging with KafkaMetricsGroup {\n+          logDirFailureChannel: LogDirFailureChannel,\n+          val hadCleanShutdown: Boolean = true) extends Logging with KafkaMetricsGroup {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cdfd934e70a527d15275716581950938ec80a2b0"}, "originalPosition": 15}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "49f1b10c84c4fcb33afe930477a85fb54f606ab2", "author": {"user": {"login": "RamanVerma", "name": "Raman Verma"}}, "url": "https://github.com/apache/kafka/commit/49f1b10c84c4fcb33afe930477a85fb54f606ab2", "committedDate": "2020-10-30T22:49:10Z", "message": "Review comment"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxODU3MTYz", "url": "https://github.com/apache/kafka/pull/9364#pullrequestreview-521857163", "createdAt": "2020-11-02T17:40:32Z", "commit": {"oid": "49f1b10c84c4fcb33afe930477a85fb54f606ab2"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 742, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}