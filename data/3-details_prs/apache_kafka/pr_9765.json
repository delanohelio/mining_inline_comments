{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQyMjgwOTA2", "number": 9765, "title": "KAFKA-10763: Fix incomplete cooperative rebalances preventing connector/task revocations", "bodyText": "Jira\nWhen two cooperative rebalances take place soon after one another, a prior rebalance may not complete before the next rebalance is started.\nUnder Eager rebalancing, no tasks would have been started, so the subsequent onRevoked call is intentionally skipped whenever rebalanceResolved was false.\nUnder Cooperative rebalancing, the same logic causes the DistributedHerder to skip stopping all of the connector/task revocations which occur in the second rebalance.\nThe DistributedHerder still removes the revoked connectors/tasks from its assignment, so that the DistributedHerder and Worker have different knowledge of running connectors/tasks.\nThis causes the connector/task instances that would have been stopped to disappear from the rebalance protocol, and left running until their workers are halted, or they fail.\nConnectors/Tasks which were then reassigned to other workers by the rebalance protocol would be duplicated, and run concurrently with zombie connectors/tasks.\nConnectors/Tasks which were reassigned back to the same worker would encounter exceptions in Worker, indicating that the connector/task existed and was already running.\n\nAdd a test for revoking and then reassigning a connector under normal circumstances\nAdd a test for revoking and then reassigning a connector following an incomplete cooperative rebalance\nChange expectRebalance to make assignment fields mutable before passing them into the DistributedHerder\nOnly skip revocation for the Eager protocol, and never skip revocation for cooperative/sessioned protocols\n\nThis should be backported to all branches with Incremental Cooperative Rebalancing, 2.3+ if possible. This is a serious consistency bug, and the fix should probably be applied widely.\nSigned-off-by: Greg Harris gregh@confluent.io\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-12-18T04:34:09Z", "url": "https://github.com/apache/kafka/pull/9765", "merged": true, "mergeCommit": {"oid": "f572545611b3e5daa71bd7b3f0c20cba01dfb46d"}, "closed": true, "closedAt": "2021-01-26T18:17:06Z", "author": {"login": "gharris1727"}, "timelineItems": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdnQefaAH2gAyNTQyMjgwOTA2OjEzNzVmMDFkYTZjZTk3OTMwN2Y3YmE1MjM2MmI0NTMxNjZjMmQzOWU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdz_htsgFqTU3NjYyMjgyNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "1375f01da6ce979307f7ba52362b453166c2d39e", "author": {"user": {"login": "gharris1727", "name": "Greg Harris"}}, "url": "https://github.com/apache/kafka/commit/1375f01da6ce979307f7ba52362b453166c2d39e", "committedDate": "2020-12-18T04:32:04Z", "message": "KAFKA-10763: Fix incomplete cooperative rebalances preventing connector/task revocations\n\nWhen two cooperative rebalances take place soon after one another, a prior rebalance may not complete before the next rebalance is started.\nUnder Eager rebalancing, no tasks would have been started, so the subsequent onRevoked call is intentionally skipped whenever rebalanceResolved was false.\nUnder Cooperative rebalancing, the same logic causes the DistributedHerder to skip stopping all of the connector/task revocations which occur in the second rebalance.\nThe DistributedHerder still removes the revoked connectors/tasks from its assignment, so that the DistributedHerder and Worker have different knowledge of running connectors/tasks.\nThis causes the connector/task instances that would have been stopped to disappear from the rebalance protocol, and left running until their workers are halted, or they fail.\nConnectors/Tasks which were then reassigned to other workers by the rebalance protocol would be duplicated, and run concurrently with zombie connectors/tasks.\nConnectors/Tasks which were reassigned back to the same worker would encounter exceptions in Worker, indicating that the connector/task existed and was already running.\n\n* Add a test for revoking and then reassigning a connector under normal circumstances\n* Add a test for revoking and then reassigning a connector following an incomplete cooperative rebalance\n* Change expectRebalance to make assignment fields mutable before passing them into the DistributedHerder\n* Only skip revocation for the Eager protocol, and never skip revocation for cooperative/sessioned protocols\n\nSigned-off-by: Greg Harris <gregh@confluent.io>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1NzA0MTk0", "url": "https://github.com/apache/kafka/pull/9765#pullrequestreview-555704194", "createdAt": "2020-12-18T18:08:47Z", "commit": {"oid": "1375f01da6ce979307f7ba52362b453166c2d39e"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODowODo0N1rOIItSkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODo1NjoyNlrOIIu5Vg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAwMTU1NQ==", "bodyText": "It's hard to tell if this actually reproduces the issue or not due to the heavy mocking required. Is there a more direct way to reproduce? Maybe in RebalanceSourceConnectorsIntegrationTest or similar? Even if the IT ends up being flaky, having that repro would boost confidence in this fix.", "url": "https://github.com/apache/kafka/pull/9765#discussion_r546001555", "createdAt": "2020-12-18T18:08:47Z", "author": {"login": "ncliang"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/distributed/DistributedHerderTest.java", "diffHunk": "@@ -566,6 +566,112 @@ public Boolean answer() throws Throwable {\n         PowerMock.verifyAll();\n     }\n \n+    @Test\n+    public void testRevoke() throws TimeoutException {\n+        revokeAndReassign(false);\n+    }\n+\n+    @Test\n+    public void testIncompleteRebalanceBeforeRevoke() throws TimeoutException {\n+        revokeAndReassign(true);\n+    }\n+\n+    public void revokeAndReassign(boolean incompleteRebalance) throws TimeoutException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1375f01da6ce979307f7ba52362b453166c2d39e"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAyNzg2Mg==", "bodyText": "Maybe add a comment explaining why the additional check is needed.", "url": "https://github.com/apache/kafka/pull/9765#discussion_r546027862", "createdAt": "2020-12-18T18:56:26Z", "author": {"login": "ncliang"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java", "diffHunk": "@@ -1740,7 +1741,7 @@ public void onRevoked(String leader, Collection<String> connectors, Collection<C\n             // Note that since we don't reset the assignment, we don't revoke leadership here. During a rebalance,\n             // it is still important to have a leader that can write configs, offsets, etc.\n \n-            if (rebalanceResolved) {\n+            if (rebalanceResolved || currentProtocolVersion >= CONNECT_PROTOCOL_V1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1375f01da6ce979307f7ba52362b453166c2d39e"}, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTc2NjIyODI3", "url": "https://github.com/apache/kafka/pull/9765#pullrequestreview-576622827", "createdAt": "2021-01-26T18:08:13Z", "commit": {"oid": "1375f01da6ce979307f7ba52362b453166c2d39e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2336, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}