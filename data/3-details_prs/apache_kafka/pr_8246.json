{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg1MDY5Nzkw", "number": 8246, "title": "KAFKA-6145: Pt 2. Include offset sums in subscription", "bodyText": "KIP-441 Pt. 2: Compute sum of offsets across all stores/changelogs in a task and include them in the subscription.\nPreviously each thread would just encode every task on disk, but we now need to read the changelog file which is unsafe to do without a lock on the task directory. So, each thread now encodes only its assigned active and standby tasks, and ignores any already-locked tasks.\nIn some cases there may be unowned and unlocked tasks on disk that were reassigned to another instance and haven't been cleaned up yet by the background thread. Each StreamThread makes a weak effort to lock any such task directories it finds, and if successful is then responsible for computing and reporting that task's offset sum (based on reading the checkpoint file)\nThis PR therefore also addresses two orthogonal issues:\n\nPrevent background cleaner thread from deleting unowned stores during a rebalance\nDeduplicate standby tasks in subscription: each thread used to include every (non-active) task found on disk in its \"standby task\" set, which meant every active, standby, and unowned task was encoded by every thread.", "createdAt": "2020-03-06T23:55:44Z", "url": "https://github.com/apache/kafka/pull/8246", "merged": true, "mergeCommit": {"oid": "542853d99b9e0d660a9cf9317be8a3f8fce4c765"}, "closed": true, "closedAt": "2020-03-14T03:56:59Z", "author": {"login": "ableegoldman"}, "timelineItems": {"totalCount": 31, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcMW3V6AFqTM3MjE4ODE3Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcN9Ng2AFqTM3NDgxOTM3Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyMTg4MTc3", "url": "https://github.com/apache/kafka/pull/8246#pullrequestreview-372188177", "createdAt": "2020-03-10T17:51:57Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxNzo1MTo1N1rOF0aVRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxODozMToxM1rOF0b0yA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUwMTcwMA==", "bodyText": "Let's get the lock first, and then check if the checkpoint file exists.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390501700", "createdAt": "2020-03-10T17:51:57Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUwMjU3NQ==", "bodyText": "Let's add this before we try to read the checkpoint file. In case we do get an IOException, we shouldn't forget that we got the lock.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390502575", "createdAt": "2020-03-10T17:53:21Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUxNzcxNA==", "bodyText": "what should we do if the offset is negative?", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390517714", "createdAt": "2020-03-10T18:17:23Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);\n+                            }\n+                        } catch (final IOException e) {\n+                            // if for any reason we can't lock this task dir and read its checkpoint file, just move on\n+                        }\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // we should just ignore any unknown files that sit in the same directory\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        return taskOffsetSums;\n+    }\n+\n+    private long computeOffsetSum(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0;\n+        for (final long offset : changelogOffsets.values()) {\n+            offsetSum += offset;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUxODM1NA==", "bodyText": "Should we try to unlock the rest? Also, we should always include the causing exception.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390518354", "createdAt": "2020-03-10T18:18:29Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);\n+                            }\n+                        } catch (final IOException e) {\n+                            // if for any reason we can't lock this task dir and read its checkpoint file, just move on\n+                        }\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // we should just ignore any unknown files that sit in the same directory\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        return taskOffsetSums;\n+    }\n+\n+    private long computeOffsetSum(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0;\n+        for (final long offset : changelogOffsets.values()) {\n+            offsetSum += offset;\n+        }\n+        return offsetSum;\n+    }\n+\n+    private void releaseTemporarilyLockedTaskDirectories() {\n+        for (final TaskId id : lockedUnassignedTaskDirectories) {\n+            try {\n+                stateDirectory.unlock(id);\n+            } catch (final IOException e) {\n+                log.error(\"Failed to release the state directory lock for task {}.\", id);\n+                throw new StreamsException(\"Unable to unlock task directory after rebalance.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUxODU0MA==", "bodyText": "we should log the exception, too.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390518540", "createdAt": "2020-03-10T18:18:46Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);\n+                            }\n+                        } catch (final IOException e) {\n+                            // if for any reason we can't lock this task dir and read its checkpoint file, just move on\n+                        }\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // we should just ignore any unknown files that sit in the same directory\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        return taskOffsetSums;\n+    }\n+\n+    private long computeOffsetSum(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0;\n+        for (final long offset : changelogOffsets.values()) {\n+            offsetSum += offset;\n+        }\n+        return offsetSum;\n+    }\n+\n+    private void releaseTemporarilyLockedTaskDirectories() {\n+        for (final TaskId id : lockedUnassignedTaskDirectories) {\n+            try {\n+                stateDirectory.unlock(id);\n+            } catch (final IOException e) {\n+                log.error(\"Failed to release the state directory lock for task {}.\", id);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUyMDYzNw==", "bodyText": "This now looks a little suspicious... In the absence of a value, should we assume standbys are caught up (0L), or that they are not (MAX_VALUE)?", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390520637", "createdAt": "2020-03-10T18:22:10Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java", "diffHunk": "@@ -200,10 +201,19 @@ public UUID processId() {\n     public Map<TaskId, Long> taskOffsetSums() {\n         if (taskOffsetSumsCache == null) {\n             taskOffsetSumsCache = new HashMap<>();\n-            for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {\n-                for (final PartitionToOffsetSum partitionOffsetSum : topicGroup.partitionToOffsetSum()) {\n-                    taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n-                                            partitionOffsetSum.offsetSum());\n+            if (data.version() >= MIN_VERSION_OFFSET_SUM_SUBSCRIPTION) {\n+                for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {\n+                    for (final PartitionToOffsetSum partitionOffsetSum : topicGroup.partitionToOffsetSum()) {\n+                        taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n+                            partitionOffsetSum.offsetSum());\n+                    }\n+                }\n+            } else {\n+                for (final TaskId task : prevTasks()) {\n+                    taskOffsetSumsCache.put(task, Task.LATEST_OFFSET);\n+                }\n+                for (final TaskId task : standbyTasks()) {\n+                    taskOffsetSumsCache.put(task, 0L);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUyMDg1NA==", "bodyText": "Oh, also, should we detect overflow and pin to MAX_VALUE in that case?", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390520854", "createdAt": "2020-03-10T18:22:37Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);\n+                            }\n+                        } catch (final IOException e) {\n+                            // if for any reason we can't lock this task dir and read its checkpoint file, just move on\n+                        }\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // we should just ignore any unknown files that sit in the same directory\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        return taskOffsetSums;\n+    }\n+\n+    private long computeOffsetSum(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0;\n+        for (final long offset : changelogOffsets.values()) {\n+            offsetSum += offset;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUxNzcxNA=="}, "originalCommit": null, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUyNjE1Mg==", "bodyText": "Can we move 01 to before 02?", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390526152", "createdAt": "2020-03-10T18:31:13Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );\n+\n+        final Long task00OffsetSum = Task.LATEST_OFFSET;\n+        final Long task01OffsetSum = 3L;\n+        final Long task02OffsetSum = 15L;\n+        final Long task10OffsetSum = 50L;\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        final Map<TaskId, Set<TopicPartition>> activeTaskAssignment = new HashMap<>(taskId00Assignment);\n+        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true);\n+        expectRestoreToBeCompleted(consumer, changeLogReader);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n+            .andReturn(singletonList(task00)).once();\n+        final StateMachineTask task02 = new StateMachineTask(taskId02, taskId02Partitions, false);\n+        task02.setChangelogOffsets(task02ChangelogOffsets);\n+        expect(standbyTaskCreator.createTasks(eq(taskId02Assignment)))\n+            .andReturn(singletonList(task02)).once();\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyMjUyNzg1", "url": "https://github.com/apache/kafka/pull/8246#pullrequestreview-372252785", "createdAt": "2020-03-10T19:21:07Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxOToyMTowOFrOF0dmYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxOToyMTowOFrOF0dmYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU1NTIzMw==", "bodyText": "Not taking a hard stance on this spelling, just aiming for consistency across the code base", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390555233", "createdAt": "2020-03-10T19:21:08Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -664,16 +664,16 @@ private void initializeTaskTime(final Map<TopicPartition, OffsetAndMetadata> off\n     }\n \n     @Override\n-    public Map<TopicPartition, Long> purgableOffsets() {\n-        final Map<TopicPartition, Long> purgableConsumedOffsets = new HashMap<>();\n+    public Map<TopicPartition, Long> purgeableOffsets() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcxODU3MTE3", "url": "https://github.com/apache/kafka/pull/8246#pullrequestreview-371857117", "createdAt": "2020-03-10T11:04:33Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxMTowNDozM1rOF0KTdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQyMToxMDozOVrOF0hLMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDIzOTA5NQ==", "bodyText": "Q: Wouldn't a log message on DEBUG-level make sense here?", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390239095", "createdAt": "2020-03-10T11:04:33Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);\n+                            }\n+                        } catch (final IOException e) {\n+                            // if for any reason we can't lock this task dir and read its checkpoint file, just move on", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI0NzUzMA==", "bodyText": "prop:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    long offsetSum = 0;\n          \n          \n            \n                    for (final long offset : changelogOffsets.values()) {\n          \n          \n            \n                        offsetSum += offset;\n          \n          \n            \n                    }\n          \n          \n            \n                    return offsetSum;\n          \n          \n            \n                    return changelogOffsets.values().stream().reduce(0L, Long::sum);", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390247530", "createdAt": "2020-03-10T11:23:11Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);\n+                            }\n+                        } catch (final IOException e) {\n+                            // if for any reason we can't lock this task dir and read its checkpoint file, just move on\n+                        }\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // we should just ignore any unknown files that sit in the same directory\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        return taskOffsetSums;\n+    }\n+\n+    private long computeOffsetSum(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0;\n+        for (final long offset : changelogOffsets.values()) {\n+            offsetSum += offset;\n+        }\n+        return offsetSum;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI0OTA4OQ==", "bodyText": "prop: rename to sumUpChangelogOffsets", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390249089", "createdAt": "2020-03-10T11:26:34Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);\n+                            }\n+                        } catch (final IOException e) {\n+                            // if for any reason we can't lock this task dir and read its checkpoint file, just move on\n+                        }\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // we should just ignore any unknown files that sit in the same directory\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        return taskOffsetSums;\n+    }\n+\n+    private long computeOffsetSum(final Map<TopicPartition, Long> changelogOffsets) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI1MzkwNw==", "bodyText": "Q: This question is unrelated to your change. Why is firstException an atomic variable? It is a local variable and shutdown() is only called from StreamThread which should be single-threaded. \\cc @guozhangwang", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390253907", "createdAt": "2020-03-10T11:36:55Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -480,6 +503,12 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n+        try {\n+            releaseTemporarilyLockedTaskDirectories();\n+        } catch (final RuntimeException e) {\n+            firstException.compareAndSet(null, e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI3NzAxMg==", "bodyText": "prop:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            for (final TaskId task : prevTasks()) {\n          \n          \n            \n                                taskOffsetSumsCache.put(task, Task.LATEST_OFFSET);\n          \n          \n            \n                            }\n          \n          \n            \n                            for (final TaskId task : standbyTasks()) {\n          \n          \n            \n                                taskOffsetSumsCache.put(task, 0L);\n          \n          \n            \n                            }\n          \n          \n            \n                            prevTasks().forEach((taskId) -> taskOffsetSumsCache.put(taskId, Task.LATEST_OFFSET));\n          \n          \n            \n                            prevTasks().forEach((taskId) -> taskOffsetSumsCache.put(taskId, 0L));", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390277012", "createdAt": "2020-03-10T12:25:52Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java", "diffHunk": "@@ -200,10 +201,19 @@ public UUID processId() {\n     public Map<TaskId, Long> taskOffsetSums() {\n         if (taskOffsetSumsCache == null) {\n             taskOffsetSumsCache = new HashMap<>();\n-            for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {\n-                for (final PartitionToOffsetSum partitionOffsetSum : topicGroup.partitionToOffsetSum()) {\n-                    taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n-                                            partitionOffsetSum.offsetSum());\n+            if (data.version() >= MIN_VERSION_OFFSET_SUM_SUBSCRIPTION) {\n+                for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {\n+                    for (final PartitionToOffsetSum partitionOffsetSum : topicGroup.partitionToOffsetSum()) {\n+                        taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n+                            partitionOffsetSum.offsetSum());\n+                    }\n+                }\n+            } else {\n+                for (final TaskId task : prevTasks()) {\n+                    taskOffsetSumsCache.put(task, Task.LATEST_OFFSET);\n+                }\n+                for (final TaskId task : standbyTasks()) {\n+                    taskOffsetSumsCache.put(task, 0L);\n                 }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI3Nzg3NQ==", "bodyText": "req:\nLOG.info(\n    \"Unable to decode subscription data: used version: {}; latest supported version: {}\",\n    version, \n    latestSupportedVersion\n);\n\nor\nLOG.info(\"Unable to decode subscription data: used version: {}; latest supported version: {}\",\n    version, \n    latestSupportedVersion\n);", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390277875", "createdAt": "2020-03-10T12:27:38Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java", "diffHunk": "@@ -266,7 +276,7 @@ public static SubscriptionInfo decode(final ByteBuffer data) {\n             subscriptionInfoData.setVersion(version);\n             subscriptionInfoData.setLatestSupportedVersion(latestSupportedVersion);\n             LOG.info(\"Unable to decode subscription data: used version: {}; latest supported version: {}\",\n-                     version, latestSupportedVersion);\n+                version, latestSupportedVersion);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI3ODg1NA==", "bodyText": "req:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                    taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n          \n          \n            \n                                        partitionOffsetSum.offsetSum());\n          \n          \n            \n                                    taskOffsetSumsCache.put(\n          \n          \n            \n                                        new TaskId(topicGroup.topicGroupId(), \n          \n          \n            \n                                        partitionOffsetSum.partition()),\n          \n          \n            \n                                        partitionOffsetSum.offsetSum()\n          \n          \n            \n                                    );", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390278854", "createdAt": "2020-03-10T12:29:48Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java", "diffHunk": "@@ -200,10 +201,19 @@ public UUID processId() {\n     public Map<TaskId, Long> taskOffsetSums() {\n         if (taskOffsetSumsCache == null) {\n             taskOffsetSumsCache = new HashMap<>();\n-            for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {\n-                for (final PartitionToOffsetSum partitionOffsetSum : topicGroup.partitionToOffsetSum()) {\n-                    taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n-                                            partitionOffsetSum.offsetSum());\n+            if (data.version() >= MIN_VERSION_OFFSET_SUM_SUBSCRIPTION) {\n+                for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {\n+                    for (final PartitionToOffsetSum partitionOffsetSum : topicGroup.partitionToOffsetSum()) {\n+                        taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n+                            partitionOffsetSum.offsetSum());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI4NTExNg==", "bodyText": "prop:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        topicGroupIdToPartitionOffsetSum.putIfAbsent(task.topicGroupId, new ArrayList<>());\n          \n          \n            \n                        topicGroupIdToPartitionOffsetSum.get(task.topicGroupId).add(\n          \n          \n            \n                        topicGroupIdToPartitionOffsetSum.computeIfAbsent(task.topicGroupId, new ArrayList<>()).add(", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390285116", "createdAt": "2020-03-10T12:42:38Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java", "diffHunk": "@@ -115,8 +116,8 @@ private void setTaskOffsetSumDataFromTaskOffsetSumMap(final Map<TaskId, Long> ta\n             topicGroupIdToPartitionOffsetSum.putIfAbsent(task.topicGroupId, new ArrayList<>());\n             topicGroupIdToPartitionOffsetSum.get(task.topicGroupId).add(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU2MzE2NQ==", "bodyText": "Q: Would it be too strict to specify offset >= 0 as an invariant and throw an IllegalStateException?\nRegarding the overflow, when computing the lag, the sum of the last committed offsets will always be larger or equal to the sum of the offset of the states of a task except for the case where the sum of the last committed offsets has already overflown but the sum of the offset of the states has not. So, if both have overflown then the difference should not be affected by the overflows. If only the sum of the last committed offsets has overflown we need to compute the difference differently, but we are able to recognize this case. All of this assumes that overflows are well defined in Java as MIN_VALUE comes after MAX_VALUE.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390563165", "createdAt": "2020-03-10T19:36:01Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -374,50 +379,68 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any active and standby\n+     * tasks assigned to this thread. May also include the offset sum for some unassigned tasks that belong to no\n+     * threads but are yet to be cleaned up (eg after rolling bounce). Each thread will make an uncommitted effort to\n+     * lock any unlocked task directories it finds on disk, and will be responsible for including its offset sum in\n+     * their subscription (and of course unlocking it again when rebalance completes).\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n-\n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n-            } else {\n-                taskOffsetSums.put(id, 0L);\n-            }\n-        }\n-        return taskOffsetSums;\n-    }\n-\n-    /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n-     */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    final Task task = tasks.get(id);\n+                    if (task != null) {\n+                        if (task.isActive() && task.state() == RUNNING) {\n+                            taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                        } else {\n+                            taskOffsetSums.put(id, computeOffsetSum(task.changelogOffsets()));\n+                        }\n+                    } else {\n+                        try {\n+                            // if we are able to lock this task dir and find a valid checkpoint file, we are\n+                            // responsible for encoding its offsets in our subscription\n+                            final File checkpointFile = new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME);\n+                            if (stateDirectory.lock(id) && checkpointFile.exists()) {\n+                                taskOffsetSums.put(id, computeOffsetSum(new OffsetCheckpoint(checkpointFile).read()));\n+                                lockedUnassignedTaskDirectories.add(id);\n+                            }\n+                        } catch (final IOException e) {\n+                            // if for any reason we can't lock this task dir and read its checkpoint file, just move on\n+                        }\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // we should just ignore any unknown files that sit in the same directory\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        return taskOffsetSums;\n+    }\n+\n+    private long computeOffsetSum(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0;\n+        for (final long offset : changelogOffsets.values()) {\n+            offsetSum += offset;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUxNzcxNA=="}, "originalCommit": null, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU2Nzg5Mw==", "bodyText": "req: Could you add unit tests for this method?", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390567893", "createdAt": "2020-03-10T19:45:03Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java", "diffHunk": "@@ -200,10 +201,19 @@ public UUID processId() {\n     public Map<TaskId, Long> taskOffsetSums() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU3MzE0OQ==", "bodyText": "req: Could you compute those from the above maps? It might make maintenance easier.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390573149", "createdAt": "2020-03-10T19:55:05Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );\n+\n+        final Long task00OffsetSum = Task.LATEST_OFFSET;\n+        final Long task01OffsetSum = 3L;\n+        final Long task02OffsetSum = 15L;\n+        final Long task10OffsetSum = 50L;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU4MDczMw==", "bodyText": "req: Are those verifications required? It seems to me they test the File object. Could you remove them?", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390580733", "createdAt": "2020-03-10T20:09:09Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );\n+\n+        final Long task00OffsetSum = Task.LATEST_OFFSET;\n+        final Long task01OffsetSum = 3L;\n+        final Long task02OffsetSum = 15L;\n+        final Long task10OffsetSum = 50L;\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        final Map<TaskId, Set<TopicPartition>> activeTaskAssignment = new HashMap<>(taskId00Assignment);\n+        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true);\n+        expectRestoreToBeCompleted(consumer, changeLogReader);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n+            .andReturn(singletonList(task00)).once();\n+        final StateMachineTask task02 = new StateMachineTask(taskId02, taskId02Partitions, false);\n+        task02.setChangelogOffsets(task02ChangelogOffsets);\n+        expect(standbyTaskCreator.createTasks(eq(taskId02Assignment)))\n+            .andReturn(singletonList(task02)).once();\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);\n+        task01.setChangelogOffsets(task01ChangelogOffsets);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId01Assignment)))\n+            .andReturn(singletonList(task01)).once();\n \n         expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n \n-        replay(activeTaskCreator, stateDirectory);\n+        expect(stateDirectory.lock(taskId10)).andReturn(true).once();\n+        expect(stateDirectory.lock(taskId12)).andReturn(false).once();\n+\n+\n+        final File task10CheckpointFile = new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        final File task12CheckpointFile = new File(taskFolders[5], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        assertThat(task10CheckpointFile.createNewFile(), is(true));\n+        assertThat(task12CheckpointFile.createNewFile(), is(true));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU5MTk5NQ==", "bodyText": "req: I assume you do not want to test handleAssignment() here, so you should not specify behaviour verification on the mock. You could simply write\nexpect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n    .andStubReturn(singletonList(task00));\n\n.andStubReturn() is behavior that is not verified in the verify() method. Using it were no behavior verification is needed makes the test more robust to changes in the productive code that should not affect this test.\nSame applies to other similar locations in this test.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390591995", "createdAt": "2020-03-10T20:30:10Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );\n+\n+        final Long task00OffsetSum = Task.LATEST_OFFSET;\n+        final Long task01OffsetSum = 3L;\n+        final Long task02OffsetSum = 15L;\n+        final Long task10OffsetSum = 50L;\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        final Map<TaskId, Set<TopicPartition>> activeTaskAssignment = new HashMap<>(taskId00Assignment);\n+        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true);\n+        expectRestoreToBeCompleted(consumer, changeLogReader);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n+            .andReturn(singletonList(task00)).once();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDU5ODIwOQ==", "bodyText": "req: Why do you verify the setup code here?", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390598209", "createdAt": "2020-03-10T20:42:25Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );\n+\n+        final Long task00OffsetSum = Task.LATEST_OFFSET;\n+        final Long task01OffsetSum = 3L;\n+        final Long task02OffsetSum = 15L;\n+        final Long task10OffsetSum = 50L;\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        final Map<TaskId, Set<TopicPartition>> activeTaskAssignment = new HashMap<>(taskId00Assignment);\n+        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true);\n+        expectRestoreToBeCompleted(consumer, changeLogReader);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n+            .andReturn(singletonList(task00)).once();\n+        final StateMachineTask task02 = new StateMachineTask(taskId02, taskId02Partitions, false);\n+        task02.setChangelogOffsets(task02ChangelogOffsets);\n+        expect(standbyTaskCreator.createTasks(eq(taskId02Assignment)))\n+            .andReturn(singletonList(task02)).once();\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);\n+        task01.setChangelogOffsets(task01ChangelogOffsets);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId01Assignment)))\n+            .andReturn(singletonList(task01)).once();\n \n         expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n \n-        replay(activeTaskCreator, stateDirectory);\n+        expect(stateDirectory.lock(taskId10)).andReturn(true).once();\n+        expect(stateDirectory.lock(taskId12)).andReturn(false).once();\n+\n+\n+        final File task10CheckpointFile = new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        final File task12CheckpointFile = new File(taskFolders[5], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        assertThat(task10CheckpointFile.createNewFile(), is(true));\n+        assertThat(task12CheckpointFile.createNewFile(), is(true));\n+        new OffsetCheckpoint(task10CheckpointFile).write(task10ChangelogOffsets);\n+\n+        replay(activeTaskCreator, standbyTaskCreator, consumer, changeLogReader, stateDirectory);\n+\n+        taskManager.handleAssignment(activeTaskAssignment, taskId02Assignment);\n+        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n+\n+        activeTaskAssignment.putAll(taskId01Assignment);\n+        taskManager.handleAssignment(activeTaskAssignment, taskId02Assignment);\n+\n+        assertThat(task00.state(), is(Task.State.RUNNING));\n+        assertThat(task01.state(), not(Task.State.RUNNING));\n+        assertThat(task02.state(), is(Task.State.RUNNING));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYwMDU4Mg==", "bodyText": "prop: Please use assertThat() since it makes verifications a bit better readable.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390600582", "createdAt": "2020-03-10T20:46:48Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );\n+\n+        final Long task00OffsetSum = Task.LATEST_OFFSET;\n+        final Long task01OffsetSum = 3L;\n+        final Long task02OffsetSum = 15L;\n+        final Long task10OffsetSum = 50L;\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        final Map<TaskId, Set<TopicPartition>> activeTaskAssignment = new HashMap<>(taskId00Assignment);\n+        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true);\n+        expectRestoreToBeCompleted(consumer, changeLogReader);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n+            .andReturn(singletonList(task00)).once();\n+        final StateMachineTask task02 = new StateMachineTask(taskId02, taskId02Partitions, false);\n+        task02.setChangelogOffsets(task02ChangelogOffsets);\n+        expect(standbyTaskCreator.createTasks(eq(taskId02Assignment)))\n+            .andReturn(singletonList(task02)).once();\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);\n+        task01.setChangelogOffsets(task01ChangelogOffsets);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId01Assignment)))\n+            .andReturn(singletonList(task01)).once();\n \n         expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n \n-        replay(activeTaskCreator, stateDirectory);\n+        expect(stateDirectory.lock(taskId10)).andReturn(true).once();\n+        expect(stateDirectory.lock(taskId12)).andReturn(false).once();\n+\n+\n+        final File task10CheckpointFile = new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        final File task12CheckpointFile = new File(taskFolders[5], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        assertThat(task10CheckpointFile.createNewFile(), is(true));\n+        assertThat(task12CheckpointFile.createNewFile(), is(true));\n+        new OffsetCheckpoint(task10CheckpointFile).write(task10ChangelogOffsets);\n+\n+        replay(activeTaskCreator, standbyTaskCreator, consumer, changeLogReader, stateDirectory);\n+\n+        taskManager.handleAssignment(activeTaskAssignment, taskId02Assignment);\n+        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n+\n+        activeTaskAssignment.putAll(taskId01Assignment);\n+        taskManager.handleAssignment(activeTaskAssignment, taskId02Assignment);\n+\n+        assertThat(task00.state(), is(Task.State.RUNNING));\n+        assertThat(task01.state(), not(Task.State.RUNNING));\n+        assertThat(task02.state(), is(Task.State.RUNNING));\n \n         final Map<TaskId, Long> taskOffsetSums = taskManager.getTaskOffsetSums();\n \n         verify(activeTaskCreator, stateDirectory);\n-\n-        assertThat(taskOffsetSums.keySet(), equalTo(mkSet(taskId01, taskId02, new TaskId(1, 1))));\n+        assertThat(taskOffsetSums.keySet(), equalTo(mkSet(taskId00, taskId01, taskId02, taskId10)));\n+        assertEquals(task00OffsetSum, taskOffsetSums.get(taskId00));\n+        assertEquals(task01OffsetSum, taskOffsetSums.get(taskId01));\n+        assertEquals(task02OffsetSum, taskOffsetSums.get(taskId02));\n+        assertEquals(task10OffsetSum, taskOffsetSums.get(taskId10));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYwMzQzNg==", "bodyText": "req: You do not need to verify the activeTaskCreator here, since you are not testing handleAssignment().", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390603436", "createdAt": "2020-03-10T20:52:06Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );\n+\n+        final Long task00OffsetSum = Task.LATEST_OFFSET;\n+        final Long task01OffsetSum = 3L;\n+        final Long task02OffsetSum = 15L;\n+        final Long task10OffsetSum = 50L;\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        final Map<TaskId, Set<TopicPartition>> activeTaskAssignment = new HashMap<>(taskId00Assignment);\n+        final StateMachineTask task00 = new StateMachineTask(taskId00, taskId00Partitions, true);\n+        expectRestoreToBeCompleted(consumer, changeLogReader);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId00Assignment)))\n+            .andReturn(singletonList(task00)).once();\n+        final StateMachineTask task02 = new StateMachineTask(taskId02, taskId02Partitions, false);\n+        task02.setChangelogOffsets(task02ChangelogOffsets);\n+        expect(standbyTaskCreator.createTasks(eq(taskId02Assignment)))\n+            .andReturn(singletonList(task02)).once();\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);\n+        task01.setChangelogOffsets(task01ChangelogOffsets);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(taskId01Assignment)))\n+            .andReturn(singletonList(task01)).once();\n \n         expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n \n-        replay(activeTaskCreator, stateDirectory);\n+        expect(stateDirectory.lock(taskId10)).andReturn(true).once();\n+        expect(stateDirectory.lock(taskId12)).andReturn(false).once();\n+\n+\n+        final File task10CheckpointFile = new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        final File task12CheckpointFile = new File(taskFolders[5], StateManagerUtil.CHECKPOINT_FILE_NAME);\n+        assertThat(task10CheckpointFile.createNewFile(), is(true));\n+        assertThat(task12CheckpointFile.createNewFile(), is(true));\n+        new OffsetCheckpoint(task10CheckpointFile).write(task10ChangelogOffsets);\n+\n+        replay(activeTaskCreator, standbyTaskCreator, consumer, changeLogReader, stateDirectory);\n+\n+        taskManager.handleAssignment(activeTaskAssignment, taskId02Assignment);\n+        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n+\n+        activeTaskAssignment.putAll(taskId01Assignment);\n+        taskManager.handleAssignment(activeTaskAssignment, taskId02Assignment);\n+\n+        assertThat(task00.state(), is(Task.State.RUNNING));\n+        assertThat(task01.state(), not(Task.State.RUNNING));\n+        assertThat(task02.state(), is(Task.State.RUNNING));\n \n         final Map<TaskId, Long> taskOffsetSums = taskManager.getTaskOffsetSums();\n \n         verify(activeTaskCreator, stateDirectory);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYwNjA1NA==", "bodyText": "req: Could you use not the same topic partitions for the changelog topic partition as for the assigned topic partitions? It had a hard time to understand that those topic partitions are just there for convenience. At least give them new variable names with a more realistic naming. Maybe you could also vary the number of topic partitions in the maps from 1 to 3.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390606054", "createdAt": "2020-03-10T20:56:59Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {\n+        final File[] taskFolders = asList(/* SHOULD report offsets for the following cases: */\n+            testFolder.newFolder(\"0_0\"),     // active running task\n+            testFolder.newFolder(\"0_1\"),     // active non-running task\n+            testFolder.newFolder(\"0_2\"),     // standby task\n+            testFolder.newFolder(\"1_0\"),     // unowned (unlocked) task with valid checkpoint\n+                                          /* should NOT report offsets for the following: */\n+            testFolder.newFolder(\"1_1\"),     // unowned (unlocked) task without checkpoint file\n+            testFolder.newFolder(\"1_2\"),     // owned/locked by another thread\n+            testFolder.newFolder(\"dummy\"))   // some random non-task dir\n+                                       .toArray(new File[0]);\n+\n+        final Map<TopicPartition, Long> task01ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 1L),\n+            mkEntry(t1p1, 2L)\n+        );\n+        final Map<TopicPartition, Long> task02ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 5L),\n+            mkEntry(t1p1, 10L)\n+        );\n+        final Map<TopicPartition, Long> task10ChangelogOffsets = mkMap(\n+            mkEntry(t1p0, 20L),\n+            mkEntry(t1p1, 30L)\n+        );", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYxMzgxMA==", "bodyText": "req: In general I think this unit test is really large. For the sake of readability and modularization, you should split it into multiple tests. Maybe for each case two unit tests: one with a single case. Then one unit test for the composite scenario with all cases and different occurrences of the different cases. If you extract and parametrize the setup, it should not be too much code duplication. Additionally, a test where stateDirectory.listTaskDirectories() returns an empty array and a test with a stateless task are missing.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390613810", "createdAt": "2020-03-10T21:10:39Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -157,26 +165,82 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void shouldReportOffsetSumsForValidLockedTasks() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzA4NTk1", "url": "https://github.com/apache/kafka/pull/8246#pullrequestreview-372708595", "createdAt": "2020-03-11T12:22:25Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjoyMjoyNVrOF00oqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNToxMTo1N1rOF07amA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkzMjY0OQ==", "bodyText": "prop: Rename to shouldNotLockAnythingIfStateDirIsEmpty().", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390932649", "createdAt": "2020-03-11T12:22:25Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkzMzgzOQ==", "bodyText": "req: Please add a similar unit test as the one above but where stateDirectory.listTaskDirectories() returns null instead of an empty array.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390933839", "createdAt": "2020-03-11T12:24:45Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n+        verify(stateDirectory);\n+        assertTrue(taskManager.lockedTaskDirectories().isEmpty());\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkzNjQxMQ==", "bodyText": "prop: Rename to tryToLockAllTaskDirectories()", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390936411", "createdAt": "2020-03-11T12:30:08Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,50 +378,134 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryLockForAllTaskDirectories()}\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+        for (final TaskId id : lockedTaskDirectories) {\n+            final Task task = tasks.get(id);\n+            if (task != null) {\n+                if (task.isActive() && task.state() == RUNNING) {\n+                    taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                } else {\n+                    taskOffsetSums.put(id, sumOfChangelogOffsets(task.changelogOffsets()));\n+                }\n             } else {\n-                taskOffsetSums.put(id, 0L);\n+                final File checkpointFile = stateDirectory.checkpointFileFor(id);\n+                try {\n+                    // If we can't read the checkpoint file or it doesn't exist, release the task directory\n+                    // so the background cleaner thread can do its thing\n+                    if (checkpointFile.exists()) {\n+                        taskOffsetSums.put(id, sumOfChangelogOffsets(new OffsetCheckpoint(checkpointFile).read()));\n+                    } else {\n+                        releaseTaskDirLock(id);\n+                    }\n+                } catch (final IOException e) {\n+                    log.warn(String.format(\"Exception caught while trying to read checkpoint for task %s:\", id), e);\n+                    releaseTaskDirLock(id);\n+                }\n             }\n         }\n+\n         return taskOffsetSums;\n     }\n \n     /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n+     * Makes a weak attempt to lock all task directories in the state dir. We are responsible for computing and\n+     * reporting the offset sum for any unassigned tasks we obtain the lock for in the upcoming rebalance. Tasks\n+     * that we locked but didn't own will be released at the end of the rebalance (unless of course we were\n+     * assigned the task as a result of the rebalance). This method should be idempotent.\n      */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n+    private void tryLockForAllTaskDirectories() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODM1Mw==", "bodyText": "req: If the release of the lock throws, we log an error message in releaseTaskDirLock(id) but swallow the exception here. This seems to me a false alarm. Imagine you analyse the log files and find an error that actually isn't one. I think, we should suppress the log message in this case.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390948353", "createdAt": "2020-03-11T12:53:00Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,50 +378,134 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryLockForAllTaskDirectories()}\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+        for (final TaskId id : lockedTaskDirectories) {\n+            final Task task = tasks.get(id);\n+            if (task != null) {\n+                if (task.isActive() && task.state() == RUNNING) {\n+                    taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                } else {\n+                    taskOffsetSums.put(id, sumOfChangelogOffsets(task.changelogOffsets()));\n+                }\n             } else {\n-                taskOffsetSums.put(id, 0L);\n+                final File checkpointFile = stateDirectory.checkpointFileFor(id);\n+                try {\n+                    // If we can't read the checkpoint file or it doesn't exist, release the task directory\n+                    // so the background cleaner thread can do its thing\n+                    if (checkpointFile.exists()) {\n+                        taskOffsetSums.put(id, sumOfChangelogOffsets(new OffsetCheckpoint(checkpointFile).read()));\n+                    } else {\n+                        releaseTaskDirLock(id);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MzA5MQ==", "bodyText": "prop: Could you please use a else branch here?", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390953091", "createdAt": "2020-03-11T13:01:59Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,50 +378,134 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryLockForAllTaskDirectories()}\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+        for (final TaskId id : lockedTaskDirectories) {\n+            final Task task = tasks.get(id);\n+            if (task != null) {\n+                if (task.isActive() && task.state() == RUNNING) {\n+                    taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                } else {\n+                    taskOffsetSums.put(id, sumOfChangelogOffsets(task.changelogOffsets()));\n+                }\n             } else {\n-                taskOffsetSums.put(id, 0L);\n+                final File checkpointFile = stateDirectory.checkpointFileFor(id);\n+                try {\n+                    // If we can't read the checkpoint file or it doesn't exist, release the task directory\n+                    // so the background cleaner thread can do its thing\n+                    if (checkpointFile.exists()) {\n+                        taskOffsetSums.put(id, sumOfChangelogOffsets(new OffsetCheckpoint(checkpointFile).read()));\n+                    } else {\n+                        releaseTaskDirLock(id);\n+                    }\n+                } catch (final IOException e) {\n+                    log.warn(String.format(\"Exception caught while trying to read checkpoint for task %s:\", id), e);\n+                    releaseTaskDirLock(id);\n+                }\n             }\n         }\n+\n         return taskOffsetSums;\n     }\n \n     /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n+     * Makes a weak attempt to lock all task directories in the state dir. We are responsible for computing and\n+     * reporting the offset sum for any unassigned tasks we obtain the lock for in the upcoming rebalance. Tasks\n+     * that we locked but didn't own will be released at the end of the rebalance (unless of course we were\n+     * assigned the task as a result of the rebalance). This method should be idempotent.\n      */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n+    private void tryLockForAllTaskDirectories() {\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    try {\n+                        if (stateDirectory.lock(id)) {\n+                            lockedTaskDirectories.add(id);\n+                            if (!tasks.containsKey(id)) {\n+                                log.debug(\"Temporarily locked unassigned task {} for the upcoming rebalance\", id);\n+                            }\n+                        }\n+                    } catch (final IOException e) {\n+                        // if for any reason we can't lock this task dir, just move on\n+                        log.warn(String.format(\"Exception caught while attempting to lock task %s:\", id), e);\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // ignore any unknown files that sit in the same directory\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempts to release the lock for the passed in task's directory. It does not remove the task from\n+     * {@code lockedTaskDirectories} so it's safe to call during iteration, and should be idempotent.\n+     */\n+    private RuntimeException releaseTaskDirLock(final TaskId taskId) {\n+        try {\n+            stateDirectory.unlock(taskId);\n+        } catch (final IOException e) {\n+            log.error(\"Failed to release the lock for task directory {}.\", taskId);\n+            return new StreamsException(String.format(\"Unable to unlock task directory %s\", taskId), e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * We must release the lock for any unassigned tasks that we temporarily locked in preparation for a\n+     * rebalance in {@link #tryLockForAllTaskDirectories()}.\n+     */\n+    private void releaseLockedUnassignedTaskDirectories() {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        final Iterator<TaskId> taskIdIterator = lockedTaskDirectories.iterator();\n+        while (taskIdIterator.hasNext()) {\n+            final TaskId id = taskIdIterator.next();\n+\n+            if (!tasks.containsKey(id)) {\n+                final RuntimeException unlockException = releaseTaskDirLock(id);\n+                if (unlockException == null) {\n+                    taskIdIterator.remove();\n+                } else {\n+                    firstException.compareAndSet(null, unlockException);\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        final RuntimeException fatalException = firstException.get();\n+        if (fatalException != null) {\n+            throw fatalException;\n+        }\n+    }\n+\n+    private long sumOfChangelogOffsets(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0L;\n+        for (final Map.Entry<TopicPartition, Long> changelogEntry : changelogOffsets.entrySet()) {\n+            final TopicPartition changelog = changelogEntry.getKey();\n+            final long offset = changelogEntry.getValue();\n+\n+            if (offset < 0L) {\n+                if (offset == -1L) {\n+                    log.debug(\"Skipping unknown offset for changelog {}\", changelog);\n+                } else {\n+                    log.warn(\"Unexpected negative offset {} for changelog {}\", offset, changelog);\n+                }\n+                continue;\n+            }\n+\n+            offsetSum += offset;\n+            if (offsetSum < 0) {\n+                return Long.MAX_VALUE;\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk2NDc0Mg==", "bodyText": "A last comment on overflows (after this I will shut up). Probably overflows are a rare event. However, if we pin the offset sum to MAX_VALUE we are stuck there. All following assignments will have the same (probably wrong) behavior. My proposal will actually make this code simpler, because we would not do the check for negative sum. The code on the assignor would need to handle the cases I pointed out in my previous comment. The only transient mistake that we would do is because of Task.LATEST_OFFSET since this value of the sum has a special meaning. As I have already pointed out, my proposal only works if Java guarantees that MAX_VALUE is followed by MIN_VALUE.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390964742", "createdAt": "2020-03-11T13:22:03Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,50 +378,134 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryLockForAllTaskDirectories()}\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+        for (final TaskId id : lockedTaskDirectories) {\n+            final Task task = tasks.get(id);\n+            if (task != null) {\n+                if (task.isActive() && task.state() == RUNNING) {\n+                    taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                } else {\n+                    taskOffsetSums.put(id, sumOfChangelogOffsets(task.changelogOffsets()));\n+                }\n             } else {\n-                taskOffsetSums.put(id, 0L);\n+                final File checkpointFile = stateDirectory.checkpointFileFor(id);\n+                try {\n+                    // If we can't read the checkpoint file or it doesn't exist, release the task directory\n+                    // so the background cleaner thread can do its thing\n+                    if (checkpointFile.exists()) {\n+                        taskOffsetSums.put(id, sumOfChangelogOffsets(new OffsetCheckpoint(checkpointFile).read()));\n+                    } else {\n+                        releaseTaskDirLock(id);\n+                    }\n+                } catch (final IOException e) {\n+                    log.warn(String.format(\"Exception caught while trying to read checkpoint for task %s:\", id), e);\n+                    releaseTaskDirLock(id);\n+                }\n             }\n         }\n+\n         return taskOffsetSums;\n     }\n \n     /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n+     * Makes a weak attempt to lock all task directories in the state dir. We are responsible for computing and\n+     * reporting the offset sum for any unassigned tasks we obtain the lock for in the upcoming rebalance. Tasks\n+     * that we locked but didn't own will be released at the end of the rebalance (unless of course we were\n+     * assigned the task as a result of the rebalance). This method should be idempotent.\n      */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n+    private void tryLockForAllTaskDirectories() {\n         final File[] stateDirs = stateDirectory.listTaskDirectories();\n         if (stateDirs != null) {\n             for (final File dir : stateDirs) {\n                 try {\n                     final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    try {\n+                        if (stateDirectory.lock(id)) {\n+                            lockedTaskDirectories.add(id);\n+                            if (!tasks.containsKey(id)) {\n+                                log.debug(\"Temporarily locked unassigned task {} for the upcoming rebalance\", id);\n+                            }\n+                        }\n+                    } catch (final IOException e) {\n+                        // if for any reason we can't lock this task dir, just move on\n+                        log.warn(String.format(\"Exception caught while attempting to lock task %s:\", id), e);\n                     }\n                 } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                    // ignore any unknown files that sit in the same directory\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempts to release the lock for the passed in task's directory. It does not remove the task from\n+     * {@code lockedTaskDirectories} so it's safe to call during iteration, and should be idempotent.\n+     */\n+    private RuntimeException releaseTaskDirLock(final TaskId taskId) {\n+        try {\n+            stateDirectory.unlock(taskId);\n+        } catch (final IOException e) {\n+            log.error(\"Failed to release the lock for task directory {}.\", taskId);\n+            return new StreamsException(String.format(\"Unable to unlock task directory %s\", taskId), e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * We must release the lock for any unassigned tasks that we temporarily locked in preparation for a\n+     * rebalance in {@link #tryLockForAllTaskDirectories()}.\n+     */\n+    private void releaseLockedUnassignedTaskDirectories() {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        final Iterator<TaskId> taskIdIterator = lockedTaskDirectories.iterator();\n+        while (taskIdIterator.hasNext()) {\n+            final TaskId id = taskIdIterator.next();\n+\n+            if (!tasks.containsKey(id)) {\n+                final RuntimeException unlockException = releaseTaskDirLock(id);\n+                if (unlockException == null) {\n+                    taskIdIterator.remove();\n+                } else {\n+                    firstException.compareAndSet(null, unlockException);\n                 }\n             }\n         }\n \n-        return locallyStoredTasks;\n+        final RuntimeException fatalException = firstException.get();\n+        if (fatalException != null) {\n+            throw fatalException;\n+        }\n+    }\n+\n+    private long sumOfChangelogOffsets(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0L;\n+        for (final Map.Entry<TopicPartition, Long> changelogEntry : changelogOffsets.entrySet()) {\n+            final TopicPartition changelog = changelogEntry.getKey();\n+            final long offset = changelogEntry.getValue();\n+\n+            if (offset < 0L) {\n+                if (offset == -1L) {\n+                    log.debug(\"Skipping unknown offset for changelog {}\", changelog);\n+                } else {\n+                    log.warn(\"Unexpected negative offset {} for changelog {}\", offset, changelog);\n+                }\n+                continue;\n+            }\n+\n+            offsetSum += offset;\n+            if (offsetSum < 0) {\n+                return Long.MAX_VALUE;\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk2NzQ3Mw==", "bodyText": "Nice! I always forget about making fields unmodifiable when they become visible to the outside.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390967473", "createdAt": "2020-03-11T13:26:22Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -676,4 +773,8 @@ public String toString(final String indent) {\n     Set<String> producerClientIds() {\n         return activeTaskCreator.producerClientIds();\n     }\n+\n+    Set<TaskId> lockedTaskDirectories() {\n+        return Collections.unmodifiableSet(lockedTaskDirectories);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 242}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk4MzE0Mw==", "bodyText": "prop: Rename topicGroup to something more appropriate.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390983143", "createdAt": "2020-03-11T13:50:13Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfo.java", "diffHunk": "@@ -200,11 +201,19 @@ public UUID processId() {\n     public Map<TaskId, Long> taskOffsetSums() {\n         if (taskOffsetSumsCache == null) {\n             taskOffsetSumsCache = new HashMap<>();\n-            for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {\n-                for (final PartitionToOffsetSum partitionOffsetSum : topicGroup.partitionToOffsetSum()) {\n-                    taskOffsetSumsCache.put(new TaskId(topicGroup.topicGroupId(), partitionOffsetSum.partition()),\n-                                            partitionOffsetSum.offsetSum());\n+            if (data.version() >= MIN_VERSION_OFFSET_SUM_SUBSCRIPTION) {\n+                for (final TaskOffsetSum topicGroup : data.taskOffsetSums()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk4NzI5Mw==", "bodyText": "req: I am missing a unit test for version >= 7.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390987293", "createdAt": "2020-03-11T13:56:13Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/assignment/SubscriptionInfoTest.java", "diffHunk": "@@ -294,13 +295,36 @@ public void shouldEncodeAndDecodeVersion7() {\n     }\n \n     @Test\n-    public void shouldConvertTaskOffsetSumMapToTaskSetsForOlderVersion() {\n+    public void shouldConvertTaskOffsetSumMapToTaskSets() {\n         final SubscriptionInfo info =\n             new SubscriptionInfo(7, LATEST_SUPPORTED_VERSION, processId, \"localhost:80\", TASK_OFFSET_SUMS);\n         assertThat(info.prevTasks(), is(ACTIVE_TASKS));\n         assertThat(info.standbyTasks(), is(STANDBY_TASKS));\n     }\n \n+    @Test\n+    public void shouldConvertTaskSetsToTaskOffsetSumMapWithOlderSubscription() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk5MzMwMw==", "bodyText": "prop: I would use taskId01.toString() here, since you are not testing the taskId01.toString() method. Our assumption is that the folder has a name that is equal to the result of taskId01.toString() and not 0_1.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r390993303", "createdAt": "2020-03-11T14:04:49Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n+        verify(stateDirectory);\n+        assertTrue(taskManager.lockedTaskDirectories().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldTryToLockValidTaskDirsAtRebalanceStart() throws IOException {\n+        expectLockObtainedFor(taskId01);\n+        expectLockFailedFor(taskId10);\n+\n+        makeTaskFolders(\n+            \"0_1\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTAyMDk1MA==", "bodyText": "req: Specify stateDirectory as\n    @Mock(type = MockType.DEFAULT)\n    private StateDirectory stateDirectory;\n\non line 122.\nThen you can omit the expectation for stateDirectory.lock(anyObject()).\nNote that with a default mock, the tests shouldNotReportOffsetSumsForUnassignedTaskWithoutCheckpoint() and shouldReleaseLockForUnassignedTasksAfterRebalance() fail, because you do not set the expectation that the state dir unlocks some task directories. You should actually set that expectations, right?", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391020950", "createdAt": "2020-03-11T14:41:44Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTAyODMxMw==", "bodyText": "req: Please also verify stateDirectory.unlock(\"0_2\"). Only verifying lockedTaskDirectories() seems too weak to me.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391028313", "createdAt": "2020-03-11T14:51:23Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n+        verify(stateDirectory);\n+        assertTrue(taskManager.lockedTaskDirectories().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldTryToLockValidTaskDirsAtRebalanceStart() throws IOException {\n+        expectLockObtainedFor(taskId01);\n+        expectLockFailedFor(taskId10);\n+\n+        makeTaskFolders(\n+            \"0_1\",\n+            \"1_0\",\n+            \"dummy\"\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        verify(stateDirectory);\n+        assertThat(taskManager.lockedTaskDirectories(), is(singleton(taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReleaseLockForUnassignedTasksAfterRebalance() throws IOException {\n+        expectLockObtainedFor(taskId00, taskId01, taskId02);\n+\n+        makeTaskFolders(\n+                \"0_0\",  // active task\n+                \"0_1\",  // standby task\n+                \"0_2\"   // unassigned but able to lock\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01, taskId02)));\n+\n+        handleAssignment(taskId00Assignment, taskId01Assignment, emptyMap());\n+        reset(consumer);\n+        expectConsumerAssignmentPaused(consumer);\n+        replay(consumer);\n+\n+        taskManager.handleRebalanceComplete();\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTAzMjQ0MA==", "bodyText": "prop: You can remove this line. You only need expectLastCall() if you need to do some further expectation settings on a call that returns void, e.g., expectLastCall().times(3)", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391032440", "createdAt": "2020-03-11T14:56:49Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -1411,6 +1587,74 @@ public void shouldTransmitProducerMetrics() {\n         assertThat(taskManager.producerMetrics(), is(dummyProducerMetrics));\n     }\n \n+    private Map<TaskId, StateMachineTask> handleAssignment(final Map<TaskId, Set<TopicPartition>> runningActiveAssignment,\n+                                                           final Map<TaskId, Set<TopicPartition>> standbyAssignment,\n+                                                           final Map<TaskId, Set<TopicPartition>> restoringActiveAssignment) {\n+        final Set<Task> runningTasks = runningActiveAssignment.entrySet().stream()\n+                                           .map(t -> new StateMachineTask(t.getKey(), t.getValue(), true))\n+                                           .collect(Collectors.toSet());\n+        final Set<Task> standbyTasks = standbyAssignment.entrySet().stream()\n+                                           .map(t -> new StateMachineTask(t.getKey(), t.getValue(), false))\n+                                           .collect(Collectors.toSet());\n+        final Set<Task> restoringTasks = restoringActiveAssignment.entrySet().stream()\n+                                             .map(t -> new StateMachineTask(t.getKey(), t.getValue(), true))\n+                                             .collect(Collectors.toSet());\n+\n+        // Initially assign only the active tasks we want to complete restoration\n+        final Map<TaskId, Set<TopicPartition>> allActiveTasksAssignment = new HashMap<>(runningActiveAssignment);\n+        allActiveTasksAssignment.putAll(restoringActiveAssignment);\n+        final Set<Task> allActiveTasks = new HashSet<>(runningTasks);\n+        allActiveTasks.addAll(restoringTasks);\n+\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(runningActiveAssignment))).andStubReturn(runningTasks);\n+        expect(standbyTaskCreator.createTasks(eq(standbyAssignment))).andStubReturn(standbyTasks);\n+        expect(activeTaskCreator.createTasks(anyObject(), eq(allActiveTasksAssignment))).andStubReturn(allActiveTasks);\n+\n+        expectRestoreToBeCompleted(consumer, changeLogReader);\n+        replay(activeTaskCreator, standbyTaskCreator, consumer, changeLogReader);\n+\n+        taskManager.handleAssignment(runningActiveAssignment, standbyAssignment);\n+        assertThat(taskManager.tryToCompleteRestoration(), is(true));\n+\n+        taskManager.handleAssignment(allActiveTasksAssignment, standbyAssignment);\n+\n+        final Map<TaskId, StateMachineTask> allTasks = new HashMap<>();\n+\n+        // Just make sure all tasks ended up in the expected state\n+        for (final Task task : runningTasks) {\n+            assertThat(task.state(), is(Task.State.RUNNING));\n+            allTasks.put(task.id(), (StateMachineTask) task);\n+        }\n+        for (final Task task : restoringTasks) {\n+            assertThat(task.state(), not(Task.State.RUNNING));\n+            allTasks.put(task.id(), (StateMachineTask) task);\n+        }\n+        for (final Task task : standbyTasks) {\n+            assertThat(task.state(), is(Task.State.RUNNING));\n+            allTasks.put(task.id(), (StateMachineTask) task);\n+        }\n+        return allTasks;\n+    }\n+\n+    private void expectLockObtainedFor(final TaskId... tasks) throws IOException {\n+        for (final TaskId task : tasks) {\n+            expect(stateDirectory.lock(task)).andReturn(true).once();\n+        }\n+    }\n+\n+    private void expectLockFailedFor(final TaskId... tasks) throws IOException {\n+        for (final TaskId task : tasks) {\n+            expect(stateDirectory.lock(task)).andReturn(false).once();\n+        }\n+    }\n+\n+    private static void expectConsumerAssignmentPaused(final Consumer<byte[], byte[]> consumer) {\n+        final Set<TopicPartition> assignment = singleton(new TopicPartition(\"assignment\", 0));\n+        expect(consumer.assignment()).andReturn(assignment);\n+        consumer.pause(assignment);\n+        expectLastCall();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 361}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTAzNTIyNA==", "bodyText": "prop: Rename to shouldReportLatestOffsetAsOffsetSumForRunningTask", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391035224", "createdAt": "2020-03-11T15:00:25Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n+        verify(stateDirectory);\n+        assertTrue(taskManager.lockedTaskDirectories().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldTryToLockValidTaskDirsAtRebalanceStart() throws IOException {\n+        expectLockObtainedFor(taskId01);\n+        expectLockFailedFor(taskId10);\n+\n+        makeTaskFolders(\n+            \"0_1\",\n+            \"1_0\",\n+            \"dummy\"\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        verify(stateDirectory);\n+        assertThat(taskManager.lockedTaskDirectories(), is(singleton(taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReleaseLockForUnassignedTasksAfterRebalance() throws IOException {\n+        expectLockObtainedFor(taskId00, taskId01, taskId02);\n+\n+        makeTaskFolders(\n+                \"0_0\",  // active task\n+                \"0_1\",  // standby task\n+                \"0_2\"   // unassigned but able to lock\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01, taskId02)));\n+\n+        handleAssignment(taskId00Assignment, taskId01Assignment, emptyMap());\n+        reset(consumer);\n+        expectConsumerAssignmentPaused(consumer);\n+        replay(consumer);\n+\n+        taskManager.handleRebalanceComplete();\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReportLatestOffsetForRunningTask() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA0MDEwMw==", "bodyText": "req: Could you add a test for taskManager.getTaskOffsetSums() when there are no locked task directories, i.e., lockedTaskDirectories is empty?", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391040103", "createdAt": "2020-03-11T15:07:03Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n+        verify(stateDirectory);\n+        assertTrue(taskManager.lockedTaskDirectories().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldTryToLockValidTaskDirsAtRebalanceStart() throws IOException {\n+        expectLockObtainedFor(taskId01);\n+        expectLockFailedFor(taskId10);\n+\n+        makeTaskFolders(\n+            \"0_1\",\n+            \"1_0\",\n+            \"dummy\"\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        verify(stateDirectory);\n+        assertThat(taskManager.lockedTaskDirectories(), is(singleton(taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReleaseLockForUnassignedTasksAfterRebalance() throws IOException {\n+        expectLockObtainedFor(taskId00, taskId01, taskId02);\n+\n+        makeTaskFolders(\n+                \"0_0\",  // active task\n+                \"0_1\",  // standby task\n+                \"0_2\"   // unassigned but able to lock\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01, taskId02)));\n+\n+        handleAssignment(taskId00Assignment, taskId01Assignment, emptyMap());\n+        reset(consumer);\n+        expectConsumerAssignmentPaused(consumer);\n+        replay(consumer);\n+\n+        taskManager.handleRebalanceComplete();\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReportLatestOffsetForRunningTask() throws IOException {\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, Task.LATEST_OFFSET));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        handleAssignment(taskId00Assignment, emptyMap(), emptyMap());\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForNonRunningActiveTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        final StateMachineTask restoringTask = handleAssignment(\n+            emptyMap(),\n+            emptyMap(),\n+            taskId00Assignment\n+        ).get(taskId00);\n+        restoringTask.setChangelogOffsets(changelogOffsets);\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForStandbyTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        final StateMachineTask restoringTask = handleAssignment(\n+            emptyMap(),\n+            taskId00Assignment,\n+            emptyMap()\n+        ).get(taskId00);\n+        restoringTask.setChangelogOffsets(changelogOffsets);\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForUnassignedLockableTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        writeCheckpointFile(taskId00, changelogOffsets);\n+\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldNotReportOffsetSumsForUnlockableTask() throws IOException {\n+        expectLockFailedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertTrue(taskManager.getTaskOffsetSums().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldNotReportOffsetSumsForUnassignedTaskWithoutCheckpoint() throws IOException {\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        expect(stateDirectory.checkpointFileFor(taskId00)).andReturn(getCheckpointFile(taskId00));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertTrue(taskManager.getTaskOffsetSums().isEmpty());\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA0MTk1Nw==", "bodyText": "prop: Rename to shouldPinOffsetToLongMaxValueInCaseOfOverflow", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391041957", "createdAt": "2020-03-11T15:09:30Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n+        verify(stateDirectory);\n+        assertTrue(taskManager.lockedTaskDirectories().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldTryToLockValidTaskDirsAtRebalanceStart() throws IOException {\n+        expectLockObtainedFor(taskId01);\n+        expectLockFailedFor(taskId10);\n+\n+        makeTaskFolders(\n+            \"0_1\",\n+            \"1_0\",\n+            \"dummy\"\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        verify(stateDirectory);\n+        assertThat(taskManager.lockedTaskDirectories(), is(singleton(taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReleaseLockForUnassignedTasksAfterRebalance() throws IOException {\n+        expectLockObtainedFor(taskId00, taskId01, taskId02);\n+\n+        makeTaskFolders(\n+                \"0_0\",  // active task\n+                \"0_1\",  // standby task\n+                \"0_2\"   // unassigned but able to lock\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01, taskId02)));\n+\n+        handleAssignment(taskId00Assignment, taskId01Assignment, emptyMap());\n+        reset(consumer);\n+        expectConsumerAssignmentPaused(consumer);\n+        replay(consumer);\n+\n+        taskManager.handleRebalanceComplete();\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReportLatestOffsetForRunningTask() throws IOException {\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, Task.LATEST_OFFSET));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        handleAssignment(taskId00Assignment, emptyMap(), emptyMap());\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForNonRunningActiveTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        final StateMachineTask restoringTask = handleAssignment(\n+            emptyMap(),\n+            emptyMap(),\n+            taskId00Assignment\n+        ).get(taskId00);\n+        restoringTask.setChangelogOffsets(changelogOffsets);\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForStandbyTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        final StateMachineTask restoringTask = handleAssignment(\n+            emptyMap(),\n+            taskId00Assignment,\n+            emptyMap()\n+        ).get(taskId00);\n+        restoringTask.setChangelogOffsets(changelogOffsets);\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForUnassignedLockableTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        writeCheckpointFile(taskId00, changelogOffsets);\n+\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldNotReportOffsetSumsForUnlockableTask() throws IOException {\n+        expectLockFailedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertTrue(taskManager.getTaskOffsetSums().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldNotReportOffsetSumsForUnassignedTaskWithoutCheckpoint() throws IOException {\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        expect(stateDirectory.checkpointFileFor(taskId00)).andReturn(getCheckpointFile(taskId00));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertTrue(taskManager.getTaskOffsetSums().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldSkipInvalidOffsetsWhenComputingOffsetSum() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 1), -1L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 0L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        writeCheckpointFile(taskId00, changelogOffsets);\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        replay(activeTaskCreator, stateDirectory);\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n \n-        final Map<TaskId, Long> taskOffsetSums = taskManager.getTaskOffsetSums();\n+    @Test\n+    public void shouldOffsetSumOverflowToLongMaxValue() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA0MzczNg==", "bodyText": "req: Could add a test for an offset that is negative but not -1? And another test with multiple offsets of different types, i.e., negative, unknown, etc. The latter could be the composite case, we already talked about.", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391043736", "createdAt": "2020-03-11T15:11:57Z", "author": {"login": "cadonna"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -160,26 +172,190 @@ public void shouldIdempotentlyUpdateSubscriptionFromActiveAssignment() {\n     }\n \n     @Test\n-    public void shouldReturnOffsetsForAllCachedTaskIdsFromDirectory() throws IOException {\n-        final File[] taskFolders = asList(testFolder.newFolder(\"0_1\"),\n-                                          testFolder.newFolder(\"0_2\"),\n-                                          testFolder.newFolder(\"0_3\"),\n-                                          testFolder.newFolder(\"1_1\"),\n-                                          testFolder.newFolder(\"dummy\")).toArray(new File[0]);\n+    public void tryLockForAllTaskDirectoriesShouldBeNoopIfStateDirIsEmpty() throws IOException {\n+        expect(stateDirectory.listTaskDirectories()).andReturn(new File[0]).once();\n+        expect(stateDirectory.lock(anyObject()))\n+            .andThrow(new RuntimeException(\"Should not try to lock anything\")).anyTimes();\n \n-        assertThat((new File(taskFolders[0], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[1], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n-        assertThat((new File(taskFolders[3], StateManagerUtil.CHECKPOINT_FILE_NAME)).createNewFile(), is(true));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        expect(stateDirectory.listTaskDirectories()).andReturn(taskFolders).once();\n+        verify(stateDirectory);\n+        assertTrue(taskManager.lockedTaskDirectories().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldTryToLockValidTaskDirsAtRebalanceStart() throws IOException {\n+        expectLockObtainedFor(taskId01);\n+        expectLockFailedFor(taskId10);\n+\n+        makeTaskFolders(\n+            \"0_1\",\n+            \"1_0\",\n+            \"dummy\"\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        verify(stateDirectory);\n+        assertThat(taskManager.lockedTaskDirectories(), is(singleton(taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReleaseLockForUnassignedTasksAfterRebalance() throws IOException {\n+        expectLockObtainedFor(taskId00, taskId01, taskId02);\n+\n+        makeTaskFolders(\n+                \"0_0\",  // active task\n+                \"0_1\",  // standby task\n+                \"0_2\"   // unassigned but able to lock\n+        );\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01, taskId02)));\n+\n+        handleAssignment(taskId00Assignment, taskId01Assignment, emptyMap());\n+        reset(consumer);\n+        expectConsumerAssignmentPaused(consumer);\n+        replay(consumer);\n+\n+        taskManager.handleRebalanceComplete();\n+        assertThat(taskManager.lockedTaskDirectories(), is(mkSet(taskId00, taskId01)));\n+    }\n+\n+    @Test\n+    public void shouldReportLatestOffsetForRunningTask() throws IOException {\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, Task.LATEST_OFFSET));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        handleAssignment(taskId00Assignment, emptyMap(), emptyMap());\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForNonRunningActiveTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        final StateMachineTask restoringTask = handleAssignment(\n+            emptyMap(),\n+            emptyMap(),\n+            taskId00Assignment\n+        ).get(taskId00);\n+        restoringTask.setChangelogOffsets(changelogOffsets);\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForStandbyTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+        final StateMachineTask restoringTask = handleAssignment(\n+            emptyMap(),\n+            taskId00Assignment,\n+            emptyMap()\n+        ).get(taskId00);\n+        restoringTask.setChangelogOffsets(changelogOffsets);\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldComputeOffsetSumForUnassignedLockableTask() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 0), 5L),\n+            mkEntry(new TopicPartition(\"changelog\", 1), 10L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 15L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        writeCheckpointFile(taskId00, changelogOffsets);\n+\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n+\n+    @Test\n+    public void shouldNotReportOffsetSumsForUnlockableTask() throws IOException {\n+        expectLockFailedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertTrue(taskManager.getTaskOffsetSums().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldNotReportOffsetSumsForUnassignedTaskWithoutCheckpoint() throws IOException {\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        expect(stateDirectory.checkpointFileFor(taskId00)).andReturn(getCheckpointFile(taskId00));\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n+\n+        assertTrue(taskManager.getTaskOffsetSums().isEmpty());\n+    }\n+\n+    @Test\n+    public void shouldSkipInvalidOffsetsWhenComputingOffsetSum() throws IOException {\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 1), -1L)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, 0L));\n+\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        writeCheckpointFile(taskId00, changelogOffsets);\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        replay(activeTaskCreator, stateDirectory);\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n+    }\n \n-        final Map<TaskId, Long> taskOffsetSums = taskManager.getTaskOffsetSums();\n+    @Test\n+    public void shouldOffsetSumOverflowToLongMaxValue() throws IOException {\n+        final long largeOffset = Long.MAX_VALUE / 2;\n+        final Map<TopicPartition, Long> changelogOffsets = mkMap(\n+            mkEntry(new TopicPartition(\"changelog\", 1), largeOffset),\n+            mkEntry(new TopicPartition(\"changelog\", 2), largeOffset),\n+            mkEntry(new TopicPartition(\"changelog\", 3), largeOffset)\n+        );\n+        final Map<TaskId, Long> expectedOffsetSums = mkMap(mkEntry(taskId00, Long.MAX_VALUE));\n \n-        verify(activeTaskCreator, stateDirectory);\n+        expectLockObtainedFor(taskId00);\n+        makeTaskFolders(\"0_0\");\n+        writeCheckpointFile(taskId00, changelogOffsets);\n+        replay(stateDirectory);\n+        taskManager.handleRebalanceStart(singleton(\"topic\"));\n \n-        assertThat(taskOffsetSums.keySet(), equalTo(mkSet(taskId01, taskId02, new TaskId(1, 1))));\n+        assertThat(taskManager.getTaskOffsetSums(), is(expectedOffsetSums));\n     }\n ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 254}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczNjgwODU5", "url": "https://github.com/apache/kafka/pull/8246#pullrequestreview-373680859", "createdAt": "2020-03-12T15:52:16Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxNTo1MjoxNlrOF1khBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQxOTowMDoyOVrOF1rZ9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTcxNzEyNQ==", "bodyText": "new question: why bother with this complexity? The background cleaner can do its thing when we're not rebalancing, right?", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391717125", "createdAt": "2020-03-12T15:52:16Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,50 +378,134 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryLockForAllTaskDirectories()}\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+        for (final TaskId id : lockedTaskDirectories) {\n+            final Task task = tasks.get(id);\n+            if (task != null) {\n+                if (task.isActive() && task.state() == RUNNING) {\n+                    taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                } else {\n+                    taskOffsetSums.put(id, sumOfChangelogOffsets(task.changelogOffsets()));\n+                }\n             } else {\n-                taskOffsetSums.put(id, 0L);\n+                final File checkpointFile = stateDirectory.checkpointFileFor(id);\n+                try {\n+                    // If we can't read the checkpoint file or it doesn't exist, release the task directory\n+                    // so the background cleaner thread can do its thing\n+                    if (checkpointFile.exists()) {\n+                        taskOffsetSums.put(id, sumOfChangelogOffsets(new OffsetCheckpoint(checkpointFile).read()));\n+                    } else {\n+                        releaseTaskDirLock(id);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODM1Mw=="}, "originalCommit": null, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgyOTYxNw==", "bodyText": "Still unsure if this is the right logic. What if we just return an \"unknown sum\" sentinel here? Then, if any store's offset is unknown, then the task's offset sum would also be reported as \"unknown\", which would let the assignor treat it as \"not caught up\".", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391829617", "createdAt": "2020-03-12T18:59:38Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,50 +378,132 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryToLockAllTaskDirectories()}\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+        for (final TaskId id : lockedTaskDirectories) {\n+            final Task task = tasks.get(id);\n+            if (task != null) {\n+                if (task.isActive() && task.state() == RUNNING) {\n+                    taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                } else {\n+                    taskOffsetSums.put(id, sumOfChangelogOffsets(task.changelogOffsets()));\n+                }\n             } else {\n-                taskOffsetSums.put(id, 0L);\n+                final File checkpointFile = stateDirectory.checkpointFileFor(id);\n+                try {\n+                    // If we can't read the checkpoint file or it doesn't exist, release the task directory\n+                    // so the background cleaner thread can do its thing\n+                    if (checkpointFile.exists()) {\n+                        taskOffsetSums.put(id, sumOfChangelogOffsets(new OffsetCheckpoint(checkpointFile).read()));\n+                    } else {\n+                        releaseTaskDirLock(id);\n+                    }\n+                } catch (final IOException e) {\n+                    log.warn(String.format(\"Exception caught while trying to read checkpoint for task %s:\", id), e);\n+                    releaseTaskDirLock(id);\n+                }\n             }\n         }\n+\n         return taskOffsetSums;\n     }\n \n     /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n+     * Makes a weak attempt to lock all task directories in the state dir. We are responsible for computing and\n+     * reporting the offset sum for any unassigned tasks we obtain the lock for in the upcoming rebalance. Tasks\n+     * that we locked but didn't own will be released at the end of the rebalance (unless of course we were\n+     * assigned the task as a result of the rebalance). This method should be idempotent.\n      */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n-        final File[] stateDirs = stateDirectory.listTaskDirectories();\n-        if (stateDirs != null) {\n-            for (final File dir : stateDirs) {\n+    private void tryToLockAllTaskDirectories() {\n+        for (final File dir : stateDirectory.listTaskDirectories()) {\n+            try {\n+                final TaskId id = TaskId.parse(dir.getName());\n                 try {\n-                    final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    if (stateDirectory.lock(id)) {\n+                        lockedTaskDirectories.add(id);\n+                        if (!tasks.containsKey(id)) {\n+                            log.debug(\"Temporarily locked unassigned task {} for the upcoming rebalance\", id);\n+                        }\n                     }\n-                } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                } catch (final IOException e) {\n+                    // if for any reason we can't lock this task dir, just move on\n+                    log.warn(String.format(\"Exception caught while attempting to lock task %s:\", id), e);\n+                }\n+            } catch (final TaskIdFormatException e) {\n+                // ignore any unknown files that sit in the same directory\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempts to release the lock for the passed in task's directory. It does not remove the task from\n+     * {@code lockedTaskDirectories} so it's safe to call during iteration, and should be idempotent.\n+     */\n+    private RuntimeException releaseTaskDirLock(final TaskId taskId) {\n+        try {\n+            stateDirectory.unlock(taskId);\n+        } catch (final IOException e) {\n+            log.warn(String.format(\"Caught the following exception while trying to unlock task %s\", taskId), e);\n+            return new StreamsException(String.format(\"Unable to unlock task directory %s\", taskId), e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * We must release the lock for any unassigned tasks that we temporarily locked in preparation for a\n+     * rebalance in {@link #tryToLockAllTaskDirectories()}.\n+     */\n+    private void releaseLockedUnassignedTaskDirectories() {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        final Iterator<TaskId> taskIdIterator = lockedTaskDirectories.iterator();\n+        while (taskIdIterator.hasNext()) {\n+            final TaskId id = taskIdIterator.next();\n+\n+            if (!tasks.containsKey(id)) {\n+                final RuntimeException unlockException = releaseTaskDirLock(id);\n+                if (unlockException == null) {\n+                    taskIdIterator.remove();\n+                } else {\n+                    log.error(\"Failed to release the lock for task directory {}.\", id);\n+                    firstException.compareAndSet(null, unlockException);\n+                }\n+            }\n+        }\n+\n+        final RuntimeException fatalException = firstException.get();\n+        if (fatalException != null) {\n+            throw fatalException;\n+        }\n+    }\n+\n+    private long sumOfChangelogOffsets(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0L;\n+        for (final Map.Entry<TopicPartition, Long> changelogEntry : changelogOffsets.entrySet()) {\n+            final TopicPartition changelog = changelogEntry.getKey();\n+            final long offset = changelogEntry.getValue();\n+\n+            if (offset < 0L) {\n+                if (offset == -1L) {\n+                    log.debug(\"Skipping unknown offset for changelog {}\", changelog);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTgzMDAwNQ==", "bodyText": "you could avoid computing the addition twice by checking after this line if offsetSum < 0", "url": "https://github.com/apache/kafka/pull/8246#discussion_r391830005", "createdAt": "2020-03-12T19:00:29Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -368,50 +378,132 @@ void handleLostAll() {\n     }\n \n     /**\n+     * Compute the offset total summed across all stores in a task. Includes offset sum for any tasks we own the\n+     * lock for, which includes assigned and unassigned tasks we locked in {@link #tryToLockAllTaskDirectories()}\n+     *\n      * @return Map from task id to its total offset summed across all state stores\n      */\n     public Map<TaskId, Long> getTaskOffsetSums() {\n         final Map<TaskId, Long> taskOffsetSums = new HashMap<>();\n \n-        for (final TaskId id : tasksOnLocalStorage()) {\n-            if (isRunning(id)) {\n-                taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+        for (final TaskId id : lockedTaskDirectories) {\n+            final Task task = tasks.get(id);\n+            if (task != null) {\n+                if (task.isActive() && task.state() == RUNNING) {\n+                    taskOffsetSums.put(id, Task.LATEST_OFFSET);\n+                } else {\n+                    taskOffsetSums.put(id, sumOfChangelogOffsets(task.changelogOffsets()));\n+                }\n             } else {\n-                taskOffsetSums.put(id, 0L);\n+                final File checkpointFile = stateDirectory.checkpointFileFor(id);\n+                try {\n+                    // If we can't read the checkpoint file or it doesn't exist, release the task directory\n+                    // so the background cleaner thread can do its thing\n+                    if (checkpointFile.exists()) {\n+                        taskOffsetSums.put(id, sumOfChangelogOffsets(new OffsetCheckpoint(checkpointFile).read()));\n+                    } else {\n+                        releaseTaskDirLock(id);\n+                    }\n+                } catch (final IOException e) {\n+                    log.warn(String.format(\"Exception caught while trying to read checkpoint for task %s:\", id), e);\n+                    releaseTaskDirLock(id);\n+                }\n             }\n         }\n+\n         return taskOffsetSums;\n     }\n \n     /**\n-     * Returns ids of tasks whose states are kept on the local storage. This includes active, standby, and previously\n-     * assigned but not yet cleaned up tasks\n+     * Makes a weak attempt to lock all task directories in the state dir. We are responsible for computing and\n+     * reporting the offset sum for any unassigned tasks we obtain the lock for in the upcoming rebalance. Tasks\n+     * that we locked but didn't own will be released at the end of the rebalance (unless of course we were\n+     * assigned the task as a result of the rebalance). This method should be idempotent.\n      */\n-    private Set<TaskId> tasksOnLocalStorage() {\n-        // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:\n-        // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.\n-        // 2) the client has just got some tasks migrated out of itself to other clients while these task states\n-        //    have not been cleaned up yet (this can happen in a rolling bounce upgrade, for example).\n-\n-        final Set<TaskId> locallyStoredTasks = new HashSet<>();\n-\n-        final File[] stateDirs = stateDirectory.listTaskDirectories();\n-        if (stateDirs != null) {\n-            for (final File dir : stateDirs) {\n+    private void tryToLockAllTaskDirectories() {\n+        for (final File dir : stateDirectory.listTaskDirectories()) {\n+            try {\n+                final TaskId id = TaskId.parse(dir.getName());\n                 try {\n-                    final TaskId id = TaskId.parse(dir.getName());\n-                    // if the checkpoint file exists, the state is valid.\n-                    if (new File(dir, StateManagerUtil.CHECKPOINT_FILE_NAME).exists()) {\n-                        locallyStoredTasks.add(id);\n+                    if (stateDirectory.lock(id)) {\n+                        lockedTaskDirectories.add(id);\n+                        if (!tasks.containsKey(id)) {\n+                            log.debug(\"Temporarily locked unassigned task {} for the upcoming rebalance\", id);\n+                        }\n                     }\n-                } catch (final TaskIdFormatException e) {\n-                    // there may be some unknown files that sits in the same directory,\n-                    // we should ignore these files instead trying to delete them as well\n+                } catch (final IOException e) {\n+                    // if for any reason we can't lock this task dir, just move on\n+                    log.warn(String.format(\"Exception caught while attempting to lock task %s:\", id), e);\n+                }\n+            } catch (final TaskIdFormatException e) {\n+                // ignore any unknown files that sit in the same directory\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Attempts to release the lock for the passed in task's directory. It does not remove the task from\n+     * {@code lockedTaskDirectories} so it's safe to call during iteration, and should be idempotent.\n+     */\n+    private RuntimeException releaseTaskDirLock(final TaskId taskId) {\n+        try {\n+            stateDirectory.unlock(taskId);\n+        } catch (final IOException e) {\n+            log.warn(String.format(\"Caught the following exception while trying to unlock task %s\", taskId), e);\n+            return new StreamsException(String.format(\"Unable to unlock task directory %s\", taskId), e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * We must release the lock for any unassigned tasks that we temporarily locked in preparation for a\n+     * rebalance in {@link #tryToLockAllTaskDirectories()}.\n+     */\n+    private void releaseLockedUnassignedTaskDirectories() {\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+\n+        final Iterator<TaskId> taskIdIterator = lockedTaskDirectories.iterator();\n+        while (taskIdIterator.hasNext()) {\n+            final TaskId id = taskIdIterator.next();\n+\n+            if (!tasks.containsKey(id)) {\n+                final RuntimeException unlockException = releaseTaskDirLock(id);\n+                if (unlockException == null) {\n+                    taskIdIterator.remove();\n+                } else {\n+                    log.error(\"Failed to release the lock for task directory {}.\", id);\n+                    firstException.compareAndSet(null, unlockException);\n+                }\n+            }\n+        }\n+\n+        final RuntimeException fatalException = firstException.get();\n+        if (fatalException != null) {\n+            throw fatalException;\n+        }\n+    }\n+\n+    private long sumOfChangelogOffsets(final Map<TopicPartition, Long> changelogOffsets) {\n+        long offsetSum = 0L;\n+        for (final Map.Entry<TopicPartition, Long> changelogEntry : changelogOffsets.entrySet()) {\n+            final TopicPartition changelog = changelogEntry.getKey();\n+            final long offset = changelogEntry.getValue();\n+\n+            if (offset < 0L) {\n+                if (offset == -1L) {\n+                    log.debug(\"Skipping unknown offset for changelog {}\", changelog);\n+                } else {\n+                    log.warn(\"Unexpected negative offset {} for changelog {}\", offset, changelog);\n                 }\n+                continue;\n+            } else if (offsetSum + offset < 0) {\n+                return Long.MAX_VALUE;\n             }\n+\n+            offsetSum += offset;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 195}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dd0366a1866541b4ee2b1a15c68aad7d5c222095", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/dd0366a1866541b4ee2b1a15c68aad7d5c222095", "committedDate": "2020-03-12T21:16:34Z", "message": "report offset sums"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9f995fd93ee45180494892fb607617c920a7e1e0", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/9f995fd93ee45180494892fb607617c920a7e1e0", "committedDate": "2020-03-12T21:16:34Z", "message": "add test case"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9ff3dfce8cea9e8c29e8f3136f848b852b2e05d7", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/9ff3dfce8cea9e8c29e8f3136f848b852b2e05d7", "committedDate": "2020-03-12T21:16:34Z", "message": "write checkpoint for unowned task"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fc9b9433a200330cc0ab7231da78d35dba5e987f", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/fc9b9433a200330cc0ab7231da78d35dba5e987f", "committedDate": "2020-03-12T21:16:34Z", "message": "checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b10999a8b5ea381a6e4d20348c01407d0e4d7f12", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/b10999a8b5ea381a6e4d20348c01407d0e4d7f12", "committedDate": "2020-03-12T21:16:34Z", "message": "fix spelling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d96d2b9059100c5d953566dbe63f4a03ddfc38b5", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/d96d2b9059100c5d953566dbe63f4a03ddfc38b5", "committedDate": "2020-03-12T21:16:34Z", "message": "decode older subscriptions to dummy offset sums"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e96442d1fd0767d2b84cc4da434515a3c20d0bca", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/e96442d1fd0767d2b84cc4da434515a3c20d0bca", "committedDate": "2020-03-12T21:16:34Z", "message": "checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "146cff7cb4afa190d8a9feebe36acd955182affc", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/146cff7cb4afa190d8a9feebe36acd955182affc", "committedDate": "2020-03-12T21:16:34Z", "message": "github comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "801dfa740d664f12bec33f25ea08044e149fab3d", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/801dfa740d664f12bec33f25ea08044e149fab3d", "committedDate": "2020-03-12T21:16:34Z", "message": "more changes from github review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3854dbfffd4eb9f0a5b09ef310a0de93dbda14e", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/f3854dbfffd4eb9f0a5b09ef310a0de93dbda14e", "committedDate": "2020-03-12T21:16:34Z", "message": "split out lock acquisition andcheckpoint reading/offset sum computation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0cf37a4b0f8db98a0c7e6b330a9b7b48ce309eb7", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/0cf37a4b0f8db98a0c7e6b330a9b7b48ce309eb7", "committedDate": "2020-03-12T21:16:34Z", "message": "add helper to StateDirectory"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a6cad4328e56be2d79d2c8386309d36fe073401e", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/a6cad4328e56be2d79d2c8386309d36fe073401e", "committedDate": "2020-03-12T21:16:34Z", "message": "annoying but better tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d1adbee9c794e9f728c8bf1a6a7e60f33212912b", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/d1adbee9c794e9f728c8bf1a6a7e60f33212912b", "committedDate": "2020-03-12T21:16:34Z", "message": "remove original monolith test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1eb7c0c57a6a1d71b6668dc32c6bba5afb9f6ca4", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/1eb7c0c57a6a1d71b6668dc32c6bba5afb9f6ca4", "committedDate": "2020-03-12T21:16:34Z", "message": "fix NPE"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2fa42cb082fe3cd7bbb660036bdba8f9a184c07f", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/2fa42cb082fe3cd7bbb660036bdba8f9a184c07f", "committedDate": "2020-03-12T21:16:34Z", "message": "fix no-valid-checkpoint test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "042b12b2a38b5c5ec2130f2ba983165ffaf6a2a5", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/042b12b2a38b5c5ec2130f2ba983165ffaf6a2a5", "committedDate": "2020-03-12T21:16:34Z", "message": "githubreview"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d4b30920c00f675e862e1fc4ce510372d4334842", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/d4b30920c00f675e862e1fc4ce510372d4334842", "committedDate": "2020-03-12T21:16:34Z", "message": "checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf64c338e070f42779751344f3d34800fafe9b96", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/cf64c338e070f42779751344f3d34800fafe9b96", "committedDate": "2020-03-12T21:16:34Z", "message": "log warning on overflow"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "35675a7e494cdc1b06ad756d63be1262903ec444", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/35675a7e494cdc1b06ad756d63be1262903ec444", "committedDate": "2020-03-12T21:22:35Z", "message": "fix active task creator test"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "35675a7e494cdc1b06ad756d63be1262903ec444", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/35675a7e494cdc1b06ad756d63be1262903ec444", "committedDate": "2020-03-12T21:22:35Z", "message": "fix active task creator test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "298f1e23a3116e537f235f7ccb5bf0904c816f32", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/298f1e23a3116e537f235f7ccb5bf0904c816f32", "committedDate": "2020-03-12T22:24:27Z", "message": "don't attempt unlock for empty or checkpoint-less task dirs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3135d653f8c7f1148381e25cc9dc423be195c7b4", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/3135d653f8c7f1148381e25cc9dc423be195c7b4", "committedDate": "2020-03-13T19:32:31Z", "message": "remove negative offset handling to enforce in separate PR"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a757918bbbf7c4b27aa29720e540a5603f890b1e", "author": {"user": {"login": "ableegoldman", "name": "A. Sophie Blee-Goldman"}}, "url": "https://github.com/apache/kafka/commit/a757918bbbf7c4b27aa29720e540a5603f890b1e", "committedDate": "2020-03-13T22:09:19Z", "message": "fix tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0Njk1NDkz", "url": "https://github.com/apache/kafka/pull/8246#pullrequestreview-374695493", "createdAt": "2020-03-14T03:55:02Z", "commit": {"oid": "a757918bbbf7c4b27aa29720e540a5603f890b1e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0ODE5Mzc2", "url": "https://github.com/apache/kafka/pull/8246#pullrequestreview-374819376", "createdAt": "2020-03-15T17:46:04Z", "commit": {"oid": "a757918bbbf7c4b27aa29720e540a5603f890b1e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQxNzo0NjowNFrOF2gRKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNVQxNzo0NjowNFrOF2gRKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjY5NjEwNw==", "bodyText": "One last thing: Could you open another PR to add unit tests that check that the array is empty for the two edge cases?", "url": "https://github.com/apache/kafka/pull/8246#discussion_r392696107", "createdAt": "2020-03-15T17:46:04Z", "author": {"login": "cadonna"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java", "diffHunk": "@@ -347,8 +349,14 @@ private synchronized void cleanRemovedTasks(final long cleanupDelayMs,\n      * @return The list of all the existing local directories for stream tasks\n      */\n     File[] listTaskDirectories() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a757918bbbf7c4b27aa29720e540a5603f890b1e"}, "originalPosition": 31}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 145, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}