{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk4NTY0MTky", "number": 9382, "reviewThreads": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwNToyMjowM1rOErA_0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQyMDoxODo0NVrOE_yw-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzNTQwNTYyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwNToyMjowM1rOHdi_Aw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOFQxOTowODowNVrOHjtEcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDc0MzkzOQ==", "bodyText": "NIT: Should this be the first check in the if () statement ?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r500743939", "createdAt": "2020-10-07T05:22:03Z", "author": {"login": "rite2nikhil"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -432,14 +455,22 @@ abstract class AbstractFetcherThread(name: String,\n       failedPartitions.removeAll(initialFetchStates.keySet)\n \n       initialFetchStates.forKeyValue { (tp, initialFetchState) =>\n-        // We can skip the truncation step iff the leader epoch matches the existing epoch\n+        // For IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+        // For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch\n         val currentState = partitionStates.stateValue(tp)\n-        val updatedState = if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.leaderEpoch) {\n+        val updatedState = if (initialFetchState.offset >= 0 && isTruncationOnFetchSupported && initialFetchState.lastFetchedEpoch.nonEmpty) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE5OTUxMg==", "bodyText": "@rite2nikhil Thanks for the review. Did you mean changing this to:\nif (isTruncationOnFetchSupported && initialFetchState.lastFetchedEpoch.nonEmpty && initialFetchState.offset >= 0)", "url": "https://github.com/apache/kafka/pull/9382#discussion_r501199512", "createdAt": "2020-10-07T17:49:07Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -432,14 +455,22 @@ abstract class AbstractFetcherThread(name: String,\n       failedPartitions.removeAll(initialFetchStates.keySet)\n \n       initialFetchStates.forKeyValue { (tp, initialFetchState) =>\n-        // We can skip the truncation step iff the leader epoch matches the existing epoch\n+        // For IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+        // For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch\n         val currentState = partitionStates.stateValue(tp)\n-        val updatedState = if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.leaderEpoch) {\n+        val updatedState = if (initialFetchState.offset >= 0 && isTruncationOnFetchSupported && initialFetchState.lastFetchedEpoch.nonEmpty) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDc0MzkzOQ=="}, "originalCommit": null, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzIwMDYyNw==", "bodyText": "yes", "url": "https://github.com/apache/kafka/pull/9382#discussion_r507200627", "createdAt": "2020-10-18T19:08:05Z", "author": {"login": "rite2nikhil"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -432,14 +455,22 @@ abstract class AbstractFetcherThread(name: String,\n       failedPartitions.removeAll(initialFetchStates.keySet)\n \n       initialFetchStates.forKeyValue { (tp, initialFetchState) =>\n-        // We can skip the truncation step iff the leader epoch matches the existing epoch\n+        // For IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+        // For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch\n         val currentState = partitionStates.stateValue(tp)\n-        val updatedState = if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.leaderEpoch) {\n+        val updatedState = if (initialFetchState.offset >= 0 && isTruncationOnFetchSupported && initialFetchState.lastFetchedEpoch.nonEmpty) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDc0MzkzOQ=="}, "originalCommit": null, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1OTA0ODk3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMDoyNTo0OVrOHg99Kg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNDozMToyMFrOHhWChg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzMTU2Mg==", "bodyText": "Wondering if it might be better not to change this type since it is used in contexts where lastFetchedEpoch is not relevant. Following the types through here, we first have use InitialFetchState in AbstractFetcherManager:\ndef addFetcherForPartitions(partitionAndOffsets: Map[TopicPartition, InitialFetchState])\nWe then convert to OffsetAndEpoch which gets passed down to AbstractFetcherThread:\ndef addPartitions(initialFetchStates: Map[TopicPartition, OffsetAndEpoch]): Set[TopicPartition]\nThen this gets converted to PartitionFetchState. I wonder if it's possible to skip the conversion through OffsetAndEpoch and use InitialFetchState consistently? Perhaps the only reason the current code doesn't do that is that InitialFetchState includes the broker end point which is not really relevant to the fetcher thread. Maybe that's not such a big deal?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504331562", "createdAt": "2020-10-14T00:25:49Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -813,8 +852,9 @@ case class OffsetTruncationState(offset: Long, truncationCompleted: Boolean) {\n   override def toString: String = \"offset:%d-truncationCompleted:%b\".format(offset, truncationCompleted)\n }\n \n-case class OffsetAndEpoch(offset: Long, leaderEpoch: Int) {\n+case class OffsetAndEpoch(offset: Long, leaderEpoch: Int, lastFetchedEpoch: Option[Int] = None) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDcyNjE1MA==", "bodyText": "I had initially added another class because I didn't want to change OffsetAndEpoch, but I removed that because it looked like too many similar classes. Your suggestion to use InitialFetchState sounds much better, updated.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504726150", "createdAt": "2020-10-14T14:31:20Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -813,8 +852,9 @@ case class OffsetTruncationState(offset: Long, truncationCompleted: Boolean) {\n   override def toString: String = \"offset:%d-truncationCompleted:%b\".format(offset, truncationCompleted)\n }\n \n-case class OffsetAndEpoch(offset: Long, leaderEpoch: Int) {\n+case class OffsetAndEpoch(offset: Long, leaderEpoch: Int, lastFetchedEpoch: Option[Int] = None) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzMTU2Mg=="}, "originalCommit": null, "originalPosition": 203}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1OTA1NjUwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMDoyOTo1NlrOHg-Bdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMDoyOTo1NlrOHg-Bdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDMzMjY2Mg==", "bodyText": "nit: unnecessary parenthesis", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504332662", "createdAt": "2020-10-14T00:29:56Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -432,14 +455,22 @@ abstract class AbstractFetcherThread(name: String,\n       failedPartitions.removeAll(initialFetchStates.keySet)\n \n       initialFetchStates.forKeyValue { (tp, initialFetchState) =>\n-        // We can skip the truncation step iff the leader epoch matches the existing epoch\n+        // For IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+        // For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch\n         val currentState = partitionStates.stateValue(tp)\n-        val updatedState = if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.leaderEpoch) {\n+        val updatedState = if (initialFetchState.offset >= 0 && isTruncationOnFetchSupported && initialFetchState.lastFetchedEpoch.nonEmpty) {\n+          if (currentState != null)\n+            currentState\n+          else\n+            PartitionFetchState(initialFetchState.offset, None, initialFetchState.leaderEpoch,\n+              state = Fetching, initialFetchState.lastFetchedEpoch)\n+        } else if (currentState != null && (currentState.currentLeaderEpoch == initialFetchState.leaderEpoch)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1OTExNDA0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMTowMjozN1rOHg-jBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMTowMjozN1rOHg-jBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0MTI1Mg==", "bodyText": "Borderline overkill perhaps, but we could check if epochEndOffsets is non-empty before acquiring the lock", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504341252", "createdAt": "2020-10-14T01:02:37Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -225,6 +227,20 @@ abstract class AbstractFetcherThread(name: String,\n     }\n   }\n \n+  private def truncateOnFetchResponse(responseData: Map[TopicPartition, FetchData]): Unit = {\n+    val epochEndOffsets = responseData\n+      .filter { case (tp, fetchData) => fetchData.error == Errors.NONE && fetchData.divergingEpoch.isPresent }\n+      .map { case (tp, fetchData) =>\n+        val divergingEpoch = fetchData.divergingEpoch.get\n+        tp -> new EpochEndOffset(Errors.NONE, divergingEpoch.epoch, divergingEpoch.endOffset)\n+      }.toMap\n+    inLock(partitionMapLock) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1OTEzMTY4OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMToxMjoxOVrOHg-s1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMToxMjoxOVrOHg-s1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0Mzc2NQ==", "bodyText": "Rather than doing an additional pass over the response partitions, would it be reasonable to build epochEndOffsets inline with the other error handling in processFetchRequest?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504343765", "createdAt": "2020-10-14T01:12:19Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -225,6 +227,20 @@ abstract class AbstractFetcherThread(name: String,\n     }\n   }\n \n+  private def truncateOnFetchResponse(responseData: Map[TopicPartition, FetchData]): Unit = {\n+    val epochEndOffsets = responseData", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1OTE0NDE0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMToxOTo0M1rOHg-0Og==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNDozMjo1MVrOHhWHVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0NTY1OA==", "bodyText": "Can be done separately, but it would be nice to figure out how to move this logic into ReplicaAlterLogDirManager since this comment seems to only make sense if we assume this is the log dir fetcher and reconciliation with the leader has already completed.\nIn fact, I wonder if it is possible to get rid of this code entirely. If the log dir fetcher is also tracking lastFetchedEpoch, then we could rely on detecting truncation dynamically through ReplicaManager.fetchMessages instead of the current somewhat clumsy coordination with the replica fetcher.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504345658", "createdAt": "2020-10-14T01:19:43Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -408,9 +428,12 @@ abstract class AbstractFetcherThread(name: String,\n   def markPartitionsForTruncation(topicPartition: TopicPartition, truncationOffset: Long): Unit = {\n     partitionMapLock.lockInterruptibly()\n     try {\n+      // It is safe to reset `lastFetchedEpoch` here since we don't expect diverging offsets", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDcyNzM4MQ==", "bodyText": "Updated. The method is still there for older versions, but it is now disabled with IBP 2.7.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504727381", "createdAt": "2020-10-14T14:32:51Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -408,9 +428,12 @@ abstract class AbstractFetcherThread(name: String,\n   def markPartitionsForTruncation(topicPartition: TopicPartition, truncationOffset: Long): Unit = {\n     partitionMapLock.lockInterruptibly()\n     try {\n+      // It is safe to reset `lastFetchedEpoch` here since we don't expect diverging offsets", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0NTY1OA=="}, "originalCommit": null, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1OTE1ODg3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMToyODo0N1rOHg-9Vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNDozNTo0MVrOHhWQzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0Nzk5MA==", "bodyText": "Below we only use currentState if the current epoch matches the initial epoch. Why is it safe to skip that check here?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504347990", "createdAt": "2020-10-14T01:28:47Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -432,14 +455,22 @@ abstract class AbstractFetcherThread(name: String,\n       failedPartitions.removeAll(initialFetchStates.keySet)\n \n       initialFetchStates.forKeyValue { (tp, initialFetchState) =>\n-        // We can skip the truncation step iff the leader epoch matches the existing epoch\n+        // For IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+        // For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch\n         val currentState = partitionStates.stateValue(tp)\n-        val updatedState = if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.leaderEpoch) {\n+        val updatedState = if (initialFetchState.offset >= 0 && isTruncationOnFetchSupported && initialFetchState.lastFetchedEpoch.nonEmpty) {\n+          if (currentState != null)\n+            currentState", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDcyOTgwNw==", "bodyText": "I refactored this code a bit and added a check for Fetching state. Not sure if I have missed something though. I think we can continue to fetch without truncating if currentState is Fetching when lastFetchedEpoch is known. If we need to truncate, we will do that later when we get told about diverging epochs. Does that make sense?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504729807", "createdAt": "2020-10-14T14:35:41Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -432,14 +455,22 @@ abstract class AbstractFetcherThread(name: String,\n       failedPartitions.removeAll(initialFetchStates.keySet)\n \n       initialFetchStates.forKeyValue { (tp, initialFetchState) =>\n-        // We can skip the truncation step iff the leader epoch matches the existing epoch\n+        // For IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+        // For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch\n         val currentState = partitionStates.stateValue(tp)\n-        val updatedState = if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.leaderEpoch) {\n+        val updatedState = if (initialFetchState.offset >= 0 && isTruncationOnFetchSupported && initialFetchState.lastFetchedEpoch.nonEmpty) {\n+          if (currentState != null)\n+            currentState", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0Nzk5MA=="}, "originalCommit": null, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1OTE2ODUxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/DelayedFetch.scala", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwMTozNDozMFrOHg_C4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNjoxMjozNlrOHhawWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0OTQwOQ==", "bodyText": "Good catch here and in FetchSession. Do you think we should consider doing these fixes separately so that we can get them into 2.7? Otherwise it might be difficult to tie this behavior to the 2.7 IBP.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504349409", "createdAt": "2020-10-14T01:34:30Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/DelayedFetch.scala", "diffHunk": "@@ -77,6 +78,7 @@ class DelayedFetch(delayMs: Long,\n    * Case E: This broker is the leader, but the requested epoch is now fenced\n    * Case F: The fetch offset locates not on the last segment of the log\n    * Case G: The accumulated bytes from all the fetching partitions exceeds the minimum bytes\n+   * Case H: A diverging epoch was found, return response to trigger truncation", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDczMTI2Nw==", "bodyText": "Makes sense, will submit another PR with just those changes.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504731267", "createdAt": "2020-10-14T14:37:24Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/DelayedFetch.scala", "diffHunk": "@@ -77,6 +78,7 @@ class DelayedFetch(delayMs: Long,\n    * Case E: This broker is the leader, but the requested epoch is now fenced\n    * Case F: The fetch offset locates not on the last segment of the log\n    * Case G: The accumulated bytes from all the fetching partitions exceeds the minimum bytes\n+   * Case H: A diverging epoch was found, return response to trigger truncation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0OTQwOQ=="}, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDgwMzQwOQ==", "bodyText": "PR with the changes in DelayedFetch and FetchSession: #9434", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504803409", "createdAt": "2020-10-14T16:12:36Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/DelayedFetch.scala", "diffHunk": "@@ -77,6 +78,7 @@ class DelayedFetch(delayMs: Long,\n    * Case E: This broker is the leader, but the requested epoch is now fenced\n    * Case F: The fetch offset locates not on the last segment of the log\n    * Case G: The accumulated bytes from all the fetching partitions exceeds the minimum bytes\n+   * Case H: A diverging epoch was found, return response to trigger truncation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0OTQwOQ=="}, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDgwMzQxOA==", "bodyText": "PR with the changes in DelayedFetch and FetchSession: #9434", "url": "https://github.com/apache/kafka/pull/9382#discussion_r504803418", "createdAt": "2020-10-14T16:12:36Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/DelayedFetch.scala", "diffHunk": "@@ -77,6 +78,7 @@ class DelayedFetch(delayMs: Long,\n    * Case E: This broker is the leader, but the requested epoch is now fenced\n    * Case F: The fetch offset locates not on the last segment of the log\n    * Case G: The accumulated bytes from all the fetching partitions exceeds the minimum bytes\n+   * Case H: A diverging epoch was found, return response to trigger truncation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM0OTQwOQ=="}, "originalCommit": null, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MTQ1NDE2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNzoyMzoxNVrOHl3njg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNzo1Mzo0NVrOHnX1Aw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ3MDYwNg==", "bodyText": "Is it not possible that the InitialFetchState has a bump to the current leader epoch? We will still need the latest epoch in order to continue fetching.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r509470606", "createdAt": "2020-10-21T17:23:15Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -426,21 +454,42 @@ abstract class AbstractFetcherThread(name: String,\n     warn(s\"Partition $topicPartition marked as failed\")\n   }\n \n-  def addPartitions(initialFetchStates: Map[TopicPartition, OffsetAndEpoch]): Set[TopicPartition] = {\n+  /**\n+   * Returns initial partition fetch state based on current state and the provided `initialFetchState`.\n+   * From IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+   * For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch.\n+   */\n+  private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\n+    if (isTruncationOnFetchSupported && initialFetchState.initOffset >= 0 && initialFetchState.lastFetchedEpoch.nonEmpty) {\n+      if (currentState == null) {\n+        return PartitionFetchState(initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\n+          state = Fetching, initialFetchState.lastFetchedEpoch)\n+      }\n+      // If we are in `Fetching` state can continue to fetch regardless of current leader epoch and truncate\n+      // if necessary based on diverging epochs returned by the leader. If we are currently in Truncating state,\n+      // fall through and handle based on current epoch.\n+      if (currentState.state == Fetching) {\n+        return currentState", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTA0NjkxNQ==", "bodyText": "Updated.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r511046915", "createdAt": "2020-10-23T17:53:45Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -426,21 +454,42 @@ abstract class AbstractFetcherThread(name: String,\n     warn(s\"Partition $topicPartition marked as failed\")\n   }\n \n-  def addPartitions(initialFetchStates: Map[TopicPartition, OffsetAndEpoch]): Set[TopicPartition] = {\n+  /**\n+   * Returns initial partition fetch state based on current state and the provided `initialFetchState`.\n+   * From IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+   * For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch.\n+   */\n+  private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\n+    if (isTruncationOnFetchSupported && initialFetchState.initOffset >= 0 && initialFetchState.lastFetchedEpoch.nonEmpty) {\n+      if (currentState == null) {\n+        return PartitionFetchState(initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\n+          state = Fetching, initialFetchState.lastFetchedEpoch)\n+      }\n+      // If we are in `Fetching` state can continue to fetch regardless of current leader epoch and truncate\n+      // if necessary based on diverging epochs returned by the leader. If we are currently in Truncating state,\n+      // fall through and handle based on current epoch.\n+      if (currentState.state == Fetching) {\n+        return currentState", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ3MDYwNg=="}, "originalCommit": null, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MTQ2NzA2OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNzoyNTozMFrOHl3wVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNzoyNTozMFrOHl3wVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ3Mjg1NA==", "bodyText": "Do we need to adjust this? I think we want to remain in the Fetching state if truncation detection is through Fetch.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r509472854", "createdAt": "2020-10-21T17:25:30Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -461,8 +510,9 @@ abstract class AbstractFetcherThread(name: String,\n         val maybeTruncationComplete = fetchOffsets.get(topicPartition) match {\n           case Some(offsetTruncationState) =>\n             val state = if (offsetTruncationState.truncationCompleted) Fetching else Truncating", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 157}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MTUyMjEzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNzozMzoxNFrOHl4VXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNzo1NDowNFrOHnX1xQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ4MjMzMg==", "bodyText": "This is a little unclear to me. I guess it is safe to reset lastFetchedEpoch as long as we reinitialize it after the next leader change. On the other hand, it seems safer to always retain the value.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r509482332", "createdAt": "2020-10-21T17:33:14Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -461,8 +510,9 @@ abstract class AbstractFetcherThread(name: String,\n         val maybeTruncationComplete = fetchOffsets.get(topicPartition) match {\n           case Some(offsetTruncationState) =>\n             val state = if (offsetTruncationState.truncationCompleted) Fetching else Truncating\n+            // Resetting `lastFetchedEpoch` since we are truncating and don't expect diverging epoch in the next fetch", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTA0NzEwOQ==", "bodyText": "Updated.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r511047109", "createdAt": "2020-10-23T17:54:04Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -461,8 +510,9 @@ abstract class AbstractFetcherThread(name: String,\n         val maybeTruncationComplete = fetchOffsets.get(topicPartition) match {\n           case Some(offsetTruncationState) =>\n             val state = if (offsetTruncationState.truncationCompleted) Fetching else Truncating\n+            // Resetting `lastFetchedEpoch` since we are truncating and don't expect diverging epoch in the next fetch", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ4MjMzMg=="}, "originalCommit": null, "originalPosition": 158}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MTUzNTU0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNzozNTozNFrOHl4egg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNzo1NDo0M1rOHnX3Vg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ4NDY3NA==", "bodyText": "Again it seems safe to keep lastFetchedEpoch in sync with the local log. If we have done a full truncation above, then lastFetchedEpoch will be None, but otherwise it seems like we should set it.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r509484674", "createdAt": "2020-10-21T17:35:34Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -629,7 +680,9 @@ abstract class AbstractFetcherThread(name: String,\n \n       val initialLag = leaderEndOffset - offsetToFetch\n       fetcherLagStats.getAndMaybePut(topicPartition).lag = initialLag\n-      PartitionFetchState(offsetToFetch, Some(initialLag), currentLeaderEpoch, state = Fetching)\n+      // We don't expect diverging epochs from the next fetch request, so resetting `lastFetchedEpoch`", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTA0NzUxMA==", "bodyText": "Updated", "url": "https://github.com/apache/kafka/pull/9382#discussion_r511047510", "createdAt": "2020-10-23T17:54:43Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -629,7 +680,9 @@ abstract class AbstractFetcherThread(name: String,\n \n       val initialLag = leaderEndOffset - offsetToFetch\n       fetcherLagStats.getAndMaybePut(topicPartition).lag = initialLag\n-      PartitionFetchState(offsetToFetch, Some(initialLag), currentLeaderEpoch, state = Fetching)\n+      // We don't expect diverging epochs from the next fetch request, so resetting `lastFetchedEpoch`", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ4NDY3NA=="}, "originalCommit": null, "originalPosition": 180}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MTU4NTc1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNzo0Mzo1NVrOHl4_ww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxNzowNToxOFrOHpHG8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ5MzE4Nw==", "bodyText": "Do we need to initialize lastFetchedEpoch? It seems like the log may not be empty at this point.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r509493187", "createdAt": "2020-10-21T17:43:55Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -770,7 +770,7 @@ class ReplicaManager(val config: KafkaConfig,\n             logManager.abortAndPauseCleaning(topicPartition)\n \n             val initialFetchState = InitialFetchState(BrokerEndPoint(config.brokerId, \"localhost\", -1),\n-              partition.getLeaderEpoch, futureLog.highWatermark)\n+              partition.getLeaderEpoch, futureLog.highWatermark, lastFetchedEpoch = None)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTA1MTgwOA==", "bodyText": "I had reset this because ReassignPartitionsIntegrationTest.testAlterLogDirReassignmentThrottle was failing consistently when the offset was test. Having spent a whole day looking at what I had broken, I think it is an existing issue. The test itself looks new and I can recreate it on trunk. Looks like https://issues.apache.org/jira/browse/KAFKA-9087. Will look into that separately. For now, I have updated this path.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r511051808", "createdAt": "2020-10-23T18:02:52Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -770,7 +770,7 @@ class ReplicaManager(val config: KafkaConfig,\n             logManager.abortAndPauseCleaning(topicPartition)\n \n             val initialFetchState = InitialFetchState(BrokerEndPoint(config.brokerId, \"localhost\", -1),\n-              partition.getLeaderEpoch, futureLog.highWatermark)\n+              partition.getLeaderEpoch, futureLog.highWatermark, lastFetchedEpoch = None)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ5MzE4Nw=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE3NDgzOA==", "bodyText": "Looking at this again, I think a bit more work is required to set the offsets and epoch correctly for AlterLogDirsThread in order to use lastFetchedEpoch. So I have reverted the changes for ReplicaAlterLogDirsThread. Will do that in a follow-on PR instead. In this PR, we will use the old truncation path in this case.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r511174838", "createdAt": "2020-10-23T22:02:56Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -770,7 +770,7 @@ class ReplicaManager(val config: KafkaConfig,\n             logManager.abortAndPauseCleaning(topicPartition)\n \n             val initialFetchState = InitialFetchState(BrokerEndPoint(config.brokerId, \"localhost\", -1),\n-              partition.getLeaderEpoch, futureLog.highWatermark)\n+              partition.getLeaderEpoch, futureLog.highWatermark, lastFetchedEpoch = None)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ5MzE4Nw=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjg3MDEzMA==", "bodyText": "Sounds fair.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r512870130", "createdAt": "2020-10-27T17:05:18Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -770,7 +770,7 @@ class ReplicaManager(val config: KafkaConfig,\n             logManager.abortAndPauseCleaning(topicPartition)\n \n             val initialFetchState = InitialFetchState(BrokerEndPoint(config.brokerId, \"localhost\", -1),\n-              partition.getLeaderEpoch, futureLog.highWatermark)\n+              partition.getLeaderEpoch, futureLog.highWatermark, lastFetchedEpoch = None)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTQ5MzE4Nw=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MTY0NjkxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxNzo1MDozNVrOHl5quQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QxNzo1NTowN1rOHnX4TQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTUwNDE4NQ==", "bodyText": "This doesn't seem right. The last fetched epoch is supposed to represent the epoch of the last fetched batch. The fetcher could be fetching the data from an older epoch here.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r509504185", "createdAt": "2020-10-21T17:50:35Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -341,11 +352,18 @@ abstract class AbstractFetcherThread(name: String,\n                       // ReplicaDirAlterThread may have removed topicPartition from the partitionStates after processing the partition data\n                       if (validBytes > 0 && partitionStates.contains(topicPartition)) {\n                         // Update partitionStates only if there is no exception during processPartitionData\n-                        val newFetchState = PartitionFetchState(nextOffset, Some(lag), currentFetchState.currentLeaderEpoch, state = Fetching)\n+                        val newFetchState = PartitionFetchState(nextOffset, Some(lag),\n+                          currentFetchState.currentLeaderEpoch, state = Fetching,\n+                          Some(currentFetchState.currentLeaderEpoch))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTA0Nzc1Nw==", "bodyText": "Oops, fixed.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r511047757", "createdAt": "2020-10-23T17:55:07Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -341,11 +352,18 @@ abstract class AbstractFetcherThread(name: String,\n                       // ReplicaDirAlterThread may have removed topicPartition from the partitionStates after processing the partition data\n                       if (validBytes > 0 && partitionStates.contains(topicPartition)) {\n                         // Update partitionStates only if there is no exception during processPartitionData\n-                        val newFetchState = PartitionFetchState(nextOffset, Some(lag), currentFetchState.currentLeaderEpoch, state = Fetching)\n+                        val newFetchState = PartitionFetchState(nextOffset, Some(lag),\n+                          currentFetchState.currentLeaderEpoch, state = Fetching,\n+                          Some(currentFetchState.currentLeaderEpoch))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTUwNDE4NQ=="}, "originalCommit": null, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMDI0MTIzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/log/Log.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwMTo1NTozM1rOHoolew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwMTo1NTozM1rOHoolew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM3MDA0Mw==", "bodyText": "nit: not sure how much it matters, but maybe we can avoid the extra garbage and just use an integer until we're ready to build the result?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r512370043", "createdAt": "2020-10-27T01:55:33Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1388,6 +1390,7 @@ class Log(@volatile private var _dir: File,\n     var validBytesCount = 0\n     var firstOffset: Option[Long] = None\n     var lastOffset = -1L\n+    var lastLeaderEpoch: Option[Int] = None", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMzUyOTI0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxNzoyNDozMlrOHpH70w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxNzoyNDozMlrOHpH70w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjg4MzY2Nw==", "bodyText": "nit: line below is misaligned", "url": "https://github.com/apache/kafka/pull/9382#discussion_r512883667", "createdAt": "2020-10-27T17:24:32Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1667,8 +1667,9 @@ class ReplicaManager(val config: KafkaConfig,\n         val partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map { partition =>\n           val leader = metadataCache.getAliveBrokers.find(_.id == partition.leaderReplicaIdOpt.get).get\n             .brokerEndPoint(config.interBrokerListenerName)\n-          val fetchOffset = partition.localLogOrException.highWatermark\n-          partition.topicPartition -> InitialFetchState(leader, partition.getLeaderEpoch, fetchOffset)\n+          val log = partition.localLogOrException\n+          val (fetchOffset, lastFetchedEpoch) = initialFetchOffsetAndEpoch(log)\n+          partition.topicPartition -> InitialFetchState(leader, partition.getLeaderEpoch, fetchOffset, lastFetchedEpoch)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMzU0ODUzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxNzoyNzo1NVrOHpIHpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowNzowOFrOH9X7Dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjg4NjY5NQ==", "bodyText": "I am wondering in what situation we would find currentState non-null here. The current logic in ReplicaManager.makeFollowers always calls removeFetcherForPartitions before adding the partition back. The reason I ask is that I wasn't sure we should be taking the last fetched epoch from the initial state or if we should keep the current one. It seems like the latter might be more current?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r512886695", "createdAt": "2020-10-27T17:27:55Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -426,21 +451,34 @@ abstract class AbstractFetcherThread(name: String,\n     warn(s\"Partition $topicPartition marked as failed\")\n   }\n \n-  def addPartitions(initialFetchStates: Map[TopicPartition, OffsetAndEpoch]): Set[TopicPartition] = {\n+  /**\n+   * Returns initial partition fetch state based on current state and the provided `initialFetchState`.\n+   * From IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+   * For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch.\n+   */\n+  private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\n+    if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) {\n+      currentState\n+    } else if (isTruncationOnFetchSupported && initialFetchState.initOffset >= 0 && initialFetchState.lastFetchedEpoch.nonEmpty &&\n+              (currentState == null || currentState.state == Fetching)) {\n+      PartitionFetchState(initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\n+          state = Fetching, initialFetchState.lastFetchedEpoch)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODE5ODc1Mw==", "bodyText": "I couldn't come up with a case where currentState is non-null, so I removed that check. Haven't seen any test failures as a result, so hopefully that is ok.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r518198753", "createdAt": "2020-11-05T16:46:18Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -426,21 +451,34 @@ abstract class AbstractFetcherThread(name: String,\n     warn(s\"Partition $topicPartition marked as failed\")\n   }\n \n-  def addPartitions(initialFetchStates: Map[TopicPartition, OffsetAndEpoch]): Set[TopicPartition] = {\n+  /**\n+   * Returns initial partition fetch state based on current state and the provided `initialFetchState`.\n+   * From IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+   * For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch.\n+   */\n+  private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\n+    if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) {\n+      currentState\n+    } else if (isTruncationOnFetchSupported && initialFetchState.initOffset >= 0 && initialFetchState.lastFetchedEpoch.nonEmpty &&\n+              (currentState == null || currentState.state == Fetching)) {\n+      PartitionFetchState(initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\n+          state = Fetching, initialFetchState.lastFetchedEpoch)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjg4NjY5NQ=="}, "originalCommit": null, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY0ODcyMg==", "bodyText": "This check is a still a little hard to follow. I think we expect that if initOffset is negative, then lastFetchedEpoch will be empty and we will hit the fetchOffsetAndTruncate case below. Is that right? On the other hand, if lastFetchedEpoch is empty, then initOffset could still be non-negative if we have an old message format, which means we need to enter Truncating so that we can truncate to the high watermark.\nOne case that is not so clear is when currentState is non-null. Then we will enter the Truncating state below regardless whether isTruncationOnFetchSupported is set or not. Is that what we want?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r533648722", "createdAt": "2020-12-01T18:57:59Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -426,21 +451,34 @@ abstract class AbstractFetcherThread(name: String,\n     warn(s\"Partition $topicPartition marked as failed\")\n   }\n \n-  def addPartitions(initialFetchStates: Map[TopicPartition, OffsetAndEpoch]): Set[TopicPartition] = {\n+  /**\n+   * Returns initial partition fetch state based on current state and the provided `initialFetchState`.\n+   * From IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+   * For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch.\n+   */\n+  private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\n+    if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) {\n+      currentState\n+    } else if (isTruncationOnFetchSupported && initialFetchState.initOffset >= 0 && initialFetchState.lastFetchedEpoch.nonEmpty &&\n+              (currentState == null || currentState.state == Fetching)) {\n+      PartitionFetchState(initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\n+          state = Fetching, initialFetchState.lastFetchedEpoch)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjg4NjY5NQ=="}, "originalCommit": null, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNzEzNQ==", "bodyText": "@hachikuji Thanks for the review! You mentioned in another comment below that we could use latestEpoch instead of using the epoch from InitialFetchOffset. I have removed lastFetchEpoch from InitialFetchOffset and updated this logic to use latestEpoch. It does look neater now, hope I haven't missed any cases. I had to update some of the unit tests which rely on the initial fetch epoch request to use IBP 2.6, but all tests have passed in my local run. I will kick off a system test run as well.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r534117135", "createdAt": "2020-12-02T12:07:08Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -426,21 +451,34 @@ abstract class AbstractFetcherThread(name: String,\n     warn(s\"Partition $topicPartition marked as failed\")\n   }\n \n-  def addPartitions(initialFetchStates: Map[TopicPartition, OffsetAndEpoch]): Set[TopicPartition] = {\n+  /**\n+   * Returns initial partition fetch state based on current state and the provided `initialFetchState`.\n+   * From IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+   * For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch.\n+   */\n+  private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\n+    if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) {\n+      currentState\n+    } else if (isTruncationOnFetchSupported && initialFetchState.initOffset >= 0 && initialFetchState.lastFetchedEpoch.nonEmpty &&\n+              (currentState == null || currentState.state == Fetching)) {\n+      PartitionFetchState(initialFetchState.initOffset, None, initialFetchState.currentLeaderEpoch,\n+          state = Fetching, initialFetchState.lastFetchedEpoch)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjg4NjY5NQ=="}, "originalCommit": null, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNDAzNjEwOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxOToyMjoyNFrOHpM40g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxOTo1Mzo1NFrOHuUHPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk2NDgxOA==", "bodyText": "It's not clear to me why we set maySkipTruncation to false here. If the truncation is not complete, wouldn't that put us in the Truncating state?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r512964818", "createdAt": "2020-10-27T19:22:24Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -221,7 +223,15 @@ abstract class AbstractFetcherThread(name: String,\n \n       val ResultWithPartitions(fetchOffsets, partitionsWithError) = maybeTruncateToEpochEndOffsets(epochEndOffsets, latestEpochsForPartitions)\n       handlePartitionsWithErrors(partitionsWithError, \"truncateToEpochEndOffsets\")\n-      updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets)\n+      updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets, isTruncationOnFetchSupported)\n+    }\n+  }\n+\n+  private def truncateOnFetchResponse(epochEndOffsets: Map[TopicPartition, EpochEndOffset]): Unit = {\n+    inLock(partitionMapLock) {\n+      val ResultWithPartitions(fetchOffsets, partitionsWithError) = maybeTruncateToEpochEndOffsets(epochEndOffsets, Map.empty)\n+      handlePartitionsWithErrors(partitionsWithError, \"truncateOnFetchResponse\")\n+      updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets, maySkipTruncation = false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODMyNjA3OQ==", "bodyText": "Fixed, removed that flag.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r518326079", "createdAt": "2020-11-05T19:53:54Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -221,7 +223,15 @@ abstract class AbstractFetcherThread(name: String,\n \n       val ResultWithPartitions(fetchOffsets, partitionsWithError) = maybeTruncateToEpochEndOffsets(epochEndOffsets, latestEpochsForPartitions)\n       handlePartitionsWithErrors(partitionsWithError, \"truncateToEpochEndOffsets\")\n-      updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets)\n+      updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets, isTruncationOnFetchSupported)\n+    }\n+  }\n+\n+  private def truncateOnFetchResponse(epochEndOffsets: Map[TopicPartition, EpochEndOffset]): Unit = {\n+    inLock(partitionMapLock) {\n+      val ResultWithPartitions(fetchOffsets, partitionsWithError) = maybeTruncateToEpochEndOffsets(epochEndOffsets, Map.empty)\n+      handlePartitionsWithErrors(partitionsWithError, \"truncateOnFetchResponse\")\n+      updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets, maySkipTruncation = false)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk2NDgxOA=="}, "originalCommit": null, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxNDA1Mjg0OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxOToyNToyOVrOHpNDaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQxNjo0OToxN1rOHuMeHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk2NzUyOQ==", "bodyText": "I'm a little uncertain about this case. If we have truncated to an earlier offset, wouldn't we also need to reset last fetched epoch? I am thinking we should remove this check and modify the first one:\nval (state, lastFetchedEpoch) = if (maySkipTruncation || offsetTruncationState.truncationCompleted)\n  (Fetching, latestEpoch(topicPartition))\nI might be missing something though.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r512967529", "createdAt": "2020-10-27T19:25:29Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -454,15 +492,23 @@ abstract class AbstractFetcherThread(name: String,\n     * truncation completed if their offsetTruncationState indicates truncation completed\n     *\n     * @param fetchOffsets the partitions to update fetch offset and maybe mark truncation complete\n+    * @param maySkipTruncation true if we can stay in Fetching mode and perform truncation later based on\n+   *                           diverging epochs from fetch responses.\n     */\n-  private def updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets: Map[TopicPartition, OffsetTruncationState]): Unit = {\n+  private def updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets: Map[TopicPartition, OffsetTruncationState],\n+                                                              maySkipTruncation: Boolean): Unit = {\n     val newStates: Map[TopicPartition, PartitionFetchState] = partitionStates.partitionStateMap.asScala\n       .map { case (topicPartition, currentFetchState) =>\n         val maybeTruncationComplete = fetchOffsets.get(topicPartition) match {\n           case Some(offsetTruncationState) =>\n-            val state = if (offsetTruncationState.truncationCompleted) Fetching else Truncating\n+            val (state, lastFetchedEpoch) = if (offsetTruncationState.truncationCompleted)\n+              (Fetching, latestEpoch(topicPartition))\n+            else if (maySkipTruncation && currentFetchState.lastFetchedEpoch.nonEmpty)\n+              (Fetching, currentFetchState.lastFetchedEpoch)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODIwMDg2MA==", "bodyText": "I was trying to to optimize for the case where lastFetchEpoch didn't need to be reset, but you are right, it would need resetting after truncation. Updated to use latestEpoch(topicPartition)).", "url": "https://github.com/apache/kafka/pull/9382#discussion_r518200860", "createdAt": "2020-11-05T16:49:17Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -454,15 +492,23 @@ abstract class AbstractFetcherThread(name: String,\n     * truncation completed if their offsetTruncationState indicates truncation completed\n     *\n     * @param fetchOffsets the partitions to update fetch offset and maybe mark truncation complete\n+    * @param maySkipTruncation true if we can stay in Fetching mode and perform truncation later based on\n+   *                           diverging epochs from fetch responses.\n     */\n-  private def updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets: Map[TopicPartition, OffsetTruncationState]): Unit = {\n+  private def updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets: Map[TopicPartition, OffsetTruncationState],\n+                                                              maySkipTruncation: Boolean): Unit = {\n     val newStates: Map[TopicPartition, PartitionFetchState] = partitionStates.partitionStateMap.asScala\n       .map { case (topicPartition, currentFetchState) =>\n         val maybeTruncationComplete = fetchOffsets.get(topicPartition) match {\n           case Some(offsetTruncationState) =>\n-            val state = if (offsetTruncationState.truncationCompleted) Fetching else Truncating\n+            val (state, lastFetchedEpoch) = if (offsetTruncationState.truncationCompleted)\n+              (Fetching, latestEpoch(topicPartition))\n+            else if (maySkipTruncation && currentFetchState.lastFetchedEpoch.nonEmpty)\n+              (Fetching, currentFetchState.lastFetchedEpoch)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjk2NzUyOQ=="}, "originalCommit": null, "originalPosition": 160}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0ODIwNjE5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxOTozOToxM1rOH88zvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMjowNzozMlrOH9X74w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY3Mjg5Mg==", "bodyText": "This is probably ok. I guess an alternative would be to not take the initial last fetched epoch from InitialFetchState, but instead use latestEpoch.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r533672892", "createdAt": "2020-12-01T19:39:13Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -669,11 +714,18 @@ abstract class AbstractFetcherThread(name: String,\n     Option(partitionStates.stateValue(topicPartition))\n   }\n \n+  /**\n+   * Returns current fetch state for each partition assigned to this thread. This is used to reassign\n+   * partitions when thread pool is resized. We return `lastFetchedEpoch=None` to ensure we go through", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDExNzM0Nw==", "bodyText": "Updated.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r534117347", "createdAt": "2020-12-02T12:07:32Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -669,11 +714,18 @@ abstract class AbstractFetcherThread(name: String,\n     Option(partitionStates.stateValue(topicPartition))\n   }\n \n+  /**\n+   * Returns current fetch state for each partition assigned to this thread. This is used to reassign\n+   * partitions when thread pool is resized. We return `lastFetchedEpoch=None` to ensure we go through", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY3Mjg5Mg=="}, "originalCommit": null, "originalPosition": 183}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0ODIzMDk3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/ReplicaFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxOTo0NTo1OVrOH89C9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxOTo0NTo1OVrOH89C9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY3Njc5MA==", "bodyText": "nit: I don't think we need this. We can override isTruncationOnFetchSupported with a val", "url": "https://github.com/apache/kafka/pull/9382#discussion_r533676790", "createdAt": "2020-12-01T19:45:59Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/ReplicaFetcherThread.scala", "diffHunk": "@@ -102,6 +103,7 @@ class ReplicaFetcherThread(name: String,\n   private val maxBytes = brokerConfig.replicaFetchResponseMaxBytes\n   private val fetchSize = brokerConfig.replicaFetchMaxBytes\n   private val brokerSupportsLeaderEpochRequest = brokerConfig.interBrokerProtocolVersion >= KAFKA_0_11_0_IV2\n+  private val brokerSupportsTruncationOnFetch = ApiVersion.isTruncationOnFetchSupported(brokerConfig.interBrokerProtocolVersion)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MzA4MjM5OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxOToyNToxNlrOH9qrBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQyMzo1NjozOFrOH9zKLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQyNDMyNQ==", "bodyText": "Hmm.. Do we actually return Some(EpochEndOffset.UNDEFINED_EPOCH) from latestEpoch? That seems surprising.\nMight be worth a comment here that we still go through the Truncating state here when the message format is old.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r534424325", "createdAt": "2020-12-02T19:25:16Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -426,21 +451,35 @@ abstract class AbstractFetcherThread(name: String,\n     warn(s\"Partition $topicPartition marked as failed\")\n   }\n \n-  def addPartitions(initialFetchStates: Map[TopicPartition, OffsetAndEpoch]): Set[TopicPartition] = {\n+  /**\n+   * Returns initial partition fetch state based on current state and the provided `initialFetchState`.\n+   * From IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+   * For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch.\n+   */\n+  private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\n+    if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) {\n+      currentState\n+    } else if (initialFetchState.initOffset < 0) {\n+      fetchOffsetAndTruncate(tp, initialFetchState.currentLeaderEpoch)\n+    } else if (isTruncationOnFetchSupported) {\n+      val lastFetchedEpoch = latestEpoch(tp)\n+      val state = if (lastFetchedEpoch.exists(_ != EpochEndOffset.UNDEFINED_EPOCH)) Fetching else Truncating", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf976df9fb3201c9f996ec71f8b325fc4a13c05c"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU2MzM3NQ==", "bodyText": "I was being lazy with that check because we were using Some(EpochEndOffset.UNDEFINED_EPOCH) in AbstractFetcherThreadTest. I have updated the test and fixed the check. Added comment as well.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r534563375", "createdAt": "2020-12-02T23:56:38Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "diffHunk": "@@ -426,21 +451,35 @@ abstract class AbstractFetcherThread(name: String,\n     warn(s\"Partition $topicPartition marked as failed\")\n   }\n \n-  def addPartitions(initialFetchStates: Map[TopicPartition, OffsetAndEpoch]): Set[TopicPartition] = {\n+  /**\n+   * Returns initial partition fetch state based on current state and the provided `initialFetchState`.\n+   * From IBP 2.7 onwards, we can rely on truncation based on diverging data returned in fetch responses.\n+   * For older versions, we can skip the truncation step iff the leader epoch matches the existing epoch.\n+   */\n+  private def partitionFetchState(tp: TopicPartition, initialFetchState: InitialFetchState, currentState: PartitionFetchState): PartitionFetchState = {\n+    if (currentState != null && currentState.currentLeaderEpoch == initialFetchState.currentLeaderEpoch) {\n+      currentState\n+    } else if (initialFetchState.initOffset < 0) {\n+      fetchOffsetAndTruncate(tp, initialFetchState.currentLeaderEpoch)\n+    } else if (isTruncationOnFetchSupported) {\n+      val lastFetchedEpoch = latestEpoch(tp)\n+      val state = if (lastFetchedEpoch.exists(_ != EpochEndOffset.UNDEFINED_EPOCH)) Fetching else Truncating", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQyNDMyNQ=="}, "originalCommit": {"oid": "bf976df9fb3201c9f996ec71f8b325fc4a13c05c"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MzEzMTk1OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/AbstractFetcherManager.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxOTozODo0N1rOH9rKAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQyMzo1NzoxMlrOH9zK8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQzMjI1OQ==", "bodyText": "This reads a bit odd following removeAllPartitions. I guess what we get from removeFetcherForPartitions is the clearing of failedPartitions and de-registration from fetcherLagStats. Not super important, but wonder if it's worth trying to consolidate a little. Maybe removeFetcherForPartitions could return the initial fetch offsets or something.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r534432259", "createdAt": "2020-12-02T19:38:47Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherManager.scala", "diffHunk": "@@ -64,8 +64,8 @@ abstract class AbstractFetcherManager[T <: AbstractFetcherThread](val name: Stri\n   def resizeThreadPool(newSize: Int): Unit = {\n     def migratePartitions(newSize: Int): Unit = {\n       fetcherThreadMap.forKeyValue { (id, thread) =>\n-        val removedPartitions = thread.partitionsAndOffsets\n-        removeFetcherForPartitions(removedPartitions.keySet)\n+        val removedPartitions = thread.removeAllPartitions()\n+        removeFetcherForPartitions(removedPartitions.keySet) // clear state for removed partitions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf976df9fb3201c9f996ec71f8b325fc4a13c05c"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU2MzU3MA==", "bodyText": "Updated.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r534563570", "createdAt": "2020-12-02T23:57:12Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/AbstractFetcherManager.scala", "diffHunk": "@@ -64,8 +64,8 @@ abstract class AbstractFetcherManager[T <: AbstractFetcherThread](val name: Stri\n   def resizeThreadPool(newSize: Int): Unit = {\n     def migratePartitions(newSize: Int): Unit = {\n       fetcherThreadMap.forKeyValue { (id, thread) =>\n-        val removedPartitions = thread.partitionsAndOffsets\n-        removeFetcherForPartitions(removedPartitions.keySet)\n+        val removedPartitions = thread.removeAllPartitions()\n+        removeFetcherForPartitions(removedPartitions.keySet) // clear state for removed partitions", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQzMjI1OQ=="}, "originalCommit": {"oid": "bf976df9fb3201c9f996ec71f8b325fc4a13c05c"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MzIyNjkzOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQyMDowNTowNFrOH9sFEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQyMzo1ODowMFrOH9zL_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ0NzM3OQ==", "bodyText": "I think this could be saved for a follow-up, but I wonder if we should consider similarly letting the initial offset be determined by the fetcher thread on initialization rather than being passed in. I find it confusing that we expect this to be the high watermark in some cases. It seems a little slippery the way we rely on it in AbstractFetcherThread.truncateToHighWatermark.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r534447379", "createdAt": "2020-12-02T20:05:04Z", "author": {"login": "hachikuji"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1691,6 +1692,18 @@ class ReplicaManager(val config: KafkaConfig,\n     partitionsToMakeFollower\n   }\n \n+  /**\n+   * From IBP 2.7 onwards, we send latest fetch epoch in the request and truncate if a\n+   * diverging epoch is returned in the response, avoiding the need for a separate\n+   * OffsetForLeaderEpoch request.\n+   */\n+  private def initialFetchOffset(log: Log): Long = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf976df9fb3201c9f996ec71f8b325fc4a13c05c"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU2MzgzOA==", "bodyText": "Yes, makes sense, will do that in a follow-up PR.", "url": "https://github.com/apache/kafka/pull/9382#discussion_r534563838", "createdAt": "2020-12-02T23:58:00Z", "author": {"login": "rajinisivaram"}, "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1691,6 +1692,18 @@ class ReplicaManager(val config: KafkaConfig,\n     partitionsToMakeFollower\n   }\n \n+  /**\n+   * From IBP 2.7 onwards, we send latest fetch epoch in the request and truncate if a\n+   * diverging epoch is returned in the response, avoiding the need for a separate\n+   * OffsetForLeaderEpoch request.\n+   */\n+  private def initialFetchOffset(log: Log): Long = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ0NzM3OQ=="}, "originalCommit": {"oid": "bf976df9fb3201c9f996ec71f8b325fc4a13c05c"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MzI3NDgzOnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQyMDoxODo0NVrOH9siaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQyMDoxODo0NVrOH9siaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDQ1NDg4OA==", "bodyText": "nit: needs update?", "url": "https://github.com/apache/kafka/pull/9382#discussion_r534454888", "createdAt": "2020-12-02T20:18:45Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaFetcherThreadTest.scala", "diffHunk": "@@ -453,6 +466,107 @@ class ReplicaFetcherThreadTest {\n                truncateToCapture.getValues.asScala.contains(101))\n   }\n \n+  @Test\n+  def shouldTruncateIfLeaderRepliesWithDivergingEpochNotKnownToFollower(): Unit = {\n+\n+    // Create a capture to track what partitions/offsets are truncated\n+    val truncateToCapture: Capture[Long] = newCapture(CaptureType.ALL)\n+\n+    val config = KafkaConfig.fromProps(TestUtils.createBrokerConfig(1, \"localhost:1234\"))\n+\n+    // Setup all dependencies\n+    val quota: ReplicationQuotaManager = createNiceMock(classOf[ReplicationQuotaManager])\n+    val logManager: LogManager = createMock(classOf[LogManager])\n+    val replicaAlterLogDirsManager: ReplicaAlterLogDirsManager = createMock(classOf[ReplicaAlterLogDirsManager])\n+    val log: Log = createNiceMock(classOf[Log])\n+    val partition: Partition = createNiceMock(classOf[Partition])\n+    val replicaManager: ReplicaManager = createMock(classOf[ReplicaManager])\n+\n+    val initialLEO = 200\n+    var latestLogEpoch: Option[Int] = Some(5)\n+\n+    // Stubs\n+    expect(partition.truncateTo(capture(truncateToCapture), anyBoolean())).anyTimes()\n+    expect(partition.localLogOrException).andReturn(log).anyTimes()\n+    expect(log.highWatermark).andReturn(115).anyTimes()\n+    expect(log.latestEpoch).andAnswer(() => latestLogEpoch).anyTimes()\n+    expect(log.endOffsetForEpoch(4)).andReturn(Some(OffsetAndEpoch(149, 4))).anyTimes()\n+    expect(log.endOffsetForEpoch(3)).andReturn(Some(OffsetAndEpoch(129, 2))).anyTimes()\n+    expect(log.endOffsetForEpoch(2)).andReturn(Some(OffsetAndEpoch(119, 1))).anyTimes()\n+    expect(log.logEndOffset).andReturn(initialLEO).anyTimes()\n+    expect(replicaManager.localLogOrException(anyObject(classOf[TopicPartition]))).andReturn(log).anyTimes()\n+    expect(replicaManager.logManager).andReturn(logManager).anyTimes()\n+    expect(replicaManager.replicaAlterLogDirsManager).andReturn(replicaAlterLogDirsManager).anyTimes()\n+    expect(replicaManager.brokerTopicStats).andReturn(mock(classOf[BrokerTopicStats]))\n+    stub(partition, replicaManager, log)\n+\n+    replay(replicaManager, logManager, quota, partition, log)\n+\n+    // Create the fetcher thread\n+    val mockNetwork = new ReplicaFetcherMockBlockingSend(Collections.emptyMap(), brokerEndPoint, new SystemTime())\n+    val thread = new ReplicaFetcherThread(\"bob\", 0, brokerEndPoint, config, failedPartitions, replicaManager, new Metrics(), new SystemTime(), quota, Some(mockNetwork)) {\n+      override def processPartitionData(topicPartition: TopicPartition, fetchOffset: Long, partitionData: FetchData): Option[LogAppendInfo] = None\n+    }\n+    thread.addPartitions(Map(t1p0 -> initialFetchState(initialLEO), t1p1 -> initialFetchState(initialLEO)))\n+    val partitions = Set(t1p0, t1p1)\n+\n+    // Loop 1 -- both topic partitions skip epoch fetch and send fetch request since\n+    // lastFetchedEpoch is set in initial fetch state.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf976df9fb3201c9f996ec71f8b325fc4a13c05c"}, "originalPosition": 216}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1711, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}