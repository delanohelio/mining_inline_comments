{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAwNDk2Nzkx", "number": 8440, "title": "KAFKA-9832: extend Kafka Streams EOS system test", "bodyText": "Call for review @abbccdda @guozhangwang\nSystem test run passed: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3886/", "createdAt": "2020-04-07T20:53:37Z", "url": "https://github.com/apache/kafka/pull/8440", "merged": true, "mergeCommit": {"oid": "17f98792617a6de39cbd0d651a03fc40a06e0ff6"}, "closed": true, "closedAt": "2020-04-15T20:13:24Z", "author": {"login": "mjsax"}, "timelineItems": {"totalCount": 46, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcVZsgiAFqTM4OTQ4MjgyMA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcXsOBqgH2gAyNDAwNDk2NzkxOjE1ZTBiOThlZjM4NGMxNTAyMmVlNzZlNmNhODhiMzI1NTY3ZTMxY2I=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDgyODIw", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389482820", "createdAt": "2020-04-07T20:54:43Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NDo0M1rOGCVv7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NDo0M1rOGCVv7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNjY3MA==", "bodyText": "This is an actually bug-fix. StandbyTasks did not set the eos flag to true for eos-beta and thus did not wipe out their stores in case of failure.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405106670", "createdAt": "2020-04-07T20:54:43Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -72,7 +72,7 @@\n \n         processorContext = new StandbyContextImpl(id, config, stateMgr, metrics);\n         closeTaskSensor = ThreadMetrics.closeTaskSensor(Thread.currentThread().getName(), metrics);\n-        this.eosEnabled = StreamsConfig.EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG));\n+        eosEnabled = StreamThread.eosEnabled(config);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDgzMzAy", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389483302", "createdAt": "2020-04-07T20:55:25Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NToyNlrOGCVxaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NToyNlrOGCVxaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzA0OA==", "bodyText": "This is an actually bug fix: consumedOffsetsAndMetadataPerTask could be empty, if only standby tasks (but no active tasks) are assigned to a thread.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107048", "createdAt": "2020-04-07T20:55:26Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -758,7 +758,9 @@ private int commitInternal(final Collection<Task> tasks) {\n                 }\n             }\n \n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDgzNDUx", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389483451", "createdAt": "2020-04-07T20:55:39Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NTozOVrOGCVx3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NTozOVrOGCVx3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzE2NA==", "bodyText": "Java8 cleanup", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107164", "createdAt": "2020-04-07T20:55:39Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -75,26 +71,20 @@ public void start() {\n                 uncaughtException = false;\n \n                 streams = createKafkaStreams(properties);\n-                streams.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDgzNTA1", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389483505", "createdAt": "2020-04-07T20:55:44Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NTo0NFrOGCVyAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NTo0NFrOGCVyAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzIwMg==", "bodyText": "Java8 cleanup", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107202", "createdAt": "2020-04-07T20:55:44Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -75,26 +71,20 @@ public void start() {\n                 uncaughtException = false;\n \n                 streams = createKafkaStreams(properties);\n-                streams.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {\n-                    @Override\n-                    public void uncaughtException(final Thread t, final Throwable e) {\n-                        System.out.println(System.currentTimeMillis());\n-                        System.out.println(\"EOS-TEST-CLIENT-EXCEPTION\");\n-                        e.printStackTrace();\n-                        System.out.flush();\n-                        uncaughtException = true;\n-                    }\n+                streams.setUncaughtExceptionHandler((t, e) -> {\n+                    System.out.println(System.currentTimeMillis());\n+                    System.out.println(\"EOS-TEST-CLIENT-EXCEPTION\");\n+                    e.printStackTrace();\n+                    System.out.flush();\n+                    uncaughtException = true;\n                 });\n-                streams.setStateListener(new KafkaStreams.StateListener() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDgzODEx", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389483811", "createdAt": "2020-04-07T20:56:12Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjoxMlrOGCVy8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjoxMlrOGCVy8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzQ0Mg==", "bodyText": "We set processing guarantee \"external\" now, via the system test properties file", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107442", "createdAt": "2020-04-07T20:56:12Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -112,8 +102,8 @@ private KafkaStreams createKafkaStreams(final Properties props) {\n         props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1);\n         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);\n         props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);\n-        props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDgzOTE4", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389483918", "createdAt": "2020-04-07T20:56:21Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjoyMlrOGCVzSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjoyMlrOGCVzSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzUyOQ==", "bodyText": "Small side improvement.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107529", "createdAt": "2020-04-07T20:56:22Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -112,8 +102,8 @@ private KafkaStreams createKafkaStreams(final Properties props) {\n         props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1);\n         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);\n         props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);\n-        props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);\n         props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+        props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 5000); // increase commit interval to make sure a client is killed having an open transaction", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 69}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDgzOTc5", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389483979", "createdAt": "2020-04-07T20:56:26Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjoyNlrOGCVzeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjoyNlrOGCVzeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzU3Nw==", "bodyText": "Java8 cleanup", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107577", "createdAt": "2020-04-07T20:56:26Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -127,41 +117,17 @@ private KafkaStreams createKafkaStreams(final Properties props) {\n         // min\n         groupedData\n             .aggregate(\n-                new Initializer<Integer>() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 77}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDg0MDIw", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389484020", "createdAt": "2020-04-07T20:56:30Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjozMVrOGCVzpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjozMVrOGCVzpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzYyMQ==", "bodyText": "Java8 cleanup", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107621", "createdAt": "2020-04-07T20:56:31Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -127,41 +117,17 @@ private KafkaStreams createKafkaStreams(final Properties props) {\n         // min\n         groupedData\n             .aggregate(\n-                new Initializer<Integer>() {\n-                    @Override\n-                    public Integer apply() {\n-                        return Integer.MAX_VALUE;\n-                    }\n-                },\n-                new Aggregator<String, Integer, Integer>() {\n-                    @Override\n-                    public Integer apply(final String aggKey,\n-                                         final Integer value,\n-                                         final Integer aggregate) {\n-                        return (value < aggregate) ? value : aggregate;\n-                    }\n-                },\n-                Materialized.<String, Integer, KeyValueStore<Bytes, byte[]>>with(null, intSerde))\n+                () -> Integer.MAX_VALUE,\n+                (aggKey, value, aggregate) -> (value < aggregate) ? value : aggregate,\n+                Materialized.with(null, intSerde))\n             .toStream()\n             .to(\"min\", Produced.with(stringSerde, intSerde));\n \n         // sum\n         groupedData.aggregate(\n-            new Initializer<Long>() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 100}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDg0MDc0", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389484074", "createdAt": "2020-04-07T20:56:35Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjozNVrOGCVz0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1NjozNVrOGCVz0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzY2NA==", "bodyText": "Java8 cleanup", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405107664", "createdAt": "2020-04-07T20:56:35Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -174,21 +140,9 @@ public Long apply(final String aggKey,\n             // max\n             groupedDataAfterRepartitioning\n                 .aggregate(\n-                    new Initializer<Integer>() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 125}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDg1Mzgz", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389485383", "createdAt": "2020-04-07T20:58:39Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1ODozOVrOGCV4Rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1ODozOVrOGCV4Rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwODgwNw==", "bodyText": "The previous shutdown hook did not wait until the \"main loop\" breaks and exits. Hence, the code after the loop was never executed making debugging harder. We introduce the terminated flag to delay the termination of the JVM until the method finished.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405108807", "createdAt": "2020-04-07T20:58:39Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDg2MTE4", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389486118", "createdAt": "2020-04-07T20:59:44Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1OTo0NFrOGCV6rA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo1OTo0NFrOGCV6rA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwOTQyMA==", "bodyText": "This is the \"main loop\" as mentioned above", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405109420", "createdAt": "2020-04-07T20:59:44Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n+                System.out.println(\"Terminating\");\n+                isRunning = false;\n+\n+                final long timeout = System.currentTimeMillis() + Duration.ofMinutes(5).toMillis();\n+                while (!terminated) {\n+                    if (System.currentTimeMillis() > timeout) {\n+                        System.out.println(\"Terminating with timeout\");\n+                        break;\n+                    }\n \n-        Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n-            System.out.println(\"Terminating\");\n-            System.out.flush();\n-            isRunning = false;\n-        });\n+                    System.out.println(\"Waiting for main thread to exit\");\n+                    try {\n+                        Thread.sleep(1000L);\n+                    } catch (final InterruptedException swallow) {\n+                        swallow.printStackTrace(System.err);\n+                        System.err.flush();\n+                        break;\n+                    }\n \n-        final Properties producerProps = new Properties();\n-        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n-        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n-        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n-        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n-        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n+                }\n \n-        final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n+                System.out.println(\"Terminated\");\n+                System.out.flush();\n+            });\n+\n+            final Properties producerProps = new Properties();\n+            producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n+            producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n+            producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+            producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n+            producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n-        final Random rand = new Random(System.currentTimeMillis());\n+            final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n \n-        while (isRunning) {\n-            final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n-            final int value = rand.nextInt(10000);\n+            final Random rand = new Random(System.currentTimeMillis());\n+            final Map<Integer, List<Long>> offsets = new HashMap<>();\n \n-            final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+            while (isRunning) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 86}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDg2Mzc1", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389486375", "createdAt": "2020-04-07T21:00:09Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowMDoxMFrOGCV7iQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowMDoxMFrOGCV7iQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwOTY0MQ==", "bodyText": "This is the code after the \"main loop\" that was never executed", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405109641", "createdAt": "2020-04-07T21:00:10Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n+                System.out.println(\"Terminating\");\n+                isRunning = false;\n+\n+                final long timeout = System.currentTimeMillis() + Duration.ofMinutes(5).toMillis();\n+                while (!terminated) {\n+                    if (System.currentTimeMillis() > timeout) {\n+                        System.out.println(\"Terminating with timeout\");\n+                        break;\n+                    }\n \n-        Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n-            System.out.println(\"Terminating\");\n-            System.out.flush();\n-            isRunning = false;\n-        });\n+                    System.out.println(\"Waiting for main thread to exit\");\n+                    try {\n+                        Thread.sleep(1000L);\n+                    } catch (final InterruptedException swallow) {\n+                        swallow.printStackTrace(System.err);\n+                        System.err.flush();\n+                        break;\n+                    }\n \n-        final Properties producerProps = new Properties();\n-        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n-        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n-        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n-        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n-        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n+                }\n \n-        final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n+                System.out.println(\"Terminated\");\n+                System.out.flush();\n+            });\n+\n+            final Properties producerProps = new Properties();\n+            producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n+            producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n+            producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+            producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n+            producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n-        final Random rand = new Random(System.currentTimeMillis());\n+            final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n \n-        while (isRunning) {\n-            final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n-            final int value = rand.nextInt(10000);\n+            final Random rand = new Random(System.currentTimeMillis());\n+            final Map<Integer, List<Long>> offsets = new HashMap<>();\n \n-            final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+            while (isRunning) {\n+                final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n+                final int value = rand.nextInt(10000);\n \n-            producer.send(record, (metadata, exception) -> {\n-                if (exception != null) {\n-                    exception.printStackTrace(System.err);\n-                    System.err.flush();\n-                    if (exception instanceof TimeoutException) {\n-                        try {\n-                            // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n-                            final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n-                            updateNumRecordsProduces(-expired);\n-                        } catch (final Exception ignore) { }\n+                final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+\n+                producer.send(record, (metadata, exception) -> {\n+                    if (exception != null) {\n+                        exception.printStackTrace(System.err);\n+                        System.err.flush();\n+                        if (exception instanceof TimeoutException) {\n+                            try {\n+                                // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n+                                final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n+                                updateNumRecordsProduces(-expired);\n+                            } catch (final Exception ignore) {\n+                            }\n+                        }\n+                    } else {\n+                        offsets.getOrDefault(metadata.partition(), new LinkedList<>()).add(metadata.offset());\n                     }\n+                });\n+\n+                updateNumRecordsProduces(1);\n+                if (numRecordsProduced % 1000 == 0) {\n+                    System.out.println(numRecordsProduced + \" records produced\");\n+                    System.out.flush();\n                 }\n-            });\n+                Utils.sleep(rand.nextInt(10));\n+            }\n+            producer.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 127}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDg2ODky", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389486892", "createdAt": "2020-04-07T21:00:57Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowMDo1OFrOGCV9NQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowMDo1OFrOGCV9NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMDA2OQ==", "bodyText": "We use a try-catch to set the flag to make sure the shutdown hook can exit quickly even in case of failure", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405110069", "createdAt": "2020-04-07T21:00:58Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n+                System.out.println(\"Terminating\");\n+                isRunning = false;\n+\n+                final long timeout = System.currentTimeMillis() + Duration.ofMinutes(5).toMillis();\n+                while (!terminated) {\n+                    if (System.currentTimeMillis() > timeout) {\n+                        System.out.println(\"Terminating with timeout\");\n+                        break;\n+                    }\n \n-        Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n-            System.out.println(\"Terminating\");\n-            System.out.flush();\n-            isRunning = false;\n-        });\n+                    System.out.println(\"Waiting for main thread to exit\");\n+                    try {\n+                        Thread.sleep(1000L);\n+                    } catch (final InterruptedException swallow) {\n+                        swallow.printStackTrace(System.err);\n+                        System.err.flush();\n+                        break;\n+                    }\n \n-        final Properties producerProps = new Properties();\n-        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n-        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n-        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n-        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n-        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n+                }\n \n-        final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n+                System.out.println(\"Terminated\");\n+                System.out.flush();\n+            });\n+\n+            final Properties producerProps = new Properties();\n+            producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n+            producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n+            producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+            producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n+            producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n-        final Random rand = new Random(System.currentTimeMillis());\n+            final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n \n-        while (isRunning) {\n-            final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n-            final int value = rand.nextInt(10000);\n+            final Random rand = new Random(System.currentTimeMillis());\n+            final Map<Integer, List<Long>> offsets = new HashMap<>();\n \n-            final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+            while (isRunning) {\n+                final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n+                final int value = rand.nextInt(10000);\n \n-            producer.send(record, (metadata, exception) -> {\n-                if (exception != null) {\n-                    exception.printStackTrace(System.err);\n-                    System.err.flush();\n-                    if (exception instanceof TimeoutException) {\n-                        try {\n-                            // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n-                            final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n-                            updateNumRecordsProduces(-expired);\n-                        } catch (final Exception ignore) { }\n+                final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+\n+                producer.send(record, (metadata, exception) -> {\n+                    if (exception != null) {\n+                        exception.printStackTrace(System.err);\n+                        System.err.flush();\n+                        if (exception instanceof TimeoutException) {\n+                            try {\n+                                // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n+                                final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n+                                updateNumRecordsProduces(-expired);\n+                            } catch (final Exception ignore) {\n+                            }\n+                        }\n+                    } else {\n+                        offsets.getOrDefault(metadata.partition(), new LinkedList<>()).add(metadata.offset());\n                     }\n+                });\n+\n+                updateNumRecordsProduces(1);\n+                if (numRecordsProduced % 1000 == 0) {\n+                    System.out.println(numRecordsProduced + \" records produced\");\n+                    System.out.flush();\n                 }\n-            });\n+                Utils.sleep(rand.nextInt(10));\n+            }\n+            producer.close();\n+            System.out.println(\"Producer closed: \" + numRecordsProduced + \" records produced\");\n+            System.out.flush();\n \n-            updateNumRecordsProduces(1);\n-            if (numRecordsProduced % 1000 == 0) {\n-                System.out.println(numRecordsProduced + \" records produced\");\n-                System.out.flush();\n+            // verify offsets\n+            for (Map.Entry<Integer, List<Long>> offsetsOfPartition : offsets.entrySet()) {\n+                offsetsOfPartition.getValue().sort(Long::compareTo);\n+                for (int i = 0; i < offsetsOfPartition.getValue().size() - 1; ++i) {\n+                    if (offsetsOfPartition.getValue().get(i) != i) {\n+                        System.err.println(\"Offset for partition \" + offsetsOfPartition.getKey() + \" is not \" + i + \" as expected but \" + offsetsOfPartition.getValue().get(i));\n+                        System.err.flush();\n+                    }\n+                }\n+                System.out.println(\"Max offset of partition \" + offsetsOfPartition.getKey() + \" is \" + offsetsOfPartition.getValue().get(offsetsOfPartition.getValue().size() - 1));\n             }\n-            Utils.sleep(rand.nextInt(10));\n-        }\n-        producer.close();\n-        System.out.println(\"Producer closed: \" + numRecordsProduced + \" records produced\");\n \n-        final Properties props = new Properties();\n-        props.put(ConsumerConfig.CLIENT_ID_CONFIG, \"verifier\");\n-        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n-        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n-        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n-        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase(Locale.ROOT));\n \n-        try (final KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(props)) {\n-            final List<TopicPartition> partitions = getAllPartitions(consumer, \"data\");\n-            System.out.println(\"Partitions: \" + partitions);\n-            consumer.assign(partitions);\n-            consumer.seekToEnd(partitions);\n+            final Properties props = new Properties();\n+            props.put(ConsumerConfig.CLIENT_ID_CONFIG, \"verifier\");\n+            props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n+            props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n+            props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n+            props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase(Locale.ROOT));\n \n-            for (final TopicPartition tp : partitions) {\n-                System.out.println(\"End-offset for \" + tp + \" is \" + consumer.position(tp));\n+            try (final KafkaConsumer<byte[], byte[]> consumer = new KafkaConsumer<>(props)) {\n+                final List<TopicPartition> partitions = getAllPartitions(consumer, \"data\");\n+                System.out.println(\"Partitions: \" + partitions);\n+                System.out.flush();\n+                consumer.assign(partitions);\n+                consumer.seekToEnd(partitions);\n+\n+                for (final TopicPartition tp : partitions) {\n+                    System.out.println(\"End-offset for \" + tp + \" is \" + consumer.position(tp));\n+                    System.out.flush();\n+                }\n             }\n+            System.out.flush();\n+        } finally {\n+            terminated = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 186}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDg5NDcw", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389489470", "createdAt": "2020-04-07T21:04:51Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowNDo1MVrOGCWFWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowNDo1MVrOGCWFWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMjE1NA==", "bodyText": "To make the verification step work, we first need to check that all transactions are finished. With EOS-alpha we never had pending transactions that would eventually be aborted by the tx-coordinator, because while we crash some instanced in between the final shutdown phase is always clean. Hence, for eos-alpha all pending transactions would be aborted by initTransaction() calls.\nFor eos-beta, thread that are killed leave open transaction that will be eventually expired by the tx-coordinator though, as we (also on restart of a thread) would generate a new transactonal.id.\nHaving no pending transactions is a requirement for the following code to do a correct verification of the result.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405112154", "createdAt": "2020-04-07T21:04:51Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -144,6 +188,14 @@ public static void verify(final String kafka, final boolean withRepartitioning)\n         props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ByteArrayDeserializer.class);\n         props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, IsolationLevel.READ_COMMITTED.toString().toLowerCase(Locale.ROOT));\n \n+        try (final KafkaConsumer<byte[], byte[]> committedConsumer = new KafkaConsumer<>(props)) {\n+            verifyAllTransactionFinished(committedConsumer, kafka, withRepartitioning);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 197}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDkwMDE4", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389490018", "createdAt": "2020-04-07T21:05:39Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowNTozOVrOGCWHFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowNTozOVrOGCWHFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMjU5Nw==", "bodyText": "Just increasing the wait time as small side improvement to spin less and reduce the output for debugging.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405112597", "createdAt": "2020-04-07T21:05:39Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -263,15 +303,15 @@ private static void ensureStreamsApplicationDown(final Admin adminClient) {\n                                                                                                      final Map<TopicPartition, Long> readEndOffsets,\n                                                                                                      final boolean withRepartitioning,\n                                                                                                      final boolean isInputTopic) {\n-        System.err.println(\"read end offset: \" + readEndOffsets);\n+        System.out.println(\"read end offset: \" + readEndOffsets);\n         final Map<String, Map<TopicPartition, List<ConsumerRecord<byte[], byte[]>>>> recordPerTopicPerPartition = new HashMap<>();\n         final Map<TopicPartition, Long> maxReceivedOffsetPerPartition = new HashMap<>();\n         final Map<TopicPartition, Long> maxConsumerPositionPerPartition = new HashMap<>();\n \n         long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n         boolean allRecordsReceived = false;\n         while (!allRecordsReceived && System.currentTimeMillis() < maxWaitTime) {\n-            final ConsumerRecords<byte[], byte[]> receivedRecords = consumer.poll(Duration.ofMillis(100));\n+            final ConsumerRecords<byte[], byte[]> receivedRecords = consumer.poll(Duration.ofSeconds(1L));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 263}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDkwNjIx", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389490621", "createdAt": "2020-04-07T21:06:36Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowNjozNlrOGCWI0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowNjozNlrOGCWI0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMzA0Mg==", "bodyText": "Because we do the verification for pending transactions first now, we have one additional record that is not part of the result and that we need to exclude (similar below)", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405113042", "createdAt": "2020-04-07T21:06:36Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -349,7 +389,7 @@ private static void verifyReceivedAllRecords(final Map<TopicPartition, List<Cons\n             final TopicPartition inputTopicPartition = new TopicPartition(\"data\", partitionRecords.getKey().partition());\n             final Iterator<ConsumerRecord<byte[], byte[]>> expectedRecord = expectedRecords.get(inputTopicPartition).iterator();\n \n-            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue()) {\n+            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue().subList(0, partitionRecords.getValue().size() - 1)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 281}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDkxMzM2", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389491336", "createdAt": "2020-04-07T21:07:45Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowNzo0NVrOGCWLTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowNzo0NVrOGCWLTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMzY3Ng==", "bodyText": "As there might be pending transaction, we need to improve the way how we verify that all transaction are finished. For this, we need to remember the offsets of our \"topic end marker messages\".", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405113676", "createdAt": "2020-04-07T21:07:45Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -550,6 +590,8 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n         producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n+        final Map<TopicPartition, Long> endMarkerOffset = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 357}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDkyNTg4", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389492588", "createdAt": "2020-04-07T21:09:50Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowOTo1MFrOGCWP3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMTowOTo1MFrOGCWP3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNDg0Nw==", "bodyText": "Instead of looking for the end-marker message per content (ie, comparing key and value), we now use the offset (that we now know) to see if we can get the endOffset() as expected in \"read_committed\" mode.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405114847", "createdAt": "2020-04-07T21:09:50Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -559,47 +601,36 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n                         exception.printStackTrace(System.err);\n                         System.err.flush();\n                         Exit.exit(1);\n+                    } else {\n+                        endMarkerOffset.put(new TopicPartition(metadata.topic(), metadata.partition()), metadata.offset());\n+                        System.out.println(\"Appended verification record to topic-partition \" + metadata.topic() + \"-\" + metadata.partition() + \" at offset \" + metadata.offset());\n+                        System.out.flush();\n                     }\n                 });\n             }\n         }\n \n-        final StringDeserializer stringDeserializer = new StringDeserializer();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 375}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDkzMTUy", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389493152", "createdAt": "2020-04-07T21:10:45Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMDo0NlrOGCWRmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMDo0NlrOGCWRmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNTI4OQ==", "bodyText": "seekToEnd() will only reach the end-marker in read-committed mode, if there is no pending transaction.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405115289", "createdAt": "2020-04-07T21:10:46Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -559,47 +601,36 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n                         exception.printStackTrace(System.err);\n                         System.err.flush();\n                         Exit.exit(1);\n+                    } else {\n+                        endMarkerOffset.put(new TopicPartition(metadata.topic(), metadata.partition()), metadata.offset());\n+                        System.out.println(\"Appended verification record to topic-partition \" + metadata.topic() + \"-\" + metadata.partition() + \" at offset \" + metadata.offset());\n+                        System.out.flush();\n                     }\n                 });\n             }\n         }\n \n-        final StringDeserializer stringDeserializer = new StringDeserializer();\n-\n         long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-        while (!partitions.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n-            final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));\n-            if (records.isEmpty()) {\n-                System.out.println(\"No data received.\");\n-                for (final TopicPartition tp : partitions) {\n-                    System.out.println(tp + \" at position \" + consumer.position(tp));\n-                }\n-            }\n-            for (final ConsumerRecord<byte[], byte[]> record : records) {\n-                maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-                final String topic = record.topic();\n-                final TopicPartition tp = new TopicPartition(topic, record.partition());\n+        while (!endMarkerOffset.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n+            consumer.seekToEnd(partitions);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 391}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDkzNjQx", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389493641", "createdAt": "2020-04-07T21:11:32Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMTozM1rOGCWTIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMTozM1rOGCWTIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNTY4MA==", "bodyText": "Strictly, position should be exactly endMarkerOffset + 1 -- it seems ok to just check for >", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405115680", "createdAt": "2020-04-07T21:11:33Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -559,47 +601,36 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n                         exception.printStackTrace(System.err);\n                         System.err.flush();\n                         Exit.exit(1);\n+                    } else {\n+                        endMarkerOffset.put(new TopicPartition(metadata.topic(), metadata.partition()), metadata.offset());\n+                        System.out.println(\"Appended verification record to topic-partition \" + metadata.topic() + \"-\" + metadata.partition() + \" at offset \" + metadata.offset());\n+                        System.out.flush();\n                     }\n                 });\n             }\n         }\n \n-        final StringDeserializer stringDeserializer = new StringDeserializer();\n-\n         long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-        while (!partitions.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n-            final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));\n-            if (records.isEmpty()) {\n-                System.out.println(\"No data received.\");\n-                for (final TopicPartition tp : partitions) {\n-                    System.out.println(tp + \" at position \" + consumer.position(tp));\n-                }\n-            }\n-            for (final ConsumerRecord<byte[], byte[]> record : records) {\n-                maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-                final String topic = record.topic();\n-                final TopicPartition tp = new TopicPartition(topic, record.partition());\n+        while (!endMarkerOffset.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n+            consumer.seekToEnd(partitions);\n \n-                try {\n-                    final String key = stringDeserializer.deserialize(topic, record.key());\n-                    final String value = stringDeserializer.deserialize(topic, record.value());\n+            final Iterator<TopicPartition> iterator = partitions.iterator();\n+            while(iterator.hasNext()) {\n+                final TopicPartition topicPartition = iterator.next();\n \n-                    if (!(\"key\".equals(key) && \"value\".equals(value) && partitions.remove(tp))) {\n-                        throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                            \"Expected record <'key','value'> from one of \" + partitions + \" but got\"\n-                            + \" <\" + key + \",\" + value + \"> [\" + record.topic() + \", \" + record.partition() + \"]\");\n-                    } else {\n-                        System.out.println(\"Verifying \" + tp + \" successful.\");\n-                    }\n-                } catch (final SerializationException e) {\n-                    throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                        \"Expected record <'key','value'> from one of \" + partitions + \" but got \" + record, e);\n+                if (consumer.position(topicPartition) > endMarkerOffset.get(topicPartition)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 410}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDk0MzAy", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389494302", "createdAt": "2020-04-07T21:12:38Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMjozOFrOGCWVWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMjozOFrOGCWVWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNjI0OA==", "bodyText": "For debugging purpose,  we now track the smallest and largest processed offset, too. This helps to understand which task during which phase processed which part of the data.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405116248", "createdAt": "2020-04-07T21:12:38Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java", "diffHunk": "@@ -45,12 +44,17 @@\n             public Processor<Object, Object> get() {\n                 return new AbstractProcessor<Object, Object>() {\n                     private int numRecordsProcessed = 0;\n+                    private long smallestOffset = Long.MAX_VALUE;\n+                    private long largestOffset = Long.MIN_VALUE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDk0NTMx", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389494531", "createdAt": "2020-04-07T21:12:57Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMjo1OFrOGCWWEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMjo1OFrOGCWWEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNjQzNA==", "bodyText": "Java8 cleanup", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405116434", "createdAt": "2020-04-07T21:12:58Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java", "diffHunk": "@@ -76,39 +101,19 @@ public K apply(final Windowed<K> winKey, final V value) {\n     public static class Agg {\n \n         KeyValueMapper<String, Long, KeyValue<String, Long>> selector() {\n-            return new KeyValueMapper<String, Long, KeyValue<String, Long>>() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDk0NzI1", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389494725", "createdAt": "2020-04-07T21:13:17Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMzoxN1rOGCWWww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxMzoxN1rOGCWWww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNjYxMQ==", "bodyText": "Removing unused method.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405116611", "createdAt": "2020-04-07T21:13:17Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/SmokeTestUtil.java", "diffHunk": "@@ -120,14 +125,6 @@ public Long apply(final String aggKey, final Long value, final Long aggregate) {\n \n     static Serde<Double> doubleSerde = Serdes.Double();\n \n-    static File createDir(final File parent, final String child) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDk1MjAz", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389495203", "createdAt": "2020-04-07T21:14:03Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxNDowM1rOGCWYRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxNDowM1rOGCWYRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNjk5OQ==", "bodyText": "Config verification step.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405116999", "createdAt": "2020-04-07T21:14:03Z", "author": {"login": "mjsax"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/StreamsEosTest.java", "diffHunk": "@@ -39,12 +39,22 @@ public static void main(final String[] args) throws IOException {\n \n         final Properties streamsProperties = Utils.loadProps(propFileName);\n         final String kafka = streamsProperties.getProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);\n+        final String processingGuarantee = streamsProperties.getProperty(StreamsConfig.PROCESSING_GUARANTEE_CONFIG);\n \n         if (kafka == null) {\n             System.err.println(\"No bootstrap kafka servers specified in \" + StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);\n             System.exit(1);\n         }\n \n+        if (\"process\".equals(command) || \"process-complex\".equals(command)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 11}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDk1NTg0", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389495584", "createdAt": "2020-04-07T21:14:41Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxNDo0MVrOGCWZWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMToxNDo0MVrOGCWZWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNzI3NA==", "bodyText": "We know set the processing guarantee via the properties file (that allows us to easily parametrize the test)", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405117274", "createdAt": "2020-04-07T21:14:41Z", "author": {"login": "mjsax"}, "path": "tests/kafkatest/services/streams.py", "diffHunk": "@@ -322,11 +322,20 @@ class StreamsEosTestBaseService(StreamsTestBaseService):\n \n     clean_node_enabled = True\n \n-    def __init__(self, test_context, kafka, command):\n+    def __init__(self, test_context, kafka, processing_guarantee, command):\n         super(StreamsEosTestBaseService, self).__init__(test_context,\n                                                         kafka,\n                                                         \"org.apache.kafka.streams.tests.StreamsEosTest\",\n                                                         command)\n+        self.PROCESSING_GUARANTEES = processing_guarantee\n+\n+    def prop_file(self):\n+        properties = {streams_property.STATE_DIR: self.PERSISTENT_ROOT,\n+                      streams_property.KAFKA_SERVERS: self.kafka.bootstrap_servers(),\n+                      streams_property.PROCESSING_GUARANTEES: self.PROCESSING_GUARANTEES}", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NTI3NjUw", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-389527650", "createdAt": "2020-04-07T22:13:28Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoxMzoyOFrOGCYDxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNzozMDo0MlrOGC5muw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NDUxOA==", "bodyText": "Could you elaborate more on why committing an empty map will fail?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405144518", "createdAt": "2020-04-07T22:13:28Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -758,7 +758,9 @@ private int commitInternal(final Collection<Task> tasks) {\n                 }\n             }\n \n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzA0OA=="}, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE4MzgwOQ==", "bodyText": "Why do we need to add cleanUp now? Is this missing originally?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405183809", "createdAt": "2020-04-08T00:05:34Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/integration/StandbyTaskEOSIntegrationTest.java", "diffHunk": "@@ -102,17 +119,19 @@ public void surviveWithOneTaskAsStandby() throws ExecutionException, Interrupted\n \n             streamInstanceOne.close(Duration.ZERO);\n             streamInstanceTwo.close(Duration.ZERO);\n+\n+            streamInstanceOne.cleanUp();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY3NjMwNg==", "bodyText": "Could you elaborate a bit? The comment doesn't seem readable.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405676306", "createdAt": "2020-04-08T17:01:28Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestClient.java", "diffHunk": "@@ -112,8 +102,8 @@ private KafkaStreams createKafkaStreams(final Properties props) {\n         props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 1);\n         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);\n         props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);\n-        props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);\n         props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0);\n+        props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 5000); // increase commit interval to make sure a client is killed having an open transaction", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNzUyOQ=="}, "originalCommit": null, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY4ODY2NQ==", "bodyText": "How does this logic work? Are we updating the offsets map when the partition is not present?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405688665", "createdAt": "2020-04-08T17:21:51Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n+                System.out.println(\"Terminating\");\n+                isRunning = false;\n+\n+                final long timeout = System.currentTimeMillis() + Duration.ofMinutes(5).toMillis();\n+                while (!terminated) {\n+                    if (System.currentTimeMillis() > timeout) {\n+                        System.out.println(\"Terminating with timeout\");\n+                        break;\n+                    }\n \n-        Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n-            System.out.println(\"Terminating\");\n-            System.out.flush();\n-            isRunning = false;\n-        });\n+                    System.out.println(\"Waiting for main thread to exit\");\n+                    try {\n+                        Thread.sleep(1000L);\n+                    } catch (final InterruptedException swallow) {\n+                        swallow.printStackTrace(System.err);\n+                        System.err.flush();\n+                        break;\n+                    }\n \n-        final Properties producerProps = new Properties();\n-        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n-        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n-        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n-        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n-        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n+                }\n \n-        final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n+                System.out.println(\"Terminated\");\n+                System.out.flush();\n+            });\n+\n+            final Properties producerProps = new Properties();\n+            producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n+            producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n+            producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+            producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n+            producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n-        final Random rand = new Random(System.currentTimeMillis());\n+            final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n \n-        while (isRunning) {\n-            final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n-            final int value = rand.nextInt(10000);\n+            final Random rand = new Random(System.currentTimeMillis());\n+            final Map<Integer, List<Long>> offsets = new HashMap<>();\n \n-            final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+            while (isRunning) {\n+                final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n+                final int value = rand.nextInt(10000);\n \n-            producer.send(record, (metadata, exception) -> {\n-                if (exception != null) {\n-                    exception.printStackTrace(System.err);\n-                    System.err.flush();\n-                    if (exception instanceof TimeoutException) {\n-                        try {\n-                            // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n-                            final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n-                            updateNumRecordsProduces(-expired);\n-                        } catch (final Exception ignore) { }\n+                final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+\n+                producer.send(record, (metadata, exception) -> {\n+                    if (exception != null) {\n+                        exception.printStackTrace(System.err);\n+                        System.err.flush();\n+                        if (exception instanceof TimeoutException) {\n+                            try {\n+                                // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n+                                final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n+                                updateNumRecordsProduces(-expired);\n+                            } catch (final Exception ignore) {\n+                            }\n+                        }\n+                    } else {\n+                        offsets.getOrDefault(metadata.partition(), new LinkedList<>()).add(metadata.offset());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY4OTkwNA==", "bodyText": "Does this verification reflect in the actual system test?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405689904", "createdAt": "2020-04-08T17:23:55Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n+                System.out.println(\"Terminating\");\n+                isRunning = false;\n+\n+                final long timeout = System.currentTimeMillis() + Duration.ofMinutes(5).toMillis();\n+                while (!terminated) {\n+                    if (System.currentTimeMillis() > timeout) {\n+                        System.out.println(\"Terminating with timeout\");\n+                        break;\n+                    }\n \n-        Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n-            System.out.println(\"Terminating\");\n-            System.out.flush();\n-            isRunning = false;\n-        });\n+                    System.out.println(\"Waiting for main thread to exit\");\n+                    try {\n+                        Thread.sleep(1000L);\n+                    } catch (final InterruptedException swallow) {\n+                        swallow.printStackTrace(System.err);\n+                        System.err.flush();\n+                        break;\n+                    }\n \n-        final Properties producerProps = new Properties();\n-        producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n-        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n-        producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n-        producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n-        producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n+                }\n \n-        final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n+                System.out.println(\"Terminated\");\n+                System.out.flush();\n+            });\n+\n+            final Properties producerProps = new Properties();\n+            producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, \"EosTest\");\n+            producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);\n+            producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n+            producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n+            producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n-        final Random rand = new Random(System.currentTimeMillis());\n+            final KafkaProducer<String, Integer> producer = new KafkaProducer<>(producerProps);\n \n-        while (isRunning) {\n-            final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n-            final int value = rand.nextInt(10000);\n+            final Random rand = new Random(System.currentTimeMillis());\n+            final Map<Integer, List<Long>> offsets = new HashMap<>();\n \n-            final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+            while (isRunning) {\n+                final String key = \"\" + rand.nextInt(MAX_NUMBER_OF_KEYS);\n+                final int value = rand.nextInt(10000);\n \n-            producer.send(record, (metadata, exception) -> {\n-                if (exception != null) {\n-                    exception.printStackTrace(System.err);\n-                    System.err.flush();\n-                    if (exception instanceof TimeoutException) {\n-                        try {\n-                            // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n-                            final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n-                            updateNumRecordsProduces(-expired);\n-                        } catch (final Exception ignore) { }\n+                final ProducerRecord<String, Integer> record = new ProducerRecord<>(\"data\", key, value);\n+\n+                producer.send(record, (metadata, exception) -> {\n+                    if (exception != null) {\n+                        exception.printStackTrace(System.err);\n+                        System.err.flush();\n+                        if (exception instanceof TimeoutException) {\n+                            try {\n+                                // message == org.apache.kafka.common.errors.TimeoutException: Expiring 4 record(s) for data-0: 30004 ms has passed since last attempt plus backoff time\n+                                final int expired = Integer.parseInt(exception.getMessage().split(\" \")[2]);\n+                                updateNumRecordsProduces(-expired);\n+                            } catch (final Exception ignore) {\n+                            }\n+                        }\n+                    } else {\n+                        offsets.getOrDefault(metadata.partition(), new LinkedList<>()).add(metadata.offset());\n                     }\n+                });\n+\n+                updateNumRecordsProduces(1);\n+                if (numRecordsProduced % 1000 == 0) {\n+                    System.out.println(numRecordsProduced + \" records produced\");\n+                    System.out.flush();\n                 }\n-            });\n+                Utils.sleep(rand.nextInt(10));\n+            }\n+            producer.close();\n+            System.out.println(\"Producer closed: \" + numRecordsProduced + \" records produced\");\n+            System.out.flush();\n \n-            updateNumRecordsProduces(1);\n-            if (numRecordsProduced % 1000 == 0) {\n-                System.out.println(numRecordsProduced + \" records produced\");\n-                System.out.flush();\n+            // verify offsets\n+            for (final Map.Entry<Integer, List<Long>> offsetsOfPartition : offsets.entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY5MzUyNw==", "bodyText": "The logic looks fragile when the partitionRecords is empty. For all -1 cases, we add one more dummy record to the array being checked, or just remove the last element from the derived array so that we could maintain the same verification.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405693527", "createdAt": "2020-04-08T17:29:38Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -349,7 +389,7 @@ private static void verifyReceivedAllRecords(final Map<TopicPartition, List<Cons\n             final TopicPartition inputTopicPartition = new TopicPartition(\"data\", partitionRecords.getKey().partition());\n             final Iterator<ConsumerRecord<byte[], byte[]>> expectedRecord = expectedRecords.get(inputTopicPartition).iterator();\n \n-            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue()) {\n+            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue().subList(0, partitionRecords.getValue().size() - 1)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMzA0Mg=="}, "originalCommit": null, "originalPosition": 281}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY5NDEzOQ==", "bodyText": "What does this sleep do?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405694139", "createdAt": "2020-04-08T17:30:42Z", "author": {"login": "abbccdda"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -559,47 +601,36 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n                         exception.printStackTrace(System.err);\n                         System.err.flush();\n                         Exit.exit(1);\n+                    } else {\n+                        endMarkerOffset.put(new TopicPartition(metadata.topic(), metadata.partition()), metadata.offset());\n+                        System.out.println(\"Appended verification record to topic-partition \" + metadata.topic() + \"-\" + metadata.partition() + \" at offset \" + metadata.offset());\n+                        System.out.flush();\n                     }\n                 });\n             }\n         }\n \n-        final StringDeserializer stringDeserializer = new StringDeserializer();\n-\n-        long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-        while (!partitions.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n-            final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));\n-            if (records.isEmpty()) {\n-                System.out.println(\"No data received.\");\n-                for (final TopicPartition tp : partitions) {\n-                    System.out.println(tp + \" at position \" + consumer.position(tp));\n-                }\n-            }\n-            for (final ConsumerRecord<byte[], byte[]> record : records) {\n-                maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-                final String topic = record.topic();\n-                final TopicPartition tp = new TopicPartition(topic, record.partition());\n+        final long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n+        while (!endMarkerOffset.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n+            consumer.seekToEnd(partitions);\n \n-                try {\n-                    final String key = stringDeserializer.deserialize(topic, record.key());\n-                    final String value = stringDeserializer.deserialize(topic, record.value());\n+            final Iterator<TopicPartition> iterator = partitions.iterator();\n+            while (iterator.hasNext()) {\n+                final TopicPartition topicPartition = iterator.next();\n \n-                    if (!(\"key\".equals(key) && \"value\".equals(value) && partitions.remove(tp))) {\n-                        throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                            \"Expected record <'key','value'> from one of \" + partitions + \" but got\"\n-                            + \" <\" + key + \",\" + value + \"> [\" + record.topic() + \", \" + record.partition() + \"]\");\n-                    } else {\n-                        System.out.println(\"Verifying \" + tp + \" successful.\");\n-                    }\n-                } catch (final SerializationException e) {\n-                    throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                        \"Expected record <'key','value'> from one of \" + partitions + \" but got \" + record, e);\n+                if (consumer.position(topicPartition) > endMarkerOffset.get(topicPartition)) {\n+                    iterator.remove();\n+                    endMarkerOffset.remove(topicPartition);\n+                    System.out.println(\"Removing \" + topicPartition + \" at position \" + consumer.position(topicPartition));\n+                } else {\n+                    System.out.println(\"Retry \" + topicPartition + \" at position \" + consumer.position(topicPartition));\n                 }\n-\n             }\n+            sleep(1000L);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 420}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwMTc3NDQ4", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-390177448", "createdAt": "2020-04-08T17:20:43Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNzoyMDo0M1rOGC5Orw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNzo0NjoyOVrOGC6KKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY4Nzk4Mw==", "bodyText": "Can we use Util.waitUntilCondition here, or even simply using a latch and wait on it instead?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405687983", "createdAt": "2020-04-08T17:20:43Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {\n+            Exit.addShutdownHook(\"streams-eos-test-driver-shutdown-hook\", () -> {\n+                System.out.println(\"Terminating\");\n+                isRunning = false;\n+\n+                final long timeout = System.currentTimeMillis() + Duration.ofMinutes(5).toMillis();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTY5NDExNQ==", "bodyText": "I think the Exit.addShutdownHook block can be outside of the largest try/finally block of the method.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405694115", "createdAt": "2020-04-08T17:30:39Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -68,72 +69,115 @@ private static synchronized void updateNumRecordsProduces(final int delta) {\n     }\n \n     static void generate(final String kafka) {\n+        try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTcwMTgzNw==", "bodyText": "I'm wondering if we have to produce another record in the verifyAllTransactionFinished; for example, could we just check the end value of the newly added offsets map maintained by producer? If yes then we can remove this extra logic here and below.", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405701837", "createdAt": "2020-04-08T17:43:58Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -349,7 +389,7 @@ private static void verifyReceivedAllRecords(final Map<TopicPartition, List<Cons\n             final TopicPartition inputTopicPartition = new TopicPartition(\"data\", partitionRecords.getKey().partition());\n             final Iterator<ConsumerRecord<byte[], byte[]>> expectedRecord = expectedRecords.get(inputTopicPartition).iterator();\n \n-            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue()) {\n+            for (final ConsumerRecord<byte[], byte[]> receivedRecord : partitionRecords.getValue().subList(0, partitionRecords.getValue().size() - 1)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMzA0Mg=="}, "originalCommit": null, "originalPosition": 281}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTcwMjMxNw==", "bodyText": "See my question above: could we just rely on the maintained offsets map last values?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405702317", "createdAt": "2020-04-08T17:44:51Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -550,6 +590,8 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n         producerProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n \n+        final Map<TopicPartition, Long> endMarkerOffset = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExMzY3Ng=="}, "originalCommit": null, "originalPosition": 357}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTcwMzIxMQ==", "bodyText": "Why we want to relax this check here?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r405703211", "createdAt": "2020-04-08T17:46:29Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/tests/EosTestDriver.java", "diffHunk": "@@ -559,47 +601,36 @@ private static void verifyAllTransactionFinished(final KafkaConsumer<byte[], byt\n                         exception.printStackTrace(System.err);\n                         System.err.flush();\n                         Exit.exit(1);\n+                    } else {\n+                        endMarkerOffset.put(new TopicPartition(metadata.topic(), metadata.partition()), metadata.offset());\n+                        System.out.println(\"Appended verification record to topic-partition \" + metadata.topic() + \"-\" + metadata.partition() + \" at offset \" + metadata.offset());\n+                        System.out.flush();\n                     }\n                 });\n             }\n         }\n \n-        final StringDeserializer stringDeserializer = new StringDeserializer();\n-\n         long maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-        while (!partitions.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n-            final ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(100));\n-            if (records.isEmpty()) {\n-                System.out.println(\"No data received.\");\n-                for (final TopicPartition tp : partitions) {\n-                    System.out.println(tp + \" at position \" + consumer.position(tp));\n-                }\n-            }\n-            for (final ConsumerRecord<byte[], byte[]> record : records) {\n-                maxWaitTime = System.currentTimeMillis() + MAX_IDLE_TIME_MS;\n-                final String topic = record.topic();\n-                final TopicPartition tp = new TopicPartition(topic, record.partition());\n+        while (!endMarkerOffset.isEmpty() && System.currentTimeMillis() < maxWaitTime) {\n+            consumer.seekToEnd(partitions);\n \n-                try {\n-                    final String key = stringDeserializer.deserialize(topic, record.key());\n-                    final String value = stringDeserializer.deserialize(topic, record.value());\n+            final Iterator<TopicPartition> iterator = partitions.iterator();\n+            while(iterator.hasNext()) {\n+                final TopicPartition topicPartition = iterator.next();\n \n-                    if (!(\"key\".equals(key) && \"value\".equals(value) && partitions.remove(tp))) {\n-                        throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                            \"Expected record <'key','value'> from one of \" + partitions + \" but got\"\n-                            + \" <\" + key + \",\" + value + \"> [\" + record.topic() + \", \" + record.partition() + \"]\");\n-                    } else {\n-                        System.out.println(\"Verifying \" + tp + \" successful.\");\n-                    }\n-                } catch (final SerializationException e) {\n-                    throw new RuntimeException(\"Post transactions verification failed. Received unexpected verification record: \" +\n-                        \"Expected record <'key','value'> from one of \" + partitions + \" but got \" + record, e);\n+                if (consumer.position(topicPartition) > endMarkerOffset.get(topicPartition)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNTY4MA=="}, "originalCommit": null, "originalPosition": 410}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "37146f4cd2ae8c09330d2be45023ff2e5a6d0eb8", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/37146f4cd2ae8c09330d2be45023ff2e5a6d0eb8", "committedDate": "2020-04-10T19:16:10Z", "message": "MINOR: extend Kafka Streams EOS system test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2f426e832506af219e0fba54228f243fa7b2d61c", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/2f426e832506af219e0fba54228f243fa7b2d61c", "committedDate": "2020-04-10T19:16:10Z", "message": "fix verification"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5053a5ede7aa142b88e5170bc818e681ec09a693", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/5053a5ede7aa142b88e5170bc818e681ec09a693", "committedDate": "2020-04-10T19:16:10Z", "message": "Extend StandbyTask unit test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "04bef596b7f032ebe540ed3e8a3ed1028eaafdec", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/04bef596b7f032ebe540ed3e8a3ed1028eaafdec", "committedDate": "2020-04-10T19:16:10Z", "message": "Extand StandbyTask integration test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9203d9c5d2fc7188da05340b43d6d54c91c603ab", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/9203d9c5d2fc7188da05340b43d6d54c91c603ab", "committedDate": "2020-04-10T19:16:10Z", "message": "Extend TaskManger unit test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fa0f27c62e035778185deb76da2d7d26ba04409e", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/fa0f27c62e035778185deb76da2d7d26ba04409e", "committedDate": "2020-04-10T19:16:10Z", "message": "fix checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e48d9d8e2b8f9e2141770aa8fd0581bc0d56f3c6", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/e48d9d8e2b8f9e2141770aa8fd0581bc0d56f3c6", "committedDate": "2020-04-10T19:16:10Z", "message": "Github comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9960c51a6b8ef1d023316b87f35ab01b94fe4279", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/9960c51a6b8ef1d023316b87f35ab01b94fe4279", "committedDate": "2020-04-10T19:16:10Z", "message": "Fix consumer setup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a7585a23c00af689701e9e67fb557864631dcfde", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/a7585a23c00af689701e9e67fb557864631dcfde", "committedDate": "2020-04-10T19:16:10Z", "message": "Fix unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b11c0ce54e85db2ab130ce6d7763fca3c036dcf3", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/b11c0ce54e85db2ab130ce6d7763fca3c036dcf3", "committedDate": "2020-04-10T19:16:10Z", "message": "Bug fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8ed96485e258a6683c60c614c15b955010ef0e17", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/8ed96485e258a6683c60c614c15b955010ef0e17", "committedDate": "2020-04-10T19:16:46Z", "message": "rebase fix"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "8ed96485e258a6683c60c614c15b955010ef0e17", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/8ed96485e258a6683c60c614c15b955010ef0e17", "committedDate": "2020-04-10T19:16:46Z", "message": "rebase fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a56062d4c9efb20110c9cae7255aa1bb4dca097", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/5a56062d4c9efb20110c9cae7255aa1bb4dca097", "committedDate": "2020-04-13T21:06:14Z", "message": "Fix"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyNDQwNTMy", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-392440532", "createdAt": "2020-04-13T21:07:47Z", "commit": {"oid": "5a56062d4c9efb20110c9cae7255aa1bb4dca097"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMTowNzo0N1rOGE1YJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMTowNzo0N1rOGE1YJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzcyMjAyMg==", "bodyText": "If we hit a TaskCorruptedException, we know that only a task in restore mode could be affected and those don't have anything to be committed (their commitNeeded flag should be set to false). Hence, we just commit all non-corrupted tasks. Afterwards we can safely call handleCorruption() (if we don't commit, we might abort a pending transaction for eos-beta incorrectly within handleCorruption())\n\\cc @abbccdda @guozhangwang", "url": "https://github.com/apache/kafka/pull/8440#discussion_r407722022", "createdAt": "2020-04-13T21:07:47Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "diffHunk": "@@ -569,7 +570,13 @@ private void runLoop() {\n                 log.warn(\"Detected the states of tasks \" + e.corruptedTaskWithChangelogs() + \" are corrupted. \" +\n                              \"Will close the task as dirty and re-create and bootstrap from scratch.\", e);\n \n-                taskManager.commitAll();\n+                taskManager.commitInternal(\n+                    taskManager.tasks()\n+                        .values()\n+                        .stream()\n+                        .filter(t -> !e.corruptedTaskWithChangelogs().containsKey(t.id()))\n+                        .collect(Collectors.toSet())\n+                );", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5a56062d4c9efb20110c9cae7255aa1bb4dca097"}, "originalPosition": 28}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyNDUwODEx", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-392450811", "createdAt": "2020-04-13T21:24:51Z", "commit": {"oid": "5a56062d4c9efb20110c9cae7255aa1bb4dca097"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMToyNDo1MlrOGE15Jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QyMToyNDo1MlrOGE15Jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzczMDQ3MQ==", "bodyText": "Should we consider adding a unit test here, since this call is externalized?", "url": "https://github.com/apache/kafka/pull/8440#discussion_r407730471", "createdAt": "2020-04-13T21:24:52Z", "author": {"login": "abbccdda"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -741,7 +741,7 @@ int commitAll() {\n         return commitInternal(tasks.values());\n     }\n \n-    private int commitInternal(final Collection<Task> tasks) {\n+    int commitInternal(final Collection<Task> tasks) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5a56062d4c9efb20110c9cae7255aa1bb4dca097"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyNTM2MTM2", "url": "https://github.com/apache/kafka/pull/8440#pullrequestreview-392536136", "createdAt": "2020-04-14T01:03:15Z", "commit": {"oid": "5a56062d4c9efb20110c9cae7255aa1bb4dca097"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d506697bf9605caebe9b421bb088bddcdbe78c52", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/d506697bf9605caebe9b421bb088bddcdbe78c52", "committedDate": "2020-04-14T01:43:29Z", "message": "FIX BROKEN SYSTEM TEST RUN"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5c76bdcdf357fc7773198b00f2e712e28ab34797", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/5c76bdcdf357fc7773198b00f2e712e28ab34797", "committedDate": "2020-04-14T22:42:50Z", "message": "Revert \"FIX BROKEN SYSTEM TEST RUN\"\n\nThis reverts commit d506697bf9605caebe9b421bb088bddcdbe78c52."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "15e0b98ef384c15022ee76e6ca88b325567e31cb", "author": {"user": {"login": "mjsax", "name": "Matthias J. Sax"}}, "url": "https://github.com/apache/kafka/commit/15e0b98ef384c15022ee76e6ca88b325567e31cb", "committedDate": "2020-04-14T23:37:29Z", "message": "Add unit tests"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1400, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}