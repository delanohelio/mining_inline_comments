{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY2MDgyNTU5", "number": 8000, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QyMjo0MDoxN1rODaVdxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMToxMDozN1rODe9l1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4OTQxMjU0OnYy", "diffSide": "RIGHT", "path": "tests/kafkatest/tests/core/transactions_test.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QyMjo0MDoxN1rOFhPAjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QyMjo0MDoxN1rOFhPAjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM5MzIyOA==", "bodyText": "During the test, I realized one scenario which is pretty dangerous. If we have a hard failure and the txn offsets get sticky on the broker side, for next consumer it could take over one minute to back-off and wait for the clearance, thus timing out on the fetch. Shrinking to 10 seconds here is just a remediation, but we should pay attention to general usage as well.", "url": "https://github.com/apache/kafka/pull/8000#discussion_r370393228", "createdAt": "2020-01-23T22:40:17Z", "author": {"login": "abbccdda"}, "path": "tests/kafkatest/tests/core/transactions_test.py", "diffHunk": "@@ -47,6 +47,7 @@ def __init__(self, test_context):\n         self.num_output_partitions = 3\n         self.num_seed_messages = 100000\n         self.transaction_size = 750\n+        self.transaction_timeout = 10000", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDI1NjM5OnYy", "diffSide": "RIGHT", "path": "tests/kafkatest/tests/core/group_mode_transactions_test.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0MDo0MVrOFkUU8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0MDo0MVrOFkUU8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyNjA5OQ==", "bodyText": "This timeout was increased from 30 to 60 seconds for the sake of rebalance caused offset wait (if triggered twice, that would be roughly 20~30 seconds)", "url": "https://github.com/apache/kafka/pull/8000#discussion_r373626099", "createdAt": "2020-01-31T18:40:41Z", "author": {"login": "abbccdda"}, "path": "tests/kafkatest/tests/core/group_mode_transactions_test.py", "diffHunk": "@@ -0,0 +1,312 @@\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from kafkatest.services.zookeeper import ZookeeperService\n+from kafkatest.services.kafka import KafkaService\n+from kafkatest.services.console_consumer import ConsoleConsumer\n+from kafkatest.services.verifiable_producer import VerifiableProducer\n+from kafkatest.services.transactional_message_copier import TransactionalMessageCopier\n+from kafkatest.utils import is_int\n+\n+from ducktape.tests.test import Test\n+from ducktape.mark import matrix\n+from ducktape.mark.resource import cluster\n+from ducktape.utils.util import wait_until\n+\n+\n+class GroupModeTransactionsTest(Test):\n+    \"\"\"This test essentially does the same effort as TransactionsTest by transactionally copying data from a source topic to\n+    a destination topic and killing the copy process as well as the broker randomly through the process.\n+    The major difference is that we choose to work as a collaborated group with same topic subscription\n+    instead of individual consumers.\n+\n+    In the end we verify that the final output\n+    topic contains exactly one committed copy of each message in the input\n+    topic\n+    \"\"\"\n+    def __init__(self, test_context):\n+        \"\"\":type test_context: ducktape.tests.test.TestContext\"\"\"\n+        super(GroupModeTransactionsTest, self).__init__(test_context=test_context)\n+\n+        self.input_topic = \"input-topic\"\n+        self.output_topic = \"output-topic\"\n+\n+        self.num_brokers = 3\n+\n+        # Test parameters\n+        self.num_input_partitions = 9\n+        self.num_output_partitions = 9\n+        self.num_copiers = 3\n+        self.num_seed_messages = 100000\n+        self.transaction_size = 750\n+        self.transaction_timeout = 10000\n+        self.consumer_group = \"grouped-transactions-test-consumer-group\"\n+\n+        self.zk = ZookeeperService(test_context, num_nodes=1)\n+        self.kafka = KafkaService(test_context,\n+                                  num_nodes=self.num_brokers,\n+                                  zk=self.zk)\n+\n+    def setUp(self):\n+        self.zk.start()\n+\n+    def seed_messages(self, topic, num_seed_messages):\n+        seed_timeout_sec = 10000\n+        seed_producer = VerifiableProducer(context=self.test_context,\n+                                           num_nodes=1,\n+                                           kafka=self.kafka,\n+                                           topic=topic,\n+                                           message_validator=is_int,\n+                                           max_messages=num_seed_messages,\n+                                           enable_idempotence=True,\n+                                           repeating_keys=self.num_input_partitions)\n+        seed_producer.start()\n+        wait_until(lambda: seed_producer.num_acked >= num_seed_messages,\n+                   timeout_sec=seed_timeout_sec,\n+                   err_msg=\"Producer failed to produce messages %d in  %ds.\" % \\\n+                           (self.num_seed_messages, seed_timeout_sec))\n+        return seed_producer.acked_by_partition\n+\n+    def get_messages_from_topic(self, topic, num_messages):\n+        consumer = self.start_consumer(topic, group_id=\"verifying_consumer\")\n+        return self.drain_consumer(consumer, num_messages)\n+\n+    def bounce_brokers(self, clean_shutdown):\n+        for node in self.kafka.nodes:\n+            if clean_shutdown:\n+                self.kafka.restart_node(node, clean_shutdown = True)\n+            else:\n+                self.kafka.stop_node(node, clean_shutdown = False)\n+                wait_until(lambda: len(self.kafka.pids(node)) == 0 and not self.kafka.is_registered(node),\n+                           timeout_sec=self.kafka.zk_session_timeout + 5,\n+                           err_msg=\"Failed to see timely deregistration of \\\n+                           hard-killed broker %s\" % str(node.account))\n+                self.kafka.start_node(node)\n+\n+    def create_and_start_message_copier(self, input_topic, output_topic, transactional_id):\n+        message_copier = TransactionalMessageCopier(\n+            context=self.test_context,\n+            num_nodes=1,\n+            kafka=self.kafka,\n+            transactional_id=transactional_id,\n+            consumer_group=self.consumer_group,\n+            input_topic=input_topic,\n+            input_partition=-1,\n+            output_topic=output_topic,\n+            max_messages=-1,\n+            transaction_size=self.transaction_size,\n+            transaction_timeout=self.transaction_timeout,\n+            group_mode=True\n+        )\n+        message_copier.start()\n+        wait_until(lambda: message_copier.alive(message_copier.nodes[0]),\n+                   timeout_sec=10,\n+                   err_msg=\"Message copier failed to start after 10 s\")\n+        return message_copier\n+\n+    def bounce_copiers(self, copiers, clean_shutdown, timeout_sec=60):", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDI2NTMxOnYy", "diffSide": "RIGHT", "path": "tests/kafkatest/tests/core/transactions_test.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0Mzo1MVrOFkUajA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0Mzo1MVrOFkUajA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyNzUzMg==", "bodyText": "The changes in transactions_test are mainly for compatibility. Nothing should be behaving differently", "url": "https://github.com/apache/kafka/pull/8000#discussion_r373627532", "createdAt": "2020-01-31T18:43:51Z", "author": {"login": "abbccdda"}, "path": "tests/kafkatest/tests/core/transactions_test.py", "diffHunk": "@@ -218,8 +225,9 @@ def setup_topics(self):\n     @cluster(num_nodes=9)\n     @matrix(failure_mode=[\"hard_bounce\", \"clean_bounce\"],\n             bounce_target=[\"brokers\", \"clients\"],\n-            check_order=[True, False])\n-    def test_transactions(self, failure_mode, bounce_target, check_order):\n+            check_order=[True, False],\n+            use_group_metadata=[True, False])", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDI3NDgyOnYy", "diffSide": "RIGHT", "path": "tools/src/main/java/org/apache/kafka/tools/VerifiableConsumer.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0NzoxM1rOFkUgjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQxODo0NzoxM1rOFkUgjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzYyOTA2OA==", "bodyText": "Only minor style fixes in this class", "url": "https://github.com/apache/kafka/pull/8000#discussion_r373629068", "createdAt": "2020-01-31T18:47:13Z", "author": {"login": "abbccdda"}, "path": "tools/src/main/java/org/apache/kafka/tools/VerifiableConsumer.java", "diffHunk": "@@ -159,8 +159,9 @@ private boolean isFinished() {\n                     partitionRecords.size(), minOffset, maxOffset));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODkxMjcyOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjoyMjo0OFrOFlljsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjoyMjo0OFrOFlljsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1Njk3OA==", "bodyText": "This line is a bit confusing: the delayed rebalance is true / false, what does this mean?", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374956978", "createdAt": "2020-02-04T22:22:48Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala", "diffHunk": "@@ -1033,7 +1034,9 @@ class GroupCoordinator(val brokerId: Int,\n     group.transitionTo(PreparingRebalance)\n \n     info(s\"Preparing to rebalance group ${group.groupId} in state ${group.currentState} with old generation \" +\n-      s\"${group.generationId} (${Topic.GROUP_METADATA_TOPIC_NAME}-${partitionFor(group.groupId)}) (reason: $reason)\")\n+      s\"${group.generationId} (${Topic.GROUP_METADATA_TOPIC_NAME}-${partitionFor(group.groupId)}) (reason: $reason), \" +\n+      s\"the remaining instances are ${group.allMembers}, and the delayed rebalance is $startEmpty \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODkxNDMxOnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjoyMzoyNVrOFllktg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMDoyMTozOVrOFoXv8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1NzIzOA==", "bodyText": "Is this part of the KIP to change the default value?", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374957238", "createdAt": "2020-02-04T22:23:25Z", "author": {"login": "guozhangwang"}, "path": "core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala", "diffHunk": "@@ -47,7 +47,7 @@ object TransactionStateManager {\n   // default transaction management config values\n   val DefaultTransactionsMaxTimeoutMs: Int = TimeUnit.MINUTES.toMillis(15).toInt\n   val DefaultTransactionalIdExpirationMs: Int = TimeUnit.DAYS.toMillis(7).toInt\n-  val DefaultAbortTimedOutTransactionsIntervalMs: Int = TimeUnit.MINUTES.toMillis(1).toInt\n+  val DefaultAbortTimedOutTransactionsIntervalMs: Int = TimeUnit.SECONDS.toMillis(10).toInt", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ5MjUxNA==", "bodyText": "Do we still want this change, even if Kafka Streams changes slip? Maybe ok, because people might write plain consumer/producer apps using the new fencing. Just double checking.", "url": "https://github.com/apache/kafka/pull/8000#discussion_r375492514", "createdAt": "2020-02-05T20:33:06Z", "author": {"login": "mjsax"}, "path": "core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala", "diffHunk": "@@ -47,7 +47,7 @@ object TransactionStateManager {\n   // default transaction management config values\n   val DefaultTransactionsMaxTimeoutMs: Int = TimeUnit.MINUTES.toMillis(15).toInt\n   val DefaultTransactionalIdExpirationMs: Int = TimeUnit.DAYS.toMillis(7).toInt\n-  val DefaultAbortTimedOutTransactionsIntervalMs: Int = TimeUnit.MINUTES.toMillis(1).toInt\n+  val DefaultAbortTimedOutTransactionsIntervalMs: Int = TimeUnit.SECONDS.toMillis(10).toInt", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1NzIzOA=="}, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg3NjQ2NA==", "bodyText": "The KIP and mailing list just get updated with this minor change FYI", "url": "https://github.com/apache/kafka/pull/8000#discussion_r377876464", "createdAt": "2020-02-11T20:21:39Z", "author": {"login": "abbccdda"}, "path": "core/src/main/scala/kafka/coordinator/transaction/TransactionStateManager.scala", "diffHunk": "@@ -47,7 +47,7 @@ object TransactionStateManager {\n   // default transaction management config values\n   val DefaultTransactionsMaxTimeoutMs: Int = TimeUnit.MINUTES.toMillis(15).toInt\n   val DefaultTransactionalIdExpirationMs: Int = TimeUnit.DAYS.toMillis(7).toInt\n-  val DefaultAbortTimedOutTransactionsIntervalMs: Int = TimeUnit.MINUTES.toMillis(1).toInt\n+  val DefaultAbortTimedOutTransactionsIntervalMs: Int = TimeUnit.SECONDS.toMillis(10).toInt", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1NzIzOA=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODkxOTcwOnYy", "diffSide": "RIGHT", "path": "tests/kafkatest/services/console_consumer.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjoyNTozOVrOFlloPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMDoyNTozN1rOFoX3bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1ODE0MQ==", "bodyText": "Where is this value used?", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374958141", "createdAt": "2020-02-04T22:25:39Z", "author": {"login": "guozhangwang"}, "path": "tests/kafkatest/services/console_consumer.py", "diffHunk": "@@ -105,9 +106,11 @@ def __init__(self, context, num_nodes, kafka, topic, group_id=\"test-consumer-gro\n         self.from_beginning = from_beginning\n         self.message_validator = message_validator\n         self.messages_consumed = {idx: [] for idx in range(1, num_nodes + 1)}\n+        self.messages_consumed_by_partition = {idx: [] for idx in range(1, num_nodes + 1)}", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg3ODM4Mg==", "bodyText": "Should be removed", "url": "https://github.com/apache/kafka/pull/8000#discussion_r377878382", "createdAt": "2020-02-11T20:25:37Z", "author": {"login": "abbccdda"}, "path": "tests/kafkatest/services/console_consumer.py", "diffHunk": "@@ -105,9 +106,11 @@ def __init__(self, context, num_nodes, kafka, topic, group_id=\"test-consumer-gro\n         self.from_beginning = from_beginning\n         self.message_validator = message_validator\n         self.messages_consumed = {idx: [] for idx in range(1, num_nodes + 1)}\n+        self.messages_consumed_by_partition = {idx: [] for idx in range(1, num_nodes + 1)}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1ODE0MQ=="}, "originalCommit": null, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODkyNDk4OnYy", "diffSide": "RIGHT", "path": "tests/kafkatest/services/transactional_message_copier.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjoyNzo0N1rOFllroQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMDoyNzowNlrOFoX6Tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1OTAwOQ==", "bodyText": "nit: some = are spaced before / after and some are not.", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374959009", "createdAt": "2020-02-04T22:27:47Z", "author": {"login": "guozhangwang"}, "path": "tests/kafkatest/services/transactional_message_copier.py", "diffHunk": "@@ -47,12 +47,13 @@ class TransactionalMessageCopier(KafkaPathResolverMixin, BackgroundThreadService\n \n     def __init__(self, context, num_nodes, kafka, transactional_id, consumer_group,\n                  input_topic, input_partition, output_topic, max_messages = -1,\n-                 transaction_size = 1000, enable_random_aborts=True):\n+                 transaction_size = 1000, transaction_timeout = None, enable_random_aborts=True, use_group_metadata=False, group_mode=False):", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg3OTExOA==", "bodyText": "Nice catch!", "url": "https://github.com/apache/kafka/pull/8000#discussion_r377879118", "createdAt": "2020-02-11T20:27:06Z", "author": {"login": "abbccdda"}, "path": "tests/kafkatest/services/transactional_message_copier.py", "diffHunk": "@@ -47,12 +47,13 @@ class TransactionalMessageCopier(KafkaPathResolverMixin, BackgroundThreadService\n \n     def __init__(self, context, num_nodes, kafka, transactional_id, consumer_group,\n                  input_topic, input_partition, output_topic, max_messages = -1,\n-                 transaction_size = 1000, enable_random_aborts=True):\n+                 transaction_size = 1000, transaction_timeout = None, enable_random_aborts=True, use_group_metadata=False, group_mode=False):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1OTAwOQ=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODk0MjU4OnYy", "diffSide": "RIGHT", "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjozMzo1OVrOFll2fQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjozMzo1OVrOFll2fQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2MTc4OQ==", "bodyText": "Why these values need to be atomic?", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374961789", "createdAt": "2020-02-04T22:33:59Z", "author": {"login": "guozhangwang"}, "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -226,80 +259,133 @@ private static String toJsonString(Map<String, Object> data) {\n         return json;\n     }\n \n-    private static String statusAsJson(long consumed, long remaining, String transactionalId) {\n+    private static String statusAsJson(long totalProcessed, long consumed, long remaining, String transactionalId, String stage) {\n         Map<String, Object> statusData = new HashMap<>();\n         statusData.put(\"progress\", transactionalId);\n+        statusData.put(\"totalProcessed\", totalProcessed);\n         statusData.put(\"consumed\", consumed);\n         statusData.put(\"remaining\", remaining);\n+        statusData.put(\"time\", FORMAT.format(new Date()));\n+        statusData.put(\"stage\", stage);\n         return toJsonString(statusData);\n     }\n \n-    private static String shutDownString(long consumed, long remaining, String transactionalId) {\n+    private static String shutDownString(long totalProcessed, long consumed, long remaining, String transactionalId) {\n         Map<String, Object> shutdownData = new HashMap<>();\n-        shutdownData.put(\"remaining\", remaining);\n-        shutdownData.put(\"consumed\", consumed);\n         shutdownData.put(\"shutdown_complete\", transactionalId);\n+        shutdownData.put(\"totalProcessed\", totalProcessed);\n+        shutdownData.put(\"consumed\", consumed);\n+        shutdownData.put(\"remaining\", remaining);\n+        shutdownData.put(\"time\", FORMAT.format(new Date()));\n         return toJsonString(shutdownData);\n     }\n \n-    public static void main(String[] args) throws IOException {\n+    public static void main(String[] args) {\n         Namespace parsedArgs = argParser().parseArgsOrFail(args);\n         Integer numMessagesPerTransaction = parsedArgs.getInt(\"messagesPerTransaction\");\n         final String transactionalId = parsedArgs.getString(\"transactionalId\");\n         final String outputTopic = parsedArgs.getString(\"outputTopic\");\n \n         String consumerGroup = parsedArgs.getString(\"consumerGroup\");\n-        TopicPartition inputPartition = new TopicPartition(parsedArgs.getString(\"inputTopic\"), parsedArgs.getInt(\"inputPartition\"));\n \n         final KafkaProducer<String, String> producer = createProducer(parsedArgs);\n         final KafkaConsumer<String, String> consumer = createConsumer(parsedArgs);\n \n-        consumer.assign(singleton(inputPartition));\n+        final AtomicLong messageCap = new AtomicLong(\n+            parsedArgs.getInt(\"maxMessages\") == -1 ? Long.MAX_VALUE : parsedArgs.getInt(\"maxMessages\"));\n+\n+        boolean groupMode = parsedArgs.getBoolean(\"groupMode\");\n+        String topicName = parsedArgs.getString(\"inputTopic\");\n+        final AtomicLong remainingMessages = new AtomicLong(messageCap.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 158}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODk0NDg2OnYy", "diffSide": "RIGHT", "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjozNDo1MlrOFll39g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQyMDo0ODozN1rOFmGqqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2MjE2Ng==", "bodyText": "I had a similar question on the other PR: are we suggesting for manual assignment to use the original overload (hence we would never deprecate it)?", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374962166", "createdAt": "2020-02-04T22:34:52Z", "author": {"login": "guozhangwang"}, "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -226,80 +259,133 @@ private static String toJsonString(Map<String, Object> data) {\n         return json;\n     }\n \n-    private static String statusAsJson(long consumed, long remaining, String transactionalId) {\n+    private static String statusAsJson(long totalProcessed, long consumed, long remaining, String transactionalId, String stage) {\n         Map<String, Object> statusData = new HashMap<>();\n         statusData.put(\"progress\", transactionalId);\n+        statusData.put(\"totalProcessed\", totalProcessed);\n         statusData.put(\"consumed\", consumed);\n         statusData.put(\"remaining\", remaining);\n+        statusData.put(\"time\", FORMAT.format(new Date()));\n+        statusData.put(\"stage\", stage);\n         return toJsonString(statusData);\n     }\n \n-    private static String shutDownString(long consumed, long remaining, String transactionalId) {\n+    private static String shutDownString(long totalProcessed, long consumed, long remaining, String transactionalId) {\n         Map<String, Object> shutdownData = new HashMap<>();\n-        shutdownData.put(\"remaining\", remaining);\n-        shutdownData.put(\"consumed\", consumed);\n         shutdownData.put(\"shutdown_complete\", transactionalId);\n+        shutdownData.put(\"totalProcessed\", totalProcessed);\n+        shutdownData.put(\"consumed\", consumed);\n+        shutdownData.put(\"remaining\", remaining);\n+        shutdownData.put(\"time\", FORMAT.format(new Date()));\n         return toJsonString(shutdownData);\n     }\n \n-    public static void main(String[] args) throws IOException {\n+    public static void main(String[] args) {\n         Namespace parsedArgs = argParser().parseArgsOrFail(args);\n         Integer numMessagesPerTransaction = parsedArgs.getInt(\"messagesPerTransaction\");\n         final String transactionalId = parsedArgs.getString(\"transactionalId\");\n         final String outputTopic = parsedArgs.getString(\"outputTopic\");\n \n         String consumerGroup = parsedArgs.getString(\"consumerGroup\");\n-        TopicPartition inputPartition = new TopicPartition(parsedArgs.getString(\"inputTopic\"), parsedArgs.getInt(\"inputPartition\"));\n \n         final KafkaProducer<String, String> producer = createProducer(parsedArgs);\n         final KafkaConsumer<String, String> consumer = createConsumer(parsedArgs);\n \n-        consumer.assign(singleton(inputPartition));\n+        final AtomicLong messageCap = new AtomicLong(\n+            parsedArgs.getInt(\"maxMessages\") == -1 ? Long.MAX_VALUE : parsedArgs.getInt(\"maxMessages\"));\n+\n+        boolean groupMode = parsedArgs.getBoolean(\"groupMode\");\n+        String topicName = parsedArgs.getString(\"inputTopic\");\n+        final AtomicLong remainingMessages = new AtomicLong(messageCap.get());\n+        final AtomicLong numMessagesProcessedSinceLastRebalance = new AtomicLong(0);\n+        final AtomicLong totalMessageProcessed = new AtomicLong(0);\n+        if (groupMode) {\n+            consumer.subscribe(Collections.singleton(topicName), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    messageCap.set(partitions.stream()\n+                        .mapToLong(partition -> messagesRemaining(consumer, partition)).sum());\n+                    log.info(\"Message cap set to {} on rebalance complete\", messageCap);\n+                    numMessagesProcessedSinceLastRebalance.set(0);\n+                    // We use message cap for remaining here as the remainingMessages are not set yet.\n+                    System.out.println(statusAsJson(totalMessageProcessed.get(),\n+                        numMessagesProcessedSinceLastRebalance.get(), messageCap.get(), transactionalId, \"RebalanceComplete\"));\n+                }\n+            });\n+        } else {\n+            TopicPartition inputPartition = new TopicPartition(topicName, parsedArgs.getInt(\"inputPartition\"));\n+            consumer.assign(singleton(inputPartition));\n+            messageCap.set(Math.min(messagesRemaining(consumer, inputPartition), messageCap.get()));\n+            remainingMessages.set(messageCap.get());\n+        }\n \n-        long maxMessages = parsedArgs.getInt(\"maxMessages\") == -1 ? Long.MAX_VALUE : parsedArgs.getInt(\"maxMessages\");\n-        maxMessages = Math.min(messagesRemaining(consumer, inputPartition), maxMessages);\n         final boolean enableRandomAborts = parsedArgs.getBoolean(\"enableRandomAborts\");\n \n         producer.initTransactions();\n \n         final AtomicBoolean isShuttingDown = new AtomicBoolean(false);\n-        final AtomicLong remainingMessages = new AtomicLong(maxMessages);\n-        final AtomicLong numMessagesProcessed = new AtomicLong(0);\n+\n         Exit.addShutdownHook(\"transactional-message-copier-shutdown-hook\", () -> {\n             isShuttingDown.set(true);\n             // Flush any remaining messages\n             producer.close();\n             synchronized (consumer) {\n                 consumer.close();\n             }\n-            System.out.println(shutDownString(numMessagesProcessed.get(), remainingMessages.get(), transactionalId));\n+            System.out.println(shutDownString(totalMessageProcessed.get(),\n+                numMessagesProcessedSinceLastRebalance.get(), remainingMessages.get(), transactionalId));\n         });\n \n+        final boolean useGroupMetadata = parsedArgs.getBoolean(\"useGroupMetadata\");\n         try {\n             Random random = new Random();\n-            while (0 < remainingMessages.get()) {\n-                System.out.println(statusAsJson(numMessagesProcessed.get(), remainingMessages.get(), transactionalId));\n+            while (remainingMessages.get() > 0) {\n+                System.out.println(statusAsJson(totalMessageProcessed.get(),\n+                    numMessagesProcessedSinceLastRebalance.get(), remainingMessages.get(), transactionalId, \"ProcessLoop\"));\n                 if (isShuttingDown.get())\n                     break;\n-                int messagesInCurrentTransaction = 0;\n-                long numMessagesForNextTransaction = Math.min(numMessagesPerTransaction, remainingMessages.get());\n+                long messagesSentWithinCurrentTxn = 0L;\n+                long messagesNeededForCurrentTxn = remainingMessages.get();\n \n                 try {\n                     producer.beginTransaction();\n-                    while (messagesInCurrentTransaction < numMessagesForNextTransaction) {\n+                    Map<Integer, Integer> partitionCount = new HashMap<>();\n+                    while (messagesSentWithinCurrentTxn < messagesNeededForCurrentTxn) {\n                         ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(200));\n+                        log.info(\"number of consumer records fetched: {}\", records.count());\n+                        if (messageCap.get() <= 0) {\n+                            // We could see no more message needed for processing after poll\n+                            break;\n+                        }\n                         for (ConsumerRecord<String, String> record : records) {\n                             producer.send(producerRecordFromConsumerRecord(outputTopic, record));\n-                            messagesInCurrentTransaction++;\n+                            messagesSentWithinCurrentTxn++;\n+                            partitionCount.put(record.partition(), partitionCount.getOrDefault(record.partition(), 0) + 1);\n                         }\n+                        messagesNeededForCurrentTxn = Math.min(numMessagesPerTransaction, remainingMessages.get());\n+                    }\n+\n+                    log.info(\"Messages sent in current transaction: {}\", messagesSentWithinCurrentTxn);\n+                    for (Map.Entry<Integer, Integer> entry : partitionCount.entrySet()) {\n+                        log.info(\"Partition {} has contributed {} records\", entry.getKey(), entry.getValue());\n+                    }\n+\n+                    if (useGroupMetadata) {\n+                        producer.sendOffsetsToTransaction(consumerPositions(consumer), consumer.groupMetadata());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 248}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ5OTQzNQ==", "bodyText": "I think the main reason why we would not deprecate the existing overload is a \"producer only\" application for which there is not even a consumer.", "url": "https://github.com/apache/kafka/pull/8000#discussion_r375499435", "createdAt": "2020-02-05T20:48:37Z", "author": {"login": "mjsax"}, "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -226,80 +259,133 @@ private static String toJsonString(Map<String, Object> data) {\n         return json;\n     }\n \n-    private static String statusAsJson(long consumed, long remaining, String transactionalId) {\n+    private static String statusAsJson(long totalProcessed, long consumed, long remaining, String transactionalId, String stage) {\n         Map<String, Object> statusData = new HashMap<>();\n         statusData.put(\"progress\", transactionalId);\n+        statusData.put(\"totalProcessed\", totalProcessed);\n         statusData.put(\"consumed\", consumed);\n         statusData.put(\"remaining\", remaining);\n+        statusData.put(\"time\", FORMAT.format(new Date()));\n+        statusData.put(\"stage\", stage);\n         return toJsonString(statusData);\n     }\n \n-    private static String shutDownString(long consumed, long remaining, String transactionalId) {\n+    private static String shutDownString(long totalProcessed, long consumed, long remaining, String transactionalId) {\n         Map<String, Object> shutdownData = new HashMap<>();\n-        shutdownData.put(\"remaining\", remaining);\n-        shutdownData.put(\"consumed\", consumed);\n         shutdownData.put(\"shutdown_complete\", transactionalId);\n+        shutdownData.put(\"totalProcessed\", totalProcessed);\n+        shutdownData.put(\"consumed\", consumed);\n+        shutdownData.put(\"remaining\", remaining);\n+        shutdownData.put(\"time\", FORMAT.format(new Date()));\n         return toJsonString(shutdownData);\n     }\n \n-    public static void main(String[] args) throws IOException {\n+    public static void main(String[] args) {\n         Namespace parsedArgs = argParser().parseArgsOrFail(args);\n         Integer numMessagesPerTransaction = parsedArgs.getInt(\"messagesPerTransaction\");\n         final String transactionalId = parsedArgs.getString(\"transactionalId\");\n         final String outputTopic = parsedArgs.getString(\"outputTopic\");\n \n         String consumerGroup = parsedArgs.getString(\"consumerGroup\");\n-        TopicPartition inputPartition = new TopicPartition(parsedArgs.getString(\"inputTopic\"), parsedArgs.getInt(\"inputPartition\"));\n \n         final KafkaProducer<String, String> producer = createProducer(parsedArgs);\n         final KafkaConsumer<String, String> consumer = createConsumer(parsedArgs);\n \n-        consumer.assign(singleton(inputPartition));\n+        final AtomicLong messageCap = new AtomicLong(\n+            parsedArgs.getInt(\"maxMessages\") == -1 ? Long.MAX_VALUE : parsedArgs.getInt(\"maxMessages\"));\n+\n+        boolean groupMode = parsedArgs.getBoolean(\"groupMode\");\n+        String topicName = parsedArgs.getString(\"inputTopic\");\n+        final AtomicLong remainingMessages = new AtomicLong(messageCap.get());\n+        final AtomicLong numMessagesProcessedSinceLastRebalance = new AtomicLong(0);\n+        final AtomicLong totalMessageProcessed = new AtomicLong(0);\n+        if (groupMode) {\n+            consumer.subscribe(Collections.singleton(topicName), new ConsumerRebalanceListener() {\n+                @Override\n+                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n+                }\n+\n+                @Override\n+                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n+                    messageCap.set(partitions.stream()\n+                        .mapToLong(partition -> messagesRemaining(consumer, partition)).sum());\n+                    log.info(\"Message cap set to {} on rebalance complete\", messageCap);\n+                    numMessagesProcessedSinceLastRebalance.set(0);\n+                    // We use message cap for remaining here as the remainingMessages are not set yet.\n+                    System.out.println(statusAsJson(totalMessageProcessed.get(),\n+                        numMessagesProcessedSinceLastRebalance.get(), messageCap.get(), transactionalId, \"RebalanceComplete\"));\n+                }\n+            });\n+        } else {\n+            TopicPartition inputPartition = new TopicPartition(topicName, parsedArgs.getInt(\"inputPartition\"));\n+            consumer.assign(singleton(inputPartition));\n+            messageCap.set(Math.min(messagesRemaining(consumer, inputPartition), messageCap.get()));\n+            remainingMessages.set(messageCap.get());\n+        }\n \n-        long maxMessages = parsedArgs.getInt(\"maxMessages\") == -1 ? Long.MAX_VALUE : parsedArgs.getInt(\"maxMessages\");\n-        maxMessages = Math.min(messagesRemaining(consumer, inputPartition), maxMessages);\n         final boolean enableRandomAborts = parsedArgs.getBoolean(\"enableRandomAborts\");\n \n         producer.initTransactions();\n \n         final AtomicBoolean isShuttingDown = new AtomicBoolean(false);\n-        final AtomicLong remainingMessages = new AtomicLong(maxMessages);\n-        final AtomicLong numMessagesProcessed = new AtomicLong(0);\n+\n         Exit.addShutdownHook(\"transactional-message-copier-shutdown-hook\", () -> {\n             isShuttingDown.set(true);\n             // Flush any remaining messages\n             producer.close();\n             synchronized (consumer) {\n                 consumer.close();\n             }\n-            System.out.println(shutDownString(numMessagesProcessed.get(), remainingMessages.get(), transactionalId));\n+            System.out.println(shutDownString(totalMessageProcessed.get(),\n+                numMessagesProcessedSinceLastRebalance.get(), remainingMessages.get(), transactionalId));\n         });\n \n+        final boolean useGroupMetadata = parsedArgs.getBoolean(\"useGroupMetadata\");\n         try {\n             Random random = new Random();\n-            while (0 < remainingMessages.get()) {\n-                System.out.println(statusAsJson(numMessagesProcessed.get(), remainingMessages.get(), transactionalId));\n+            while (remainingMessages.get() > 0) {\n+                System.out.println(statusAsJson(totalMessageProcessed.get(),\n+                    numMessagesProcessedSinceLastRebalance.get(), remainingMessages.get(), transactionalId, \"ProcessLoop\"));\n                 if (isShuttingDown.get())\n                     break;\n-                int messagesInCurrentTransaction = 0;\n-                long numMessagesForNextTransaction = Math.min(numMessagesPerTransaction, remainingMessages.get());\n+                long messagesSentWithinCurrentTxn = 0L;\n+                long messagesNeededForCurrentTxn = remainingMessages.get();\n \n                 try {\n                     producer.beginTransaction();\n-                    while (messagesInCurrentTransaction < numMessagesForNextTransaction) {\n+                    Map<Integer, Integer> partitionCount = new HashMap<>();\n+                    while (messagesSentWithinCurrentTxn < messagesNeededForCurrentTxn) {\n                         ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(200));\n+                        log.info(\"number of consumer records fetched: {}\", records.count());\n+                        if (messageCap.get() <= 0) {\n+                            // We could see no more message needed for processing after poll\n+                            break;\n+                        }\n                         for (ConsumerRecord<String, String> record : records) {\n                             producer.send(producerRecordFromConsumerRecord(outputTopic, record));\n-                            messagesInCurrentTransaction++;\n+                            messagesSentWithinCurrentTxn++;\n+                            partitionCount.put(record.partition(), partitionCount.getOrDefault(record.partition(), 0) + 1);\n                         }\n+                        messagesNeededForCurrentTxn = Math.min(numMessagesPerTransaction, remainingMessages.get());\n+                    }\n+\n+                    log.info(\"Messages sent in current transaction: {}\", messagesSentWithinCurrentTxn);\n+                    for (Map.Entry<Integer, Integer> entry : partitionCount.entrySet()) {\n+                        log.info(\"Partition {} has contributed {} records\", entry.getKey(), entry.getValue());\n+                    }\n+\n+                    if (useGroupMetadata) {\n+                        producer.sendOffsetsToTransaction(consumerPositions(consumer), consumer.groupMetadata());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2MjE2Ng=="}, "originalCommit": null, "originalPosition": 248}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODk1NDgxOnYy", "diffSide": "RIGHT", "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjozODoxOVrOFll97w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjozODoxOVrOFll97w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2MzY5NQ==", "bodyText": "nit: consumedSinceLastRebalanced.", "url": "https://github.com/apache/kafka/pull/8000#discussion_r374963695", "createdAt": "2020-02-04T22:38:19Z", "author": {"login": "guozhangwang"}, "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -226,80 +259,133 @@ private static String toJsonString(Map<String, Object> data) {\n         return json;\n     }\n \n-    private static String statusAsJson(long consumed, long remaining, String transactionalId) {\n+    private static String statusAsJson(long totalProcessed, long consumed, long remaining, String transactionalId, String stage) {\n         Map<String, Object> statusData = new HashMap<>();\n         statusData.put(\"progress\", transactionalId);\n+        statusData.put(\"totalProcessed\", totalProcessed);\n         statusData.put(\"consumed\", consumed);\n         statusData.put(\"remaining\", remaining);\n+        statusData.put(\"time\", FORMAT.format(new Date()));\n+        statusData.put(\"stage\", stage);\n         return toJsonString(statusData);\n     }\n \n-    private static String shutDownString(long consumed, long remaining, String transactionalId) {\n+    private static String shutDownString(long totalProcessed, long consumed, long remaining, String transactionalId) {\n         Map<String, Object> shutdownData = new HashMap<>();\n-        shutdownData.put(\"remaining\", remaining);\n-        shutdownData.put(\"consumed\", consumed);\n         shutdownData.put(\"shutdown_complete\", transactionalId);\n+        shutdownData.put(\"totalProcessed\", totalProcessed);\n+        shutdownData.put(\"consumed\", consumed);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyMjMwNjU3OnYy", "diffSide": "RIGHT", "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQyMDozMTo1MlrOFmGNhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQyMDozMTo1MlrOFmGNhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ5MTk3NQ==", "bodyText": "Is INFO level appropriate? (If yes, the message read a little too code centric IMHO)", "url": "https://github.com/apache/kafka/pull/8000#discussion_r375491975", "createdAt": "2020-02-05T20:31:52Z", "author": {"login": "mjsax"}, "path": "core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala", "diffHunk": "@@ -362,7 +362,10 @@ private[group] class GroupMetadata(val groupId: String, initialState: GroupState\n \n   def notYetRejoinedMembers = members.values.filter(!_.isAwaitingJoin).toList\n \n-  def hasAllMembersJoined = members.size == numMembersAwaitingJoin && pendingMembers.isEmpty\n+  def hasAllMembersJoined = {\n+    info(s\"The join complete condition checking: members => $members, members waiting join count: $numMembersAwaitingJoin, pending members: $pendingMembers\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyMjM0OTEyOnYy", "diffSide": "RIGHT", "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQyMDo0NzowMlrOFmGn2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTowOTozOVrOFoZLDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ5ODcxMw==", "bodyText": "If we are in groupMode and enable userGroupMetadata should we use a different transaction.id for each producer instead of the same?", "url": "https://github.com/apache/kafka/pull/8000#discussion_r375498713", "createdAt": "2020-02-05T20:47:02Z", "author": {"login": "mjsax"}, "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -137,16 +156,28 @@ private static ArgumentParser argParser() {\n                 .dest(\"enableRandomAborts\")\n                 .help(\"Whether or not to enable random transaction aborts (for system testing)\");\n \n+        parser.addArgument(\"--group-mode\")\n+                .action(storeTrue())\n+                .type(Boolean.class)\n+                .metavar(\"GROUP-MODE\")\n+                .dest(\"groupMode\")\n+                .help(\"Whether to let consumer subscribe to the input topic or do manual assign. If we do\" +\n+                          \" subscription based consumption, the input partition shall be ignored\");\n+\n+        parser.addArgument(\"--use-group-metadata\")\n+                .action(storeTrue())\n+                .type(Boolean.class)\n+                .metavar(\"USE-GROUP-METADATA\")\n+                .dest(\"useGroupMetadata\")\n+                .help(\"Whether to use the new transactional commit API with group metadata\");\n+\n         return parser;\n     }\n \n     private static KafkaProducer<String, String> createProducer(Namespace parsedArgs) {\n-        String transactionalId = parsedArgs.getString(\"transactionalId\");\n-        String brokerList = parsedArgs.getString(\"brokerList\");\n-\n         Properties props = new Properties();\n-        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);\n-        props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId);\n+        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, parsedArgs.getString(\"brokerList\"));\n+        props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, parsedArgs.getString(\"transactionalId\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg5OTc4OA==", "bodyText": "Should be guaranteed by the python test suite level", "url": "https://github.com/apache/kafka/pull/8000#discussion_r377899788", "createdAt": "2020-02-11T21:09:39Z", "author": {"login": "abbccdda"}, "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -137,16 +156,28 @@ private static ArgumentParser argParser() {\n                 .dest(\"enableRandomAborts\")\n                 .help(\"Whether or not to enable random transaction aborts (for system testing)\");\n \n+        parser.addArgument(\"--group-mode\")\n+                .action(storeTrue())\n+                .type(Boolean.class)\n+                .metavar(\"GROUP-MODE\")\n+                .dest(\"groupMode\")\n+                .help(\"Whether to let consumer subscribe to the input topic or do manual assign. If we do\" +\n+                          \" subscription based consumption, the input partition shall be ignored\");\n+\n+        parser.addArgument(\"--use-group-metadata\")\n+                .action(storeTrue())\n+                .type(Boolean.class)\n+                .metavar(\"USE-GROUP-METADATA\")\n+                .dest(\"useGroupMetadata\")\n+                .help(\"Whether to use the new transactional commit API with group metadata\");\n+\n         return parser;\n     }\n \n     private static KafkaProducer<String, String> createProducer(Namespace parsedArgs) {\n-        String transactionalId = parsedArgs.getString(\"transactionalId\");\n-        String brokerList = parsedArgs.getString(\"brokerList\");\n-\n         Properties props = new Properties();\n-        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList);\n-        props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId);\n+        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, parsedArgs.getString(\"brokerList\"));\n+        props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, parsedArgs.getString(\"transactionalId\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTQ5ODcxMw=="}, "originalCommit": null, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyMjM1OTI2OnYy", "diffSide": "RIGHT", "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQyMDo1MDoyOVrOFmGuBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQwMjoxMTozNFrOFofqhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTUwMDI5Mg==", "bodyText": "Do we really need to a a logger instead of just using System.out.println()? And if yes, should we update all outputs to use the logger? It's a little unclear why we would mix both approaches?", "url": "https://github.com/apache/kafka/pull/8000#discussion_r375500292", "createdAt": "2020-02-05T20:50:29Z", "author": {"login": "mjsax"}, "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -54,6 +61,9 @@\n  */\n public class TransactionalMessageCopier {\n \n+    private static final Logger log = LoggerFactory.getLogger(TransactionalMessageCopier.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAwNjE1MQ==", "bodyText": "Correct, this gets removed.", "url": "https://github.com/apache/kafka/pull/8000#discussion_r378006151", "createdAt": "2020-02-12T02:11:34Z", "author": {"login": "abbccdda"}, "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -54,6 +61,9 @@\n  */\n public class TransactionalMessageCopier {\n \n+    private static final Logger log = LoggerFactory.getLogger(TransactionalMessageCopier.class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTUwMDI5Mg=="}, "originalCommit": null, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNzkyMTczOnYy", "diffSide": "RIGHT", "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTowNzo1OFrOFoZH0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMTowNzo1OFrOFoZH0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg5ODk2MA==", "bodyText": "Add a reminder in EOS example to people", "url": "https://github.com/apache/kafka/pull/8000#discussion_r377898960", "createdAt": "2020-02-11T21:07:58Z", "author": {"login": "abbccdda"}, "path": "examples/src/main/java/kafka/examples/ExactlyOnceMessageProcessor.java", "diffHunk": "@@ -76,8 +76,11 @@ public ExactlyOnceMessageProcessor(final String mode,\n         this.numInstances = numInstances;\n         this.instanceIdx = instanceIdx;\n         this.transactionalId = \"Processor-\" + instanceIdx;\n+        // If we are using the group mode, it is recommended to have a relatively short txn timeout\n+        // in order to clear pending offsets faster.\n+        final int transactionTimeoutMs = this.mode.equals(\"groupMode\") ? 10000 : -1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17b1ebb275d0bef7744a0215fc2392c4c97285cc"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNzkyOTgyOnYy", "diffSide": "RIGHT", "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMToxMDozN1rOFoZM3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQyMToxMDozN1rOFoZM3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzkwMDI1Mg==", "bodyText": "Need to add synchronized as we are using static date FORMAT which shall triggers spotsbug for concurrent access.", "url": "https://github.com/apache/kafka/pull/8000#discussion_r377900252", "createdAt": "2020-02-11T21:10:37Z", "author": {"login": "abbccdda"}, "path": "tools/src/main/java/org/apache/kafka/tools/TransactionalMessageCopier.java", "diffHunk": "@@ -226,87 +256,124 @@ private static String toJsonString(Map<String, Object> data) {\n         return json;\n     }\n \n-    private static String statusAsJson(long consumed, long remaining, String transactionalId) {\n+    private static synchronized String statusAsJson(long totalProcessed, long consumedSinceLastRebalanced, long remaining, String transactionalId, String stage) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17b1ebb275d0bef7744a0215fc2392c4c97285cc"}, "originalPosition": 111}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4426, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}