{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY1Njc1OTk2", "number": 7997, "reviewThreads": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwMToyNDowMVrODbbRZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMzo1MzoyMVrODdK0wQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMDg0OTY1OnYy", "diffSide": "RIGHT", "path": "checkstyle/suppressions.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwMToyNDowMVrOFi6D3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwMjo0NToxMFrOFi7IpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE0NzE2Nw==", "bodyText": "All those additional exception indicate that we might need to do even more refactoring -- refactoring should reduce code complexity... (did we try to remove some exception to see what we gained? -- maybe it's just shifted?)", "url": "https://github.com/apache/kafka/pull/7997#discussion_r372147167", "createdAt": "2020-01-29T01:24:01Z", "author": {"login": "mjsax"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -224,7 +228,7 @@\n               files=\"SmokeTestDriver.java\"/>\n \n     <suppress checks=\"NPathComplexity\"\n-              files=\"EosTestDriver|KStreamKStreamJoinTest.java|RelationalSmokeTest.java|SmokeTestDriver.java|KStreamKStreamLeftJoinTest.java|KTableKTableForeignKeyJoinIntegrationTest.java\"/>\n+              files=\"EosTestDriver|KStreamKStreamJoinTest.java|RelationalSmokeTest.java|SmokeTestDriver.java|KStreamKStreamLeftJoinTest.java|KTableKTableForeignKeyJoinIntegrationTest.java|StoreChangelogReaderTest.java\"/>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9877d3691132456f21d598222632776c89912b31"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE2NDc3Mg==", "bodyText": "Yes some of those are added temporarily during the development, and could be removed now, will do that.", "url": "https://github.com/apache/kafka/pull/7997#discussion_r372164772", "createdAt": "2020-01-29T02:45:10Z", "author": {"login": "guozhangwang"}, "path": "checkstyle/suppressions.xml", "diffHunk": "@@ -224,7 +228,7 @@\n               files=\"SmokeTestDriver.java\"/>\n \n     <suppress checks=\"NPathComplexity\"\n-              files=\"EosTestDriver|KStreamKStreamJoinTest.java|RelationalSmokeTest.java|SmokeTestDriver.java|KStreamKStreamLeftJoinTest.java|KTableKTableForeignKeyJoinIntegrationTest.java\"/>\n+              files=\"EosTestDriver|KStreamKStreamJoinTest.java|RelationalSmokeTest.java|SmokeTestDriver.java|KStreamKStreamLeftJoinTest.java|KTableKTableForeignKeyJoinIntegrationTest.java|StoreChangelogReaderTest.java\"/>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE0NzE2Nw=="}, "originalCommit": {"oid": "9877d3691132456f21d598222632776c89912b31"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDYwOTUwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMTowMjoxMVrOFkX17g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QyMToxMzowMlrOFlAIDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzY4MzY5NA==", "bodyText": "There's still a TODO here that hasn't been handled.", "url": "https://github.com/apache/kafka/pull/7997#discussion_r373683694", "createdAt": "2020-01-31T21:02:11Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java", "diffHunk": "@@ -331,9 +307,9 @@ private void restoreState(final StateRestoreCallback stateRestoreCallback,\n                     log.warn(\"Restoring GlobalStore {} failed due to: {}. Deleting global store to recreate from scratch.\",\n                         storeName,\n                         recoverableException.toString());\n-                    reinitializeStateStoresForPartitions(recoverableException.partitions(), globalProcessorContext);\n \n-                    stateRestoreListener.onRestoreStart(topicPartition, storeName, offset, highWatermark);\n+                    // TODO K9113: we remove the re-init logic and push it to be handled by the thread directly", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27e758a096cbc3c4960d84907b60ed757ad1f678"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDM0MzY5Mw==", "bodyText": "Yes it is to be handled in a follow-up PR.", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374343693", "createdAt": "2020-02-03T21:13:02Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java", "diffHunk": "@@ -331,9 +307,9 @@ private void restoreState(final StateRestoreCallback stateRestoreCallback,\n                     log.warn(\"Restoring GlobalStore {} failed due to: {}. Deleting global store to recreate from scratch.\",\n                         storeName,\n                         recoverableException.toString());\n-                    reinitializeStateStoresForPartitions(recoverableException.partitions(), globalProcessorContext);\n \n-                    stateRestoreListener.onRestoreStart(topicPartition, storeName, offset, highWatermark);\n+                    // TODO K9113: we remove the re-init logic and push it to be handled by the thread directly", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzY4MzY5NA=="}, "originalCommit": {"oid": "27e758a096cbc3c4960d84907b60ed757ad1f678"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDYxNzk5OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0zMVQyMTowNTo1NVrOFkX7aw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wM1QyMToxNToxOVrOFlAMHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzY4NTA5OQ==", "bodyText": "Do we really want to log all the topics we're subscribing to in regex form here? I.e., what value does this log message provide?", "url": "https://github.com/apache/kafka/pull/7997#discussion_r373685099", "createdAt": "2020-01-31T21:05:55Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java", "diffHunk": "@@ -1225,14 +1226,15 @@ boolean usesPatternSubscription() {\n     }\n \n     synchronized Pattern sourceTopicPattern() {\n-        log.debug(\"Found pattern subscribed source topics, falling back to pattern subscription for the main consumer.\");\n-\n         if (topicPattern == null) {\n             final List<String> allSourceTopics = maybeDecorateInternalSourceTopics(sourceTopicNames);\n             Collections.sort(allSourceTopics);\n             topicPattern = buildPattern(allSourceTopics, nodeToSourcePatterns.values());\n         }\n \n+        log.debug(\"Found pattern subscribed source topics, falling back to pattern \" +\n+            \"subscription for the main consumer: {}\", topicPattern);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "27e758a096cbc3c4960d84907b60ed757ad1f678"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDM0NDczNA==", "bodyText": "I raised this to @ableegoldman in another PR about this already, we will do some further cleanup after this PR (but it was not introduced by this JIRA, I think it was there long time ago: 57b2f68#r37098609", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374344734", "createdAt": "2020-02-03T21:15:19Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java", "diffHunk": "@@ -1225,14 +1226,15 @@ boolean usesPatternSubscription() {\n     }\n \n     synchronized Pattern sourceTopicPattern() {\n-        log.debug(\"Found pattern subscribed source topics, falling back to pattern subscription for the main consumer.\");\n-\n         if (topicPattern == null) {\n             final List<String> allSourceTopics = maybeDecorateInternalSourceTopics(sourceTopicNames);\n             Collections.sort(allSourceTopics);\n             topicPattern = buildPattern(allSourceTopics, nodeToSourcePatterns.values());\n         }\n \n+        log.debug(\"Found pattern subscribed source topics, falling back to pattern \" +\n+            \"subscription for the main consumer: {}\", topicPattern);\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzY4NTA5OQ=="}, "originalCommit": {"oid": "27e758a096cbc3c4960d84907b60ed757ad1f678"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODc3MTQwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTozMjoyNlrOFlkMvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTo1MTo0OVrOFlku5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNDcxOA==", "bodyText": "unresolved TODO here. it's unclear to me why we would only consider this task corrupted in EOS mode. It seems like the lack of a checkpoint file just means that we loaded a cached task in an undefined state, and we should discard it and restore. If we have the file, but some of a changelog is missing from it, then maybe it just never got written, before shutdown, or more likely, the topology has changed and the task is corrupted, whether or not we are in EOS.", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374934718", "createdAt": "2020-02-04T21:32:26Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -32,102 +33,198 @@\n \n import java.io.File;\n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Collection;\n import java.util.HashMap;\n-import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n-import java.util.Optional;\n-import java.util.Set;\n+import java.util.stream.Collectors;\n \n-import static java.util.Collections.emptyMap;\n-import static java.util.Collections.unmodifiableList;\n+import static java.lang.String.format;\n import static org.apache.kafka.streams.processor.internals.StateManagerUtil.CHECKPOINT_FILE_NAME;\n import static org.apache.kafka.streams.processor.internals.StateManagerUtil.converterForStore;\n import static org.apache.kafka.streams.processor.internals.StateRestoreCallbackAdapter.adapt;\n \n-\n+/**\n+ * ProcessorStateManager is the source of truth for the current offset for each state store,\n+ * which is either the read offset during restoring, or the written offset during normal processing.\n+ *\n+ * The offset is initialized as null when the state store is registered, and then it can be updated by\n+ * loading checkpoint file, restore state stores, or passing from the record collector's written offsets.\n+ *\n+ * When checkpointing, if the offset is not null it would be written to the file.\n+ *\n+ * The manager is also responsible for restoring state stores via their registered restore callback,\n+ * which is used for both updating standby tasks as well as restoring active tasks.\n+ */\n public class ProcessorStateManager implements StateManager {\n+\n+    public static class StateStoreMetadata {\n+        private final StateStore stateStore;\n+\n+        // corresponding changelog partition of the store, this and the following two fields\n+        // will only be not-null if the state store is logged (i.e. changelog partition and restorer provided)\n+        private final TopicPartition changelogPartition;\n+\n+        // could be used for both active restoration and standby\n+        private final StateRestoreCallback restoreCallback;\n+\n+        // record converters used for restoration and standby\n+        private final RecordConverter recordConverter;\n+\n+        // indicating the current snapshot of the store as the offset of last changelog record that has been\n+        // applied to the store used for both restoration (active and standby tasks restored offset) and\n+        // normal processing that update stores (written offset); could be null (when initialized)\n+        //\n+        // the offset is updated in three ways:\n+        //   1. when loading from the checkpoint file, when the corresponding task has acquired the state\n+        //      directory lock and have registered all the state store; it is only one-time\n+        //   2. when updating with restore records (by both restoring active and standby),\n+        //      update to the last restore record's offset\n+        //   3. when checkpointing with the given written offsets from record collector,\n+        //      update blindly with the given offset\n+        private Long offset;\n+\n+        private StateStoreMetadata(final StateStore stateStore) {\n+            this.stateStore = stateStore;\n+            this.restoreCallback = null;\n+            this.recordConverter = null;\n+            this.changelogPartition = null;\n+            this.offset = null;\n+        }\n+\n+        private StateStoreMetadata(final StateStore stateStore,\n+                                   final TopicPartition changelogPartition,\n+                                   final StateRestoreCallback restoreCallback,\n+                                   final RecordConverter recordConverter) {\n+            if (restoreCallback == null) {\n+                throw new IllegalStateException(\"Log enabled store should always provide a restore callback upon registration\");\n+            }\n+\n+            this.stateStore = stateStore;\n+            this.changelogPartition = changelogPartition;\n+            this.restoreCallback = restoreCallback;\n+            this.recordConverter = recordConverter;\n+            this.offset = null;\n+        }\n+\n+        private void setOffset(final Long offset) {\n+            this.offset = offset;\n+        }\n+\n+        // the offset is exposed to the changelog reader to determine if restoration is completed\n+        Long offset() {\n+            return this.offset;\n+        }\n+\n+        TopicPartition changelogPartition() {\n+            return this.changelogPartition;\n+        }\n+\n+        StateStore store() {\n+            return this.stateStore;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"StateStoreMetadata (\" + stateStore.name() + \" : \" + changelogPartition + \" @ \" + offset;\n+        }\n+    }\n+\n     private static final String STATE_CHANGELOG_TOPIC_SUFFIX = \"-changelog\";\n \n     private final Logger log;\n     private final TaskId taskId;\n     private final String logPrefix;\n-    private final boolean isStandby;\n-    private final ChangelogReader changelogReader;\n-    private final Map<TopicPartition, Long> offsetLimits;\n-    private final Map<TopicPartition, Long> standbyRestoredOffsets;\n-    private final Map<String, StateRestoreCallback> restoreCallbacks; // used for standby tasks, keyed by state topic name\n-    private final Map<String, RecordConverter> recordConverters; // used for standby tasks, keyed by state topic name\n+    private final TaskType taskType;\n+    private final ChangelogRegister changelogReader;\n     private final Map<String, String> storeToChangelogTopic;\n+    private final Collection<TopicPartition> sourcePartitions;\n \n     // must be maintained in topological order\n-    private final FixedOrderMap<String, Optional<StateStore>> registeredStores = new FixedOrderMap<>();\n-    private final FixedOrderMap<String, Optional<StateStore>> globalStores = new FixedOrderMap<>();\n+    private final FixedOrderMap<String, StateStoreMetadata> stores = new FixedOrderMap<>();\n+    private final FixedOrderMap<String, StateStore> globalStores = new FixedOrderMap<>();\n \n-    private final List<TopicPartition> changelogPartitions = new ArrayList<>();\n-\n-    // TODO: this map does not work with customized grouper where multiple partitions\n-    // of the same topic can be assigned to the same task.\n-    private final Map<String, TopicPartition> partitionForTopic;\n-\n-    private final boolean eosEnabled;\n     private final File baseDir;\n-    private OffsetCheckpoint checkpointFile;\n-    private final Map<TopicPartition, Long> checkpointFileCache = new HashMap<>();\n-    private final Map<TopicPartition, Long> initialLoadedCheckpoints;\n+    private final OffsetCheckpoint checkpointFile;\n+\n+    public static String storeChangelogTopic(final String applicationId,\n+                                             final String storeName) {\n+        return applicationId + \"-\" + storeName + STATE_CHANGELOG_TOPIC_SUFFIX;\n+    }\n \n     /**\n      * @throws ProcessorStateException if the task directory does not exist and could not be created\n-     * @throws IOException             if any severe error happens while creating or locking the state directory\n      */\n     public ProcessorStateManager(final TaskId taskId,\n                                  final Collection<TopicPartition> sources,\n-                                 final boolean isStandby,\n+                                 final TaskType taskType,\n                                  final StateDirectory stateDirectory,\n                                  final Map<String, String> storeToChangelogTopic,\n-                                 final ChangelogReader changelogReader,\n-                                 final boolean eosEnabled,\n-                                 final LogContext logContext) throws IOException {\n-        this.eosEnabled = eosEnabled;\n+                                 final ChangelogRegister changelogReader,\n+                                 final LogContext logContext) throws ProcessorStateException {\n+        this.logPrefix = format(\"task [%s] \", taskId);\n+        this.log = logContext.logger(ProcessorStateManager.class);\n \n-        log = logContext.logger(ProcessorStateManager.class);\n         this.taskId = taskId;\n+        this.taskType = taskType;\n+        this.sourcePartitions = sources;\n         this.changelogReader = changelogReader;\n-        logPrefix = String.format(\"task [%s] \", taskId);\n+        this.storeToChangelogTopic = storeToChangelogTopic;\n \n-        partitionForTopic = new HashMap<>();\n-        for (final TopicPartition source : sources) {\n-            partitionForTopic.put(source.topic(), source);\n-        }\n-        offsetLimits = new HashMap<>();\n-        standbyRestoredOffsets = new ConcurrentHashMap<>();\n-        this.isStandby = isStandby;\n-        restoreCallbacks = isStandby ? new ConcurrentHashMap<>() : null;\n-        recordConverters = isStandby ? new HashMap<>() : null;\n-        this.storeToChangelogTopic = new HashMap<>(storeToChangelogTopic);\n-\n-        baseDir = stateDirectory.directoryForTask(taskId);\n-        checkpointFile = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n-        initialLoadedCheckpoints = checkpointFile.read();\n-\n-        log.trace(\"Checkpointable offsets read from checkpoint: {}\", initialLoadedCheckpoints);\n-\n-        if (eosEnabled) {\n-            // with EOS enabled, there should never be a checkpoint file _during_ processing.\n-            // delete the checkpoint file after loading its stored offsets.\n-            checkpointFile.delete();\n-            checkpointFile = null;\n-        }\n+        this.baseDir = stateDirectory.directoryForTask(taskId);\n+        this.checkpointFile = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n \n         log.debug(\"Created state store manager for task {}\", taskId);\n     }\n \n+    void registerGlobalStateStores(final List<StateStore> stateStores) {\n+        log.debug(\"Register global stores {}\", stateStores);\n+        for (final StateStore stateStore : stateStores) {\n+            globalStores.put(stateStore.name(), stateStore);\n+        }\n+    }\n+\n+    @Override\n+    public StateStore getGlobalStore(final String name) {\n+        return globalStores.get(name);\n+    }\n \n-    public static String storeChangelogTopic(final String applicationId,\n-                                             final String storeName) {\n-        return applicationId + \"-\" + storeName + STATE_CHANGELOG_TOPIC_SUFFIX;\n+    // package-private for test only\n+    void initializeStoreOffsetsFromCheckpoint() {\n+        try {\n+            final Map<TopicPartition, Long> loadedCheckpoints = checkpointFile.read();\n+\n+            log.trace(\"Loaded offsets from the checkpoint file: {}\", loadedCheckpoints);\n+\n+            for (final StateStoreMetadata store : stores.values()) {\n+                if (store.changelogPartition == null) {\n+                    log.info(\"State store {} is not logged and hence would not be restored\", store.stateStore.name());\n+                } else {\n+                    if (loadedCheckpoints.containsKey(store.changelogPartition)) {\n+                        store.setOffset(loadedCheckpoints.remove(store.changelogPartition));\n+\n+                        log.debug(\"State store {} initialized from checkpoint with offset {} at changelog {}\",\n+                            store.stateStore.name(), store.offset, store.changelogPartition);\n+                    } else {\n+                        // TODO K9113: for EOS when there's no checkpointed offset, we should treat it as TaskCorrupted", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3a38ab8fc840ce6e1dc4866d5b6e1f73a52e83a"}, "originalPosition": 253}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk0MzQ2Mg==", "bodyText": "Today we write the checkpoint file before we commit the offsets, so under non-EOS we can just restore from scratch without wiping the local store image since restoring is just overwriting the local store and we can just restore to the end of the changelog (this may result in duplicates but is fine under non-EOS); but with non-EOS we have to first wipe out before restoring from scratch.", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374943462", "createdAt": "2020-02-04T21:51:49Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -32,102 +33,198 @@\n \n import java.io.File;\n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Collection;\n import java.util.HashMap;\n-import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n-import java.util.Optional;\n-import java.util.Set;\n+import java.util.stream.Collectors;\n \n-import static java.util.Collections.emptyMap;\n-import static java.util.Collections.unmodifiableList;\n+import static java.lang.String.format;\n import static org.apache.kafka.streams.processor.internals.StateManagerUtil.CHECKPOINT_FILE_NAME;\n import static org.apache.kafka.streams.processor.internals.StateManagerUtil.converterForStore;\n import static org.apache.kafka.streams.processor.internals.StateRestoreCallbackAdapter.adapt;\n \n-\n+/**\n+ * ProcessorStateManager is the source of truth for the current offset for each state store,\n+ * which is either the read offset during restoring, or the written offset during normal processing.\n+ *\n+ * The offset is initialized as null when the state store is registered, and then it can be updated by\n+ * loading checkpoint file, restore state stores, or passing from the record collector's written offsets.\n+ *\n+ * When checkpointing, if the offset is not null it would be written to the file.\n+ *\n+ * The manager is also responsible for restoring state stores via their registered restore callback,\n+ * which is used for both updating standby tasks as well as restoring active tasks.\n+ */\n public class ProcessorStateManager implements StateManager {\n+\n+    public static class StateStoreMetadata {\n+        private final StateStore stateStore;\n+\n+        // corresponding changelog partition of the store, this and the following two fields\n+        // will only be not-null if the state store is logged (i.e. changelog partition and restorer provided)\n+        private final TopicPartition changelogPartition;\n+\n+        // could be used for both active restoration and standby\n+        private final StateRestoreCallback restoreCallback;\n+\n+        // record converters used for restoration and standby\n+        private final RecordConverter recordConverter;\n+\n+        // indicating the current snapshot of the store as the offset of last changelog record that has been\n+        // applied to the store used for both restoration (active and standby tasks restored offset) and\n+        // normal processing that update stores (written offset); could be null (when initialized)\n+        //\n+        // the offset is updated in three ways:\n+        //   1. when loading from the checkpoint file, when the corresponding task has acquired the state\n+        //      directory lock and have registered all the state store; it is only one-time\n+        //   2. when updating with restore records (by both restoring active and standby),\n+        //      update to the last restore record's offset\n+        //   3. when checkpointing with the given written offsets from record collector,\n+        //      update blindly with the given offset\n+        private Long offset;\n+\n+        private StateStoreMetadata(final StateStore stateStore) {\n+            this.stateStore = stateStore;\n+            this.restoreCallback = null;\n+            this.recordConverter = null;\n+            this.changelogPartition = null;\n+            this.offset = null;\n+        }\n+\n+        private StateStoreMetadata(final StateStore stateStore,\n+                                   final TopicPartition changelogPartition,\n+                                   final StateRestoreCallback restoreCallback,\n+                                   final RecordConverter recordConverter) {\n+            if (restoreCallback == null) {\n+                throw new IllegalStateException(\"Log enabled store should always provide a restore callback upon registration\");\n+            }\n+\n+            this.stateStore = stateStore;\n+            this.changelogPartition = changelogPartition;\n+            this.restoreCallback = restoreCallback;\n+            this.recordConverter = recordConverter;\n+            this.offset = null;\n+        }\n+\n+        private void setOffset(final Long offset) {\n+            this.offset = offset;\n+        }\n+\n+        // the offset is exposed to the changelog reader to determine if restoration is completed\n+        Long offset() {\n+            return this.offset;\n+        }\n+\n+        TopicPartition changelogPartition() {\n+            return this.changelogPartition;\n+        }\n+\n+        StateStore store() {\n+            return this.stateStore;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"StateStoreMetadata (\" + stateStore.name() + \" : \" + changelogPartition + \" @ \" + offset;\n+        }\n+    }\n+\n     private static final String STATE_CHANGELOG_TOPIC_SUFFIX = \"-changelog\";\n \n     private final Logger log;\n     private final TaskId taskId;\n     private final String logPrefix;\n-    private final boolean isStandby;\n-    private final ChangelogReader changelogReader;\n-    private final Map<TopicPartition, Long> offsetLimits;\n-    private final Map<TopicPartition, Long> standbyRestoredOffsets;\n-    private final Map<String, StateRestoreCallback> restoreCallbacks; // used for standby tasks, keyed by state topic name\n-    private final Map<String, RecordConverter> recordConverters; // used for standby tasks, keyed by state topic name\n+    private final TaskType taskType;\n+    private final ChangelogRegister changelogReader;\n     private final Map<String, String> storeToChangelogTopic;\n+    private final Collection<TopicPartition> sourcePartitions;\n \n     // must be maintained in topological order\n-    private final FixedOrderMap<String, Optional<StateStore>> registeredStores = new FixedOrderMap<>();\n-    private final FixedOrderMap<String, Optional<StateStore>> globalStores = new FixedOrderMap<>();\n+    private final FixedOrderMap<String, StateStoreMetadata> stores = new FixedOrderMap<>();\n+    private final FixedOrderMap<String, StateStore> globalStores = new FixedOrderMap<>();\n \n-    private final List<TopicPartition> changelogPartitions = new ArrayList<>();\n-\n-    // TODO: this map does not work with customized grouper where multiple partitions\n-    // of the same topic can be assigned to the same task.\n-    private final Map<String, TopicPartition> partitionForTopic;\n-\n-    private final boolean eosEnabled;\n     private final File baseDir;\n-    private OffsetCheckpoint checkpointFile;\n-    private final Map<TopicPartition, Long> checkpointFileCache = new HashMap<>();\n-    private final Map<TopicPartition, Long> initialLoadedCheckpoints;\n+    private final OffsetCheckpoint checkpointFile;\n+\n+    public static String storeChangelogTopic(final String applicationId,\n+                                             final String storeName) {\n+        return applicationId + \"-\" + storeName + STATE_CHANGELOG_TOPIC_SUFFIX;\n+    }\n \n     /**\n      * @throws ProcessorStateException if the task directory does not exist and could not be created\n-     * @throws IOException             if any severe error happens while creating or locking the state directory\n      */\n     public ProcessorStateManager(final TaskId taskId,\n                                  final Collection<TopicPartition> sources,\n-                                 final boolean isStandby,\n+                                 final TaskType taskType,\n                                  final StateDirectory stateDirectory,\n                                  final Map<String, String> storeToChangelogTopic,\n-                                 final ChangelogReader changelogReader,\n-                                 final boolean eosEnabled,\n-                                 final LogContext logContext) throws IOException {\n-        this.eosEnabled = eosEnabled;\n+                                 final ChangelogRegister changelogReader,\n+                                 final LogContext logContext) throws ProcessorStateException {\n+        this.logPrefix = format(\"task [%s] \", taskId);\n+        this.log = logContext.logger(ProcessorStateManager.class);\n \n-        log = logContext.logger(ProcessorStateManager.class);\n         this.taskId = taskId;\n+        this.taskType = taskType;\n+        this.sourcePartitions = sources;\n         this.changelogReader = changelogReader;\n-        logPrefix = String.format(\"task [%s] \", taskId);\n+        this.storeToChangelogTopic = storeToChangelogTopic;\n \n-        partitionForTopic = new HashMap<>();\n-        for (final TopicPartition source : sources) {\n-            partitionForTopic.put(source.topic(), source);\n-        }\n-        offsetLimits = new HashMap<>();\n-        standbyRestoredOffsets = new ConcurrentHashMap<>();\n-        this.isStandby = isStandby;\n-        restoreCallbacks = isStandby ? new ConcurrentHashMap<>() : null;\n-        recordConverters = isStandby ? new HashMap<>() : null;\n-        this.storeToChangelogTopic = new HashMap<>(storeToChangelogTopic);\n-\n-        baseDir = stateDirectory.directoryForTask(taskId);\n-        checkpointFile = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n-        initialLoadedCheckpoints = checkpointFile.read();\n-\n-        log.trace(\"Checkpointable offsets read from checkpoint: {}\", initialLoadedCheckpoints);\n-\n-        if (eosEnabled) {\n-            // with EOS enabled, there should never be a checkpoint file _during_ processing.\n-            // delete the checkpoint file after loading its stored offsets.\n-            checkpointFile.delete();\n-            checkpointFile = null;\n-        }\n+        this.baseDir = stateDirectory.directoryForTask(taskId);\n+        this.checkpointFile = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n \n         log.debug(\"Created state store manager for task {}\", taskId);\n     }\n \n+    void registerGlobalStateStores(final List<StateStore> stateStores) {\n+        log.debug(\"Register global stores {}\", stateStores);\n+        for (final StateStore stateStore : stateStores) {\n+            globalStores.put(stateStore.name(), stateStore);\n+        }\n+    }\n+\n+    @Override\n+    public StateStore getGlobalStore(final String name) {\n+        return globalStores.get(name);\n+    }\n \n-    public static String storeChangelogTopic(final String applicationId,\n-                                             final String storeName) {\n-        return applicationId + \"-\" + storeName + STATE_CHANGELOG_TOPIC_SUFFIX;\n+    // package-private for test only\n+    void initializeStoreOffsetsFromCheckpoint() {\n+        try {\n+            final Map<TopicPartition, Long> loadedCheckpoints = checkpointFile.read();\n+\n+            log.trace(\"Loaded offsets from the checkpoint file: {}\", loadedCheckpoints);\n+\n+            for (final StateStoreMetadata store : stores.values()) {\n+                if (store.changelogPartition == null) {\n+                    log.info(\"State store {} is not logged and hence would not be restored\", store.stateStore.name());\n+                } else {\n+                    if (loadedCheckpoints.containsKey(store.changelogPartition)) {\n+                        store.setOffset(loadedCheckpoints.remove(store.changelogPartition));\n+\n+                        log.debug(\"State store {} initialized from checkpoint with offset {} at changelog {}\",\n+                            store.stateStore.name(), store.offset, store.changelogPartition);\n+                    } else {\n+                        // TODO K9113: for EOS when there's no checkpointed offset, we should treat it as TaskCorrupted", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNDcxOA=="}, "originalCommit": {"oid": "e3a38ab8fc840ce6e1dc4866d5b6e1f73a52e83a"}, "originalPosition": 253}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODc3MjI2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTozMjo0NFrOFlkNPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTo1Mzo1MFrOFlky6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNDg0Nw==", "bodyText": "seems like this also indicates the task is corrupted", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374934847", "createdAt": "2020-02-04T21:32:44Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -32,102 +33,198 @@\n \n import java.io.File;\n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Collection;\n import java.util.HashMap;\n-import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n-import java.util.Optional;\n-import java.util.Set;\n+import java.util.stream.Collectors;\n \n-import static java.util.Collections.emptyMap;\n-import static java.util.Collections.unmodifiableList;\n+import static java.lang.String.format;\n import static org.apache.kafka.streams.processor.internals.StateManagerUtil.CHECKPOINT_FILE_NAME;\n import static org.apache.kafka.streams.processor.internals.StateManagerUtil.converterForStore;\n import static org.apache.kafka.streams.processor.internals.StateRestoreCallbackAdapter.adapt;\n \n-\n+/**\n+ * ProcessorStateManager is the source of truth for the current offset for each state store,\n+ * which is either the read offset during restoring, or the written offset during normal processing.\n+ *\n+ * The offset is initialized as null when the state store is registered, and then it can be updated by\n+ * loading checkpoint file, restore state stores, or passing from the record collector's written offsets.\n+ *\n+ * When checkpointing, if the offset is not null it would be written to the file.\n+ *\n+ * The manager is also responsible for restoring state stores via their registered restore callback,\n+ * which is used for both updating standby tasks as well as restoring active tasks.\n+ */\n public class ProcessorStateManager implements StateManager {\n+\n+    public static class StateStoreMetadata {\n+        private final StateStore stateStore;\n+\n+        // corresponding changelog partition of the store, this and the following two fields\n+        // will only be not-null if the state store is logged (i.e. changelog partition and restorer provided)\n+        private final TopicPartition changelogPartition;\n+\n+        // could be used for both active restoration and standby\n+        private final StateRestoreCallback restoreCallback;\n+\n+        // record converters used for restoration and standby\n+        private final RecordConverter recordConverter;\n+\n+        // indicating the current snapshot of the store as the offset of last changelog record that has been\n+        // applied to the store used for both restoration (active and standby tasks restored offset) and\n+        // normal processing that update stores (written offset); could be null (when initialized)\n+        //\n+        // the offset is updated in three ways:\n+        //   1. when loading from the checkpoint file, when the corresponding task has acquired the state\n+        //      directory lock and have registered all the state store; it is only one-time\n+        //   2. when updating with restore records (by both restoring active and standby),\n+        //      update to the last restore record's offset\n+        //   3. when checkpointing with the given written offsets from record collector,\n+        //      update blindly with the given offset\n+        private Long offset;\n+\n+        private StateStoreMetadata(final StateStore stateStore) {\n+            this.stateStore = stateStore;\n+            this.restoreCallback = null;\n+            this.recordConverter = null;\n+            this.changelogPartition = null;\n+            this.offset = null;\n+        }\n+\n+        private StateStoreMetadata(final StateStore stateStore,\n+                                   final TopicPartition changelogPartition,\n+                                   final StateRestoreCallback restoreCallback,\n+                                   final RecordConverter recordConverter) {\n+            if (restoreCallback == null) {\n+                throw new IllegalStateException(\"Log enabled store should always provide a restore callback upon registration\");\n+            }\n+\n+            this.stateStore = stateStore;\n+            this.changelogPartition = changelogPartition;\n+            this.restoreCallback = restoreCallback;\n+            this.recordConverter = recordConverter;\n+            this.offset = null;\n+        }\n+\n+        private void setOffset(final Long offset) {\n+            this.offset = offset;\n+        }\n+\n+        // the offset is exposed to the changelog reader to determine if restoration is completed\n+        Long offset() {\n+            return this.offset;\n+        }\n+\n+        TopicPartition changelogPartition() {\n+            return this.changelogPartition;\n+        }\n+\n+        StateStore store() {\n+            return this.stateStore;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"StateStoreMetadata (\" + stateStore.name() + \" : \" + changelogPartition + \" @ \" + offset;\n+        }\n+    }\n+\n     private static final String STATE_CHANGELOG_TOPIC_SUFFIX = \"-changelog\";\n \n     private final Logger log;\n     private final TaskId taskId;\n     private final String logPrefix;\n-    private final boolean isStandby;\n-    private final ChangelogReader changelogReader;\n-    private final Map<TopicPartition, Long> offsetLimits;\n-    private final Map<TopicPartition, Long> standbyRestoredOffsets;\n-    private final Map<String, StateRestoreCallback> restoreCallbacks; // used for standby tasks, keyed by state topic name\n-    private final Map<String, RecordConverter> recordConverters; // used for standby tasks, keyed by state topic name\n+    private final TaskType taskType;\n+    private final ChangelogRegister changelogReader;\n     private final Map<String, String> storeToChangelogTopic;\n+    private final Collection<TopicPartition> sourcePartitions;\n \n     // must be maintained in topological order\n-    private final FixedOrderMap<String, Optional<StateStore>> registeredStores = new FixedOrderMap<>();\n-    private final FixedOrderMap<String, Optional<StateStore>> globalStores = new FixedOrderMap<>();\n+    private final FixedOrderMap<String, StateStoreMetadata> stores = new FixedOrderMap<>();\n+    private final FixedOrderMap<String, StateStore> globalStores = new FixedOrderMap<>();\n \n-    private final List<TopicPartition> changelogPartitions = new ArrayList<>();\n-\n-    // TODO: this map does not work with customized grouper where multiple partitions\n-    // of the same topic can be assigned to the same task.\n-    private final Map<String, TopicPartition> partitionForTopic;\n-\n-    private final boolean eosEnabled;\n     private final File baseDir;\n-    private OffsetCheckpoint checkpointFile;\n-    private final Map<TopicPartition, Long> checkpointFileCache = new HashMap<>();\n-    private final Map<TopicPartition, Long> initialLoadedCheckpoints;\n+    private final OffsetCheckpoint checkpointFile;\n+\n+    public static String storeChangelogTopic(final String applicationId,\n+                                             final String storeName) {\n+        return applicationId + \"-\" + storeName + STATE_CHANGELOG_TOPIC_SUFFIX;\n+    }\n \n     /**\n      * @throws ProcessorStateException if the task directory does not exist and could not be created\n-     * @throws IOException             if any severe error happens while creating or locking the state directory\n      */\n     public ProcessorStateManager(final TaskId taskId,\n                                  final Collection<TopicPartition> sources,\n-                                 final boolean isStandby,\n+                                 final TaskType taskType,\n                                  final StateDirectory stateDirectory,\n                                  final Map<String, String> storeToChangelogTopic,\n-                                 final ChangelogReader changelogReader,\n-                                 final boolean eosEnabled,\n-                                 final LogContext logContext) throws IOException {\n-        this.eosEnabled = eosEnabled;\n+                                 final ChangelogRegister changelogReader,\n+                                 final LogContext logContext) throws ProcessorStateException {\n+        this.logPrefix = format(\"task [%s] \", taskId);\n+        this.log = logContext.logger(ProcessorStateManager.class);\n \n-        log = logContext.logger(ProcessorStateManager.class);\n         this.taskId = taskId;\n+        this.taskType = taskType;\n+        this.sourcePartitions = sources;\n         this.changelogReader = changelogReader;\n-        logPrefix = String.format(\"task [%s] \", taskId);\n+        this.storeToChangelogTopic = storeToChangelogTopic;\n \n-        partitionForTopic = new HashMap<>();\n-        for (final TopicPartition source : sources) {\n-            partitionForTopic.put(source.topic(), source);\n-        }\n-        offsetLimits = new HashMap<>();\n-        standbyRestoredOffsets = new ConcurrentHashMap<>();\n-        this.isStandby = isStandby;\n-        restoreCallbacks = isStandby ? new ConcurrentHashMap<>() : null;\n-        recordConverters = isStandby ? new HashMap<>() : null;\n-        this.storeToChangelogTopic = new HashMap<>(storeToChangelogTopic);\n-\n-        baseDir = stateDirectory.directoryForTask(taskId);\n-        checkpointFile = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n-        initialLoadedCheckpoints = checkpointFile.read();\n-\n-        log.trace(\"Checkpointable offsets read from checkpoint: {}\", initialLoadedCheckpoints);\n-\n-        if (eosEnabled) {\n-            // with EOS enabled, there should never be a checkpoint file _during_ processing.\n-            // delete the checkpoint file after loading its stored offsets.\n-            checkpointFile.delete();\n-            checkpointFile = null;\n-        }\n+        this.baseDir = stateDirectory.directoryForTask(taskId);\n+        this.checkpointFile = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n \n         log.debug(\"Created state store manager for task {}\", taskId);\n     }\n \n+    void registerGlobalStateStores(final List<StateStore> stateStores) {\n+        log.debug(\"Register global stores {}\", stateStores);\n+        for (final StateStore stateStore : stateStores) {\n+            globalStores.put(stateStore.name(), stateStore);\n+        }\n+    }\n+\n+    @Override\n+    public StateStore getGlobalStore(final String name) {\n+        return globalStores.get(name);\n+    }\n \n-    public static String storeChangelogTopic(final String applicationId,\n-                                             final String storeName) {\n-        return applicationId + \"-\" + storeName + STATE_CHANGELOG_TOPIC_SUFFIX;\n+    // package-private for test only\n+    void initializeStoreOffsetsFromCheckpoint() {\n+        try {\n+            final Map<TopicPartition, Long> loadedCheckpoints = checkpointFile.read();\n+\n+            log.trace(\"Loaded offsets from the checkpoint file: {}\", loadedCheckpoints);\n+\n+            for (final StateStoreMetadata store : stores.values()) {\n+                if (store.changelogPartition == null) {\n+                    log.info(\"State store {} is not logged and hence would not be restored\", store.stateStore.name());\n+                } else {\n+                    if (loadedCheckpoints.containsKey(store.changelogPartition)) {\n+                        store.setOffset(loadedCheckpoints.remove(store.changelogPartition));\n+\n+                        log.debug(\"State store {} initialized from checkpoint with offset {} at changelog {}\",\n+                            store.stateStore.name(), store.offset, store.changelogPartition);\n+                    } else {\n+                        // TODO K9113: for EOS when there's no checkpointed offset, we should treat it as TaskCorrupted\n+\n+                        log.info(\"State store {} did not find checkpoint offset, hence would \" +\n+                                \"default to the starting offset at changelog {}\",\n+                            store.stateStore.name(), store.changelogPartition);\n+                    }\n+                }\n+            }\n+\n+            if (!loadedCheckpoints.isEmpty()) {\n+                log.warn(\"Some loaded checkpoint offsets cannot find their corresponding state stores: {}\", loadedCheckpoints);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3a38ab8fc840ce6e1dc4866d5b6e1f73a52e83a"}, "originalPosition": 263}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk0NDQ5MA==", "bodyText": "Arguably yes, I'm just following the old logic intentionally here -- I think originally we want to be more compatible with topology changes (if some topology optimizations decides that some stores are not needed) but on second thought I think this is not safe either. We can consider making this change in another PR to make it stricter.", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374944490", "createdAt": "2020-02-04T21:53:50Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -32,102 +33,198 @@\n \n import java.io.File;\n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Collection;\n import java.util.HashMap;\n-import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n-import java.util.Optional;\n-import java.util.Set;\n+import java.util.stream.Collectors;\n \n-import static java.util.Collections.emptyMap;\n-import static java.util.Collections.unmodifiableList;\n+import static java.lang.String.format;\n import static org.apache.kafka.streams.processor.internals.StateManagerUtil.CHECKPOINT_FILE_NAME;\n import static org.apache.kafka.streams.processor.internals.StateManagerUtil.converterForStore;\n import static org.apache.kafka.streams.processor.internals.StateRestoreCallbackAdapter.adapt;\n \n-\n+/**\n+ * ProcessorStateManager is the source of truth for the current offset for each state store,\n+ * which is either the read offset during restoring, or the written offset during normal processing.\n+ *\n+ * The offset is initialized as null when the state store is registered, and then it can be updated by\n+ * loading checkpoint file, restore state stores, or passing from the record collector's written offsets.\n+ *\n+ * When checkpointing, if the offset is not null it would be written to the file.\n+ *\n+ * The manager is also responsible for restoring state stores via their registered restore callback,\n+ * which is used for both updating standby tasks as well as restoring active tasks.\n+ */\n public class ProcessorStateManager implements StateManager {\n+\n+    public static class StateStoreMetadata {\n+        private final StateStore stateStore;\n+\n+        // corresponding changelog partition of the store, this and the following two fields\n+        // will only be not-null if the state store is logged (i.e. changelog partition and restorer provided)\n+        private final TopicPartition changelogPartition;\n+\n+        // could be used for both active restoration and standby\n+        private final StateRestoreCallback restoreCallback;\n+\n+        // record converters used for restoration and standby\n+        private final RecordConverter recordConverter;\n+\n+        // indicating the current snapshot of the store as the offset of last changelog record that has been\n+        // applied to the store used for both restoration (active and standby tasks restored offset) and\n+        // normal processing that update stores (written offset); could be null (when initialized)\n+        //\n+        // the offset is updated in three ways:\n+        //   1. when loading from the checkpoint file, when the corresponding task has acquired the state\n+        //      directory lock and have registered all the state store; it is only one-time\n+        //   2. when updating with restore records (by both restoring active and standby),\n+        //      update to the last restore record's offset\n+        //   3. when checkpointing with the given written offsets from record collector,\n+        //      update blindly with the given offset\n+        private Long offset;\n+\n+        private StateStoreMetadata(final StateStore stateStore) {\n+            this.stateStore = stateStore;\n+            this.restoreCallback = null;\n+            this.recordConverter = null;\n+            this.changelogPartition = null;\n+            this.offset = null;\n+        }\n+\n+        private StateStoreMetadata(final StateStore stateStore,\n+                                   final TopicPartition changelogPartition,\n+                                   final StateRestoreCallback restoreCallback,\n+                                   final RecordConverter recordConverter) {\n+            if (restoreCallback == null) {\n+                throw new IllegalStateException(\"Log enabled store should always provide a restore callback upon registration\");\n+            }\n+\n+            this.stateStore = stateStore;\n+            this.changelogPartition = changelogPartition;\n+            this.restoreCallback = restoreCallback;\n+            this.recordConverter = recordConverter;\n+            this.offset = null;\n+        }\n+\n+        private void setOffset(final Long offset) {\n+            this.offset = offset;\n+        }\n+\n+        // the offset is exposed to the changelog reader to determine if restoration is completed\n+        Long offset() {\n+            return this.offset;\n+        }\n+\n+        TopicPartition changelogPartition() {\n+            return this.changelogPartition;\n+        }\n+\n+        StateStore store() {\n+            return this.stateStore;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"StateStoreMetadata (\" + stateStore.name() + \" : \" + changelogPartition + \" @ \" + offset;\n+        }\n+    }\n+\n     private static final String STATE_CHANGELOG_TOPIC_SUFFIX = \"-changelog\";\n \n     private final Logger log;\n     private final TaskId taskId;\n     private final String logPrefix;\n-    private final boolean isStandby;\n-    private final ChangelogReader changelogReader;\n-    private final Map<TopicPartition, Long> offsetLimits;\n-    private final Map<TopicPartition, Long> standbyRestoredOffsets;\n-    private final Map<String, StateRestoreCallback> restoreCallbacks; // used for standby tasks, keyed by state topic name\n-    private final Map<String, RecordConverter> recordConverters; // used for standby tasks, keyed by state topic name\n+    private final TaskType taskType;\n+    private final ChangelogRegister changelogReader;\n     private final Map<String, String> storeToChangelogTopic;\n+    private final Collection<TopicPartition> sourcePartitions;\n \n     // must be maintained in topological order\n-    private final FixedOrderMap<String, Optional<StateStore>> registeredStores = new FixedOrderMap<>();\n-    private final FixedOrderMap<String, Optional<StateStore>> globalStores = new FixedOrderMap<>();\n+    private final FixedOrderMap<String, StateStoreMetadata> stores = new FixedOrderMap<>();\n+    private final FixedOrderMap<String, StateStore> globalStores = new FixedOrderMap<>();\n \n-    private final List<TopicPartition> changelogPartitions = new ArrayList<>();\n-\n-    // TODO: this map does not work with customized grouper where multiple partitions\n-    // of the same topic can be assigned to the same task.\n-    private final Map<String, TopicPartition> partitionForTopic;\n-\n-    private final boolean eosEnabled;\n     private final File baseDir;\n-    private OffsetCheckpoint checkpointFile;\n-    private final Map<TopicPartition, Long> checkpointFileCache = new HashMap<>();\n-    private final Map<TopicPartition, Long> initialLoadedCheckpoints;\n+    private final OffsetCheckpoint checkpointFile;\n+\n+    public static String storeChangelogTopic(final String applicationId,\n+                                             final String storeName) {\n+        return applicationId + \"-\" + storeName + STATE_CHANGELOG_TOPIC_SUFFIX;\n+    }\n \n     /**\n      * @throws ProcessorStateException if the task directory does not exist and could not be created\n-     * @throws IOException             if any severe error happens while creating or locking the state directory\n      */\n     public ProcessorStateManager(final TaskId taskId,\n                                  final Collection<TopicPartition> sources,\n-                                 final boolean isStandby,\n+                                 final TaskType taskType,\n                                  final StateDirectory stateDirectory,\n                                  final Map<String, String> storeToChangelogTopic,\n-                                 final ChangelogReader changelogReader,\n-                                 final boolean eosEnabled,\n-                                 final LogContext logContext) throws IOException {\n-        this.eosEnabled = eosEnabled;\n+                                 final ChangelogRegister changelogReader,\n+                                 final LogContext logContext) throws ProcessorStateException {\n+        this.logPrefix = format(\"task [%s] \", taskId);\n+        this.log = logContext.logger(ProcessorStateManager.class);\n \n-        log = logContext.logger(ProcessorStateManager.class);\n         this.taskId = taskId;\n+        this.taskType = taskType;\n+        this.sourcePartitions = sources;\n         this.changelogReader = changelogReader;\n-        logPrefix = String.format(\"task [%s] \", taskId);\n+        this.storeToChangelogTopic = storeToChangelogTopic;\n \n-        partitionForTopic = new HashMap<>();\n-        for (final TopicPartition source : sources) {\n-            partitionForTopic.put(source.topic(), source);\n-        }\n-        offsetLimits = new HashMap<>();\n-        standbyRestoredOffsets = new ConcurrentHashMap<>();\n-        this.isStandby = isStandby;\n-        restoreCallbacks = isStandby ? new ConcurrentHashMap<>() : null;\n-        recordConverters = isStandby ? new HashMap<>() : null;\n-        this.storeToChangelogTopic = new HashMap<>(storeToChangelogTopic);\n-\n-        baseDir = stateDirectory.directoryForTask(taskId);\n-        checkpointFile = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n-        initialLoadedCheckpoints = checkpointFile.read();\n-\n-        log.trace(\"Checkpointable offsets read from checkpoint: {}\", initialLoadedCheckpoints);\n-\n-        if (eosEnabled) {\n-            // with EOS enabled, there should never be a checkpoint file _during_ processing.\n-            // delete the checkpoint file after loading its stored offsets.\n-            checkpointFile.delete();\n-            checkpointFile = null;\n-        }\n+        this.baseDir = stateDirectory.directoryForTask(taskId);\n+        this.checkpointFile = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n \n         log.debug(\"Created state store manager for task {}\", taskId);\n     }\n \n+    void registerGlobalStateStores(final List<StateStore> stateStores) {\n+        log.debug(\"Register global stores {}\", stateStores);\n+        for (final StateStore stateStore : stateStores) {\n+            globalStores.put(stateStore.name(), stateStore);\n+        }\n+    }\n+\n+    @Override\n+    public StateStore getGlobalStore(final String name) {\n+        return globalStores.get(name);\n+    }\n \n-    public static String storeChangelogTopic(final String applicationId,\n-                                             final String storeName) {\n-        return applicationId + \"-\" + storeName + STATE_CHANGELOG_TOPIC_SUFFIX;\n+    // package-private for test only\n+    void initializeStoreOffsetsFromCheckpoint() {\n+        try {\n+            final Map<TopicPartition, Long> loadedCheckpoints = checkpointFile.read();\n+\n+            log.trace(\"Loaded offsets from the checkpoint file: {}\", loadedCheckpoints);\n+\n+            for (final StateStoreMetadata store : stores.values()) {\n+                if (store.changelogPartition == null) {\n+                    log.info(\"State store {} is not logged and hence would not be restored\", store.stateStore.name());\n+                } else {\n+                    if (loadedCheckpoints.containsKey(store.changelogPartition)) {\n+                        store.setOffset(loadedCheckpoints.remove(store.changelogPartition));\n+\n+                        log.debug(\"State store {} initialized from checkpoint with offset {} at changelog {}\",\n+                            store.stateStore.name(), store.offset, store.changelogPartition);\n+                    } else {\n+                        // TODO K9113: for EOS when there's no checkpointed offset, we should treat it as TaskCorrupted\n+\n+                        log.info(\"State store {} did not find checkpoint offset, hence would \" +\n+                                \"default to the starting offset at changelog {}\",\n+                            store.stateStore.name(), store.changelogPartition);\n+                    }\n+                }\n+            }\n+\n+            if (!loadedCheckpoints.isEmpty()) {\n+                log.warn(\"Some loaded checkpoint offsets cannot find their corresponding state stores: {}\", loadedCheckpoints);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNDg0Nw=="}, "originalCommit": {"oid": "e3a38ab8fc840ce6e1dc4866d5b6e1f73a52e83a"}, "originalPosition": 263}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODc3MzM3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTozMzowOVrOFlkN7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTo1NjozNVrOFlk3uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNTAyMQ==", "bodyText": "in retrospect, I like the idea of doing this regardless of EOS. Why should we deliberately produce wrong results in ALO mode? We can certainly optimize to be able to use semi-trustworthy data, but let's treat that separately from EOS vs ALO", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374935021", "createdAt": "2020-02-04T21:33:09Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -32,102 +33,198 @@\n \n import java.io.File;\n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Collection;\n import java.util.HashMap;\n-import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n-import java.util.Optional;\n-import java.util.Set;\n+import java.util.stream.Collectors;\n \n-import static java.util.Collections.emptyMap;\n-import static java.util.Collections.unmodifiableList;\n+import static java.lang.String.format;\n import static org.apache.kafka.streams.processor.internals.StateManagerUtil.CHECKPOINT_FILE_NAME;\n import static org.apache.kafka.streams.processor.internals.StateManagerUtil.converterForStore;\n import static org.apache.kafka.streams.processor.internals.StateRestoreCallbackAdapter.adapt;\n \n-\n+/**\n+ * ProcessorStateManager is the source of truth for the current offset for each state store,\n+ * which is either the read offset during restoring, or the written offset during normal processing.\n+ *\n+ * The offset is initialized as null when the state store is registered, and then it can be updated by\n+ * loading checkpoint file, restore state stores, or passing from the record collector's written offsets.\n+ *\n+ * When checkpointing, if the offset is not null it would be written to the file.\n+ *\n+ * The manager is also responsible for restoring state stores via their registered restore callback,\n+ * which is used for both updating standby tasks as well as restoring active tasks.\n+ */\n public class ProcessorStateManager implements StateManager {\n+\n+    public static class StateStoreMetadata {\n+        private final StateStore stateStore;\n+\n+        // corresponding changelog partition of the store, this and the following two fields\n+        // will only be not-null if the state store is logged (i.e. changelog partition and restorer provided)\n+        private final TopicPartition changelogPartition;\n+\n+        // could be used for both active restoration and standby\n+        private final StateRestoreCallback restoreCallback;\n+\n+        // record converters used for restoration and standby\n+        private final RecordConverter recordConverter;\n+\n+        // indicating the current snapshot of the store as the offset of last changelog record that has been\n+        // applied to the store used for both restoration (active and standby tasks restored offset) and\n+        // normal processing that update stores (written offset); could be null (when initialized)\n+        //\n+        // the offset is updated in three ways:\n+        //   1. when loading from the checkpoint file, when the corresponding task has acquired the state\n+        //      directory lock and have registered all the state store; it is only one-time\n+        //   2. when updating with restore records (by both restoring active and standby),\n+        //      update to the last restore record's offset\n+        //   3. when checkpointing with the given written offsets from record collector,\n+        //      update blindly with the given offset\n+        private Long offset;\n+\n+        private StateStoreMetadata(final StateStore stateStore) {\n+            this.stateStore = stateStore;\n+            this.restoreCallback = null;\n+            this.recordConverter = null;\n+            this.changelogPartition = null;\n+            this.offset = null;\n+        }\n+\n+        private StateStoreMetadata(final StateStore stateStore,\n+                                   final TopicPartition changelogPartition,\n+                                   final StateRestoreCallback restoreCallback,\n+                                   final RecordConverter recordConverter) {\n+            if (restoreCallback == null) {\n+                throw new IllegalStateException(\"Log enabled store should always provide a restore callback upon registration\");\n+            }\n+\n+            this.stateStore = stateStore;\n+            this.changelogPartition = changelogPartition;\n+            this.restoreCallback = restoreCallback;\n+            this.recordConverter = recordConverter;\n+            this.offset = null;\n+        }\n+\n+        private void setOffset(final Long offset) {\n+            this.offset = offset;\n+        }\n+\n+        // the offset is exposed to the changelog reader to determine if restoration is completed\n+        Long offset() {\n+            return this.offset;\n+        }\n+\n+        TopicPartition changelogPartition() {\n+            return this.changelogPartition;\n+        }\n+\n+        StateStore store() {\n+            return this.stateStore;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"StateStoreMetadata (\" + stateStore.name() + \" : \" + changelogPartition + \" @ \" + offset;\n+        }\n+    }\n+\n     private static final String STATE_CHANGELOG_TOPIC_SUFFIX = \"-changelog\";\n \n     private final Logger log;\n     private final TaskId taskId;\n     private final String logPrefix;\n-    private final boolean isStandby;\n-    private final ChangelogReader changelogReader;\n-    private final Map<TopicPartition, Long> offsetLimits;\n-    private final Map<TopicPartition, Long> standbyRestoredOffsets;\n-    private final Map<String, StateRestoreCallback> restoreCallbacks; // used for standby tasks, keyed by state topic name\n-    private final Map<String, RecordConverter> recordConverters; // used for standby tasks, keyed by state topic name\n+    private final TaskType taskType;\n+    private final ChangelogRegister changelogReader;\n     private final Map<String, String> storeToChangelogTopic;\n+    private final Collection<TopicPartition> sourcePartitions;\n \n     // must be maintained in topological order\n-    private final FixedOrderMap<String, Optional<StateStore>> registeredStores = new FixedOrderMap<>();\n-    private final FixedOrderMap<String, Optional<StateStore>> globalStores = new FixedOrderMap<>();\n+    private final FixedOrderMap<String, StateStoreMetadata> stores = new FixedOrderMap<>();\n+    private final FixedOrderMap<String, StateStore> globalStores = new FixedOrderMap<>();\n \n-    private final List<TopicPartition> changelogPartitions = new ArrayList<>();\n-\n-    // TODO: this map does not work with customized grouper where multiple partitions\n-    // of the same topic can be assigned to the same task.\n-    private final Map<String, TopicPartition> partitionForTopic;\n-\n-    private final boolean eosEnabled;\n     private final File baseDir;\n-    private OffsetCheckpoint checkpointFile;\n-    private final Map<TopicPartition, Long> checkpointFileCache = new HashMap<>();\n-    private final Map<TopicPartition, Long> initialLoadedCheckpoints;\n+    private final OffsetCheckpoint checkpointFile;\n+\n+    public static String storeChangelogTopic(final String applicationId,\n+                                             final String storeName) {\n+        return applicationId + \"-\" + storeName + STATE_CHANGELOG_TOPIC_SUFFIX;\n+    }\n \n     /**\n      * @throws ProcessorStateException if the task directory does not exist and could not be created\n-     * @throws IOException             if any severe error happens while creating or locking the state directory\n      */\n     public ProcessorStateManager(final TaskId taskId,\n                                  final Collection<TopicPartition> sources,\n-                                 final boolean isStandby,\n+                                 final TaskType taskType,\n                                  final StateDirectory stateDirectory,\n                                  final Map<String, String> storeToChangelogTopic,\n-                                 final ChangelogReader changelogReader,\n-                                 final boolean eosEnabled,\n-                                 final LogContext logContext) throws IOException {\n-        this.eosEnabled = eosEnabled;\n+                                 final ChangelogRegister changelogReader,\n+                                 final LogContext logContext) throws ProcessorStateException {\n+        this.logPrefix = format(\"task [%s] \", taskId);\n+        this.log = logContext.logger(ProcessorStateManager.class);\n \n-        log = logContext.logger(ProcessorStateManager.class);\n         this.taskId = taskId;\n+        this.taskType = taskType;\n+        this.sourcePartitions = sources;\n         this.changelogReader = changelogReader;\n-        logPrefix = String.format(\"task [%s] \", taskId);\n+        this.storeToChangelogTopic = storeToChangelogTopic;\n \n-        partitionForTopic = new HashMap<>();\n-        for (final TopicPartition source : sources) {\n-            partitionForTopic.put(source.topic(), source);\n-        }\n-        offsetLimits = new HashMap<>();\n-        standbyRestoredOffsets = new ConcurrentHashMap<>();\n-        this.isStandby = isStandby;\n-        restoreCallbacks = isStandby ? new ConcurrentHashMap<>() : null;\n-        recordConverters = isStandby ? new HashMap<>() : null;\n-        this.storeToChangelogTopic = new HashMap<>(storeToChangelogTopic);\n-\n-        baseDir = stateDirectory.directoryForTask(taskId);\n-        checkpointFile = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n-        initialLoadedCheckpoints = checkpointFile.read();\n-\n-        log.trace(\"Checkpointable offsets read from checkpoint: {}\", initialLoadedCheckpoints);\n-\n-        if (eosEnabled) {\n-            // with EOS enabled, there should never be a checkpoint file _during_ processing.\n-            // delete the checkpoint file after loading its stored offsets.\n-            checkpointFile.delete();\n-            checkpointFile = null;\n-        }\n+        this.baseDir = stateDirectory.directoryForTask(taskId);\n+        this.checkpointFile = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n \n         log.debug(\"Created state store manager for task {}\", taskId);\n     }\n \n+    void registerGlobalStateStores(final List<StateStore> stateStores) {\n+        log.debug(\"Register global stores {}\", stateStores);\n+        for (final StateStore stateStore : stateStores) {\n+            globalStores.put(stateStore.name(), stateStore);\n+        }\n+    }\n+\n+    @Override\n+    public StateStore getGlobalStore(final String name) {\n+        return globalStores.get(name);\n+    }\n \n-    public static String storeChangelogTopic(final String applicationId,\n-                                             final String storeName) {\n-        return applicationId + \"-\" + storeName + STATE_CHANGELOG_TOPIC_SUFFIX;\n+    // package-private for test only\n+    void initializeStoreOffsetsFromCheckpoint() {\n+        try {\n+            final Map<TopicPartition, Long> loadedCheckpoints = checkpointFile.read();\n+\n+            log.trace(\"Loaded offsets from the checkpoint file: {}\", loadedCheckpoints);\n+\n+            for (final StateStoreMetadata store : stores.values()) {\n+                if (store.changelogPartition == null) {\n+                    log.info(\"State store {} is not logged and hence would not be restored\", store.stateStore.name());\n+                } else {\n+                    if (loadedCheckpoints.containsKey(store.changelogPartition)) {\n+                        store.setOffset(loadedCheckpoints.remove(store.changelogPartition));\n+\n+                        log.debug(\"State store {} initialized from checkpoint with offset {} at changelog {}\",\n+                            store.stateStore.name(), store.offset, store.changelogPartition);\n+                    } else {\n+                        // TODO K9113: for EOS when there's no checkpointed offset, we should treat it as TaskCorrupted\n+\n+                        log.info(\"State store {} did not find checkpoint offset, hence would \" +\n+                                \"default to the starting offset at changelog {}\",\n+                            store.stateStore.name(), store.changelogPartition);\n+                    }\n+                }\n+            }\n+\n+            if (!loadedCheckpoints.isEmpty()) {\n+                log.warn(\"Some loaded checkpoint offsets cannot find their corresponding state stores: {}\", loadedCheckpoints);\n+            }\n+\n+            checkpointFile.delete();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3a38ab8fc840ce6e1dc4866d5b6e1f73a52e83a"}, "originalPosition": 266}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk0NTcyMw==", "bodyText": "The idea is that for ALO if we failed to write the first checkpoint after restarting we can still fallback to the original checkpoint even though the store may have been updated and hence there would be duplicates. But since the window gap (just one commit interval) is so small I think it does not worth making the code more complicated with EO v.s. ALO.", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374945723", "createdAt": "2020-02-04T21:56:35Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -32,102 +33,198 @@\n \n import java.io.File;\n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Collection;\n import java.util.HashMap;\n-import java.util.HashSet;\n import java.util.List;\n import java.util.Map;\n-import java.util.Optional;\n-import java.util.Set;\n+import java.util.stream.Collectors;\n \n-import static java.util.Collections.emptyMap;\n-import static java.util.Collections.unmodifiableList;\n+import static java.lang.String.format;\n import static org.apache.kafka.streams.processor.internals.StateManagerUtil.CHECKPOINT_FILE_NAME;\n import static org.apache.kafka.streams.processor.internals.StateManagerUtil.converterForStore;\n import static org.apache.kafka.streams.processor.internals.StateRestoreCallbackAdapter.adapt;\n \n-\n+/**\n+ * ProcessorStateManager is the source of truth for the current offset for each state store,\n+ * which is either the read offset during restoring, or the written offset during normal processing.\n+ *\n+ * The offset is initialized as null when the state store is registered, and then it can be updated by\n+ * loading checkpoint file, restore state stores, or passing from the record collector's written offsets.\n+ *\n+ * When checkpointing, if the offset is not null it would be written to the file.\n+ *\n+ * The manager is also responsible for restoring state stores via their registered restore callback,\n+ * which is used for both updating standby tasks as well as restoring active tasks.\n+ */\n public class ProcessorStateManager implements StateManager {\n+\n+    public static class StateStoreMetadata {\n+        private final StateStore stateStore;\n+\n+        // corresponding changelog partition of the store, this and the following two fields\n+        // will only be not-null if the state store is logged (i.e. changelog partition and restorer provided)\n+        private final TopicPartition changelogPartition;\n+\n+        // could be used for both active restoration and standby\n+        private final StateRestoreCallback restoreCallback;\n+\n+        // record converters used for restoration and standby\n+        private final RecordConverter recordConverter;\n+\n+        // indicating the current snapshot of the store as the offset of last changelog record that has been\n+        // applied to the store used for both restoration (active and standby tasks restored offset) and\n+        // normal processing that update stores (written offset); could be null (when initialized)\n+        //\n+        // the offset is updated in three ways:\n+        //   1. when loading from the checkpoint file, when the corresponding task has acquired the state\n+        //      directory lock and have registered all the state store; it is only one-time\n+        //   2. when updating with restore records (by both restoring active and standby),\n+        //      update to the last restore record's offset\n+        //   3. when checkpointing with the given written offsets from record collector,\n+        //      update blindly with the given offset\n+        private Long offset;\n+\n+        private StateStoreMetadata(final StateStore stateStore) {\n+            this.stateStore = stateStore;\n+            this.restoreCallback = null;\n+            this.recordConverter = null;\n+            this.changelogPartition = null;\n+            this.offset = null;\n+        }\n+\n+        private StateStoreMetadata(final StateStore stateStore,\n+                                   final TopicPartition changelogPartition,\n+                                   final StateRestoreCallback restoreCallback,\n+                                   final RecordConverter recordConverter) {\n+            if (restoreCallback == null) {\n+                throw new IllegalStateException(\"Log enabled store should always provide a restore callback upon registration\");\n+            }\n+\n+            this.stateStore = stateStore;\n+            this.changelogPartition = changelogPartition;\n+            this.restoreCallback = restoreCallback;\n+            this.recordConverter = recordConverter;\n+            this.offset = null;\n+        }\n+\n+        private void setOffset(final Long offset) {\n+            this.offset = offset;\n+        }\n+\n+        // the offset is exposed to the changelog reader to determine if restoration is completed\n+        Long offset() {\n+            return this.offset;\n+        }\n+\n+        TopicPartition changelogPartition() {\n+            return this.changelogPartition;\n+        }\n+\n+        StateStore store() {\n+            return this.stateStore;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"StateStoreMetadata (\" + stateStore.name() + \" : \" + changelogPartition + \" @ \" + offset;\n+        }\n+    }\n+\n     private static final String STATE_CHANGELOG_TOPIC_SUFFIX = \"-changelog\";\n \n     private final Logger log;\n     private final TaskId taskId;\n     private final String logPrefix;\n-    private final boolean isStandby;\n-    private final ChangelogReader changelogReader;\n-    private final Map<TopicPartition, Long> offsetLimits;\n-    private final Map<TopicPartition, Long> standbyRestoredOffsets;\n-    private final Map<String, StateRestoreCallback> restoreCallbacks; // used for standby tasks, keyed by state topic name\n-    private final Map<String, RecordConverter> recordConverters; // used for standby tasks, keyed by state topic name\n+    private final TaskType taskType;\n+    private final ChangelogRegister changelogReader;\n     private final Map<String, String> storeToChangelogTopic;\n+    private final Collection<TopicPartition> sourcePartitions;\n \n     // must be maintained in topological order\n-    private final FixedOrderMap<String, Optional<StateStore>> registeredStores = new FixedOrderMap<>();\n-    private final FixedOrderMap<String, Optional<StateStore>> globalStores = new FixedOrderMap<>();\n+    private final FixedOrderMap<String, StateStoreMetadata> stores = new FixedOrderMap<>();\n+    private final FixedOrderMap<String, StateStore> globalStores = new FixedOrderMap<>();\n \n-    private final List<TopicPartition> changelogPartitions = new ArrayList<>();\n-\n-    // TODO: this map does not work with customized grouper where multiple partitions\n-    // of the same topic can be assigned to the same task.\n-    private final Map<String, TopicPartition> partitionForTopic;\n-\n-    private final boolean eosEnabled;\n     private final File baseDir;\n-    private OffsetCheckpoint checkpointFile;\n-    private final Map<TopicPartition, Long> checkpointFileCache = new HashMap<>();\n-    private final Map<TopicPartition, Long> initialLoadedCheckpoints;\n+    private final OffsetCheckpoint checkpointFile;\n+\n+    public static String storeChangelogTopic(final String applicationId,\n+                                             final String storeName) {\n+        return applicationId + \"-\" + storeName + STATE_CHANGELOG_TOPIC_SUFFIX;\n+    }\n \n     /**\n      * @throws ProcessorStateException if the task directory does not exist and could not be created\n-     * @throws IOException             if any severe error happens while creating or locking the state directory\n      */\n     public ProcessorStateManager(final TaskId taskId,\n                                  final Collection<TopicPartition> sources,\n-                                 final boolean isStandby,\n+                                 final TaskType taskType,\n                                  final StateDirectory stateDirectory,\n                                  final Map<String, String> storeToChangelogTopic,\n-                                 final ChangelogReader changelogReader,\n-                                 final boolean eosEnabled,\n-                                 final LogContext logContext) throws IOException {\n-        this.eosEnabled = eosEnabled;\n+                                 final ChangelogRegister changelogReader,\n+                                 final LogContext logContext) throws ProcessorStateException {\n+        this.logPrefix = format(\"task [%s] \", taskId);\n+        this.log = logContext.logger(ProcessorStateManager.class);\n \n-        log = logContext.logger(ProcessorStateManager.class);\n         this.taskId = taskId;\n+        this.taskType = taskType;\n+        this.sourcePartitions = sources;\n         this.changelogReader = changelogReader;\n-        logPrefix = String.format(\"task [%s] \", taskId);\n+        this.storeToChangelogTopic = storeToChangelogTopic;\n \n-        partitionForTopic = new HashMap<>();\n-        for (final TopicPartition source : sources) {\n-            partitionForTopic.put(source.topic(), source);\n-        }\n-        offsetLimits = new HashMap<>();\n-        standbyRestoredOffsets = new ConcurrentHashMap<>();\n-        this.isStandby = isStandby;\n-        restoreCallbacks = isStandby ? new ConcurrentHashMap<>() : null;\n-        recordConverters = isStandby ? new HashMap<>() : null;\n-        this.storeToChangelogTopic = new HashMap<>(storeToChangelogTopic);\n-\n-        baseDir = stateDirectory.directoryForTask(taskId);\n-        checkpointFile = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n-        initialLoadedCheckpoints = checkpointFile.read();\n-\n-        log.trace(\"Checkpointable offsets read from checkpoint: {}\", initialLoadedCheckpoints);\n-\n-        if (eosEnabled) {\n-            // with EOS enabled, there should never be a checkpoint file _during_ processing.\n-            // delete the checkpoint file after loading its stored offsets.\n-            checkpointFile.delete();\n-            checkpointFile = null;\n-        }\n+        this.baseDir = stateDirectory.directoryForTask(taskId);\n+        this.checkpointFile = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n \n         log.debug(\"Created state store manager for task {}\", taskId);\n     }\n \n+    void registerGlobalStateStores(final List<StateStore> stateStores) {\n+        log.debug(\"Register global stores {}\", stateStores);\n+        for (final StateStore stateStore : stateStores) {\n+            globalStores.put(stateStore.name(), stateStore);\n+        }\n+    }\n+\n+    @Override\n+    public StateStore getGlobalStore(final String name) {\n+        return globalStores.get(name);\n+    }\n \n-    public static String storeChangelogTopic(final String applicationId,\n-                                             final String storeName) {\n-        return applicationId + \"-\" + storeName + STATE_CHANGELOG_TOPIC_SUFFIX;\n+    // package-private for test only\n+    void initializeStoreOffsetsFromCheckpoint() {\n+        try {\n+            final Map<TopicPartition, Long> loadedCheckpoints = checkpointFile.read();\n+\n+            log.trace(\"Loaded offsets from the checkpoint file: {}\", loadedCheckpoints);\n+\n+            for (final StateStoreMetadata store : stores.values()) {\n+                if (store.changelogPartition == null) {\n+                    log.info(\"State store {} is not logged and hence would not be restored\", store.stateStore.name());\n+                } else {\n+                    if (loadedCheckpoints.containsKey(store.changelogPartition)) {\n+                        store.setOffset(loadedCheckpoints.remove(store.changelogPartition));\n+\n+                        log.debug(\"State store {} initialized from checkpoint with offset {} at changelog {}\",\n+                            store.stateStore.name(), store.offset, store.changelogPartition);\n+                    } else {\n+                        // TODO K9113: for EOS when there's no checkpointed offset, we should treat it as TaskCorrupted\n+\n+                        log.info(\"State store {} did not find checkpoint offset, hence would \" +\n+                                \"default to the starting offset at changelog {}\",\n+                            store.stateStore.name(), store.changelogPartition);\n+                    }\n+                }\n+            }\n+\n+            if (!loadedCheckpoints.isEmpty()) {\n+                log.warn(\"Some loaded checkpoint offsets cannot find their corresponding state stores: {}\", loadedCheckpoints);\n+            }\n+\n+            checkpointFile.delete();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNTAyMQ=="}, "originalCommit": {"oid": "e3a38ab8fc840ce6e1dc4866d5b6e1f73a52e83a"}, "originalPosition": 266}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODc3ODc0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTozNDo1NVrOFlkRHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTozNDo1NVrOFlkRHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNTgzNw==", "bodyText": "these kinds of comments paradoxically lead to unmaintainability. Either a method is part of the public contract or not. If it is, then this comment would become out of date, if not, then the changelog reader shouldn't be using it. The recommendation is simply to delete the comment (and similar ones). This even applies to \"for testing\" comments. I have found several methods in use in this code base that were commented \"visible for testing\". Even for tests, either move both the class and the test into an isolated package and use package-private, refactor the test, or remove the comment. Also, if the changelog reader really needs four \"holes\" poked into this class, then we should reconsider the relationship between the state manager and the changelog reader.", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374935837", "createdAt": "2020-02-04T21:34:55Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -136,158 +233,149 @@ public File baseDir() {\n     }\n \n     @Override\n-    public void register(final StateStore store,\n-                         final StateRestoreCallback stateRestoreCallback) {\n+    public void registerStore(final StateStore store, final StateRestoreCallback stateRestoreCallback) {\n         final String storeName = store.name();\n-        log.debug(\"Registering state store {} to its state manager\", storeName);\n \n         if (CHECKPOINT_FILE_NAME.equals(storeName)) {\n-            throw new IllegalArgumentException(String.format(\"%sIllegal store name: %s\", logPrefix, storeName));\n+            throw new IllegalArgumentException(format(\"%sIllegal store name: %s, which collides with the pre-defined \" +\n+                \"checkpoint file name\", logPrefix, storeName));\n         }\n \n-        if (registeredStores.containsKey(storeName) && registeredStores.get(storeName).isPresent()) {\n-            throw new IllegalArgumentException(String.format(\"%sStore %s has already been registered.\", logPrefix, storeName));\n+        if (stores.containsKey(storeName)) {\n+            throw new IllegalArgumentException(format(\"%sStore %s has already been registered.\", logPrefix, storeName));\n         }\n \n-        // check that the underlying change log topic exist or not\n         final String topic = storeToChangelogTopic.get(storeName);\n-        if (topic != null) {\n-            final TopicPartition storePartition = new TopicPartition(topic, getPartition(topic));\n-\n-            final RecordConverter recordConverter = converterForStore(store);\n-\n-            if (isStandby) {\n-                log.trace(\"Preparing standby replica of persistent state store {} with changelog topic {}\", storeName, topic);\n \n-                restoreCallbacks.put(topic, stateRestoreCallback);\n-                recordConverters.put(topic, recordConverter);\n-            } else {\n-                final Long restoreCheckpoint = store.persistent() ? initialLoadedCheckpoints.get(storePartition) : null;\n-                if (restoreCheckpoint != null) {\n-                    checkpointFileCache.put(storePartition, restoreCheckpoint);\n-                }\n-                log.trace(\"Restoring state store {} from changelog topic {} at checkpoint {}\", storeName, topic, restoreCheckpoint);\n-\n-                final StateRestorer restorer = new StateRestorer(\n-                    storePartition,\n-                    new CompositeRestoreListener(stateRestoreCallback),\n-                    restoreCheckpoint,\n-                    offsetLimit(storePartition),\n-                    store.persistent(),\n-                    storeName,\n-                    recordConverter\n-                );\n-\n-                changelogReader.register(restorer);\n-            }\n-            changelogPartitions.add(storePartition);\n+        // if the store name does not exist in the changelog map, it means the underlying store\n+        // is not log enabled (including global stores), and hence it does not need to be restored\n+        if (topic != null) {\n+            // NOTE we assume the partition of the topic can always be inferred from the task id;\n+            // if user ever use a default partition grouper (deprecated in KIP-528) this would break and\n+            // it is not a regression (it would always break anyways)\n+            final TopicPartition storePartition = new TopicPartition(topic, taskId.partition);\n+            final StateStoreMetadata storeMetadata = new StateStoreMetadata(\n+                store,\n+                storePartition,\n+                stateRestoreCallback,\n+                converterForStore(store));\n+            stores.put(storeName, storeMetadata);\n+\n+            changelogReader.register(storePartition, this);\n+        } else {\n+            stores.put(storeName, new StateStoreMetadata(store));\n         }\n \n-        registeredStores.put(storeName, Optional.of(store));\n+        log.debug(\"Registered state store {} to its state manager\", storeName);\n     }\n \n     @Override\n-    public void reinitializeStateStoresForPartitions(final Collection<TopicPartition> partitions,\n-                                                     final InternalProcessorContext processorContext) {\n-        StateManagerUtil.reinitializeStateStoresForPartitions(log,\n-                                                              eosEnabled,\n-                                                              baseDir,\n-                                                              registeredStores,\n-                                                              storeToChangelogTopic,\n-                                                              partitions,\n-                                                              processorContext,\n-                                                              checkpointFile,\n-                                                              checkpointFileCache\n-        );\n+    public StateStore getStore(final String name) {\n+        if (stores.containsKey(name)) {\n+            return stores.get(name).stateStore;\n+        } else {\n+            return null;\n+        }\n     }\n \n-    void clearCheckpoints() throws IOException {\n-        if (checkpointFile != null) {\n-            checkpointFile.delete();\n-            checkpointFile = null;\n+    Collection<TopicPartition> changelogPartitions() {\n+        return changelogOffsets().keySet();\n+    }\n \n-            checkpointFileCache.clear();\n+    @Override\n+    public Map<TopicPartition, Long> changelogOffsets() {\n+        // return the current offsets for those logged stores\n+        final Map<TopicPartition, Long> changelogOffsets = new HashMap<>();\n+        for (final StateStoreMetadata storeMetadata : stores.values()) {\n+            if (storeMetadata.changelogPartition != null) {\n+                // for changelog whose offset is unknown, use 0L indicating earliest offset\n+                // otherwise return the current offset + 1 as the next offset to fetch\n+                changelogOffsets.put(\n+                    storeMetadata.changelogPartition,\n+                    storeMetadata.offset == null ? 0L : storeMetadata.offset + 1L);\n+            }\n         }\n+        return changelogOffsets;\n     }\n \n-    @Override\n-    public Map<TopicPartition, Long> checkpointed() {\n-        updateCheckpointFileCache(emptyMap());\n-        final Map<TopicPartition, Long> partitionsAndOffsets = new HashMap<>();\n+    TaskId taskId() {\n+        return taskId;\n+    }\n+\n+    // used by the changelog reader only", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3a38ab8fc840ce6e1dc4866d5b6e1f73a52e83a"}, "originalPosition": 406}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODc4NjAyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTozNzo0MFrOFlkVzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMTo1ODo1MlrOFlk7vQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNzAzOA==", "bodyText": "maybe it doesn't matter, but this method makes the only usage an n^2 algorithm. If we instead inverted the stores collection and used it for lookups in register, it would be o(n)", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374937038", "createdAt": "2020-02-04T21:37:40Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -304,162 +392,79 @@ public void flush() {\n      * @throws ProcessorStateException if any error happens when closing the state stores\n      */\n     @Override\n-    public void close(final boolean clean) throws ProcessorStateException {\n-        ProcessorStateException firstException = null;\n+    public void close() throws ProcessorStateException {\n+        RuntimeException firstException = null;\n         // attempting to close the stores, just in case they\n         // are not closed by a ProcessorNode yet\n-        if (!registeredStores.isEmpty()) {\n-            log.debug(\"Closing its state manager and all the registered state stores\");\n-            for (final Map.Entry<String, Optional<StateStore>> entry : registeredStores.entrySet()) {\n-                if (entry.getValue().isPresent()) {\n-                    final StateStore store = entry.getValue().get();\n-                    log.debug(\"Closing storage engine {}\", store.name());\n-                    try {\n-                        store.close();\n-                        registeredStores.put(store.name(), Optional.empty());\n-                    } catch (final RuntimeException e) {\n-                        if (firstException == null) {\n-                            firstException = new ProcessorStateException(String.format(\"%sFailed to close state store %s\", logPrefix, store.name()), e);\n-                        }\n-                        log.error(\"Failed to close state store {}: \", store.name(), e);\n+        if (!stores.isEmpty()) {\n+            log.debug(\"Closing its state manager and all the registered state stores: {}\", stores);\n+            for (final Map.Entry<String, StateStoreMetadata> entry : stores.entrySet()) {\n+                final StateStore store = entry.getValue().stateStore;\n+                log.trace(\"Closing store {}\", store.name());\n+                try {\n+                    store.close();\n+                } catch (final RuntimeException exception) {\n+                    if (firstException == null) {\n+                        // do NOT wrap the error if it is actually caused by Streams itself\n+                        if (exception instanceof StreamsException)\n+                            firstException = exception;\n+                        else\n+                            firstException = new ProcessorStateException(\n+                                format(\"%sFailed to close state store %s\", logPrefix, store.name()), exception);\n                     }\n-                } else {\n-                    log.info(\"Skipping to close non-initialized store {}\", entry.getKey());\n+                    log.error(\"Failed to close state store {}: \", store.name(), exception);\n                 }\n             }\n         }\n \n-        if (!clean && eosEnabled) {\n-            // delete the checkpoint file if this is an unclean close\n-            try {\n-                clearCheckpoints();\n-            } catch (final IOException e) {\n-                throw new ProcessorStateException(String.format(\"%sError while deleting the checkpoint file\", logPrefix), e);\n-            }\n-        }\n-\n         if (firstException != null) {\n             throw firstException;\n         }\n     }\n \n     @Override\n-    public void checkpoint(final Map<TopicPartition, Long> checkpointableOffsetsFromProcessing) {\n-        ensureStoresRegistered();\n-\n-        // write the checkpoint file before closing\n-        if (checkpointFile == null) {\n-            checkpointFile = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n-        }\n+    public void checkpoint(final Map<TopicPartition, Long> writtenOffsets) {\n+        // first update each state store's current offset, then checkpoint\n+        // those stores that are only logged and persistent to the checkpoint file\n+        for (final Map.Entry<TopicPartition, Long> entry : writtenOffsets.entrySet()) {\n+            final StateStoreMetadata store = findStore(entry.getKey());\n \n-        updateCheckpointFileCache(checkpointableOffsetsFromProcessing);\n+            if (store != null) {\n+                store.setOffset(entry.getValue());\n \n-        log.debug(\"Writing checkpoint: {}\", checkpointFileCache);\n-        try {\n-            checkpointFile.write(checkpointFileCache);\n-        } catch (final IOException e) {\n-            log.warn(\"Failed to write offset checkpoint file to [{}]\", checkpointFile, e);\n-        }\n-    }\n-\n-    private void updateCheckpointFileCache(final Map<TopicPartition, Long> checkpointableOffsetsFromProcessing) {\n-        final Set<TopicPartition> validCheckpointableTopics = validCheckpointableTopics();\n-        final Map<TopicPartition, Long> restoredOffsets = validCheckpointableOffsets(\n-            changelogReader.restoredOffsets(),\n-            validCheckpointableTopics\n-        );\n-        log.trace(\"Checkpointable offsets updated with restored offsets: {}\", checkpointFileCache);\n-        for (final TopicPartition topicPartition : validCheckpointableTopics) {\n-            if (checkpointableOffsetsFromProcessing.containsKey(topicPartition)) {\n-                // if we have just recently processed some offsets,\n-                // store the last offset + 1 (the log position after restoration)\n-                checkpointFileCache.put(topicPartition, checkpointableOffsetsFromProcessing.get(topicPartition) + 1);\n-            } else if (standbyRestoredOffsets.containsKey(topicPartition)) {\n-                // or if we restored some offset as a standby task, use it\n-                checkpointFileCache.put(topicPartition, standbyRestoredOffsets.get(topicPartition));\n-            } else if (restoredOffsets.containsKey(topicPartition)) {\n-                // or if we restored some offset as an active task, use it\n-                checkpointFileCache.put(topicPartition, restoredOffsets.get(topicPartition));\n-            } else if (checkpointFileCache.containsKey(topicPartition)) {\n-                // or if we have a prior value we've cached (and written to the checkpoint file), then keep it\n-            } else {\n-                // As a last resort, fall back to the offset we loaded from the checkpoint file at startup, but\n-                // only if the offset is actually valid for our current state stores.\n-                final Long loadedOffset =\n-                    validCheckpointableOffsets(initialLoadedCheckpoints, validCheckpointableTopics).get(topicPartition);\n-                if (loadedOffset != null) {\n-                    checkpointFileCache.put(topicPartition, loadedOffset);\n-                }\n+                log.debug(\"State store {} updated to written offset {} at changelog {}\",\n+                    store.stateStore.name(), store.offset, store.changelogPartition);\n             }\n         }\n-    }\n-\n-    private int getPartition(final String topic) {\n-        final TopicPartition partition = partitionForTopic.get(topic);\n-        return partition == null ? taskId.partition : partition.partition();\n-    }\n-\n-    void registerGlobalStateStores(final List<StateStore> stateStores) {\n-        log.debug(\"Register global stores {}\", stateStores);\n-        for (final StateStore stateStore : stateStores) {\n-            globalStores.put(stateStore.name(), Optional.of(stateStore));\n-        }\n-    }\n \n-    @Override\n-    public StateStore getGlobalStore(final String name) {\n-        return globalStores.getOrDefault(name, Optional.empty()).orElse(null);\n-    }\n-\n-    Collection<TopicPartition> changelogPartitions() {\n-        return unmodifiableList(changelogPartitions);\n-    }\n-\n-    void ensureStoresRegistered() {\n-        for (final Map.Entry<String, Optional<StateStore>> entry : registeredStores.entrySet()) {\n-            if (!entry.getValue().isPresent()) {\n-                throw new IllegalStateException(\n-                    \"store [\" + entry.getKey() + \"] has not been correctly registered. This is a bug in Kafka Streams.\"\n-                );\n+        final Map<TopicPartition, Long> checkpointingOffsets = new HashMap<>();\n+        for (final StateStoreMetadata storeMetadata : stores.values()) {\n+            // store is logged, persistent, and has a valid current offset\n+            if (storeMetadata.changelogPartition != null &&\n+                storeMetadata.stateStore.persistent() &&\n+                storeMetadata.offset != null) {\n+                checkpointingOffsets.put(storeMetadata.changelogPartition, storeMetadata.offset);\n             }\n         }\n-    }\n-\n-    private Set<TopicPartition> validCheckpointableTopics() {\n-        // it's only valid to record checkpoints for registered stores that are both persistent and change-logged\n-\n-        final Set<TopicPartition> result = new HashSet<>(storeToChangelogTopic.size());\n-        for (final Map.Entry<String, String> storeToChangelog : storeToChangelogTopic.entrySet()) {\n-            final String storeName = storeToChangelog.getKey();\n-            if (registeredStores.containsKey(storeName)\n-                && registeredStores.get(storeName).isPresent()\n-                && registeredStores.get(storeName).get().persistent()) {\n \n-                final String changelogTopic = storeToChangelog.getValue();\n-                result.add(new TopicPartition(changelogTopic, getPartition(changelogTopic)));\n-            }\n+        log.debug(\"Writing checkpoint: {}\", checkpointingOffsets);\n+        try {\n+            checkpointFile.write(checkpointingOffsets);\n+        } catch (final IOException e) {\n+            log.warn(\"Failed to write offset checkpoint file to [{}]\", checkpointFile, e);\n         }\n-        return result;\n     }\n \n-    private static Map<TopicPartition, Long> validCheckpointableOffsets(\n-        final Map<TopicPartition, Long> checkpointableOffsets,\n-        final Set<TopicPartition> validCheckpointableTopics) {\n+    private StateStoreMetadata findStore(final TopicPartition changelogPartition) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3a38ab8fc840ce6e1dc4866d5b6e1f73a52e83a"}, "originalPosition": 722}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk0Njc0OQ==", "bodyText": "I've thought about it: inverting the stores collection makes other calls that depends on the store name more complicated, while keeping two collections indexed by storeName / changelog partition is not much worthy since within a task there are usually no more than 10 stores so this n^2 algorithm should not be a big deal.", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374946749", "createdAt": "2020-02-04T21:58:52Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -304,162 +392,79 @@ public void flush() {\n      * @throws ProcessorStateException if any error happens when closing the state stores\n      */\n     @Override\n-    public void close(final boolean clean) throws ProcessorStateException {\n-        ProcessorStateException firstException = null;\n+    public void close() throws ProcessorStateException {\n+        RuntimeException firstException = null;\n         // attempting to close the stores, just in case they\n         // are not closed by a ProcessorNode yet\n-        if (!registeredStores.isEmpty()) {\n-            log.debug(\"Closing its state manager and all the registered state stores\");\n-            for (final Map.Entry<String, Optional<StateStore>> entry : registeredStores.entrySet()) {\n-                if (entry.getValue().isPresent()) {\n-                    final StateStore store = entry.getValue().get();\n-                    log.debug(\"Closing storage engine {}\", store.name());\n-                    try {\n-                        store.close();\n-                        registeredStores.put(store.name(), Optional.empty());\n-                    } catch (final RuntimeException e) {\n-                        if (firstException == null) {\n-                            firstException = new ProcessorStateException(String.format(\"%sFailed to close state store %s\", logPrefix, store.name()), e);\n-                        }\n-                        log.error(\"Failed to close state store {}: \", store.name(), e);\n+        if (!stores.isEmpty()) {\n+            log.debug(\"Closing its state manager and all the registered state stores: {}\", stores);\n+            for (final Map.Entry<String, StateStoreMetadata> entry : stores.entrySet()) {\n+                final StateStore store = entry.getValue().stateStore;\n+                log.trace(\"Closing store {}\", store.name());\n+                try {\n+                    store.close();\n+                } catch (final RuntimeException exception) {\n+                    if (firstException == null) {\n+                        // do NOT wrap the error if it is actually caused by Streams itself\n+                        if (exception instanceof StreamsException)\n+                            firstException = exception;\n+                        else\n+                            firstException = new ProcessorStateException(\n+                                format(\"%sFailed to close state store %s\", logPrefix, store.name()), exception);\n                     }\n-                } else {\n-                    log.info(\"Skipping to close non-initialized store {}\", entry.getKey());\n+                    log.error(\"Failed to close state store {}: \", store.name(), exception);\n                 }\n             }\n         }\n \n-        if (!clean && eosEnabled) {\n-            // delete the checkpoint file if this is an unclean close\n-            try {\n-                clearCheckpoints();\n-            } catch (final IOException e) {\n-                throw new ProcessorStateException(String.format(\"%sError while deleting the checkpoint file\", logPrefix), e);\n-            }\n-        }\n-\n         if (firstException != null) {\n             throw firstException;\n         }\n     }\n \n     @Override\n-    public void checkpoint(final Map<TopicPartition, Long> checkpointableOffsetsFromProcessing) {\n-        ensureStoresRegistered();\n-\n-        // write the checkpoint file before closing\n-        if (checkpointFile == null) {\n-            checkpointFile = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n-        }\n+    public void checkpoint(final Map<TopicPartition, Long> writtenOffsets) {\n+        // first update each state store's current offset, then checkpoint\n+        // those stores that are only logged and persistent to the checkpoint file\n+        for (final Map.Entry<TopicPartition, Long> entry : writtenOffsets.entrySet()) {\n+            final StateStoreMetadata store = findStore(entry.getKey());\n \n-        updateCheckpointFileCache(checkpointableOffsetsFromProcessing);\n+            if (store != null) {\n+                store.setOffset(entry.getValue());\n \n-        log.debug(\"Writing checkpoint: {}\", checkpointFileCache);\n-        try {\n-            checkpointFile.write(checkpointFileCache);\n-        } catch (final IOException e) {\n-            log.warn(\"Failed to write offset checkpoint file to [{}]\", checkpointFile, e);\n-        }\n-    }\n-\n-    private void updateCheckpointFileCache(final Map<TopicPartition, Long> checkpointableOffsetsFromProcessing) {\n-        final Set<TopicPartition> validCheckpointableTopics = validCheckpointableTopics();\n-        final Map<TopicPartition, Long> restoredOffsets = validCheckpointableOffsets(\n-            changelogReader.restoredOffsets(),\n-            validCheckpointableTopics\n-        );\n-        log.trace(\"Checkpointable offsets updated with restored offsets: {}\", checkpointFileCache);\n-        for (final TopicPartition topicPartition : validCheckpointableTopics) {\n-            if (checkpointableOffsetsFromProcessing.containsKey(topicPartition)) {\n-                // if we have just recently processed some offsets,\n-                // store the last offset + 1 (the log position after restoration)\n-                checkpointFileCache.put(topicPartition, checkpointableOffsetsFromProcessing.get(topicPartition) + 1);\n-            } else if (standbyRestoredOffsets.containsKey(topicPartition)) {\n-                // or if we restored some offset as a standby task, use it\n-                checkpointFileCache.put(topicPartition, standbyRestoredOffsets.get(topicPartition));\n-            } else if (restoredOffsets.containsKey(topicPartition)) {\n-                // or if we restored some offset as an active task, use it\n-                checkpointFileCache.put(topicPartition, restoredOffsets.get(topicPartition));\n-            } else if (checkpointFileCache.containsKey(topicPartition)) {\n-                // or if we have a prior value we've cached (and written to the checkpoint file), then keep it\n-            } else {\n-                // As a last resort, fall back to the offset we loaded from the checkpoint file at startup, but\n-                // only if the offset is actually valid for our current state stores.\n-                final Long loadedOffset =\n-                    validCheckpointableOffsets(initialLoadedCheckpoints, validCheckpointableTopics).get(topicPartition);\n-                if (loadedOffset != null) {\n-                    checkpointFileCache.put(topicPartition, loadedOffset);\n-                }\n+                log.debug(\"State store {} updated to written offset {} at changelog {}\",\n+                    store.stateStore.name(), store.offset, store.changelogPartition);\n             }\n         }\n-    }\n-\n-    private int getPartition(final String topic) {\n-        final TopicPartition partition = partitionForTopic.get(topic);\n-        return partition == null ? taskId.partition : partition.partition();\n-    }\n-\n-    void registerGlobalStateStores(final List<StateStore> stateStores) {\n-        log.debug(\"Register global stores {}\", stateStores);\n-        for (final StateStore stateStore : stateStores) {\n-            globalStores.put(stateStore.name(), Optional.of(stateStore));\n-        }\n-    }\n \n-    @Override\n-    public StateStore getGlobalStore(final String name) {\n-        return globalStores.getOrDefault(name, Optional.empty()).orElse(null);\n-    }\n-\n-    Collection<TopicPartition> changelogPartitions() {\n-        return unmodifiableList(changelogPartitions);\n-    }\n-\n-    void ensureStoresRegistered() {\n-        for (final Map.Entry<String, Optional<StateStore>> entry : registeredStores.entrySet()) {\n-            if (!entry.getValue().isPresent()) {\n-                throw new IllegalStateException(\n-                    \"store [\" + entry.getKey() + \"] has not been correctly registered. This is a bug in Kafka Streams.\"\n-                );\n+        final Map<TopicPartition, Long> checkpointingOffsets = new HashMap<>();\n+        for (final StateStoreMetadata storeMetadata : stores.values()) {\n+            // store is logged, persistent, and has a valid current offset\n+            if (storeMetadata.changelogPartition != null &&\n+                storeMetadata.stateStore.persistent() &&\n+                storeMetadata.offset != null) {\n+                checkpointingOffsets.put(storeMetadata.changelogPartition, storeMetadata.offset);\n             }\n         }\n-    }\n-\n-    private Set<TopicPartition> validCheckpointableTopics() {\n-        // it's only valid to record checkpoints for registered stores that are both persistent and change-logged\n-\n-        final Set<TopicPartition> result = new HashSet<>(storeToChangelogTopic.size());\n-        for (final Map.Entry<String, String> storeToChangelog : storeToChangelogTopic.entrySet()) {\n-            final String storeName = storeToChangelog.getKey();\n-            if (registeredStores.containsKey(storeName)\n-                && registeredStores.get(storeName).isPresent()\n-                && registeredStores.get(storeName).get().persistent()) {\n \n-                final String changelogTopic = storeToChangelog.getValue();\n-                result.add(new TopicPartition(changelogTopic, getPartition(changelogTopic)));\n-            }\n+        log.debug(\"Writing checkpoint: {}\", checkpointingOffsets);\n+        try {\n+            checkpointFile.write(checkpointingOffsets);\n+        } catch (final IOException e) {\n+            log.warn(\"Failed to write offset checkpoint file to [{}]\", checkpointFile, e);\n         }\n-        return result;\n     }\n \n-    private static Map<TopicPartition, Long> validCheckpointableOffsets(\n-        final Map<TopicPartition, Long> checkpointableOffsets,\n-        final Set<TopicPartition> validCheckpointableTopics) {\n+    private StateStoreMetadata findStore(final TopicPartition changelogPartition) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDkzNzAzOA=="}, "originalCommit": {"oid": "e3a38ab8fc840ce6e1dc4866d5b6e1f73a52e83a"}, "originalPosition": 722}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODkwMjA1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjoxODo0NlrOFllc6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjoxODo0NlrOFllc6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk1NTI0MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                // TODO: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks\n          \n          \n            \n                // TODO K9113: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks\n          \n      \n    \n    \n  \n\nJust marking this for later follow-up.", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374955241", "createdAt": "2020-02-04T22:18:46Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -599,47 +534,36 @@ public String toString() {\n     }\n \n     public String toString(final String indent) {\n-        final StringBuilder builder = new StringBuilder();\n-        builder.append(\"TaskManager\\n\");\n-        builder.append(indent).append(\"\\tMetadataState:\\n\");\n-        builder.append(streamsMetadataState.toString(indent + \"\\t\\t\"));\n-        builder.append(indent).append(\"\\tActive tasks:\\n\");\n-        builder.append(active.toString(indent + \"\\t\\t\"));\n-        builder.append(indent).append(\"\\tStandby tasks:\\n\");\n-        builder.append(standby.toString(indent + \"\\t\\t\"));\n-        return builder.toString();\n-    }\n-\n-    // this should be safe to call whether the restore consumer is assigned standby or active restoring partitions\n-    // as the removal will be a no-op\n-    private void removeChangelogsFromRestoreConsumer(final Collection<TopicPartition> changelogs, final boolean areStandbyPartitions) {\n-        if (!changelogs.isEmpty() && areStandbyPartitions == restoreConsumerAssignedStandbys) {\n-            final Set<TopicPartition> updatedAssignment = new HashSet<>(restoreConsumer.assignment());\n-            updatedAssignment.removeAll(changelogs);\n-            restoreConsumer.assign(updatedAssignment);\n+        final StringBuilder stringBuilder = new StringBuilder();\n+        stringBuilder.append(\"TaskManager\\n\");\n+        stringBuilder.append(indent).append(\"\\tMetadataState:\\n\");\n+        stringBuilder.append(indent).append(\"\\tTasks:\\n\");\n+        for (final Task task : tasks.values()) {\n+            stringBuilder.append(indent)\n+                         .append(\"\\t\\t\")\n+                         .append(task.id())\n+                         .append(\" \")\n+                         .append(task.state())\n+                         .append(\" \")\n+                         .append(task.getClass().getSimpleName())\n+                         .append('(').append(task.isActive() ? \"active\" : \"standby\").append(')');\n         }\n+        return stringBuilder.toString();\n     }\n \n-    private Set<TaskId> partitionsToTaskSet(final Collection<TopicPartition> partitions) {\n-        final Set<TaskId> taskIds = new HashSet<>();\n-        for (final TopicPartition tp : partitions) {\n-            final TaskId id = partitionsToTaskId.get(tp);\n-            if (id != null) {\n-                taskIds.add(id);\n-            } else {\n-                log.error(\"Failed to lookup taskId for partition {}\", tp);\n-                throw new StreamsException(\"Found partition in assignment with no corresponding task\");\n+    // below are for testing only\n+    StandbyTask standbyTask(final TopicPartition partition) {\n+        for (final Task task : (Iterable<Task>) standbyTaskStream()::iterator) {\n+            if (task.inputPartitions().contains(partition)) {\n+                return (StandbyTask) task;\n             }\n         }\n-        return taskIds;\n-    }\n-\n-    // the following functions are for testing only\n-    Map<TaskId, Set<TopicPartition>> assignedActiveTasks() {\n-        return assignedActiveTasks;\n+        return null;\n     }\n \n-    Map<TaskId, Set<TopicPartition>> assignedStandbyTasks() {\n-        return assignedStandbyTasks;\n+    // TODO: this is used from StreamThread only for a hack to collect metrics from the record collectors inside of StreamTasks", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3a38ab8fc840ce6e1dc4866d5b6e1f73a52e83a"}, "originalPosition": 958}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODk2ODE0OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjo0Mjo1NlrOFlmFyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMzo1MTowMlrOFlnjqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2NTcwNw==", "bodyText": "This is unused", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374965707", "createdAt": "2020-02-04T22:42:56Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java", "diffHunk": "@@ -145,27 +99,24 @@ private StreamsConfig createConfig(final File baseDir) throws IOException {\n         )));\n     }\n \n-    private final MockConsumer<byte[], byte[]> consumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);\n     private final MockRestoreConsumer<Integer, Integer> restoreStateConsumer = new MockRestoreConsumer<>(\n         new IntegerSerializer(),\n         new IntegerSerializer()\n     );\n-    private final StoreChangelogReader changelogReader = new StoreChangelogReader(\n-        restoreStateConsumer,\n-        Duration.ZERO,\n-        stateRestoreListener,\n-        new LogContext(\"standby-task-test \")\n-    );\n \n-    private final byte[] recordValue = intSerializer.serialize(null, 10);\n-    private final byte[] recordKey = intSerializer.serialize(null, 1);\n+    @Mock(type = MockType.NICE)\n+    private ChangelogReader changelogReader;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e20571ec794c4dbe471a8cde765f3fc8999910c"}, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk4OTczOQ==", "bodyText": "Ack", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374989739", "createdAt": "2020-02-04T23:51:02Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java", "diffHunk": "@@ -145,27 +99,24 @@ private StreamsConfig createConfig(final File baseDir) throws IOException {\n         )));\n     }\n \n-    private final MockConsumer<byte[], byte[]> consumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);\n     private final MockRestoreConsumer<Integer, Integer> restoreStateConsumer = new MockRestoreConsumer<>(\n         new IntegerSerializer(),\n         new IntegerSerializer()\n     );\n-    private final StoreChangelogReader changelogReader = new StoreChangelogReader(\n-        restoreStateConsumer,\n-        Duration.ZERO,\n-        stateRestoreListener,\n-        new LogContext(\"standby-task-test \")\n-    );\n \n-    private final byte[] recordValue = intSerializer.serialize(null, 10);\n-    private final byte[] recordKey = intSerializer.serialize(null, 1);\n+    @Mock(type = MockType.NICE)\n+    private ChangelogReader changelogReader;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2NTcwNw=="}, "originalCommit": {"oid": "3e20571ec794c4dbe471a8cde765f3fc8999910c"}, "originalPosition": 165}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODk2ODQ5OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjo0MzowNlrOFlmGAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjo0MzowNlrOFlmGAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2NTc2Mw==", "bodyText": "This is also unused", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374965763", "createdAt": "2020-02-04T22:43:06Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StandbyTaskTest.java", "diffHunk": "@@ -145,27 +99,24 @@ private StreamsConfig createConfig(final File baseDir) throws IOException {\n         )));\n     }\n \n-    private final MockConsumer<byte[], byte[]> consumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);\n     private final MockRestoreConsumer<Integer, Integer> restoreStateConsumer = new MockRestoreConsumer<>(\n         new IntegerSerializer(),\n         new IntegerSerializer()\n     );\n-    private final StoreChangelogReader changelogReader = new StoreChangelogReader(\n-        restoreStateConsumer,\n-        Duration.ZERO,\n-        stateRestoreListener,\n-        new LogContext(\"standby-task-test \")\n-    );\n \n-    private final byte[] recordValue = intSerializer.serialize(null, 10);\n-    private final byte[] recordKey = intSerializer.serialize(null, 1);\n+    @Mock(type = MockType.NICE)\n+    private ChangelogReader changelogReader;\n \n-    private final String threadName = \"threadName\";\n-    private final StreamsMetricsImpl streamsMetrics =\n-        new StreamsMetricsImpl(new Metrics(), threadName, StreamsConfig.METRICS_LATEST);\n+    @Mock(type = MockType.NICE)\n+    private ProcessorStateManager stateManager;\n+\n+    @Mock(type = MockType.NICE)\n+    private RecordCollector recordCollector;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e20571ec794c4dbe471a8cde765f3fc8999910c"}, "originalPosition": 174}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxODk4NDU3OnYy", "diffSide": "RIGHT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMjo0OToxOFrOFlmPzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMzoxMTowOFrOFlmwNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2ODI2OA==", "bodyText": "Did you want to fix this as part of this PR or as a follow-on?", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374968268", "createdAt": "2020-02-04T22:49:18Z", "author": {"login": "vvcephei"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java", "diffHunk": "@@ -1439,17 +1353,25 @@ public void shouldAlwaysReturnEmptyTasksMetadataWhileRebalancingStateAndTasksNot\n         activeTasks.put(task1, Collections.singleton(t1p1));\n         standbyTasks.put(task2, Collections.singleton(t1p2));\n \n-        thread.taskManager().setAssignmentMetadata(activeTasks, standbyTasks);\n+        thread.taskManager().handleAssignment(activeTasks, standbyTasks);\n \n         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);\n \n         assertThreadMetadataHasEmptyTasksWithState(thread.threadMetadata(), StreamThread.State.PARTITIONS_ASSIGNED);\n     }\n \n+    private void assertThreadMetadataHasEmptyTasksWithState(final ThreadMetadata metadata, final StreamThread.State state) {\n+        assertEquals(state.name(), metadata.threadState());\n+        assertTrue(metadata.activeTasks().isEmpty());\n+        assertTrue(metadata.standbyTasks().isEmpty());\n+    }\n+\n+    @Ignore\n     @Test\n+    // FIXME: should unblock this test after we added invalid offset handling", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e20571ec794c4dbe471a8cde765f3fc8999910c"}, "originalPosition": 1046}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk3NjU2NQ==", "bodyText": "As a follow-up.", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374976565", "createdAt": "2020-02-04T23:11:08Z", "author": {"login": "guozhangwang"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java", "diffHunk": "@@ -1439,17 +1353,25 @@ public void shouldAlwaysReturnEmptyTasksMetadataWhileRebalancingStateAndTasksNot\n         activeTasks.put(task1, Collections.singleton(t1p1));\n         standbyTasks.put(task2, Collections.singleton(t1p2));\n \n-        thread.taskManager().setAssignmentMetadata(activeTasks, standbyTasks);\n+        thread.taskManager().handleAssignment(activeTasks, standbyTasks);\n \n         thread.rebalanceListener.onPartitionsAssigned(assignedPartitions);\n \n         assertThreadMetadataHasEmptyTasksWithState(thread.threadMetadata(), StreamThread.State.PARTITIONS_ASSIGNED);\n     }\n \n+    private void assertThreadMetadataHasEmptyTasksWithState(final ThreadMetadata metadata, final StreamThread.State state) {\n+        assertEquals(state.name(), metadata.threadState());\n+        assertTrue(metadata.activeTasks().isEmpty());\n+        assertTrue(metadata.standbyTasks().isEmpty());\n+    }\n+\n+    @Ignore\n     @Test\n+    // FIXME: should unblock this test after we added invalid offset handling", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk2ODI2OA=="}, "originalCommit": {"oid": "3e20571ec794c4dbe471a8cde765f3fc8999910c"}, "originalPosition": 1046}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxOTEyNjQxOnYy", "diffSide": "RIGHT", "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNFQyMzo1MzoyMVrOFlnmaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQwMDoyMzowNVrOFloG0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk5MDQ0MA==", "bodyText": "@ableegoldman @mjsax This is another bug I found while trouble-shooting the system test failures (dates before the cleanup): when we got a task-migrated exception, and then enforce a rebalance, we call unsubscribe which would trigger onLeavePrepare, here if it was from task-migrated then it is likely that we are already undergoing a rebalance and in that case we should lose the tasks instead of revoke them since otherwise, we would still try to commit which would fail with a RebalanceInProgress exception.", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374990440", "createdAt": "2020-02-04T23:53:21Z", "author": {"login": "guozhangwang"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java", "diffHunk": "@@ -709,10 +709,10 @@ public void onLeavePrepare() {\n \n         if (subscriptions.hasAutoAssignedPartitions() && !droppedPartitions.isEmpty()) {\n             final Exception e;\n-            if (generation() != Generation.NO_GENERATION) {\n-                e = invokePartitionsRevoked(droppedPartitions);\n-            } else {\n+            if (generation() == Generation.NO_GENERATION || rebalanceInProgress()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3e20571ec794c4dbe471a8cde765f3fc8999910c"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk5NTQ5MQ==", "bodyText": "Good catch. Also it's pretty unfortunate that we can only trigger a rebalance from outside the client by unsubscribing and closing/suspending the entire assignment...this limits the usefulness of KIP-429 during version probing upgrades.\nIt also has some implications for the \"rebalances are cheap\" assumption of KIP-441. Would be better phrased as \"rebalances are cheap, except for the member who triggers them\".", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374995491", "createdAt": "2020-02-05T00:11:15Z", "author": {"login": "ableegoldman"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java", "diffHunk": "@@ -709,10 +709,10 @@ public void onLeavePrepare() {\n \n         if (subscriptions.hasAutoAssignedPartitions() && !droppedPartitions.isEmpty()) {\n             final Exception e;\n-            if (generation() != Generation.NO_GENERATION) {\n-                e = invokePartitionsRevoked(droppedPartitions);\n-            } else {\n+            if (generation() == Generation.NO_GENERATION || rebalanceInProgress()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk5MDQ0MA=="}, "originalCommit": {"oid": "3e20571ec794c4dbe471a8cde765f3fc8999910c"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk5NjI3NA==", "bodyText": "Yeah ... maybe we should consider adding a new API to consumer to rejoin the group, in a cheaper way.", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374996274", "createdAt": "2020-02-05T00:14:00Z", "author": {"login": "guozhangwang"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java", "diffHunk": "@@ -709,10 +709,10 @@ public void onLeavePrepare() {\n \n         if (subscriptions.hasAutoAssignedPartitions() && !droppedPartitions.isEmpty()) {\n             final Exception e;\n-            if (generation() != Generation.NO_GENERATION) {\n-                e = invokePartitionsRevoked(droppedPartitions);\n-            } else {\n+            if (generation() == Generation.NO_GENERATION || rebalanceInProgress()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk5MDQ0MA=="}, "originalCommit": {"oid": "3e20571ec794c4dbe471a8cde765f3fc8999910c"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk5ODczNw==", "bodyText": "I agree :) -- would be happy to write up a small KIP for it and kick off discussion", "url": "https://github.com/apache/kafka/pull/7997#discussion_r374998737", "createdAt": "2020-02-05T00:23:05Z", "author": {"login": "ableegoldman"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java", "diffHunk": "@@ -709,10 +709,10 @@ public void onLeavePrepare() {\n \n         if (subscriptions.hasAutoAssignedPartitions() && !droppedPartitions.isEmpty()) {\n             final Exception e;\n-            if (generation() != Generation.NO_GENERATION) {\n-                e = invokePartitionsRevoked(droppedPartitions);\n-            } else {\n+            if (generation() == Generation.NO_GENERATION || rebalanceInProgress()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDk5MDQ0MA=="}, "originalCommit": {"oid": "3e20571ec794c4dbe471a8cde765f3fc8999910c"}, "originalPosition": 50}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4419, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}