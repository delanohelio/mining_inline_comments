{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEwOTI5Mzgz", "number": 8586, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMDoyNzoxMlrOD32SvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwNDo0NzowNVrOD4SWNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU5ODg3ODA0OnYy", "diffSide": "LEFT", "path": "core/src/test/scala/unit/kafka/server/SimpleFetchTest.scala", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQwMDoyNzoxMlrOGOVr5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMDoxNTo0N1rOGPUjtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4ODU0OQ==", "bodyText": "I decided to get rid of this. I believe it is already covered by test cases in ReplicaManagerTest. See for example, testFetchBeyondHighWatermarkReturnEmptyResponse.", "url": "https://github.com/apache/kafka/pull/8586#discussion_r417688549", "createdAt": "2020-04-30T00:27:12Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/server/SimpleFetchTest.scala", "diffHunk": "@@ -1,206 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package kafka.server\n-\n-import java.io.File\n-\n-import kafka.api._\n-import kafka.utils._\n-import kafka.log.Log\n-import kafka.log.LogManager\n-import kafka.server.QuotaFactory.UnboundedQuota\n-import kafka.zk.KafkaZkClient\n-import org.apache.kafka.common.metrics.Metrics\n-import org.apache.kafka.common.requests.FetchRequest.PartitionData\n-import org.junit.{After, Before, Test}\n-import java.util.{Optional, Properties}\n-import java.util.concurrent.atomic.AtomicBoolean\n-\n-import org.apache.kafka.common.TopicPartition\n-import org.apache.kafka.common.record.{CompressionType, MemoryRecords, SimpleRecord}\n-import org.easymock.EasyMock\n-import org.junit.Assert._\n-\n-class SimpleFetchTest {\n-\n-  val replicaLagTimeMaxMs = 100L\n-  val replicaFetchWaitMaxMs = 100\n-  val replicaLagMaxMessages = 10L\n-\n-  val overridingProps = new Properties()\n-  overridingProps.put(KafkaConfig.ReplicaLagTimeMaxMsProp, replicaLagTimeMaxMs.toString)\n-  overridingProps.put(KafkaConfig.ReplicaFetchWaitMaxMsProp, replicaFetchWaitMaxMs.toString)\n-\n-  val configs = TestUtils.createBrokerConfigs(2, TestUtils.MockZkConnect).map(KafkaConfig.fromProps(_, overridingProps))\n-\n-  // set the replica manager with the partition\n-  val time = new MockTime\n-  val metrics = new Metrics\n-  val leaderLEO = 20L\n-  val followerLEO = 15L\n-  val partitionHW = 5\n-\n-  val fetchSize = 100\n-  val recordToHW = new SimpleRecord(\"recordToHW\".getBytes())\n-  val recordToLEO = new SimpleRecord(\"recordToLEO\".getBytes())\n-\n-  val topic = \"test-topic\"\n-  val partitionId = 0\n-  val topicPartition = new TopicPartition(topic, partitionId)\n-\n-  val fetchInfo = Seq(topicPartition -> new PartitionData(0, 0, fetchSize,\n-    Optional.empty()))\n-\n-  var replicaManager: ReplicaManager = _\n-\n-  @Before\n-  def setUp(): Unit = {\n-    // create nice mock since we don't particularly care about zkclient calls\n-    val kafkaZkClient: KafkaZkClient = EasyMock.createNiceMock(classOf[KafkaZkClient])\n-    EasyMock.replay(kafkaZkClient)\n-\n-    // create nice mock since we don't particularly care about scheduler calls\n-    val scheduler: KafkaScheduler = EasyMock.createNiceMock(classOf[KafkaScheduler])\n-    EasyMock.replay(scheduler)\n-\n-    // create the log which takes read with either HW max offset or none max offset\n-    val log: Log = EasyMock.createNiceMock(classOf[Log])\n-    EasyMock.expect(log.logStartOffset).andReturn(0).anyTimes()\n-    EasyMock.expect(log.logEndOffset).andReturn(leaderLEO).anyTimes()\n-    EasyMock.expect(log.dir).andReturn(TestUtils.tempDir()).anyTimes()\n-    EasyMock.expect(log.logEndOffsetMetadata).andReturn(LogOffsetMetadata(leaderLEO)).anyTimes()\n-    EasyMock.expect(log.maybeIncrementHighWatermark(EasyMock.anyObject[LogOffsetMetadata]))\n-      .andReturn(Some(LogOffsetMetadata(partitionHW))).anyTimes()\n-    EasyMock.expect(log.highWatermark).andReturn(partitionHW).anyTimes()\n-    EasyMock.expect(log.lastStableOffset).andReturn(partitionHW).anyTimes()\n-    EasyMock.expect(log.read(\n-      startOffset = 0,\n-      maxLength = fetchSize,\n-      isolation = FetchHighWatermark,\n-      minOneMessage = true))\n-      .andReturn(FetchDataInfo(\n-        LogOffsetMetadata(0L, 0L, 0),\n-        MemoryRecords.withRecords(CompressionType.NONE, recordToHW)\n-      )).anyTimes()\n-    EasyMock.expect(log.read(\n-      startOffset = 0,\n-      maxLength = fetchSize,\n-      isolation = FetchLogEnd,\n-      minOneMessage = true))\n-      .andReturn(FetchDataInfo(\n-        LogOffsetMetadata(0L, 0L, 0),\n-        MemoryRecords.withRecords(CompressionType.NONE, recordToLEO)\n-      )).anyTimes()\n-    EasyMock.replay(log)\n-\n-    // create the log manager that is aware of this mock log\n-    val logManager: LogManager = EasyMock.createMock(classOf[LogManager])\n-    EasyMock.expect(logManager.getLog(topicPartition, false)).andReturn(Some(log)).anyTimes()\n-    EasyMock.expect(logManager.liveLogDirs).andReturn(Array.empty[File]).anyTimes()\n-    EasyMock.replay(logManager)\n-\n-    // create the replica manager\n-    replicaManager = new ReplicaManager(configs.head, metrics, time, kafkaZkClient, scheduler, logManager,\n-      new AtomicBoolean(false), QuotaFactory.instantiate(configs.head, metrics, time, \"\"), new BrokerTopicStats,\n-      new MetadataCache(configs.head.brokerId), new LogDirFailureChannel(configs.head.logDirs.size))\n-\n-    // add the partition with two replicas, both in ISR\n-    val partition = replicaManager.createPartition(new TopicPartition(topic, partitionId))\n-\n-    // create the leader replica with the local log\n-    log.updateHighWatermark(partitionHW)\n-    partition.leaderReplicaIdOpt = Some(configs.head.brokerId)\n-    partition.setLog(log, false)\n-\n-    // create the follower replica with defined log end offset\n-    val followerId = configs(1).brokerId\n-    val allReplicas = Seq(configs.head.brokerId, followerId)\n-    partition.updateAssignmentAndIsr(\n-      assignment = allReplicas,\n-      isr = allReplicas.toSet,\n-      addingReplicas = Seq.empty,\n-      removingReplicas = Seq.empty\n-    )\n-    val leo = LogOffsetMetadata(followerLEO, 0L, followerLEO.toInt)\n-    partition.updateFollowerFetchState(\n-      followerId,\n-      followerFetchOffsetMetadata = leo,\n-      followerStartOffset = 0L,\n-      followerFetchTimeMs= time.milliseconds,\n-      leaderEndOffset = leo.messageOffset,\n-      partition.localLogOrException.highWatermark)\n-  }\n-\n-  @After\n-  def tearDown(): Unit = {\n-    replicaManager.shutdown(false)\n-    metrics.close()\n-  }\n-\n-  /**\n-   * The scenario for this test is that there is one topic that has one partition\n-   * with one leader replica on broker \"0\" and one follower replica on broker \"1\"\n-   * inside the replica manager's metadata.\n-   *\n-   * The leader replica on \"0\" has HW of \"5\" and LEO of \"20\".  The follower on\n-   * broker \"1\" has a local replica with a HW matching the leader's (\"5\") and\n-   * LEO of \"15\", meaning it's not in-sync but is still in ISR (hasn't yet expired from ISR).\n-   *\n-   * When a fetch operation with read committed data turned on is received, the replica manager\n-   * should only return data up to the HW of the partition; when a fetch operation with read\n-   * committed data turned off is received, the replica manager could return data up to the LEO\n-   * of the local leader replica's log.\n-   *\n-   * This test also verifies counts of fetch requests recorded by the ReplicaManager\n-   */\n-  @Test\n-  def testReadFromLog(): Unit = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e512e3f18ea3f1daa5546505e12082b112c14cea"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQxNDgwNA==", "bodyText": "That test name is a bit misleading. Should we name it testFetchBeyondHighwatermarkForConsumerAndFollower or something?", "url": "https://github.com/apache/kafka/pull/8586#discussion_r418414804", "createdAt": "2020-05-01T04:45:37Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/SimpleFetchTest.scala", "diffHunk": "@@ -1,206 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package kafka.server\n-\n-import java.io.File\n-\n-import kafka.api._\n-import kafka.utils._\n-import kafka.log.Log\n-import kafka.log.LogManager\n-import kafka.server.QuotaFactory.UnboundedQuota\n-import kafka.zk.KafkaZkClient\n-import org.apache.kafka.common.metrics.Metrics\n-import org.apache.kafka.common.requests.FetchRequest.PartitionData\n-import org.junit.{After, Before, Test}\n-import java.util.{Optional, Properties}\n-import java.util.concurrent.atomic.AtomicBoolean\n-\n-import org.apache.kafka.common.TopicPartition\n-import org.apache.kafka.common.record.{CompressionType, MemoryRecords, SimpleRecord}\n-import org.easymock.EasyMock\n-import org.junit.Assert._\n-\n-class SimpleFetchTest {\n-\n-  val replicaLagTimeMaxMs = 100L\n-  val replicaFetchWaitMaxMs = 100\n-  val replicaLagMaxMessages = 10L\n-\n-  val overridingProps = new Properties()\n-  overridingProps.put(KafkaConfig.ReplicaLagTimeMaxMsProp, replicaLagTimeMaxMs.toString)\n-  overridingProps.put(KafkaConfig.ReplicaFetchWaitMaxMsProp, replicaFetchWaitMaxMs.toString)\n-\n-  val configs = TestUtils.createBrokerConfigs(2, TestUtils.MockZkConnect).map(KafkaConfig.fromProps(_, overridingProps))\n-\n-  // set the replica manager with the partition\n-  val time = new MockTime\n-  val metrics = new Metrics\n-  val leaderLEO = 20L\n-  val followerLEO = 15L\n-  val partitionHW = 5\n-\n-  val fetchSize = 100\n-  val recordToHW = new SimpleRecord(\"recordToHW\".getBytes())\n-  val recordToLEO = new SimpleRecord(\"recordToLEO\".getBytes())\n-\n-  val topic = \"test-topic\"\n-  val partitionId = 0\n-  val topicPartition = new TopicPartition(topic, partitionId)\n-\n-  val fetchInfo = Seq(topicPartition -> new PartitionData(0, 0, fetchSize,\n-    Optional.empty()))\n-\n-  var replicaManager: ReplicaManager = _\n-\n-  @Before\n-  def setUp(): Unit = {\n-    // create nice mock since we don't particularly care about zkclient calls\n-    val kafkaZkClient: KafkaZkClient = EasyMock.createNiceMock(classOf[KafkaZkClient])\n-    EasyMock.replay(kafkaZkClient)\n-\n-    // create nice mock since we don't particularly care about scheduler calls\n-    val scheduler: KafkaScheduler = EasyMock.createNiceMock(classOf[KafkaScheduler])\n-    EasyMock.replay(scheduler)\n-\n-    // create the log which takes read with either HW max offset or none max offset\n-    val log: Log = EasyMock.createNiceMock(classOf[Log])\n-    EasyMock.expect(log.logStartOffset).andReturn(0).anyTimes()\n-    EasyMock.expect(log.logEndOffset).andReturn(leaderLEO).anyTimes()\n-    EasyMock.expect(log.dir).andReturn(TestUtils.tempDir()).anyTimes()\n-    EasyMock.expect(log.logEndOffsetMetadata).andReturn(LogOffsetMetadata(leaderLEO)).anyTimes()\n-    EasyMock.expect(log.maybeIncrementHighWatermark(EasyMock.anyObject[LogOffsetMetadata]))\n-      .andReturn(Some(LogOffsetMetadata(partitionHW))).anyTimes()\n-    EasyMock.expect(log.highWatermark).andReturn(partitionHW).anyTimes()\n-    EasyMock.expect(log.lastStableOffset).andReturn(partitionHW).anyTimes()\n-    EasyMock.expect(log.read(\n-      startOffset = 0,\n-      maxLength = fetchSize,\n-      isolation = FetchHighWatermark,\n-      minOneMessage = true))\n-      .andReturn(FetchDataInfo(\n-        LogOffsetMetadata(0L, 0L, 0),\n-        MemoryRecords.withRecords(CompressionType.NONE, recordToHW)\n-      )).anyTimes()\n-    EasyMock.expect(log.read(\n-      startOffset = 0,\n-      maxLength = fetchSize,\n-      isolation = FetchLogEnd,\n-      minOneMessage = true))\n-      .andReturn(FetchDataInfo(\n-        LogOffsetMetadata(0L, 0L, 0),\n-        MemoryRecords.withRecords(CompressionType.NONE, recordToLEO)\n-      )).anyTimes()\n-    EasyMock.replay(log)\n-\n-    // create the log manager that is aware of this mock log\n-    val logManager: LogManager = EasyMock.createMock(classOf[LogManager])\n-    EasyMock.expect(logManager.getLog(topicPartition, false)).andReturn(Some(log)).anyTimes()\n-    EasyMock.expect(logManager.liveLogDirs).andReturn(Array.empty[File]).anyTimes()\n-    EasyMock.replay(logManager)\n-\n-    // create the replica manager\n-    replicaManager = new ReplicaManager(configs.head, metrics, time, kafkaZkClient, scheduler, logManager,\n-      new AtomicBoolean(false), QuotaFactory.instantiate(configs.head, metrics, time, \"\"), new BrokerTopicStats,\n-      new MetadataCache(configs.head.brokerId), new LogDirFailureChannel(configs.head.logDirs.size))\n-\n-    // add the partition with two replicas, both in ISR\n-    val partition = replicaManager.createPartition(new TopicPartition(topic, partitionId))\n-\n-    // create the leader replica with the local log\n-    log.updateHighWatermark(partitionHW)\n-    partition.leaderReplicaIdOpt = Some(configs.head.brokerId)\n-    partition.setLog(log, false)\n-\n-    // create the follower replica with defined log end offset\n-    val followerId = configs(1).brokerId\n-    val allReplicas = Seq(configs.head.brokerId, followerId)\n-    partition.updateAssignmentAndIsr(\n-      assignment = allReplicas,\n-      isr = allReplicas.toSet,\n-      addingReplicas = Seq.empty,\n-      removingReplicas = Seq.empty\n-    )\n-    val leo = LogOffsetMetadata(followerLEO, 0L, followerLEO.toInt)\n-    partition.updateFollowerFetchState(\n-      followerId,\n-      followerFetchOffsetMetadata = leo,\n-      followerStartOffset = 0L,\n-      followerFetchTimeMs= time.milliseconds,\n-      leaderEndOffset = leo.messageOffset,\n-      partition.localLogOrException.highWatermark)\n-  }\n-\n-  @After\n-  def tearDown(): Unit = {\n-    replicaManager.shutdown(false)\n-    metrics.close()\n-  }\n-\n-  /**\n-   * The scenario for this test is that there is one topic that has one partition\n-   * with one leader replica on broker \"0\" and one follower replica on broker \"1\"\n-   * inside the replica manager's metadata.\n-   *\n-   * The leader replica on \"0\" has HW of \"5\" and LEO of \"20\".  The follower on\n-   * broker \"1\" has a local replica with a HW matching the leader's (\"5\") and\n-   * LEO of \"15\", meaning it's not in-sync but is still in ISR (hasn't yet expired from ISR).\n-   *\n-   * When a fetch operation with read committed data turned on is received, the replica manager\n-   * should only return data up to the HW of the partition; when a fetch operation with read\n-   * committed data turned off is received, the replica manager could return data up to the LEO\n-   * of the local leader replica's log.\n-   *\n-   * This test also verifies counts of fetch requests recorded by the ReplicaManager\n-   */\n-  @Test\n-  def testReadFromLog(): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4ODU0OQ=="}, "originalCommit": {"oid": "e512e3f18ea3f1daa5546505e12082b112c14cea"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODY1NTM2OA==", "bodyText": "Yeah, that's fair. How about just testFetchBeyondHighwatermark?", "url": "https://github.com/apache/kafka/pull/8586#discussion_r418655368", "createdAt": "2020-05-01T17:47:57Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/server/SimpleFetchTest.scala", "diffHunk": "@@ -1,206 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package kafka.server\n-\n-import java.io.File\n-\n-import kafka.api._\n-import kafka.utils._\n-import kafka.log.Log\n-import kafka.log.LogManager\n-import kafka.server.QuotaFactory.UnboundedQuota\n-import kafka.zk.KafkaZkClient\n-import org.apache.kafka.common.metrics.Metrics\n-import org.apache.kafka.common.requests.FetchRequest.PartitionData\n-import org.junit.{After, Before, Test}\n-import java.util.{Optional, Properties}\n-import java.util.concurrent.atomic.AtomicBoolean\n-\n-import org.apache.kafka.common.TopicPartition\n-import org.apache.kafka.common.record.{CompressionType, MemoryRecords, SimpleRecord}\n-import org.easymock.EasyMock\n-import org.junit.Assert._\n-\n-class SimpleFetchTest {\n-\n-  val replicaLagTimeMaxMs = 100L\n-  val replicaFetchWaitMaxMs = 100\n-  val replicaLagMaxMessages = 10L\n-\n-  val overridingProps = new Properties()\n-  overridingProps.put(KafkaConfig.ReplicaLagTimeMaxMsProp, replicaLagTimeMaxMs.toString)\n-  overridingProps.put(KafkaConfig.ReplicaFetchWaitMaxMsProp, replicaFetchWaitMaxMs.toString)\n-\n-  val configs = TestUtils.createBrokerConfigs(2, TestUtils.MockZkConnect).map(KafkaConfig.fromProps(_, overridingProps))\n-\n-  // set the replica manager with the partition\n-  val time = new MockTime\n-  val metrics = new Metrics\n-  val leaderLEO = 20L\n-  val followerLEO = 15L\n-  val partitionHW = 5\n-\n-  val fetchSize = 100\n-  val recordToHW = new SimpleRecord(\"recordToHW\".getBytes())\n-  val recordToLEO = new SimpleRecord(\"recordToLEO\".getBytes())\n-\n-  val topic = \"test-topic\"\n-  val partitionId = 0\n-  val topicPartition = new TopicPartition(topic, partitionId)\n-\n-  val fetchInfo = Seq(topicPartition -> new PartitionData(0, 0, fetchSize,\n-    Optional.empty()))\n-\n-  var replicaManager: ReplicaManager = _\n-\n-  @Before\n-  def setUp(): Unit = {\n-    // create nice mock since we don't particularly care about zkclient calls\n-    val kafkaZkClient: KafkaZkClient = EasyMock.createNiceMock(classOf[KafkaZkClient])\n-    EasyMock.replay(kafkaZkClient)\n-\n-    // create nice mock since we don't particularly care about scheduler calls\n-    val scheduler: KafkaScheduler = EasyMock.createNiceMock(classOf[KafkaScheduler])\n-    EasyMock.replay(scheduler)\n-\n-    // create the log which takes read with either HW max offset or none max offset\n-    val log: Log = EasyMock.createNiceMock(classOf[Log])\n-    EasyMock.expect(log.logStartOffset).andReturn(0).anyTimes()\n-    EasyMock.expect(log.logEndOffset).andReturn(leaderLEO).anyTimes()\n-    EasyMock.expect(log.dir).andReturn(TestUtils.tempDir()).anyTimes()\n-    EasyMock.expect(log.logEndOffsetMetadata).andReturn(LogOffsetMetadata(leaderLEO)).anyTimes()\n-    EasyMock.expect(log.maybeIncrementHighWatermark(EasyMock.anyObject[LogOffsetMetadata]))\n-      .andReturn(Some(LogOffsetMetadata(partitionHW))).anyTimes()\n-    EasyMock.expect(log.highWatermark).andReturn(partitionHW).anyTimes()\n-    EasyMock.expect(log.lastStableOffset).andReturn(partitionHW).anyTimes()\n-    EasyMock.expect(log.read(\n-      startOffset = 0,\n-      maxLength = fetchSize,\n-      isolation = FetchHighWatermark,\n-      minOneMessage = true))\n-      .andReturn(FetchDataInfo(\n-        LogOffsetMetadata(0L, 0L, 0),\n-        MemoryRecords.withRecords(CompressionType.NONE, recordToHW)\n-      )).anyTimes()\n-    EasyMock.expect(log.read(\n-      startOffset = 0,\n-      maxLength = fetchSize,\n-      isolation = FetchLogEnd,\n-      minOneMessage = true))\n-      .andReturn(FetchDataInfo(\n-        LogOffsetMetadata(0L, 0L, 0),\n-        MemoryRecords.withRecords(CompressionType.NONE, recordToLEO)\n-      )).anyTimes()\n-    EasyMock.replay(log)\n-\n-    // create the log manager that is aware of this mock log\n-    val logManager: LogManager = EasyMock.createMock(classOf[LogManager])\n-    EasyMock.expect(logManager.getLog(topicPartition, false)).andReturn(Some(log)).anyTimes()\n-    EasyMock.expect(logManager.liveLogDirs).andReturn(Array.empty[File]).anyTimes()\n-    EasyMock.replay(logManager)\n-\n-    // create the replica manager\n-    replicaManager = new ReplicaManager(configs.head, metrics, time, kafkaZkClient, scheduler, logManager,\n-      new AtomicBoolean(false), QuotaFactory.instantiate(configs.head, metrics, time, \"\"), new BrokerTopicStats,\n-      new MetadataCache(configs.head.brokerId), new LogDirFailureChannel(configs.head.logDirs.size))\n-\n-    // add the partition with two replicas, both in ISR\n-    val partition = replicaManager.createPartition(new TopicPartition(topic, partitionId))\n-\n-    // create the leader replica with the local log\n-    log.updateHighWatermark(partitionHW)\n-    partition.leaderReplicaIdOpt = Some(configs.head.brokerId)\n-    partition.setLog(log, false)\n-\n-    // create the follower replica with defined log end offset\n-    val followerId = configs(1).brokerId\n-    val allReplicas = Seq(configs.head.brokerId, followerId)\n-    partition.updateAssignmentAndIsr(\n-      assignment = allReplicas,\n-      isr = allReplicas.toSet,\n-      addingReplicas = Seq.empty,\n-      removingReplicas = Seq.empty\n-    )\n-    val leo = LogOffsetMetadata(followerLEO, 0L, followerLEO.toInt)\n-    partition.updateFollowerFetchState(\n-      followerId,\n-      followerFetchOffsetMetadata = leo,\n-      followerStartOffset = 0L,\n-      followerFetchTimeMs= time.milliseconds,\n-      leaderEndOffset = leo.messageOffset,\n-      partition.localLogOrException.highWatermark)\n-  }\n-\n-  @After\n-  def tearDown(): Unit = {\n-    replicaManager.shutdown(false)\n-    metrics.close()\n-  }\n-\n-  /**\n-   * The scenario for this test is that there is one topic that has one partition\n-   * with one leader replica on broker \"0\" and one follower replica on broker \"1\"\n-   * inside the replica manager's metadata.\n-   *\n-   * The leader replica on \"0\" has HW of \"5\" and LEO of \"20\".  The follower on\n-   * broker \"1\" has a local replica with a HW matching the leader's (\"5\") and\n-   * LEO of \"15\", meaning it's not in-sync but is still in ISR (hasn't yet expired from ISR).\n-   *\n-   * When a fetch operation with read committed data turned on is received, the replica manager\n-   * should only return data up to the HW of the partition; when a fetch operation with read\n-   * committed data turned off is received, the replica manager could return data up to the LEO\n-   * of the local leader replica's log.\n-   *\n-   * This test also verifies counts of fetch requests recorded by the ReplicaManager\n-   */\n-  @Test\n-  def testReadFromLog(): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4ODU0OQ=="}, "originalCommit": {"oid": "e512e3f18ea3f1daa5546505e12082b112c14cea"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODY3OTk1Mw==", "bodyText": "That's fine. Is it worth adding a brief comment stating what it's testing?", "url": "https://github.com/apache/kafka/pull/8586#discussion_r418679953", "createdAt": "2020-05-01T18:42:19Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/SimpleFetchTest.scala", "diffHunk": "@@ -1,206 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package kafka.server\n-\n-import java.io.File\n-\n-import kafka.api._\n-import kafka.utils._\n-import kafka.log.Log\n-import kafka.log.LogManager\n-import kafka.server.QuotaFactory.UnboundedQuota\n-import kafka.zk.KafkaZkClient\n-import org.apache.kafka.common.metrics.Metrics\n-import org.apache.kafka.common.requests.FetchRequest.PartitionData\n-import org.junit.{After, Before, Test}\n-import java.util.{Optional, Properties}\n-import java.util.concurrent.atomic.AtomicBoolean\n-\n-import org.apache.kafka.common.TopicPartition\n-import org.apache.kafka.common.record.{CompressionType, MemoryRecords, SimpleRecord}\n-import org.easymock.EasyMock\n-import org.junit.Assert._\n-\n-class SimpleFetchTest {\n-\n-  val replicaLagTimeMaxMs = 100L\n-  val replicaFetchWaitMaxMs = 100\n-  val replicaLagMaxMessages = 10L\n-\n-  val overridingProps = new Properties()\n-  overridingProps.put(KafkaConfig.ReplicaLagTimeMaxMsProp, replicaLagTimeMaxMs.toString)\n-  overridingProps.put(KafkaConfig.ReplicaFetchWaitMaxMsProp, replicaFetchWaitMaxMs.toString)\n-\n-  val configs = TestUtils.createBrokerConfigs(2, TestUtils.MockZkConnect).map(KafkaConfig.fromProps(_, overridingProps))\n-\n-  // set the replica manager with the partition\n-  val time = new MockTime\n-  val metrics = new Metrics\n-  val leaderLEO = 20L\n-  val followerLEO = 15L\n-  val partitionHW = 5\n-\n-  val fetchSize = 100\n-  val recordToHW = new SimpleRecord(\"recordToHW\".getBytes())\n-  val recordToLEO = new SimpleRecord(\"recordToLEO\".getBytes())\n-\n-  val topic = \"test-topic\"\n-  val partitionId = 0\n-  val topicPartition = new TopicPartition(topic, partitionId)\n-\n-  val fetchInfo = Seq(topicPartition -> new PartitionData(0, 0, fetchSize,\n-    Optional.empty()))\n-\n-  var replicaManager: ReplicaManager = _\n-\n-  @Before\n-  def setUp(): Unit = {\n-    // create nice mock since we don't particularly care about zkclient calls\n-    val kafkaZkClient: KafkaZkClient = EasyMock.createNiceMock(classOf[KafkaZkClient])\n-    EasyMock.replay(kafkaZkClient)\n-\n-    // create nice mock since we don't particularly care about scheduler calls\n-    val scheduler: KafkaScheduler = EasyMock.createNiceMock(classOf[KafkaScheduler])\n-    EasyMock.replay(scheduler)\n-\n-    // create the log which takes read with either HW max offset or none max offset\n-    val log: Log = EasyMock.createNiceMock(classOf[Log])\n-    EasyMock.expect(log.logStartOffset).andReturn(0).anyTimes()\n-    EasyMock.expect(log.logEndOffset).andReturn(leaderLEO).anyTimes()\n-    EasyMock.expect(log.dir).andReturn(TestUtils.tempDir()).anyTimes()\n-    EasyMock.expect(log.logEndOffsetMetadata).andReturn(LogOffsetMetadata(leaderLEO)).anyTimes()\n-    EasyMock.expect(log.maybeIncrementHighWatermark(EasyMock.anyObject[LogOffsetMetadata]))\n-      .andReturn(Some(LogOffsetMetadata(partitionHW))).anyTimes()\n-    EasyMock.expect(log.highWatermark).andReturn(partitionHW).anyTimes()\n-    EasyMock.expect(log.lastStableOffset).andReturn(partitionHW).anyTimes()\n-    EasyMock.expect(log.read(\n-      startOffset = 0,\n-      maxLength = fetchSize,\n-      isolation = FetchHighWatermark,\n-      minOneMessage = true))\n-      .andReturn(FetchDataInfo(\n-        LogOffsetMetadata(0L, 0L, 0),\n-        MemoryRecords.withRecords(CompressionType.NONE, recordToHW)\n-      )).anyTimes()\n-    EasyMock.expect(log.read(\n-      startOffset = 0,\n-      maxLength = fetchSize,\n-      isolation = FetchLogEnd,\n-      minOneMessage = true))\n-      .andReturn(FetchDataInfo(\n-        LogOffsetMetadata(0L, 0L, 0),\n-        MemoryRecords.withRecords(CompressionType.NONE, recordToLEO)\n-      )).anyTimes()\n-    EasyMock.replay(log)\n-\n-    // create the log manager that is aware of this mock log\n-    val logManager: LogManager = EasyMock.createMock(classOf[LogManager])\n-    EasyMock.expect(logManager.getLog(topicPartition, false)).andReturn(Some(log)).anyTimes()\n-    EasyMock.expect(logManager.liveLogDirs).andReturn(Array.empty[File]).anyTimes()\n-    EasyMock.replay(logManager)\n-\n-    // create the replica manager\n-    replicaManager = new ReplicaManager(configs.head, metrics, time, kafkaZkClient, scheduler, logManager,\n-      new AtomicBoolean(false), QuotaFactory.instantiate(configs.head, metrics, time, \"\"), new BrokerTopicStats,\n-      new MetadataCache(configs.head.brokerId), new LogDirFailureChannel(configs.head.logDirs.size))\n-\n-    // add the partition with two replicas, both in ISR\n-    val partition = replicaManager.createPartition(new TopicPartition(topic, partitionId))\n-\n-    // create the leader replica with the local log\n-    log.updateHighWatermark(partitionHW)\n-    partition.leaderReplicaIdOpt = Some(configs.head.brokerId)\n-    partition.setLog(log, false)\n-\n-    // create the follower replica with defined log end offset\n-    val followerId = configs(1).brokerId\n-    val allReplicas = Seq(configs.head.brokerId, followerId)\n-    partition.updateAssignmentAndIsr(\n-      assignment = allReplicas,\n-      isr = allReplicas.toSet,\n-      addingReplicas = Seq.empty,\n-      removingReplicas = Seq.empty\n-    )\n-    val leo = LogOffsetMetadata(followerLEO, 0L, followerLEO.toInt)\n-    partition.updateFollowerFetchState(\n-      followerId,\n-      followerFetchOffsetMetadata = leo,\n-      followerStartOffset = 0L,\n-      followerFetchTimeMs= time.milliseconds,\n-      leaderEndOffset = leo.messageOffset,\n-      partition.localLogOrException.highWatermark)\n-  }\n-\n-  @After\n-  def tearDown(): Unit = {\n-    replicaManager.shutdown(false)\n-    metrics.close()\n-  }\n-\n-  /**\n-   * The scenario for this test is that there is one topic that has one partition\n-   * with one leader replica on broker \"0\" and one follower replica on broker \"1\"\n-   * inside the replica manager's metadata.\n-   *\n-   * The leader replica on \"0\" has HW of \"5\" and LEO of \"20\".  The follower on\n-   * broker \"1\" has a local replica with a HW matching the leader's (\"5\") and\n-   * LEO of \"15\", meaning it's not in-sync but is still in ISR (hasn't yet expired from ISR).\n-   *\n-   * When a fetch operation with read committed data turned on is received, the replica manager\n-   * should only return data up to the HW of the partition; when a fetch operation with read\n-   * committed data turned off is received, the replica manager could return data up to the LEO\n-   * of the local leader replica's log.\n-   *\n-   * This test also verifies counts of fetch requests recorded by the ReplicaManager\n-   */\n-  @Test\n-  def testReadFromLog(): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4ODU0OQ=="}, "originalCommit": {"oid": "e512e3f18ea3f1daa5546505e12082b112c14cea"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODcxODY0NQ==", "bodyText": "Borderline overkill I guess, but I added a few comments explaining the test behavior.", "url": "https://github.com/apache/kafka/pull/8586#discussion_r418718645", "createdAt": "2020-05-01T20:15:47Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/server/SimpleFetchTest.scala", "diffHunk": "@@ -1,206 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one or more\n- * contributor license agreements.  See the NOTICE file distributed with\n- * this work for additional information regarding copyright ownership.\n- * The ASF licenses this file to You under the Apache License, Version 2.0\n- * (the \"License\"); you may not use this file except in compliance with\n- * the License.  You may obtain a copy of the License at\n- *\n- *    http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package kafka.server\n-\n-import java.io.File\n-\n-import kafka.api._\n-import kafka.utils._\n-import kafka.log.Log\n-import kafka.log.LogManager\n-import kafka.server.QuotaFactory.UnboundedQuota\n-import kafka.zk.KafkaZkClient\n-import org.apache.kafka.common.metrics.Metrics\n-import org.apache.kafka.common.requests.FetchRequest.PartitionData\n-import org.junit.{After, Before, Test}\n-import java.util.{Optional, Properties}\n-import java.util.concurrent.atomic.AtomicBoolean\n-\n-import org.apache.kafka.common.TopicPartition\n-import org.apache.kafka.common.record.{CompressionType, MemoryRecords, SimpleRecord}\n-import org.easymock.EasyMock\n-import org.junit.Assert._\n-\n-class SimpleFetchTest {\n-\n-  val replicaLagTimeMaxMs = 100L\n-  val replicaFetchWaitMaxMs = 100\n-  val replicaLagMaxMessages = 10L\n-\n-  val overridingProps = new Properties()\n-  overridingProps.put(KafkaConfig.ReplicaLagTimeMaxMsProp, replicaLagTimeMaxMs.toString)\n-  overridingProps.put(KafkaConfig.ReplicaFetchWaitMaxMsProp, replicaFetchWaitMaxMs.toString)\n-\n-  val configs = TestUtils.createBrokerConfigs(2, TestUtils.MockZkConnect).map(KafkaConfig.fromProps(_, overridingProps))\n-\n-  // set the replica manager with the partition\n-  val time = new MockTime\n-  val metrics = new Metrics\n-  val leaderLEO = 20L\n-  val followerLEO = 15L\n-  val partitionHW = 5\n-\n-  val fetchSize = 100\n-  val recordToHW = new SimpleRecord(\"recordToHW\".getBytes())\n-  val recordToLEO = new SimpleRecord(\"recordToLEO\".getBytes())\n-\n-  val topic = \"test-topic\"\n-  val partitionId = 0\n-  val topicPartition = new TopicPartition(topic, partitionId)\n-\n-  val fetchInfo = Seq(topicPartition -> new PartitionData(0, 0, fetchSize,\n-    Optional.empty()))\n-\n-  var replicaManager: ReplicaManager = _\n-\n-  @Before\n-  def setUp(): Unit = {\n-    // create nice mock since we don't particularly care about zkclient calls\n-    val kafkaZkClient: KafkaZkClient = EasyMock.createNiceMock(classOf[KafkaZkClient])\n-    EasyMock.replay(kafkaZkClient)\n-\n-    // create nice mock since we don't particularly care about scheduler calls\n-    val scheduler: KafkaScheduler = EasyMock.createNiceMock(classOf[KafkaScheduler])\n-    EasyMock.replay(scheduler)\n-\n-    // create the log which takes read with either HW max offset or none max offset\n-    val log: Log = EasyMock.createNiceMock(classOf[Log])\n-    EasyMock.expect(log.logStartOffset).andReturn(0).anyTimes()\n-    EasyMock.expect(log.logEndOffset).andReturn(leaderLEO).anyTimes()\n-    EasyMock.expect(log.dir).andReturn(TestUtils.tempDir()).anyTimes()\n-    EasyMock.expect(log.logEndOffsetMetadata).andReturn(LogOffsetMetadata(leaderLEO)).anyTimes()\n-    EasyMock.expect(log.maybeIncrementHighWatermark(EasyMock.anyObject[LogOffsetMetadata]))\n-      .andReturn(Some(LogOffsetMetadata(partitionHW))).anyTimes()\n-    EasyMock.expect(log.highWatermark).andReturn(partitionHW).anyTimes()\n-    EasyMock.expect(log.lastStableOffset).andReturn(partitionHW).anyTimes()\n-    EasyMock.expect(log.read(\n-      startOffset = 0,\n-      maxLength = fetchSize,\n-      isolation = FetchHighWatermark,\n-      minOneMessage = true))\n-      .andReturn(FetchDataInfo(\n-        LogOffsetMetadata(0L, 0L, 0),\n-        MemoryRecords.withRecords(CompressionType.NONE, recordToHW)\n-      )).anyTimes()\n-    EasyMock.expect(log.read(\n-      startOffset = 0,\n-      maxLength = fetchSize,\n-      isolation = FetchLogEnd,\n-      minOneMessage = true))\n-      .andReturn(FetchDataInfo(\n-        LogOffsetMetadata(0L, 0L, 0),\n-        MemoryRecords.withRecords(CompressionType.NONE, recordToLEO)\n-      )).anyTimes()\n-    EasyMock.replay(log)\n-\n-    // create the log manager that is aware of this mock log\n-    val logManager: LogManager = EasyMock.createMock(classOf[LogManager])\n-    EasyMock.expect(logManager.getLog(topicPartition, false)).andReturn(Some(log)).anyTimes()\n-    EasyMock.expect(logManager.liveLogDirs).andReturn(Array.empty[File]).anyTimes()\n-    EasyMock.replay(logManager)\n-\n-    // create the replica manager\n-    replicaManager = new ReplicaManager(configs.head, metrics, time, kafkaZkClient, scheduler, logManager,\n-      new AtomicBoolean(false), QuotaFactory.instantiate(configs.head, metrics, time, \"\"), new BrokerTopicStats,\n-      new MetadataCache(configs.head.brokerId), new LogDirFailureChannel(configs.head.logDirs.size))\n-\n-    // add the partition with two replicas, both in ISR\n-    val partition = replicaManager.createPartition(new TopicPartition(topic, partitionId))\n-\n-    // create the leader replica with the local log\n-    log.updateHighWatermark(partitionHW)\n-    partition.leaderReplicaIdOpt = Some(configs.head.brokerId)\n-    partition.setLog(log, false)\n-\n-    // create the follower replica with defined log end offset\n-    val followerId = configs(1).brokerId\n-    val allReplicas = Seq(configs.head.brokerId, followerId)\n-    partition.updateAssignmentAndIsr(\n-      assignment = allReplicas,\n-      isr = allReplicas.toSet,\n-      addingReplicas = Seq.empty,\n-      removingReplicas = Seq.empty\n-    )\n-    val leo = LogOffsetMetadata(followerLEO, 0L, followerLEO.toInt)\n-    partition.updateFollowerFetchState(\n-      followerId,\n-      followerFetchOffsetMetadata = leo,\n-      followerStartOffset = 0L,\n-      followerFetchTimeMs= time.milliseconds,\n-      leaderEndOffset = leo.messageOffset,\n-      partition.localLogOrException.highWatermark)\n-  }\n-\n-  @After\n-  def tearDown(): Unit = {\n-    replicaManager.shutdown(false)\n-    metrics.close()\n-  }\n-\n-  /**\n-   * The scenario for this test is that there is one topic that has one partition\n-   * with one leader replica on broker \"0\" and one follower replica on broker \"1\"\n-   * inside the replica manager's metadata.\n-   *\n-   * The leader replica on \"0\" has HW of \"5\" and LEO of \"20\".  The follower on\n-   * broker \"1\" has a local replica with a HW matching the leader's (\"5\") and\n-   * LEO of \"15\", meaning it's not in-sync but is still in ISR (hasn't yet expired from ISR).\n-   *\n-   * When a fetch operation with read committed data turned on is received, the replica manager\n-   * should only return data up to the HW of the partition; when a fetch operation with read\n-   * committed data turned off is received, the replica manager could return data up to the LEO\n-   * of the local leader replica's log.\n-   *\n-   * This test also verifies counts of fetch requests recorded by the ReplicaManager\n-   */\n-  @Test\n-  def testReadFromLog(): Unit = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY4ODU0OQ=="}, "originalCommit": {"oid": "e512e3f18ea3f1daa5546505e12082b112c14cea"}, "originalPosition": 171}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwMzQ3NDQ1OnYy", "diffSide": "RIGHT", "path": "core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwNDo0NzowNVrOGPCB-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQxNzo0Mzo0MVrOGPQkwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQxNTA5OA==", "bodyText": "This would return 3 without the fix?", "url": "https://github.com/apache/kafka/pull/8586#discussion_r418415098", "createdAt": "2020-05-01T04:47:05Z", "author": {"login": "ijuma"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala", "diffHunk": "@@ -1017,6 +1018,51 @@ class ReplicaManagerTest {\n     assertEquals(Errors.NOT_LEADER_FOR_PARTITION, fetchResult.get.error)\n   }\n \n+  @Test\n+  def testFetchRequestRateMetrics(): Unit = {\n+    val mockTimer = new MockTimer\n+    val replicaManager = setupReplicaManagerWithMockedPurgatories(mockTimer, aliveBrokerIds = Seq(0, 1))\n+\n+    val tp0 = new TopicPartition(topic, 0)\n+    val offsetCheckpoints = new LazyOffsetCheckpoints(replicaManager.highWatermarkCheckpoints)\n+    replicaManager.createPartition(tp0).createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints)\n+    val partition0Replicas = Seq[Integer](0, 1).asJava\n+\n+    val becomeLeaderRequest = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0, brokerEpoch,\n+      Seq(new LeaderAndIsrPartitionState()\n+        .setTopicName(tp0.topic)\n+        .setPartitionIndex(tp0.partition)\n+        .setControllerEpoch(0)\n+        .setLeader(0)\n+        .setLeaderEpoch(1)\n+        .setIsr(partition0Replicas)\n+        .setZkVersion(0)\n+        .setReplicas(partition0Replicas)\n+        .setIsNew(true)).asJava,\n+      Set(new Node(0, \"host1\", 0), new Node(1, \"host2\", 1)).asJava).build()\n+    replicaManager.becomeLeaderOrFollower(1, becomeLeaderRequest, (_, _) => ())\n+\n+    def assertMetricCount(expected: Int): Unit = {\n+      assertEquals(expected, replicaManager.brokerTopicStats.allTopicsStats.totalFetchRequestRate.count)\n+      assertEquals(expected, replicaManager.brokerTopicStats.topicStats(topic).totalFetchRequestRate.count)\n+    }\n+\n+    val partitionData = new FetchRequest.PartitionData(0L, 0L, 100,\n+      Optional.empty())\n+\n+    val nonPurgatoryFetchResult = sendConsumerFetch(replicaManager, tp0, partitionData, None, timeout = 0)\n+    assertNotNull(nonPurgatoryFetchResult.get)\n+    assertEquals(Errors.NONE, nonPurgatoryFetchResult.get.error)\n+    assertMetricCount(1)\n+\n+    val purgatoryFetchResult = sendConsumerFetch(replicaManager, tp0, partitionData, None, timeout = 10)\n+    assertNull(purgatoryFetchResult.get)\n+    mockTimer.advanceClock(11)\n+    assertNotNull(purgatoryFetchResult.get)\n+    assertEquals(Errors.NONE, purgatoryFetchResult.get.error)\n+    assertMetricCount(2)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e512e3f18ea3f1daa5546505e12082b112c14cea"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODY1MzM3Ng==", "bodyText": "Right.", "url": "https://github.com/apache/kafka/pull/8586#discussion_r418653376", "createdAt": "2020-05-01T17:43:41Z", "author": {"login": "hachikuji"}, "path": "core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala", "diffHunk": "@@ -1017,6 +1018,51 @@ class ReplicaManagerTest {\n     assertEquals(Errors.NOT_LEADER_FOR_PARTITION, fetchResult.get.error)\n   }\n \n+  @Test\n+  def testFetchRequestRateMetrics(): Unit = {\n+    val mockTimer = new MockTimer\n+    val replicaManager = setupReplicaManagerWithMockedPurgatories(mockTimer, aliveBrokerIds = Seq(0, 1))\n+\n+    val tp0 = new TopicPartition(topic, 0)\n+    val offsetCheckpoints = new LazyOffsetCheckpoints(replicaManager.highWatermarkCheckpoints)\n+    replicaManager.createPartition(tp0).createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints)\n+    val partition0Replicas = Seq[Integer](0, 1).asJava\n+\n+    val becomeLeaderRequest = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0, brokerEpoch,\n+      Seq(new LeaderAndIsrPartitionState()\n+        .setTopicName(tp0.topic)\n+        .setPartitionIndex(tp0.partition)\n+        .setControllerEpoch(0)\n+        .setLeader(0)\n+        .setLeaderEpoch(1)\n+        .setIsr(partition0Replicas)\n+        .setZkVersion(0)\n+        .setReplicas(partition0Replicas)\n+        .setIsNew(true)).asJava,\n+      Set(new Node(0, \"host1\", 0), new Node(1, \"host2\", 1)).asJava).build()\n+    replicaManager.becomeLeaderOrFollower(1, becomeLeaderRequest, (_, _) => ())\n+\n+    def assertMetricCount(expected: Int): Unit = {\n+      assertEquals(expected, replicaManager.brokerTopicStats.allTopicsStats.totalFetchRequestRate.count)\n+      assertEquals(expected, replicaManager.brokerTopicStats.topicStats(topic).totalFetchRequestRate.count)\n+    }\n+\n+    val partitionData = new FetchRequest.PartitionData(0L, 0L, 100,\n+      Optional.empty())\n+\n+    val nonPurgatoryFetchResult = sendConsumerFetch(replicaManager, tp0, partitionData, None, timeout = 0)\n+    assertNotNull(nonPurgatoryFetchResult.get)\n+    assertEquals(Errors.NONE, nonPurgatoryFetchResult.get.error)\n+    assertMetricCount(1)\n+\n+    val purgatoryFetchResult = sendConsumerFetch(replicaManager, tp0, partitionData, None, timeout = 10)\n+    assertNull(purgatoryFetchResult.get)\n+    mockTimer.advanceClock(11)\n+    assertNotNull(purgatoryFetchResult.get)\n+    assertEquals(Errors.NONE, purgatoryFetchResult.get.error)\n+    assertMetricCount(2)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQxNTA5OA=="}, "originalCommit": {"oid": "e512e3f18ea3f1daa5546505e12082b112c14cea"}, "originalPosition": 54}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2829, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}