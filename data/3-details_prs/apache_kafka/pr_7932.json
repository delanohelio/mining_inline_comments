{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYxNjA1Mzc4", "number": 7932, "title": "KAFKA-8764: LogCleanerManager endless loop while compacting/clea", "bodyText": "This PR fixes LogCleaner's endless loop while clearing LogSegemnts with holes.\nIn rare cases, when clearing LogSegments with missing records, LogCleaner was unable to progress resulting with high CPU usage, high disk read/writes and excessive cleaner logs (if enabled). This PR addresses such situation by skipping missing record(s) and, as result, avoiding endless loop while clearing such Logs.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-01-10T20:14:07Z", "url": "https://github.com/apache/kafka/pull/7932", "merged": true, "mergeCommit": {"oid": "06e296be9b8d1ce777074b0e4953b6ad66105d55"}, "closed": true, "closedAt": "2020-01-14T22:03:45Z", "author": {"login": "trajakovic"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb5AltWAH2gAyMzYxNjA1Mzc4OmIyZWMxMDU0ZWQwM2Q5NDZiNzFkMTViMzJlYjA1MGViNDdkNmZhMjA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABb6UEChgFqTM0MjY5Mzk1Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "b2ec1054ed03d946b71d15b32eb050eb47d6fa20", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/b2ec1054ed03d946b71d15b32eb050eb47d6fa20", "committedDate": "2020-01-10T15:49:48Z", "message": "fixes KAFKA-8764 LogCleanerManager endless loop while compacting/cleaning segments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQxNDg4NTAx", "url": "https://github.com/apache/kafka/pull/7932#pullrequestreview-341488501", "createdAt": "2020-01-11T00:45:49Z", "commit": {"oid": "b2ec1054ed03d946b71d15b32eb050eb47d6fa20"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMVQwMDo0NTo0OVrOFcjT2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMVQwMDo0NTo0OVrOFcjT2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTQ4Mjk2OQ==", "bodyText": "I am not sure if this completely fixes the issue. Let's say we have one non active segment with offset range [100, 190) and an active segment of [200, 300). With this change, we will set the latestOffset in the map to 190 (which is an improvement from 0). However, this can still trigger another round of unnecessary cleaning since the first segment will still be treated as cleanable (instead of cleaned).\nOne way to fix this more completely is that when we have finished iterating all batches in a segment, we update the latest offset in the map to the start offset of the next segment. In the above example, this will set the latest offset of the map to 200 and the next round of cleaning won't be triggered until the active segment rolls.", "url": "https://github.com/apache/kafka/pull/7932#discussion_r365482969", "createdAt": "2020-01-11T00:45:49Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleaner.scala", "diffHunk": "@@ -931,11 +931,15 @@ private[log] class Cleaner(val id: Int,\n             stats.indexMessagesRead(batch.countOrNull)\n           } else {\n             for (record <- batch.asScala) {\n-              if (record.hasKey && record.offset >= startOffset) {\n-                if (map.size < maxDesiredMapSize)\n-                  map.put(record.key, record.offset)\n-                else\n-                  return true\n+              if (record.hasKey) {\n+                if (record.offset >= startOffset) {\n+                  if (map.size < maxDesiredMapSize)\n+                    map.put(record.key, record.offset)\n+                  else\n+                    return true\n+                } else {\n+                  map.updateLatestOffset(record.offset + 1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2ec1054ed03d946b71d15b32eb050eb47d6fa20"}, "originalPosition": 16}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e0c2dcc39fefdabeb67534a93b0bd32d53dbe815", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/e0c2dcc39fefdabeb67534a93b0bd32d53dbe815", "committedDate": "2020-01-13T15:01:27Z", "message": "add tests replicating KAFKA-8764 issue\nupdate LogCleaner to fix KAFKA-8764 issue"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQxOTEwNzQ4", "url": "https://github.com/apache/kafka/pull/7932#pullrequestreview-341910748", "createdAt": "2020-01-13T15:08:49Z", "commit": {"oid": "e0c2dcc39fefdabeb67534a93b0bd32d53dbe815"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xM1QxNTowODo0OVrOFc5-Xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xM1QxNTowODo0OVrOFc5-Xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTg1NDMwMw==", "bodyText": "@junrao\nwell you were right, and after i wrote test case(es), first fix didn't resolve issue, so I've reverted the code, and give 2nd chance to fix it.", "url": "https://github.com/apache/kafka/pull/7932#discussion_r365854303", "createdAt": "2020-01-13T15:08:49Z", "author": {"login": "trajakovic"}, "path": "core/src/main/scala/kafka/log/LogCleaner.scala", "diffHunk": "@@ -957,6 +961,12 @@ private[log] class Cleaner(val id: Int,\n       if(position == startPosition)\n         growBuffersOrFail(segment.log, position, maxLogMessageSize, records)\n     }\n+\n+    // check for missing offsets at the end of logSegment\n+    if (lastOffsetInSegment < nextSegmentStartOffset - 1L) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c2dcc39fefdabeb67534a93b0bd32d53dbe815"}, "originalPosition": 74}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQxOTE1MTU5", "url": "https://github.com/apache/kafka/pull/7932#pullrequestreview-341915159", "createdAt": "2020-01-13T15:14:50Z", "commit": {"oid": "e0c2dcc39fefdabeb67534a93b0bd32d53dbe815"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xM1QxNToxNDo1MFrOFc6LdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xM1QxNToxNDo1MFrOFc6LdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTg1NzY1Mg==", "bodyText": "Don't know more idiomatic way to write this in Scala", "url": "https://github.com/apache/kafka/pull/7932#discussion_r365857652", "createdAt": "2020-01-13T15:14:50Z", "author": {"login": "trajakovic"}, "path": "core/src/main/scala/kafka/log/LogCleaner.scala", "diffHunk": "@@ -867,6 +867,11 @@ private[log] class Cleaner(val id: Int,\n                                   stats: CleanerStats): Unit = {\n     map.clear()\n     val dirty = log.logSegments(start, end).toBuffer\n+    val nextSegmentStartOffsets : mutable.Buffer[Long] = if (dirty.nonEmpty) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c2dcc39fefdabeb67534a93b0bd32d53dbe815"}, "originalPosition": 4}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2c464ac94f7cf025d430dea6a3f12627e75a6794", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/2c464ac94f7cf025d430dea6a3f12627e75a6794", "committedDate": "2020-01-13T19:23:55Z", "message": "add mutable.ListBuffer import\nmake code mode Scala way"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQyMDk2NzQx", "url": "https://github.com/apache/kafka/pull/7932#pullrequestreview-342096741", "createdAt": "2020-01-13T19:50:18Z", "commit": {"oid": "2c464ac94f7cf025d430dea6a3f12627e75a6794"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xM1QxOTo1MDoxOFrOFdCnKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xM1QyMToyODoxOVrOFdFKfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTk5NTgxNg==", "bodyText": "It would be useful to add a comment on why we need to do this. Also, not sure if we need to do the test in the line below since nextSegment's start offset is guaranteed to be larger than all offsets in previous segments.", "url": "https://github.com/apache/kafka/pull/7932#discussion_r365995816", "createdAt": "2020-01-13T19:50:18Z", "author": {"login": "junrao"}, "path": "core/src/main/scala/kafka/log/LogCleaner.scala", "diffHunk": "@@ -953,6 +962,12 @@ private[log] class Cleaner(val id: Int,\n       if(position == startPosition)\n         growBuffersOrFail(segment.log, position, maxLogMessageSize, records)\n     }\n+\n+    // check for missing offsets at the end of logSegment", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c464ac94f7cf025d430dea6a3f12627e75a6794"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NjAzNzYyOQ==", "bodyText": "The error logging is a bit unintuitive. How about \"Cleaning point should pass offset gap\"? Ditto below.", "url": "https://github.com/apache/kafka/pull/7932#discussion_r366037629", "createdAt": "2020-01-13T21:28:19Z", "author": {"login": "junrao"}, "path": "core/src/test/scala/unit/kafka/log/LogCleanerTest.scala", "diffHunk": "@@ -1561,6 +1561,49 @@ class LogCleanerTest {\n     assertEquals(\"The tombstone should be retained.\", 1, log.logSegments.head.log.batches.iterator.next().lastOffset)\n   }\n \n+  /**\n+   * Verify that the clean is able to move beyond missing offsets records in dirty log\n+   */\n+  @Test\n+  def testCleaningBeyondMissingOffsets(): Unit = {\n+    val logProps = new Properties()\n+    logProps.put(LogConfig.SegmentBytesProp, 1024*1024: java.lang.Integer)\n+    logProps.put(LogConfig.CleanupPolicyProp, LogConfig.Compact)\n+    val logConfig = LogConfig(logProps)\n+    val cleaner = makeCleaner(Int.MaxValue)\n+\n+    {\n+      val log = makeLog(dir = TestUtils.randomPartitionLogDir(tmpdir), config = logConfig)\n+      writeToLog(log, (0 to 9) zip (0 to 9), (0L to 9L))\n+      // roll new segment with baseOffset 11, leaving previous with holes in offset range [9,10]\n+      log.roll(Some(11L))\n+\n+      // active segment record\n+      log.appendAsFollower(messageWithOffset(1015, 1015, 11L))\n+\n+      val (nextDirtyOffset, _) = cleaner.clean(LogToClean(log.topicPartition, log, 0L, log.activeSegment.baseOffset, needCompactionNow = true))\n+      assertEquals(\"Missing offsets should be skipped until next segment baseOffset\", log.activeSegment.baseOffset, nextDirtyOffset)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c464ac94f7cf025d430dea6a3f12627e75a6794"}, "originalPosition": 25}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "630841d86ba826b407fb8915055255be0775fe4b", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/630841d86ba826b407fb8915055255be0775fe4b", "committedDate": "2020-01-14T11:04:50Z", "message": "update asserts to more meaningful messages"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dcd952ccaa29830429a9cbf70823fd0328cd3b13", "author": {"user": null}, "url": "https://github.com/apache/kafka/commit/dcd952ccaa29830429a9cbf70823fd0328cd3b13", "committedDate": "2020-01-14T11:12:55Z", "message": "remove always true condition\nupdate comment to reflect case it solves"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQyNjkzOTU2", "url": "https://github.com/apache/kafka/pull/7932#pullrequestreview-342693956", "createdAt": "2020-01-14T17:05:04Z", "commit": {"oid": "dcd952ccaa29830429a9cbf70823fd0328cd3b13"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1950, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}