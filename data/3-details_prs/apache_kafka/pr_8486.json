{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAzMzA2NDYw", "number": 8486, "title": "KAFKA-9840: Skip End Offset validation when the leader epoch is not reliable", "bodyText": "This PR provides two fixes:\n\nComplete offset validation if the leader epoch is not reliable\nRefresh metadata when the return offset or epoch is not defined\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-04-14T17:03:56Z", "url": "https://github.com/apache/kafka/pull/8486", "merged": true, "mergeCommit": {"oid": "910f3179960067135ec8ad4ab83d4582ff3847b5"}, "closed": true, "closedAt": "2020-06-05T22:53:14Z", "author": {"login": "abbccdda"}, "timelineItems": {"totalCount": 42, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcXmnM0gBqjMyMzIwNjk3MjM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcoIvSwgH2gAyNDAzMzA2NDYwOjg3Y2EyMzRjMzk1ZDFkMGIwNTY2MjY5YzRjZDI5OWRhOTM2ZDJjYjM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "f4c87f5c6bae2691d9a1c1943eda32627ee4990e", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/f4c87f5c6bae2691d9a1c1943eda32627ee4990e", "committedDate": "2020-04-21T19:42:14Z", "message": "do not send"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3ODA4MTc4", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-397808178", "createdAt": "2020-04-22T02:56:27Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwMjo1NjoyN1rOGJhCkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwMjo1NjoyN1rOGJhCkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjYzMTY5OA==", "bodyText": "Move the function closer to its caller.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r412631698", "createdAt": "2020-04-22T02:56:27Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -498,6 +498,104 @@ public void validateOffsetsIfNeeded() {\n         validateOffsetsAsync(partitionsToValidate);\n     }\n \n+    /**\n+     * For each partition which needs validation, make an asynchronous request to get the end-offsets for the partition\n+     * with the epoch less than or equal to the epoch the partition last saw.\n+     *\n+     * Requests are grouped by Node for efficiency.\n+     */\n+    private void validateOffsetsAsync(Map<TopicPartition, SubscriptionState.FetchPosition> partitionsToValidate) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 10}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3ODA4MzU4", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-397808358", "createdAt": "2020-04-22T02:56:58Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwMjo1Njo1OFrOGJhDRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwMjo1Njo1OFrOGJhDRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjYzMTg3OQ==", "bodyText": "Change one: immediately complete the validation when the leader epoch is not reliable.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r412631879", "createdAt": "2020-04-22T02:56:58Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -498,6 +498,104 @@ public void validateOffsetsIfNeeded() {\n         validateOffsetsAsync(partitionsToValidate);\n     }\n \n+    /**\n+     * For each partition which needs validation, make an asynchronous request to get the end-offsets for the partition\n+     * with the epoch less than or equal to the epoch the partition last saw.\n+     *\n+     * Requests are grouped by Node for efficiency.\n+     */\n+    private void validateOffsetsAsync(Map<TopicPartition, SubscriptionState.FetchPosition> partitionsToValidate) {\n+        final Map<Node, Map<TopicPartition, SubscriptionState.FetchPosition>> regrouped =\n+            regroupFetchPositionsByLeader(partitionsToValidate);\n+\n+        regrouped.forEach((node, fetchPositions) -> {\n+            if (node.isEmpty()) {\n+                metadata.requestUpdate();\n+                return;\n+            }\n+\n+            NodeApiVersions nodeApiVersions = apiVersions.get(node.idString());\n+            if (nodeApiVersions == null) {\n+                client.tryConnect(node);\n+                return;\n+            }\n+\n+            if (!hasUsableOffsetForLeaderEpochVersion(nodeApiVersions)) {\n+                log.debug(\"Skipping validation of fetch offsets for partitions {} since the broker does not \" +\n+                              \"support the required protocol version (introduced in Kafka 2.3)\",\n+                    fetchPositions.keySet());\n+                completeAllValidations(fetchPositions);\n+                return;\n+            }\n+\n+            // We need to get the client epoch state before sending out the leader epoch request, and use it to\n+            // decide whether we need to validate offsets.\n+            if (!metadata.hasReliableLeaderEpochs()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3ODA4NTcw", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-397808570", "createdAt": "2020-04-22T02:57:44Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwMjo1Nzo0NFrOGJhEHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwMjo1Nzo0NFrOGJhEHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjYzMjA5Mg==", "bodyText": "Change two: do not complete the validation as the returned epoch or offset is invalid.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r412632092", "createdAt": "2020-04-22T02:57:44Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -498,6 +498,104 @@ public void validateOffsetsIfNeeded() {\n         validateOffsetsAsync(partitionsToValidate);\n     }\n \n+    /**\n+     * For each partition which needs validation, make an asynchronous request to get the end-offsets for the partition\n+     * with the epoch less than or equal to the epoch the partition last saw.\n+     *\n+     * Requests are grouped by Node for efficiency.\n+     */\n+    private void validateOffsetsAsync(Map<TopicPartition, SubscriptionState.FetchPosition> partitionsToValidate) {\n+        final Map<Node, Map<TopicPartition, SubscriptionState.FetchPosition>> regrouped =\n+            regroupFetchPositionsByLeader(partitionsToValidate);\n+\n+        regrouped.forEach((node, fetchPositions) -> {\n+            if (node.isEmpty()) {\n+                metadata.requestUpdate();\n+                return;\n+            }\n+\n+            NodeApiVersions nodeApiVersions = apiVersions.get(node.idString());\n+            if (nodeApiVersions == null) {\n+                client.tryConnect(node);\n+                return;\n+            }\n+\n+            if (!hasUsableOffsetForLeaderEpochVersion(nodeApiVersions)) {\n+                log.debug(\"Skipping validation of fetch offsets for partitions {} since the broker does not \" +\n+                              \"support the required protocol version (introduced in Kafka 2.3)\",\n+                    fetchPositions.keySet());\n+                completeAllValidations(fetchPositions);\n+                return;\n+            }\n+\n+            // We need to get the client epoch state before sending out the leader epoch request, and use it to\n+            // decide whether we need to validate offsets.\n+            if (!metadata.hasReliableLeaderEpochs()) {\n+                log.debug(\"Skipping validation of fetch offsets for partitions {} since the provided leader broker \" +\n+                              \"is not reliable\", fetchPositions.keySet());\n+                completeAllValidations(fetchPositions);\n+                return;\n+            }\n+\n+            subscriptions.setNextAllowedRetry(fetchPositions.keySet(), time.milliseconds() + requestTimeoutMs);\n+\n+            RequestFuture<OffsetsForLeaderEpochClient.OffsetForEpochResult> future =\n+                offsetsForLeaderEpochClient.sendAsyncRequest(node, fetchPositions);\n+\n+            future.addListener(new RequestFutureListener<OffsetsForLeaderEpochClient.OffsetForEpochResult>() {\n+                @Override\n+                public void onSuccess(OffsetsForLeaderEpochClient.OffsetForEpochResult offsetsResult) {\n+                    Map<TopicPartition, OffsetAndMetadata> truncationWithoutResetPolicy = new HashMap<>();\n+                    if (!offsetsResult.partitionsToRetry().isEmpty()) {\n+                        subscriptions.setNextAllowedRetry(offsetsResult.partitionsToRetry(), time.milliseconds() + retryBackoffMs);\n+                        metadata.requestUpdate();\n+                    }\n+\n+                    // For each OffsetsForLeader response, check if the end-offset is lower than our current offset\n+                    // for the partition. If so, it means we have experienced log truncation and need to reposition\n+                    // that partition's offset.\n+                    //\n+                    // In addition, check whether the returned offset and epoch are valid. If not, then we should treat\n+                    // it as out of range and update metadata for rediscovery.\n+                    offsetsResult.endOffsets().forEach((respTopicPartition, respEndOffset) -> {\n+                        if (respEndOffset.hasUndefinedEpochOrOffset()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3ODA4NzMz", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-397808733", "createdAt": "2020-04-22T02:58:16Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwMjo1ODoxNlrOGJhEzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwMjo1ODoxNlrOGJhEzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjYzMjI2OQ==", "bodyText": "Side cleanup", "url": "https://github.com/apache/kafka/pull/8486#discussion_r412632269", "createdAt": "2020-04-22T02:58:16Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetsForLeaderEpochClient.java", "diffHunk": "@@ -85,10 +85,6 @@ protected OffsetForEpochResult handleResponse(\n                 case KAFKA_STORAGE_ERROR:\n                 case OFFSET_NOT_AVAILABLE:\n                 case LEADER_NOT_AVAILABLE:\n-                    logger().debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3ODA4OTM3", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-397808937", "createdAt": "2020-04-22T02:58:58Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwMjo1ODo1OFrOGJhFrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwMjo1ODo1OFrOGJhFrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjYzMjQ5Mw==", "bodyText": "Change the function signature to be able to pass in response version.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r412632493", "createdAt": "2020-04-22T02:58:58Z", "author": {"login": "abbccdda"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/MetadataResponse.java", "diffHunk": "@@ -464,22 +466,41 @@ public static MetadataResponse prepareResponse(int throttleTimeMs, Collection<No\n             }\n             responseData.topics().add(metadataResponseTopic);\n         });\n-        return new MetadataResponse(responseData);\n+        return new MetadataResponse(responseData.toStruct(responseVersion), responseVersion);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3ODA5NzI4", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-397809728", "createdAt": "2020-04-22T03:01:36Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwMzowMTozNlrOGJhJBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwMzowMTozNlrOGJhJBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjYzMzM0OQ==", "bodyText": "Added test coverage L203 and L214, other changes are just side cleanups and signature refactoring in this file.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r412633349", "createdAt": "2020-04-22T03:01:36Z", "author": {"login": "abbccdda"}, "path": "clients/src/test/java/org/apache/kafka/clients/MetadataTest.java", "diffHunk": "@@ -199,19 +200,21 @@ public void testIgnoreLeaderEpochInOlderMetadataResponse() {\n             MetadataResponse response = new MetadataResponse(struct, version);\n             assertFalse(response.hasReliableLeaderEpochs());\n             metadata.updateWithCurrentRequestVersion(response, false, 100);\n+            assertFalse(metadata.hasReliableLeaderEpochs());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3ODA5ODQw", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-397809840", "createdAt": "2020-04-22T03:01:59Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwMzowMTo1OVrOGJhJkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMlQwMzowMTo1OVrOGJhJkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjYzMzQ4OQ==", "bodyText": "Only side cleanups", "url": "https://github.com/apache/kafka/pull/8486#discussion_r412633489", "createdAt": "2020-04-22T03:01:59Z", "author": {"login": "abbccdda"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/SubscriptionStateTest.java", "diffHunk": "@@ -547,17 +547,15 @@ public void testMaybeCompleteValidationAfterOffsetReset() {\n         int initialOffsetEpoch = 5;\n \n         SubscriptionState.FetchPosition initialPosition = new SubscriptionState.FetchPosition(initialOffset,\n-                Optional.of(initialOffsetEpoch), new Metadata.LeaderAndEpoch(Optional.of(broker1), Optional.of(currentEpoch)));\n+            Optional.of(initialOffsetEpoch), new Metadata.LeaderAndEpoch(Optional.of(broker1), Optional.of(currentEpoch)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "84e5e904688932948c2db73e05c8236c29113330", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/84e5e904688932948c2db73e05c8236c29113330", "committedDate": "2020-04-22T03:19:10Z", "message": "Offset undefined test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk4NjczOTE5", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-398673919", "createdAt": "2020-04-23T00:07:38Z", "commit": {"oid": "84e5e904688932948c2db73e05c8236c29113330"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QwMDowNzozOFrOGKQ51Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QwMDoxMjo0NVrOGKRBFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQxNTg5Mw==", "bodyText": "Could you not move the function while changing it so that the diff is easier to check -- e.g. here I'd have to blindly trust you that there are only changes of this func :)", "url": "https://github.com/apache/kafka/pull/8486#discussion_r413415893", "createdAt": "2020-04-23T00:07:38Z", "author": {"login": "guozhangwang"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -498,6 +498,104 @@ public void validateOffsetsIfNeeded() {\n         validateOffsetsAsync(partitionsToValidate);\n     }\n \n+    /**\n+     * For each partition which needs validation, make an asynchronous request to get the end-offsets for the partition\n+     * with the epoch less than or equal to the epoch the partition last saw.\n+     *\n+     * Requests are grouped by Node for efficiency.\n+     */\n+    private void validateOffsetsAsync(Map<TopicPartition, SubscriptionState.FetchPosition> partitionsToValidate) {\n+        final Map<Node, Map<TopicPartition, SubscriptionState.FetchPosition>> regrouped =\n+            regroupFetchPositionsByLeader(partitionsToValidate);\n+\n+        regrouped.forEach((node, fetchPositions) -> {\n+            if (node.isEmpty()) {\n+                metadata.requestUpdate();\n+                return;\n+            }\n+\n+            NodeApiVersions nodeApiVersions = apiVersions.get(node.idString());\n+            if (nodeApiVersions == null) {\n+                client.tryConnect(node);\n+                return;\n+            }\n+\n+            if (!hasUsableOffsetForLeaderEpochVersion(nodeApiVersions)) {\n+                log.debug(\"Skipping validation of fetch offsets for partitions {} since the broker does not \" +\n+                              \"support the required protocol version (introduced in Kafka 2.3)\",\n+                    fetchPositions.keySet());\n+                completeAllValidations(fetchPositions);\n+                return;\n+            }\n+\n+            // We need to get the client epoch state before sending out the leader epoch request, and use it to\n+            // decide whether we need to validate offsets.\n+            if (!metadata.hasReliableLeaderEpochs()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjYzMTg3OQ=="}, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQxNzAyMQ==", "bodyText": "nit: ... or offset {} from OffsetsForLeaderEpoch response", "url": "https://github.com/apache/kafka/pull/8486#discussion_r413417021", "createdAt": "2020-04-23T00:10:48Z", "author": {"login": "guozhangwang"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -498,6 +498,104 @@ public void validateOffsetsIfNeeded() {\n         validateOffsetsAsync(partitionsToValidate);\n     }\n \n+    /**\n+     * For each partition which needs validation, make an asynchronous request to get the end-offsets for the partition\n+     * with the epoch less than or equal to the epoch the partition last saw.\n+     *\n+     * Requests are grouped by Node for efficiency.\n+     */\n+    private void validateOffsetsAsync(Map<TopicPartition, SubscriptionState.FetchPosition> partitionsToValidate) {\n+        final Map<Node, Map<TopicPartition, SubscriptionState.FetchPosition>> regrouped =\n+            regroupFetchPositionsByLeader(partitionsToValidate);\n+\n+        regrouped.forEach((node, fetchPositions) -> {\n+            if (node.isEmpty()) {\n+                metadata.requestUpdate();\n+                return;\n+            }\n+\n+            NodeApiVersions nodeApiVersions = apiVersions.get(node.idString());\n+            if (nodeApiVersions == null) {\n+                client.tryConnect(node);\n+                return;\n+            }\n+\n+            if (!hasUsableOffsetForLeaderEpochVersion(nodeApiVersions)) {\n+                log.debug(\"Skipping validation of fetch offsets for partitions {} since the broker does not \" +\n+                              \"support the required protocol version (introduced in Kafka 2.3)\",\n+                    fetchPositions.keySet());\n+                completeAllValidations(fetchPositions);\n+                return;\n+            }\n+\n+            // We need to get the client epoch state before sending out the leader epoch request, and use it to\n+            // decide whether we need to validate offsets.\n+            if (!metadata.hasReliableLeaderEpochs()) {\n+                log.debug(\"Skipping validation of fetch offsets for partitions {} since the provided leader broker \" +\n+                              \"is not reliable\", fetchPositions.keySet());\n+                completeAllValidations(fetchPositions);\n+                return;\n+            }\n+\n+            subscriptions.setNextAllowedRetry(fetchPositions.keySet(), time.milliseconds() + requestTimeoutMs);\n+\n+            RequestFuture<OffsetsForLeaderEpochClient.OffsetForEpochResult> future =\n+                offsetsForLeaderEpochClient.sendAsyncRequest(node, fetchPositions);\n+\n+            future.addListener(new RequestFutureListener<OffsetsForLeaderEpochClient.OffsetForEpochResult>() {\n+                @Override\n+                public void onSuccess(OffsetsForLeaderEpochClient.OffsetForEpochResult offsetsResult) {\n+                    Map<TopicPartition, OffsetAndMetadata> truncationWithoutResetPolicy = new HashMap<>();\n+                    if (!offsetsResult.partitionsToRetry().isEmpty()) {\n+                        subscriptions.setNextAllowedRetry(offsetsResult.partitionsToRetry(), time.milliseconds() + retryBackoffMs);\n+                        metadata.requestUpdate();\n+                    }\n+\n+                    // For each OffsetsForLeader response, check if the end-offset is lower than our current offset\n+                    // for the partition. If so, it means we have experienced log truncation and need to reposition\n+                    // that partition's offset.\n+                    //\n+                    // In addition, check whether the returned offset and epoch are valid. If not, then we should treat\n+                    // it as out of range and update metadata for rediscovery.\n+                    offsetsResult.endOffsets().forEach((respTopicPartition, respEndOffset) -> {\n+                        if (respEndOffset.hasUndefinedEpochOrOffset()) {\n+                            // Should attempt to find the new leader in the next try.\n+                            log.debug(\"Requesting metadata update for partition {} due to undefined epoch or offset {}\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84e5e904688932948c2db73e05c8236c29113330"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQxNzI4MQ==", "bodyText": "Why we can remove this logic?", "url": "https://github.com/apache/kafka/pull/8486#discussion_r413417281", "createdAt": "2020-04-23T00:11:26Z", "author": {"login": "guozhangwang"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetsForLeaderEpochClient.java", "diffHunk": "@@ -85,10 +85,6 @@ protected OffsetForEpochResult handleResponse(\n                 case KAFKA_STORAGE_ERROR:\n                 case OFFSET_NOT_AVAILABLE:\n                 case LEADER_NOT_AVAILABLE:\n-                    logger().debug(\"Attempt to fetch offsets for partition {} failed due to {}, retrying.\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjYzMjI2OQ=="}, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQxNzc1MQ==", "bodyText": "For my own understanding: if endOffset is UNDEFINED the epoch should always be UNDEFINED too? If that's the case we can just rely on leaderEpoch alone?", "url": "https://github.com/apache/kafka/pull/8486#discussion_r413417751", "createdAt": "2020-04-23T00:12:45Z", "author": {"login": "guozhangwang"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/EpochEndOffset.java", "diffHunk": "@@ -86,4 +84,9 @@ public boolean equals(Object o) {\n     public int hashCode() {\n         return Objects.hash(error, leaderEpoch, endOffset);\n     }\n+\n+    public boolean hasUndefinedEpochOrOffset() {\n+        return this.endOffset == UNDEFINED_EPOCH_OFFSET ||", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84e5e904688932948c2db73e05c8236c29113330"}, "originalPosition": 17}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "cf7338846a00a5ae81a2899f2ccbd1de01b148c5", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/cf7338846a00a5ae81a2899f2ccbd1de01b148c5", "committedDate": "2020-04-23T16:46:19Z", "message": "address comment"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk5MzMyOTcz", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-399332973", "createdAt": "2020-04-23T17:43:40Z", "commit": {"oid": "cf7338846a00a5ae81a2899f2ccbd1de01b148c5"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cf7338846a00a5ae81a2899f2ccbd1de01b148c5", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/cf7338846a00a5ae81a2899f2ccbd1de01b148c5", "committedDate": "2020-04-23T16:46:19Z", "message": "address comment"}, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIwNTkwOTMx", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-420590931", "createdAt": "2020-05-29T01:09:45Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "fddb49e3ae5e4a906ca46f40778ae914ddc760e4", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/fddb49e3ae5e4a906ca46f40778ae914ddc760e4", "committedDate": "2020-06-02T04:58:22Z", "message": "avoid offset validation when the leader epoch is not reliable, and check epoch/offset in response"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "fddb49e3ae5e4a906ca46f40778ae914ddc760e4", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/fddb49e3ae5e4a906ca46f40778ae914ddc760e4", "committedDate": "2020-06-02T04:58:22Z", "message": "avoid offset validation when the leader epoch is not reliable, and check epoch/offset in response"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMDU0MTU3", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-423054157", "createdAt": "2020-06-02T21:00:45Z", "commit": {"oid": "fddb49e3ae5e4a906ca46f40778ae914ddc760e4"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMTowMDo0NlrOGeDzyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQyMjo1MDozOVrOGeGhMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE3Mjg3Mw==", "bodyText": "I think this is basically the same code we have handling out of range errors from fetch responses. Does it makes sense to add a helper?", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434172873", "createdAt": "2020-06-02T21:00:46Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -812,13 +813,25 @@ public void onSuccess(OffsetsForLeaderEpochClient.OffsetForEpochResult offsetsRe\n                     // For each OffsetsForLeader response, check if the end-offset is lower than our current offset\n                     // for the partition. If so, it means we have experienced log truncation and need to reposition\n                     // that partition's offset.\n+                    //\n+                    // In addition, check whether the returned offset and epoch are valid. If not, then we should reset\n+                    // its offset if reset policy is configured, or throw out of range exception.\n                     offsetsResult.endOffsets().forEach((respTopicPartition, respEndOffset) -> {\n-                        SubscriptionState.FetchPosition requestPosition = fetchPostitions.get(respTopicPartition);\n-                        Optional<OffsetAndMetadata> divergentOffsetOpt = subscriptions.maybeCompleteValidation(\n+                        SubscriptionState.FetchPosition requestPosition = fetchPositions.get(respTopicPartition);\n+\n+                        if (respEndOffset.hasUndefinedEpochOrOffset()) {\n+                            if (subscriptions.hasDefaultOffsetResetPolicy()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fddb49e3ae5e4a906ca46f40778ae914ddc760e4"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIwNTg1NA==", "bodyText": "Could we add some detail to this message? Maybe something like \"Leader reported no end offset larger than current fetch epoch\" or something like that.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434205854", "createdAt": "2020-06-02T22:17:25Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -812,13 +813,25 @@ public void onSuccess(OffsetsForLeaderEpochClient.OffsetForEpochResult offsetsRe\n                     // For each OffsetsForLeader response, check if the end-offset is lower than our current offset\n                     // for the partition. If so, it means we have experienced log truncation and need to reposition\n                     // that partition's offset.\n+                    //\n+                    // In addition, check whether the returned offset and epoch are valid. If not, then we should reset\n+                    // its offset if reset policy is configured, or throw out of range exception.\n                     offsetsResult.endOffsets().forEach((respTopicPartition, respEndOffset) -> {\n-                        SubscriptionState.FetchPosition requestPosition = fetchPostitions.get(respTopicPartition);\n-                        Optional<OffsetAndMetadata> divergentOffsetOpt = subscriptions.maybeCompleteValidation(\n+                        SubscriptionState.FetchPosition requestPosition = fetchPositions.get(respTopicPartition);\n+\n+                        if (respEndOffset.hasUndefinedEpochOrOffset()) {\n+                            if (subscriptions.hasDefaultOffsetResetPolicy()) {\n+                                log.info(\"Fetch offset {} is out of range for partition {}, resetting offset\", requestPosition, respTopicPartition);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fddb49e3ae5e4a906ca46f40778ae914ddc760e4"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIwNjQ3Ng==", "bodyText": "Also would be useful if we could include the epoch in the exception message as well as similar change to emphasize that this was raised during epoch validation.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434206476", "createdAt": "2020-06-02T22:19:13Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -812,13 +813,25 @@ public void onSuccess(OffsetsForLeaderEpochClient.OffsetForEpochResult offsetsRe\n                     // For each OffsetsForLeader response, check if the end-offset is lower than our current offset\n                     // for the partition. If so, it means we have experienced log truncation and need to reposition\n                     // that partition's offset.\n+                    //\n+                    // In addition, check whether the returned offset and epoch are valid. If not, then we should reset\n+                    // its offset if reset policy is configured, or throw out of range exception.\n                     offsetsResult.endOffsets().forEach((respTopicPartition, respEndOffset) -> {\n-                        SubscriptionState.FetchPosition requestPosition = fetchPostitions.get(respTopicPartition);\n-                        Optional<OffsetAndMetadata> divergentOffsetOpt = subscriptions.maybeCompleteValidation(\n+                        SubscriptionState.FetchPosition requestPosition = fetchPositions.get(respTopicPartition);\n+\n+                        if (respEndOffset.hasUndefinedEpochOrOffset()) {\n+                            if (subscriptions.hasDefaultOffsetResetPolicy()) {\n+                                log.info(\"Fetch offset {} is out of range for partition {}, resetting offset\", requestPosition, respTopicPartition);\n+                                subscriptions.requestOffsetReset(respTopicPartition);\n+                            } else {\n+                                throw new OffsetOutOfRangeException(Collections.singletonMap(respTopicPartition, requestPosition.offset));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fddb49e3ae5e4a906ca46f40778ae914ddc760e4"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIwODAyMA==", "bodyText": "Older versions did not return the epoch, so it was possible to see an offset defined without an epoch. However, the version that the consumer relies on should always have both or neither. Anyway, I think it is reasonable to be a little stricter here.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434208020", "createdAt": "2020-06-02T22:23:36Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/common/requests/EpochEndOffset.java", "diffHunk": "@@ -86,4 +84,9 @@ public boolean equals(Object o) {\n     public int hashCode() {\n         return Objects.hash(error, leaderEpoch, endOffset);\n     }\n+\n+    public boolean hasUndefinedEpochOrOffset() {\n+        return this.endOffset == UNDEFINED_EPOCH_OFFSET ||", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzQxNzc1MQ=="}, "originalCommit": {"oid": "84e5e904688932948c2db73e05c8236c29113330"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIwODk0OA==", "bodyText": "nit: while we're at it, drop the \"to\"?", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434208948", "createdAt": "2020-06-02T22:26:10Z", "author": {"login": "hachikuji"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinatorTest.java", "diffHunk": "@@ -611,7 +611,7 @@ public void testHeartbeatIllegalGenerationResponseWithOldGeneration() throws Int\n \n         final AbstractCoordinator.Generation currGen = coordinator.generation();\n \n-        // let the heartbeat request to send out a request\n+        // let the heartbeat thread to send out a request", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fddb49e3ae5e4a906ca46f40778ae914ddc760e4"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIwOTc4Mw==", "bodyText": "nit: a matter of taste I guess, but I find the tests are easier to follow when the assertions are inline.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434209783", "createdAt": "2020-06-02T22:28:24Z", "author": {"login": "hachikuji"}, "path": "clients/src/test/java/org/apache/kafka/common/requests/EpochEndOffsetTest.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.requests;\n+\n+import org.apache.kafka.common.protocol.Errors;\n+import org.junit.Test;\n+\n+import static org.apache.kafka.common.requests.EpochEndOffset.UNDEFINED_EPOCH;\n+import static org.apache.kafka.common.requests.EpochEndOffset.UNDEFINED_EPOCH_OFFSET;\n+import static org.junit.Assert.assertEquals;\n+\n+public class EpochEndOffsetTest {\n+\n+    @Test\n+    public void testConstructor() {\n+        int leaderEpoch = 5;\n+        long endOffset = 10L;\n+        EpochEndOffset epochEndOffset = new EpochEndOffset(Errors.FENCED_LEADER_EPOCH, leaderEpoch, endOffset);\n+\n+        verify(leaderEpoch, endOffset, true, Errors.FENCED_LEADER_EPOCH, false, epochEndOffset);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fddb49e3ae5e4a906ca46f40778ae914ddc760e4"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIxNTcwMg==", "bodyText": "Do we need to change some of these other tests? We're still using initialUpdateResponse above in assignFromUser and below.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434215702", "createdAt": "2020-06-02T22:45:48Z", "author": {"login": "hachikuji"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -337,7 +345,7 @@ public void testFetchSkipsBlackedOutNodes() {\n \n         assignFromUser(singleton(tp0));\n         subscriptions.seek(tp0, 0);\n-        client.updateMetadata(initialUpdateResponse);\n+        client.updateMetadata(initialUpdateResponseWithLeaderEpoch);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fddb49e3ae5e4a906ca46f40778ae914ddc760e4"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIxNjM2MQ==", "bodyText": "nit: if one of these is an uncommon case, maybe we can make it local to the test cases that need it.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434216361", "createdAt": "2020-06-02T22:47:53Z", "author": {"login": "hachikuji"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -149,7 +148,11 @@\n     private TopicPartition tp1 = new TopicPartition(topicName, 1);\n     private TopicPartition tp2 = new TopicPartition(topicName, 2);\n     private TopicPartition tp3 = new TopicPartition(topicName, 3);\n-    private MetadataResponse initialUpdateResponse = TestUtils.metadataUpdateWith(1, singletonMap(topicName, 4));\n+    private int validLeaderEpoch = 0;\n+    private MetadataResponse initialUpdateResponse =\n+        TestUtils.metadataUpdateWith(1, singletonMap(topicName, 4));\n+    private MetadataResponse initialUpdateResponseWithLeaderEpoch =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fddb49e3ae5e4a906ca46f40778ae914ddc760e4"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIxNzI2Ng==", "bodyText": "Is it useful also to check the case when no reset policy is defined?", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434217266", "createdAt": "2020-06-02T22:50:39Z", "author": {"login": "hachikuji"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -3751,6 +3755,85 @@ public void testOffsetValidationSkippedForOldBroker() {\n         }\n     }\n \n+    @Test\n+    public void testOffsetValidationSkippedForOldResponse() {\n+        // Old responses may provide unreliable leader epoch,\n+        // so we should skip offset validation and not send the request.\n+        buildFetcher();\n+        assignFromUser(singleton(tp0));\n+\n+        Map<String, Integer> partitionCounts = new HashMap<>();\n+        partitionCounts.put(tp0.topic(), 4);\n+\n+        final int epochOne = 1;\n+\n+        metadata.updateWithCurrentRequestVersion(TestUtils.metadataUpdateWith(\"dummy\", 1,\n+            Collections.emptyMap(), partitionCounts, tp -> epochOne), false, 0L);\n+\n+        Node node = metadata.fetch().nodes().get(0);\n+        assertFalse(client.isConnected(node.idString()));\n+\n+        // Seek with a position and leader+epoch\n+        Metadata.LeaderAndEpoch leaderAndEpoch = new Metadata.LeaderAndEpoch(\n+            metadata.currentLeader(tp0).leader, Optional.of(epochOne));\n+        subscriptions.seekUnvalidated(tp0, new SubscriptionState.FetchPosition(20L, Optional.of(epochOne), leaderAndEpoch));\n+        assertFalse(client.isConnected(node.idString()));\n+        assertTrue(subscriptions.awaitingValidation(tp0));\n+\n+        // Inject an older version of the metadata response\n+        final short responseVersion = 8;\n+        metadata.updateWithCurrentRequestVersion(TestUtils.metadataUpdateWith(\"dummy\", 1,\n+            Collections.emptyMap(), partitionCounts, responseVersion), false, 0L);\n+        fetcher.validateOffsetsIfNeeded();\n+        // Offset validation is skipped\n+        assertFalse(subscriptions.awaitingValidation(tp0));\n+    }\n+\n+    @Test\n+    public void testOffsetValidationResetOffsetForUndefinedEpoch() {\n+        testOffsetValidationWithGivenEpochOffset(new EpochEndOffset(EpochEndOffset.UNDEFINED_EPOCH, 0L));\n+\n+    }\n+    @Test\n+    public void testOffsetValidationResetOffsetForUndefinedOffset() {\n+        testOffsetValidationWithGivenEpochOffset(new EpochEndOffset(2, EpochEndOffset.UNDEFINED_EPOCH_OFFSET));\n+    }\n+\n+    private void testOffsetValidationWithGivenEpochOffset(final EpochEndOffset epochEndOffset) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fddb49e3ae5e4a906ca46f40778ae914ddc760e4"}, "originalPosition": 238}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dbaa133d0cf2749d7b27e236bb4ba7da3c2412ff", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/dbaa133d0cf2749d7b27e236bb4ba7da3c2412ff", "committedDate": "2020-06-03T00:17:48Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMTQzMTAy", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-423143102", "createdAt": "2020-06-03T00:29:53Z", "commit": {"oid": "dbaa133d0cf2749d7b27e236bb4ba7da3c2412ff"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMDoyOTo1M1rOGeIQNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMDoyOTo1M1rOGeIQNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI0NTY4NA==", "bodyText": "Why make this change? We're losing the epoch information that is contained in OffsetAndMetadata. Note that this is a public API, so we cannot remove divergentOffsets without a KIP.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434245684", "createdAt": "2020-06-03T00:29:53Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/LogTruncationException.java", "diffHunk": "@@ -33,18 +32,8 @@\n  */\n public class LogTruncationException extends OffsetOutOfRangeException {\n \n-    private final Map<TopicPartition, OffsetAndMetadata> divergentOffsets;\n-\n     public LogTruncationException(Map<TopicPartition, OffsetAndMetadata> divergentOffsets) {\n-        super(Utils.transformMap(divergentOffsets, Function.identity(), OffsetAndMetadata::offset));\n-        this.divergentOffsets = Collections.unmodifiableMap(divergentOffsets);\n-    }\n-\n-    /**\n-     * Get the offsets for the partitions which were truncated. This is the first offset which is known to diverge\n-     * from what the consumer read.\n-     */\n-    public Map<TopicPartition, OffsetAndMetadata> divergentOffsets() {\n-        return divergentOffsets;\n+        super(Utils.transformMap(divergentOffsets, Function.identity(), OffsetAndMetadata::offset),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dbaa133d0cf2749d7b27e236bb4ba7da3c2412ff"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMTQzOTM0", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-423143934", "createdAt": "2020-06-03T00:32:20Z", "commit": {"oid": "dbaa133d0cf2749d7b27e236bb4ba7da3c2412ff"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMDozMjoyMVrOGeITAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMDozMjoyMVrOGeITAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI0NjQwMg==", "bodyText": "We should probably make this an overload since this is a public API", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434246402", "createdAt": "2020-06-03T00:32:21Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/OffsetOutOfRangeException.java", "diffHunk": "@@ -30,8 +30,9 @@\n     private static final long serialVersionUID = 1L;\n     private final Map<TopicPartition, Long> offsetOutOfRangePartitions;\n \n-    public OffsetOutOfRangeException(Map<TopicPartition, Long> offsetOutOfRangePartitions) {\n-        super(\"Offsets out of range with no configured reset policy for partitions: \" + offsetOutOfRangePartitions);\n+    public OffsetOutOfRangeException(Map<TopicPartition, Long> offsetOutOfRangePartitions, String reason) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dbaa133d0cf2749d7b27e236bb4ba7da3c2412ff"}, "originalPosition": 6}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "94e09111f7d6be3739b33274ba60443ea40cfc30", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/94e09111f7d6be3739b33274ba60443ea40cfc30", "committedDate": "2020-06-03T01:20:01Z", "message": "address last comment"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzMTY5Nzk3", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-423169797", "createdAt": "2020-06-03T02:02:03Z", "commit": {"oid": "94e09111f7d6be3739b33274ba60443ea40cfc30"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMjowMjowNFrOGeJnEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QwMjoyOTo0M1rOGeJ_bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI2NzkyMg==", "bodyText": "I'd suggest a minor change to this. One of the issues here is that we don't get the divergent offsets in the message itself. I know that might not seem like a big deal, but sometimes the exception trace is all we get from the user. It's also surprising when an exception constructor takes a string parameter which is not the message itself. Can we do something like this instead?\nclass LogTruncationException {\n  public LogTruncationException(Map<TopicPartition, OffsetAndMetadata> divergentOffsets) {\n    super(Utils.transformMap(divergentOffsets, Function.identity(), OffsetAndMetadata::offset),\n      \"Detected log truncation with diverging offsets \" + divergentOffsets);\n    this.divergentOffsets = Collections.unmodifiableMap(divergentOffsets);\n  }\n}\n\nclass OffsetOutOfRangeException {\n  public OffsetOutOfRangeException(Map<TopicPartition, Long> offsetOutOfRangePartitions) {\n    this(offsetOutOfRangePartitions, \"Offsets out of range with no configured reset policy for partitions: \" +\n            offsetOutOfRangePartitions);\n  }\n\n  public OffsetOutOfRangeException(String message, Map<TopicPartition, Long> offsetOutOfRangePartitions) {\n    super(message);\n    this.offsetOutOfRangePartitions = offsetOutOfRangePartitions;\n  }\n}\n\nclass Fetcher {\n  private handleOffsetOutOfRange(..., String reason) {\n    ...\n    return new OffsetOutOfRangeException(offsetOutOfRangePartitions, \"Offsets out of range with no configured reset policy for partitions: \" +\n            offsetOutOfRangePartitions + \", root cause: \" + reason);\n  }\n}", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434267922", "createdAt": "2020-06-03T02:02:04Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/LogTruncationException.java", "diffHunk": "@@ -36,7 +36,8 @@\n     private final Map<TopicPartition, OffsetAndMetadata> divergentOffsets;\n \n     public LogTruncationException(Map<TopicPartition, OffsetAndMetadata> divergentOffsets) {\n-        super(Utils.transformMap(divergentOffsets, Function.identity(), OffsetAndMetadata::offset));\n+        super(Utils.transformMap(divergentOffsets, Function.identity(), OffsetAndMetadata::offset),\n+            \"detected log truncation\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94e09111f7d6be3739b33274ba60443ea40cfc30"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI2OTAwOA==", "bodyText": "Another improvement we can make here is to add the fetch offset to the exception message. Also, I'm considering if we should log this event even if we throw the exception back to the user. Otherwise, the user application might swallow it and we won't know it happened.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434269008", "createdAt": "2020-06-03T02:06:54Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -1304,6 +1310,19 @@ private CompletedFetch initializeCompletedFetch(CompletedFetch nextCompletedFetc\n         return completedFetch;\n     }\n \n+    private void handleOffsetOutOfRange(long fetchOffset,\n+                                        TopicPartition topicPartition,\n+                                        String reason) {\n+        if (subscriptions.hasDefaultOffsetResetPolicy()) {\n+            log.info(\"Fetch offset {} is out of range for partition {}, resetting offset\",\n+                topicPartition, fetchOffset);\n+            subscriptions.requestOffsetReset(topicPartition);\n+        } else {\n+            throw new OffsetOutOfRangeException(Collections.singletonMap(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94e09111f7d6be3739b33274ba60443ea40cfc30"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3MTU1Nw==", "bodyText": "I think we can let this take FetchPosition instead of just the offset. Inside initializeCompletedFetch, we can pull the position from this line:\n                    if (fetchOffset != subscriptions.position(tp).offset) {", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434271557", "createdAt": "2020-06-03T02:17:47Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -1304,6 +1310,19 @@ private CompletedFetch initializeCompletedFetch(CompletedFetch nextCompletedFetc\n         return completedFetch;\n     }\n \n+    private void handleOffsetOutOfRange(long fetchOffset,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94e09111f7d6be3739b33274ba60443ea40cfc30"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3MjEzNw==", "bodyText": "As below, should we log an event here to make sure we will have it in the logs even if the user discards it?", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434272137", "createdAt": "2020-06-03T02:20:11Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -812,13 +813,22 @@ public void onSuccess(OffsetsForLeaderEpochClient.OffsetForEpochResult offsetsRe\n                     // For each OffsetsForLeader response, check if the end-offset is lower than our current offset\n                     // for the partition. If so, it means we have experienced log truncation and need to reposition\n                     // that partition's offset.\n+                    //\n+                    // In addition, check whether the returned offset and epoch are valid. If not, then we should reset\n+                    // its offset if reset policy is configured, or throw out of range exception.\n                     offsetsResult.endOffsets().forEach((respTopicPartition, respEndOffset) -> {\n-                        SubscriptionState.FetchPosition requestPosition = fetchPostitions.get(respTopicPartition);\n-                        Optional<OffsetAndMetadata> divergentOffsetOpt = subscriptions.maybeCompleteValidation(\n+                        SubscriptionState.FetchPosition requestPosition = fetchPositions.get(respTopicPartition);\n+\n+                        if (respEndOffset.hasUndefinedEpochOrOffset()) {\n+                            handleOffsetOutOfRange(requestPosition.offset, respTopicPartition,\n+                                \"Failed leader offset epoch validation for \" + respEndOffset\n+                                + \" since no end offset larger than current fetch epoch was reported\");\n+                        } else {\n+                            Optional<OffsetAndMetadata> divergentOffsetOpt = subscriptions.maybeCompleteValidation(\n                                 respTopicPartition, requestPosition, respEndOffset);\n-                        divergentOffsetOpt.ifPresent(divergentOffset -> {\n-                            truncationWithoutResetPolicy.put(respTopicPartition, divergentOffset);\n-                        });\n+                            divergentOffsetOpt.ifPresent(\n+                                divergentOffset -> truncationWithoutResetPolicy.put(respTopicPartition, divergentOffset));\n+                        }\n                     });\n \n                     if (!truncationWithoutResetPolicy.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94e09111f7d6be3739b33274ba60443ea40cfc30"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3NDE1OA==", "bodyText": "Hmm.. Can we assert the raised exception somehow? It's not clear to me that it is getting raised appropriately and we don't have any tests for it.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434274158", "createdAt": "2020-06-03T02:29:43Z", "author": {"login": "hachikuji"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "diffHunk": "@@ -3831,7 +3845,12 @@ private void testOffsetValidationWithGivenEpochOffset(final EpochEndOffset epoch\n         consumerClient.poll(time.timer(Duration.ZERO));\n \n         assertEquals(0, subscriptions.position(tp0).offset);\n-        assertFalse(subscriptions.awaitingValidation(tp0));\n+\n+        if (offsetResetStrategy == OffsetResetStrategy.NONE) {\n+            assertTrue(subscriptions.awaitingValidation(tp0));\n+        } else {\n+            assertFalse(subscriptions.awaitingValidation(tp0));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dbaa133d0cf2749d7b27e236bb4ba7da3c2412ff"}, "originalPosition": 85}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03407c45c110c0700897fd80e41dc0bf99d062d0", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/03407c45c110c0700897fd80e41dc0bf99d062d0", "committedDate": "2020-06-03T03:32:44Z", "message": "fix further comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "03407c45c110c0700897fd80e41dc0bf99d062d0", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/03407c45c110c0700897fd80e41dc0bf99d062d0", "committedDate": "2020-06-03T03:32:44Z", "message": "fix further comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzNzE0Nzg3", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-423714787", "createdAt": "2020-06-03T16:10:40Z", "commit": {"oid": "03407c45c110c0700897fd80e41dc0bf99d062d0"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNjoxMDo0MVrOGejNcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNjoyMDoyNFrOGejmnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY4NzM0Nw==", "bodyText": "Could we make it info in both cases? If the application has not set a reset policy, then they are expecting to handle the exception.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434687347", "createdAt": "2020-06-03T16:10:41Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -1310,16 +1313,23 @@ private CompletedFetch initializeCompletedFetch(CompletedFetch nextCompletedFetc\n         return completedFetch;\n     }\n \n-    private void handleOffsetOutOfRange(long fetchOffset,\n+    private void handleOffsetOutOfRange(FetchPosition fetchPosition,\n                                         TopicPartition topicPartition,\n                                         String reason) {\n         if (subscriptions.hasDefaultOffsetResetPolicy()) {\n-            log.info(\"Fetch offset {} is out of range for partition {}, resetting offset\",\n-                topicPartition, fetchOffset);\n+            log.info(\"Fetch offset epoch {} is out of range for partition {}, resetting offset\",\n+                fetchPosition, topicPartition);\n             subscriptions.requestOffsetReset(topicPartition);\n         } else {\n-            throw new OffsetOutOfRangeException(Collections.singletonMap(\n-                topicPartition, fetchOffset), reason);\n+            Map<TopicPartition, Long> offsetOutOfRangePartitions =\n+                Collections.singletonMap(topicPartition, fetchPosition.offset);\n+            String errorMessage = String.format(\"Offsets out of range \" +\n+                \"with no configured reset policy for partitions: %s\" +\n+                \", for fetch offset: %d, \" +\n+                \"root cause: %s\",\n+                offsetOutOfRangePartitions, fetchPosition.offset, reason);\n+            log.error(errorMessage);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "03407c45c110c0700897fd80e41dc0bf99d062d0"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY4OTkyNQ==", "bodyText": "Didn't notice this before, but this is handled inside a loop. If one partition hits an error, then the raised exception will prevent us from completing the validation for other partitions.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434689925", "createdAt": "2020-06-03T16:14:37Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -817,10 +819,10 @@ public void onSuccess(OffsetsForLeaderEpochClient.OffsetForEpochResult offsetsRe\n                     // In addition, check whether the returned offset and epoch are valid. If not, then we should reset\n                     // its offset if reset policy is configured, or throw out of range exception.\n                     offsetsResult.endOffsets().forEach((respTopicPartition, respEndOffset) -> {\n-                        SubscriptionState.FetchPosition requestPosition = fetchPositions.get(respTopicPartition);\n+                        FetchPosition requestPosition = fetchPositions.get(respTopicPartition);\n \n                         if (respEndOffset.hasUndefinedEpochOrOffset()) {\n-                            handleOffsetOutOfRange(requestPosition.offset, respTopicPartition,\n+                            handleOffsetOutOfRange(requestPosition, respTopicPartition,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "03407c45c110c0700897fd80e41dc0bf99d062d0"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY5MDU1Nw==", "bodyText": "Seems like it's more useful to reference requestPosition here since the end offset is undefined.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434690557", "createdAt": "2020-06-03T16:15:29Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -817,10 +819,10 @@ public void onSuccess(OffsetsForLeaderEpochClient.OffsetForEpochResult offsetsRe\n                     // In addition, check whether the returned offset and epoch are valid. If not, then we should reset\n                     // its offset if reset policy is configured, or throw out of range exception.\n                     offsetsResult.endOffsets().forEach((respTopicPartition, respEndOffset) -> {\n-                        SubscriptionState.FetchPosition requestPosition = fetchPositions.get(respTopicPartition);\n+                        FetchPosition requestPosition = fetchPositions.get(respTopicPartition);\n \n                         if (respEndOffset.hasUndefinedEpochOrOffset()) {\n-                            handleOffsetOutOfRange(requestPosition.offset, respTopicPartition,\n+                            handleOffsetOutOfRange(requestPosition, respTopicPartition,\n                                 \"Failed leader offset epoch validation for \" + respEndOffset", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "03407c45c110c0700897fd80e41dc0bf99d062d0"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY5Mzc4OQ==", "bodyText": "As in the other case, can we make this info? It would also be helpful if we could include the request offset and epoch. Potentially we could move this log line into the loop above where we already access requestPosition.\nAlso nit: use {}", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434693789", "createdAt": "2020-06-03T16:20:24Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -832,6 +834,7 @@ public void onSuccess(OffsetsForLeaderEpochClient.OffsetForEpochResult offsetsRe\n                     });\n \n                     if (!truncationWithoutResetPolicy.isEmpty()) {\n+                        log.error(\"Detected log truncation with diverging offsets \" + truncationWithoutResetPolicy);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "03407c45c110c0700897fd80e41dc0bf99d062d0"}, "originalPosition": 98}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a6c0785bcabc90fb3fea227a7e59befeaecd9582", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/a6c0785bcabc90fb3fea227a7e59befeaecd9582", "committedDate": "2020-06-03T16:36:41Z", "message": "address logging comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzNzUwNzE4", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-423750718", "createdAt": "2020-06-03T16:54:22Z", "commit": {"oid": "a6c0785bcabc90fb3fea227a7e59befeaecd9582"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNjo1NDoyMlrOGek8rA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzoxMDowMlrOGelg5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcxNTgyMA==", "bodyText": "Can you add the topic partition to this message?", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434715820", "createdAt": "2020-06-03T16:54:22Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -822,19 +822,25 @@ public void onSuccess(OffsetForEpochResult offsetsResult) {\n                         FetchPosition requestPosition = fetchPositions.get(respTopicPartition);\n \n                         if (respEndOffset.hasUndefinedEpochOrOffset()) {\n-                            handleOffsetOutOfRange(requestPosition, respTopicPartition,\n-                                \"Failed leader offset epoch validation for \" + respEndOffset\n-                                + \" since no end offset larger than current fetch epoch was reported\");\n+                            try {\n+                                handleOffsetOutOfRange(requestPosition, respTopicPartition,\n+                                    \"Failed leader offset epoch validation for \" + requestPosition\n+                                        + \" since no end offset larger than current fetch epoch was reported\");\n+                            } catch (OffsetOutOfRangeException e) {\n+                                // Swallow the OffsetOutOfRangeException to finish all partitions validation.\n+                            }\n                         } else {\n                             Optional<OffsetAndMetadata> divergentOffsetOpt = subscriptions.maybeCompleteValidation(\n                                 respTopicPartition, requestPosition, respEndOffset);\n                             divergentOffsetOpt.ifPresent(\n-                                divergentOffset -> truncationWithoutResetPolicy.put(respTopicPartition, divergentOffset));\n+                                divergentOffset -> {\n+                                    log.info(\"Detected log truncation with diverging offset: {}\", divergentOffset);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a6c0785bcabc90fb3fea227a7e59befeaecd9582"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcyNTA5Mw==", "bodyText": "I don't feel great about having this in the code, even if it's supposed to be temporary. I think we should just fix the exception propagation bug in this patch. It seems like it would be straightforward to do something similar to what is done in the onFailure path.\n                    if (!(e instanceof RetriableException) && !cachedOffsetForLeaderException.compareAndSet(null, e)) {\n                        log.error(\"Discarding error in OffsetsForLeaderEpoch because another error is pending\", e);\n                    }\nAbove may not be ideal, but at least it provides a way to propagate individual errors.", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434725093", "createdAt": "2020-06-03T17:10:02Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -822,19 +822,25 @@ public void onSuccess(OffsetForEpochResult offsetsResult) {\n                         FetchPosition requestPosition = fetchPositions.get(respTopicPartition);\n \n                         if (respEndOffset.hasUndefinedEpochOrOffset()) {\n-                            handleOffsetOutOfRange(requestPosition, respTopicPartition,\n-                                \"Failed leader offset epoch validation for \" + respEndOffset\n-                                + \" since no end offset larger than current fetch epoch was reported\");\n+                            try {\n+                                handleOffsetOutOfRange(requestPosition, respTopicPartition,\n+                                    \"Failed leader offset epoch validation for \" + requestPosition\n+                                        + \" since no end offset larger than current fetch epoch was reported\");\n+                            } catch (OffsetOutOfRangeException e) {\n+                                // Swallow the OffsetOutOfRangeException to finish all partitions validation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a6c0785bcabc90fb3fea227a7e59befeaecd9582"}, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIzODA5MzE4", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-423809318", "createdAt": "2020-06-03T18:12:21Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxODoxMjoyMlrOGentmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxODoxMjoyMlrOGentmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2MTExNA==", "bodyText": "Can we fix the LogTruncationException case below as well?", "url": "https://github.com/apache/kafka/pull/8486#discussion_r434761114", "createdAt": "2020-06-03T18:12:22Z", "author": {"login": "hachikuji"}, "path": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "diffHunk": "@@ -827,14 +827,15 @@ public void onSuccess(OffsetForEpochResult offsetsResult) {\n                                     \"Failed leader offset epoch validation for \" + requestPosition\n                                         + \" since no end offset larger than current fetch epoch was reported\");\n                             } catch (OffsetOutOfRangeException e) {\n-                                // Swallow the OffsetOutOfRangeException to finish all partitions validation.\n+                                setFatalOffsetForLeaderException(e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "6a66b1b0b0d9e0eb8c3e5b95ac3fd5ce660590b5", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/6a66b1b0b0d9e0eb8c3e5b95ac3fd5ce660590b5", "committedDate": "2020-06-03T20:34:18Z", "message": "propagate exception in onSuccess callback"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "6a66b1b0b0d9e0eb8c3e5b95ac3fd5ce660590b5", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/6a66b1b0b0d9e0eb8c3e5b95ac3fd5ce660590b5", "committedDate": "2020-06-03T20:34:18Z", "message": "propagate exception in onSuccess callback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI0MDM5NjI3", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-424039627", "createdAt": "2020-06-04T01:23:45Z", "commit": {"oid": "6a66b1b0b0d9e0eb8c3e5b95ac3fd5ce660590b5"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a97cccd9876ae5790b40998d6f9ac16b435a91c", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/1a97cccd9876ae5790b40998d6f9ac16b435a91c", "committedDate": "2020-06-04T18:45:25Z", "message": "fix KafkaConsumerTest"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI0NzcxNjI4", "url": "https://github.com/apache/kafka/pull/8486#pullrequestreview-424771628", "createdAt": "2020-06-04T19:41:21Z", "commit": {"oid": "1a97cccd9876ae5790b40998d6f9ac16b435a91c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOTo0MToyMVrOGfVKXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxOTo0MToyMVrOGfVKXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTUwNTc1Ng==", "bodyText": "This is the actual fix, the other parts are mostly cleanup", "url": "https://github.com/apache/kafka/pull/8486#discussion_r435505756", "createdAt": "2020-06-04T19:41:21Z", "author": {"login": "abbccdda"}, "path": "clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java", "diffHunk": "@@ -585,34 +585,29 @@ public void testFetchProgressWithMissingPartitionPosition() {\n         consumer.seekToBeginning(singleton(tp1));\n \n         client.prepareResponse(\n-                new MockClient.RequestMatcher() {\n-                    @Override\n-                    public boolean matches(AbstractRequest body) {\n-                        ListOffsetRequest request = (ListOffsetRequest) body;\n-                        Map<TopicPartition, ListOffsetRequest.PartitionData> timestamps = request.partitionTimestamps();\n-                        return timestamps.get(tp0).timestamp == ListOffsetRequest.LATEST_TIMESTAMP &&\n-                                timestamps.get(tp1).timestamp == ListOffsetRequest.EARLIEST_TIMESTAMP;\n-                    }\n-                }, listOffsetsResponse(Collections.singletonMap(tp0, 50L),\n+            body -> {\n+                ListOffsetRequest request = (ListOffsetRequest) body;\n+                Map<TopicPartition, ListOffsetRequest.PartitionData> timestamps = request.partitionTimestamps();\n+                return timestamps.get(tp0).timestamp == ListOffsetRequest.LATEST_TIMESTAMP &&\n+                        timestamps.get(tp1).timestamp == ListOffsetRequest.EARLIEST_TIMESTAMP;\n+            }, listOffsetsResponse(Collections.singletonMap(tp0, 50L),\n                         Collections.singletonMap(tp1, Errors.NOT_LEADER_FOR_PARTITION)));\n         client.prepareResponse(\n-                new MockClient.RequestMatcher() {\n-                    @Override\n-                    public boolean matches(AbstractRequest body) {\n-                        FetchRequest request = (FetchRequest) body;\n-                        return request.fetchData().keySet().equals(singleton(tp0)) &&\n-                                request.fetchData().get(tp0).fetchOffset == 50L;\n+            body -> {\n+                FetchRequest request = (FetchRequest) body;\n+                return request.fetchData().keySet().equals(singleton(tp0)) &&\n+                        request.fetchData().get(tp0).fetchOffset == 50L;\n \n-                    }\n-                }, fetchResponse(tp0, 50L, 5));\n+            }, fetchResponse(tp0, 50L, 5));\n \n         ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1));\n         assertEquals(5, records.count());\n         assertEquals(singleton(tp0), records.partitions());\n     }\n \n     private void initMetadata(MockClient mockClient, Map<String, Integer> partitionCounts) {\n-        MetadataResponse initialMetadata = TestUtils.metadataUpdateWith(1, partitionCounts);\n+        int leaderEpoch = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1a97cccd9876ae5790b40998d6f9ac16b435a91c"}, "originalPosition": 81}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "87ca234c395d1d0b0566269c4cd299da936d2cb3", "author": {"user": {"login": "abbccdda", "name": "Boyang Chen"}}, "url": "https://github.com/apache/kafka/commit/87ca234c395d1d0b0566269c4cd299da936d2cb3", "committedDate": "2020-06-05T01:53:57Z", "message": "remove unnecessary complete validation logic"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1515, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}