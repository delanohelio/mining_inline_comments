{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMzNDAyOTk3", "number": 8856, "reviewThreads": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNTozNzowN1rOEFGkQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQyMDoxNzowNlrOEGF-OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNzg1OTIwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNTozNzowN1rOGjIiYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzozMjowMFrOGjMNuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MzIxOQ==", "bodyText": "When closing-clean a standby task, we would checkpoint the file and close the state store which would also flush it as well, so I think we do not need to call\ntask.prepareCommit();\ntask.postCommit();\n\nwhich is just to flush the stores and write checkpoint files, right?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439493219", "createdAt": "2020-06-12T15:37:07Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -220,12 +215,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {\n+                        if (task.isActive()) {\n+                            log.error(\"Active task {} was revoked and should have already been committed\", task.id());\n+                            throw new IllegalStateException(\"Revoked active task was not committed during handleRevocation\");\n+                        } else {\n+                            task.prepareCommit();\n+                            task.postCommit();\n+                        }\n                     }\n+                    completeTaskCloseClean(task);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUyMDg3MA==", "bodyText": "Actually we don't checkpoint during closeClean anymore. You always have to commit (if clean && commitNeeded) before closing", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439520870", "createdAt": "2020-06-12T16:25:12Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -220,12 +215,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {\n+                        if (task.isActive()) {\n+                            log.error(\"Active task {} was revoked and should have already been committed\", task.id());\n+                            throw new IllegalStateException(\"Revoked active task was not committed during handleRevocation\");\n+                        } else {\n+                            task.prepareCommit();\n+                            task.postCommit();\n+                        }\n                     }\n+                    completeTaskCloseClean(task);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MzIxOQ=="}, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUyMzY0MQ==", "bodyText": "Is there a different PR removing that? I still see\nif (clean) {\n                // since there's no written offsets we can checkpoint with empty map,\n                // and the state current offset would be used to checkpoint\n                stateMgr.checkpoint(Collections.emptyMap());\n                offsetSnapshotSinceLastCommit = new HashMap<>(stateMgr.changelogOffsets());\n            }\n\nin Standby.close() in trunk.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439523641", "createdAt": "2020-06-12T16:30:41Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -220,12 +215,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {\n+                        if (task.isActive()) {\n+                            log.error(\"Active task {} was revoked and should have already been committed\", task.id());\n+                            throw new IllegalStateException(\"Revoked active task was not committed during handleRevocation\");\n+                        } else {\n+                            task.prepareCommit();\n+                            task.postCommit();\n+                        }\n                     }\n+                    completeTaskCloseClean(task);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MzIxOQ=="}, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUyOTQ4MA==", "bodyText": "I suspect your trunk is out of date \ud83d\ude42\n(that code & comment is now in postCommit)", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439529480", "createdAt": "2020-06-12T16:43:03Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -220,12 +215,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {\n+                        if (task.isActive()) {\n+                            log.error(\"Active task {} was revoked and should have already been committed\", task.id());\n+                            throw new IllegalStateException(\"Revoked active task was not committed during handleRevocation\");\n+                        } else {\n+                            task.prepareCommit();\n+                            task.postCommit();\n+                        }\n                     }\n+                    completeTaskCloseClean(task);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MzIxOQ=="}, "originalCommit": null, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1MzQ2NQ==", "bodyText": "You're right :)", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439553465", "createdAt": "2020-06-12T17:32:00Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -220,12 +215,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {\n+                        if (task.isActive()) {\n+                            log.error(\"Active task {} was revoked and should have already been committed\", task.id());\n+                            throw new IllegalStateException(\"Revoked active task was not committed during handleRevocation\");\n+                        } else {\n+                            task.prepareCommit();\n+                            task.postCommit();\n+                        }\n                     }\n+                    completeTaskCloseClean(task);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MzIxOQ=="}, "originalCommit": null, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNzg2MzkyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNTozODoyMFrOGjIlJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNjoyODo1N1rOGjKVwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MzkyNA==", "bodyText": "nit: Add in the above javadoc that we should only revoke active tasks here?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439493924", "createdAt": "2020-06-12T15:38:20Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -440,41 +402,35 @@ boolean tryToCompleteRestoration() {\n      * @throws TaskMigratedException if the task producer got fenced (EOS only)\n      */\n     void handleRevocation(final Collection<TopicPartition> revokedPartitions) {\n-        final Set<TopicPartition> remainingPartitions = new HashSet<>(revokedPartitions);\n+        final Set<TopicPartition> remainingRevokedPartitions = new HashSet<>(revokedPartitions);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUyMjc1Mw==", "bodyText": "ack", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439522753", "createdAt": "2020-06-12T16:28:57Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -440,41 +402,35 @@ boolean tryToCompleteRestoration() {\n      * @throws TaskMigratedException if the task producer got fenced (EOS only)\n      */\n     void handleRevocation(final Collection<TopicPartition> revokedPartitions) {\n-        final Set<TopicPartition> remainingPartitions = new HashSet<>(revokedPartitions);\n+        final Set<TopicPartition> remainingRevokedPartitions = new HashSet<>(revokedPartitions);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MzkyNA=="}, "originalCommit": null, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNzg2NjE2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": true, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNTozOTowMVrOGjImnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMDozNToyNVrOGjUiSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5NDMwMQ==", "bodyText": "nit: add a comment above task.suspend() that for active it should always be an no-op?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439494301", "createdAt": "2020-06-12T15:39:01Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -220,12 +215,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUyMjk3OA==", "bodyText": "ack", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439522978", "createdAt": "2020-06-12T16:29:23Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -220,12 +215,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5NDMwMQ=="}, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUyNDY1OQ==", "bodyText": "Actually it's not always a no-op, since we will bail on suspending/committing the remaining active tasks if any of them throws an exception in handleRevocation -- so it's possible some active tasks are not yet suspended at this point.\nI will add a comment to clarify", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439524659", "createdAt": "2020-06-12T16:32:53Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -220,12 +215,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5NDMwMQ=="}, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0MDc0OA==", "bodyText": "Not sure if I can follow -- if it's a no-op, why do we call it? Or do you say, we need to tall if for standbies as we don't suspend them presiously?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439540748", "createdAt": "2020-06-12T17:05:45Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -220,12 +215,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5NDMwMQ=="}, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0NTg5OQ==", "bodyText": "We need to do it for all standbys, and we may need to do it for some actives. Since suspending is now idempotent we may as well just call suspend universally", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439545899", "createdAt": "2020-06-12T17:16:39Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -220,12 +215,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5NDMwMQ=="}, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY4ODIzOA==", "bodyText": "For this case, it won't be a no-op for some active tasks? So we should not have such a comment?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439688238", "createdAt": "2020-06-13T00:29:03Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -220,12 +215,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5NDMwMQ=="}, "originalCommit": null, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY4OTgwMQ==", "bodyText": "The current comment is\n// Should be a no-op for active tasks, unless we hit an exception during handleRevocation", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439689801", "createdAt": "2020-06-13T00:35:25Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -220,12 +215,18 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             } else {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5NDMwMQ=="}, "originalCommit": null, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczODA5OTYyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNjo0ODoyOVrOGjK6pQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxODo0Mjo1NVrOGjOMOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzMjE5Nw==", "bodyText": "I get that standbys should never really be in RESTORING state, but it still doesn't seem like it's philosophically any more illegal to suspend from RESTORING than it is from RUNNING. I'd vote to legalize RESTORING here. It does seem like a useful sanity check for CLOSED to be illegal, though.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439532197", "createdAt": "2020-06-12T16:48:29Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -112,10 +112,19 @@ public void completeRestoration() {\n     @Override\n     public void suspend() {\n         log.trace(\"No-op suspend with state {}\", state());\n-        if (state() == State.RUNNING) {\n-            transitionTo(State.SUSPENDED);\n-        } else if (state() == State.RESTORING) {\n-            throw new IllegalStateException(\"Illegal state \" + state() + \" while suspending standby task \" + id);\n+        switch (state()) {\n+            case CREATED:\n+            case RUNNING:\n+            case SUSPENDED:\n+                transitionTo(State.SUSPENDED);\n+                break;\n+\n+            case RESTORING:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU4NTg0OA==", "bodyText": "I agree 100%, but at some point in the past we started checking for RESTORING and throwing IllegalStateException all over StandbyTask. I wanted to keep the changes here to a minimum and figured we should at least be consistent with the current pattern elsewhere", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439585848", "createdAt": "2020-06-12T18:42:55Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -112,10 +112,19 @@ public void completeRestoration() {\n     @Override\n     public void suspend() {\n         log.trace(\"No-op suspend with state {}\", state());\n-        if (state() == State.RUNNING) {\n-            transitionTo(State.SUSPENDED);\n-        } else if (state() == State.RESTORING) {\n-            throw new IllegalStateException(\"Illegal state \" + state() + \" while suspending standby task \" + id);\n+        switch (state()) {\n+            case CREATED:\n+            case RUNNING:\n+            case SUSPENDED:\n+                transitionTo(State.SUSPENDED);\n+                break;\n+\n+            case RESTORING:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzMjE5Nw=="}, "originalCommit": null, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczODExMjY4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNjo1Mjo0NlrOGjLC1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxODo0MzoyN1rOGjONRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzNDI5NQ==", "bodyText": "It looks like your changes in the tasks have prohibited any state from transitioning to CLOSED except SUSPENDED. Should we update the state machine to reflect this?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439534295", "createdAt": "2020-06-12T16:52:46Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -66,11 +66,11 @@\n      * </pre>\n      */\n     enum State {\n-        CREATED(1, 4),         // 0\n-        RESTORING(2, 3, 4),    // 1\n-        RUNNING(3),            // 2\n-        SUSPENDED(1, 4),       // 3\n-        CLOSED(0);             // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks\n+        CREATED(1, 3, 4),         // 0\n+        RESTORING(2, 3, 4),       // 1\n+        RUNNING(3),               // 2\n+        SUSPENDED(1, 3, 4),       // 3\n+        CLOSED(0);                // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU4NjExOA==", "bodyText": "I think you're looking at an old version of the PR :)", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439586118", "createdAt": "2020-06-12T18:43:27Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -66,11 +66,11 @@\n      * </pre>\n      */\n     enum State {\n-        CREATED(1, 4),         // 0\n-        RESTORING(2, 3, 4),    // 1\n-        RUNNING(3),            // 2\n-        SUSPENDED(1, 4),       // 3\n-        CLOSED(0);             // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks\n+        CREATED(1, 3, 4),         // 0\n+        RESTORING(2, 3, 4),       // 1\n+        RUNNING(3),               // 2\n+        SUSPENDED(1, 3, 4),       // 3\n+        CLOSED(0);                // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzNDI5NQ=="}, "originalCommit": null, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczODExNzk4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNjo1NDo0MFrOGjLGYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxODo0NDowNlrOGjOOXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzNTIwMQ==", "bodyText": "I think we previously followed the \"loop over iterator and remove during iteration\" pattern, and we got away from it because it was too confusing. Do we really need to re-introduce it now?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439535201", "createdAt": "2020-06-12T16:54:40Z", "author": {"login": "vvcephei"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -219,13 +214,19 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 tasksToRecycle.add(task);\n             } else {\n                 try {\n-                    task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    task.suspend(); // Should be a no-op for all active tasks, unless we hit an exception during handleRevocation\n+                    if (task.commitNeeded()) {\n+                        if (task.isActive()) {\n+                            log.error(\"Active task {} was revoked and should have already been committed\", task.id());\n+                            throw new IllegalStateException(\"Revoked active task was not committed during handleRevocation\");\n+                        } else {\n+                            task.prepareCommit();\n+                            task.postCommit();\n+                        }\n                     }\n+                    completeTaskCloseClean(task);\n+                    cleanUpTaskProducer(task, taskCloseExceptions);\n+                    taskIter.remove();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU4NjM5OA==", "bodyText": "Oh, I thought we removed/refactored it for other reasons. I'm happy to undo this", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439586398", "createdAt": "2020-06-12T18:44:06Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -219,13 +214,19 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 tasksToRecycle.add(task);\n             } else {\n                 try {\n-                    task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    task.suspend(); // Should be a no-op for all active tasks, unless we hit an exception during handleRevocation\n+                    if (task.commitNeeded()) {\n+                        if (task.isActive()) {\n+                            log.error(\"Active task {} was revoked and should have already been committed\", task.id());\n+                            throw new IllegalStateException(\"Revoked active task was not committed during handleRevocation\");\n+                        } else {\n+                            task.prepareCommit();\n+                            task.postCommit();\n+                        }\n                     }\n+                    completeTaskCloseClean(task);\n+                    cleanUpTaskProducer(task, taskCloseExceptions);\n+                    taskIter.remove();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzNTIwMQ=="}, "originalCommit": null, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczODEyOTMyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNjo1ODoxOVrOGjLNtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzozMDoxN1rOGjMKkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzNzA3Ng==", "bodyText": "Can we update the comment with the state transitions above, too?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439537076", "createdAt": "2020-06-12T16:58:19Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -66,11 +66,11 @@\n      * </pre>\n      */\n     enum State {\n-        CREATED(1, 4),         // 0\n-        RESTORING(2, 3, 4),    // 1\n-        RUNNING(3),            // 2\n-        SUSPENDED(1, 4),       // 3\n-        CLOSED(0);             // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks\n+        CREATED(1, 3, 4),         // 0\n+        RESTORING(2, 3, 4),       // 1\n+        RUNNING(3),               // 2\n+        SUSPENDED(1, 3, 4),       // 3\n+        CLOSED(0);                // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1MjY1Ng==", "bodyText": ":/ yeah...", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439552656", "createdAt": "2020-06-12T17:30:17Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -66,11 +66,11 @@\n      * </pre>\n      */\n     enum State {\n-        CREATED(1, 4),         // 0\n-        RESTORING(2, 3, 4),    // 1\n-        RUNNING(3),            // 2\n-        SUSPENDED(1, 4),       // 3\n-        CLOSED(0);             // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks\n+        CREATED(1, 3, 4),         // 0\n+        RESTORING(2, 3, 4),       // 1\n+        RUNNING(3),               // 2\n+        SUSPENDED(1, 3, 4),       // 3\n+        CLOSED(0);                // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzNzA3Ng=="}, "originalCommit": null, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczODEzNjExOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzowMDoyM1rOGjLSFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzoyNjozNlrOGjMDIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzODE5Nw==", "bodyText": "and there is no new commit needed -> this seem to be miss leading because the commitNeeded flag is not really a guard for this case. -- Also, if the previous commit has complete is something we don't really know here.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439538197", "createdAt": "2020-06-12T17:00:23Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -528,7 +521,8 @@ private void maybeScheduleCheckpoint() {\n \n     private void writeCheckpointIfNeed() {\n         if (commitNeeded) {\n-            throw new IllegalStateException(\"A checkpoint should only be written if no commit is needed.\");\n+            throw new IllegalStateException(\"A checkpoint should only be written if the previous commit has completed\"\n+                                                + \" and there is no new commit needed.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0OTY3Ng==", "bodyText": "and there is no new commit needed -> this seem to be miss leading because the commitNeeded flag is not really a guard for this case.\n\nIsn't that exactly what the commitNeeded flag is?\nThat said, looking at this again, I agree this new phrasing doesn't really make sense. But the original comment took me a while to understand also (shouldn't we only write a checkpoint if there's a commit needed?) IIUC the point is that we should not write a checkpoint before doing the actual commit, ie there should not be pending uncommitted data when we write the checkpoint.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439549676", "createdAt": "2020-06-12T17:24:33Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -528,7 +521,8 @@ private void maybeScheduleCheckpoint() {\n \n     private void writeCheckpointIfNeed() {\n         if (commitNeeded) {\n-            throw new IllegalStateException(\"A checkpoint should only be written if no commit is needed.\");\n+            throw new IllegalStateException(\"A checkpoint should only be written if the previous commit has completed\"\n+                                                + \" and there is no new commit needed.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzODE5Nw=="}, "originalCommit": null, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1MDc1NQ==", "bodyText": "I'll revert it to the original and add a error log message: does\nlog.error(\"Tried to write a checkpoint with pending uncommitted data, should complete the commit first.\");\nmake sense to you?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439550755", "createdAt": "2020-06-12T17:26:36Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -528,7 +521,8 @@ private void maybeScheduleCheckpoint() {\n \n     private void writeCheckpointIfNeed() {\n         if (commitNeeded) {\n-            throw new IllegalStateException(\"A checkpoint should only be written if no commit is needed.\");\n+            throw new IllegalStateException(\"A checkpoint should only be written if the previous commit has completed\"\n+                                                + \" and there is no new commit needed.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzODE5Nw=="}, "originalCommit": null, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczODE0MTQ1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzowMjoxOFrOGjLVhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzoyNzo1MVrOGjMFlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzOTA3Ng==", "bodyText": "Seems we need to transit from RESTORING to SUSPENDED now, before closing, and never directly from RESTORING to CLOSED?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439539076", "createdAt": "2020-06-12T17:02:18Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -66,11 +66,11 @@\n      * </pre>\n      */\n     enum State {\n-        CREATED(1, 4),         // 0\n-        RESTORING(2, 3, 4),    // 1\n-        RUNNING(3),            // 2\n-        SUSPENDED(1, 4),       // 3\n-        CLOSED(0);             // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks\n+        CREATED(1, 3, 4),         // 0\n+        RESTORING(2, 3, 4),       // 1", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1MTM4MA==", "bodyText": "Ack good catch. Same with CREATED", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439551380", "createdAt": "2020-06-12T17:27:51Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -66,11 +66,11 @@\n      * </pre>\n      */\n     enum State {\n-        CREATED(1, 4),         // 0\n-        RESTORING(2, 3, 4),    // 1\n-        RUNNING(3),            // 2\n-        SUSPENDED(1, 4),       // 3\n-        CLOSED(0);             // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks\n+        CREATED(1, 3, 4),         // 0\n+        RESTORING(2, 3, 4),       // 1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzOTA3Ng=="}, "originalCommit": null, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczODE2MDczOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzowODo1N1rOGjLh-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzozNjowMlrOGjMVRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0MjI2NA==", "bodyText": "Above, we call suspend() blindly and have a comment that for active it's a no-op. -- Might be good to align both cases to use the same pattern (I don't care which one we pick)?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439542264", "createdAt": "2020-06-12T17:08:57Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -239,54 +240,15 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             }\n         }\n \n-        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            try {\n-                for (final Task task : additionalTasksForCommitting) {\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                    }\n-                }\n-\n-                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n-\n-                for (final Task task : additionalTasksForCommitting) {\n-                    task.postCommit();\n-                }\n-            } catch (final RuntimeException e) {\n-                log.error(\"Failed to batch commit tasks, \" +\n-                    \"will close all tasks involved in this commit as dirty by the end\", e);\n-                dirtyTasks.addAll(additionalTasksForCommitting);\n-                dirtyTasks.addAll(tasksToClose);\n-\n-                tasksToClose.clear();\n-                // Just add first taskId to re-throw by the end.\n-                taskCloseExceptions.put(consumedOffsetsAndMetadataPerTask.keySet().iterator().next(), e);\n-            }\n-        }\n-\n-        for (final Task task : tasksToClose) {\n-            try {\n-                completeTaskCloseClean(task);\n-                cleanUpTaskProducer(task, taskCloseExceptions);\n-                tasks.remove(task.id());\n-            } catch (final RuntimeException e) {\n-                final String uncleanMessage = String.format(\"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\", task.id());\n-                log.error(uncleanMessage, e);\n-                taskCloseExceptions.put(task.id(), e);\n-                // We've already recorded the exception (which is the point of clean).\n-                // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                dirtyTasks.add(task);\n-            }\n-        }\n-\n         for (final Task oldTask : tasksToRecycle) {\n             final Task newTask;\n             try {\n                 if (oldTask.isActive()) {\n+                    // If active, the task should have already been suspended and committed during handleRevocation", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU1NTM5OQ==", "bodyText": "Actually I forgot to update this. We should always call suspend since some active tasks may not have been suspended during handleRevocation.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439555399", "createdAt": "2020-06-12T17:36:02Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -239,54 +240,15 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n             }\n         }\n \n-        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            try {\n-                for (final Task task : additionalTasksForCommitting) {\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                    }\n-                }\n-\n-                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n-\n-                for (final Task task : additionalTasksForCommitting) {\n-                    task.postCommit();\n-                }\n-            } catch (final RuntimeException e) {\n-                log.error(\"Failed to batch commit tasks, \" +\n-                    \"will close all tasks involved in this commit as dirty by the end\", e);\n-                dirtyTasks.addAll(additionalTasksForCommitting);\n-                dirtyTasks.addAll(tasksToClose);\n-\n-                tasksToClose.clear();\n-                // Just add first taskId to re-throw by the end.\n-                taskCloseExceptions.put(consumedOffsetsAndMetadataPerTask.keySet().iterator().next(), e);\n-            }\n-        }\n-\n-        for (final Task task : tasksToClose) {\n-            try {\n-                completeTaskCloseClean(task);\n-                cleanUpTaskProducer(task, taskCloseExceptions);\n-                tasks.remove(task.id());\n-            } catch (final RuntimeException e) {\n-                final String uncleanMessage = String.format(\"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\", task.id());\n-                log.error(uncleanMessage, e);\n-                taskCloseExceptions.put(task.id(), e);\n-                // We've already recorded the exception (which is the point of clean).\n-                // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                dirtyTasks.add(task);\n-            }\n-        }\n-\n         for (final Task oldTask : tasksToRecycle) {\n             final Task newTask;\n             try {\n                 if (oldTask.isActive()) {\n+                    // If active, the task should have already been suspended and committed during handleRevocation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0MjI2NA=="}, "originalCommit": null, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczODU1NDkwOnYy", "diffSide": "LEFT", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxOTozMjo1N1rOGjPemw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxOTozMjo1N1rOGjPemw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTYwNjkzOQ==", "bodyText": "This was a test I added a little while back in response to a bugfix, but it no longer makes sense in the current context (in fact it's currently not really testing anything at all, since the original point was to make sure the changelog reader partitions were cleaned up but that's not even the responsibility of the TaskManager anymore)", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439606939", "createdAt": "2020-06-12T19:32:57Z", "author": {"login": "ableegoldman"}, "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -952,58 +951,11 @@ public void shouldNotCommitOnHandleAssignmentIfOnlyStandbyTaskClosed() {\n     }\n \n     @Test\n-    public void shouldCleanupAnyTasksClosedAsDirtyAfterCommitException() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTExMDA4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMToxMjoyMVrOGjU3Aw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMjoyMzoyOFrOGjVNuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTEwNw==", "bodyText": "So far, we did not allow idempotent state transitions in the state machine itself, but handle it caller side. -- It seem inconsistent to allow SUSPENDED -> SUSPEND but not CREATE -> CREATED etc.\nI would recommend to keep the current pattern and avoid calling transiteState() if the task is already in the target state. -- I would also be happy to change it, but for this case, we should change it for all cases. However, this would enlarge the scope of this PR and I think it better not to do it in this PR.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439695107", "createdAt": "2020-06-13T01:12:21Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -56,21 +56,21 @@\n      *          |            |              |     |\n      *          |            v              |     |\n      *          |     +------+--------+     |     |\n-     *          |     | Suspended (3) | <---+     |    //TODO Suspended(3) could be removed after we've stable on KIP-429\n-     *          |     +------+--------+           |\n-     *          |            |                    |\n-     *          |            v                    |\n-     *          |      +-----+-------+            |\n-     *          +----> | Closed (4)  | -----------+\n+     *          +---->| Suspended (3) | ----+     |    //TODO Suspended(3) could be removed after we've stable on KIP-429\n+     *                +------+--------+           |\n+     *                       |                    |\n+     *                       v                    |\n+     *                 +-----+-------+            |\n+     *                 | Closed (4)  | -----------+\n      *                 +-------------+\n      * </pre>\n      */\n     enum State {\n-        CREATED(1, 4),         // 0\n-        RESTORING(2, 3, 4),    // 1\n-        RUNNING(3),            // 2\n-        SUSPENDED(1, 4),       // 3\n-        CLOSED(0);             // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks\n+        CREATED(1, 3),            // 0\n+        RESTORING(2, 3),          // 1\n+        RUNNING(3),               // 2\n+        SUSPENDED(1, 3, 4),       // 3", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTcwMDkyMA==", "bodyText": "I see. I was just thinking we should make the idempotency explicit for each state by allowing/disallowing the transition, but I agree we can do that in a followup PR", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439700920", "createdAt": "2020-06-13T02:23:28Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -56,21 +56,21 @@\n      *          |            |              |     |\n      *          |            v              |     |\n      *          |     +------+--------+     |     |\n-     *          |     | Suspended (3) | <---+     |    //TODO Suspended(3) could be removed after we've stable on KIP-429\n-     *          |     +------+--------+           |\n-     *          |            |                    |\n-     *          |            v                    |\n-     *          |      +-----+-------+            |\n-     *          +----> | Closed (4)  | -----------+\n+     *          +---->| Suspended (3) | ----+     |    //TODO Suspended(3) could be removed after we've stable on KIP-429\n+     *                +------+--------+           |\n+     *                       |                    |\n+     *                       v                    |\n+     *                 +-----+-------+            |\n+     *                 | Closed (4)  | -----------+\n      *                 +-------------+\n      * </pre>\n      */\n     enum State {\n-        CREATED(1, 4),         // 0\n-        RESTORING(2, 3, 4),    // 1\n-        RUNNING(3),            // 2\n-        SUSPENDED(1, 4),       // 3\n-        CLOSED(0);             // 4, we allow CLOSED to transit to CREATED to handle corrupted tasks\n+        CREATED(1, 3),            // 0\n+        RESTORING(2, 3),          // 1\n+        RUNNING(3),               // 2\n+        SUSPENDED(1, 3, 4),       // 3", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTEwNw=="}, "originalCommit": null, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTExMDc2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMToxMzowOFrOGjU3Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMToxMzowOFrOGjU3Wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTE5NA==", "bodyText": "The SUSPEND case should be no-op IMHO and not call transiteTo() (compare my other comment in Task.java).", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439695194", "createdAt": "2020-06-13T01:13:08Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java", "diffHunk": "@@ -112,10 +112,19 @@ public void completeRestoration() {\n     @Override\n     public void suspend() {\n         log.trace(\"No-op suspend with state {}\", state());\n-        if (state() == State.RUNNING) {\n-            transitionTo(State.SUSPENDED);\n-        } else if (state() == State.RESTORING) {\n-            throw new IllegalStateException(\"Illegal state \" + state() + \" while suspending standby task \" + id);\n+        switch (state()) {\n+            case CREATED:\n+            case RUNNING:\n+            case SUSPENDED:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTExMTYyOnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMToxNDozN1rOGjU30g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMToxNDozN1rOGjU30g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTMxNA==", "bodyText": "IMHO, we should keep the SUSPENDED case for consistency reasons. Only merge CREATED and RESTORING (cf. my other comment on Task.java)", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439695314", "createdAt": "2020-06-13T01:14:37Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -250,14 +250,10 @@ public void completeRestoration() {\n     public void suspend() {\n         switch (state()) {\n             case CREATED:\n-            case SUSPENDED:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTExMzE0OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMToxNjoyMVrOGjU4mA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMjoxMDozN1rOGjVJtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTUxMg==", "bodyText": "Is the comment necessary? Seem the code is self-explaining?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439695512", "createdAt": "2020-06-13T01:16:21Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -474,20 +470,17 @@ public void update(final Set<TopicPartition> topicPartitions, final Map<String,\n \n     @Override\n     public void closeAndRecycleState() {\n-        suspend();\n-        prepareCommit();\n-        writeCheckpointIfNeed();\n-\n+        // Stream tasks should have already been suspended and their consumed offsets committed before recycling", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5OTg5NQ==", "bodyText": "Yeah it does seem unnecessary", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439699895", "createdAt": "2020-06-13T02:10:37Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -474,20 +470,17 @@ public void update(final Set<TopicPartition> topicPartitions, final Map<String,\n \n     @Override\n     public void closeAndRecycleState() {\n-        suspend();\n-        prepareCommit();\n-        writeCheckpointIfNeed();\n-\n+        // Stream tasks should have already been suspended and their consumed offsets committed before recycling", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTUxMg=="}, "originalCommit": null, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTExNDMxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMToxODoyNFrOGjU5VA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMjoyMTozOVrOGjVM5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTcwMA==", "bodyText": "Why remove the < arrow? We can still transit from RESTORING to SUSPENDED.\nSuper-nit: +---->| Suspended -> +---> | Suspended", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439695700", "createdAt": "2020-06-13T01:18:24Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -56,21 +56,21 @@\n      *          |            |              |     |\n      *          |            v              |     |\n      *          |     +------+--------+     |     |\n-     *          |     | Suspended (3) | <---+     |    //TODO Suspended(3) could be removed after we've stable on KIP-429\n-     *          |     +------+--------+           |\n-     *          |            |                    |\n-     *          |            v                    |\n-     *          |      +-----+-------+            |\n-     *          +----> | Closed (4)  | -----------+\n+     *          +---->| Suspended (3) | ----+     |    //TODO Suspended(3) could be removed after we've stable on KIP-429", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTcwMDcxMQ==", "bodyText": "The diff makes it hard to tell, but I \"merged\" the path to SUSPENDED from CREATED and RESTORING. I find it a bit easier to follow when all the arrows are unidirectional", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439700711", "createdAt": "2020-06-13T02:21:39Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/Task.java", "diffHunk": "@@ -56,21 +56,21 @@\n      *          |            |              |     |\n      *          |            v              |     |\n      *          |     +------+--------+     |     |\n-     *          |     | Suspended (3) | <---+     |    //TODO Suspended(3) could be removed after we've stable on KIP-429\n-     *          |     +------+--------+           |\n-     *          |            |                    |\n-     *          |            v                    |\n-     *          |      +-----+-------+            |\n-     *          +----> | Closed (4)  | -----------+\n+     *          +---->| Suspended (3) | ----+     |    //TODO Suspended(3) could be removed after we've stable on KIP-429", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTcwMA=="}, "originalCommit": null, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTExNTMwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMToyMDowMlrOGjU55g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMjoxMTozMVrOGjVJ-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTg0Ng==", "bodyText": "Nit: Why LinkedList<Task> tasksToClose? Should we only declare it as List?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439695846", "createdAt": "2020-06-13T01:20:02Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -215,91 +215,54 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n-        final Set<Task> tasksToRecycle = new HashSet<>();\n-\n         builder.addSubscribedTopicsFromAssignment(\n             activeTasks.values().stream().flatMap(Collection::stream).collect(Collectors.toList()),\n             logPrefix\n         );\n \n-        // first rectify all existing tasks\n         final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions = new LinkedHashMap<>();\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n+        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n+        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n+        final LinkedList<Task> tasksToClose = new LinkedList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5OTk2MA==", "bodyText": "ack", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439699960", "createdAt": "2020-06-13T02:11:31Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -215,91 +215,54 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n-        final Set<Task> tasksToRecycle = new HashSet<>();\n-\n         builder.addSubscribedTopicsFromAssignment(\n             activeTasks.values().stream().flatMap(Collection::stream).collect(Collectors.toList()),\n             logPrefix\n         );\n \n-        // first rectify all existing tasks\n         final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions = new LinkedHashMap<>();\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n+        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n+        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n+        final LinkedList<Task> tasksToClose = new LinkedList<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NTg0Ng=="}, "originalCommit": null, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTEyMzIxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMTozMjo1MVrOGjU-Vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQwMjo0ODo1MlrOGkJXwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5Njk4Mg==", "bodyText": "If we hit an exception in handleRevocation why would we continue here? Are we still in a \"clean enough\" state to actually continue?\nBelow we call completeTaskCloseClean(task) what seem incorrect for this case as it might close clean task even if we did not successfully commit before.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439696982", "createdAt": "2020-06-13T01:32:51Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -215,91 +215,54 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n-        final Set<Task> tasksToRecycle = new HashSet<>();\n-\n         builder.addSubscribedTopicsFromAssignment(\n             activeTasks.values().stream().flatMap(Collection::stream).collect(Collectors.toList()),\n             logPrefix\n         );\n \n-        // first rectify all existing tasks\n         final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions = new LinkedHashMap<>();\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n+        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n+        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n+        final LinkedList<Task> tasksToClose = new LinkedList<>();\n+        final Set<Task> tasksToRecycle = new HashSet<>();\n         final Set<Task> dirtyTasks = new HashSet<>();\n \n+        // first rectify all existing tasks\n         for (final Task task : tasks.values()) {\n             if (activeTasks.containsKey(task.id()) && task.isActive()) {\n                 updateInputPartitionsAndResume(task, activeTasks.get(task.id()));\n-                if (task.commitNeeded()) {\n-                    additionalTasksForCommitting.add(task);\n-                }\n                 activeTasksToCreate.remove(task.id());\n             } else if (standbyTasks.containsKey(task.id()) && !task.isActive()) {\n                 updateInputPartitionsAndResume(task, standbyTasks.get(task.id()));\n                 standbyTasksToCreate.remove(task.id());\n-                // check for tasks that were owned previously but have changed active/standby status\n             } else if (activeTasks.containsKey(task.id()) || standbyTasks.containsKey(task.id())) {\n+                // check for tasks that were owned previously but have changed active/standby status\n                 tasksToRecycle.add(task);\n             } else {\n-                try {\n-                    task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                    }\n-                } catch (final RuntimeException e) {\n-                    final String uncleanMessage = String.format(\n-                        \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n-                        task.id());\n-                    log.error(uncleanMessage, e);\n-                    taskCloseExceptions.put(task.id(), e);\n-                    // We've already recorded the exception (which is the point of clean).\n-                    // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                    dirtyTasks.add(task);\n-                }\n+                tasksToClose.add(task);\n             }\n         }\n \n-        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+        for (final Task task : tasksToClose) {\n             try {\n-                for (final Task task : additionalTasksForCommitting) {\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                task.suspend(); // Should be a no-op for active tasks, unless we hit an exception during handleRevocation", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTcwMTkxNw==", "bodyText": "If we hit an exception in handleRevocation on some task then we would skip out on suspending the rest of the tasks, ie the set of not-suspended tasks does not contain the task that threw (of course if one task threw an exception then its likely others will too, but not guaranteed).\nBut maybe it's cleaner to catch exceptions during handleRevocation and at least make sure every task gets suspended? I'll try that\nOn a related note, if we always  have to commit before closing (or at least attempt to), should we just remove the writeCheckpointIfNeeded call from closeClean? Seems like the pre/postCommit should be responsible for whether to checkpoint, not close. In this case, it's completely fine to attempt a clean close of a dirty task, as the closeClean method will just maybe throw in which case we can close dirty. WDYT?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439701917", "createdAt": "2020-06-13T02:38:51Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -215,91 +215,54 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n-        final Set<Task> tasksToRecycle = new HashSet<>();\n-\n         builder.addSubscribedTopicsFromAssignment(\n             activeTasks.values().stream().flatMap(Collection::stream).collect(Collectors.toList()),\n             logPrefix\n         );\n \n-        // first rectify all existing tasks\n         final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions = new LinkedHashMap<>();\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n+        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n+        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n+        final LinkedList<Task> tasksToClose = new LinkedList<>();\n+        final Set<Task> tasksToRecycle = new HashSet<>();\n         final Set<Task> dirtyTasks = new HashSet<>();\n \n+        // first rectify all existing tasks\n         for (final Task task : tasks.values()) {\n             if (activeTasks.containsKey(task.id()) && task.isActive()) {\n                 updateInputPartitionsAndResume(task, activeTasks.get(task.id()));\n-                if (task.commitNeeded()) {\n-                    additionalTasksForCommitting.add(task);\n-                }\n                 activeTasksToCreate.remove(task.id());\n             } else if (standbyTasks.containsKey(task.id()) && !task.isActive()) {\n                 updateInputPartitionsAndResume(task, standbyTasks.get(task.id()));\n                 standbyTasksToCreate.remove(task.id());\n-                // check for tasks that were owned previously but have changed active/standby status\n             } else if (activeTasks.containsKey(task.id()) || standbyTasks.containsKey(task.id())) {\n+                // check for tasks that were owned previously but have changed active/standby status\n                 tasksToRecycle.add(task);\n             } else {\n-                try {\n-                    task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                    }\n-                } catch (final RuntimeException e) {\n-                    final String uncleanMessage = String.format(\n-                        \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n-                        task.id());\n-                    log.error(uncleanMessage, e);\n-                    taskCloseExceptions.put(task.id(), e);\n-                    // We've already recorded the exception (which is the point of clean).\n-                    // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                    dirtyTasks.add(task);\n-                }\n+                tasksToClose.add(task);\n             }\n         }\n \n-        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+        for (final Task task : tasksToClose) {\n             try {\n-                for (final Task task : additionalTasksForCommitting) {\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                task.suspend(); // Should be a no-op for active tasks, unless we hit an exception during handleRevocation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5Njk4Mg=="}, "originalCommit": null, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTcwMjUzNg==", "bodyText": "Instead of checkpointing, we can check if clean && commitNeeded & checkpoint != null and then throw an exception on closeClean (which would result in calling closeDirty)", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439702536", "createdAt": "2020-06-13T02:50:01Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -215,91 +215,54 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n-        final Set<Task> tasksToRecycle = new HashSet<>();\n-\n         builder.addSubscribedTopicsFromAssignment(\n             activeTasks.values().stream().flatMap(Collection::stream).collect(Collectors.toList()),\n             logPrefix\n         );\n \n-        // first rectify all existing tasks\n         final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions = new LinkedHashMap<>();\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n+        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n+        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n+        final LinkedList<Task> tasksToClose = new LinkedList<>();\n+        final Set<Task> tasksToRecycle = new HashSet<>();\n         final Set<Task> dirtyTasks = new HashSet<>();\n \n+        // first rectify all existing tasks\n         for (final Task task : tasks.values()) {\n             if (activeTasks.containsKey(task.id()) && task.isActive()) {\n                 updateInputPartitionsAndResume(task, activeTasks.get(task.id()));\n-                if (task.commitNeeded()) {\n-                    additionalTasksForCommitting.add(task);\n-                }\n                 activeTasksToCreate.remove(task.id());\n             } else if (standbyTasks.containsKey(task.id()) && !task.isActive()) {\n                 updateInputPartitionsAndResume(task, standbyTasks.get(task.id()));\n                 standbyTasksToCreate.remove(task.id());\n-                // check for tasks that were owned previously but have changed active/standby status\n             } else if (activeTasks.containsKey(task.id()) || standbyTasks.containsKey(task.id())) {\n+                // check for tasks that were owned previously but have changed active/standby status\n                 tasksToRecycle.add(task);\n             } else {\n-                try {\n-                    task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                    }\n-                } catch (final RuntimeException e) {\n-                    final String uncleanMessage = String.format(\n-                        \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n-                        task.id());\n-                    log.error(uncleanMessage, e);\n-                    taskCloseExceptions.put(task.id(), e);\n-                    // We've already recorded the exception (which is the point of clean).\n-                    // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                    dirtyTasks.add(task);\n-                }\n+                tasksToClose.add(task);\n             }\n         }\n \n-        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+        for (final Task task : tasksToClose) {\n             try {\n-                for (final Task task : additionalTasksForCommitting) {\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                task.suspend(); // Should be a no-op for active tasks, unless we hit an exception during handleRevocation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5Njk4Mg=="}, "originalCommit": null, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1MTkwNA==", "bodyText": "I re-read the current code structure and got some questions:\n\nwe collect checkpoint from prepareCommit and check if it is not null in postCommit, but the actual checkpoint value itself is always collectable post the commit, and hence what's only required to that we need to know if we need to write a checkpoint file or not. Previously this needs to be decided since we may transit the state in between but now from the source code it seems to me that we would only call prepare/post before suspend / close ever, so this is no longer required actually, i.e. we can decide whether we need to checkpoint and then collect the checkpoint map and write the file if needed in a single call. Is that right?\n\n\nI think I agree with you that it is cleaner to make sure in handleRevocation, we still transit those revoked partition's corresponding tasks to suspended even if some of their commit call failed.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439851904", "createdAt": "2020-06-14T17:38:50Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -215,91 +215,54 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n-        final Set<Task> tasksToRecycle = new HashSet<>();\n-\n         builder.addSubscribedTopicsFromAssignment(\n             activeTasks.values().stream().flatMap(Collection::stream).collect(Collectors.toList()),\n             logPrefix\n         );\n \n-        // first rectify all existing tasks\n         final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions = new LinkedHashMap<>();\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n+        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n+        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n+        final LinkedList<Task> tasksToClose = new LinkedList<>();\n+        final Set<Task> tasksToRecycle = new HashSet<>();\n         final Set<Task> dirtyTasks = new HashSet<>();\n \n+        // first rectify all existing tasks\n         for (final Task task : tasks.values()) {\n             if (activeTasks.containsKey(task.id()) && task.isActive()) {\n                 updateInputPartitionsAndResume(task, activeTasks.get(task.id()));\n-                if (task.commitNeeded()) {\n-                    additionalTasksForCommitting.add(task);\n-                }\n                 activeTasksToCreate.remove(task.id());\n             } else if (standbyTasks.containsKey(task.id()) && !task.isActive()) {\n                 updateInputPartitionsAndResume(task, standbyTasks.get(task.id()));\n                 standbyTasksToCreate.remove(task.id());\n-                // check for tasks that were owned previously but have changed active/standby status\n             } else if (activeTasks.containsKey(task.id()) || standbyTasks.containsKey(task.id())) {\n+                // check for tasks that were owned previously but have changed active/standby status\n                 tasksToRecycle.add(task);\n             } else {\n-                try {\n-                    task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                    }\n-                } catch (final RuntimeException e) {\n-                    final String uncleanMessage = String.format(\n-                        \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n-                        task.id());\n-                    log.error(uncleanMessage, e);\n-                    taskCloseExceptions.put(task.id(), e);\n-                    // We've already recorded the exception (which is the point of clean).\n-                    // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                    dirtyTasks.add(task);\n-                }\n+                tasksToClose.add(task);\n             }\n         }\n \n-        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+        for (final Task task : tasksToClose) {\n             try {\n-                for (final Task task : additionalTasksForCommitting) {\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                task.suspend(); // Should be a no-op for active tasks, unless we hit an exception during handleRevocation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5Njk4Mg=="}, "originalCommit": null, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ5MzAwNQ==", "bodyText": "I think you're right, we don't need to keep track of the current checkpoint offsets at all and can just write the current checkpointableOffsets in postCommit\ndone", "url": "https://github.com/apache/kafka/pull/8856#discussion_r440493005", "createdAt": "2020-06-15T23:02:05Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -215,91 +215,54 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n-        final Set<Task> tasksToRecycle = new HashSet<>();\n-\n         builder.addSubscribedTopicsFromAssignment(\n             activeTasks.values().stream().flatMap(Collection::stream).collect(Collectors.toList()),\n             logPrefix\n         );\n \n-        // first rectify all existing tasks\n         final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions = new LinkedHashMap<>();\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n+        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n+        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n+        final LinkedList<Task> tasksToClose = new LinkedList<>();\n+        final Set<Task> tasksToRecycle = new HashSet<>();\n         final Set<Task> dirtyTasks = new HashSet<>();\n \n+        // first rectify all existing tasks\n         for (final Task task : tasks.values()) {\n             if (activeTasks.containsKey(task.id()) && task.isActive()) {\n                 updateInputPartitionsAndResume(task, activeTasks.get(task.id()));\n-                if (task.commitNeeded()) {\n-                    additionalTasksForCommitting.add(task);\n-                }\n                 activeTasksToCreate.remove(task.id());\n             } else if (standbyTasks.containsKey(task.id()) && !task.isActive()) {\n                 updateInputPartitionsAndResume(task, standbyTasks.get(task.id()));\n                 standbyTasksToCreate.remove(task.id());\n-                // check for tasks that were owned previously but have changed active/standby status\n             } else if (activeTasks.containsKey(task.id()) || standbyTasks.containsKey(task.id())) {\n+                // check for tasks that were owned previously but have changed active/standby status\n                 tasksToRecycle.add(task);\n             } else {\n-                try {\n-                    task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                    }\n-                } catch (final RuntimeException e) {\n-                    final String uncleanMessage = String.format(\n-                        \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n-                        task.id());\n-                    log.error(uncleanMessage, e);\n-                    taskCloseExceptions.put(task.id(), e);\n-                    // We've already recorded the exception (which is the point of clean).\n-                    // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                    dirtyTasks.add(task);\n-                }\n+                tasksToClose.add(task);\n             }\n         }\n \n-        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+        for (final Task task : tasksToClose) {\n             try {\n-                for (final Task task : additionalTasksForCommitting) {\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                task.suspend(); // Should be a no-op for active tasks, unless we hit an exception during handleRevocation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5Njk4Mg=="}, "originalCommit": null, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDU1NTE0Mg==", "bodyText": "postCommit only writes a checkpoint for non-eos. Thus, we still need to write a checkpoint in close() for the eos-case (if just blindly for all cases as we do atm).", "url": "https://github.com/apache/kafka/pull/8856#discussion_r440555142", "createdAt": "2020-06-16T02:47:29Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -215,91 +215,54 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n-        final Set<Task> tasksToRecycle = new HashSet<>();\n-\n         builder.addSubscribedTopicsFromAssignment(\n             activeTasks.values().stream().flatMap(Collection::stream).collect(Collectors.toList()),\n             logPrefix\n         );\n \n-        // first rectify all existing tasks\n         final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions = new LinkedHashMap<>();\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n+        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n+        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n+        final LinkedList<Task> tasksToClose = new LinkedList<>();\n+        final Set<Task> tasksToRecycle = new HashSet<>();\n         final Set<Task> dirtyTasks = new HashSet<>();\n \n+        // first rectify all existing tasks\n         for (final Task task : tasks.values()) {\n             if (activeTasks.containsKey(task.id()) && task.isActive()) {\n                 updateInputPartitionsAndResume(task, activeTasks.get(task.id()));\n-                if (task.commitNeeded()) {\n-                    additionalTasksForCommitting.add(task);\n-                }\n                 activeTasksToCreate.remove(task.id());\n             } else if (standbyTasks.containsKey(task.id()) && !task.isActive()) {\n                 updateInputPartitionsAndResume(task, standbyTasks.get(task.id()));\n                 standbyTasksToCreate.remove(task.id());\n-                // check for tasks that were owned previously but have changed active/standby status\n             } else if (activeTasks.containsKey(task.id()) || standbyTasks.containsKey(task.id())) {\n+                // check for tasks that were owned previously but have changed active/standby status\n                 tasksToRecycle.add(task);\n             } else {\n-                try {\n-                    task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                    }\n-                } catch (final RuntimeException e) {\n-                    final String uncleanMessage = String.format(\n-                        \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n-                        task.id());\n-                    log.error(uncleanMessage, e);\n-                    taskCloseExceptions.put(task.id(), e);\n-                    // We've already recorded the exception (which is the point of clean).\n-                    // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                    dirtyTasks.add(task);\n-                }\n+                tasksToClose.add(task);\n             }\n         }\n \n-        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+        for (final Task task : tasksToClose) {\n             try {\n-                for (final Task task : additionalTasksForCommitting) {\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                task.suspend(); // Should be a no-op for active tasks, unless we hit an exception during handleRevocation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5Njk4Mg=="}, "originalCommit": null, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDU1NTQ1OA==", "bodyText": "postCommit will always write the checkpoint if the task is in SUSPENDED, which it should always be before being closed", "url": "https://github.com/apache/kafka/pull/8856#discussion_r440555458", "createdAt": "2020-06-16T02:48:52Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -215,91 +215,54 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                      \"\\tExisting standby tasks: {}\",\n                  activeTasks.keySet(), standbyTasks.keySet(), activeTaskIds(), standbyTaskIds());\n \n-        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n-        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n-        final Set<Task> tasksToRecycle = new HashSet<>();\n-\n         builder.addSubscribedTopicsFromAssignment(\n             activeTasks.values().stream().flatMap(Collection::stream).collect(Collectors.toList()),\n             logPrefix\n         );\n \n-        // first rectify all existing tasks\n         final LinkedHashMap<TaskId, RuntimeException> taskCloseExceptions = new LinkedHashMap<>();\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n+        final Map<TaskId, Set<TopicPartition>> activeTasksToCreate = new HashMap<>(activeTasks);\n+        final Map<TaskId, Set<TopicPartition>> standbyTasksToCreate = new HashMap<>(standbyTasks);\n+        final LinkedList<Task> tasksToClose = new LinkedList<>();\n+        final Set<Task> tasksToRecycle = new HashSet<>();\n         final Set<Task> dirtyTasks = new HashSet<>();\n \n+        // first rectify all existing tasks\n         for (final Task task : tasks.values()) {\n             if (activeTasks.containsKey(task.id()) && task.isActive()) {\n                 updateInputPartitionsAndResume(task, activeTasks.get(task.id()));\n-                if (task.commitNeeded()) {\n-                    additionalTasksForCommitting.add(task);\n-                }\n                 activeTasksToCreate.remove(task.id());\n             } else if (standbyTasks.containsKey(task.id()) && !task.isActive()) {\n                 updateInputPartitionsAndResume(task, standbyTasks.get(task.id()));\n                 standbyTasksToCreate.remove(task.id());\n-                // check for tasks that were owned previously but have changed active/standby status\n             } else if (activeTasks.containsKey(task.id()) || standbyTasks.containsKey(task.id())) {\n+                // check for tasks that were owned previously but have changed active/standby status\n                 tasksToRecycle.add(task);\n             } else {\n-                try {\n-                    task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                    }\n-                } catch (final RuntimeException e) {\n-                    final String uncleanMessage = String.format(\n-                        \"Failed to close task %s cleanly. Attempting to close remaining tasks before re-throwing:\",\n-                        task.id());\n-                    log.error(uncleanMessage, e);\n-                    taskCloseExceptions.put(task.id(), e);\n-                    // We've already recorded the exception (which is the point of clean).\n-                    // Now, we should go ahead and complete the close because a half-closed task is no good to anyone.\n-                    dirtyTasks.add(task);\n-                }\n+                tasksToClose.add(task);\n             }\n         }\n \n-        if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+        for (final Task task : tasksToClose) {\n             try {\n-                for (final Task task : additionalTasksForCommitting) {\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                task.suspend(); // Should be a no-op for active tasks, unless we hit an exception during handleRevocation", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5Njk4Mg=="}, "originalCommit": null, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTEyNjIxOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xM1QwMTozOTowN1rOGjVAEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzo1MDo0OVrOGkmnAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NzQyNg==", "bodyText": "Why do we do the try-catch as outer-layer? In an exception occurs, we should stop looping through the tasks to call postCommit() -- is this intended? If yes, why?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439697426", "createdAt": "2020-06-13T01:39:07Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -714,13 +696,20 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n-        if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        try {\n+            if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            }\n+            for (final TaskId taskId : consumedOffsetsAndMetadataPerTask.keySet()) {\n+                final Task task = tasks.get(taskId);\n+                task.postCommit();\n+            }\n+        } catch (final RuntimeException e) {\n+            firstException.compareAndSet(null, e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTcwMDMxMQ==", "bodyText": "Well if commit throws an exception, then we shouldn't call postCommit right?\nOr are you saying if  commit succeeds but postCommit throws for one task, we should still loop through and try to postCommit all the other tasks?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439700311", "createdAt": "2020-06-13T02:16:26Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -714,13 +696,20 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n-        if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        try {\n+            if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            }\n+            for (final TaskId taskId : consumedOffsetsAndMetadataPerTask.keySet()) {\n+                final Task task = tasks.get(taskId);\n+                task.postCommit();\n+            }\n+        } catch (final RuntimeException e) {\n+            firstException.compareAndSet(null, e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NzQyNg=="}, "originalCommit": null, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1MjU2NA==", "bodyText": "Yeah I think if the actual consumer.commit call failed, then we should not trigger postCommit for any one.\nAs for postCommit, I think it should never fail (we swallow the IO exception happened, because for non-EOS it is just fine, for EOS we would bootstrap from scratch).", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439852564", "createdAt": "2020-06-14T17:46:27Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -714,13 +696,20 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n-        if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        try {\n+            if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            }\n+            for (final TaskId taskId : consumedOffsetsAndMetadataPerTask.keySet()) {\n+                final Task task = tasks.get(taskId);\n+                task.postCommit();\n+            }\n+        } catch (final RuntimeException e) {\n+            firstException.compareAndSet(null, e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NzQyNg=="}, "originalCommit": null, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDU1NDI0OQ==", "bodyText": "I meant the later. And I agree that if commit fails, we should not call postCommit().\nFor failure in postCommit: we make assumptions about the current code what seems dangerous (ie, not future prove)? -- IMHO, if postCommit fails, we need to close the corresponding task dirty and either recreate it, or rebalance, but we should also continue to call postCommit() for all other tasks?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r440554249", "createdAt": "2020-06-16T02:44:10Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -714,13 +696,20 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n-        if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        try {\n+            if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            }\n+            for (final TaskId taskId : consumedOffsetsAndMetadataPerTask.keySet()) {\n+                final Task task = tasks.get(taskId);\n+                task.postCommit();\n+            }\n+        } catch (final RuntimeException e) {\n+            firstException.compareAndSet(null, e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NzQyNg=="}, "originalCommit": null, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDU1NTkxOA==", "bodyText": "I see. Then I think it makes sense to always attempt to write the checkpoint/call postCommit for a task that was successfully committed, regardless of whether something went wrong during postCommit with a different task\nAnd I agree, we should not make assumptions about the current code not throwing, unless it's explicitly in the contract of the method that it will never throw (which is not the case for postCommit", "url": "https://github.com/apache/kafka/pull/8856#discussion_r440555918", "createdAt": "2020-06-16T02:50:38Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -714,13 +696,20 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n-        if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        try {\n+            if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            }\n+            for (final TaskId taskId : consumedOffsetsAndMetadataPerTask.keySet()) {\n+                final Task task = tasks.get(taskId);\n+                task.postCommit();\n+            }\n+        } catch (final RuntimeException e) {\n+            firstException.compareAndSet(null, e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NzQyNg=="}, "originalCommit": null, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAzNDQ5Nw==", "bodyText": "Sounds good, in that case the nested try-catch would be necessary.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441034497", "createdAt": "2020-06-16T17:50:49Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -714,13 +696,20 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n-        if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        try {\n+            if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            }\n+            for (final TaskId taskId : consumedOffsetsAndMetadataPerTask.keySet()) {\n+                final Task task = tasks.get(taskId);\n+                task.postCommit();\n+            }\n+        } catch (final RuntimeException e) {\n+            firstException.compareAndSet(null, e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY5NzQyNg=="}, "originalCommit": null, "originalPosition": 235}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MDMxMzcyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQxNzozNTowN1rOGjeaYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQyMjowNzowMVrOGkEYfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1MTYxNg==", "bodyText": "Seems we would never commit and checkpoint state manager any more.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r439851616", "createdAt": "2020-06-14T17:35:07Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -539,19 +537,18 @@ private void writeCheckpointIfNeed() {\n     /**\n      * <pre>\n      * the following order must be followed:\n-     *  1. checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed\n+     *  1. commit/checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ3MzcyNg==", "bodyText": "ack", "url": "https://github.com/apache/kafka/pull/8856#discussion_r440473726", "createdAt": "2020-06-15T22:07:01Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -539,19 +537,18 @@ private void writeCheckpointIfNeed() {\n     /**\n      * <pre>\n      * the following order must be followed:\n-     *  1. checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed\n+     *  1. commit/checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg1MTYxNg=="}, "originalCommit": null, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzY5OTU4OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozNzowNFrOGkmHtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozNzowNFrOGkmHtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyNjQ4NQ==", "bodyText": "nit: was an active scheduled checkpoint -> there was a pending uncommitted data.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441026485", "createdAt": "2020-06-16T17:37:04Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -502,56 +494,24 @@ public void closeAndRecycleState() {\n         log.info(\"Closed clean and recycled state\");\n     }\n \n-    private void maybeScheduleCheckpoint() {\n-        switch (state()) {\n-            case RESTORING:\n-            case SUSPENDED:\n-                this.checkpoint = checkpointableOffsets();\n-\n-                break;\n-\n-            case RUNNING:\n-                if (!eosEnabled) {\n-                    this.checkpoint = checkpointableOffsets();\n-                }\n-\n-                break;\n-\n-            case CREATED:\n-            case CLOSED:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while scheduling checkpoint for active task \" + id);\n-\n-            default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while scheduling checkpoint for active task \" + id);\n-        }\n-    }\n-\n-    private void writeCheckpointIfNeed() {\n+    private void maybeWriteCheckpoint() {\n         if (commitNeeded) {\n+            log.error(\"Tried to write a checkpoint with pending uncommitted data, should complete the commit first.\");\n             throw new IllegalStateException(\"A checkpoint should only be written if no commit is needed.\");\n         }\n-        if (checkpoint != null) {\n-            stateMgr.checkpoint(checkpoint);\n-            checkpoint = null;\n-        }\n+        stateMgr.checkpoint(checkpointableOffsets());\n     }\n \n     /**\n-     * <pre>\n-     * the following order must be followed:\n-     *  1. checkpoint the state manager -- even if we crash before this step, EOS is still guaranteed\n-     *  2. then if we are closing on EOS and dirty, wipe out the state store directory\n-     *  3. finally release the state manager lock\n-     * </pre>\n+     * You must commit a task and checkpoint the state manager before closing as this will release the state dir lock\n      */\n     private void close(final boolean clean) {\n-        if (clean) {\n-            executeAndMaybeSwallow(true, this::writeCheckpointIfNeed, \"state manager checkpoint\", log);\n+        if (clean && commitNeeded) {\n+            log.debug(\"Tried to close clean but there was an active scheduled checkpoint, this means we failed to\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 172}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzgxODczOnYy", "diffSide": "LEFT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "isResolved": false, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoxMDoyN1rOGknUCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQyMDoxNDoyNFrOGkrghQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NjAyNA==", "bodyText": "I think we still need to make this call -- in eager rebalancing, we suspend a task when we get a partition revoked. For this case, we \"forget\" the current offset within the consumer and thus need to clear the partition grouper. Otherwise, we might read the data a second time, if the partition is reassigned (what would violate EOS).", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441046024", "createdAt": "2020-06-16T18:10:27Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -409,32 +407,28 @@ public void resume() {\n         return committableOffsets;\n     }\n \n+    /**\n+     * This should only be called if the attempted commit succeeded for this task\n+     */\n     @Override\n     public void postCommit() {\n         commitRequested = false;\n         commitNeeded = false;\n \n         switch (state()) {\n             case RESTORING:\n-                writeCheckpointIfNeed();\n+            case SUSPENDED:\n+                maybeWriteCheckpoint();\n \n                 break;\n \n             case RUNNING:\n-                if (!eosEnabled) { // if RUNNING, checkpoint only for non-eos\n-                    writeCheckpointIfNeed();\n+                if (!eosEnabled) {\n+                    maybeWriteCheckpoint();\n                 }\n \n                 break;\n \n-            case SUSPENDED:\n-                writeCheckpointIfNeed();\n-                // we cannot `clear()` the `PartitionGroup` in `suspend()` already, but only after committing,\n-                // because otherwise we loose the partition-time information\n-                partitionGroup.clear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0OTgxNw==", "bodyText": "I see. So we should only clear it here, and not in close\nJust curious, why do we \"forget\" the current offset? I mean, haven't we just committed the current offset before suspending (and if that failed we would close all tasks right away). Maybe I'm misunderstanding what you mean", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441049817", "createdAt": "2020-06-16T18:16:56Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -409,32 +407,28 @@ public void resume() {\n         return committableOffsets;\n     }\n \n+    /**\n+     * This should only be called if the attempted commit succeeded for this task\n+     */\n     @Override\n     public void postCommit() {\n         commitRequested = false;\n         commitNeeded = false;\n \n         switch (state()) {\n             case RESTORING:\n-                writeCheckpointIfNeed();\n+            case SUSPENDED:\n+                maybeWriteCheckpoint();\n \n                 break;\n \n             case RUNNING:\n-                if (!eosEnabled) { // if RUNNING, checkpoint only for non-eos\n-                    writeCheckpointIfNeed();\n+                if (!eosEnabled) {\n+                    maybeWriteCheckpoint();\n                 }\n \n                 break;\n \n-            case SUSPENDED:\n-                writeCheckpointIfNeed();\n-                // we cannot `clear()` the `PartitionGroup` in `suspend()` already, but only after committing,\n-                // because otherwise we loose the partition-time information\n-                partitionGroup.clear();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NjAyNA=="}, "originalCommit": null, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA2NTA5MQ==", "bodyText": "The consumer tracks offset internal, however, we buffer data in our internal queue. Thus, the offset tracked by the consumer, might be larger than the offset we commit (we take the offset we commit not from the consumer, but it's based on the records we did take out of the queue and processed).\nIn eager rebalancing, the consumer clears its internal state if a partition in revoked (and we only suspend the task), including the tracked offsets. If the partition in re-assigned, the consumer fetches the last committed offset to start fetching. Thus, if we don't clear the queue, we might fetch same data that is already in the queue a second time.\nDoes this make sense?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441065091", "createdAt": "2020-06-16T18:40:01Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -409,32 +407,28 @@ public void resume() {\n         return committableOffsets;\n     }\n \n+    /**\n+     * This should only be called if the attempted commit succeeded for this task\n+     */\n     @Override\n     public void postCommit() {\n         commitRequested = false;\n         commitNeeded = false;\n \n         switch (state()) {\n             case RESTORING:\n-                writeCheckpointIfNeed();\n+            case SUSPENDED:\n+                maybeWriteCheckpoint();\n \n                 break;\n \n             case RUNNING:\n-                if (!eosEnabled) { // if RUNNING, checkpoint only for non-eos\n-                    writeCheckpointIfNeed();\n+                if (!eosEnabled) {\n+                    maybeWriteCheckpoint();\n                 }\n \n                 break;\n \n-            case SUSPENDED:\n-                writeCheckpointIfNeed();\n-                // we cannot `clear()` the `PartitionGroup` in `suspend()` already, but only after committing,\n-                // because otherwise we loose the partition-time information\n-                partitionGroup.clear();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NjAyNA=="}, "originalCommit": null, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA4ODg0Nw==", "bodyText": "Ah that's a good catch. Makes sense to me.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441088847", "createdAt": "2020-06-16T19:23:35Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -409,32 +407,28 @@ public void resume() {\n         return committableOffsets;\n     }\n \n+    /**\n+     * This should only be called if the attempted commit succeeded for this task\n+     */\n     @Override\n     public void postCommit() {\n         commitRequested = false;\n         commitNeeded = false;\n \n         switch (state()) {\n             case RESTORING:\n-                writeCheckpointIfNeed();\n+            case SUSPENDED:\n+                maybeWriteCheckpoint();\n \n                 break;\n \n             case RUNNING:\n-                if (!eosEnabled) { // if RUNNING, checkpoint only for non-eos\n-                    writeCheckpointIfNeed();\n+                if (!eosEnabled) {\n+                    maybeWriteCheckpoint();\n                 }\n \n                 break;\n \n-            case SUSPENDED:\n-                writeCheckpointIfNeed();\n-                // we cannot `clear()` the `PartitionGroup` in `suspend()` already, but only after committing,\n-                // because otherwise we loose the partition-time information\n-                partitionGroup.clear();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NjAyNA=="}, "originalCommit": null, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA4OTM5Nw==", "bodyText": "Got it, thanks for the explanation. I'll move it back to postCommit with a note", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441089397", "createdAt": "2020-06-16T19:24:40Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -409,32 +407,28 @@ public void resume() {\n         return committableOffsets;\n     }\n \n+    /**\n+     * This should only be called if the attempted commit succeeded for this task\n+     */\n     @Override\n     public void postCommit() {\n         commitRequested = false;\n         commitNeeded = false;\n \n         switch (state()) {\n             case RESTORING:\n-                writeCheckpointIfNeed();\n+            case SUSPENDED:\n+                maybeWriteCheckpoint();\n \n                 break;\n \n             case RUNNING:\n-                if (!eosEnabled) { // if RUNNING, checkpoint only for non-eos\n-                    writeCheckpointIfNeed();\n+                if (!eosEnabled) {\n+                    maybeWriteCheckpoint();\n                 }\n \n                 break;\n \n-            case SUSPENDED:\n-                writeCheckpointIfNeed();\n-                // we cannot `clear()` the `PartitionGroup` in `suspend()` already, but only after committing,\n-                // because otherwise we loose the partition-time information\n-                partitionGroup.clear();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NjAyNA=="}, "originalCommit": null, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA5MjcwMw==", "bodyText": "I'm not 100% certain that the Consumer does clear its internal buffer on revocation. At least, I couldn't find it in the code, but maybe I'm looking in the wrong place.\nNot arguing we shouldn't clear the partition group here, was just wondering about this for my own sake. Hm", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441092703", "createdAt": "2020-06-16T19:31:01Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -409,32 +407,28 @@ public void resume() {\n         return committableOffsets;\n     }\n \n+    /**\n+     * This should only be called if the attempted commit succeeded for this task\n+     */\n     @Override\n     public void postCommit() {\n         commitRequested = false;\n         commitNeeded = false;\n \n         switch (state()) {\n             case RESTORING:\n-                writeCheckpointIfNeed();\n+            case SUSPENDED:\n+                maybeWriteCheckpoint();\n \n                 break;\n \n             case RUNNING:\n-                if (!eosEnabled) { // if RUNNING, checkpoint only for non-eos\n-                    writeCheckpointIfNeed();\n+                if (!eosEnabled) {\n+                    maybeWriteCheckpoint();\n                 }\n \n                 break;\n \n-            case SUSPENDED:\n-                writeCheckpointIfNeed();\n-                // we cannot `clear()` the `PartitionGroup` in `suspend()` already, but only after committing,\n-                // because otherwise we loose the partition-time information\n-                partitionGroup.clear();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NjAyNA=="}, "originalCommit": null, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTEwOTAxMA==", "bodyText": "Not 100% familiar with the consumer code, but in SubscriptionState#assignFromSubscribed new TopicPartitionState are created with position = null.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441109010", "createdAt": "2020-06-16T20:02:18Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -409,32 +407,28 @@ public void resume() {\n         return committableOffsets;\n     }\n \n+    /**\n+     * This should only be called if the attempted commit succeeded for this task\n+     */\n     @Override\n     public void postCommit() {\n         commitRequested = false;\n         commitNeeded = false;\n \n         switch (state()) {\n             case RESTORING:\n-                writeCheckpointIfNeed();\n+            case SUSPENDED:\n+                maybeWriteCheckpoint();\n \n                 break;\n \n             case RUNNING:\n-                if (!eosEnabled) { // if RUNNING, checkpoint only for non-eos\n-                    writeCheckpointIfNeed();\n+                if (!eosEnabled) {\n+                    maybeWriteCheckpoint();\n                 }\n \n                 break;\n \n-            case SUSPENDED:\n-                writeCheckpointIfNeed();\n-                // we cannot `clear()` the `PartitionGroup` in `suspend()` already, but only after committing,\n-                // because otherwise we loose the partition-time information\n-                partitionGroup.clear();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NjAyNA=="}, "originalCommit": null, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTExNDc1Nw==", "bodyText": "Ah that seems right. Thanks!", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441114757", "createdAt": "2020-06-16T20:14:24Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -409,32 +407,28 @@ public void resume() {\n         return committableOffsets;\n     }\n \n+    /**\n+     * This should only be called if the attempted commit succeeded for this task\n+     */\n     @Override\n     public void postCommit() {\n         commitRequested = false;\n         commitNeeded = false;\n \n         switch (state()) {\n             case RESTORING:\n-                writeCheckpointIfNeed();\n+            case SUSPENDED:\n+                maybeWriteCheckpoint();\n \n                 break;\n \n             case RUNNING:\n-                if (!eosEnabled) { // if RUNNING, checkpoint only for non-eos\n-                    writeCheckpointIfNeed();\n+                if (!eosEnabled) {\n+                    maybeWriteCheckpoint();\n                 }\n \n                 break;\n \n-            case SUSPENDED:\n-                writeCheckpointIfNeed();\n-                // we cannot `clear()` the `PartitionGroup` in `suspend()` already, but only after committing,\n-                // because otherwise we loose the partition-time information\n-                partitionGroup.clear();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NjAyNA=="}, "originalCommit": null, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzgyNzAyOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoxMjo1MVrOGknZQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoxMjo1MVrOGknZQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0NzM2MA==", "bodyText": "As we always suspend a task before closing (even for unclean closing), I think we can actually remove this call? (We only needed it before, because SUSPEND could be skipped.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441047360", "createdAt": "2020-06-16T18:12:51Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -474,26 +468,24 @@ public void update(final Set<TopicPartition> topicPartitions, final Map<String,\n \n     @Override\n     public void closeAndRecycleState() {\n-        suspend();\n-        prepareCommit();\n-        writeCheckpointIfNeed();\n-\n         switch (state()) {\n-            case CREATED:\n             case SUSPENDED:\n                 stateMgr.recycle();\n                 recordCollector.close();\n \n                 break;\n \n-            case RESTORING: // we should have transitioned to `SUSPENDED` already\n-            case RUNNING: // we should have transitioned to `SUSPENDED` already\n+            case CREATED:\n+            case RESTORING:\n+            case RUNNING:\n             case CLOSED:\n                 throw new IllegalStateException(\"Illegal state \" + state() + \" while recycling active task \" + id);\n             default:\n                 throw new IllegalStateException(\"Unknown state \" + state() + \" while recycling active task \" + id);\n         }\n \n+        // we cannot `clear()` the `PartitionGroup` in `suspend()` already, but only after committing,\n+        // because otherwise we loose the partition-time information\n         partitionGroup.clear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzgzNzk3OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoxNjowOFrOGkngcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxOTowOTo1NFrOGkpeZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0OTIwMA==", "bodyText": "Seems we call stateMgr.checkpoint unconditionally now. Should we rename this this writeCheckpoint ? Or even remove it all together as we if (commitNeeded) check is \"just\" a guard and the method is a single liner now?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441049200", "createdAt": "2020-06-16T18:16:08Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -502,56 +494,24 @@ public void closeAndRecycleState() {\n         log.info(\"Closed clean and recycled state\");\n     }\n \n-    private void maybeScheduleCheckpoint() {\n-        switch (state()) {\n-            case RESTORING:\n-            case SUSPENDED:\n-                this.checkpoint = checkpointableOffsets();\n-\n-                break;\n-\n-            case RUNNING:\n-                if (!eosEnabled) {\n-                    this.checkpoint = checkpointableOffsets();\n-                }\n-\n-                break;\n-\n-            case CREATED:\n-            case CLOSED:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while scheduling checkpoint for active task \" + id);\n-\n-            default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while scheduling checkpoint for active task \" + id);\n-        }\n-    }\n-\n-    private void writeCheckpointIfNeed() {\n+    private void maybeWriteCheckpoint() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA4MTQ0Ng==", "bodyText": "Well, the commitNeeded guard is a good idea imo. But I agree we should rename this", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441081446", "createdAt": "2020-06-16T19:09:54Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -502,56 +494,24 @@ public void closeAndRecycleState() {\n         log.info(\"Closed clean and recycled state\");\n     }\n \n-    private void maybeScheduleCheckpoint() {\n-        switch (state()) {\n-            case RESTORING:\n-            case SUSPENDED:\n-                this.checkpoint = checkpointableOffsets();\n-\n-                break;\n-\n-            case RUNNING:\n-                if (!eosEnabled) {\n-                    this.checkpoint = checkpointableOffsets();\n-                }\n-\n-                break;\n-\n-            case CREATED:\n-            case CLOSED:\n-                throw new IllegalStateException(\"Illegal state \" + state() + \" while scheduling checkpoint for active task \" + id);\n-\n-            default:\n-                throw new IllegalStateException(\"Unknown state \" + state() + \" while scheduling checkpoint for active task \" + id);\n-        }\n-    }\n-\n-    private void writeCheckpointIfNeed() {\n+    private void maybeWriteCheckpoint() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA0OTIwMA=="}, "originalCommit": null, "originalPosition": 147}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0Nzg4MTA1OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoyNDoyN1rOGkn7lQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODozMDo1NVrOGkoKGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA1NjE0OQ==", "bodyText": "Is this necessarily a warning? A wall-clock-time punctuation could have set commitNeeded to true?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441056149", "createdAt": "2020-06-16T18:24:27Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -465,44 +429,82 @@ boolean tryToCompleteRestoration() {\n     }\n \n     /**\n+     * Handle the revoked partitions and prepare for closing the associated tasks in {@link #handleAssignment(Map, Map)}\n+     * We should commit the revoked tasks now as we will not officially own them anymore when {@link #handleAssignment(Map, Map)}\n+     * is called. Note that only active task partitions are passed in from the rebalance listener, so we only need to\n+     * consider/commit active tasks here\n+     *\n+     * If eos-beta is used, we must commit ALL tasks. Otherwise, we can just commit those (active) tasks which are revoked\n+     *\n      * @throws TaskMigratedException if the task producer got fenced (EOS only)\n      */\n     void handleRevocation(final Collection<TopicPartition> revokedPartitions) {\n-        final Set<TopicPartition> remainingPartitions = new HashSet<>(revokedPartitions);\n+        final Set<TopicPartition> remainingRevokedPartitions = new HashSet<>(revokedPartitions);\n \n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        for (final Task task : tasks.values()) {\n-            if (remainingPartitions.containsAll(task.inputPartitions())) {\n-                task.suspend();\n-                final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+        final Set<Task> tasksToCommit = new HashSet<>();\n+        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n \n-                if (!committableOffsets.isEmpty()) {\n-                    consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+        for (final Task task : activeTaskIterable()) {\n+            if (remainingRevokedPartitions.containsAll(task.inputPartitions())) {\n+                try {\n+                    task.suspend();\n+                    if (task.commitNeeded()) {\n+                        tasksToCommit.add(task);\n+                    }\n+                } catch (final RuntimeException e) {\n+                    log.error(\"Caught the following exception while trying to suspend revoked task \" + task.id(), e);\n+                    firstException.compareAndSet(null, new StreamsException(\"Failed to suspend \" + task.id(), e));\n                 }\n-            } else if (task.isActive() && task.commitNeeded()) {\n-                final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+            } else if (task.commitNeeded()) {\n+                additionalTasksForCommitting.add(task);\n+            }\n+            remainingRevokedPartitions.removeAll(task.inputPartitions());\n+        }\n \n-                if (!committableOffsets.isEmpty()) {\n-                    consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                }\n+        if (!remainingRevokedPartitions.isEmpty()) {\n+            log.warn(\"The following partitions {} are missing from the task partitions. It could potentially \" +\n+                         \"due to race condition of consumer detecting the heartbeat failure, or the tasks \" +\n+                         \"have been cleaned up by the handleAssignment callback.\", remainingRevokedPartitions);\n+        }\n+\n+        final RuntimeException suspendException = firstException.get();\n+        if (suspendException != null) {\n+            throw suspendException;\n+        }\n+\n+        // If using eos-beta, if we must commit any task then we must commit all of them\n+        // TODO: when KAFKA-9450 is done this will be less expensive, and we can simplify by always committing everything\n+        if (processingMode ==  EXACTLY_ONCE_BETA && !tasksToCommit.isEmpty()) {\n+            tasksToCommit.addAll(additionalTasksForCommitting);\n+        }\n+\n+        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n+        for (final Task task : tasksToCommit) {\n+            final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+            if (!committableOffsets.isEmpty()) {\n+                consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+            } else {\n+                log.warn(\"Task {} claimed to need a commit but had no committable consumed offsets\", task.id());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA1OTg2NQ==", "bodyText": "Oh good point. I'll remove this", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441059865", "createdAt": "2020-06-16T18:30:55Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -465,44 +429,82 @@ boolean tryToCompleteRestoration() {\n     }\n \n     /**\n+     * Handle the revoked partitions and prepare for closing the associated tasks in {@link #handleAssignment(Map, Map)}\n+     * We should commit the revoked tasks now as we will not officially own them anymore when {@link #handleAssignment(Map, Map)}\n+     * is called. Note that only active task partitions are passed in from the rebalance listener, so we only need to\n+     * consider/commit active tasks here\n+     *\n+     * If eos-beta is used, we must commit ALL tasks. Otherwise, we can just commit those (active) tasks which are revoked\n+     *\n      * @throws TaskMigratedException if the task producer got fenced (EOS only)\n      */\n     void handleRevocation(final Collection<TopicPartition> revokedPartitions) {\n-        final Set<TopicPartition> remainingPartitions = new HashSet<>(revokedPartitions);\n+        final Set<TopicPartition> remainingRevokedPartitions = new HashSet<>(revokedPartitions);\n \n-        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n-        for (final Task task : tasks.values()) {\n-            if (remainingPartitions.containsAll(task.inputPartitions())) {\n-                task.suspend();\n-                final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+        final Set<Task> tasksToCommit = new HashSet<>();\n+        final Set<Task> additionalTasksForCommitting = new HashSet<>();\n \n-                if (!committableOffsets.isEmpty()) {\n-                    consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+        final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n+        for (final Task task : activeTaskIterable()) {\n+            if (remainingRevokedPartitions.containsAll(task.inputPartitions())) {\n+                try {\n+                    task.suspend();\n+                    if (task.commitNeeded()) {\n+                        tasksToCommit.add(task);\n+                    }\n+                } catch (final RuntimeException e) {\n+                    log.error(\"Caught the following exception while trying to suspend revoked task \" + task.id(), e);\n+                    firstException.compareAndSet(null, new StreamsException(\"Failed to suspend \" + task.id(), e));\n                 }\n-            } else if (task.isActive() && task.commitNeeded()) {\n-                final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+            } else if (task.commitNeeded()) {\n+                additionalTasksForCommitting.add(task);\n+            }\n+            remainingRevokedPartitions.removeAll(task.inputPartitions());\n+        }\n \n-                if (!committableOffsets.isEmpty()) {\n-                    consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n-                }\n+        if (!remainingRevokedPartitions.isEmpty()) {\n+            log.warn(\"The following partitions {} are missing from the task partitions. It could potentially \" +\n+                         \"due to race condition of consumer detecting the heartbeat failure, or the tasks \" +\n+                         \"have been cleaned up by the handleAssignment callback.\", remainingRevokedPartitions);\n+        }\n+\n+        final RuntimeException suspendException = firstException.get();\n+        if (suspendException != null) {\n+            throw suspendException;\n+        }\n+\n+        // If using eos-beta, if we must commit any task then we must commit all of them\n+        // TODO: when KAFKA-9450 is done this will be less expensive, and we can simplify by always committing everything\n+        if (processingMode ==  EXACTLY_ONCE_BETA && !tasksToCommit.isEmpty()) {\n+            tasksToCommit.addAll(additionalTasksForCommitting);\n+        }\n+\n+        final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n+        for (final Task task : tasksToCommit) {\n+            final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+            if (!committableOffsets.isEmpty()) {\n+                consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+            } else {\n+                log.warn(\"Task {} claimed to need a commit but had no committable consumed offsets\", task.id());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA1NjE0OQ=="}, "originalCommit": null, "originalPosition": 192}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0Nzg5ODk2OnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxODoyOTozN1rOGkoHHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxOTo0ODoxOVrOGkqteA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA1OTEwMA==", "bodyText": "Thinking about punctuation, should we actually call commitOffsetsOrTransaction() unconditionally (ie, not consider if consumedOffsetsAndMetadataPerTask is empty or not?\nWe can still move the check inside consumedOffsetsAndMetadataPerTask, but for EOS there might pending writes from punctuation that we still need to commit?\nThis would apply to all calls of commitOffsetsOrTransaction ?", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441059100", "createdAt": "2020-06-16T18:29:37Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -714,13 +717,26 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n-        if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        try {\n+            if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 249}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA4MTE3Nw==", "bodyText": "Hm. So in the punctuation case -- where commitNeeded is true but consumedOffsets is empty -- we still need to call commitOffsetsOrTransaction (and postCommit) because the punctuation may for example write to a state store and generate changelog records. So we would need to commit that transaction, and also write the checkpoint file.\nMakes sense", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441081177", "createdAt": "2020-06-16T19:09:27Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -714,13 +717,26 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n-        if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        try {\n+            if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA1OTEwMA=="}, "originalCommit": null, "originalPosition": 249}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA5MjM4Mg==", "bodyText": "Committable offsets here should contain consumed offsets, and punctuation itself should never update those consumed offsets right?\nI think we can skip the call if consumedOffsetsAndMetadataPerTask is empty.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441092382", "createdAt": "2020-06-16T19:30:25Z", "author": {"login": "guozhangwang"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -714,13 +717,26 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n-        if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        try {\n+            if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA1OTEwMA=="}, "originalCommit": null, "originalPosition": 249}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTEwMTY4OA==", "bodyText": "Committable offsets here should contain consumed offsets, and punctuation itself should never update those consumed offsets right\n\nYes.\n\nI think we can skip the call if consumedOffsetsAndMetadataPerTask is empty.\n\nFor non-eos, yes, because for non-eos commitOffsetsOrTransaction() would only commit offsets via the consumer (this can be skipped if empty). However, for eos (alpha and beta), we might have a pending transaction that we need to commit on the producer, too.", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441101688", "createdAt": "2020-06-16T19:48:19Z", "author": {"login": "mjsax"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -714,13 +717,26 @@ void shutdown(final boolean clean) {\n             }\n         }\n \n-        if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+        try {\n+            if (clean && !consumedOffsetsAndMetadataPerTask.isEmpty()) {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTA1OTEwMA=="}, "originalCommit": null, "originalPosition": 249}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0ODI0NzYwOnYy", "diffSide": "RIGHT", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQyMDoxNzowNlrOGkrl7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQyMDoxNzowNlrOGkrl7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTExNjE0MQ==", "bodyText": "As @mjsax pointed out, we should still commit even if there are no consumed offsets. However, we should not commit the offsets/transaction if there are no active tasks that need committing", "url": "https://github.com/apache/kafka/pull/8856#discussion_r441116141", "createdAt": "2020-06-16T20:17:06Z", "author": {"login": "ableegoldman"}, "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -690,18 +686,21 @@ void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n         final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCommit = new HashSet<>();\n         final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n \n         for (final Task task : tasks.values()) {\n             if (clean) {\n                 try {\n                     task.suspend();\n-                    final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n-\n-                    tasksToClose.add(task);\n-                    if (!committableOffsets.isEmpty()) {\n-                        consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n+                    if (task.commitNeeded()) {\n+                        tasksToCommit.add(task);\n+                        final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+                        if (task.isActive()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2529fb91b156a520ceb4178665ab313051f68cfd"}, "originalPosition": 238}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2370, "cost": 1, "resetAt": "2021-11-12T11:18:39Z"}}}