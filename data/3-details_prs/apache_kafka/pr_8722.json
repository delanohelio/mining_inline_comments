{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIyMzUyMDMz", "number": 8722, "title": "KAFKA-5295: Allow source connectors to specify topic-specific settings for new topics (KIP-158)", "bodyText": "Kafka Connect workers have been able to create Connect's internal topics using the new admin client for some time now (see KAFKA-4667). However, tasks of source connectors are still relying upon the broker to auto-create topics with default config settings if they don't exist, or expect these topics to exist before the connector is deployed, if their configuration needs to be specialized.\nWith the implementation of KIP-158 here, if topic.creation.enable=true, Kafka Connect will supply the source tasks of connectors that are configured to create topics with an admin client that will allow them to create new topics on-the-fly before writing the first source records to a new topic. Additionally, each source connector has the opportunity to customize the topic-specific settings of these new topics by defining groups of topic configurations.\nThis feature is tested here via unit tests (old tests that have been adjusted and new ones) as well as integration tests.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)", "createdAt": "2020-05-24T02:09:50Z", "url": "https://github.com/apache/kafka/pull/8722", "merged": true, "mergeCommit": {"oid": "371f14c3c12d2e341ac96bd52393b43a10acfa84"}, "closed": true, "closedAt": "2020-05-27T05:07:35Z", "author": {"login": "kkonstantine"}, "timelineItems": {"totalCount": 40, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABckdI2QABqjMzNjgwNDMzNTE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABclOhSzgH2gAyNDIyMzUyMDMzOjk4NTA1MzVkMWE0MjkxNjc5YmE2NjY3OGFjMzgwNGQ2OWYxNWFlYzU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3MzcwOTkw", "url": "https://github.com/apache/kafka/pull/8722#pullrequestreview-417370990", "createdAt": "2020-05-24T16:01:49Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNFQxNjowMTo0OVrOGZvzxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNFQxNjowMTo0OVrOGZvzxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1MDg4Nw==", "bodyText": "These had to be moved out of this class, or else tests for StandaloneHerder would break. They are now reused here as well as TopicCreationConfig", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429650887", "createdAt": "2020-05-24T16:01:49Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedConfig.java", "diffHunk": "@@ -193,15 +192,6 @@\n     public static final String INTER_WORKER_VERIFICATION_ALGORITHMS_DOC = \"A list of permitted algorithms for verifying internal requests\";\n     public static final List<String> INTER_WORKER_VERIFICATION_ALGORITHMS_DEFAULT = Collections.singletonList(INTER_WORKER_SIGNATURE_ALGORITHM_DEFAULT);\n \n-    private static final Validator REPLICATION_FACTOR_VALIDATOR = LambdaValidator.with(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 27}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3Mzc1Njc1", "url": "https://github.com/apache/kafka/pull/8722#pullrequestreview-417375675", "createdAt": "2020-05-24T17:13:40Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNFQxNzoxMzo0MFrOGZwKXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNFQxODoxMzo0N1rOGZwcpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1NjY3MA==", "bodyText": "Since these are the same for all topicCreationGroups, WDYT about pulling these out of this lambda? I think it would help with readability.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429656670", "createdAt": "2020-05-24T17:13:40Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SourceConnectorConfig.java", "diffHunk": "@@ -16,21 +16,153 @@\n  */\n package org.apache.kafka.connect.runtime;\n \n+import org.apache.kafka.common.config.AbstractConfig;\n import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n import org.apache.kafka.connect.runtime.isolation.Plugins;\n \n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_GROUP;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_PREFIX;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.EXCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.INCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.PARTITIONS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.REPLICATION_FACTOR_CONFIG;\n \n public class SourceConnectorConfig extends ConnectorConfig {\n \n-    private static ConfigDef config = ConnectorConfig.configDef();\n+    protected static final String TOPIC_CREATION_GROUP = \"Topic Creation\";\n+\n+    public static final String TOPIC_CREATION_PREFIX = \"topic.creation.\";\n+\n+    public static final String TOPIC_CREATION_GROUPS_CONFIG = TOPIC_CREATION_PREFIX + \"groups\";\n+    private static final String TOPIC_CREATION_GROUPS_DOC = \"Groups of configurations for topics \"\n+            + \"created by source connectors\";\n+    private static final String TOPIC_CREATION_GROUPS_DISPLAY = \"Topic Creation Groups\";\n+\n+    private static class EnrichedSourceConnectorConfig extends AbstractConfig {\n+        EnrichedSourceConnectorConfig(ConfigDef configDef, Map<String, String> props) {\n+            super(configDef, props);\n+        }\n+\n+        @Override\n+        public Object get(String key) {\n+            return super.get(key);\n+        }\n+    }\n+\n+    private static ConfigDef config = SourceConnectorConfig.configDef();\n+    private final EnrichedSourceConnectorConfig enrichedSourceConfig;\n \n     public static ConfigDef configDef() {\n-        return config;\n+        int orderInGroup = 0;\n+        return new ConfigDef(ConnectorConfig.configDef())\n+                .define(TOPIC_CREATION_GROUPS_CONFIG, ConfigDef.Type.LIST, Collections.emptyList(),\n+                        ConfigDef.CompositeValidator.of(new ConfigDef.NonNullValidator(), ConfigDef.LambdaValidator.with(\n+                            (name, value) -> {\n+                                List<?> groupAliases = (List<?>) value;\n+                                if (groupAliases.size() > new HashSet<>(groupAliases).size()) {\n+                                    throw new ConfigException(name, value, \"Duplicate alias provided.\");\n+                                }\n+                            },\n+                            () -> \"unique topic creation groups\")),\n+                        ConfigDef.Importance.LOW, TOPIC_CREATION_GROUPS_DOC, TOPIC_CREATION_GROUP,\n+                        ++orderInGroup, ConfigDef.Width.LONG, TOPIC_CREATION_GROUPS_DISPLAY);\n     }\n \n-    public SourceConnectorConfig(Plugins plugins, Map<String, String> props) {\n+    public static ConfigDef embedDefaultGroup(ConfigDef baseConfigDef) {\n+        String defaultGroup = \"default\";\n+        ConfigDef newDefaultDef = new ConfigDef(baseConfigDef);\n+        newDefaultDef.embed(DEFAULT_TOPIC_CREATION_PREFIX, defaultGroup, 0, TopicCreationConfig.defaultGroupConfigDef());\n+        return newDefaultDef;\n+    }\n+\n+    /**\n+     * Returns an enriched {@link ConfigDef} building upon the {@code ConfigDef}, using the current configuration specified in {@code props} as an input.\n+     *\n+     * @param baseConfigDef\n+     * @param props\n+     * @return the enriched configuration definition\n+     */\n+    public static ConfigDef enrich(ConfigDef baseConfigDef, Map<String, String> props, AbstractConfig defaultGroupConfig) {\n+        List<Object> topicCreationGroups = new ArrayList<>();\n+        Object aliases = ConfigDef.parseType(TOPIC_CREATION_GROUPS_CONFIG, props.get(TOPIC_CREATION_GROUPS_CONFIG), ConfigDef.Type.LIST);\n+        if (aliases instanceof List) {\n+            topicCreationGroups.addAll((List<?>) aliases);\n+        }\n+\n+        ConfigDef newDef = new ConfigDef(baseConfigDef);\n+        String defaultGroupPrefix = TOPIC_CREATION_PREFIX + DEFAULT_TOPIC_CREATION_GROUP + \".\";\n+        topicCreationGroups.stream().distinct().forEach(group -> {\n+            if (!(group instanceof String)) {\n+                throw new ConfigException(\"Item in \" + TOPIC_CREATION_GROUPS_CONFIG + \" property is not of type String\");\n+            }\n+            String alias = (String) group;\n+            String prefix = TOPIC_CREATION_PREFIX + alias + \".\";\n+            String configGroup = TOPIC_CREATION_GROUP + \": \" + alias;\n+            newDef.embed(prefix, configGroup, 0, TopicCreationConfig.configDef(\n+                    configGroup,\n+                    defaultGroupConfig.getShort(defaultGroupPrefix + REPLICATION_FACTOR_CONFIG),\n+                    defaultGroupConfig.getInt(defaultGroupPrefix + PARTITIONS_CONFIG)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1NzE4MQ==", "bodyText": "I know you're following the text that was in the KIP, which made sense because $alias appeared in the property key rule in the table. However, as a user the appearance of $alias here would be really hard to follow. I'd argue since we're dynamically adding configkeys for all topic creation group aliases and therefore the doc will apply to a specific include key (e.g., topic.creation.rule1.include), we should remove this sentence from the documentation of each of the generated configkeys.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        + \"that match this inclusion list. $alias applies to any group defined in topic\"\n          \n          \n            \n                        + \".creation.groups but not the default\";\n          \n          \n            \n                        + \"that match this inclusion list.\";\n          \n      \n    \n    \n  \n\nSame with the other documentation constants in this class.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429657181", "createdAt": "2020-05-24T17:19:39Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+\n+import java.util.Collections;\n+\n+public class TopicCreationConfig {\n+\n+    public static final String DEFAULT_TOPIC_CREATION_PREFIX = \"topic.creation.default.\";\n+    public static final String DEFAULT_TOPIC_CREATION_GROUP = \"default\";\n+\n+    public static final String INCLUDE_REGEX_CONFIG = \"include\";\n+    private static final String INCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to include topics that \"\n+            + \"match their values and apply this group's specific configuration to the topics \"\n+            + \"that match this inclusion list. $alias applies to any group defined in topic\"\n+            + \".creation.groups but not the default\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1NzkzMg==", "bodyText": "Are you keeping this blank for backward compatibility?", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429657932", "createdAt": "2020-05-24T17:28:21Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "diffHunk": "@@ -678,7 +701,8 @@ ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) {\n         if (topic != null && !topic.isEmpty()) {\n             Map<String, Object> producerProps = producerConfigs(id, \"connector-dlq-producer-\" + id, config, connConfig, connectorClass,\n                                                                 connectorClientConfigOverridePolicy);\n-            Map<String, Object> adminProps = adminConfigs(id, config, connConfig, connectorClass, connectorClientConfigOverridePolicy);\n+            // Leaving default client id empty means that the admin client will set the default at instantiation time\n+            Map<String, Object> adminProps = adminConfigs(id, \"\", config, connConfig, connectorClass, connectorClientConfigOverridePolicy);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1ODI2MA==", "bodyText": "This seems to not match the proposed behavior, specifically with respect to source connectors having to opt in by setting at least topic.creation.default.replication.factor and topic.creation.default.partitions.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429658260", "createdAt": "2020-05-24T17:32:35Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfig.java", "diffHunk": "@@ -250,9 +250,16 @@\n             + \"user requests to reset the set of active topics per connector.\";\n     protected static final boolean TOPIC_TRACKING_ALLOW_RESET_DEFAULT = true;\n \n+    public static final String TOPIC_CREATION_ENABLE_CONFIG = \"topic.creation.enable\";\n+    protected static final String TOPIC_CREATION_ENABLE_DOC = \"If set to true, it allows \"\n+            + \"source connectors to create topics with custom settings. If enabled, each connector \"\n+            + \"task will use an admin clients to create its topics and will not depend on \"\n+            + \"auto.create.topics.enable being set on Kafka brokers.\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1ODcxMA==", "bodyText": "Why not use the new method on WorkerConfig instead?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    return config.getBoolean(TOPIC_CREATION_ENABLE_CONFIG);\n          \n          \n            \n                    return config.topicCreationEnable();", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429658710", "createdAt": "2020-05-24T17:38:49Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "diffHunk": "@@ -802,6 +826,16 @@ public String workerId() {\n         return workerId;\n     }\n \n+    /**\n+     * Returns whether this worker is configured to allow source connectors to create the topics\n+     * that they use with custom configurations, if these topics don't already exist.\n+     *\n+     * @return true if topic creation by source connectors is allowed; false otherwise\n+     */\n+    public boolean isTopicCreationEnabled() {\n+        return config.getBoolean(TOPIC_CREATION_ENABLE_CONFIG);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1ODc3MQ==", "bodyText": "Use the WorkerConfig method:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    this.isTopicCreationEnabled =\n          \n          \n            \n                            workerConfig.getBoolean(TOPIC_CREATION_ENABLE_CONFIG) && topicGroups != null;\n          \n          \n            \n                    this.isTopicCreationEnabled = workerConfig.topicCreationEnable() && topicGroups != null;", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429658771", "createdAt": "2020-05-24T17:39:56Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -142,6 +159,18 @@ public WorkerSourceTask(ConnectorTaskId id,\n         this.sourceTaskMetricsGroup = new SourceTaskMetricsGroup(id, connectMetrics);\n         this.producerSendException = new AtomicReference<>();\n         this.isTopicTrackingEnabled = workerConfig.getBoolean(TOPIC_TRACKING_ENABLE_CONFIG);\n+        this.isTopicCreationEnabled =\n+                workerConfig.getBoolean(TOPIC_CREATION_ENABLE_CONFIG) && topicGroups != null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1OTExNQ==", "bodyText": "This seems like maybe it might be worth encapsulating a lot of this logic -- and even some of the logic in maybeCreateGroup -- in a class that maybe can be more easily tested.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429659115", "createdAt": "2020-05-24T17:43:43Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -142,6 +159,18 @@ public WorkerSourceTask(ConnectorTaskId id,\n         this.sourceTaskMetricsGroup = new SourceTaskMetricsGroup(id, connectMetrics);\n         this.producerSendException = new AtomicReference<>();\n         this.isTopicTrackingEnabled = workerConfig.getBoolean(TOPIC_TRACKING_ENABLE_CONFIG);\n+        this.isTopicCreationEnabled =\n+                workerConfig.getBoolean(TOPIC_CREATION_ENABLE_CONFIG) && topicGroups != null;\n+        if (isTopicCreationEnabled) {\n+            this.defaultTopicGroup = topicGroups.get(DEFAULT_TOPIC_CREATION_GROUP);\n+            this.topicGroups = new LinkedHashMap<>(topicGroups);\n+            this.topicGroups.remove(DEFAULT_TOPIC_CREATION_GROUP);\n+            this.topicCache = new HashSet<>();\n+        } else {\n+            this.defaultTopicGroup = null;\n+            this.topicGroups = Collections.emptyMap();\n+            this.topicCache = Collections.emptySet();\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1OTUxMQ==", "bodyText": "WDYT about including the name of the topic creation group that was used (it's only included in debug logs so far) and the actual topic settings used to create the topic? Something like:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        log.info(\"Created topic '{}'\", newTopic);\n          \n          \n            \n                        log.info(\"Created topic '{}' using creation group '{}' and topic settings {}\", newTopic, topicGroup, topicGroup.topicSettings());\n          \n      \n    \n    \n  \n\nif the NewTopicCreationGroup.toString() method output the name only, or\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        log.info(\"Created topic '{}'\", newTopic);\n          \n          \n            \n                        log.info(\"Created topic '{}' using creation group {}\", newTopic, topicGroup);\n          \n      \n    \n    \n  \n\nif the NewTopicCreationGroup.toString() method output the name and topic settings.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429659511", "createdAt": "2020-05-24T17:49:36Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -384,6 +417,40 @@ public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n         return true;\n     }\n \n+    // Due to transformations that may change the destination topic of a record (such as\n+    // RegexRouter) topic creation can not be batched for multiple topics\n+    private void maybeCreateTopic(String topic) {\n+        if (!isTopicCreationEnabled || topicCache.contains(topic)) {\n+            return;\n+        }\n+        log.info(\"The task will send records to topic '{}' for the first time. Checking \"\n+                + \"whether topic exists\", topic);\n+        Map<String, TopicDescription> existing = admin.describeTopics(topic);\n+        if (!existing.isEmpty()) {\n+            log.info(\"Topic '{}' already exists.\", topic);\n+            topicCache.add(topic);\n+            return;\n+        }\n+\n+        log.info(\"Creating topic '{}'\", topic);\n+        NewTopicCreationGroup topicGroup = topicGroups.values().stream()\n+                .filter(group -> group.matches(topic))\n+                .findFirst()\n+                .orElse(defaultTopicGroup);\n+        log.debug(\"Topic '{}' matched topic creation group: {}\", topic, topicGroup);\n+        NewTopic newTopic = topicGroup.newTopic(topic);\n+\n+        if (admin.createTopic(newTopic)) {\n+            topicCache.add(topic);\n+            log.info(\"Created topic '{}'\", newTopic);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 205}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1OTU3Ng==", "bodyText": "Need a NewTopicCreationGroup.toString() implementation for this to work", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429659576", "createdAt": "2020-05-24T17:50:12Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -384,6 +417,40 @@ public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n         return true;\n     }\n \n+    // Due to transformations that may change the destination topic of a record (such as\n+    // RegexRouter) topic creation can not be batched for multiple topics\n+    private void maybeCreateTopic(String topic) {\n+        if (!isTopicCreationEnabled || topicCache.contains(topic)) {\n+            return;\n+        }\n+        log.info(\"The task will send records to topic '{}' for the first time. Checking \"\n+                + \"whether topic exists\", topic);\n+        Map<String, TopicDescription> existing = admin.describeTopics(topic);\n+        if (!existing.isEmpty()) {\n+            log.info(\"Topic '{}' already exists.\", topic);\n+            topicCache.add(topic);\n+            return;\n+        }\n+\n+        log.info(\"Creating topic '{}'\", topic);\n+        NewTopicCreationGroup topicGroup = topicGroups.values().stream()\n+                .filter(group -> group.matches(topic))\n+                .findFirst()\n+                .orElse(defaultTopicGroup);\n+        log.debug(\"Topic '{}' matched topic creation group: {}\", topic, topicGroup);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 200}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1OTcxNQ==", "bodyText": "This class needs a toString() method since instances of this class are used in log statements.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429659715", "createdAt": "2020-05-24T17:52:24Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java", "diffHunk": "@@ -178,6 +191,47 @@ public NewTopic build() {\n         }\n     }\n \n+    public static class NewTopicCreationGroup {\n+        private final Pattern inclusionPattern;\n+        private final Pattern exclusionPattern;\n+        private final int numPartitions;\n+        private final short replicationFactor;\n+        private final Map<String, Object> otherConfigs;\n+\n+        protected NewTopicCreationGroup(String group, SourceConnectorConfig config) {\n+            inclusionPattern = Pattern.compile(String.join(\"|\", config.topicCreationInclude(group)));\n+            exclusionPattern = Pattern.compile(String.join(\"|\", config.topicCreationExclude(group)));\n+            numPartitions = config.topicCreationPartitions(group);\n+            replicationFactor = config.topicCreationReplicationFactor(group);\n+            otherConfigs = config.topicCreationOtherConfigs(group);\n+        }\n+\n+        public boolean matches(String topic) {\n+            return !exclusionPattern.matcher(topic).matches() && inclusionPattern.matcher(topic).matches();\n+        }\n+\n+        public NewTopic newTopic(String topic) {\n+            NewTopicBuilder builder = new NewTopicBuilder(topic);\n+            return builder.partitions(numPartitions)\n+                    .replicationFactor(replicationFactor)\n+                    .config(otherConfigs)\n+                    .build();\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY2MDAyMg==", "bodyText": "I guess we're lucky here that topic names can't have , characters and therefore the regex can be split by , as part of the topic.creation.<alias>.include and topic.creation.<alias>.exclude properties being parsed as lists, and then recombined into a single regex.\nWould it be simpler to use a single regex for each of the include and exclude properties rather than lists? Would that be safer -- and maybe a bit simpler UX and implementation -- than the current proposal?", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429660022", "createdAt": "2020-05-24T17:56:18Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java", "diffHunk": "@@ -178,6 +191,47 @@ public NewTopic build() {\n         }\n     }\n \n+    public static class NewTopicCreationGroup {\n+        private final Pattern inclusionPattern;\n+        private final Pattern exclusionPattern;\n+        private final int numPartitions;\n+        private final short replicationFactor;\n+        private final Map<String, Object> otherConfigs;\n+\n+        protected NewTopicCreationGroup(String group, SourceConnectorConfig config) {\n+            inclusionPattern = Pattern.compile(String.join(\"|\", config.topicCreationInclude(group)));\n+            exclusionPattern = Pattern.compile(String.join(\"|\", config.topicCreationExclude(group)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY2MDUzOQ==", "bodyText": "Why are we setting these and not relying upon the defaults? Is there value in explicitly setting these in this test?", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429660539", "createdAt": "2020-05-24T18:03:49Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskWithTopicCreationTest.java", "diffHunk": "@@ -0,0 +1,654 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.utils.MockTime;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.connect.connector.ConnectRecord;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaBuilder;\n+import org.apache.kafka.connect.data.Struct;\n+import org.apache.kafka.connect.errors.RetriableException;\n+import org.apache.kafka.connect.integration.MonitorableSourceConnector;\n+import org.apache.kafka.connect.json.JsonConverter;\n+import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;\n+import org.apache.kafka.connect.runtime.errors.ErrorHandlingMetrics;\n+import org.apache.kafka.connect.runtime.errors.ErrorReporter;\n+import org.apache.kafka.connect.runtime.errors.LogReporter;\n+import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;\n+import org.apache.kafka.connect.runtime.errors.ToleranceType;\n+import org.apache.kafka.connect.runtime.isolation.PluginClassLoader;\n+import org.apache.kafka.connect.runtime.isolation.Plugins;\n+import org.apache.kafka.connect.runtime.standalone.StandaloneConfig;\n+import org.apache.kafka.connect.sink.SinkConnector;\n+import org.apache.kafka.connect.sink.SinkRecord;\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.source.SourceRecord;\n+import org.apache.kafka.connect.source.SourceTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.apache.kafka.connect.storage.OffsetStorageReaderImpl;\n+import org.apache.kafka.connect.storage.OffsetStorageWriter;\n+import org.apache.kafka.connect.storage.StatusBackingStore;\n+import org.apache.kafka.connect.storage.StringConverter;\n+import org.apache.kafka.connect.transforms.Transformation;\n+import org.apache.kafka.connect.transforms.util.SimpleConfig;\n+import org.apache.kafka.connect.util.ConnectorTaskId;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+import org.easymock.Capture;\n+import org.easymock.EasyMock;\n+import org.easymock.IExpectationSetters;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.powermock.api.easymock.PowerMock;\n+import org.powermock.api.easymock.annotation.Mock;\n+import org.powermock.core.classloader.annotations.PowerMockIgnore;\n+import org.powermock.core.classloader.annotations.PrepareForTest;\n+import org.powermock.modules.junit4.PowerMockRunner;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.time.Duration;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static java.util.Collections.emptyMap;\n+import static java.util.Collections.singletonList;\n+import static org.apache.kafka.common.utils.Time.SYSTEM;\n+import static org.apache.kafka.connect.integration.MonitorableSourceConnector.TOPIC_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.CONNECTOR_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.KEY_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.TASKS_MAX_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.VALUE_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.SourceConnectorConfig.TOPIC_CREATION_GROUPS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_PREFIX;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.INCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.PARTITIONS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.connect.runtime.WorkerConfig.TOPIC_CREATION_ENABLE_CONFIG;\n+import static org.junit.Assert.assertEquals;\n+\n+@RunWith(PowerMockRunner.class)\n+@PrepareForTest({WorkerSinkTask.class, WorkerSourceTask.class})\n+@PowerMockIgnore(\"javax.management.*\")\n+public class ErrorHandlingTaskWithTopicCreationTest {\n+\n+    private static final String TOPIC = \"test\";\n+    private static final int PARTITION1 = 12;\n+    private static final int PARTITION2 = 13;\n+    private static final long FIRST_OFFSET = 45;\n+\n+    @Mock Plugins plugins;\n+\n+    private static final Map<String, String> TASK_PROPS = new HashMap<>();\n+\n+    static {\n+        TASK_PROPS.put(SinkConnector.TOPICS_CONFIG, TOPIC);\n+        TASK_PROPS.put(TaskConfig.TASK_CLASS_CONFIG, TestSinkTask.class.getName());\n+    }\n+\n+    public static final long OPERATOR_RETRY_TIMEOUT_MILLIS = 60000;\n+    public static final long OPERATOR_RETRY_MAX_DELAY_MILLIS = 5000;\n+    public static final ToleranceType OPERATOR_TOLERANCE_TYPE = ToleranceType.ALL;\n+\n+    private static final TaskConfig TASK_CONFIG = new TaskConfig(TASK_PROPS);\n+\n+    private ConnectorTaskId taskId = new ConnectorTaskId(\"job\", 0);\n+    private TargetState initialState = TargetState.STARTED;\n+    private Time time;\n+    private MockConnectMetrics metrics;\n+    @SuppressWarnings(\"unused\")\n+    @Mock\n+    private SinkTask sinkTask;\n+    @SuppressWarnings(\"unused\")\n+    @Mock\n+    private SourceTask sourceTask;\n+    private Capture<WorkerSinkTaskContext> sinkTaskContext = EasyMock.newCapture();\n+    private WorkerConfig workerConfig;\n+    private SourceConnectorConfig sourceConfig;\n+    @Mock\n+    private PluginClassLoader pluginLoader;\n+    @SuppressWarnings(\"unused\")\n+    @Mock\n+    private HeaderConverter headerConverter;\n+    private WorkerSinkTask workerSinkTask;\n+    private WorkerSourceTask workerSourceTask;\n+    @SuppressWarnings(\"unused\")\n+    @Mock\n+    private KafkaConsumer<byte[], byte[]> consumer;\n+    @SuppressWarnings(\"unused\")\n+    @Mock\n+    private KafkaProducer<byte[], byte[]> producer;\n+    @SuppressWarnings(\"unused\")\n+    @Mock private TopicAdmin admin;\n+\n+    @Mock\n+    OffsetStorageReaderImpl offsetReader;\n+    @Mock\n+    OffsetStorageWriter offsetWriter;\n+\n+    private Capture<ConsumerRebalanceListener> rebalanceListener = EasyMock.newCapture();\n+    @SuppressWarnings(\"unused\")\n+    @Mock\n+    private TaskStatus.Listener statusListener;\n+    @SuppressWarnings(\"unused\")\n+    @Mock private StatusBackingStore statusBackingStore;\n+\n+    private ErrorHandlingMetrics errorHandlingMetrics;\n+\n+    // when this test becomes parameterized, this variable will be a test parameter\n+    public boolean enableTopicCreation = true;\n+\n+    @Before\n+    public void setup() {\n+        time = new MockTime(0, 0, 0);\n+        metrics = new MockConnectMetrics();\n+        Map<String, String> workerProps = new HashMap<>();\n+        workerProps.put(\"key.converter\", \"org.apache.kafka.connect.json.JsonConverter\");\n+        workerProps.put(\"value.converter\", \"org.apache.kafka.connect.json.JsonConverter\");\n+        workerProps.put(\"internal.key.converter\", \"org.apache.kafka.connect.json.JsonConverter\");\n+        workerProps.put(\"internal.value.converter\", \"org.apache.kafka.connect.json.JsonConverter\");\n+        workerProps.put(\"internal.key.converter.schemas.enable\", \"false\");\n+        workerProps.put(\"internal.value.converter.schemas.enable\", \"false\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY2MDgwMw==", "bodyText": "Even though SourceConnectorConfig was an existing class without a unit test, we're adding a significant amount of non-trivial logic that we should unit test.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429660803", "createdAt": "2020-05-24T18:07:02Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SourceConnectorConfig.java", "diffHunk": "@@ -16,21 +16,153 @@\n  */\n package org.apache.kafka.connect.runtime;\n \n+import org.apache.kafka.common.config.AbstractConfig;\n import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n import org.apache.kafka.connect.runtime.isolation.Plugins;\n \n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_GROUP;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_PREFIX;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.EXCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.INCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.PARTITIONS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.REPLICATION_FACTOR_CONFIG;\n \n public class SourceConnectorConfig extends ConnectorConfig {\n \n-    private static ConfigDef config = ConnectorConfig.configDef();\n+    protected static final String TOPIC_CREATION_GROUP = \"Topic Creation\";\n+\n+    public static final String TOPIC_CREATION_PREFIX = \"topic.creation.\";\n+\n+    public static final String TOPIC_CREATION_GROUPS_CONFIG = TOPIC_CREATION_PREFIX + \"groups\";\n+    private static final String TOPIC_CREATION_GROUPS_DOC = \"Groups of configurations for topics \"\n+            + \"created by source connectors\";\n+    private static final String TOPIC_CREATION_GROUPS_DISPLAY = \"Topic Creation Groups\";\n+\n+    private static class EnrichedSourceConnectorConfig extends AbstractConfig {\n+        EnrichedSourceConnectorConfig(ConfigDef configDef, Map<String, String> props) {\n+            super(configDef, props);\n+        }\n+\n+        @Override\n+        public Object get(String key) {\n+            return super.get(key);\n+        }\n+    }\n+\n+    private static ConfigDef config = SourceConnectorConfig.configDef();\n+    private final EnrichedSourceConnectorConfig enrichedSourceConfig;\n \n     public static ConfigDef configDef() {\n-        return config;\n+        int orderInGroup = 0;\n+        return new ConfigDef(ConnectorConfig.configDef())\n+                .define(TOPIC_CREATION_GROUPS_CONFIG, ConfigDef.Type.LIST, Collections.emptyList(),\n+                        ConfigDef.CompositeValidator.of(new ConfigDef.NonNullValidator(), ConfigDef.LambdaValidator.with(\n+                            (name, value) -> {\n+                                List<?> groupAliases = (List<?>) value;\n+                                if (groupAliases.size() > new HashSet<>(groupAliases).size()) {\n+                                    throw new ConfigException(name, value, \"Duplicate alias provided.\");\n+                                }\n+                            },\n+                            () -> \"unique topic creation groups\")),\n+                        ConfigDef.Importance.LOW, TOPIC_CREATION_GROUPS_DOC, TOPIC_CREATION_GROUP,\n+                        ++orderInGroup, ConfigDef.Width.LONG, TOPIC_CREATION_GROUPS_DISPLAY);\n     }\n \n-    public SourceConnectorConfig(Plugins plugins, Map<String, String> props) {\n+    public static ConfigDef embedDefaultGroup(ConfigDef baseConfigDef) {\n+        String defaultGroup = \"default\";\n+        ConfigDef newDefaultDef = new ConfigDef(baseConfigDef);\n+        newDefaultDef.embed(DEFAULT_TOPIC_CREATION_PREFIX, defaultGroup, 0, TopicCreationConfig.defaultGroupConfigDef());\n+        return newDefaultDef;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY2MDk3MA==", "bodyText": "Are these useful?", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429660970", "createdAt": "2020-05-24T18:09:05Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerWithTopicCreationTest.java", "diffHunk": "@@ -0,0 +1,1408 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.Configurable;\n+import org.apache.kafka.common.config.AbstractConfig;\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.common.config.provider.MockFileConfigProvider;\n+import org.apache.kafka.common.utils.MockTime;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.connect.connector.Connector;\n+import org.apache.kafka.connect.connector.ConnectorContext;\n+import org.apache.kafka.connect.connector.Task;\n+import org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy;\n+import org.apache.kafka.connect.connector.policy.ConnectorClientConfigOverridePolicy;\n+import org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.json.JsonConverter;\n+import org.apache.kafka.connect.json.JsonConverterConfig;\n+import org.apache.kafka.connect.runtime.ConnectMetrics.MetricGroup;\n+import org.apache.kafka.connect.runtime.MockConnectMetrics.MockMetricsReporter;\n+import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;\n+import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator;\n+import org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader;\n+import org.apache.kafka.connect.runtime.isolation.PluginClassLoader;\n+import org.apache.kafka.connect.runtime.isolation.Plugins;\n+import org.apache.kafka.connect.runtime.isolation.Plugins.ClassLoaderUsage;\n+import org.apache.kafka.connect.runtime.rest.entities.ConnectorStateInfo;\n+import org.apache.kafka.connect.runtime.standalone.StandaloneConfig;\n+import org.apache.kafka.connect.sink.SinkTask;\n+import org.apache.kafka.connect.source.SourceRecord;\n+import org.apache.kafka.connect.source.SourceTask;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.apache.kafka.connect.storage.OffsetBackingStore;\n+import org.apache.kafka.connect.storage.OffsetStorageReader;\n+import org.apache.kafka.connect.storage.OffsetStorageWriter;\n+import org.apache.kafka.connect.storage.StatusBackingStore;\n+import org.apache.kafka.connect.util.ConnectorTaskId;\n+import org.apache.kafka.connect.util.ThreadedTest;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+import org.easymock.EasyMock;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.powermock.api.easymock.PowerMock;\n+import org.powermock.api.easymock.annotation.Mock;\n+import org.powermock.api.easymock.annotation.MockNice;\n+import org.powermock.api.easymock.annotation.MockStrict;\n+import org.powermock.core.classloader.annotations.PowerMockIgnore;\n+import org.powermock.core.classloader.annotations.PrepareForTest;\n+import org.powermock.modules.junit4.PowerMockRunner;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.ExecutorService;\n+\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_PREFIX;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.PARTITIONS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.connect.runtime.WorkerConfig.TOPIC_CREATION_ENABLE_CONFIG;\n+import static org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperatorTest.NOOP_OPERATOR;\n+import static org.easymock.EasyMock.anyObject;\n+import static org.easymock.EasyMock.eq;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.fail;\n+\n+@RunWith(PowerMockRunner.class)\n+@PrepareForTest({Worker.class, Plugins.class})\n+@PowerMockIgnore(\"javax.management.*\")\n+public class WorkerWithTopicCreationTest extends ThreadedTest {\n+\n+    private static final String CONNECTOR_ID = \"test-connector\";\n+    private static final ConnectorTaskId TASK_ID = new ConnectorTaskId(\"job\", 0);\n+    private static final String WORKER_ID = \"localhost:8083\";\n+    private final ConnectorClientConfigOverridePolicy noneConnectorClientConfigOverridePolicy = new NoneConnectorClientConfigOverridePolicy();\n+    private final ConnectorClientConfigOverridePolicy allConnectorClientConfigOverridePolicy = new AllConnectorClientConfigOverridePolicy();\n+\n+    private Map<String, String> workerProps = new HashMap<>();\n+    private WorkerConfig config;\n+    private Worker worker;\n+\n+    private Map<String, String> defaultProducerConfigs = new HashMap<>();\n+    private Map<String, String> defaultConsumerConfigs = new HashMap<>();\n+\n+    @Mock\n+    private Plugins plugins;\n+    @Mock\n+    private PluginClassLoader pluginLoader;\n+    @Mock\n+    private DelegatingClassLoader delegatingLoader;\n+    @Mock\n+    private OffsetBackingStore offsetBackingStore;\n+    @MockStrict\n+    private TaskStatus.Listener taskStatusListener;\n+    @MockStrict\n+    private ConnectorStatus.Listener connectorStatusListener;\n+\n+    @Mock private Herder herder;\n+    @Mock private StatusBackingStore statusBackingStore;\n+    @Mock private Connector connector;\n+    @Mock private ConnectorContext ctx;\n+    @Mock private TestSourceTask task;\n+    @Mock private WorkerSourceTask workerTask;\n+    @Mock private Converter keyConverter;\n+    @Mock private Converter valueConverter;\n+    @Mock private Converter taskKeyConverter;\n+    @Mock private Converter taskValueConverter;\n+    @Mock private HeaderConverter taskHeaderConverter;\n+    @Mock private ExecutorService executorService;\n+    @MockNice private ConnectorConfig connectorConfig;\n+    private String mockFileProviderTestId;\n+    private Map<String, String> connectorProps;\n+\n+    // when this test becomes parameterized, this variable will be a test parameter\n+    public boolean enableTopicCreation = true;\n+\n+    @Before\n+    public void setup() {\n+        super.setup();\n+        workerProps.put(\"key.converter\", \"org.apache.kafka.connect.json.JsonConverter\");\n+        workerProps.put(\"value.converter\", \"org.apache.kafka.connect.json.JsonConverter\");\n+        workerProps.put(\"internal.key.converter\", \"org.apache.kafka.connect.json.JsonConverter\");\n+        workerProps.put(\"internal.value.converter\", \"org.apache.kafka.connect.json.JsonConverter\");\n+        workerProps.put(\"internal.key.converter.schemas.enable\", \"false\");\n+        workerProps.put(\"internal.value.converter.schemas.enable\", \"false\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY2MTE2Ng==", "bodyText": "I don't think this is correct anymore, is it?", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429661166", "createdAt": "2020-05-24T18:11:16Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/SourceConnectorsIntegrationTest.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.integration;\n+\n+import org.apache.kafka.connect.runtime.SourceConnectorConfig;\n+import org.apache.kafka.connect.storage.StringConverter;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.kafka.connect.integration.MonitorableSourceConnector.TOPIC_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.CONNECTOR_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.KEY_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.NAME_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.TASKS_MAX_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.VALUE_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.SourceConnectorConfig.TOPIC_CREATION_GROUPS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_PREFIX;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.EXCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.INCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.PARTITIONS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.connect.runtime.WorkerConfig.CONNECTOR_CLIENT_POLICY_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.WorkerConfig.TOPIC_CREATION_ENABLE_CONFIG;\n+import static org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.DEFAULT_NUM_BROKERS;\n+\n+/**\n+ * Integration test for the endpoints that offer topic tracking of a connector's active\n+ * topics.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY2MTM0NA==", "bodyText": "How about an integration test that relies upon broker autocreation of topics?", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429661344", "createdAt": "2020-05-24T18:13:31Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/SourceConnectorsIntegrationTest.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.integration;\n+\n+import org.apache.kafka.connect.runtime.SourceConnectorConfig;\n+import org.apache.kafka.connect.storage.StringConverter;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.kafka.connect.integration.MonitorableSourceConnector.TOPIC_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.CONNECTOR_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.KEY_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.NAME_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.TASKS_MAX_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.VALUE_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.SourceConnectorConfig.TOPIC_CREATION_GROUPS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_PREFIX;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.EXCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.INCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.PARTITIONS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.connect.runtime.WorkerConfig.CONNECTOR_CLIENT_POLICY_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.WorkerConfig.TOPIC_CREATION_ENABLE_CONFIG;\n+import static org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.DEFAULT_NUM_BROKERS;\n+\n+/**\n+ * Integration test for the endpoints that offer topic tracking of a connector's active\n+ * topics.\n+ */\n+@Category(IntegrationTest.class)\n+public class SourceConnectorsIntegrationTest {\n+\n+    private static final int NUM_WORKERS = 3;\n+    private static final int NUM_TASKS = 1;\n+    private static final String FOO_TOPIC = \"foo-topic\";\n+    private static final String FOO_CONNECTOR = \"foo-source\";\n+    private static final String BAR_TOPIC = \"bar-topic\";\n+    private static final String BAR_CONNECTOR = \"bar-source\";\n+    private static final String FOO_GROUP = \"foo\";\n+    private static final String BAR_GROUP = \"bar\";\n+    private static final int DEFAULT_REPLICATION_FACTOR = DEFAULT_NUM_BROKERS;\n+    private static final int DEFAULT_PARTITIONS = 1;\n+    private static final int FOO_GROUP_REPLICATION_FACTOR = DEFAULT_NUM_BROKERS;\n+    private static final int FOO_GROUP_PARTITIONS = 9;\n+\n+    private EmbeddedConnectCluster.Builder connectBuilder;\n+    private EmbeddedConnectCluster connect;\n+    Map<String, String> workerProps = new HashMap<>();\n+    Properties brokerProps = new Properties();\n+\n+    @Before\n+    public void setup() {\n+        // setup Connect worker properties\n+        workerProps.put(CONNECTOR_CLIENT_POLICY_CLASS_CONFIG, \"All\");\n+\n+        // setup Kafka broker properties\n+        brokerProps.put(\"auto.create.topics.enable\", String.valueOf(false));\n+\n+        // build a Connect cluster backed by Kafka and Zk\n+        connectBuilder = new EmbeddedConnectCluster.Builder()\n+                .name(\"connect-cluster\")\n+                .numWorkers(NUM_WORKERS)\n+                .workerProps(workerProps)\n+                .brokerProps(brokerProps)\n+                .maskExitProcedures(true); // true is the default, setting here as example\n+    }\n+\n+    @After\n+    public void close() {\n+        // stop all Connect, Kafka and Zk threads.\n+        connect.stop();\n+    }\n+\n+    @Test\n+    public void testCreateTopic() throws InterruptedException {\n+        connect = connectBuilder.build();\n+        // start the clusters\n+        connect.start();\n+\n+        connect.assertions().assertAtLeastNumWorkersAreUp(NUM_WORKERS, \"Initial group of workers did not start in time.\");\n+\n+        Map<String, String> fooProps = sourceConnectorPropsWithGroups(FOO_TOPIC);\n+\n+        // start a source connector\n+        connect.configureConnector(FOO_CONNECTOR, fooProps);\n+        fooProps.put(NAME_CONFIG, FOO_CONNECTOR);\n+\n+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(fooProps.get(CONNECTOR_CLASS_CONFIG), fooProps, 0,\n+                \"Validating connector configuration produced an unexpected number or errors.\");\n+\n+        connect.assertions().assertConnectorAndAtLeastNumTasksAreRunning(FOO_CONNECTOR, NUM_TASKS,\n+                \"Connector tasks did not start in time.\");\n+\n+        connect.assertions().assertTopicsExist(FOO_TOPIC);\n+        connect.assertions().assertTopicSettings(FOO_TOPIC, FOO_GROUP_REPLICATION_FACTOR, FOO_GROUP_PARTITIONS);\n+    }\n+\n+    @Test\n+    public void testSwitchingToTopicCreationEnabled() throws InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY2MTM1MQ==", "bodyText": "Should we also assert that the topic actually exists before we start the connector? Otherwise, we might not be testing what we think we are.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429661351", "createdAt": "2020-05-24T18:13:47Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/SourceConnectorsIntegrationTest.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.integration;\n+\n+import org.apache.kafka.connect.runtime.SourceConnectorConfig;\n+import org.apache.kafka.connect.storage.StringConverter;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.kafka.connect.integration.MonitorableSourceConnector.TOPIC_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.CONNECTOR_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.KEY_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.NAME_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.TASKS_MAX_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.VALUE_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.SourceConnectorConfig.TOPIC_CREATION_GROUPS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_PREFIX;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.EXCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.INCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.PARTITIONS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.connect.runtime.WorkerConfig.CONNECTOR_CLIENT_POLICY_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.WorkerConfig.TOPIC_CREATION_ENABLE_CONFIG;\n+import static org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.DEFAULT_NUM_BROKERS;\n+\n+/**\n+ * Integration test for the endpoints that offer topic tracking of a connector's active\n+ * topics.\n+ */\n+@Category(IntegrationTest.class)\n+public class SourceConnectorsIntegrationTest {\n+\n+    private static final int NUM_WORKERS = 3;\n+    private static final int NUM_TASKS = 1;\n+    private static final String FOO_TOPIC = \"foo-topic\";\n+    private static final String FOO_CONNECTOR = \"foo-source\";\n+    private static final String BAR_TOPIC = \"bar-topic\";\n+    private static final String BAR_CONNECTOR = \"bar-source\";\n+    private static final String FOO_GROUP = \"foo\";\n+    private static final String BAR_GROUP = \"bar\";\n+    private static final int DEFAULT_REPLICATION_FACTOR = DEFAULT_NUM_BROKERS;\n+    private static final int DEFAULT_PARTITIONS = 1;\n+    private static final int FOO_GROUP_REPLICATION_FACTOR = DEFAULT_NUM_BROKERS;\n+    private static final int FOO_GROUP_PARTITIONS = 9;\n+\n+    private EmbeddedConnectCluster.Builder connectBuilder;\n+    private EmbeddedConnectCluster connect;\n+    Map<String, String> workerProps = new HashMap<>();\n+    Properties brokerProps = new Properties();\n+\n+    @Before\n+    public void setup() {\n+        // setup Connect worker properties\n+        workerProps.put(CONNECTOR_CLIENT_POLICY_CLASS_CONFIG, \"All\");\n+\n+        // setup Kafka broker properties\n+        brokerProps.put(\"auto.create.topics.enable\", String.valueOf(false));\n+\n+        // build a Connect cluster backed by Kafka and Zk\n+        connectBuilder = new EmbeddedConnectCluster.Builder()\n+                .name(\"connect-cluster\")\n+                .numWorkers(NUM_WORKERS)\n+                .workerProps(workerProps)\n+                .brokerProps(brokerProps)\n+                .maskExitProcedures(true); // true is the default, setting here as example\n+    }\n+\n+    @After\n+    public void close() {\n+        // stop all Connect, Kafka and Zk threads.\n+        connect.stop();\n+    }\n+\n+    @Test\n+    public void testCreateTopic() throws InterruptedException {\n+        connect = connectBuilder.build();\n+        // start the clusters\n+        connect.start();\n+\n+        connect.assertions().assertAtLeastNumWorkersAreUp(NUM_WORKERS, \"Initial group of workers did not start in time.\");\n+\n+        Map<String, String> fooProps = sourceConnectorPropsWithGroups(FOO_TOPIC);\n+\n+        // start a source connector\n+        connect.configureConnector(FOO_CONNECTOR, fooProps);\n+        fooProps.put(NAME_CONFIG, FOO_CONNECTOR);\n+\n+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(fooProps.get(CONNECTOR_CLASS_CONFIG), fooProps, 0,\n+                \"Validating connector configuration produced an unexpected number or errors.\");\n+\n+        connect.assertions().assertConnectorAndAtLeastNumTasksAreRunning(FOO_CONNECTOR, NUM_TASKS,\n+                \"Connector tasks did not start in time.\");\n+\n+        connect.assertions().assertTopicsExist(FOO_TOPIC);\n+        connect.assertions().assertTopicSettings(FOO_TOPIC, FOO_GROUP_REPLICATION_FACTOR, FOO_GROUP_PARTITIONS);\n+    }\n+\n+    @Test\n+    public void testSwitchingToTopicCreationEnabled() throws InterruptedException {\n+        workerProps.put(TOPIC_CREATION_ENABLE_CONFIG, String.valueOf(false));\n+        connect = connectBuilder.build();\n+        // start the clusters\n+        connect.start();\n+\n+        connect.kafka().createTopic(BAR_TOPIC, 1);\n+        connect.assertions().assertAtLeastNumWorkersAreUp(NUM_WORKERS, \"Initial group of workers did not start in time.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 129}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3MzkzNjY1", "url": "https://github.com/apache/kafka/pull/8722#pullrequestreview-417393665", "createdAt": "2020-05-24T21:30:26Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNFQyMTozMDoyNlrOGZxapg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQwMDo1ODoyN1rOGZyfmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY3NzIyMg==", "bodyText": "Good point. We could always substitute with String.format but since it doesn't add much, I've removed it here and below.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429677222", "createdAt": "2020-05-24T21:30:26Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+\n+import java.util.Collections;\n+\n+public class TopicCreationConfig {\n+\n+    public static final String DEFAULT_TOPIC_CREATION_PREFIX = \"topic.creation.default.\";\n+    public static final String DEFAULT_TOPIC_CREATION_GROUP = \"default\";\n+\n+    public static final String INCLUDE_REGEX_CONFIG = \"include\";\n+    private static final String INCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to include topics that \"\n+            + \"match their values and apply this group's specific configuration to the topics \"\n+            + \"that match this inclusion list. $alias applies to any group defined in topic\"\n+            + \".creation.groups but not the default\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1NzE4MQ=="}, "originalCommit": null, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY3NzMyMw==", "bodyText": "Yes, indeed. You think we should set them instead?", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429677323", "createdAt": "2020-05-24T21:31:55Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "diffHunk": "@@ -678,7 +701,8 @@ ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) {\n         if (topic != null && !topic.isEmpty()) {\n             Map<String, Object> producerProps = producerConfigs(id, \"connector-dlq-producer-\" + id, config, connConfig, connectorClass,\n                                                                 connectorClientConfigOverridePolicy);\n-            Map<String, Object> adminProps = adminConfigs(id, config, connConfig, connectorClass, connectorClientConfigOverridePolicy);\n+            // Leaving default client id empty means that the admin client will set the default at instantiation time\n+            Map<String, Object> adminProps = adminConfigs(id, \"\", config, connConfig, connectorClass, connectorClientConfigOverridePolicy);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1NzkzMg=="}, "originalCommit": null, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY3NzUxMA==", "bodyText": "All I can think of is that I added this one here first, thinking I might not need a similar method in the config. Fixed.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429677510", "createdAt": "2020-05-24T21:34:35Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "diffHunk": "@@ -802,6 +826,16 @@ public String workerId() {\n         return workerId;\n     }\n \n+    /**\n+     * Returns whether this worker is configured to allow source connectors to create the topics\n+     * that they use with custom configurations, if these topics don't already exist.\n+     *\n+     * @return true if topic creation by source connectors is allowed; false otherwise\n+     */\n+    public boolean isTopicCreationEnabled() {\n+        return config.getBoolean(TOPIC_CREATION_ENABLE_CONFIG);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1ODcxMA=="}, "originalCommit": null, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY3NzgzMA==", "bodyText": "I added the additional info, but maybe there's still room for improvement.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429677830", "createdAt": "2020-05-24T21:39:12Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfig.java", "diffHunk": "@@ -250,9 +250,16 @@\n             + \"user requests to reset the set of active topics per connector.\";\n     protected static final boolean TOPIC_TRACKING_ALLOW_RESET_DEFAULT = true;\n \n+    public static final String TOPIC_CREATION_ENABLE_CONFIG = \"topic.creation.enable\";\n+    protected static final String TOPIC_CREATION_ENABLE_DOC = \"If set to true, it allows \"\n+            + \"source connectors to create topics with custom settings. If enabled, each connector \"\n+            + \"task will use an admin clients to create its topics and will not depend on \"\n+            + \"auto.create.topics.enable being set on Kafka brokers.\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1ODI2MA=="}, "originalCommit": null, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY4OTAyNg==", "bodyText": "Good point. Refactored and added a few simple tests.\nThis class can be used for more assertions eventually, but this will require a refactor of the constructor arguments, that I'd suggest addressing separately.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429689026", "createdAt": "2020-05-25T00:05:46Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -142,6 +159,18 @@ public WorkerSourceTask(ConnectorTaskId id,\n         this.sourceTaskMetricsGroup = new SourceTaskMetricsGroup(id, connectMetrics);\n         this.producerSendException = new AtomicReference<>();\n         this.isTopicTrackingEnabled = workerConfig.getBoolean(TOPIC_TRACKING_ENABLE_CONFIG);\n+        this.isTopicCreationEnabled =\n+                workerConfig.getBoolean(TOPIC_CREATION_ENABLE_CONFIG) && topicGroups != null;\n+        if (isTopicCreationEnabled) {\n+            this.defaultTopicGroup = topicGroups.get(DEFAULT_TOPIC_CREATION_GROUP);\n+            this.topicGroups = new LinkedHashMap<>(topicGroups);\n+            this.topicGroups.remove(DEFAULT_TOPIC_CREATION_GROUP);\n+            this.topicCache = new HashSet<>();\n+        } else {\n+            this.defaultTopicGroup = null;\n+            this.topicGroups = Collections.emptyMap();\n+            this.topicCache = Collections.emptySet();\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1OTExNQ=="}, "originalCommit": null, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY4OTI4NA==", "bodyText": "\ud83d\udc4d Added", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429689284", "createdAt": "2020-05-25T00:08:53Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -384,6 +417,40 @@ public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n         return true;\n     }\n \n+    // Due to transformations that may change the destination topic of a record (such as\n+    // RegexRouter) topic creation can not be batched for multiple topics\n+    private void maybeCreateTopic(String topic) {\n+        if (!isTopicCreationEnabled || topicCache.contains(topic)) {\n+            return;\n+        }\n+        log.info(\"The task will send records to topic '{}' for the first time. Checking \"\n+                + \"whether topic exists\", topic);\n+        Map<String, TopicDescription> existing = admin.describeTopics(topic);\n+        if (!existing.isEmpty()) {\n+            log.info(\"Topic '{}' already exists.\", topic);\n+            topicCache.add(topic);\n+            return;\n+        }\n+\n+        log.info(\"Creating topic '{}'\", topic);\n+        NewTopicCreationGroup topicGroup = topicGroups.values().stream()\n+                .filter(group -> group.matches(topic))\n+                .findFirst()\n+                .orElse(defaultTopicGroup);\n+        log.debug(\"Topic '{}' matched topic creation group: {}\", topic, topicGroup);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1OTU3Ng=="}, "originalCommit": null, "originalPosition": 200}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MDg1MQ==", "bodyText": "\ud83d\udc4d went with option 2.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429690851", "createdAt": "2020-05-25T00:24:12Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -384,6 +417,40 @@ public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n         return true;\n     }\n \n+    // Due to transformations that may change the destination topic of a record (such as\n+    // RegexRouter) topic creation can not be batched for multiple topics\n+    private void maybeCreateTopic(String topic) {\n+        if (!isTopicCreationEnabled || topicCache.contains(topic)) {\n+            return;\n+        }\n+        log.info(\"The task will send records to topic '{}' for the first time. Checking \"\n+                + \"whether topic exists\", topic);\n+        Map<String, TopicDescription> existing = admin.describeTopics(topic);\n+        if (!existing.isEmpty()) {\n+            log.info(\"Topic '{}' already exists.\", topic);\n+            topicCache.add(topic);\n+            return;\n+        }\n+\n+        log.info(\"Creating topic '{}'\", topic);\n+        NewTopicCreationGroup topicGroup = topicGroups.values().stream()\n+                .filter(group -> group.matches(topic))\n+                .findFirst()\n+                .orElse(defaultTopicGroup);\n+        log.debug(\"Topic '{}' matched topic creation group: {}\", topic, topicGroup);\n+        NewTopic newTopic = topicGroup.newTopic(topic);\n+\n+        if (admin.createTopic(newTopic)) {\n+            topicCache.add(topic);\n+            log.info(\"Created topic '{}'\", newTopic);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1OTUxMQ=="}, "originalCommit": null, "originalPosition": 205}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MTEzNA==", "bodyText": "Added.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429691134", "createdAt": "2020-05-25T00:26:32Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java", "diffHunk": "@@ -178,6 +191,47 @@ public NewTopic build() {\n         }\n     }\n \n+    public static class NewTopicCreationGroup {\n+        private final Pattern inclusionPattern;\n+        private final Pattern exclusionPattern;\n+        private final int numPartitions;\n+        private final short replicationFactor;\n+        private final Map<String, Object> otherConfigs;\n+\n+        protected NewTopicCreationGroup(String group, SourceConnectorConfig config) {\n+            inclusionPattern = Pattern.compile(String.join(\"|\", config.topicCreationInclude(group)));\n+            exclusionPattern = Pattern.compile(String.join(\"|\", config.topicCreationExclude(group)));\n+            numPartitions = config.topicCreationPartitions(group);\n+            replicationFactor = config.topicCreationReplicationFactor(group);\n+            otherConfigs = config.topicCreationOtherConfigs(group);\n+        }\n+\n+        public boolean matches(String topic) {\n+            return !exclusionPattern.matcher(topic).matches() && inclusionPattern.matcher(topic).matches();\n+        }\n+\n+        public NewTopic newTopic(String topic) {\n+            NewTopicBuilder builder = new NewTopicBuilder(topic);\n+            return builder.partitions(numPartitions)\n+                    .replicationFactor(replicationFactor)\n+                    .config(otherConfigs)\n+                    .build();\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1OTcxNQ=="}, "originalCommit": null, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MjkzOQ==", "bodyText": "Not a bad idea. Good thing is that this type of test runs fast. Added.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429692939", "createdAt": "2020-05-25T00:42:27Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/SourceConnectorsIntegrationTest.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.integration;\n+\n+import org.apache.kafka.connect.runtime.SourceConnectorConfig;\n+import org.apache.kafka.connect.storage.StringConverter;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.kafka.connect.integration.MonitorableSourceConnector.TOPIC_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.CONNECTOR_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.KEY_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.NAME_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.TASKS_MAX_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.VALUE_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.SourceConnectorConfig.TOPIC_CREATION_GROUPS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_PREFIX;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.EXCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.INCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.PARTITIONS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.connect.runtime.WorkerConfig.CONNECTOR_CLIENT_POLICY_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.WorkerConfig.TOPIC_CREATION_ENABLE_CONFIG;\n+import static org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.DEFAULT_NUM_BROKERS;\n+\n+/**\n+ * Integration test for the endpoints that offer topic tracking of a connector's active\n+ * topics.\n+ */\n+@Category(IntegrationTest.class)\n+public class SourceConnectorsIntegrationTest {\n+\n+    private static final int NUM_WORKERS = 3;\n+    private static final int NUM_TASKS = 1;\n+    private static final String FOO_TOPIC = \"foo-topic\";\n+    private static final String FOO_CONNECTOR = \"foo-source\";\n+    private static final String BAR_TOPIC = \"bar-topic\";\n+    private static final String BAR_CONNECTOR = \"bar-source\";\n+    private static final String FOO_GROUP = \"foo\";\n+    private static final String BAR_GROUP = \"bar\";\n+    private static final int DEFAULT_REPLICATION_FACTOR = DEFAULT_NUM_BROKERS;\n+    private static final int DEFAULT_PARTITIONS = 1;\n+    private static final int FOO_GROUP_REPLICATION_FACTOR = DEFAULT_NUM_BROKERS;\n+    private static final int FOO_GROUP_PARTITIONS = 9;\n+\n+    private EmbeddedConnectCluster.Builder connectBuilder;\n+    private EmbeddedConnectCluster connect;\n+    Map<String, String> workerProps = new HashMap<>();\n+    Properties brokerProps = new Properties();\n+\n+    @Before\n+    public void setup() {\n+        // setup Connect worker properties\n+        workerProps.put(CONNECTOR_CLIENT_POLICY_CLASS_CONFIG, \"All\");\n+\n+        // setup Kafka broker properties\n+        brokerProps.put(\"auto.create.topics.enable\", String.valueOf(false));\n+\n+        // build a Connect cluster backed by Kafka and Zk\n+        connectBuilder = new EmbeddedConnectCluster.Builder()\n+                .name(\"connect-cluster\")\n+                .numWorkers(NUM_WORKERS)\n+                .workerProps(workerProps)\n+                .brokerProps(brokerProps)\n+                .maskExitProcedures(true); // true is the default, setting here as example\n+    }\n+\n+    @After\n+    public void close() {\n+        // stop all Connect, Kafka and Zk threads.\n+        connect.stop();\n+    }\n+\n+    @Test\n+    public void testCreateTopic() throws InterruptedException {\n+        connect = connectBuilder.build();\n+        // start the clusters\n+        connect.start();\n+\n+        connect.assertions().assertAtLeastNumWorkersAreUp(NUM_WORKERS, \"Initial group of workers did not start in time.\");\n+\n+        Map<String, String> fooProps = sourceConnectorPropsWithGroups(FOO_TOPIC);\n+\n+        // start a source connector\n+        connect.configureConnector(FOO_CONNECTOR, fooProps);\n+        fooProps.put(NAME_CONFIG, FOO_CONNECTOR);\n+\n+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(fooProps.get(CONNECTOR_CLASS_CONFIG), fooProps, 0,\n+                \"Validating connector configuration produced an unexpected number or errors.\");\n+\n+        connect.assertions().assertConnectorAndAtLeastNumTasksAreRunning(FOO_CONNECTOR, NUM_TASKS,\n+                \"Connector tasks did not start in time.\");\n+\n+        connect.assertions().assertTopicsExist(FOO_TOPIC);\n+        connect.assertions().assertTopicSettings(FOO_TOPIC, FOO_GROUP_REPLICATION_FACTOR, FOO_GROUP_PARTITIONS);\n+    }\n+\n+    @Test\n+    public void testSwitchingToTopicCreationEnabled() throws InterruptedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY2MTM0NA=="}, "originalCommit": null, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5MzE4MA==", "bodyText": "Added that assertion and I also changed createTopic to pass specific properties.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429693180", "createdAt": "2020-05-25T00:44:37Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/integration/SourceConnectorsIntegrationTest.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.integration;\n+\n+import org.apache.kafka.connect.runtime.SourceConnectorConfig;\n+import org.apache.kafka.connect.storage.StringConverter;\n+import org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.kafka.connect.integration.MonitorableSourceConnector.TOPIC_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.CONNECTOR_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.KEY_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.NAME_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.TASKS_MAX_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.VALUE_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.SourceConnectorConfig.TOPIC_CREATION_GROUPS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_PREFIX;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.EXCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.INCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.PARTITIONS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.connect.runtime.WorkerConfig.CONNECTOR_CLIENT_POLICY_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.WorkerConfig.TOPIC_CREATION_ENABLE_CONFIG;\n+import static org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.DEFAULT_NUM_BROKERS;\n+\n+/**\n+ * Integration test for the endpoints that offer topic tracking of a connector's active\n+ * topics.\n+ */\n+@Category(IntegrationTest.class)\n+public class SourceConnectorsIntegrationTest {\n+\n+    private static final int NUM_WORKERS = 3;\n+    private static final int NUM_TASKS = 1;\n+    private static final String FOO_TOPIC = \"foo-topic\";\n+    private static final String FOO_CONNECTOR = \"foo-source\";\n+    private static final String BAR_TOPIC = \"bar-topic\";\n+    private static final String BAR_CONNECTOR = \"bar-source\";\n+    private static final String FOO_GROUP = \"foo\";\n+    private static final String BAR_GROUP = \"bar\";\n+    private static final int DEFAULT_REPLICATION_FACTOR = DEFAULT_NUM_BROKERS;\n+    private static final int DEFAULT_PARTITIONS = 1;\n+    private static final int FOO_GROUP_REPLICATION_FACTOR = DEFAULT_NUM_BROKERS;\n+    private static final int FOO_GROUP_PARTITIONS = 9;\n+\n+    private EmbeddedConnectCluster.Builder connectBuilder;\n+    private EmbeddedConnectCluster connect;\n+    Map<String, String> workerProps = new HashMap<>();\n+    Properties brokerProps = new Properties();\n+\n+    @Before\n+    public void setup() {\n+        // setup Connect worker properties\n+        workerProps.put(CONNECTOR_CLIENT_POLICY_CLASS_CONFIG, \"All\");\n+\n+        // setup Kafka broker properties\n+        brokerProps.put(\"auto.create.topics.enable\", String.valueOf(false));\n+\n+        // build a Connect cluster backed by Kafka and Zk\n+        connectBuilder = new EmbeddedConnectCluster.Builder()\n+                .name(\"connect-cluster\")\n+                .numWorkers(NUM_WORKERS)\n+                .workerProps(workerProps)\n+                .brokerProps(brokerProps)\n+                .maskExitProcedures(true); // true is the default, setting here as example\n+    }\n+\n+    @After\n+    public void close() {\n+        // stop all Connect, Kafka and Zk threads.\n+        connect.stop();\n+    }\n+\n+    @Test\n+    public void testCreateTopic() throws InterruptedException {\n+        connect = connectBuilder.build();\n+        // start the clusters\n+        connect.start();\n+\n+        connect.assertions().assertAtLeastNumWorkersAreUp(NUM_WORKERS, \"Initial group of workers did not start in time.\");\n+\n+        Map<String, String> fooProps = sourceConnectorPropsWithGroups(FOO_TOPIC);\n+\n+        // start a source connector\n+        connect.configureConnector(FOO_CONNECTOR, fooProps);\n+        fooProps.put(NAME_CONFIG, FOO_CONNECTOR);\n+\n+        connect.assertions().assertExactlyNumErrorsOnConnectorConfigValidation(fooProps.get(CONNECTOR_CLASS_CONFIG), fooProps, 0,\n+                \"Validating connector configuration produced an unexpected number or errors.\");\n+\n+        connect.assertions().assertConnectorAndAtLeastNumTasksAreRunning(FOO_CONNECTOR, NUM_TASKS,\n+                \"Connector tasks did not start in time.\");\n+\n+        connect.assertions().assertTopicsExist(FOO_TOPIC);\n+        connect.assertions().assertTopicSettings(FOO_TOPIC, FOO_GROUP_REPLICATION_FACTOR, FOO_GROUP_PARTITIONS);\n+    }\n+\n+    @Test\n+    public void testSwitchingToTopicCreationEnabled() throws InterruptedException {\n+        workerProps.put(TOPIC_CREATION_ENABLE_CONFIG, String.valueOf(false));\n+        connect = connectBuilder.build();\n+        // start the clusters\n+        connect.start();\n+\n+        connect.kafka().createTopic(BAR_TOPIC, 1);\n+        connect.assertions().assertAtLeastNumWorkersAreUp(NUM_WORKERS, \"Initial group of workers did not start in time.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY2MTM1MQ=="}, "originalCommit": null, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY5NDg3NA==", "bodyText": "Indeed and I think that if something is permitted by a definition that is not expected to change any time soon we should be free to use it.\nThe good news with the current approach is that if you want to encode your regex as a single pattern, this config with accept it as is. Given that, I'd prefer to keep it as a list since it might be slightly simpler to write or extend while still being able to use a single regex without change if you prefer.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429694874", "createdAt": "2020-05-25T00:58:27Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java", "diffHunk": "@@ -178,6 +191,47 @@ public NewTopic build() {\n         }\n     }\n \n+    public static class NewTopicCreationGroup {\n+        private final Pattern inclusionPattern;\n+        private final Pattern exclusionPattern;\n+        private final int numPartitions;\n+        private final short replicationFactor;\n+        private final Map<String, Object> otherConfigs;\n+\n+        protected NewTopicCreationGroup(String group, SourceConnectorConfig config) {\n+            inclusionPattern = Pattern.compile(String.join(\"|\", config.topicCreationInclude(group)));\n+            exclusionPattern = Pattern.compile(String.join(\"|\", config.topicCreationExclude(group)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY2MDAyMg=="}, "originalCommit": null, "originalPosition": 68}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/6a06ba6b877b67b347d2fe3fcc7e70747c629d50", "committedDate": "2020-05-25T02:28:23Z", "message": "KAFKA-5295: Adapt tests after the changes of KAFKA-4794/KIP-131"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3Nzg1ODI3", "url": "https://github.com/apache/kafka/pull/8722#pullrequestreview-417785827", "createdAt": "2020-05-25T15:43:23Z", "commit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQxNTo0MzoyM1rOGaE4mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQxNjowNTowOFrOGaFWzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk5NjE4Ng==", "bodyText": "Maybe:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                protected static final String TOPIC_CREATION_ENABLE_DOC = \"If set to true, it allows \"\n          \n          \n            \n                        + \"source connectors to create topics with custom settings. If enabled, in \"\n          \n          \n            \n                        + \"connectors that specify topic creation properties with the prefix `\" + TOPIC_CREATION_PREFIX\n          \n          \n            \n                        + \"` each task will use an admin client to create its topics and will not depend on \"\n          \n          \n            \n                        + \"auto.create.topics.enable being set on Kafka brokers.\";\n          \n          \n            \n                protected static final String TOPIC_CREATION_ENABLE_DOC = \"If set to true, it allows \"\n          \n          \n            \n                        + \"source connectors to create topics by specifying topic creation properties \"\n          \n          \n            \n                        + \"with the prefix `\" + TOPIC_CREATION_PREFIX + \"`. Each task will use an\n          \n          \n            \n                        + \"admin client to create its topics and will not depend on the Kafka brokers \"\n          \n          \n            \n                        + \"to create topics automatically.\";", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429996186", "createdAt": "2020-05-25T15:43:23Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfig.java", "diffHunk": "@@ -250,9 +251,17 @@\n             + \"user requests to reset the set of active topics per connector.\";\n     protected static final boolean TOPIC_TRACKING_ALLOW_RESET_DEFAULT = true;\n \n+    public static final String TOPIC_CREATION_ENABLE_CONFIG = \"topic.creation.enable\";\n+    protected static final String TOPIC_CREATION_ENABLE_DOC = \"If set to true, it allows \"\n+            + \"source connectors to create topics with custom settings. If enabled, in \"\n+            + \"connectors that specify topic creation properties with the prefix `\" + TOPIC_CREATION_PREFIX\n+            + \"` each task will use an admin client to create its topics and will not depend on \"\n+            + \"auto.create.topics.enable being set on Kafka brokers.\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk5NjgyMw==", "bodyText": "Nit: Since this is static, how about moving to before the member fields and methods?", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429996823", "createdAt": "2020-05-25T15:45:05Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -142,6 +155,57 @@ public WorkerSourceTask(ConnectorTaskId id,\n         this.sourceTaskMetricsGroup = new SourceTaskMetricsGroup(id, connectMetrics);\n         this.producerSendException = new AtomicReference<>();\n         this.isTopicTrackingEnabled = workerConfig.getBoolean(TOPIC_TRACKING_ENABLE_CONFIG);\n+        this.topicCreation = TopicCreation.newTopicCreation(workerConfig, topicGroups);\n+    }\n+\n+    public static class TopicCreation {\n+        private static final TopicCreation EMPTY =\n+                new TopicCreation(false, null, Collections.emptyMap(), Collections.emptySet());\n+\n+        private final boolean isTopicCreationEnabled;\n+        private final NewTopicCreationGroup defaultTopicGroup;\n+        private final Map<String, NewTopicCreationGroup> topicGroups;\n+        private final Set<String> topicCache;\n+\n+        protected TopicCreation(boolean isTopicCreationEnabled,\n+                                NewTopicCreationGroup defaultTopicGroup,\n+                                Map<String, NewTopicCreationGroup> topicGroups,\n+                                Set<String> topicCache) {\n+            this.isTopicCreationEnabled = isTopicCreationEnabled;\n+            this.defaultTopicGroup = defaultTopicGroup;\n+            this.topicGroups = topicGroups;\n+            this.topicCache = topicCache;\n+        }\n+\n+        public static TopicCreation newTopicCreation(WorkerConfig workerConfig,\n+                                                     Map<String, NewTopicCreationGroup> topicGroups) {\n+            if (!workerConfig.topicCreationEnable() || topicGroups == null) {\n+                return EMPTY;\n+            }\n+            Map<String, NewTopicCreationGroup> groups = new LinkedHashMap<>(topicGroups);\n+            groups.remove(DEFAULT_TOPIC_CREATION_GROUP);\n+            return new TopicCreation(true, topicGroups.get(DEFAULT_TOPIC_CREATION_GROUP), groups, new HashSet<>());\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk5NzQ2Mw==", "bodyText": "Couldn't this also be added to the TopicCreation class? Seems like it'd help unit test this logic in isolation, more easily covering the corner cases.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429997463", "createdAt": "2020-05-25T15:46:54Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -384,6 +452,40 @@ public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n         return true;\n     }\n \n+    // Due to transformations that may change the destination topic of a record (such as\n+    // RegexRouter) topic creation can not be batched for multiple topics\n+    private void maybeCreateTopic(String topic) {\n+        if (!topicCreation.isTopicCreationEnabled() || topicCreation.topicCache().contains(topic)) {\n+            return;\n+        }\n+        log.info(\"The task will send records to topic '{}' for the first time. Checking \"\n+                + \"whether topic exists\", topic);\n+        Map<String, TopicDescription> existing = admin.describeTopics(topic);\n+        if (!existing.isEmpty()) {\n+            log.info(\"Topic '{}' already exists.\", topic);\n+            topicCreation.topicCache().add(topic);\n+            return;\n+        }\n+\n+        log.info(\"Creating topic '{}'\", topic);\n+        NewTopicCreationGroup topicGroup = topicCreation.topicGroups().values().stream()\n+                .filter(group -> group.matches(topic))\n+                .findFirst()\n+                .orElse(topicCreation.defaultTopicGroup());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50"}, "originalPosition": 234}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk5ODQzMA==", "bodyText": "Again, this logic could be moved into TopicCreation for easier testing and to make this code more readable:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (!topicCreation.isTopicCreationEnabled() || topicCreation.topicCache().contains(topic)) {\n          \n          \n            \n                        return;\n          \n          \n            \n                    }\n          \n          \n            \n                    if (!topicCreation.isCreateTopicRequired(topic)) {\n          \n          \n            \n                        return;\n          \n          \n            \n                    }", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429998430", "createdAt": "2020-05-25T15:49:23Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -384,6 +452,40 @@ public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n         return true;\n     }\n \n+    // Due to transformations that may change the destination topic of a record (such as\n+    // RegexRouter) topic creation can not be batched for multiple topics\n+    private void maybeCreateTopic(String topic) {\n+        if (!topicCreation.isTopicCreationEnabled() || topicCreation.topicCache().contains(topic)) {\n+            return;\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50"}, "originalPosition": 220}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk5OTE0OQ==", "bodyText": "Another thing for potentially moving into TopicCreation, such as maybe adding a addTopic(topic) method. Doing so might allow you to hide most of the implementation details of TopicCreation.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        topicCreation.topicCache().add(topic);\n          \n          \n            \n                        topicCreation.addTopic(topic);", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429999149", "createdAt": "2020-05-25T15:51:18Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -384,6 +452,40 @@ public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n         return true;\n     }\n \n+    // Due to transformations that may change the destination topic of a record (such as\n+    // RegexRouter) topic creation can not be batched for multiple topics\n+    private void maybeCreateTopic(String topic) {\n+        if (!topicCreation.isTopicCreationEnabled() || topicCreation.topicCache().contains(topic)) {\n+            return;\n+        }\n+        log.info(\"The task will send records to topic '{}' for the first time. Checking \"\n+                + \"whether topic exists\", topic);\n+        Map<String, TopicDescription> existing = admin.describeTopics(topic);\n+        if (!existing.isEmpty()) {\n+            log.info(\"Topic '{}' already exists.\", topic);\n+            topicCreation.topicCache().add(topic);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50"}, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk5OTQ0Ng==", "bodyText": "Another thing for potentially moving into TopicCreation, such as maybe adding a addTopic(topic) method. Doing so might allow you to hide most of the implementation details of TopicCreation.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        topicCreation.topicCache().add(topic);\n          \n          \n            \n                        topicCreation.addTopic(topic);", "url": "https://github.com/apache/kafka/pull/8722#discussion_r429999446", "createdAt": "2020-05-25T15:52:04Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -384,6 +452,40 @@ public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n         return true;\n     }\n \n+    // Due to transformations that may change the destination topic of a record (such as\n+    // RegexRouter) topic creation can not be batched for multiple topics\n+    private void maybeCreateTopic(String topic) {\n+        if (!topicCreation.isTopicCreationEnabled() || topicCreation.topicCache().contains(topic)) {\n+            return;\n+        }\n+        log.info(\"The task will send records to topic '{}' for the first time. Checking \"\n+                + \"whether topic exists\", topic);\n+        Map<String, TopicDescription> existing = admin.describeTopics(topic);\n+        if (!existing.isEmpty()) {\n+            log.info(\"Topic '{}' already exists.\", topic);\n+            topicCreation.topicCache().add(topic);\n+            return;\n+        }\n+\n+        log.info(\"Creating topic '{}'\", topic);\n+        NewTopicCreationGroup topicGroup = topicCreation.topicGroups().values().stream()\n+                .filter(group -> group.matches(topic))\n+                .findFirst()\n+                .orElse(topicCreation.defaultTopicGroup());\n+        log.debug(\"Topic '{}' matched topic creation group: {}\", topic, topicGroup);\n+        NewTopic newTopic = topicGroup.newTopic(topic);\n+\n+        if (admin.createTopic(newTopic)) {\n+            topicCreation.topicCache().add(topic);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50"}, "originalPosition": 239}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDAwMDE4Mw==", "bodyText": "Nit: wouldn't this public static method be better before any of the member fields or methods?", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430000183", "createdAt": "2020-05-25T15:54:23Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java", "diffHunk": "@@ -178,6 +192,76 @@ public NewTopic build() {\n         }\n     }\n \n+    public static class NewTopicCreationGroup {\n+        private final String name;\n+        private final Pattern inclusionPattern;\n+        private final Pattern exclusionPattern;\n+        private final int numPartitions;\n+        private final short replicationFactor;\n+        private final Map<String, Object> otherConfigs;\n+\n+        protected NewTopicCreationGroup(String group, SourceConnectorConfig config) {\n+            this.name = group;\n+            this.inclusionPattern = Pattern.compile(String.join(\"|\", config.topicCreationInclude(group)));\n+            this.exclusionPattern = Pattern.compile(String.join(\"|\", config.topicCreationExclude(group)));\n+            this.numPartitions = config.topicCreationPartitions(group);\n+            this.replicationFactor = config.topicCreationReplicationFactor(group);\n+            this.otherConfigs = config.topicCreationOtherConfigs(group);\n+        }\n+\n+        public String name() {\n+            return name;\n+        }\n+\n+        public boolean matches(String topic) {\n+            return !exclusionPattern.matcher(topic).matches() && inclusionPattern.matcher(topic).matches();\n+        }\n+\n+        public NewTopic newTopic(String topic) {\n+            NewTopicBuilder builder = new NewTopicBuilder(topic);\n+            return builder.partitions(numPartitions)\n+                    .replicationFactor(replicationFactor)\n+                    .config(otherConfigs)\n+                    .build();\n+        }\n+\n+        public static Map<String, NewTopicCreationGroup> configuredGroups(SourceConnectorConfig config) {\n+            List<String> groupNames = config.getList(TOPIC_CREATION_GROUPS_CONFIG);\n+            Map<String, NewTopicCreationGroup> groups = new LinkedHashMap<>();\n+            for (String group : groupNames) {\n+                groups.put(group, new NewTopicCreationGroup(group, config));\n+            }\n+            // Even if there was a group called 'default' in the config, it will be overriden here.\n+            // Order matters for all the topic groups besides the default, since it will be\n+            // removed from this collection by the Worker\n+            groups.put(DEFAULT_TOPIC_CREATION_GROUP, new NewTopicCreationGroup(DEFAULT_TOPIC_CREATION_GROUP, config));\n+            return groups;\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDAwMTI2Mw==", "bodyText": "NewTopicCreationGroup still needs a toString() method, since it's used in this log statement.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430001263", "createdAt": "2020-05-25T15:57:26Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -384,6 +452,40 @@ public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n         return true;\n     }\n \n+    // Due to transformations that may change the destination topic of a record (such as\n+    // RegexRouter) topic creation can not be batched for multiple topics\n+    private void maybeCreateTopic(String topic) {\n+        if (!topicCreation.isTopicCreationEnabled() || topicCreation.topicCache().contains(topic)) {\n+            return;\n+        }\n+        log.info(\"The task will send records to topic '{}' for the first time. Checking \"\n+                + \"whether topic exists\", topic);\n+        Map<String, TopicDescription> existing = admin.describeTopics(topic);\n+        if (!existing.isEmpty()) {\n+            log.info(\"Topic '{}' already exists.\", topic);\n+            topicCreation.topicCache().add(topic);\n+            return;\n+        }\n+\n+        log.info(\"Creating topic '{}'\", topic);\n+        NewTopicCreationGroup topicGroup = topicCreation.topicGroups().values().stream()\n+                .filter(group -> group.matches(topic))\n+                .findFirst()\n+                .orElse(topicCreation.defaultTopicGroup());\n+        log.debug(\"Topic '{}' matched topic creation group: {}\", topic, topicGroup);\n+        NewTopic newTopic = topicGroup.newTopic(topic);\n+\n+        if (admin.createTopic(newTopic)) {\n+            topicCreation.topicCache().add(topic);\n+            log.info(\"Created topic '{}' using creation group {}\", newTopic, topicGroup);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50"}, "originalPosition": 240}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDAwMzczNw==", "bodyText": "Seems like this could be pulled out into utils (?), at which point it's also a lot easier to test more thoroughly.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430003737", "createdAt": "2020-05-25T16:04:37Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -142,6 +155,57 @@ public WorkerSourceTask(ConnectorTaskId id,\n         this.sourceTaskMetricsGroup = new SourceTaskMetricsGroup(id, connectMetrics);\n         this.producerSendException = new AtomicReference<>();\n         this.isTopicTrackingEnabled = workerConfig.getBoolean(TOPIC_TRACKING_ENABLE_CONFIG);\n+        this.topicCreation = TopicCreation.newTopicCreation(workerConfig, topicGroups);\n+    }\n+\n+    public static class TopicCreation {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDAwMzkxNg==", "bodyText": "Nit: Since this is static, how about moving to before the member fields and methods?", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430003916", "createdAt": "2020-05-25T16:05:08Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -142,6 +155,57 @@ public WorkerSourceTask(ConnectorTaskId id,\n         this.sourceTaskMetricsGroup = new SourceTaskMetricsGroup(id, connectMetrics);\n         this.producerSendException = new AtomicReference<>();\n         this.isTopicTrackingEnabled = workerConfig.getBoolean(TOPIC_TRACKING_ENABLE_CONFIG);\n+        this.topicCreation = TopicCreation.newTopicCreation(workerConfig, topicGroups);\n+    }\n+\n+    public static class TopicCreation {\n+        private static final TopicCreation EMPTY =\n+                new TopicCreation(false, null, Collections.emptyMap(), Collections.emptySet());\n+\n+        private final boolean isTopicCreationEnabled;\n+        private final NewTopicCreationGroup defaultTopicGroup;\n+        private final Map<String, NewTopicCreationGroup> topicGroups;\n+        private final Set<String> topicCache;\n+\n+        protected TopicCreation(boolean isTopicCreationEnabled,\n+                                NewTopicCreationGroup defaultTopicGroup,\n+                                Map<String, NewTopicCreationGroup> topicGroups,\n+                                Set<String> topicCache) {\n+            this.isTopicCreationEnabled = isTopicCreationEnabled;\n+            this.defaultTopicGroup = defaultTopicGroup;\n+            this.topicGroups = topicGroups;\n+            this.topicCache = topicCache;\n+        }\n+\n+        public static TopicCreation newTopicCreation(WorkerConfig workerConfig,\n+                                                     Map<String, NewTopicCreationGroup> topicGroups) {\n+            if (!workerConfig.topicCreationEnable() || topicGroups == null) {\n+                return EMPTY;\n+            }\n+            Map<String, NewTopicCreationGroup> groups = new LinkedHashMap<>(topicGroups);\n+            groups.remove(DEFAULT_TOPIC_CREATION_GROUP);\n+            return new TopicCreation(true, topicGroups.get(DEFAULT_TOPIC_CREATION_GROUP), groups, new HashSet<>());\n+        }\n+\n+        public static TopicCreation empty() {\n+            return EMPTY;\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50"}, "originalPosition": 110}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5d3ca71df0ba24f3b3b8ef415994aa180053941f", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/5d3ca71df0ba24f3b3b8ef415994aa180053941f", "committedDate": "2020-05-25T21:45:52Z", "message": "KAFKA-5295: Add tests first"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1bab2240bb33a1827ffab4c617fabf116c419266", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/1bab2240bb33a1827ffab4c617fabf116c419266", "committedDate": "2020-05-25T21:45:52Z", "message": "KAFKA-5295: Add configuration for topic creation by source connectors"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cff6302222952cf6a056cebb96824c188777a568", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/cff6302222952cf6a056cebb96824c188777a568", "committedDate": "2020-05-25T21:45:52Z", "message": "KAFKA-5295: Adjust tests to new source connector configs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bf250eb37a76f02e96fed8222c8c01a445d436a3", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/bf250eb37a76f02e96fed8222c8c01a445d436a3", "committedDate": "2020-05-25T21:45:52Z", "message": "KAFKA-5295: Implement topic creation by tasks based on configuration groups"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "11d9e64cb9af1ebf14dfe08b0b492f023a9901ef", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/11d9e64cb9af1ebf14dfe08b0b492f023a9901ef", "committedDate": "2020-05-25T21:45:52Z", "message": "KAFKA-5295: Adapt existing tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3e25d6d1f72fc2b5ddd6e09ca1c1588aef5faa32", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/3e25d6d1f72fc2b5ddd6e09ca1c1588aef5faa32", "committedDate": "2020-05-26T00:41:45Z", "message": "KAFKA-5295: Adapt existing integration tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "05b48eb7343a6d185b5b1bc6dcc725d7f81ffe6d", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/05b48eb7343a6d185b5b1bc6dcc725d7f81ffe6d", "committedDate": "2020-05-26T00:41:45Z", "message": "KAFKA-5295: Adjust checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "91b6d4ee2c31a95e12689e5aa0cc9ad318045107", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/91b6d4ee2c31a95e12689e5aa0cc9ad318045107", "committedDate": "2020-05-26T00:41:45Z", "message": "KAFKA-5295: Tests with topic creation enabled"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "435ca349fae4f7c8ea7f29e53730aa9fd1ed6728", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/435ca349fae4f7c8ea7f29e53730aa9fd1ed6728", "committedDate": "2020-05-26T00:41:46Z", "message": "KAFKA-5295: New tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "800a40f479512ab3764d481a4102ad118a1bf1ef", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/800a40f479512ab3764d481a4102ad118a1bf1ef", "committedDate": "2020-05-26T00:41:46Z", "message": "KAFKA-5295: Keep connectors that don't use topic creation unaffected"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e98c70205afe7a0a8cc9bbdf051bc802a9252d1", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/1e98c70205afe7a0a8cc9bbdf051bc802a9252d1", "committedDate": "2020-05-26T00:41:46Z", "message": "KAFKA-5295: Upgrade integration test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f58be222070ed77dc80bfc17f44174de6c811869", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/f58be222070ed77dc80bfc17f44174de6c811869", "committedDate": "2020-05-26T00:41:46Z", "message": "KAFKA-5295: Minor cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "958cbe5eeba8992c4d2e7ed8594df5102881f842", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/958cbe5eeba8992c4d2e7ed8594df5102881f842", "committedDate": "2020-05-26T00:41:46Z", "message": "KAFKA-5295: Remove redundant code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0f90de4691dba7059b64b039365ee468fb008faa", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/0f90de4691dba7059b64b039365ee468fb008faa", "committedDate": "2020-05-26T00:41:46Z", "message": "KAFKA-5295: Maximize coverage for WorkerSourceTask"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4840a6bc003c7d71388ff5ba56246623bdb3c95e", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/4840a6bc003c7d71388ff5ba56246623bdb3c95e", "committedDate": "2020-05-26T00:41:46Z", "message": "KAFKA-5295: Unit tests for SourceConnectorConfig"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "98f737664d98623cecb7fecc49506ea7ed41bf57", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/98f737664d98623cecb7fecc49506ea7ed41bf57", "committedDate": "2020-05-26T00:41:46Z", "message": "KAFKA-5295: Address Randall's comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d5b6d266c2227352fa17a7fe7ecab6780aad3af3", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/d5b6d266c2227352fa17a7fe7ecab6780aad3af3", "committedDate": "2020-05-26T00:41:46Z", "message": "KAFKA-5295: Adapt tests after the changes of KAFKA-4794/KIP-131"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9fa3e099411def7b63b29925e3380e5ca308fdb7", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/9fa3e099411def7b63b29925e3380e5ca308fdb7", "committedDate": "2020-05-26T00:41:46Z", "message": "Fix doc in WorkerConfig\n\nCo-authored-by: Randall Hauch <rhauch@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e73395be6f435ad84cae837a97e4438a762b3288", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/e73395be6f435ad84cae837a97e4438a762b3288", "committedDate": "2020-05-26T00:41:46Z", "message": "KAFKA-5295: Set the id for the admin client used by the DLQ"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "645e2b2be42432a3f8589e31fddcbc619768ac92", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/645e2b2be42432a3f8589e31fddcbc619768ac92", "committedDate": "2020-05-26T00:41:46Z", "message": "KAFKA-5295: Refactor TopicCreation under the util package"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b8da7c53c4e89e760db011e54fdd4aa616f06224", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/b8da7c53c4e89e760db011e54fdd4aa616f06224", "committedDate": "2020-05-26T01:05:17Z", "message": "KAFKA-5295: Refactor TopicCreationGroup under its own class in util package"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/a880d6e6f926c830dc15261b752a764cb7844285", "committedDate": "2020-05-26T01:31:54Z", "message": "KAFKA-5295: Add isTopicCreationRequired and addTopic in TopicCreation util"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "453bbe24af39a9a12ba381598fb1ec65a9a6bd0e", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/453bbe24af39a9a12ba381598fb1ec65a9a6bd0e", "committedDate": "2020-05-25T22:12:05Z", "message": "Merge branch 'kip-158' of github.com:kkonstantine/kafka into kip-158"}, "afterCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/a880d6e6f926c830dc15261b752a764cb7844285", "committedDate": "2020-05-26T01:31:54Z", "message": "KAFKA-5295: Add isTopicCreationRequired and addTopic in TopicCreation util"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE3ODkyMTEy", "url": "https://github.com/apache/kafka/pull/8722#pullrequestreview-417892112", "createdAt": "2020-05-25T22:35:44Z", "commit": {"oid": "453bbe24af39a9a12ba381598fb1ec65a9a6bd0e"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNVQyMjozNTo0NFrOGaKkjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwMTozMjozMFrOGaMJNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDA4OTM1OQ==", "bodyText": "I have to admit two things:\na) I tried it when I wrote it, because I remembered that auto formatting in IntelliJ applies a similar type of ordering (although I can't reproduce with my current settings).\nb) I'm not a huge fan. I think I prefer the ordering that says:\nstatic member fields first, followed by non-static member fields, then constructors, then some logical ordering of the rest of the methods. I can see factory methods following constructors, but other than that I think a logical ordering of methods gives us good flexibility.\nI checked in case I had missed a recent updated guideline in Google Java Style Guide which we loosely follow, or elsewhere. Found another old but reasonable recommendation. But I didn't find any clear guideline for adding static methods before constructors, member fields and other methods. Given that this imposes an ordering other than logical for methods, I feel we could skip on that one. wdyt?", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430089359", "createdAt": "2020-05-25T22:35:44Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java", "diffHunk": "@@ -178,6 +192,76 @@ public NewTopic build() {\n         }\n     }\n \n+    public static class NewTopicCreationGroup {\n+        private final String name;\n+        private final Pattern inclusionPattern;\n+        private final Pattern exclusionPattern;\n+        private final int numPartitions;\n+        private final short replicationFactor;\n+        private final Map<String, Object> otherConfigs;\n+\n+        protected NewTopicCreationGroup(String group, SourceConnectorConfig config) {\n+            this.name = group;\n+            this.inclusionPattern = Pattern.compile(String.join(\"|\", config.topicCreationInclude(group)));\n+            this.exclusionPattern = Pattern.compile(String.join(\"|\", config.topicCreationExclude(group)));\n+            this.numPartitions = config.topicCreationPartitions(group);\n+            this.replicationFactor = config.topicCreationReplicationFactor(group);\n+            this.otherConfigs = config.topicCreationOtherConfigs(group);\n+        }\n+\n+        public String name() {\n+            return name;\n+        }\n+\n+        public boolean matches(String topic) {\n+            return !exclusionPattern.matcher(topic).matches() && inclusionPattern.matcher(topic).matches();\n+        }\n+\n+        public NewTopic newTopic(String topic) {\n+            NewTopicBuilder builder = new NewTopicBuilder(topic);\n+            return builder.partitions(numPartitions)\n+                    .replicationFactor(replicationFactor)\n+                    .config(otherConfigs)\n+                    .build();\n+        }\n+\n+        public static Map<String, NewTopicCreationGroup> configuredGroups(SourceConnectorConfig config) {\n+            List<String> groupNames = config.getList(TOPIC_CREATION_GROUPS_CONFIG);\n+            Map<String, NewTopicCreationGroup> groups = new LinkedHashMap<>();\n+            for (String group : groupNames) {\n+                groups.put(group, new NewTopicCreationGroup(group, config));\n+            }\n+            // Even if there was a group called 'default' in the config, it will be overriden here.\n+            // Order matters for all the topic groups besides the default, since it will be\n+            // removed from this collection by the Worker\n+            groups.put(DEFAULT_TOPIC_CREATION_GROUP, new NewTopicCreationGroup(DEFAULT_TOPIC_CREATION_GROUP, config));\n+            return groups;\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDAwMDE4Mw=="}, "originalCommit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDA4OTc1NA==", "bodyText": "Definitely. I'm in favor too if we don't mind the change. Added connector-dlq-adminclient-", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430089754", "createdAt": "2020-05-25T22:38:21Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "diffHunk": "@@ -678,7 +701,8 @@ ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) {\n         if (topic != null && !topic.isEmpty()) {\n             Map<String, Object> producerProps = producerConfigs(id, \"connector-dlq-producer-\" + id, config, connConfig, connectorClass,\n                                                                 connectorClientConfigOverridePolicy);\n-            Map<String, Object> adminProps = adminConfigs(id, config, connConfig, connectorClass, connectorClientConfigOverridePolicy);\n+            // Leaving default client id empty means that the admin client will set the default at instantiation time\n+            Map<String, Object> adminProps = adminConfigs(id, \"\", config, connConfig, connectorClass, connectorClientConfigOverridePolicy);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1NzkzMg=="}, "originalCommit": null, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDA5NDM1Mw==", "bodyText": "See reply on the subject on the similar comment. This factory method is right below the constructor here.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430094353", "createdAt": "2020-05-25T23:14:57Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -142,6 +155,57 @@ public WorkerSourceTask(ConnectorTaskId id,\n         this.sourceTaskMetricsGroup = new SourceTaskMetricsGroup(id, connectMetrics);\n         this.producerSendException = new AtomicReference<>();\n         this.isTopicTrackingEnabled = workerConfig.getBoolean(TOPIC_TRACKING_ENABLE_CONFIG);\n+        this.topicCreation = TopicCreation.newTopicCreation(workerConfig, topicGroups);\n+    }\n+\n+    public static class TopicCreation {\n+        private static final TopicCreation EMPTY =\n+                new TopicCreation(false, null, Collections.emptyMap(), Collections.emptySet());\n+\n+        private final boolean isTopicCreationEnabled;\n+        private final NewTopicCreationGroup defaultTopicGroup;\n+        private final Map<String, NewTopicCreationGroup> topicGroups;\n+        private final Set<String> topicCache;\n+\n+        protected TopicCreation(boolean isTopicCreationEnabled,\n+                                NewTopicCreationGroup defaultTopicGroup,\n+                                Map<String, NewTopicCreationGroup> topicGroups,\n+                                Set<String> topicCache) {\n+            this.isTopicCreationEnabled = isTopicCreationEnabled;\n+            this.defaultTopicGroup = defaultTopicGroup;\n+            this.topicGroups = topicGroups;\n+            this.topicCache = topicCache;\n+        }\n+\n+        public static TopicCreation newTopicCreation(WorkerConfig workerConfig,\n+                                                     Map<String, NewTopicCreationGroup> topicGroups) {\n+            if (!workerConfig.topicCreationEnable() || topicGroups == null) {\n+                return EMPTY;\n+            }\n+            Map<String, NewTopicCreationGroup> groups = new LinkedHashMap<>(topicGroups);\n+            groups.remove(DEFAULT_TOPIC_CREATION_GROUP);\n+            return new TopicCreation(true, topicGroups.get(DEFAULT_TOPIC_CREATION_GROUP), groups, new HashSet<>());\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk5NjgyMw=="}, "originalCommit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDExMTQ4MA==", "bodyText": "\ud83d\udc4d good catch for a second time. I added it, then removed after I added the name field and didn't add it back. It should be there now.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430111480", "createdAt": "2020-05-26T01:12:41Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -384,6 +452,40 @@ public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n         return true;\n     }\n \n+    // Due to transformations that may change the destination topic of a record (such as\n+    // RegexRouter) topic creation can not be batched for multiple topics\n+    private void maybeCreateTopic(String topic) {\n+        if (!topicCreation.isTopicCreationEnabled() || topicCreation.topicCache().contains(topic)) {\n+            return;\n+        }\n+        log.info(\"The task will send records to topic '{}' for the first time. Checking \"\n+                + \"whether topic exists\", topic);\n+        Map<String, TopicDescription> existing = admin.describeTopics(topic);\n+        if (!existing.isEmpty()) {\n+            log.info(\"Topic '{}' already exists.\", topic);\n+            topicCreation.topicCache().add(topic);\n+            return;\n+        }\n+\n+        log.info(\"Creating topic '{}'\", topic);\n+        NewTopicCreationGroup topicGroup = topicCreation.topicGroups().values().stream()\n+                .filter(group -> group.matches(topic))\n+                .findFirst()\n+                .orElse(topicCreation.defaultTopicGroup());\n+        log.debug(\"Topic '{}' matched topic creation group: {}\", topic, topicGroup);\n+        NewTopic newTopic = topicGroup.newTopic(topic);\n+\n+        if (admin.createTopic(newTopic)) {\n+            topicCreation.topicCache().add(topic);\n+            log.info(\"Created topic '{}' using creation group {}\", newTopic, topicGroup);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDAwMTI2Mw=="}, "originalCommit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50"}, "originalPosition": 240}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDExMTgwNw==", "bodyText": "Added for real this time :)", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430111807", "createdAt": "2020-05-26T01:14:21Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java", "diffHunk": "@@ -178,6 +191,47 @@ public NewTopic build() {\n         }\n     }\n \n+    public static class NewTopicCreationGroup {\n+        private final Pattern inclusionPattern;\n+        private final Pattern exclusionPattern;\n+        private final int numPartitions;\n+        private final short replicationFactor;\n+        private final Map<String, Object> otherConfigs;\n+\n+        protected NewTopicCreationGroup(String group, SourceConnectorConfig config) {\n+            inclusionPattern = Pattern.compile(String.join(\"|\", config.topicCreationInclude(group)));\n+            exclusionPattern = Pattern.compile(String.join(\"|\", config.topicCreationExclude(group)));\n+            numPartitions = config.topicCreationPartitions(group);\n+            replicationFactor = config.topicCreationReplicationFactor(group);\n+            otherConfigs = config.topicCreationOtherConfigs(group);\n+        }\n+\n+        public boolean matches(String topic) {\n+            return !exclusionPattern.matcher(topic).matches() && inclusionPattern.matcher(topic).matches();\n+        }\n+\n+        public NewTopic newTopic(String topic) {\n+            NewTopicBuilder builder = new NewTopicBuilder(topic);\n+            return builder.partitions(numPartitions)\n+                    .replicationFactor(replicationFactor)\n+                    .config(otherConfigs)\n+                    .build();\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY1OTcxNQ=="}, "originalCommit": null, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDExNTEyNg==", "bodyText": "Good observation. Done.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430115126", "createdAt": "2020-05-26T01:32:30Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -384,6 +452,40 @@ public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n         return true;\n     }\n \n+    // Due to transformations that may change the destination topic of a record (such as\n+    // RegexRouter) topic creation can not be batched for multiple topics\n+    private void maybeCreateTopic(String topic) {\n+        if (!topicCreation.isTopicCreationEnabled() || topicCreation.topicCache().contains(topic)) {\n+            return;\n+        }\n+        log.info(\"The task will send records to topic '{}' for the first time. Checking \"\n+                + \"whether topic exists\", topic);\n+        Map<String, TopicDescription> existing = admin.describeTopics(topic);\n+        if (!existing.isEmpty()) {\n+            log.info(\"Topic '{}' already exists.\", topic);\n+            topicCreation.topicCache().add(topic);\n+            return;\n+        }\n+\n+        log.info(\"Creating topic '{}'\", topic);\n+        NewTopicCreationGroup topicGroup = topicCreation.topicGroups().values().stream()\n+                .filter(group -> group.matches(topic))\n+                .findFirst()\n+                .orElse(topicCreation.defaultTopicGroup());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk5NzQ2Mw=="}, "originalCommit": {"oid": "6a06ba6b877b67b347d2fe3fcc7e70747c629d50"}, "originalPosition": 234}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4MzM3NzQw", "url": "https://github.com/apache/kafka/pull/8722#pullrequestreview-418337740", "createdAt": "2020-05-26T14:10:46Z", "commit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "state": "COMMENTED", "comments": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNDoxMDo0N1rOGagILg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQxNzo0NDoxM1rOGapVgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ0MjU0Mg==", "bodyText": "I'm fine with not ordering all public static methods first, but I think you'll agree that having this member getter method appearing before the constructor does not follow the conventions and patters we use throughout the project.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430442542", "createdAt": "2020-05-26T14:10:47Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SourceConnectorConfig.java", "diffHunk": "@@ -16,21 +16,157 @@\n  */\n package org.apache.kafka.connect.runtime;\n \n+import org.apache.kafka.common.config.AbstractConfig;\n import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n import org.apache.kafka.connect.runtime.isolation.Plugins;\n \n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_GROUP;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_PREFIX;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.EXCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.INCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.PARTITIONS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.REPLICATION_FACTOR_CONFIG;\n \n public class SourceConnectorConfig extends ConnectorConfig {\n \n-    private static ConfigDef config = ConnectorConfig.configDef();\n+    protected static final String TOPIC_CREATION_GROUP = \"Topic Creation\";\n+\n+    public static final String TOPIC_CREATION_PREFIX = \"topic.creation.\";\n+\n+    public static final String TOPIC_CREATION_GROUPS_CONFIG = TOPIC_CREATION_PREFIX + \"groups\";\n+    private static final String TOPIC_CREATION_GROUPS_DOC = \"Groups of configurations for topics \"\n+            + \"created by source connectors\";\n+    private static final String TOPIC_CREATION_GROUPS_DISPLAY = \"Topic Creation Groups\";\n+\n+    private static class EnrichedSourceConnectorConfig extends AbstractConfig {\n+        EnrichedSourceConnectorConfig(ConfigDef configDef, Map<String, String> props) {\n+            super(configDef, props);\n+        }\n+\n+        @Override\n+        public Object get(String key) {\n+            return super.get(key);\n+        }\n+    }\n+\n+    private static ConfigDef config = SourceConnectorConfig.configDef();\n+    private final EnrichedSourceConnectorConfig enrichedSourceConfig;\n \n     public static ConfigDef configDef() {\n-        return config;\n+        int orderInGroup = 0;\n+        return new ConfigDef(ConnectorConfig.configDef())\n+                .define(TOPIC_CREATION_GROUPS_CONFIG, ConfigDef.Type.LIST, Collections.emptyList(),\n+                        ConfigDef.CompositeValidator.of(new ConfigDef.NonNullValidator(), ConfigDef.LambdaValidator.with(\n+                            (name, value) -> {\n+                                List<?> groupAliases = (List<?>) value;\n+                                if (groupAliases.size() > new HashSet<>(groupAliases).size()) {\n+                                    throw new ConfigException(name, value, \"Duplicate alias provided.\");\n+                                }\n+                            },\n+                            () -> \"unique topic creation groups\")),\n+                        ConfigDef.Importance.LOW, TOPIC_CREATION_GROUPS_DOC, TOPIC_CREATION_GROUP,\n+                        ++orderInGroup, ConfigDef.Width.LONG, TOPIC_CREATION_GROUPS_DISPLAY);\n+    }\n+\n+    public static ConfigDef embedDefaultGroup(ConfigDef baseConfigDef) {\n+        String defaultGroup = \"default\";\n+        ConfigDef newDefaultDef = new ConfigDef(baseConfigDef);\n+        newDefaultDef.embed(DEFAULT_TOPIC_CREATION_PREFIX, defaultGroup, 0, TopicCreationConfig.defaultGroupConfigDef());\n+        return newDefaultDef;\n+    }\n+\n+    /**\n+     * Returns an enriched {@link ConfigDef} building upon the {@code ConfigDef}, using the current configuration specified in {@code props} as an input.\n+     *\n+     * @param baseConfigDef the base configuration definition to be enriched\n+     * @param props the non parsed configuration properties\n+     * @return the enriched configuration definition\n+     */\n+    public static ConfigDef enrich(ConfigDef baseConfigDef, Map<String, String> props, AbstractConfig defaultGroupConfig) {\n+        List<Object> topicCreationGroups = new ArrayList<>();\n+        Object aliases = ConfigDef.parseType(TOPIC_CREATION_GROUPS_CONFIG, props.get(TOPIC_CREATION_GROUPS_CONFIG), ConfigDef.Type.LIST);\n+        if (aliases instanceof List) {\n+            topicCreationGroups.addAll((List<?>) aliases);\n+        }\n+\n+        ConfigDef newDef = new ConfigDef(baseConfigDef);\n+        String defaultGroupPrefix = TOPIC_CREATION_PREFIX + DEFAULT_TOPIC_CREATION_GROUP + \".\";\n+        short defaultGroupReplicationFactor = defaultGroupConfig.getShort(defaultGroupPrefix + REPLICATION_FACTOR_CONFIG);\n+        int defaultGroupPartitions = defaultGroupConfig.getInt(defaultGroupPrefix + PARTITIONS_CONFIG);\n+        topicCreationGroups.stream().distinct().forEach(group -> {\n+            if (!(group instanceof String)) {\n+                throw new ConfigException(\"Item in \" + TOPIC_CREATION_GROUPS_CONFIG + \" property is not of type String\");\n+            }\n+            String alias = (String) group;\n+            String prefix = TOPIC_CREATION_PREFIX + alias + \".\";\n+            String configGroup = TOPIC_CREATION_GROUP + \": \" + alias;\n+            newDef.embed(prefix, configGroup, 0,\n+                    TopicCreationConfig.configDef(configGroup, defaultGroupReplicationFactor, defaultGroupPartitions));\n+        });\n+        return newDef;\n+    }\n+\n+    @Override\n+    public Object get(String key) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ1NjUzMw==", "bodyText": "This still is unclear, particularly what \"match their values\" means. Maybe:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static final String INCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n          \n          \n            \n                        + \"expressions that may match topic names. This list is used to include topics that \"\n          \n          \n            \n                        + \"match their values and apply this group's specific configuration to the topics \"\n          \n          \n            \n                        + \"that match this inclusion list.\";\n          \n          \n            \n                private static final String INCLUDE_REGEX_DOC = \"A list of regular expression literals \"\n          \n          \n            \n                        + \"used to match the names topics used by the source connector. This list is used \"\n          \n          \n            \n                        + \"to include topics that should be created using the topic settings defined by this group.\";", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430456533", "createdAt": "2020-05-26T14:29:15Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+public class TopicCreationConfig {\n+\n+    public static final String DEFAULT_TOPIC_CREATION_PREFIX = \"topic.creation.default.\";\n+    public static final String DEFAULT_TOPIC_CREATION_GROUP = \"default\";\n+\n+    public static final String INCLUDE_REGEX_CONFIG = \"include\";\n+    private static final String INCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to include topics that \"\n+            + \"match their values and apply this group's specific configuration to the topics \"\n+            + \"that match this inclusion list.\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ1OTEyNA==", "bodyText": "This still is unclear, particularly what \"match their values\" means. Maybe:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static final String EXCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n          \n          \n            \n                        + \"expressions that may match topic names. This list is used to exclude topics that \"\n          \n          \n            \n                        + \"match their values and refrain from applying this group's specific configuration \"\n          \n          \n            \n                        + \"to the topics that match this exclusion list. Note that exclusion rules have \"\n          \n          \n            \n                        + \"precedent and override any inclusion rules for topics. \";\n          \n          \n            \n                private static final String INCLUDE_REGEX_DOC = \"A list of regular expression literals \"\n          \n          \n            \n                        + \"used to match the names topics used by the source connector. This list is used \"\n          \n          \n            \n                        + \"to exclude topics from being created with the topic settings defined by this group. \"\n          \n          \n            \n                        + \"Note that exclusion rules have precedent and override any inclusion rules for the topics.\";", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430459124", "createdAt": "2020-05-26T14:32:32Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+public class TopicCreationConfig {\n+\n+    public static final String DEFAULT_TOPIC_CREATION_PREFIX = \"topic.creation.default.\";\n+    public static final String DEFAULT_TOPIC_CREATION_GROUP = \"default\";\n+\n+    public static final String INCLUDE_REGEX_CONFIG = \"include\";\n+    private static final String INCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to include topics that \"\n+            + \"match their values and apply this group's specific configuration to the topics \"\n+            + \"that match this inclusion list.\";\n+\n+    public static final String EXCLUDE_REGEX_CONFIG = \"exclude\";\n+    private static final String EXCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to exclude topics that \"\n+            + \"match their values and refrain from applying this group's specific configuration \"\n+            + \"to the topics that match this exclusion list. Note that exclusion rules have \"\n+            + \"precedent and override any inclusion rules for topics. \";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ2MjQ1MQ==", "bodyText": "Minor tweaks:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static final String REPLICATION_FACTOR_DOC = \"The replication factor for new topics \"\n          \n          \n            \n                        + \"created for this connector. This value must not be larger than the number of \"\n          \n          \n            \n                        + \"brokers in the Kafka cluster, or otherwise an error will be thrown when the \"\n          \n          \n            \n                        + \"connector will attempt to create a topic. For the default group this configuration\"\n          \n          \n            \n                        + \" is required. For any other group defined in topic.creation.groups this config is \"\n          \n          \n            \n                        + \"optional and if it's missing it gets the value the default group\";\n          \n          \n            \n                private static final String REPLICATION_FACTOR_DOC = \"The replication factor for new topics \"\n          \n          \n            \n                        + \"created for this connector using this group. This value may be -1 to use the broker's\"\n          \n          \n            \n                        + \"default replication factor, or may be a positive number not larger than the number of \"\n          \n          \n            \n                        + \"brokers in the Kafka cluster. A value larger than the number of brokers in the Kafka cluster \"\n          \n          \n            \n                        + \"will result in an error when the new topic is created. For the default group this configuration \"\n          \n          \n            \n                        + \"is required. For any other group defined in topic.creation.groups this config is \"\n          \n          \n            \n                        + \"optional and if it's missing it gets the value of the default group\";", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430462451", "createdAt": "2020-05-26T14:37:04Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+public class TopicCreationConfig {\n+\n+    public static final String DEFAULT_TOPIC_CREATION_PREFIX = \"topic.creation.default.\";\n+    public static final String DEFAULT_TOPIC_CREATION_GROUP = \"default\";\n+\n+    public static final String INCLUDE_REGEX_CONFIG = \"include\";\n+    private static final String INCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to include topics that \"\n+            + \"match their values and apply this group's specific configuration to the topics \"\n+            + \"that match this inclusion list.\";\n+\n+    public static final String EXCLUDE_REGEX_CONFIG = \"exclude\";\n+    private static final String EXCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to exclude topics that \"\n+            + \"match their values and refrain from applying this group's specific configuration \"\n+            + \"to the topics that match this exclusion list. Note that exclusion rules have \"\n+            + \"precedent and override any inclusion rules for topics. \";\n+\n+    public static final String REPLICATION_FACTOR_CONFIG = \"replication.factor\";\n+    private static final String REPLICATION_FACTOR_DOC = \"The replication factor for new topics \"\n+            + \"created for this connector. This value must not be larger than the number of \"\n+            + \"brokers in the Kafka cluster, or otherwise an error will be thrown when the \"\n+            + \"connector will attempt to create a topic. For the default group this configuration\"\n+            + \" is required. For any other group defined in topic.creation.groups this config is \"\n+            + \"optional and if it's missing it gets the value the default group\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ2Mzc2MQ==", "bodyText": "Minor tweaks:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                private static final String PARTITIONS_DOC = \"The number of partitions new topics created for\"\n          \n          \n            \n                        + \" this connector. For the default group this configuration is required. For any \"\n          \n          \n            \n                        + \"other group defined in topic.creation.groups this config is optional and if it's \"\n          \n          \n            \n                        + \"missing it gets the value the default group\";\n          \n          \n            \n                private static final String PARTITIONS_DOC = \"The number of partitions new topics created for \"\n          \n          \n            \n                        + \"this connector. This value may be -1 to use the broker's default number of partitions, \"\n          \n          \n            \n                        + \"or a positive number representing the desired number of partitions. \"\n          \n          \n            \n                        + \"For the default group this configuration is required. For any \"\n          \n          \n            \n                        + \"other group defined in topic.creation.groups this config is optional and if it's \"\n          \n          \n            \n                        + \"missing it gets the value of the default group\";", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430463761", "createdAt": "2020-05-26T14:38:48Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+public class TopicCreationConfig {\n+\n+    public static final String DEFAULT_TOPIC_CREATION_PREFIX = \"topic.creation.default.\";\n+    public static final String DEFAULT_TOPIC_CREATION_GROUP = \"default\";\n+\n+    public static final String INCLUDE_REGEX_CONFIG = \"include\";\n+    private static final String INCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to include topics that \"\n+            + \"match their values and apply this group's specific configuration to the topics \"\n+            + \"that match this inclusion list.\";\n+\n+    public static final String EXCLUDE_REGEX_CONFIG = \"exclude\";\n+    private static final String EXCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to exclude topics that \"\n+            + \"match their values and refrain from applying this group's specific configuration \"\n+            + \"to the topics that match this exclusion list. Note that exclusion rules have \"\n+            + \"precedent and override any inclusion rules for topics. \";\n+\n+    public static final String REPLICATION_FACTOR_CONFIG = \"replication.factor\";\n+    private static final String REPLICATION_FACTOR_DOC = \"The replication factor for new topics \"\n+            + \"created for this connector. This value must not be larger than the number of \"\n+            + \"brokers in the Kafka cluster, or otherwise an error will be thrown when the \"\n+            + \"connector will attempt to create a topic. For the default group this configuration\"\n+            + \" is required. For any other group defined in topic.creation.groups this config is \"\n+            + \"optional and if it's missing it gets the value the default group\";\n+\n+    public static final String PARTITIONS_CONFIG = \"partitions\";\n+    private static final String PARTITIONS_DOC = \"The number of partitions new topics created for\"\n+            + \" this connector. For the default group this configuration is required. For any \"\n+            + \"other group defined in topic.creation.groups this config is optional and if it's \"\n+            + \"missing it gets the value the default group\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ2NDQyOA==", "bodyText": "WDYT?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    () -> \"Positive number, or -1 to use the broker's default\"\n          \n          \n            \n                    () -> \"Positive number not larger than the number of brokers in the Kafka cluster, or -1 to use the broker's default\"", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430464428", "createdAt": "2020-05-26T14:39:42Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+public class TopicCreationConfig {\n+\n+    public static final String DEFAULT_TOPIC_CREATION_PREFIX = \"topic.creation.default.\";\n+    public static final String DEFAULT_TOPIC_CREATION_GROUP = \"default\";\n+\n+    public static final String INCLUDE_REGEX_CONFIG = \"include\";\n+    private static final String INCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to include topics that \"\n+            + \"match their values and apply this group's specific configuration to the topics \"\n+            + \"that match this inclusion list.\";\n+\n+    public static final String EXCLUDE_REGEX_CONFIG = \"exclude\";\n+    private static final String EXCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to exclude topics that \"\n+            + \"match their values and refrain from applying this group's specific configuration \"\n+            + \"to the topics that match this exclusion list. Note that exclusion rules have \"\n+            + \"precedent and override any inclusion rules for topics. \";\n+\n+    public static final String REPLICATION_FACTOR_CONFIG = \"replication.factor\";\n+    private static final String REPLICATION_FACTOR_DOC = \"The replication factor for new topics \"\n+            + \"created for this connector. This value must not be larger than the number of \"\n+            + \"brokers in the Kafka cluster, or otherwise an error will be thrown when the \"\n+            + \"connector will attempt to create a topic. For the default group this configuration\"\n+            + \" is required. For any other group defined in topic.creation.groups this config is \"\n+            + \"optional and if it's missing it gets the value the default group\";\n+\n+    public static final String PARTITIONS_CONFIG = \"partitions\";\n+    private static final String PARTITIONS_DOC = \"The number of partitions new topics created for\"\n+            + \" this connector. For the default group this configuration is required. For any \"\n+            + \"other group defined in topic.creation.groups this config is optional and if it's \"\n+            + \"missing it gets the value the default group\";\n+\n+    public static final ConfigDef.Validator REPLICATION_FACTOR_VALIDATOR = ConfigDef.LambdaValidator.with(\n+        (name, value) -> validateReplicationFactor(name, (short) value),\n+        () -> \"Positive number, or -1 to use the broker's default\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ2NTMzNw==", "bodyText": "Should we include the PatternSyntaxException or its message in the ConfigException? As it stands, it will be clear that the regex is invalid but not why the regex is invalid.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430465337", "createdAt": "2020-05-26T14:40:57Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+public class TopicCreationConfig {\n+\n+    public static final String DEFAULT_TOPIC_CREATION_PREFIX = \"topic.creation.default.\";\n+    public static final String DEFAULT_TOPIC_CREATION_GROUP = \"default\";\n+\n+    public static final String INCLUDE_REGEX_CONFIG = \"include\";\n+    private static final String INCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to include topics that \"\n+            + \"match their values and apply this group's specific configuration to the topics \"\n+            + \"that match this inclusion list.\";\n+\n+    public static final String EXCLUDE_REGEX_CONFIG = \"exclude\";\n+    private static final String EXCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to exclude topics that \"\n+            + \"match their values and refrain from applying this group's specific configuration \"\n+            + \"to the topics that match this exclusion list. Note that exclusion rules have \"\n+            + \"precedent and override any inclusion rules for topics. \";\n+\n+    public static final String REPLICATION_FACTOR_CONFIG = \"replication.factor\";\n+    private static final String REPLICATION_FACTOR_DOC = \"The replication factor for new topics \"\n+            + \"created for this connector. This value must not be larger than the number of \"\n+            + \"brokers in the Kafka cluster, or otherwise an error will be thrown when the \"\n+            + \"connector will attempt to create a topic. For the default group this configuration\"\n+            + \" is required. For any other group defined in topic.creation.groups this config is \"\n+            + \"optional and if it's missing it gets the value the default group\";\n+\n+    public static final String PARTITIONS_CONFIG = \"partitions\";\n+    private static final String PARTITIONS_DOC = \"The number of partitions new topics created for\"\n+            + \" this connector. For the default group this configuration is required. For any \"\n+            + \"other group defined in topic.creation.groups this config is optional and if it's \"\n+            + \"missing it gets the value the default group\";\n+\n+    public static final ConfigDef.Validator REPLICATION_FACTOR_VALIDATOR = ConfigDef.LambdaValidator.with(\n+        (name, value) -> validateReplicationFactor(name, (short) value),\n+        () -> \"Positive number, or -1 to use the broker's default\"\n+    );\n+    public static final ConfigDef.Validator PARTITIONS_VALIDATOR = ConfigDef.LambdaValidator.with(\n+        (name, value) -> validatePartitions(name, (int) value),\n+        () -> \"Positive number, or -1 to use the broker's default\"\n+    );\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConfigDef.Validator REGEX_VALIDATOR = ConfigDef.LambdaValidator.with(\n+        (name, value) -> {\n+            try {\n+                ((List<String>) value).forEach(Pattern::compile);\n+            } catch (PatternSyntaxException e) {\n+                throw new ConfigException(name, value, \"Syntax error in regular expression\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ2NTcxMA==", "bodyText": "How about:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                \"Replication factor must be positive, or -1 to use the broker's default\");\n          \n          \n            \n                                \"Replication factor must be positive and not larger than the number of brokers in the Kafka cluster, or -1 to use the broker's default\");", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430465710", "createdAt": "2020-05-26T14:41:30Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+public class TopicCreationConfig {\n+\n+    public static final String DEFAULT_TOPIC_CREATION_PREFIX = \"topic.creation.default.\";\n+    public static final String DEFAULT_TOPIC_CREATION_GROUP = \"default\";\n+\n+    public static final String INCLUDE_REGEX_CONFIG = \"include\";\n+    private static final String INCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to include topics that \"\n+            + \"match their values and apply this group's specific configuration to the topics \"\n+            + \"that match this inclusion list.\";\n+\n+    public static final String EXCLUDE_REGEX_CONFIG = \"exclude\";\n+    private static final String EXCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to exclude topics that \"\n+            + \"match their values and refrain from applying this group's specific configuration \"\n+            + \"to the topics that match this exclusion list. Note that exclusion rules have \"\n+            + \"precedent and override any inclusion rules for topics. \";\n+\n+    public static final String REPLICATION_FACTOR_CONFIG = \"replication.factor\";\n+    private static final String REPLICATION_FACTOR_DOC = \"The replication factor for new topics \"\n+            + \"created for this connector. This value must not be larger than the number of \"\n+            + \"brokers in the Kafka cluster, or otherwise an error will be thrown when the \"\n+            + \"connector will attempt to create a topic. For the default group this configuration\"\n+            + \" is required. For any other group defined in topic.creation.groups this config is \"\n+            + \"optional and if it's missing it gets the value the default group\";\n+\n+    public static final String PARTITIONS_CONFIG = \"partitions\";\n+    private static final String PARTITIONS_DOC = \"The number of partitions new topics created for\"\n+            + \" this connector. For the default group this configuration is required. For any \"\n+            + \"other group defined in topic.creation.groups this config is optional and if it's \"\n+            + \"missing it gets the value the default group\";\n+\n+    public static final ConfigDef.Validator REPLICATION_FACTOR_VALIDATOR = ConfigDef.LambdaValidator.with(\n+        (name, value) -> validateReplicationFactor(name, (short) value),\n+        () -> \"Positive number, or -1 to use the broker's default\"\n+    );\n+    public static final ConfigDef.Validator PARTITIONS_VALIDATOR = ConfigDef.LambdaValidator.with(\n+        (name, value) -> validatePartitions(name, (int) value),\n+        () -> \"Positive number, or -1 to use the broker's default\"\n+    );\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConfigDef.Validator REGEX_VALIDATOR = ConfigDef.LambdaValidator.with(\n+        (name, value) -> {\n+            try {\n+                ((List<String>) value).forEach(Pattern::compile);\n+            } catch (PatternSyntaxException e) {\n+                throw new ConfigException(name, value, \"Syntax error in regular expression\");\n+            }\n+        },\n+        () -> \"Positive number, or -1 to use the broker's default\"\n+    );\n+\n+    private static void validatePartitions(String configName, int factor) {\n+        if (factor != TopicAdmin.NO_PARTITIONS && factor < 1) {\n+            throw new ConfigException(configName, factor,\n+                    \"Number of partitions must be positive, or -1 to use the broker's default\");\n+        }\n+    }\n+\n+    private static void validateReplicationFactor(String configName, short factor) {\n+        if (factor != TopicAdmin.NO_REPLICATION_FACTOR && factor < 1) {\n+            throw new ConfigException(configName, factor,\n+                    \"Replication factor must be positive, or -1 to use the broker's default\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ3MTc0NQ==", "bodyText": "I'm not sure whether this is a good idea or not, but if configDef(String, ...) were changed to take Object for the defaultReplicationFactor and defaultPartitionCount, then this method could be replaced with:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public static ConfigDef defaultGroupConfigDef() {\n          \n          \n            \n                    int orderInGroup = 0;\n          \n          \n            \n                    ConfigDef configDef = new ConfigDef();\n          \n          \n            \n                    configDef\n          \n          \n            \n                            .define(INCLUDE_REGEX_CONFIG, ConfigDef.Type.LIST, \".*\",\n          \n          \n            \n                                    new ConfigDef.NonNullValidator(), ConfigDef.Importance.LOW,\n          \n          \n            \n                                    INCLUDE_REGEX_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup, ConfigDef.Width.LONG,\n          \n          \n            \n                                    \"Inclusion Topic Pattern for \" + DEFAULT_TOPIC_CREATION_GROUP)\n          \n          \n            \n                            .define(EXCLUDE_REGEX_CONFIG, ConfigDef.Type.LIST, Collections.emptyList(),\n          \n          \n            \n                                    new ConfigDef.NonNullValidator(), ConfigDef.Importance.LOW,\n          \n          \n            \n                                    EXCLUDE_REGEX_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup, ConfigDef.Width.LONG,\n          \n          \n            \n                                    \"Exclusion Topic Pattern for \" + DEFAULT_TOPIC_CREATION_GROUP)\n          \n          \n            \n                            .define(REPLICATION_FACTOR_CONFIG, ConfigDef.Type.SHORT,\n          \n          \n            \n                                    ConfigDef.NO_DEFAULT_VALUE, REPLICATION_FACTOR_VALIDATOR,\n          \n          \n            \n                                    ConfigDef.Importance.LOW, REPLICATION_FACTOR_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup,\n          \n          \n            \n                                    ConfigDef.Width.LONG, \"Replication Factor for Topics in \" + DEFAULT_TOPIC_CREATION_GROUP)\n          \n          \n            \n                            .define(PARTITIONS_CONFIG, ConfigDef.Type.INT,\n          \n          \n            \n                                    ConfigDef.NO_DEFAULT_VALUE, PARTITIONS_VALIDATOR,\n          \n          \n            \n                                    ConfigDef.Importance.LOW, PARTITIONS_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup,\n          \n          \n            \n                                    ConfigDef.Width.LONG, \"Partition Count for Topics in \" + DEFAULT_TOPIC_CREATION_GROUP);\n          \n          \n            \n                    return configDef;\n          \n          \n            \n                }\n          \n          \n            \n                public static ConfigDef defaultGroupConfigDef() {\n          \n          \n            \n                    return configDef(DEFAULT_TOPIC_CREATION_GROUP, ConfigDef.NO_DEFAULT_VALUE, ConfigDef.NO_DEFAULT_VALUE);\n          \n          \n            \n                }\n          \n      \n    \n    \n  \n\nEven though we'd lose a bit of type safety on the configDef(...) method, we'd more clearly show how the default is similar to the other rules.\nAnother alternative to maintain configDef(...) type safety is to accept Short and Integer, use NO_DEFAULT_VALUE if the parameters are null, and then change this method to:\n    public static ConfigDef defaultGroupConfigDef() {\n        return configDef(DEFAULT_TOPIC_CREATION_GROUP, null, null);\n    }\n\nAgain, not sure it's worth doing this. Up to you.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430471745", "createdAt": "2020-05-26T14:49:23Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+public class TopicCreationConfig {\n+\n+    public static final String DEFAULT_TOPIC_CREATION_PREFIX = \"topic.creation.default.\";\n+    public static final String DEFAULT_TOPIC_CREATION_GROUP = \"default\";\n+\n+    public static final String INCLUDE_REGEX_CONFIG = \"include\";\n+    private static final String INCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to include topics that \"\n+            + \"match their values and apply this group's specific configuration to the topics \"\n+            + \"that match this inclusion list.\";\n+\n+    public static final String EXCLUDE_REGEX_CONFIG = \"exclude\";\n+    private static final String EXCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to exclude topics that \"\n+            + \"match their values and refrain from applying this group's specific configuration \"\n+            + \"to the topics that match this exclusion list. Note that exclusion rules have \"\n+            + \"precedent and override any inclusion rules for topics. \";\n+\n+    public static final String REPLICATION_FACTOR_CONFIG = \"replication.factor\";\n+    private static final String REPLICATION_FACTOR_DOC = \"The replication factor for new topics \"\n+            + \"created for this connector. This value must not be larger than the number of \"\n+            + \"brokers in the Kafka cluster, or otherwise an error will be thrown when the \"\n+            + \"connector will attempt to create a topic. For the default group this configuration\"\n+            + \" is required. For any other group defined in topic.creation.groups this config is \"\n+            + \"optional and if it's missing it gets the value the default group\";\n+\n+    public static final String PARTITIONS_CONFIG = \"partitions\";\n+    private static final String PARTITIONS_DOC = \"The number of partitions new topics created for\"\n+            + \" this connector. For the default group this configuration is required. For any \"\n+            + \"other group defined in topic.creation.groups this config is optional and if it's \"\n+            + \"missing it gets the value the default group\";\n+\n+    public static final ConfigDef.Validator REPLICATION_FACTOR_VALIDATOR = ConfigDef.LambdaValidator.with(\n+        (name, value) -> validateReplicationFactor(name, (short) value),\n+        () -> \"Positive number, or -1 to use the broker's default\"\n+    );\n+    public static final ConfigDef.Validator PARTITIONS_VALIDATOR = ConfigDef.LambdaValidator.with(\n+        (name, value) -> validatePartitions(name, (int) value),\n+        () -> \"Positive number, or -1 to use the broker's default\"\n+    );\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConfigDef.Validator REGEX_VALIDATOR = ConfigDef.LambdaValidator.with(\n+        (name, value) -> {\n+            try {\n+                ((List<String>) value).forEach(Pattern::compile);\n+            } catch (PatternSyntaxException e) {\n+                throw new ConfigException(name, value, \"Syntax error in regular expression\");\n+            }\n+        },\n+        () -> \"Positive number, or -1 to use the broker's default\"\n+    );\n+\n+    private static void validatePartitions(String configName, int factor) {\n+        if (factor != TopicAdmin.NO_PARTITIONS && factor < 1) {\n+            throw new ConfigException(configName, factor,\n+                    \"Number of partitions must be positive, or -1 to use the broker's default\");\n+        }\n+    }\n+\n+    private static void validateReplicationFactor(String configName, short factor) {\n+        if (factor != TopicAdmin.NO_REPLICATION_FACTOR && factor < 1) {\n+            throw new ConfigException(configName, factor,\n+                    \"Replication factor must be positive, or -1 to use the broker's default\");\n+        }\n+    }\n+\n+    public static ConfigDef configDef(String group, short defaultReplicationFactor, int defaultParitionCount) {\n+        int orderInGroup = 0;\n+        ConfigDef configDef = new ConfigDef();\n+        configDef\n+                .define(INCLUDE_REGEX_CONFIG, ConfigDef.Type.LIST, Collections.emptyList(),\n+                        REGEX_VALIDATOR, ConfigDef.Importance.LOW,\n+                        INCLUDE_REGEX_DOC, group, ++orderInGroup, ConfigDef.Width.LONG,\n+                        \"Inclusion Topic Pattern for \" + group)\n+                .define(EXCLUDE_REGEX_CONFIG, ConfigDef.Type.LIST, Collections.emptyList(),\n+                        REGEX_VALIDATOR, ConfigDef.Importance.LOW,\n+                        EXCLUDE_REGEX_DOC, group, ++orderInGroup, ConfigDef.Width.LONG,\n+                        \"Exclusion Topic Pattern for \" + group)\n+                .define(REPLICATION_FACTOR_CONFIG, ConfigDef.Type.SHORT,\n+                        defaultReplicationFactor, REPLICATION_FACTOR_VALIDATOR,\n+                        ConfigDef.Importance.LOW, REPLICATION_FACTOR_DOC, group, ++orderInGroup,\n+                        ConfigDef.Width.LONG, \"Replication Factor for Topics in \" + group)\n+                .define(PARTITIONS_CONFIG, ConfigDef.Type.INT,\n+                        defaultParitionCount, PARTITIONS_VALIDATOR,\n+                        ConfigDef.Importance.LOW, PARTITIONS_DOC, group, ++orderInGroup,\n+                        ConfigDef.Width.LONG, \"Partition Count for Topics in \" + group);\n+        return configDef;\n+    }\n+\n+    public static ConfigDef defaultGroupConfigDef() {\n+        int orderInGroup = 0;\n+        ConfigDef configDef = new ConfigDef();\n+        configDef\n+                .define(INCLUDE_REGEX_CONFIG, ConfigDef.Type.LIST, \".*\",\n+                        new ConfigDef.NonNullValidator(), ConfigDef.Importance.LOW,\n+                        INCLUDE_REGEX_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup, ConfigDef.Width.LONG,\n+                        \"Inclusion Topic Pattern for \" + DEFAULT_TOPIC_CREATION_GROUP)\n+                .define(EXCLUDE_REGEX_CONFIG, ConfigDef.Type.LIST, Collections.emptyList(),\n+                        new ConfigDef.NonNullValidator(), ConfigDef.Importance.LOW,\n+                        EXCLUDE_REGEX_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup, ConfigDef.Width.LONG,\n+                        \"Exclusion Topic Pattern for \" + DEFAULT_TOPIC_CREATION_GROUP)\n+                .define(REPLICATION_FACTOR_CONFIG, ConfigDef.Type.SHORT,\n+                        ConfigDef.NO_DEFAULT_VALUE, REPLICATION_FACTOR_VALIDATOR,\n+                        ConfigDef.Importance.LOW, REPLICATION_FACTOR_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup,\n+                        ConfigDef.Width.LONG, \"Replication Factor for Topics in \" + DEFAULT_TOPIC_CREATION_GROUP)\n+                .define(PARTITIONS_CONFIG, ConfigDef.Type.INT,\n+                        ConfigDef.NO_DEFAULT_VALUE, PARTITIONS_VALIDATOR,\n+                        ConfigDef.Importance.LOW, PARTITIONS_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup,\n+                        ConfigDef.Width.LONG, \"Partition Count for Topics in \" + DEFAULT_TOPIC_CREATION_GROUP);\n+        return configDef;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ3NzQ2MQ==", "bodyText": "The \"source connectors to create topics by specifying...\" seems strange, since source connectors don't actually create topics and even if they did they wouldn't do so by specifying anything. Maybe:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                protected static final String TOPIC_CREATION_ENABLE_DOC = \"If set to true, it allows \"\n          \n          \n            \n                        + \"source connectors to create topics by specifying topic creation properties \"\n          \n          \n            \n                        + \"with the prefix `\" + TOPIC_CREATION_PREFIX + \"`. Each task will use an \"\n          \n          \n            \n                        + \"admin client to create its topics and will not depend on the Kafka brokers \"\n          \n          \n            \n                        + \"to create topics automatically.\";\n          \n          \n            \n                protected static final String TOPIC_CREATION_ENABLE_DOC = \"Whether to allow \"\n          \n          \n            \n                        + \"automatic creation of topics used by source connectors, when source connector \"\n          \n          \n            \n                        + \"are configured with `\" + TOPIC_CREATION_PREFIX + \"` properties. Each task will use an \"\n          \n          \n            \n                        + \"admin client to create its topics and will not depend on the Kafka brokers \"\n          \n          \n            \n                        + \"to create topics automatically.\";", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430477461", "createdAt": "2020-05-26T14:56:50Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfig.java", "diffHunk": "@@ -250,9 +251,17 @@\n             + \"user requests to reset the set of active topics per connector.\";\n     protected static final boolean TOPIC_TRACKING_ALLOW_RESET_DEFAULT = true;\n \n+    public static final String TOPIC_CREATION_ENABLE_CONFIG = \"topic.creation.enable\";\n+    protected static final String TOPIC_CREATION_ENABLE_DOC = \"If set to true, it allows \"\n+            + \"source connectors to create topics by specifying topic creation properties \"\n+            + \"with the prefix `\" + TOPIC_CREATION_PREFIX + \"`. Each task will use an \"\n+            + \"admin client to create its topics and will not depend on the Kafka brokers \"\n+            + \"to create topics automatically.\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ3ODEyOQ==", "bodyText": "Should this have a finally block that nulls the admin field?", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430478129", "createdAt": "2020-05-26T14:57:44Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -166,11 +176,14 @@ protected void close() {\n                 log.warn(\"Could not close producer\", t);\n             }\n         }\n-        try {\n-            transformationChain.close();\n-        } catch (Throwable t) {\n-            log.warn(\"Could not close transformation chain\", t);\n+        if (admin != null) {\n+            try {\n+                admin.close(Duration.ofSeconds(30));\n+            } catch (Throwable t) {\n+                log.warn(\"Failed to close admin client on time\", t);\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ3OTc1MA==", "bodyText": "I know this line existed before, but this writes out the record's key and value to the log. We should instead only write the record coordinates.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430479750", "createdAt": "2020-05-26T14:59:52Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -344,37 +357,38 @@ private boolean sendRecords() {\n                 }\n             }\n             try {\n+                maybeCreateTopic(record.topic());\n                 final String topic = producerRecord.topic();\n                 producer.send(\n-                        producerRecord,\n-                        new Callback() {\n-                            @Override\n-                            public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n-                                if (e != null) {\n-                                    log.error(\"{} failed to send record to {}:\", WorkerSourceTask.this, topic, e);\n-                                    log.debug(\"{} Failed record: {}\", WorkerSourceTask.this, preTransformRecord);\n-                                    producerSendException.compareAndSet(null, e);\n-                                } else {\n-                                    recordSent(producerRecord);\n-                                    counter.completeRecord();\n-                                    log.trace(\"{} Wrote record successfully: topic {} partition {} offset {}\",\n-                                            WorkerSourceTask.this,\n-                                            recordMetadata.topic(), recordMetadata.partition(),\n-                                            recordMetadata.offset());\n-                                    commitTaskRecord(preTransformRecord, recordMetadata);\n-                                    if (isTopicTrackingEnabled) {\n-                                        recordActiveTopic(producerRecord.topic());\n-                                    }\n-                                }\n+                    producerRecord,\n+                    (recordMetadata, e) -> {\n+                        if (e != null) {\n+                            log.error(\"{} failed to send record to {}: \", WorkerSourceTask.this, topic, e);\n+                            log.debug(\"{} Failed record: {}\", WorkerSourceTask.this, preTransformRecord);\n+                            producerSendException.compareAndSet(null, e);\n+                        } else {\n+                            recordSent(producerRecord);\n+                            counter.completeRecord();\n+                            log.trace(\"{} Wrote record successfully: topic {} partition {} offset {}\",\n+                                    WorkerSourceTask.this,\n+                                    recordMetadata.topic(), recordMetadata.partition(),\n+                                    recordMetadata.offset());\n+                            commitTaskRecord(preTransformRecord, recordMetadata);\n+                            if (isTopicTrackingEnabled) {\n+                                recordActiveTopic(producerRecord.topic());\n                             }\n-                        });\n+                        }\n+                    });\n                 lastSendFailed = false;\n-            } catch (org.apache.kafka.common.errors.RetriableException e) {\n-                log.warn(\"{} Failed to send {}, backing off before retrying:\", this, producerRecord, e);\n+            } catch (RetriableException | org.apache.kafka.common.errors.RetriableException e) {\n+                log.warn(\"{} Failed to send {}, backing off before retrying: \", this, producerRecord, e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ4MDIwNw==", "bodyText": "This is a new log line, but it's similar to an existing one above. Nevertheless, this will write out the record's key and value to the log. We should instead only write the record coordinates.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430480207", "createdAt": "2020-05-26T15:00:29Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -344,37 +357,38 @@ private boolean sendRecords() {\n                 }\n             }\n             try {\n+                maybeCreateTopic(record.topic());\n                 final String topic = producerRecord.topic();\n                 producer.send(\n-                        producerRecord,\n-                        new Callback() {\n-                            @Override\n-                            public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n-                                if (e != null) {\n-                                    log.error(\"{} failed to send record to {}:\", WorkerSourceTask.this, topic, e);\n-                                    log.debug(\"{} Failed record: {}\", WorkerSourceTask.this, preTransformRecord);\n-                                    producerSendException.compareAndSet(null, e);\n-                                } else {\n-                                    recordSent(producerRecord);\n-                                    counter.completeRecord();\n-                                    log.trace(\"{} Wrote record successfully: topic {} partition {} offset {}\",\n-                                            WorkerSourceTask.this,\n-                                            recordMetadata.topic(), recordMetadata.partition(),\n-                                            recordMetadata.offset());\n-                                    commitTaskRecord(preTransformRecord, recordMetadata);\n-                                    if (isTopicTrackingEnabled) {\n-                                        recordActiveTopic(producerRecord.topic());\n-                                    }\n-                                }\n+                    producerRecord,\n+                    (recordMetadata, e) -> {\n+                        if (e != null) {\n+                            log.error(\"{} failed to send record to {}: \", WorkerSourceTask.this, topic, e);\n+                            log.debug(\"{} Failed record: {}\", WorkerSourceTask.this, preTransformRecord);\n+                            producerSendException.compareAndSet(null, e);\n+                        } else {\n+                            recordSent(producerRecord);\n+                            counter.completeRecord();\n+                            log.trace(\"{} Wrote record successfully: topic {} partition {} offset {}\",\n+                                    WorkerSourceTask.this,\n+                                    recordMetadata.topic(), recordMetadata.partition(),\n+                                    recordMetadata.offset());\n+                            commitTaskRecord(preTransformRecord, recordMetadata);\n+                            if (isTopicTrackingEnabled) {\n+                                recordActiveTopic(producerRecord.topic());\n                             }\n-                        });\n+                        }\n+                    });\n                 lastSendFailed = false;\n-            } catch (org.apache.kafka.common.errors.RetriableException e) {\n-                log.warn(\"{} Failed to send {}, backing off before retrying:\", this, producerRecord, e);\n+            } catch (RetriableException | org.apache.kafka.common.errors.RetriableException e) {\n+                log.warn(\"{} Failed to send {}, backing off before retrying: \", this, producerRecord, e);\n                 toSend = toSend.subList(processed, toSend.size());\n                 lastSendFailed = true;\n                 counter.retryRemaining();\n                 return false;\n+            } catch (ConnectException e) {\n+                log.warn(\"{} Failed to send {} with unrecoverable exception: \", this, producerRecord, e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ4MDYyNg==", "bodyText": "It may be worth changing this line in a subsequent PR that can be backported.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430480626", "createdAt": "2020-05-26T15:01:05Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java", "diffHunk": "@@ -344,37 +357,38 @@ private boolean sendRecords() {\n                 }\n             }\n             try {\n+                maybeCreateTopic(record.topic());\n                 final String topic = producerRecord.topic();\n                 producer.send(\n-                        producerRecord,\n-                        new Callback() {\n-                            @Override\n-                            public void onCompletion(RecordMetadata recordMetadata, Exception e) {\n-                                if (e != null) {\n-                                    log.error(\"{} failed to send record to {}:\", WorkerSourceTask.this, topic, e);\n-                                    log.debug(\"{} Failed record: {}\", WorkerSourceTask.this, preTransformRecord);\n-                                    producerSendException.compareAndSet(null, e);\n-                                } else {\n-                                    recordSent(producerRecord);\n-                                    counter.completeRecord();\n-                                    log.trace(\"{} Wrote record successfully: topic {} partition {} offset {}\",\n-                                            WorkerSourceTask.this,\n-                                            recordMetadata.topic(), recordMetadata.partition(),\n-                                            recordMetadata.offset());\n-                                    commitTaskRecord(preTransformRecord, recordMetadata);\n-                                    if (isTopicTrackingEnabled) {\n-                                        recordActiveTopic(producerRecord.topic());\n-                                    }\n-                                }\n+                    producerRecord,\n+                    (recordMetadata, e) -> {\n+                        if (e != null) {\n+                            log.error(\"{} failed to send record to {}: \", WorkerSourceTask.this, topic, e);\n+                            log.debug(\"{} Failed record: {}\", WorkerSourceTask.this, preTransformRecord);\n+                            producerSendException.compareAndSet(null, e);\n+                        } else {\n+                            recordSent(producerRecord);\n+                            counter.completeRecord();\n+                            log.trace(\"{} Wrote record successfully: topic {} partition {} offset {}\",\n+                                    WorkerSourceTask.this,\n+                                    recordMetadata.topic(), recordMetadata.partition(),\n+                                    recordMetadata.offset());\n+                            commitTaskRecord(preTransformRecord, recordMetadata);\n+                            if (isTopicTrackingEnabled) {\n+                                recordActiveTopic(producerRecord.topic());\n                             }\n-                        });\n+                        }\n+                    });\n                 lastSendFailed = false;\n-            } catch (org.apache.kafka.common.errors.RetriableException e) {\n-                log.warn(\"{} Failed to send {}, backing off before retrying:\", this, producerRecord, e);\n+            } catch (RetriableException | org.apache.kafka.common.errors.RetriableException e) {\n+                log.warn(\"{} Failed to send {}, backing off before retrying: \", this, producerRecord, e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ3OTc1MA=="}, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5MzQxMA==", "bodyText": "This test class is already fairly complex. Can these move to a new TopicCreationTest class to correspond to the new TopicCreation class?", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430593410", "createdAt": "2020-05-26T17:44:13Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerSourceTaskWithTopicCreationTest.java", "diffHunk": "@@ -0,0 +1,1490 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.clients.admin.NewTopic;\n+import org.apache.kafka.clients.admin.TopicDescription;\n+import org.apache.kafka.clients.producer.KafkaProducer;\n+import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.InvalidRecordException;\n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.TopicPartitionInfo;\n+import org.apache.kafka.common.errors.InvalidTopicException;\n+import org.apache.kafka.common.errors.TopicAuthorizationException;\n+import org.apache.kafka.common.header.Header;\n+import org.apache.kafka.common.header.Headers;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.apache.kafka.common.utils.Time;\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.SchemaAndValue;\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.apache.kafka.connect.errors.RetriableException;\n+import org.apache.kafka.connect.header.ConnectHeaders;\n+import org.apache.kafka.connect.integration.MonitorableSourceConnector;\n+import org.apache.kafka.connect.runtime.ConnectMetrics.MetricGroup;\n+import org.apache.kafka.connect.runtime.WorkerSourceTask.SourceTaskMetricsGroup;\n+import org.apache.kafka.connect.runtime.distributed.ClusterConfigState;\n+import org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperatorTest;\n+import org.apache.kafka.connect.runtime.isolation.Plugins;\n+import org.apache.kafka.connect.runtime.standalone.StandaloneConfig;\n+import org.apache.kafka.connect.source.SourceRecord;\n+import org.apache.kafka.connect.source.SourceTask;\n+import org.apache.kafka.connect.source.SourceTaskContext;\n+import org.apache.kafka.connect.storage.CloseableOffsetStorageReader;\n+import org.apache.kafka.connect.storage.Converter;\n+import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.apache.kafka.connect.storage.OffsetStorageWriter;\n+import org.apache.kafka.connect.storage.StatusBackingStore;\n+import org.apache.kafka.connect.storage.StringConverter;\n+import org.apache.kafka.connect.util.Callback;\n+import org.apache.kafka.connect.util.ConnectorTaskId;\n+import org.apache.kafka.connect.util.ThreadedTest;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+import org.apache.kafka.connect.util.TopicCreation;\n+import org.apache.kafka.connect.util.TopicCreationGroup;\n+import org.easymock.Capture;\n+import org.easymock.EasyMock;\n+import org.easymock.IAnswer;\n+import org.easymock.IExpectationSetters;\n+import org.junit.After;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.powermock.api.easymock.PowerMock;\n+import org.powermock.api.easymock.annotation.Mock;\n+import org.powermock.api.easymock.annotation.MockStrict;\n+import org.powermock.core.classloader.annotations.PowerMockIgnore;\n+import org.powermock.modules.junit4.PowerMockRunner;\n+import org.powermock.reflect.Whitebox;\n+\n+import java.nio.ByteBuffer;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static org.apache.kafka.connect.integration.MonitorableSourceConnector.TOPIC_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.CONNECTOR_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.KEY_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.TASKS_MAX_CONFIG;\n+import static org.apache.kafka.connect.runtime.ConnectorConfig.VALUE_CONVERTER_CLASS_CONFIG;\n+import static org.apache.kafka.connect.runtime.SourceConnectorConfig.TOPIC_CREATION_GROUPS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_GROUP;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_PREFIX;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.EXCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.INCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.PARTITIONS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.REPLICATION_FACTOR_CONFIG;\n+import static org.apache.kafka.connect.runtime.WorkerConfig.TOPIC_CREATION_ENABLE_CONFIG;\n+import static org.hamcrest.CoreMatchers.hasItems;\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+\n+@PowerMockIgnore({\"javax.management.*\",\n+                  \"org.apache.log4j.*\"})\n+@RunWith(PowerMockRunner.class)\n+public class WorkerSourceTaskWithTopicCreationTest extends ThreadedTest {\n+    private static final String TOPIC = \"topic\";\n+    private static final String OTHER_TOPIC = \"other-topic\";\n+    private static final Map<String, byte[]> PARTITION = Collections.singletonMap(\"key\", \"partition\".getBytes());\n+    private static final Map<String, Integer> OFFSET = Collections.singletonMap(\"key\", 12);\n+\n+    // Connect-format data\n+    private static final Schema KEY_SCHEMA = Schema.INT32_SCHEMA;\n+    private static final Integer KEY = -1;\n+    private static final Schema RECORD_SCHEMA = Schema.INT64_SCHEMA;\n+    private static final Long RECORD = 12L;\n+    // Serialized data. The actual format of this data doesn't matter -- we just want to see that the right version\n+    // is used in the right place.\n+    private static final byte[] SERIALIZED_KEY = \"converted-key\".getBytes();\n+    private static final byte[] SERIALIZED_RECORD = \"converted-record\".getBytes();\n+\n+    private ExecutorService executor = Executors.newSingleThreadExecutor();\n+    private ConnectorTaskId taskId = new ConnectorTaskId(\"job\", 0);\n+    private ConnectorTaskId taskId1 = new ConnectorTaskId(\"job\", 1);\n+    private WorkerConfig config;\n+    private SourceConnectorConfig sourceConfig;\n+    private Plugins plugins;\n+    private MockConnectMetrics metrics;\n+    @Mock private SourceTask sourceTask;\n+    @Mock private Converter keyConverter;\n+    @Mock private Converter valueConverter;\n+    @Mock private HeaderConverter headerConverter;\n+    @Mock private TransformationChain<SourceRecord> transformationChain;\n+    @Mock private KafkaProducer<byte[], byte[]> producer;\n+    @Mock private TopicAdmin admin;\n+    @Mock private CloseableOffsetStorageReader offsetReader;\n+    @Mock private OffsetStorageWriter offsetWriter;\n+    @Mock private ClusterConfigState clusterConfigState;\n+    private WorkerSourceTask workerTask;\n+    @Mock private Future<RecordMetadata> sendFuture;\n+    @MockStrict private TaskStatus.Listener statusListener;\n+    @Mock private StatusBackingStore statusBackingStore;\n+\n+    private Capture<org.apache.kafka.clients.producer.Callback> producerCallbacks;\n+\n+    private static final Map<String, String> TASK_PROPS = new HashMap<>();\n+    static {\n+        TASK_PROPS.put(TaskConfig.TASK_CLASS_CONFIG, TestSourceTask.class.getName());\n+    }\n+    private static final TaskConfig TASK_CONFIG = new TaskConfig(TASK_PROPS);\n+\n+    private static final List<SourceRecord> RECORDS = Arrays.asList(\n+            new SourceRecord(PARTITION, OFFSET, TOPIC, null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD)\n+    );\n+\n+    // when this test becomes parameterized, this variable will be a test parameter\n+    public boolean enableTopicCreation = true;\n+\n+    @Override\n+    public void setup() {\n+        super.setup();\n+        Map<String, String> workerProps = workerProps();\n+        plugins = new Plugins(workerProps);\n+        config = new StandaloneConfig(workerProps);\n+        sourceConfig = new SourceConnectorConfig(plugins, sourceConnectorPropsWithGroups(TOPIC), true);\n+        producerCallbacks = EasyMock.newCapture();\n+        metrics = new MockConnectMetrics();\n+    }\n+\n+    private Map<String, String> workerProps() {\n+        Map<String, String> props = new HashMap<>();\n+        props.put(\"key.converter\", \"org.apache.kafka.connect.json.JsonConverter\");\n+        props.put(\"value.converter\", \"org.apache.kafka.connect.json.JsonConverter\");\n+        props.put(\"internal.key.converter\", \"org.apache.kafka.connect.json.JsonConverter\");\n+        props.put(\"internal.value.converter\", \"org.apache.kafka.connect.json.JsonConverter\");\n+        props.put(\"internal.key.converter.schemas.enable\", \"false\");\n+        props.put(\"internal.value.converter.schemas.enable\", \"false\");\n+        props.put(\"offset.storage.file.filename\", \"/tmp/connect.offsets\");\n+        props.put(TOPIC_CREATION_ENABLE_CONFIG, String.valueOf(enableTopicCreation));\n+        return props;\n+    }\n+\n+    private Map<String, String> sourceConnectorPropsWithGroups(String topic) {\n+        // setup up props for the source connector\n+        Map<String, String> props = new HashMap<>();\n+        props.put(\"name\", \"foo-connector\");\n+        props.put(CONNECTOR_CLASS_CONFIG, MonitorableSourceConnector.class.getSimpleName());\n+        props.put(TASKS_MAX_CONFIG, String.valueOf(1));\n+        props.put(TOPIC_CONFIG, topic);\n+        props.put(KEY_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n+        props.put(VALUE_CONVERTER_CLASS_CONFIG, StringConverter.class.getName());\n+        props.put(TOPIC_CREATION_GROUPS_CONFIG, String.join(\",\", \"foo\", \"bar\"));\n+        props.put(DEFAULT_TOPIC_CREATION_PREFIX + REPLICATION_FACTOR_CONFIG, String.valueOf(1));\n+        props.put(DEFAULT_TOPIC_CREATION_PREFIX + PARTITIONS_CONFIG, String.valueOf(1));\n+        props.put(SourceConnectorConfig.TOPIC_CREATION_PREFIX + \"foo\" + \".\" + INCLUDE_REGEX_CONFIG, topic);\n+        props.put(SourceConnectorConfig.TOPIC_CREATION_PREFIX + \"bar\" + \".\" + INCLUDE_REGEX_CONFIG, \".*\");\n+        props.put(SourceConnectorConfig.TOPIC_CREATION_PREFIX + \"bar\" + \".\" + EXCLUDE_REGEX_CONFIG, topic);\n+        return props;\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        if (metrics != null) metrics.stop();\n+    }\n+\n+    private void createWorkerTask() {\n+        createWorkerTask(TargetState.STARTED);\n+    }\n+\n+    private void createWorkerTask(TargetState initialState) {\n+        createWorkerTask(initialState, keyConverter, valueConverter, headerConverter);\n+    }\n+\n+    private void createWorkerTask(TargetState initialState, Converter keyConverter, Converter valueConverter, HeaderConverter headerConverter) {\n+        workerTask = new WorkerSourceTask(taskId, sourceTask, statusListener, initialState, keyConverter, valueConverter, headerConverter,\n+                transformationChain, producer, admin, TopicCreationGroup.configuredGroups(sourceConfig),\n+                offsetReader, offsetWriter, config, clusterConfigState, metrics, plugins.delegatingLoader(), Time.SYSTEM,\n+                RetryWithToleranceOperatorTest.NOOP_OPERATOR, statusBackingStore);\n+    }\n+\n+    @Test\n+    public void testStartPaused() throws Exception {\n+        final CountDownLatch pauseLatch = new CountDownLatch(1);\n+\n+        createWorkerTask(TargetState.PAUSED);\n+\n+        statusListener.onPause(taskId);\n+        EasyMock.expectLastCall().andAnswer(new IAnswer<Void>() {\n+            @Override\n+            public Void answer() throws Throwable {\n+                pauseLatch.countDown();\n+                return null;\n+            }\n+        });\n+\n+        expectClose();\n+\n+        statusListener.onShutdown(taskId);\n+        EasyMock.expectLastCall();\n+\n+        PowerMock.replayAll();\n+\n+        workerTask.initialize(TASK_CONFIG);\n+        Future<?> taskFuture = executor.submit(workerTask);\n+\n+        assertTrue(pauseLatch.await(5, TimeUnit.SECONDS));\n+        workerTask.stop();\n+        assertTrue(workerTask.awaitStop(1000));\n+\n+        taskFuture.get();\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testPause() throws Exception {\n+        createWorkerTask();\n+\n+        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));\n+        EasyMock.expectLastCall();\n+        sourceTask.start(TASK_PROPS);\n+        EasyMock.expectLastCall();\n+        statusListener.onStartup(taskId);\n+        EasyMock.expectLastCall();\n+\n+        AtomicInteger count = new AtomicInteger(0);\n+        CountDownLatch pollLatch = expectPolls(10, count);\n+        // In this test, we don't flush, so nothing goes any further than the offset writer\n+\n+        expectTopicCreation(TOPIC);\n+\n+        statusListener.onPause(taskId);\n+        EasyMock.expectLastCall();\n+\n+        sourceTask.stop();\n+        EasyMock.expectLastCall();\n+        expectOffsetFlush(true);\n+\n+        statusListener.onShutdown(taskId);\n+        EasyMock.expectLastCall();\n+\n+        expectClose();\n+\n+        PowerMock.replayAll();\n+\n+        workerTask.initialize(TASK_CONFIG);\n+        Future<?> taskFuture = executor.submit(workerTask);\n+        assertTrue(awaitLatch(pollLatch));\n+\n+        workerTask.transitionTo(TargetState.PAUSED);\n+\n+        int priorCount = count.get();\n+        Thread.sleep(100);\n+\n+        // since the transition is observed asynchronously, the count could be off by one loop iteration\n+        assertTrue(count.get() - priorCount <= 1);\n+\n+        workerTask.stop();\n+        assertTrue(workerTask.awaitStop(1000));\n+\n+        taskFuture.get();\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testPollsInBackground() throws Exception {\n+        createWorkerTask();\n+\n+        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));\n+        EasyMock.expectLastCall();\n+        sourceTask.start(TASK_PROPS);\n+        EasyMock.expectLastCall();\n+        statusListener.onStartup(taskId);\n+        EasyMock.expectLastCall();\n+\n+        final CountDownLatch pollLatch = expectPolls(10);\n+        // In this test, we don't flush, so nothing goes any further than the offset writer\n+\n+        expectTopicCreation(TOPIC);\n+\n+        sourceTask.stop();\n+        EasyMock.expectLastCall();\n+        expectOffsetFlush(true);\n+\n+        statusListener.onShutdown(taskId);\n+        EasyMock.expectLastCall();\n+\n+        expectClose();\n+\n+        PowerMock.replayAll();\n+\n+        workerTask.initialize(TASK_CONFIG);\n+        Future<?> taskFuture = executor.submit(workerTask);\n+\n+        assertTrue(awaitLatch(pollLatch));\n+        workerTask.stop();\n+        assertTrue(workerTask.awaitStop(1000));\n+\n+        taskFuture.get();\n+        assertPollMetrics(10);\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testFailureInPoll() throws Exception {\n+        createWorkerTask();\n+\n+        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));\n+        EasyMock.expectLastCall();\n+        sourceTask.start(TASK_PROPS);\n+        EasyMock.expectLastCall();\n+        statusListener.onStartup(taskId);\n+        EasyMock.expectLastCall();\n+\n+        final CountDownLatch pollLatch = new CountDownLatch(1);\n+        final RuntimeException exception = new RuntimeException();\n+        EasyMock.expect(sourceTask.poll()).andAnswer(new IAnswer<List<SourceRecord>>() {\n+            @Override\n+            public List<SourceRecord> answer() throws Throwable {\n+                pollLatch.countDown();\n+                throw exception;\n+            }\n+        });\n+\n+        statusListener.onFailure(taskId, exception);\n+        EasyMock.expectLastCall();\n+\n+        sourceTask.stop();\n+        EasyMock.expectLastCall();\n+        expectOffsetFlush(true);\n+\n+        expectClose();\n+\n+        PowerMock.replayAll();\n+\n+        workerTask.initialize(TASK_CONFIG);\n+        Future<?> taskFuture = executor.submit(workerTask);\n+\n+        assertTrue(awaitLatch(pollLatch));\n+        workerTask.stop();\n+        assertTrue(workerTask.awaitStop(1000));\n+\n+        taskFuture.get();\n+        assertPollMetrics(0);\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testPollReturnsNoRecords() throws Exception {\n+        // Test that the task handles an empty list of records\n+        createWorkerTask();\n+\n+        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));\n+        EasyMock.expectLastCall();\n+        sourceTask.start(TASK_PROPS);\n+        EasyMock.expectLastCall();\n+        statusListener.onStartup(taskId);\n+        EasyMock.expectLastCall();\n+\n+        // We'll wait for some data, then trigger a flush\n+        final CountDownLatch pollLatch = expectEmptyPolls(1, new AtomicInteger());\n+        expectOffsetFlush(true);\n+\n+        sourceTask.stop();\n+        EasyMock.expectLastCall();\n+        expectOffsetFlush(true);\n+\n+        statusListener.onShutdown(taskId);\n+        EasyMock.expectLastCall();\n+\n+        expectClose();\n+\n+        PowerMock.replayAll();\n+\n+        workerTask.initialize(TASK_CONFIG);\n+        Future<?> taskFuture = executor.submit(workerTask);\n+\n+        assertTrue(awaitLatch(pollLatch));\n+        assertTrue(workerTask.commitOffsets());\n+        workerTask.stop();\n+        assertTrue(workerTask.awaitStop(1000));\n+\n+        taskFuture.get();\n+        assertPollMetrics(0);\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testCommit() throws Exception {\n+        // Test that the task commits properly when prompted\n+        createWorkerTask();\n+\n+        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));\n+        EasyMock.expectLastCall();\n+        sourceTask.start(TASK_PROPS);\n+        EasyMock.expectLastCall();\n+        statusListener.onStartup(taskId);\n+        EasyMock.expectLastCall();\n+\n+        // We'll wait for some data, then trigger a flush\n+        final CountDownLatch pollLatch = expectPolls(1);\n+        expectOffsetFlush(true);\n+\n+        expectTopicCreation(TOPIC);\n+\n+        sourceTask.stop();\n+        EasyMock.expectLastCall();\n+        expectOffsetFlush(true);\n+\n+        statusListener.onShutdown(taskId);\n+        EasyMock.expectLastCall();\n+\n+        expectClose();\n+\n+        PowerMock.replayAll();\n+\n+        workerTask.initialize(TASK_CONFIG);\n+        Future<?> taskFuture = executor.submit(workerTask);\n+\n+        assertTrue(awaitLatch(pollLatch));\n+        assertTrue(workerTask.commitOffsets());\n+        workerTask.stop();\n+        assertTrue(workerTask.awaitStop(1000));\n+\n+        taskFuture.get();\n+        assertPollMetrics(1);\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testCommitFailure() throws Exception {\n+        // Test that the task commits properly when prompted\n+        createWorkerTask();\n+\n+        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));\n+        EasyMock.expectLastCall();\n+        sourceTask.start(TASK_PROPS);\n+        EasyMock.expectLastCall();\n+        statusListener.onStartup(taskId);\n+        EasyMock.expectLastCall();\n+\n+        // We'll wait for some data, then trigger a flush\n+        final CountDownLatch pollLatch = expectPolls(1);\n+        expectOffsetFlush(true);\n+\n+        expectTopicCreation(TOPIC);\n+\n+        sourceTask.stop();\n+        EasyMock.expectLastCall();\n+        expectOffsetFlush(false);\n+\n+        statusListener.onShutdown(taskId);\n+        EasyMock.expectLastCall();\n+\n+        expectClose();\n+\n+        PowerMock.replayAll();\n+\n+        workerTask.initialize(TASK_CONFIG);\n+        Future<?> taskFuture = executor.submit(workerTask);\n+\n+        assertTrue(awaitLatch(pollLatch));\n+        assertTrue(workerTask.commitOffsets());\n+        workerTask.stop();\n+        assertTrue(workerTask.awaitStop(1000));\n+\n+        taskFuture.get();\n+        assertPollMetrics(1);\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testSendRecordsConvertsData() throws Exception {\n+        createWorkerTask();\n+\n+        List<SourceRecord> records = new ArrayList<>();\n+        // Can just use the same record for key and value\n+        records.add(new SourceRecord(PARTITION, OFFSET, TOPIC, null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD));\n+\n+        Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecordAnyTimes();\n+\n+        expectTopicCreation(TOPIC);\n+\n+        PowerMock.replayAll();\n+\n+        Whitebox.setInternalState(workerTask, \"toSend\", records);\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(SERIALIZED_KEY, sent.getValue().key());\n+        assertEquals(SERIALIZED_RECORD, sent.getValue().value());\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testSendRecordsPropagatesTimestamp() throws Exception {\n+        final Long timestamp = System.currentTimeMillis();\n+\n+        createWorkerTask();\n+\n+        List<SourceRecord> records = Collections.singletonList(\n+                new SourceRecord(PARTITION, OFFSET, TOPIC, null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD, timestamp)\n+        );\n+\n+        Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecordAnyTimes();\n+\n+        expectTopicCreation(TOPIC);\n+\n+        PowerMock.replayAll();\n+\n+        Whitebox.setInternalState(workerTask, \"toSend\", records);\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(timestamp, sent.getValue().timestamp());\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test(expected = InvalidRecordException.class)\n+    public void testSendRecordsCorruptTimestamp() throws Exception {\n+        final Long timestamp = -3L;\n+        createWorkerTask();\n+\n+        List<SourceRecord> records = Collections.singletonList(\n+                new SourceRecord(PARTITION, OFFSET, TOPIC, null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD, timestamp)\n+        );\n+\n+        Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecordAnyTimes();\n+\n+        PowerMock.replayAll();\n+\n+        Whitebox.setInternalState(workerTask, \"toSend\", records);\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(null, sent.getValue().timestamp());\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testSendRecordsNoTimestamp() throws Exception {\n+        final Long timestamp = -1L;\n+        createWorkerTask();\n+\n+        List<SourceRecord> records = Collections.singletonList(\n+                new SourceRecord(PARTITION, OFFSET, TOPIC, null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD, timestamp)\n+        );\n+\n+        Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecordAnyTimes();\n+\n+        expectTopicCreation(TOPIC);\n+\n+        PowerMock.replayAll();\n+\n+        Whitebox.setInternalState(workerTask, \"toSend\", records);\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(null, sent.getValue().timestamp());\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testSendRecordsRetries() throws Exception {\n+        createWorkerTask();\n+\n+        // Differentiate only by Kafka partition so we can reuse conversion expectations\n+        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, TOPIC, 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, TOPIC, 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record3 = new SourceRecord(PARTITION, OFFSET, TOPIC, 3, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+\n+        expectTopicCreation(TOPIC);\n+\n+        // First round\n+        expectSendRecordOnce(false);\n+        // Any Producer retriable exception should work here\n+        expectSendRecordSyncFailure(new org.apache.kafka.common.errors.TimeoutException(\"retriable sync failure\"));\n+\n+        // Second round\n+        expectSendRecordOnce(true);\n+        expectSendRecordOnce(false);\n+\n+        PowerMock.replayAll();\n+\n+        // Try to send 3, make first pass, second fail. Should save last two\n+        Whitebox.setInternalState(workerTask, \"toSend\", Arrays.asList(record1, record2, record3));\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(true, Whitebox.getInternalState(workerTask, \"lastSendFailed\"));\n+        assertEquals(Arrays.asList(record2, record3), Whitebox.getInternalState(workerTask, \"toSend\"));\n+\n+        // Next they all succeed\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(false, Whitebox.getInternalState(workerTask, \"lastSendFailed\"));\n+        assertNull(Whitebox.getInternalState(workerTask, \"toSend\"));\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test(expected = ConnectException.class)\n+    public void testSendRecordsProducerCallbackFail() throws Exception {\n+        createWorkerTask();\n+\n+        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, TOPIC, 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, TOPIC, 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+\n+        expectTopicCreation(TOPIC);\n+\n+        expectSendRecordProducerCallbackFail();\n+\n+        PowerMock.replayAll();\n+\n+        Whitebox.setInternalState(workerTask, \"toSend\", Arrays.asList(record1, record2));\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+    }\n+\n+    @Test(expected = ConnectException.class)\n+    public void testSendRecordsProducerSendFailsImmediately() throws Exception {\n+        createWorkerTask();\n+\n+        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, TOPIC, 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, TOPIC, 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+\n+        expectPreliminaryCalls();\n+        expectTopicCreation(TOPIC);\n+\n+        EasyMock.expect(producer.send(EasyMock.anyObject(), EasyMock.anyObject()))\n+                .andThrow(new KafkaException(\"Producer closed while send in progress\", new InvalidTopicException(TOPIC)));\n+\n+        PowerMock.replayAll();\n+\n+        Whitebox.setInternalState(workerTask, \"toSend\", Arrays.asList(record1, record2));\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+    }\n+\n+    @Test\n+    public void testSendRecordsTaskCommitRecordFail() throws Exception {\n+        createWorkerTask();\n+\n+        // Differentiate only by Kafka partition so we can reuse conversion expectations\n+        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, TOPIC, 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, TOPIC, 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record3 = new SourceRecord(PARTITION, OFFSET, TOPIC, 3, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+\n+        expectTopicCreation(TOPIC);\n+\n+        // Source task commit record failure will not cause the task to abort\n+        expectSendRecordOnce(false);\n+        expectSendRecordTaskCommitRecordFail(false, false);\n+        expectSendRecordOnce(false);\n+\n+        PowerMock.replayAll();\n+\n+        Whitebox.setInternalState(workerTask, \"toSend\", Arrays.asList(record1, record2, record3));\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(false, Whitebox.getInternalState(workerTask, \"lastSendFailed\"));\n+        assertNull(Whitebox.getInternalState(workerTask, \"toSend\"));\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testSlowTaskStart() throws Exception {\n+        final CountDownLatch startupLatch = new CountDownLatch(1);\n+        final CountDownLatch finishStartupLatch = new CountDownLatch(1);\n+\n+        createWorkerTask();\n+\n+        sourceTask.initialize(EasyMock.anyObject(SourceTaskContext.class));\n+        EasyMock.expectLastCall();\n+        sourceTask.start(TASK_PROPS);\n+        EasyMock.expectLastCall().andAnswer(new IAnswer<Object>() {\n+            @Override\n+            public Object answer() throws Throwable {\n+                startupLatch.countDown();\n+                assertTrue(awaitLatch(finishStartupLatch));\n+                return null;\n+            }\n+        });\n+\n+        statusListener.onStartup(taskId);\n+        EasyMock.expectLastCall();\n+\n+        sourceTask.stop();\n+        EasyMock.expectLastCall();\n+        expectOffsetFlush(true);\n+\n+        statusListener.onShutdown(taskId);\n+        EasyMock.expectLastCall();\n+\n+        expectClose();\n+\n+        PowerMock.replayAll();\n+\n+        workerTask.initialize(TASK_CONFIG);\n+        Future<?> workerTaskFuture = executor.submit(workerTask);\n+\n+        // Stopping immediately while the other thread has work to do should result in no polling, no offset commits,\n+        // exiting the work thread immediately, and the stop() method will be invoked in the background thread since it\n+        // cannot be invoked immediately in the thread trying to stop the task.\n+        assertTrue(awaitLatch(startupLatch));\n+        workerTask.stop();\n+        finishStartupLatch.countDown();\n+        assertTrue(workerTask.awaitStop(1000));\n+\n+        workerTaskFuture.get();\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testCancel() {\n+        createWorkerTask();\n+\n+        offsetReader.close();\n+        PowerMock.expectLastCall();\n+\n+        PowerMock.replayAll();\n+\n+        workerTask.cancel();\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testMetricsGroup() {\n+        SourceTaskMetricsGroup group = new SourceTaskMetricsGroup(taskId, metrics);\n+        SourceTaskMetricsGroup group1 = new SourceTaskMetricsGroup(taskId1, metrics);\n+        for (int i = 0; i != 10; ++i) {\n+            group.recordPoll(100, 1000 + i * 100);\n+            group.recordWrite(10);\n+        }\n+        for (int i = 0; i != 20; ++i) {\n+            group1.recordPoll(100, 1000 + i * 100);\n+            group1.recordWrite(10);\n+        }\n+        assertEquals(1900.0, metrics.currentMetricValueAsDouble(group.metricGroup(), \"poll-batch-max-time-ms\"), 0.001d);\n+        assertEquals(1450.0, metrics.currentMetricValueAsDouble(group.metricGroup(), \"poll-batch-avg-time-ms\"), 0.001d);\n+        assertEquals(33.333, metrics.currentMetricValueAsDouble(group.metricGroup(), \"source-record-poll-rate\"), 0.001d);\n+        assertEquals(1000, metrics.currentMetricValueAsDouble(group.metricGroup(), \"source-record-poll-total\"), 0.001d);\n+        assertEquals(3.3333, metrics.currentMetricValueAsDouble(group.metricGroup(), \"source-record-write-rate\"), 0.001d);\n+        assertEquals(100, metrics.currentMetricValueAsDouble(group.metricGroup(), \"source-record-write-total\"), 0.001d);\n+        assertEquals(900.0, metrics.currentMetricValueAsDouble(group.metricGroup(), \"source-record-active-count\"), 0.001d);\n+\n+        // Close the group\n+        group.close();\n+\n+        for (MetricName metricName : group.metricGroup().metrics().metrics().keySet()) {\n+            // Metrics for this group should no longer exist\n+            assertFalse(group.metricGroup().groupId().includes(metricName));\n+        }\n+        // Sensors for this group should no longer exist\n+        assertNull(group.metricGroup().metrics().getSensor(\"sink-record-read\"));\n+        assertNull(group.metricGroup().metrics().getSensor(\"sink-record-send\"));\n+        assertNull(group.metricGroup().metrics().getSensor(\"sink-record-active-count\"));\n+        assertNull(group.metricGroup().metrics().getSensor(\"partition-count\"));\n+        assertNull(group.metricGroup().metrics().getSensor(\"offset-seq-number\"));\n+        assertNull(group.metricGroup().metrics().getSensor(\"offset-commit-completion\"));\n+        assertNull(group.metricGroup().metrics().getSensor(\"offset-commit-completion-skip\"));\n+        assertNull(group.metricGroup().metrics().getSensor(\"put-batch-time\"));\n+\n+        assertEquals(2900.0, metrics.currentMetricValueAsDouble(group1.metricGroup(), \"poll-batch-max-time-ms\"), 0.001d);\n+        assertEquals(1950.0, metrics.currentMetricValueAsDouble(group1.metricGroup(), \"poll-batch-avg-time-ms\"), 0.001d);\n+        assertEquals(66.667, metrics.currentMetricValueAsDouble(group1.metricGroup(), \"source-record-poll-rate\"), 0.001d);\n+        assertEquals(2000, metrics.currentMetricValueAsDouble(group1.metricGroup(), \"source-record-poll-total\"), 0.001d);\n+        assertEquals(6.667, metrics.currentMetricValueAsDouble(group1.metricGroup(), \"source-record-write-rate\"), 0.001d);\n+        assertEquals(200, metrics.currentMetricValueAsDouble(group1.metricGroup(), \"source-record-write-total\"), 0.001d);\n+        assertEquals(1800.0, metrics.currentMetricValueAsDouble(group1.metricGroup(), \"source-record-active-count\"), 0.001d);\n+    }\n+\n+    @Test\n+    public void testHeaders() throws Exception {\n+        Headers headers = new RecordHeaders();\n+        headers.add(\"header_key\", \"header_value\".getBytes());\n+\n+        org.apache.kafka.connect.header.Headers connectHeaders = new ConnectHeaders();\n+        connectHeaders.add(\"header_key\", new SchemaAndValue(Schema.STRING_SCHEMA, \"header_value\"));\n+\n+        createWorkerTask();\n+\n+        List<SourceRecord> records = new ArrayList<>();\n+        records.add(new SourceRecord(PARTITION, OFFSET, TOPIC, null, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD, null, connectHeaders));\n+\n+        expectTopicCreation(TOPIC);\n+\n+        Capture<ProducerRecord<byte[], byte[]>> sent = expectSendRecord(TOPIC, true, false, true, true, true, headers);\n+\n+        PowerMock.replayAll();\n+\n+        Whitebox.setInternalState(workerTask, \"toSend\", records);\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(SERIALIZED_KEY, sent.getValue().key());\n+        assertEquals(SERIALIZED_RECORD, sent.getValue().value());\n+        assertEquals(headers, sent.getValue().headers());\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testHeadersWithCustomConverter() throws Exception {\n+        StringConverter stringConverter = new StringConverter();\n+        TestConverterWithHeaders testConverter = new TestConverterWithHeaders();\n+\n+        createWorkerTask(TargetState.STARTED, stringConverter, testConverter, stringConverter);\n+\n+        List<SourceRecord> records = new ArrayList<>();\n+\n+        String stringA = \"\u00c1rv\u00edzt\u0171r\u0151 t\u00fck\u00f6rf\u00far\u00f3g\u00e9p\";\n+        org.apache.kafka.connect.header.Headers headersA = new ConnectHeaders();\n+        String encodingA = \"latin2\";\n+        headersA.addString(\"encoding\", encodingA);\n+\n+        records.add(new SourceRecord(PARTITION, OFFSET, TOPIC, null, Schema.STRING_SCHEMA, \"a\", Schema.STRING_SCHEMA, stringA, null, headersA));\n+\n+        String stringB = \"\u0422\u0435\u0441\u0442\u043e\u0432\u043e\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435\";\n+        org.apache.kafka.connect.header.Headers headersB = new ConnectHeaders();\n+        String encodingB = \"koi8_r\";\n+        headersB.addString(\"encoding\", encodingB);\n+\n+        records.add(new SourceRecord(PARTITION, OFFSET, TOPIC, null, Schema.STRING_SCHEMA, \"b\", Schema.STRING_SCHEMA, stringB, null, headersB));\n+\n+        expectTopicCreation(TOPIC);\n+\n+        Capture<ProducerRecord<byte[], byte[]>> sentRecordA = expectSendRecord(TOPIC, false, false, true, true, false, null);\n+        Capture<ProducerRecord<byte[], byte[]>> sentRecordB = expectSendRecord(TOPIC, false, false, true, true, false, null);\n+\n+        PowerMock.replayAll();\n+\n+        Whitebox.setInternalState(workerTask, \"toSend\", records);\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+\n+        assertEquals(ByteBuffer.wrap(\"a\".getBytes()), ByteBuffer.wrap(sentRecordA.getValue().key()));\n+        assertEquals(\n+            ByteBuffer.wrap(stringA.getBytes(encodingA)),\n+            ByteBuffer.wrap(sentRecordA.getValue().value())\n+        );\n+        assertEquals(encodingA, new String(sentRecordA.getValue().headers().lastHeader(\"encoding\").value()));\n+\n+        assertEquals(ByteBuffer.wrap(\"b\".getBytes()), ByteBuffer.wrap(sentRecordB.getValue().key()));\n+        assertEquals(\n+            ByteBuffer.wrap(stringB.getBytes(encodingB)),\n+            ByteBuffer.wrap(sentRecordB.getValue().value())\n+        );\n+        assertEquals(encodingB, new String(sentRecordB.getValue().headers().lastHeader(\"encoding\").value()));\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testTopicCreateWhenTopicExists() throws Exception {\n+        createWorkerTask();\n+\n+        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, TOPIC, 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, TOPIC, 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+\n+        expectPreliminaryCalls();\n+        TopicPartitionInfo topicPartitionInfo = new TopicPartitionInfo(0, null, Collections.emptyList(), Collections.emptyList());\n+        TopicDescription topicDesc = new TopicDescription(TOPIC, false, Collections.singletonList(topicPartitionInfo));\n+        EasyMock.expect(admin.describeTopics(TOPIC)).andReturn(Collections.singletonMap(TOPIC, topicDesc));\n+\n+        expectSendRecordTaskCommitRecordSucceed(false, false);\n+        expectSendRecordTaskCommitRecordSucceed(false, false);\n+\n+        PowerMock.replayAll();\n+\n+        Whitebox.setInternalState(workerTask, \"toSend\", Arrays.asList(record1, record2));\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+    }\n+\n+    @Test\n+    public void testSendRecordsTopicDescribeRetries() throws Exception {\n+        createWorkerTask();\n+\n+        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, TOPIC, 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, TOPIC, 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+\n+        expectPreliminaryCalls();\n+        // First round - call to describe the topic times out\n+        EasyMock.expect(admin.describeTopics(TOPIC))\n+                .andThrow(new RetriableException(new TimeoutException(\"timeout\")));\n+\n+        // Second round - calls to describe and create succeed\n+        expectTopicCreation(TOPIC);\n+        // Exactly two records are sent\n+        expectSendRecordTaskCommitRecordSucceed(false, false);\n+        expectSendRecordTaskCommitRecordSucceed(false, false);\n+\n+        PowerMock.replayAll();\n+\n+        Whitebox.setInternalState(workerTask, \"toSend\", Arrays.asList(record1, record2));\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(true, Whitebox.getInternalState(workerTask, \"lastSendFailed\"));\n+        assertEquals(Arrays.asList(record1, record2), Whitebox.getInternalState(workerTask, \"toSend\"));\n+\n+        // Next they all succeed\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(false, Whitebox.getInternalState(workerTask, \"lastSendFailed\"));\n+        assertNull(Whitebox.getInternalState(workerTask, \"toSend\"));\n+    }\n+\n+    @Test\n+    public void testSendRecordsTopicCreateRetries() throws Exception {\n+        createWorkerTask();\n+\n+        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, TOPIC, 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, TOPIC, 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+\n+        // First call to describe the topic times out\n+        expectPreliminaryCalls();\n+        EasyMock.expect(admin.describeTopics(TOPIC)).andReturn(Collections.emptyMap());\n+        Capture<NewTopic> newTopicCapture = EasyMock.newCapture();\n+        EasyMock.expect(admin.createTopic(EasyMock.capture(newTopicCapture)))\n+                .andThrow(new RetriableException(new TimeoutException(\"timeout\")));\n+\n+        // Second round\n+        expectTopicCreation(TOPIC);\n+        expectSendRecordTaskCommitRecordSucceed(false, false);\n+        expectSendRecordTaskCommitRecordSucceed(false, false);\n+\n+        PowerMock.replayAll();\n+\n+        Whitebox.setInternalState(workerTask, \"toSend\", Arrays.asList(record1, record2));\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(true, Whitebox.getInternalState(workerTask, \"lastSendFailed\"));\n+        assertEquals(Arrays.asList(record1, record2), Whitebox.getInternalState(workerTask, \"toSend\"));\n+\n+        // Next they all succeed\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(false, Whitebox.getInternalState(workerTask, \"lastSendFailed\"));\n+        assertNull(Whitebox.getInternalState(workerTask, \"toSend\"));\n+    }\n+\n+    @Test\n+    public void testSendRecordsTopicDescribeRetriesMidway() throws Exception {\n+        createWorkerTask();\n+\n+        // Differentiate only by Kafka partition so we can reuse conversion expectations\n+        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, TOPIC, 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, TOPIC, 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record3 = new SourceRecord(PARTITION, OFFSET, OTHER_TOPIC, 3, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+\n+        // First round\n+        expectPreliminaryCalls(OTHER_TOPIC);\n+        expectTopicCreation(TOPIC);\n+        expectSendRecordTaskCommitRecordSucceed(false, false);\n+        expectSendRecordTaskCommitRecordSucceed(false, false);\n+\n+        // First call to describe the topic times out\n+        EasyMock.expect(admin.describeTopics(OTHER_TOPIC))\n+                .andThrow(new RetriableException(new TimeoutException(\"timeout\")));\n+\n+        // Second round\n+        expectTopicCreation(OTHER_TOPIC);\n+        expectSendRecord(OTHER_TOPIC, false, true, true, true, true, emptyHeaders());\n+\n+        PowerMock.replayAll();\n+\n+        // Try to send 3, make first pass, second fail. Should save last two\n+        Whitebox.setInternalState(workerTask, \"toSend\", Arrays.asList(record1, record2, record3));\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(true, Whitebox.getInternalState(workerTask, \"lastSendFailed\"));\n+        assertEquals(Arrays.asList(record3), Whitebox.getInternalState(workerTask, \"toSend\"));\n+\n+        // Next they all succeed\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(false, Whitebox.getInternalState(workerTask, \"lastSendFailed\"));\n+        assertNull(Whitebox.getInternalState(workerTask, \"toSend\"));\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test\n+    public void testSendRecordsTopicCreateRetriesMidway() throws Exception {\n+        createWorkerTask();\n+\n+        // Differentiate only by Kafka partition so we can reuse conversion expectations\n+        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, TOPIC, 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, TOPIC, 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record3 = new SourceRecord(PARTITION, OFFSET, OTHER_TOPIC, 3, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+\n+        // First round\n+        expectPreliminaryCalls(OTHER_TOPIC);\n+        expectTopicCreation(TOPIC);\n+        expectSendRecordTaskCommitRecordSucceed(false, false);\n+        expectSendRecordTaskCommitRecordSucceed(false, false);\n+\n+        EasyMock.expect(admin.describeTopics(OTHER_TOPIC)).andReturn(Collections.emptyMap());\n+        // First call to create the topic times out\n+        Capture<NewTopic> newTopicCapture = EasyMock.newCapture();\n+        EasyMock.expect(admin.createTopic(EasyMock.capture(newTopicCapture)))\n+                .andThrow(new RetriableException(new TimeoutException(\"timeout\")));\n+\n+        // Second round\n+        expectTopicCreation(OTHER_TOPIC);\n+        expectSendRecord(OTHER_TOPIC, false, true, true, true, true, emptyHeaders());\n+\n+        PowerMock.replayAll();\n+\n+        // Try to send 3, make first pass, second fail. Should save last two\n+        Whitebox.setInternalState(workerTask, \"toSend\", Arrays.asList(record1, record2, record3));\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(true, Whitebox.getInternalState(workerTask, \"lastSendFailed\"));\n+        assertEquals(Arrays.asList(record3), Whitebox.getInternalState(workerTask, \"toSend\"));\n+\n+        // Next they all succeed\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertEquals(false, Whitebox.getInternalState(workerTask, \"lastSendFailed\"));\n+        assertNull(Whitebox.getInternalState(workerTask, \"toSend\"));\n+\n+        PowerMock.verifyAll();\n+    }\n+\n+    @Test(expected = ConnectException.class)\n+    public void testTopicDescribeFails() throws Exception {\n+        createWorkerTask();\n+\n+        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, TOPIC, 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, TOPIC, 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+\n+        expectPreliminaryCalls();\n+        EasyMock.expect(admin.describeTopics(TOPIC))\n+                .andThrow(new ConnectException(new TopicAuthorizationException(\"unauthorized\")));\n+\n+        PowerMock.replayAll();\n+\n+        Whitebox.setInternalState(workerTask, \"toSend\", Arrays.asList(record1, record2));\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+    }\n+\n+    @Test(expected = ConnectException.class)\n+    public void testTopicCreateFails() throws Exception {\n+        createWorkerTask();\n+\n+        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, TOPIC, 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, TOPIC, 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+\n+        expectPreliminaryCalls();\n+        EasyMock.expect(admin.describeTopics(TOPIC)).andReturn(Collections.emptyMap());\n+\n+        Capture<NewTopic> newTopicCapture = EasyMock.newCapture();\n+        EasyMock.expect(admin.createTopic(EasyMock.capture(newTopicCapture)))\n+                .andThrow(new ConnectException(new TopicAuthorizationException(\"unauthorized\")));\n+\n+        PowerMock.replayAll();\n+\n+        Whitebox.setInternalState(workerTask, \"toSend\", Arrays.asList(record1, record2));\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertNotNull(newTopicCapture.getValue());\n+    }\n+\n+    @Test(expected = ConnectException.class)\n+    public void testTopicCreateFailsWithExceptionWhenCreateReturnsFalse() throws Exception {\n+        createWorkerTask();\n+\n+        SourceRecord record1 = new SourceRecord(PARTITION, OFFSET, TOPIC, 1, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+        SourceRecord record2 = new SourceRecord(PARTITION, OFFSET, TOPIC, 2, KEY_SCHEMA, KEY, RECORD_SCHEMA, RECORD);\n+\n+        expectPreliminaryCalls();\n+        EasyMock.expect(admin.describeTopics(TOPIC)).andReturn(Collections.emptyMap());\n+\n+        Capture<NewTopic> newTopicCapture = EasyMock.newCapture();\n+        EasyMock.expect(admin.createTopic(EasyMock.capture(newTopicCapture))).andReturn(false);\n+\n+        PowerMock.replayAll();\n+\n+        Whitebox.setInternalState(workerTask, \"toSend\", Arrays.asList(record1, record2));\n+        Whitebox.invokeMethod(workerTask, \"sendRecords\");\n+        assertNotNull(newTopicCapture.getValue());\n+    }\n+\n+    @Test\n+    public void testTopicCreationClassWhenTopicCreationIsEnabled() {\n+        TopicCreationGroup expectedDefaultGroup =\n+                TopicCreationGroup.configuredGroups(sourceConfig).get(DEFAULT_TOPIC_CREATION_GROUP);\n+\n+        TopicCreation topicCreation = TopicCreation.newTopicCreation(config,\n+                TopicCreationGroup.configuredGroups(sourceConfig));\n+\n+        assertTrue(topicCreation.isTopicCreationEnabled());\n+        assertTrue(topicCreation.isTopicCreationRequired(TOPIC));\n+        assertThat(topicCreation.defaultTopicGroup(), is(expectedDefaultGroup));\n+        assertEquals(2, topicCreation.topicGroups().size());\n+        assertThat(topicCreation.topicGroups().keySet(), hasItems(\"foo\", \"bar\"));\n+        topicCreation.addTopic(TOPIC);\n+        assertFalse(topicCreation.isTopicCreationRequired(TOPIC));\n+    }\n+\n+    @Test\n+    public void testTopicCreationClassWhenTopicCreationIsDisabled() {\n+        Map<String, String> workerProps = workerProps();\n+        workerProps.put(TOPIC_CREATION_ENABLE_CONFIG, String.valueOf(false));\n+        config = new StandaloneConfig(workerProps);\n+\n+        TopicCreation topicCreation = TopicCreation.newTopicCreation(config,\n+                TopicCreationGroup.configuredGroups(sourceConfig));\n+\n+        assertFalse(topicCreation.isTopicCreationEnabled());\n+        assertFalse(topicCreation.isTopicCreationRequired(TOPIC));\n+        assertNull(topicCreation.defaultTopicGroup());\n+        assertEquals(0, topicCreation.topicGroups().size());\n+        assertThat(topicCreation.topicGroups(), is(Collections.emptyMap()));\n+        topicCreation.addTopic(TOPIC);\n+        assertFalse(topicCreation.isTopicCreationRequired(TOPIC));\n+    }\n+\n+    @Test\n+    public void testEmptyTopicCreationClass() {\n+        TopicCreation topicCreation = TopicCreation.newTopicCreation(config, null);\n+\n+        assertFalse(topicCreation.isTopicCreationEnabled());\n+        assertFalse(topicCreation.isTopicCreationRequired(TOPIC));\n+        assertNull(topicCreation.defaultTopicGroup());\n+        assertEquals(0, topicCreation.topicGroups().size());\n+        assertThat(topicCreation.topicGroups(), is(Collections.emptyMap()));\n+        topicCreation.addTopic(TOPIC);\n+        assertFalse(topicCreation.isTopicCreationRequired(TOPIC));\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 1169}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "79fdf9995b9939325d8dd6baf687b0175af960be", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/79fdf9995b9939325d8dd6baf687b0175af960be", "committedDate": "2020-05-26T20:02:48Z", "message": "KAFKA-5295: Refactor and add TopicCreation unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f57a9d692b8fed7f26ae634ac04be52a1285b952", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/f57a9d692b8fed7f26ae634ac04be52a1285b952", "committedDate": "2020-05-26T20:50:32Z", "message": "Update REPLICATION_FACTOR_DOC\n\nCo-authored-by: Randall Hauch <rhauch@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e61a8a4192df0dceb443517e7ccf38d7a8d4fdd7", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/e61a8a4192df0dceb443517e7ccf38d7a8d4fdd7", "committedDate": "2020-05-26T21:11:59Z", "message": "Apply doc suggestions from code review\n\nCo-authored-by: Randall Hauch <rhauch@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "540cd0c6b60c7b718eecfe052f27ef4e86237bb6", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/540cd0c6b60c7b718eecfe052f27ef4e86237bb6", "committedDate": "2020-05-26T21:13:42Z", "message": "KAFKA-5295: Address minor comments from code review"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4Njk5OTI5", "url": "https://github.com/apache/kafka/pull/8722#pullrequestreview-418699929", "createdAt": "2020-05-26T21:42:10Z", "commit": {"oid": "540cd0c6b60c7b718eecfe052f27ef4e86237bb6"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMTo0MjoxMFrOGaxUVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMTo1Mzo0MFrOGaxnPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcyNDE4MQ==", "bodyText": "Nice catch.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430724181", "createdAt": "2020-05-26T21:42:10Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/SourceConnectorConfig.java", "diffHunk": "@@ -16,21 +16,167 @@\n  */\n package org.apache.kafka.connect.runtime;\n \n+import org.apache.kafka.common.config.AbstractConfig;\n import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n import org.apache.kafka.connect.runtime.isolation.Plugins;\n \n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_GROUP;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.DEFAULT_TOPIC_CREATION_PREFIX;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.EXCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.INCLUDE_REGEX_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.PARTITIONS_CONFIG;\n+import static org.apache.kafka.connect.runtime.TopicCreationConfig.REPLICATION_FACTOR_CONFIG;\n \n public class SourceConnectorConfig extends ConnectorConfig {\n \n-    private static ConfigDef config = ConnectorConfig.configDef();\n+    protected static final String TOPIC_CREATION_GROUP = \"Topic Creation\";\n+\n+    public static final String TOPIC_CREATION_PREFIX = \"topic.creation.\";\n+\n+    public static final String TOPIC_CREATION_GROUPS_CONFIG = TOPIC_CREATION_PREFIX + \"groups\";\n+    private static final String TOPIC_CREATION_GROUPS_DOC = \"Groups of configurations for topics \"\n+            + \"created by source connectors\";\n+    private static final String TOPIC_CREATION_GROUPS_DISPLAY = \"Topic Creation Groups\";\n+\n+    private static class EnrichedSourceConnectorConfig extends AbstractConfig {\n+        EnrichedSourceConnectorConfig(ConfigDef configDef, Map<String, String> props) {\n+            super(configDef, props);\n+        }\n+\n+        @Override\n+        public Object get(String key) {\n+            return super.get(key);\n+        }\n+    }\n+\n+    private static ConfigDef config = SourceConnectorConfig.configDef();\n+    private final EnrichedSourceConnectorConfig enrichedSourceConfig;\n \n     public static ConfigDef configDef() {\n-        return config;\n+        int orderInGroup = 0;\n+        return new ConfigDef(ConnectorConfig.configDef())\n+                .define(TOPIC_CREATION_GROUPS_CONFIG, ConfigDef.Type.LIST, Collections.emptyList(),\n+                        ConfigDef.CompositeValidator.of(new ConfigDef.NonNullValidator(), ConfigDef.LambdaValidator.with(\n+                            (name, value) -> {\n+                                List<?> groupAliases = (List<?>) value;\n+                                if (groupAliases.size() > new HashSet<>(groupAliases).size()) {\n+                                    throw new ConfigException(name, value, \"Duplicate alias provided.\");\n+                                }\n+                            },\n+                            () -> \"unique topic creation groups\")),\n+                        ConfigDef.Importance.LOW, TOPIC_CREATION_GROUPS_DOC, TOPIC_CREATION_GROUP,\n+                        ++orderInGroup, ConfigDef.Width.LONG, TOPIC_CREATION_GROUPS_DISPLAY);\n     }\n \n-    public SourceConnectorConfig(Plugins plugins, Map<String, String> props) {\n+    public static ConfigDef embedDefaultGroup(ConfigDef baseConfigDef) {\n+        String defaultGroup = \"default\";\n+        ConfigDef newDefaultDef = new ConfigDef(baseConfigDef);\n+        newDefaultDef.embed(DEFAULT_TOPIC_CREATION_PREFIX, defaultGroup, 0, TopicCreationConfig.defaultGroupConfigDef());\n+        return newDefaultDef;\n+    }\n+\n+    /**\n+     * Returns an enriched {@link ConfigDef} building upon the {@code ConfigDef}, using the current configuration specified in {@code props} as an input.\n+     *\n+     * @param baseConfigDef the base configuration definition to be enriched\n+     * @param props the non parsed configuration properties\n+     * @return the enriched configuration definition\n+     */\n+    public static ConfigDef enrich(ConfigDef baseConfigDef, Map<String, String> props, AbstractConfig defaultGroupConfig) {\n+        List<Object> topicCreationGroups = new ArrayList<>();\n+        Object aliases = ConfigDef.parseType(TOPIC_CREATION_GROUPS_CONFIG, props.get(TOPIC_CREATION_GROUPS_CONFIG), ConfigDef.Type.LIST);\n+        if (aliases instanceof List) {\n+            topicCreationGroups.addAll((List<?>) aliases);\n+        }\n+\n+        ConfigDef newDef = new ConfigDef(baseConfigDef);\n+        String defaultGroupPrefix = TOPIC_CREATION_PREFIX + DEFAULT_TOPIC_CREATION_GROUP + \".\";\n+        short defaultGroupReplicationFactor = defaultGroupConfig.getShort(defaultGroupPrefix + REPLICATION_FACTOR_CONFIG);\n+        int defaultGroupPartitions = defaultGroupConfig.getInt(defaultGroupPrefix + PARTITIONS_CONFIG);\n+        topicCreationGroups.stream().distinct().forEach(group -> {\n+            if (!(group instanceof String)) {\n+                throw new ConfigException(\"Item in \" + TOPIC_CREATION_GROUPS_CONFIG + \" property is not of type String\");\n+            }\n+            String alias = (String) group;\n+            String prefix = TOPIC_CREATION_PREFIX + alias + \".\";\n+            String configGroup = TOPIC_CREATION_GROUP + \": \" + alias;\n+            newDef.embed(prefix, configGroup, 0,\n+                    TopicCreationConfig.configDef(configGroup, defaultGroupReplicationFactor, defaultGroupPartitions));\n+        });\n+        return newDef;\n+    }\n+\n+    public SourceConnectorConfig(Plugins plugins, Map<String, String> props, boolean createTopics) {\n         super(plugins, config, props);\n+        if (createTopics && props.entrySet().stream().anyMatch(e -> e.getKey().startsWith(TOPIC_CREATION_PREFIX))) {\n+            ConfigDef defaultConfigDef = embedDefaultGroup(config);\n+            // This config is only used to set default values for partitions and replication\n+            // factor from the default group and otherwise it remains unused\n+            AbstractConfig defaultGroup = new AbstractConfig(defaultConfigDef, props, false);\n+\n+            // If the user has added regex of include or exclude patterns in the default group,\n+            // they should be ignored.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "540cd0c6b60c7b718eecfe052f27ef4e86237bb6"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcyOTAyMQ==", "bodyText": "This is misnamed, and that was my fault when I made a wrong suggestion earlier.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430729021", "createdAt": "2020-05-26T21:53:40Z", "author": {"login": "rhauch"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+public class TopicCreationConfig {\n+\n+    public static final String DEFAULT_TOPIC_CREATION_PREFIX = \"topic.creation.default.\";\n+    public static final String DEFAULT_TOPIC_CREATION_GROUP = \"default\";\n+\n+    public static final String INCLUDE_REGEX_CONFIG = \"include\";\n+    private static final String INCLUDE_REGEX_DOC = \"A list of regular expression literals \"\n+            + \"used to match the names topics used by the source connector. This list is used \"\n+            + \"to include topics that should be created using the topic settings defined by this group.\";\n+\n+    public static final String EXCLUDE_REGEX_CONFIG = \"exclude\";\n+    private static final String INCLUDE_REGEX_DOC = \"A list of regular expression literals \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "540cd0c6b60c7b718eecfe052f27ef4e86237bb6"}, "originalPosition": 39}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "83ac913ab7f699480975a9b08ddbe18891030e80", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/83ac913ab7f699480975a9b08ddbe18891030e80", "committedDate": "2020-05-26T22:02:54Z", "message": "KAFKA-5295: Fix build and last minor typos"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NzA4Mjk3", "url": "https://github.com/apache/kafka/pull/8722#pullrequestreview-418708297", "createdAt": "2020-05-26T21:58:06Z", "commit": {"oid": "540cd0c6b60c7b718eecfe052f27ef4e86237bb6"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMTo1ODowNlrOGaxueQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQyMTo1ODo0NVrOGaxvXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDczMDg3Mw==", "bodyText": "NP. Fixed.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430730873", "createdAt": "2020-05-26T21:58:06Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+public class TopicCreationConfig {\n+\n+    public static final String DEFAULT_TOPIC_CREATION_PREFIX = \"topic.creation.default.\";\n+    public static final String DEFAULT_TOPIC_CREATION_GROUP = \"default\";\n+\n+    public static final String INCLUDE_REGEX_CONFIG = \"include\";\n+    private static final String INCLUDE_REGEX_DOC = \"A list of regular expression literals \"\n+            + \"used to match the names topics used by the source connector. This list is used \"\n+            + \"to include topics that should be created using the topic settings defined by this group.\";\n+\n+    public static final String EXCLUDE_REGEX_CONFIG = \"exclude\";\n+    private static final String INCLUDE_REGEX_DOC = \"A list of regular expression literals \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDcyOTAyMQ=="}, "originalCommit": {"oid": "540cd0c6b60c7b718eecfe052f27ef4e86237bb6"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDczMTEwMA==", "bodyText": "I'm in favor of including since it's well tested. We could reconsider a consolidation in the future.", "url": "https://github.com/apache/kafka/pull/8722#discussion_r430731100", "createdAt": "2020-05-26T21:58:45Z", "author": {"login": "kkonstantine"}, "path": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/TopicCreationConfig.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.connect.runtime;\n+\n+import org.apache.kafka.common.config.ConfigDef;\n+import org.apache.kafka.common.config.ConfigException;\n+import org.apache.kafka.connect.util.TopicAdmin;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.regex.Pattern;\n+import java.util.regex.PatternSyntaxException;\n+\n+public class TopicCreationConfig {\n+\n+    public static final String DEFAULT_TOPIC_CREATION_PREFIX = \"topic.creation.default.\";\n+    public static final String DEFAULT_TOPIC_CREATION_GROUP = \"default\";\n+\n+    public static final String INCLUDE_REGEX_CONFIG = \"include\";\n+    private static final String INCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to include topics that \"\n+            + \"match their values and apply this group's specific configuration to the topics \"\n+            + \"that match this inclusion list.\";\n+\n+    public static final String EXCLUDE_REGEX_CONFIG = \"exclude\";\n+    private static final String EXCLUDE_REGEX_DOC = \"A list of strings that represent regular \"\n+            + \"expressions that may match topic names. This list is used to exclude topics that \"\n+            + \"match their values and refrain from applying this group's specific configuration \"\n+            + \"to the topics that match this exclusion list. Note that exclusion rules have \"\n+            + \"precedent and override any inclusion rules for topics. \";\n+\n+    public static final String REPLICATION_FACTOR_CONFIG = \"replication.factor\";\n+    private static final String REPLICATION_FACTOR_DOC = \"The replication factor for new topics \"\n+            + \"created for this connector. This value must not be larger than the number of \"\n+            + \"brokers in the Kafka cluster, or otherwise an error will be thrown when the \"\n+            + \"connector will attempt to create a topic. For the default group this configuration\"\n+            + \" is required. For any other group defined in topic.creation.groups this config is \"\n+            + \"optional and if it's missing it gets the value the default group\";\n+\n+    public static final String PARTITIONS_CONFIG = \"partitions\";\n+    private static final String PARTITIONS_DOC = \"The number of partitions new topics created for\"\n+            + \" this connector. For the default group this configuration is required. For any \"\n+            + \"other group defined in topic.creation.groups this config is optional and if it's \"\n+            + \"missing it gets the value the default group\";\n+\n+    public static final ConfigDef.Validator REPLICATION_FACTOR_VALIDATOR = ConfigDef.LambdaValidator.with(\n+        (name, value) -> validateReplicationFactor(name, (short) value),\n+        () -> \"Positive number, or -1 to use the broker's default\"\n+    );\n+    public static final ConfigDef.Validator PARTITIONS_VALIDATOR = ConfigDef.LambdaValidator.with(\n+        (name, value) -> validatePartitions(name, (int) value),\n+        () -> \"Positive number, or -1 to use the broker's default\"\n+    );\n+    @SuppressWarnings(\"unchecked\")\n+    public static final ConfigDef.Validator REGEX_VALIDATOR = ConfigDef.LambdaValidator.with(\n+        (name, value) -> {\n+            try {\n+                ((List<String>) value).forEach(Pattern::compile);\n+            } catch (PatternSyntaxException e) {\n+                throw new ConfigException(name, value, \"Syntax error in regular expression\");\n+            }\n+        },\n+        () -> \"Positive number, or -1 to use the broker's default\"\n+    );\n+\n+    private static void validatePartitions(String configName, int factor) {\n+        if (factor != TopicAdmin.NO_PARTITIONS && factor < 1) {\n+            throw new ConfigException(configName, factor,\n+                    \"Number of partitions must be positive, or -1 to use the broker's default\");\n+        }\n+    }\n+\n+    private static void validateReplicationFactor(String configName, short factor) {\n+        if (factor != TopicAdmin.NO_REPLICATION_FACTOR && factor < 1) {\n+            throw new ConfigException(configName, factor,\n+                    \"Replication factor must be positive, or -1 to use the broker's default\");\n+        }\n+    }\n+\n+    public static ConfigDef configDef(String group, short defaultReplicationFactor, int defaultParitionCount) {\n+        int orderInGroup = 0;\n+        ConfigDef configDef = new ConfigDef();\n+        configDef\n+                .define(INCLUDE_REGEX_CONFIG, ConfigDef.Type.LIST, Collections.emptyList(),\n+                        REGEX_VALIDATOR, ConfigDef.Importance.LOW,\n+                        INCLUDE_REGEX_DOC, group, ++orderInGroup, ConfigDef.Width.LONG,\n+                        \"Inclusion Topic Pattern for \" + group)\n+                .define(EXCLUDE_REGEX_CONFIG, ConfigDef.Type.LIST, Collections.emptyList(),\n+                        REGEX_VALIDATOR, ConfigDef.Importance.LOW,\n+                        EXCLUDE_REGEX_DOC, group, ++orderInGroup, ConfigDef.Width.LONG,\n+                        \"Exclusion Topic Pattern for \" + group)\n+                .define(REPLICATION_FACTOR_CONFIG, ConfigDef.Type.SHORT,\n+                        defaultReplicationFactor, REPLICATION_FACTOR_VALIDATOR,\n+                        ConfigDef.Importance.LOW, REPLICATION_FACTOR_DOC, group, ++orderInGroup,\n+                        ConfigDef.Width.LONG, \"Replication Factor for Topics in \" + group)\n+                .define(PARTITIONS_CONFIG, ConfigDef.Type.INT,\n+                        defaultParitionCount, PARTITIONS_VALIDATOR,\n+                        ConfigDef.Importance.LOW, PARTITIONS_DOC, group, ++orderInGroup,\n+                        ConfigDef.Width.LONG, \"Partition Count for Topics in \" + group);\n+        return configDef;\n+    }\n+\n+    public static ConfigDef defaultGroupConfigDef() {\n+        int orderInGroup = 0;\n+        ConfigDef configDef = new ConfigDef();\n+        configDef\n+                .define(INCLUDE_REGEX_CONFIG, ConfigDef.Type.LIST, \".*\",\n+                        new ConfigDef.NonNullValidator(), ConfigDef.Importance.LOW,\n+                        INCLUDE_REGEX_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup, ConfigDef.Width.LONG,\n+                        \"Inclusion Topic Pattern for \" + DEFAULT_TOPIC_CREATION_GROUP)\n+                .define(EXCLUDE_REGEX_CONFIG, ConfigDef.Type.LIST, Collections.emptyList(),\n+                        new ConfigDef.NonNullValidator(), ConfigDef.Importance.LOW,\n+                        EXCLUDE_REGEX_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup, ConfigDef.Width.LONG,\n+                        \"Exclusion Topic Pattern for \" + DEFAULT_TOPIC_CREATION_GROUP)\n+                .define(REPLICATION_FACTOR_CONFIG, ConfigDef.Type.SHORT,\n+                        ConfigDef.NO_DEFAULT_VALUE, REPLICATION_FACTOR_VALIDATOR,\n+                        ConfigDef.Importance.LOW, REPLICATION_FACTOR_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup,\n+                        ConfigDef.Width.LONG, \"Replication Factor for Topics in \" + DEFAULT_TOPIC_CREATION_GROUP)\n+                .define(PARTITIONS_CONFIG, ConfigDef.Type.INT,\n+                        ConfigDef.NO_DEFAULT_VALUE, PARTITIONS_VALIDATOR,\n+                        ConfigDef.Importance.LOW, PARTITIONS_DOC, DEFAULT_TOPIC_CREATION_GROUP, ++orderInGroup,\n+                        ConfigDef.Width.LONG, \"Partition Count for Topics in \" + DEFAULT_TOPIC_CREATION_GROUP);\n+        return configDef;\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDQ3MTc0NQ=="}, "originalCommit": {"oid": "a880d6e6f926c830dc15261b752a764cb7844285"}, "originalPosition": 138}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE4NzE0MTM0", "url": "https://github.com/apache/kafka/pull/8722#pullrequestreview-418714134", "createdAt": "2020-05-26T22:10:16Z", "commit": {"oid": "83ac913ab7f699480975a9b08ddbe18891030e80"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9850535d1a4291679ba66678ac3804d69f15aec5", "author": {"user": {"login": "kkonstantine", "name": "Konstantine Karantasis"}}, "url": "https://github.com/apache/kafka/commit/9850535d1a4291679ba66678ac3804d69f15aec5", "committedDate": "2020-05-27T00:56:19Z", "message": "KAFKA-5295: Fix test after change in validation error"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1166, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}