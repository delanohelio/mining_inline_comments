{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM0NTcxNTUy", "number": 188, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNDo0MToxMFrOEFkQBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMTo0MzowOVrOEHFnFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MjcyMjYzOnYy", "diffSide": "RIGHT", "path": "pom.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNDo0MToxMFrOGj1Qgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNDo1Nzo0NVrOGj2AFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIyNTkyMg==", "bodyText": "Hive 2.3.7 supposedly depends on Hadoop 2.7.2 so why did you need to move to 2.8.1?", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r440225922", "createdAt": "2020-06-15T14:41:10Z", "author": {"login": "massdosage"}, "path": "pom.xml", "diffHunk": "@@ -54,7 +54,7 @@\n     <httpcomponents.httpclient.version>4.5.11</httpcomponents.httpclient.version>\n     <jackson.version>2.10.0</jackson.version>\n     <!-- END: AWS jdk version + dependencies -->\n-    <hadoop.version>2.7.1</hadoop.version>\n+    <hadoop.version>2.8.1</hadoop.version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd619ed3f2c12883fc661e1383502a508e1f5ff6"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIzODEwMw==", "bodyText": "Hmm yep I think i did this to fix some Parquet writer error, will revert as this was fixed elsewhere", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r440238103", "createdAt": "2020-06-15T14:57:45Z", "author": {"login": "max-jacobs"}, "path": "pom.xml", "diffHunk": "@@ -54,7 +54,7 @@\n     <httpcomponents.httpclient.version>4.5.11</httpcomponents.httpclient.version>\n     <jackson.version>2.10.0</jackson.version>\n     <!-- END: AWS jdk version + dependencies -->\n-    <hadoop.version>2.7.1</hadoop.version>\n+    <hadoop.version>2.8.1</hadoop.version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIyNTkyMg=="}, "originalCommit": {"oid": "dd619ed3f2c12883fc661e1383502a508e1f5ff6"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MjcyNTkyOnYy", "diffSide": "RIGHT", "path": "circus-train-integration-tests/src/test/java/com/hotels/bdp/circustrain/integration/utils/ThriftMetastoreServerRuleExtension.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNDo0MTo1OFrOGj1Srw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzozNjoyMlrOGmVGEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIyNjQ3OQ==", "bodyText": "Is this class just a convenience to override the conf values?", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r440226479", "createdAt": "2020-06-15T14:41:58Z", "author": {"login": "massdosage"}, "path": "circus-train-integration-tests/src/test/java/com/hotels/bdp/circustrain/integration/utils/ThriftMetastoreServerRuleExtension.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/**\n+ * Copyright (C) 2016-2020 Expedia, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hotels.bdp.circustrain.integration.utils;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+\n+import com.hotels.beeju.ThriftHiveMetaStoreJUnitRule;\n+\n+public class ThriftMetastoreServerRuleExtension extends ThriftHiveMetaStoreJUnitRule {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd619ed3f2c12883fc661e1383502a508e1f5ff6"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIzOTcwNA==", "bodyText": "Pretty much yep, beforeTest() is called from within beeju, so this is where we will initialise beeju with the hiverunner metastore", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r440239704", "createdAt": "2020-06-15T14:59:52Z", "author": {"login": "max-jacobs"}, "path": "circus-train-integration-tests/src/test/java/com/hotels/bdp/circustrain/integration/utils/ThriftMetastoreServerRuleExtension.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/**\n+ * Copyright (C) 2016-2020 Expedia, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hotels.bdp.circustrain.integration.utils;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+\n+import com.hotels.beeju.ThriftHiveMetaStoreJUnitRule;\n+\n+public class ThriftMetastoreServerRuleExtension extends ThriftHiveMetaStoreJUnitRule {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIyNjQ3OQ=="}, "originalCommit": {"oid": "dd619ed3f2c12883fc661e1383502a508e1f5ff6"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyNTQwMw==", "bodyText": "OK", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r441525403", "createdAt": "2020-06-17T13:00:12Z", "author": {"login": "massdosage"}, "path": "circus-train-integration-tests/src/test/java/com/hotels/bdp/circustrain/integration/utils/ThriftMetastoreServerRuleExtension.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/**\n+ * Copyright (C) 2016-2020 Expedia, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hotels.bdp.circustrain.integration.utils;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+\n+import com.hotels.beeju.ThriftHiveMetaStoreJUnitRule;\n+\n+public class ThriftMetastoreServerRuleExtension extends ThriftHiveMetaStoreJUnitRule {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIyNjQ3OQ=="}, "originalCommit": {"oid": "dd619ed3f2c12883fc661e1383502a508e1f5ff6"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjgzNzg4Nw==", "bodyText": "We should push this to Beeju once and fix it there (We also do this in other internal projects).", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r442837887", "createdAt": "2020-06-19T13:23:34Z", "author": {"login": "patduin"}, "path": "circus-train-integration-tests/src/test/java/com/hotels/bdp/circustrain/integration/utils/ThriftMetastoreServerRuleExtension.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/**\n+ * Copyright (C) 2016-2020 Expedia, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hotels.bdp.circustrain.integration.utils;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+\n+import com.hotels.beeju.ThriftHiveMetaStoreJUnitRule;\n+\n+public class ThriftMetastoreServerRuleExtension extends ThriftHiveMetaStoreJUnitRule {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIyNjQ3OQ=="}, "originalCommit": {"oid": "dd619ed3f2c12883fc661e1383502a508e1f5ff6"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0NDY5MQ==", "bodyText": "Agreed, can someone do that now? It not can we add an issue over at BeeJU and refer to this?", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r442844691", "createdAt": "2020-06-19T13:36:22Z", "author": {"login": "massdosage"}, "path": "circus-train-integration-tests/src/test/java/com/hotels/bdp/circustrain/integration/utils/ThriftMetastoreServerRuleExtension.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/**\n+ * Copyright (C) 2016-2020 Expedia, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hotels.bdp.circustrain.integration.utils;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+\n+import com.hotels.beeju.ThriftHiveMetaStoreJUnitRule;\n+\n+public class ThriftMetastoreServerRuleExtension extends ThriftHiveMetaStoreJUnitRule {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIyNjQ3OQ=="}, "originalCommit": {"oid": "dd619ed3f2c12883fc661e1383502a508e1f5ff6"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0Mjc0MjU2OnYy", "diffSide": "RIGHT", "path": "circus-train-integration-tests/src/test/java/com/hotels/bdp/circustrain/integration/CircusTrainParquetSchemaEvolutionIntegrationTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNDo0NTozMlrOGj1c9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxMTozMzozNlrOGkXMdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIyOTExMA==", "bodyText": "So is this the class you're not so sure about adding as it increases the test running time so much? One option could be to use a Junit 4 Category (SchemaTests) and then turn off this category by default when Maven runs the test. Then in the SCHEMA_EVOLUTION.md you could add a section about the tests and show how one can run them on the CLI by telling Maven to specifically run tests with that category. I assume that's the idea here - they're a safety net that we should never need but they could be useful every now and then just to make sure we haven't broken any of the schema evolution rules as we understand them today?", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r440229110", "createdAt": "2020-06-15T14:45:32Z", "author": {"login": "massdosage"}, "path": "circus-train-integration-tests/src/test/java/com/hotels/bdp/circustrain/integration/CircusTrainParquetSchemaEvolutionIntegrationTest.java", "diffHunk": "@@ -0,0 +1,682 @@\n+/**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd619ed3f2c12883fc661e1383502a508e1f5ff6"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDI0MTU5MA==", "bodyText": "Yep this is the one. In total these tests take ~10 mins on my machine. Plus they all pass (when in reality the non supported schema evolutions shouldn't - for those cases I've made the tests expect an exception to be thrown).\nThey're a safety net, and also useful if a feature is ever added to CT to support some of the currently failing evolutions. They're essentially supporting tests for the added readme as \"proof\".", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r440241590", "createdAt": "2020-06-15T15:02:35Z", "author": {"login": "max-jacobs"}, "path": "circus-train-integration-tests/src/test/java/com/hotels/bdp/circustrain/integration/CircusTrainParquetSchemaEvolutionIntegrationTest.java", "diffHunk": "@@ -0,0 +1,682 @@\n+/**", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIyOTExMA=="}, "originalCommit": {"oid": "dd619ed3f2c12883fc661e1383502a508e1f5ff6"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDc4MTk0MA==", "bodyText": "I've added junit category as suggested, added to the doc too. Let me know what you think, thanks!", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r440781940", "createdAt": "2020-06-16T11:33:36Z", "author": {"login": "max-jacobs"}, "path": "circus-train-integration-tests/src/test/java/com/hotels/bdp/circustrain/integration/CircusTrainParquetSchemaEvolutionIntegrationTest.java", "diffHunk": "@@ -0,0 +1,682 @@\n+/**", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIyOTExMA=="}, "originalCommit": {"oid": "dd619ed3f2c12883fc661e1383502a508e1f5ff6"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MDc3OTcxOnYy", "diffSide": "RIGHT", "path": "circus-train-common-test/src/main/java/com/hotels/bdp/circustrain/common/test/base/CircusTrainRunner.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxMjo0OTo1NVrOGlELPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxMjoyOToyNVrOGltDVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUxODkxMQ==", "bodyText": "We pass this in but then don't use it for anything? Or am I missing something here?", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r441518911", "createdAt": "2020-06-17T12:49:55Z", "author": {"login": "massdosage"}, "path": "circus-train-common-test/src/main/java/com/hotels/bdp/circustrain/common/test/base/CircusTrainRunner.java", "diffHunk": "@@ -146,6 +160,7 @@ private CircusTrainRunner(\n       String graphiteUri,\n       File housekeepingDbLocation) {\n     this.databaseName = databaseName;\n+    this.replicaDatabaseName = replicaDatabaseName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efd4028767ba9f3a5534ecce6242ac33d1c0f0ef"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjExMDEwMQ==", "bodyText": "Yep its used in the new integration tests. It seemed easier with Hiverunner to just setup one metastore with a source db and a replica db. So I had to change the runner here to be able to set different dbs. The other integration tests don't use this, so the replicaDatabaseName just defaults to the databaseName if empty.", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r442110101", "createdAt": "2020-06-18T09:56:16Z", "author": {"login": "max-jacobs"}, "path": "circus-train-common-test/src/main/java/com/hotels/bdp/circustrain/common/test/base/CircusTrainRunner.java", "diffHunk": "@@ -146,6 +160,7 @@ private CircusTrainRunner(\n       String graphiteUri,\n       File housekeepingDbLocation) {\n     this.databaseName = databaseName;\n+    this.replicaDatabaseName = replicaDatabaseName;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUxODkxMQ=="}, "originalCommit": {"oid": "efd4028767ba9f3a5534ecce6242ac33d1c0f0ef"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjE4ODYzMA==", "bodyText": "Ah, OK.", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r442188630", "createdAt": "2020-06-18T12:29:25Z", "author": {"login": "massdosage"}, "path": "circus-train-common-test/src/main/java/com/hotels/bdp/circustrain/common/test/base/CircusTrainRunner.java", "diffHunk": "@@ -146,6 +160,7 @@ private CircusTrainRunner(\n       String graphiteUri,\n       File housekeepingDbLocation) {\n     this.databaseName = databaseName;\n+    this.replicaDatabaseName = replicaDatabaseName;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUxODkxMQ=="}, "originalCommit": {"oid": "efd4028767ba9f3a5534ecce6242ac33d1c0f0ef"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MDgwNzEzOnYy", "diffSide": "RIGHT", "path": "circus-train-integration-tests/pom.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxMjo1Njo0OFrOGlEcKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxMDoxMzo0MFrOGlo2rQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyMzI0Mg==", "bodyText": "I think this might be more elegant (if it works based on https://stackoverflow.com/questions/14132174/how-to-exclude-all-junit4-tests-with-a-given-category-using-maven-surefire):\n\nAdd a property called something like surefire.excluded.groups which by default is set to the value used in the category and then use that here in an excludedGroups element.\nIf people want to run these tests they just override the property on the CLI to be empty and then it will run everything.\nThen we don't need new profiles?", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r441523242", "createdAt": "2020-06-17T12:56:48Z", "author": {"login": "massdosage"}, "path": "circus-train-integration-tests/pom.xml", "diffHunk": "@@ -198,6 +264,7 @@\n         <configuration>\n           <forkCount>1</forkCount>\n           <reuseForks>false</reuseForks>\n+          <groups>${groups}</groups>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efd4028767ba9f3a5534ecce6242ac33d1c0f0ef"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjExOTg1Mw==", "bodyText": "Yeah thats nicer, will update", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r442119853", "createdAt": "2020-06-18T10:13:40Z", "author": {"login": "max-jacobs"}, "path": "circus-train-integration-tests/pom.xml", "diffHunk": "@@ -198,6 +264,7 @@\n         <configuration>\n           <forkCount>1</forkCount>\n           <reuseForks>false</reuseForks>\n+          <groups>${groups}</groups>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyMzI0Mg=="}, "originalCommit": {"oid": "efd4028767ba9f3a5534ecce6242ac33d1c0f0ef"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MDgxMDExOnYy", "diffSide": "RIGHT", "path": "circus-train-integration-tests/src/test/java/com/hotels/bdp/circustrain/integration/CircusTrainParquetSchemaEvolutionIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxMjo1NzoyN1rOGlEd8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxMjo1NzoyN1rOGlEd8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyMzY5OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              private static final Logger LOG = LoggerFactory.getLogger(CircusTrainParquetSchemaEvolutionIntegrationTest.class);\n          \n          \n            \n              private static final Logger log = LoggerFactory.getLogger(CircusTrainParquetSchemaEvolutionIntegrationTest.class);", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r441523699", "createdAt": "2020-06-17T12:57:27Z", "author": {"login": "massdosage"}, "path": "circus-train-integration-tests/src/test/java/com/hotels/bdp/circustrain/integration/CircusTrainParquetSchemaEvolutionIntegrationTest.java", "diffHunk": "@@ -0,0 +1,685 @@\n+/**\n+ * Copyright (C) 2016-2020 Expedia, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hotels.bdp.circustrain.integration;\n+\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.Assert.assertThat;\n+\n+import static com.hotels.bdp.circustrain.integration.IntegrationTestHelper.EVOLUTION_COLUMN;\n+import static com.hotels.bdp.circustrain.integration.IntegrationTestHelper.PARTITIONED_TABLE;\n+import static com.hotels.bdp.circustrain.integration.utils.TestUtils.newTablePartition;\n+import static com.hotels.bdp.circustrain.integration.utils.TestUtils.toUri;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.contrib.java.lang.system.ExpectedSystemExit;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import fm.last.commons.test.file.ClassDataFolder;\n+import fm.last.commons.test.file.DataFolder;\n+\n+import com.google.common.collect.Lists;\n+import com.klarna.hiverunner.HiveShell;\n+import com.klarna.hiverunner.StandaloneHiveRunner;\n+import com.klarna.hiverunner.annotations.HiveSQL;\n+\n+import com.hotels.bdp.circustrain.common.test.base.CircusTrainRunner;\n+import com.hotels.bdp.circustrain.common.test.junit.rules.ServerSocketRule;\n+import com.hotels.bdp.circustrain.integration.utils.SchemaEvolution;\n+import com.hotels.bdp.circustrain.integration.utils.ThriftMetastoreServerRuleExtension;\n+import com.hotels.hcommon.hive.metastore.iterator.PartitionIterator;\n+\n+@Category(SchemaEvolution.class)\n+@RunWith(StandaloneHiveRunner.class)\n+public class CircusTrainParquetSchemaEvolutionIntegrationTest {\n+\n+  private static String SOURCE_DB = \"source_db\";\n+  private static String TABLE = \"ct_table_p\";\n+  private static String REPLICA_DB = \"replica_db\";\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(CircusTrainParquetSchemaEvolutionIntegrationTest.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efd4028767ba9f3a5534ecce6242ac33d1c0f0ef"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MDgxNjY0OnYy", "diffSide": "RIGHT", "path": "circus-train-integration-tests/src/test/java/com/hotels/bdp/circustrain/integration/CircusTrainParquetSchemaEvolutionIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxMjo1OTowNFrOGlEh5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwOTo1ODo0MVrOGloV-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyNDcwOA==", "bodyText": "What's the benefit of the try/catch/rethrow over letting this just throw the original exception?", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r441524708", "createdAt": "2020-06-17T12:59:04Z", "author": {"login": "massdosage"}, "path": "circus-train-integration-tests/src/test/java/com/hotels/bdp/circustrain/integration/CircusTrainParquetSchemaEvolutionIntegrationTest.java", "diffHunk": "@@ -0,0 +1,685 @@\n+/**\n+ * Copyright (C) 2016-2020 Expedia, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hotels.bdp.circustrain.integration;\n+\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.Assert.assertThat;\n+\n+import static com.hotels.bdp.circustrain.integration.IntegrationTestHelper.EVOLUTION_COLUMN;\n+import static com.hotels.bdp.circustrain.integration.IntegrationTestHelper.PARTITIONED_TABLE;\n+import static com.hotels.bdp.circustrain.integration.utils.TestUtils.newTablePartition;\n+import static com.hotels.bdp.circustrain.integration.utils.TestUtils.toUri;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.contrib.java.lang.system.ExpectedSystemExit;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import fm.last.commons.test.file.ClassDataFolder;\n+import fm.last.commons.test.file.DataFolder;\n+\n+import com.google.common.collect.Lists;\n+import com.klarna.hiverunner.HiveShell;\n+import com.klarna.hiverunner.StandaloneHiveRunner;\n+import com.klarna.hiverunner.annotations.HiveSQL;\n+\n+import com.hotels.bdp.circustrain.common.test.base.CircusTrainRunner;\n+import com.hotels.bdp.circustrain.common.test.junit.rules.ServerSocketRule;\n+import com.hotels.bdp.circustrain.integration.utils.SchemaEvolution;\n+import com.hotels.bdp.circustrain.integration.utils.ThriftMetastoreServerRuleExtension;\n+import com.hotels.hcommon.hive.metastore.iterator.PartitionIterator;\n+\n+@Category(SchemaEvolution.class)\n+@RunWith(StandaloneHiveRunner.class)\n+public class CircusTrainParquetSchemaEvolutionIntegrationTest {\n+\n+  private static String SOURCE_DB = \"source_db\";\n+  private static String TABLE = \"ct_table_p\";\n+  private static String REPLICA_DB = \"replica_db\";\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(CircusTrainParquetSchemaEvolutionIntegrationTest.class);\n+\n+  @HiveSQL(files = {}, autoStart = false)\n+  private HiveShell shell;\n+\n+  public @Rule ExpectedSystemExit exit = ExpectedSystemExit.none();\n+  public @Rule TemporaryFolder temporaryFolder = new TemporaryFolder();\n+  public @Rule DataFolder dataFolder = new ClassDataFolder();\n+  public @Rule ServerSocketRule serverSocketRule = new ServerSocketRule();\n+\n+  private File sourceWarehouseUri;\n+  private File replicaWarehouseUri;\n+  private File housekeepingDbLocation;\n+\n+  private IntegrationTestHelper helper;\n+\n+  private ThriftMetastoreServerRuleExtension thriftMetaStoreRule;\n+  private HiveMetaStoreClient metaStoreClient;\n+\n+  @Before\n+  public void init() throws Throwable {\n+    shell.start();\n+    shell.execute(\"CREATE DATABASE \" + SOURCE_DB);\n+    shell.execute(\"CREATE DATABASE \" + REPLICA_DB);\n+    HiveConf hiveConf = shell.getHiveConf();\n+    thriftMetaStoreRule = new ThriftMetastoreServerRuleExtension(hiveConf);\n+    thriftMetaStoreRule.before();\n+    metaStoreClient = thriftMetaStoreRule.client();\n+\n+    sourceWarehouseUri = temporaryFolder.newFolder(\"source-warehouse\");\n+    replicaWarehouseUri = temporaryFolder.newFolder(\"replica-warehouse\");\n+    helper = new IntegrationTestHelper(metaStoreClient);\n+    temporaryFolder.newFolder(\"db\");\n+    housekeepingDbLocation = new File(new File(temporaryFolder.getRoot(), \"db\"), \"housekeeping\");\n+  }\n+\n+  @After\n+  public void teardown() {\n+    thriftMetaStoreRule.after();\n+  }\n+\n+  @Test\n+  public void addFieldAtEnd() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"after\");\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tNULL\\t1\",\n+        \"2\\tafter\\t2\"\n+    );\n+    runTest(schema, evolvedSchema, new FieldDataWrapper(), afterEvolution);\n+  }\n+\n+  @Test\n+  public void removeField() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"before\");\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1\",\n+        \"2\\t2\"\n+    );\n+    runTest(schema, evolvedSchema, beforeEvolution, new FieldDataWrapper());\n+  }\n+\n+  // Replication success - old expectedData not \"renamed\" too\n+  @Test\n+  public void renameField() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN + \"_renamed\")\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"before\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN + \"_renamed\", \"after\");\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tbefore\\t1\",\n+        \"2\\tafter\\t2\"\n+    );\n+    \n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void addDefaultValue() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().nullableString(EVOLUTION_COLUMN, \"default\")\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"before\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"after\");\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tbefore\\t1\",\n+        \"2\\tafter\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void removeDefaultValue() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().nullableString(EVOLUTION_COLUMN, \"default\")\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"before\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"after\");\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tbefore\\t1\",\n+        \"2\\tafter\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void makeFieldNullable() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .nullable()\n+        .stringType()\n+        .noDefault()\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"before\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, null);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tbefore\\t1\",\n+        \"2\\tNULL\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void makeFieldStruct() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .record(EVOLUTION_COLUMN + \"_struct\")\n+        .fields()\n+        .requiredString(\"after_col\")\n+        .endRecord()\n+        .noDefault()\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"before\");\n+    Map<String, String> after = new HashMap<>();\n+    after.put(\"after_col\", \"after\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, after);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tbefore\\t1\",\n+        \"2\\ttrue\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void addColumnToStruct() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .record(EVOLUTION_COLUMN + \"_struct\")\n+        .fields()\n+        .requiredString(\"name\")\n+        .requiredString(\"city\")\n+        .endRecord()\n+        .noDefault()\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .record(EVOLUTION_COLUMN + \"_struct\")\n+        .fields()\n+        .requiredString(\"name\")\n+        .requiredString(\"city\")\n+        .requiredString(\"dob\")\n+        .endRecord()\n+        .noDefault()\n+        .endRecord();\n+    Map<String, String> before = new HashMap<>();\n+    before.put(\"name\", \"lisa\");\n+    before.put(\"city\", \"blackpool\");\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, before);\n+    Map<String, String> after = new HashMap<>();\n+    after.put(\"name\", \"adam\");\n+    after.put(\"city\", \"london\");\n+    after.put(\"dob\", \"22/09/1992\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, after);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t{\\\"name\\\":\\\"lisa\\\",\\\"city\\\":\\\"blackpool\\\",\\\"dob\\\":null}\\t1\",\n+        \"2\\t{\\\"name\\\":\\\"adam\\\",\\\"city\\\":\\\"london\\\",\\\"dob\\\":\\\"22/09/1992\\\"}\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void removeColumnFromStruct() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .record(EVOLUTION_COLUMN + \"_struct\")\n+        .fields()\n+        .requiredString(\"name\")\n+        .requiredString(\"city\")\n+        .requiredString(\"dob\")\n+        .endRecord()\n+        .noDefault()\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .record(EVOLUTION_COLUMN + \"_struct\")\n+        .fields()\n+        .requiredString(\"name\")\n+        .requiredString(\"city\")\n+        .endRecord()\n+        .noDefault()\n+        .endRecord();\n+    Map<String, String> before = new HashMap<>();\n+    before.put(\"name\", \"lisa\");\n+    before.put(\"city\", \"blackpool\");\n+    before.put(\"dob\", \"22/09/1992\");\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, before);\n+    Map<String, String> after = new HashMap<>();\n+    after.put(\"name\", \"adam\");\n+    after.put(\"city\", \"london\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, after);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t{\\\"name\\\":\\\"lisa\\\",\\\"city\\\":\\\"blackpool\\\"}\\t1\",\n+        \"2\\t{\\\"name\\\":\\\"adam\\\",\\\"city\\\":\\\"london\\\"}\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void promoteIntToLong() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredInt(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredFloat(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2L);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void promoteIntToFloat() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredInt(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredFloat(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2f);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void promoteIntToDouble() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredInt(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredDouble(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2d);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void promoteFloatToDouble() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredFloat(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredDouble(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1f);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2d);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void promoteLongToFloat() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredFloat(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1L);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2f);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void promoteLongToDouble() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredDouble(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1L);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2d);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test(expected = Exception.class)\n+  public void demoteLongToInt() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredInt(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1L);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1\\t1\",\n+        \"2\\t2\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test(expected = Exception.class)\n+  public void demoteDoubleToFloat() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredInt(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1d);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2f);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test(expected = Exception.class)\n+  public void demoteDoubleToInt() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredInt(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1d);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1\\t1\",\n+        \"2\\t2\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test(expected = Exception.class)\n+  public void demoteFloatToInt() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredInt(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1f);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1\\t1\",\n+        \"2\\t2\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test(expected = Exception.class)\n+  public void demoteDoubleToLong() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredDouble(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1d);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2l);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test(expected = Exception.class)\n+  public void demoteFloatToLong() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredFloat(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1f);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2l);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void addValueToEnum() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .enumeration(EVOLUTION_COLUMN + \"_enum\")\n+        .symbols(\"FIRST\")\n+        .noDefault()\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .enumeration(EVOLUTION_COLUMN + \"_enum\")\n+        .symbols(\"FIRST\", \"SECOND\")\n+        .noDefault()\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"FIRST\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"SECOND\");\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tFIRST\\t1\",\n+        \"2\\tSECOND\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void removeValueFromEnum() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .enumeration(EVOLUTION_COLUMN + \"_enum\")\n+        .symbols(\"FIRST\", \"SECOND\")\n+        .noDefault()\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .enumeration(EVOLUTION_COLUMN + \"_enum\")\n+        .symbols(\"FIRST\")\n+        .noDefault()\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"SECOND\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"FIRST\");\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tSECOND\\t1\",\n+        \"2\\tFIRST\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  private static class FieldDataWrapper {\n+\n+    private String fieldName;\n+    private Object expectedData;\n+\n+    public FieldDataWrapper() {\n+    }\n+\n+    public FieldDataWrapper(String fieldName, Object expectedData) {\n+      this.fieldName = fieldName;\n+      this.expectedData = expectedData;\n+    }\n+  }\n+\n+  private SchemaBuilder.FieldAssembler<Schema> getSchemaFieldAssembler() {\n+    return SchemaBuilder\n+        .builder(\"name.space\")\n+        .record(PARTITIONED_TABLE)\n+        .fields()\n+        .requiredInt(\"id\");\n+  }\n+\n+  private void runTest(\n+      Schema schema,\n+      Schema evolvedSchema,\n+      FieldDataWrapper beforeEvolution,\n+      FieldDataWrapper afterEvolution) throws Exception {\n+    // Create initial replica table with the original schema (setting a Circus Train event id manually).\n+    Table replicaTable = helper.createParquetPartitionedTable(\n+        toUri(replicaWarehouseUri, REPLICA_DB, TABLE),\n+        REPLICA_DB,\n+        TABLE,\n+        schema,\n+        beforeEvolution.fieldName,\n+        beforeEvolution.expectedData,\n+        1);\n+    LOG.info(\">>>> Table {} \", metaStoreClient.getTable(REPLICA_DB, TABLE));\n+\n+    replicaTable.getParameters().put(\"com.hotels.bdp.circustrain.replication.event\", \"event_id\");\n+    metaStoreClient.alter_table(REPLICA_DB, TABLE, replicaTable);\n+\n+    // Create the source table with the evolved schema\n+    Table sourceTable = helper.createParquetPartitionedTable(\n+        toUri(sourceWarehouseUri, SOURCE_DB, TABLE),\n+        SOURCE_DB,\n+        TABLE,\n+        evolvedSchema,\n+        afterEvolution.fieldName,\n+        afterEvolution.expectedData,\n+        2);\n+    LOG.info(\">>>> Table {} \", metaStoreClient.getTable(SOURCE_DB, TABLE));\n+\n+    // Create the original partition (with the original schema) and add to the source table\n+    URI partition = helper.createData(toUri(sourceWarehouseUri, SOURCE_DB, TABLE),\n+        schema, Integer.toString(1), 1, beforeEvolution.fieldName, beforeEvolution.expectedData);\n+    metaStoreClient.add_partitions(Arrays.asList(\n+        newTablePartition(sourceTable, Arrays.asList(\"1\"), partition)\n+    ));\n+\n+    CircusTrainRunner runner = CircusTrainRunner\n+        .builder(SOURCE_DB, sourceWarehouseUri, replicaWarehouseUri, housekeepingDbLocation)\n+        .replicaDatabaseName(REPLICA_DB)\n+        .sourceMetaStore(thriftMetaStoreRule.getThriftConnectionUri(), thriftMetaStoreRule.connectionURL(),\n+            thriftMetaStoreRule.driverClassName())\n+        .replicaMetaStore(thriftMetaStoreRule.getThriftConnectionUri())\n+        .build();\n+\n+    // Set up the asserts\n+    exit.expectSystemExitWithStatus(0);\n+\n+    // Do the replication\n+    File config = dataFolder.getFile(\"partitioned-single-table-one-partition.yml\");\n+    runner.run(config.getAbsolutePath());\n+  }\n+\n+  private void runDataChecks(Schema schema, List<String> expectedData) throws Exception {\n+    try {\n+      assertTable(thriftMetaStoreRule.newClient(), schema, SOURCE_DB, TABLE, expectedData);\n+      assertTable(thriftMetaStoreRule.newClient(), schema, REPLICA_DB, TABLE, expectedData);\n+    } catch (Exception e) {\n+      throw new Exception(\"Test failed\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efd4028767ba9f3a5534ecce6242ac33d1c0f0ef"}, "originalPosition": 654}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjExMTQ4Mw==", "bodyText": "Good point, no benefit, will remove", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r442111483", "createdAt": "2020-06-18T09:58:41Z", "author": {"login": "max-jacobs"}, "path": "circus-train-integration-tests/src/test/java/com/hotels/bdp/circustrain/integration/CircusTrainParquetSchemaEvolutionIntegrationTest.java", "diffHunk": "@@ -0,0 +1,685 @@\n+/**\n+ * Copyright (C) 2016-2020 Expedia, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hotels.bdp.circustrain.integration;\n+\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.Assert.assertThat;\n+\n+import static com.hotels.bdp.circustrain.integration.IntegrationTestHelper.EVOLUTION_COLUMN;\n+import static com.hotels.bdp.circustrain.integration.IntegrationTestHelper.PARTITIONED_TABLE;\n+import static com.hotels.bdp.circustrain.integration.utils.TestUtils.newTablePartition;\n+import static com.hotels.bdp.circustrain.integration.utils.TestUtils.toUri;\n+\n+import java.io.File;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.SchemaBuilder;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorGenerator;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.contrib.java.lang.system.ExpectedSystemExit;\n+import org.junit.experimental.categories.Category;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import fm.last.commons.test.file.ClassDataFolder;\n+import fm.last.commons.test.file.DataFolder;\n+\n+import com.google.common.collect.Lists;\n+import com.klarna.hiverunner.HiveShell;\n+import com.klarna.hiverunner.StandaloneHiveRunner;\n+import com.klarna.hiverunner.annotations.HiveSQL;\n+\n+import com.hotels.bdp.circustrain.common.test.base.CircusTrainRunner;\n+import com.hotels.bdp.circustrain.common.test.junit.rules.ServerSocketRule;\n+import com.hotels.bdp.circustrain.integration.utils.SchemaEvolution;\n+import com.hotels.bdp.circustrain.integration.utils.ThriftMetastoreServerRuleExtension;\n+import com.hotels.hcommon.hive.metastore.iterator.PartitionIterator;\n+\n+@Category(SchemaEvolution.class)\n+@RunWith(StandaloneHiveRunner.class)\n+public class CircusTrainParquetSchemaEvolutionIntegrationTest {\n+\n+  private static String SOURCE_DB = \"source_db\";\n+  private static String TABLE = \"ct_table_p\";\n+  private static String REPLICA_DB = \"replica_db\";\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(CircusTrainParquetSchemaEvolutionIntegrationTest.class);\n+\n+  @HiveSQL(files = {}, autoStart = false)\n+  private HiveShell shell;\n+\n+  public @Rule ExpectedSystemExit exit = ExpectedSystemExit.none();\n+  public @Rule TemporaryFolder temporaryFolder = new TemporaryFolder();\n+  public @Rule DataFolder dataFolder = new ClassDataFolder();\n+  public @Rule ServerSocketRule serverSocketRule = new ServerSocketRule();\n+\n+  private File sourceWarehouseUri;\n+  private File replicaWarehouseUri;\n+  private File housekeepingDbLocation;\n+\n+  private IntegrationTestHelper helper;\n+\n+  private ThriftMetastoreServerRuleExtension thriftMetaStoreRule;\n+  private HiveMetaStoreClient metaStoreClient;\n+\n+  @Before\n+  public void init() throws Throwable {\n+    shell.start();\n+    shell.execute(\"CREATE DATABASE \" + SOURCE_DB);\n+    shell.execute(\"CREATE DATABASE \" + REPLICA_DB);\n+    HiveConf hiveConf = shell.getHiveConf();\n+    thriftMetaStoreRule = new ThriftMetastoreServerRuleExtension(hiveConf);\n+    thriftMetaStoreRule.before();\n+    metaStoreClient = thriftMetaStoreRule.client();\n+\n+    sourceWarehouseUri = temporaryFolder.newFolder(\"source-warehouse\");\n+    replicaWarehouseUri = temporaryFolder.newFolder(\"replica-warehouse\");\n+    helper = new IntegrationTestHelper(metaStoreClient);\n+    temporaryFolder.newFolder(\"db\");\n+    housekeepingDbLocation = new File(new File(temporaryFolder.getRoot(), \"db\"), \"housekeeping\");\n+  }\n+\n+  @After\n+  public void teardown() {\n+    thriftMetaStoreRule.after();\n+  }\n+\n+  @Test\n+  public void addFieldAtEnd() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"after\");\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tNULL\\t1\",\n+        \"2\\tafter\\t2\"\n+    );\n+    runTest(schema, evolvedSchema, new FieldDataWrapper(), afterEvolution);\n+  }\n+\n+  @Test\n+  public void removeField() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"before\");\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1\",\n+        \"2\\t2\"\n+    );\n+    runTest(schema, evolvedSchema, beforeEvolution, new FieldDataWrapper());\n+  }\n+\n+  // Replication success - old expectedData not \"renamed\" too\n+  @Test\n+  public void renameField() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN + \"_renamed\")\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"before\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN + \"_renamed\", \"after\");\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tbefore\\t1\",\n+        \"2\\tafter\\t2\"\n+    );\n+    \n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void addDefaultValue() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().nullableString(EVOLUTION_COLUMN, \"default\")\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"before\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"after\");\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tbefore\\t1\",\n+        \"2\\tafter\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void removeDefaultValue() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().nullableString(EVOLUTION_COLUMN, \"default\")\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"before\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"after\");\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tbefore\\t1\",\n+        \"2\\tafter\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void makeFieldNullable() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .nullable()\n+        .stringType()\n+        .noDefault()\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"before\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, null);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tbefore\\t1\",\n+        \"2\\tNULL\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void makeFieldStruct() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredString(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .record(EVOLUTION_COLUMN + \"_struct\")\n+        .fields()\n+        .requiredString(\"after_col\")\n+        .endRecord()\n+        .noDefault()\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"before\");\n+    Map<String, String> after = new HashMap<>();\n+    after.put(\"after_col\", \"after\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, after);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tbefore\\t1\",\n+        \"2\\ttrue\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void addColumnToStruct() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .record(EVOLUTION_COLUMN + \"_struct\")\n+        .fields()\n+        .requiredString(\"name\")\n+        .requiredString(\"city\")\n+        .endRecord()\n+        .noDefault()\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .record(EVOLUTION_COLUMN + \"_struct\")\n+        .fields()\n+        .requiredString(\"name\")\n+        .requiredString(\"city\")\n+        .requiredString(\"dob\")\n+        .endRecord()\n+        .noDefault()\n+        .endRecord();\n+    Map<String, String> before = new HashMap<>();\n+    before.put(\"name\", \"lisa\");\n+    before.put(\"city\", \"blackpool\");\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, before);\n+    Map<String, String> after = new HashMap<>();\n+    after.put(\"name\", \"adam\");\n+    after.put(\"city\", \"london\");\n+    after.put(\"dob\", \"22/09/1992\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, after);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t{\\\"name\\\":\\\"lisa\\\",\\\"city\\\":\\\"blackpool\\\",\\\"dob\\\":null}\\t1\",\n+        \"2\\t{\\\"name\\\":\\\"adam\\\",\\\"city\\\":\\\"london\\\",\\\"dob\\\":\\\"22/09/1992\\\"}\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void removeColumnFromStruct() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .record(EVOLUTION_COLUMN + \"_struct\")\n+        .fields()\n+        .requiredString(\"name\")\n+        .requiredString(\"city\")\n+        .requiredString(\"dob\")\n+        .endRecord()\n+        .noDefault()\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .record(EVOLUTION_COLUMN + \"_struct\")\n+        .fields()\n+        .requiredString(\"name\")\n+        .requiredString(\"city\")\n+        .endRecord()\n+        .noDefault()\n+        .endRecord();\n+    Map<String, String> before = new HashMap<>();\n+    before.put(\"name\", \"lisa\");\n+    before.put(\"city\", \"blackpool\");\n+    before.put(\"dob\", \"22/09/1992\");\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, before);\n+    Map<String, String> after = new HashMap<>();\n+    after.put(\"name\", \"adam\");\n+    after.put(\"city\", \"london\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, after);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t{\\\"name\\\":\\\"lisa\\\",\\\"city\\\":\\\"blackpool\\\"}\\t1\",\n+        \"2\\t{\\\"name\\\":\\\"adam\\\",\\\"city\\\":\\\"london\\\"}\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void promoteIntToLong() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredInt(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredFloat(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2L);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void promoteIntToFloat() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredInt(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredFloat(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2f);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void promoteIntToDouble() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredInt(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredDouble(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2d);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void promoteFloatToDouble() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredFloat(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredDouble(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1f);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2d);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void promoteLongToFloat() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredFloat(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1L);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2f);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void promoteLongToDouble() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredDouble(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1L);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2d);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test(expected = Exception.class)\n+  public void demoteLongToInt() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredInt(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1L);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1\\t1\",\n+        \"2\\t2\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test(expected = Exception.class)\n+  public void demoteDoubleToFloat() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredInt(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1d);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2f);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test(expected = Exception.class)\n+  public void demoteDoubleToInt() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredInt(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1d);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1\\t1\",\n+        \"2\\t2\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test(expected = Exception.class)\n+  public void demoteFloatToInt() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredInt(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1f);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1\\t1\",\n+        \"2\\t2\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test(expected = Exception.class)\n+  public void demoteDoubleToLong() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredDouble(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1d);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2l);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test(expected = Exception.class)\n+  public void demoteFloatToLong() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().requiredFloat(EVOLUTION_COLUMN)\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().requiredLong(EVOLUTION_COLUMN)\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 1f);\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, 2l);\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\t1.0\\t1\",\n+        \"2\\t2.0\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void addValueToEnum() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .enumeration(EVOLUTION_COLUMN + \"_enum\")\n+        .symbols(\"FIRST\")\n+        .noDefault()\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .enumeration(EVOLUTION_COLUMN + \"_enum\")\n+        .symbols(\"FIRST\", \"SECOND\")\n+        .noDefault()\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"FIRST\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"SECOND\");\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tFIRST\\t1\",\n+        \"2\\tSECOND\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  @Test\n+  public void removeValueFromEnum() throws Exception {\n+    Schema schema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .enumeration(EVOLUTION_COLUMN + \"_enum\")\n+        .symbols(\"FIRST\", \"SECOND\")\n+        .noDefault()\n+        .endRecord();\n+    Schema evolvedSchema = getSchemaFieldAssembler().name(EVOLUTION_COLUMN)\n+        .type()\n+        .enumeration(EVOLUTION_COLUMN + \"_enum\")\n+        .symbols(\"FIRST\")\n+        .noDefault()\n+        .endRecord();\n+    FieldDataWrapper beforeEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"SECOND\");\n+    FieldDataWrapper afterEvolution = new FieldDataWrapper(EVOLUTION_COLUMN, \"FIRST\");\n+    List<String> expectedData = Lists.newArrayList(\n+        \"1\\tSECOND\\t1\",\n+        \"2\\tFIRST\\t2\"\n+    );\n+\n+    runTest(schema, evolvedSchema, beforeEvolution, afterEvolution);\n+    runDataChecks(evolvedSchema, expectedData);\n+  }\n+\n+  private static class FieldDataWrapper {\n+\n+    private String fieldName;\n+    private Object expectedData;\n+\n+    public FieldDataWrapper() {\n+    }\n+\n+    public FieldDataWrapper(String fieldName, Object expectedData) {\n+      this.fieldName = fieldName;\n+      this.expectedData = expectedData;\n+    }\n+  }\n+\n+  private SchemaBuilder.FieldAssembler<Schema> getSchemaFieldAssembler() {\n+    return SchemaBuilder\n+        .builder(\"name.space\")\n+        .record(PARTITIONED_TABLE)\n+        .fields()\n+        .requiredInt(\"id\");\n+  }\n+\n+  private void runTest(\n+      Schema schema,\n+      Schema evolvedSchema,\n+      FieldDataWrapper beforeEvolution,\n+      FieldDataWrapper afterEvolution) throws Exception {\n+    // Create initial replica table with the original schema (setting a Circus Train event id manually).\n+    Table replicaTable = helper.createParquetPartitionedTable(\n+        toUri(replicaWarehouseUri, REPLICA_DB, TABLE),\n+        REPLICA_DB,\n+        TABLE,\n+        schema,\n+        beforeEvolution.fieldName,\n+        beforeEvolution.expectedData,\n+        1);\n+    LOG.info(\">>>> Table {} \", metaStoreClient.getTable(REPLICA_DB, TABLE));\n+\n+    replicaTable.getParameters().put(\"com.hotels.bdp.circustrain.replication.event\", \"event_id\");\n+    metaStoreClient.alter_table(REPLICA_DB, TABLE, replicaTable);\n+\n+    // Create the source table with the evolved schema\n+    Table sourceTable = helper.createParquetPartitionedTable(\n+        toUri(sourceWarehouseUri, SOURCE_DB, TABLE),\n+        SOURCE_DB,\n+        TABLE,\n+        evolvedSchema,\n+        afterEvolution.fieldName,\n+        afterEvolution.expectedData,\n+        2);\n+    LOG.info(\">>>> Table {} \", metaStoreClient.getTable(SOURCE_DB, TABLE));\n+\n+    // Create the original partition (with the original schema) and add to the source table\n+    URI partition = helper.createData(toUri(sourceWarehouseUri, SOURCE_DB, TABLE),\n+        schema, Integer.toString(1), 1, beforeEvolution.fieldName, beforeEvolution.expectedData);\n+    metaStoreClient.add_partitions(Arrays.asList(\n+        newTablePartition(sourceTable, Arrays.asList(\"1\"), partition)\n+    ));\n+\n+    CircusTrainRunner runner = CircusTrainRunner\n+        .builder(SOURCE_DB, sourceWarehouseUri, replicaWarehouseUri, housekeepingDbLocation)\n+        .replicaDatabaseName(REPLICA_DB)\n+        .sourceMetaStore(thriftMetaStoreRule.getThriftConnectionUri(), thriftMetaStoreRule.connectionURL(),\n+            thriftMetaStoreRule.driverClassName())\n+        .replicaMetaStore(thriftMetaStoreRule.getThriftConnectionUri())\n+        .build();\n+\n+    // Set up the asserts\n+    exit.expectSystemExitWithStatus(0);\n+\n+    // Do the replication\n+    File config = dataFolder.getFile(\"partitioned-single-table-one-partition.yml\");\n+    runner.run(config.getAbsolutePath());\n+  }\n+\n+  private void runDataChecks(Schema schema, List<String> expectedData) throws Exception {\n+    try {\n+      assertTable(thriftMetaStoreRule.newClient(), schema, SOURCE_DB, TABLE, expectedData);\n+      assertTable(thriftMetaStoreRule.newClient(), schema, REPLICA_DB, TABLE, expectedData);\n+    } catch (Exception e) {\n+      throw new Exception(\"Test failed\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyNDcwOA=="}, "originalCommit": {"oid": "efd4028767ba9f3a5534ecce6242ac33d1c0f0ef"}, "originalPosition": 654}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NDkxNDkyOnYy", "diffSide": "RIGHT", "path": "circus-train-integration-tests/pom.xml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxMjozMDowNlrOGltE5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxMjozMDowNlrOGltE5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjE4OTAzMQ==", "bodyText": "Minor, but since you have this twice and they're related you could make this a property.", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r442189031", "createdAt": "2020-06-18T12:30:06Z", "author": {"login": "massdosage"}, "path": "circus-train-integration-tests/pom.xml", "diffHunk": "@@ -14,9 +15,20 @@\n     <hadoop.version>2.8.1</hadoop.version>\n     <!-- override for hadoop-common dependency that is used in this test -->\n     <guava.version>17.0</guava.version>\n+    <surefire.excluded.groups>com.hotels.bdp.circustrain.integration.utils.SchemaEvolution</surefire.excluded.groups>\n   </properties>\n \n   <dependencies>\n+    <dependency>\n+      <groupId>org.apache.logging.log4j</groupId>\n+      <artifactId>log4j-api</artifactId>\n+      <version>2.5</version>\n+    </dependency>\n+    <dependency>\n+      <groupId>org.apache.logging.log4j</groupId>\n+      <artifactId>log4j-core</artifactId>\n+      <version>2.5</version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05868f5d9ee562996e161ede66e230c238f4e56e"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NDkxNzU1OnYy", "diffSide": "RIGHT", "path": "circus-train-integration-tests/pom.xml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxMjozMDo1MlrOGltGow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwOTo1NDo1MFrOGmPDvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjE4OTQ3NQ==", "bodyText": "Should this be test or provided scope? I'd be concerned of possible classpath clash if this ends up in CT where it didn't beore.", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r442189475", "createdAt": "2020-06-18T12:30:52Z", "author": {"login": "massdosage"}, "path": "circus-train-integration-tests/pom.xml", "diffHunk": "@@ -188,6 +206,39 @@\n       <version>1.7.7</version>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.apache.avro</groupId>\n+      <artifactId>avro-mapred</artifactId>\n+      <version>1.7.7</version>\n+      <scope>test</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>javax.servlet</groupId>\n+      <artifactId>javax.servlet-api</artifactId>\n+      <version>3.1.0</version>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.esotericsoftware</groupId>\n+      <artifactId>kryo</artifactId>\n+      <version>4.0.2</version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05868f5d9ee562996e161ede66e230c238f4e56e"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc0NTc4OA==", "bodyText": "will move this to the test scope", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r442745788", "createdAt": "2020-06-19T09:54:50Z", "author": {"login": "max-jacobs"}, "path": "circus-train-integration-tests/pom.xml", "diffHunk": "@@ -188,6 +206,39 @@\n       <version>1.7.7</version>\n       <scope>test</scope>\n     </dependency>\n+    <dependency>\n+      <groupId>org.apache.avro</groupId>\n+      <artifactId>avro-mapred</artifactId>\n+      <version>1.7.7</version>\n+      <scope>test</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>javax.servlet</groupId>\n+      <artifactId>javax.servlet-api</artifactId>\n+      <version>3.1.0</version>\n+      <scope>provided</scope>\n+    </dependency>\n+    <dependency>\n+      <groupId>com.esotericsoftware</groupId>\n+      <artifactId>kryo</artifactId>\n+      <version>4.0.2</version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjE4OTQ3NQ=="}, "originalCommit": {"oid": "05868f5d9ee562996e161ede66e230c238f4e56e"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODUzMzkwOnYy", "diffSide": "RIGHT", "path": "circus-train-integration-tests/pom.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMDo0ODoxOVrOGmQjOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMTozOToxMVrOGmR0cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc3MDIzNA==", "bodyText": "I'm pretty sure you didn't mean to do this!", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r442770234", "createdAt": "2020-06-19T10:48:19Z", "author": {"login": "massdosage"}, "path": "circus-train-integration-tests/pom.xml", "diffHunk": "@@ -135,12 +148,18 @@\n       <groupId>com.hotels</groupId>\n       <artifactId>circus-train-common-test</artifactId>\n       <version>${project.version}</version>\n+      <exclusions>\n+        <exclusion>\n+          <groupId>javax.servlet</groupId>\n+          <artifactId>javax.servlet-api</artifactId>\n+        </exclusion>\n+      </exclusions>\n       <scope>test</scope>\n     </dependency>\n     <dependency>\n       <groupId>com.google.code.gson</groupId>\n       <artifactId>gson</artifactId>\n-      <version>2.5</version>\n+      <version>${log4j-api.version}</version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "996cdd1902eb7ef046f72af046fb034abe2870ed"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc5MTAyNw==", "bodyText": ":(", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r442791027", "createdAt": "2020-06-19T11:39:11Z", "author": {"login": "max-jacobs"}, "path": "circus-train-integration-tests/pom.xml", "diffHunk": "@@ -135,12 +148,18 @@\n       <groupId>com.hotels</groupId>\n       <artifactId>circus-train-common-test</artifactId>\n       <version>${project.version}</version>\n+      <exclusions>\n+        <exclusion>\n+          <groupId>javax.servlet</groupId>\n+          <artifactId>javax.servlet-api</artifactId>\n+        </exclusion>\n+      </exclusions>\n       <scope>test</scope>\n     </dependency>\n     <dependency>\n       <groupId>com.google.code.gson</groupId>\n       <artifactId>gson</artifactId>\n-      <version>2.5</version>\n+      <version>${log4j-api.version}</version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc3MDIzNA=="}, "originalCommit": {"oid": "996cdd1902eb7ef046f72af046fb034abe2870ed"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODY3NDE0OnYy", "diffSide": "RIGHT", "path": "circus-train-integration-tests/pom.xml", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMTo0MzowOVrOGmR6kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMTo0MzowOVrOGmR6kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc5MjU5NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                <log4j-api.version>2.5</log4j-api.version>\n          \n          \n            \n                <log4j.version>2.5</log4j.version>\n          \n      \n    \n    \n  \n\n(it's not just the api)", "url": "https://github.com/ExpediaGroup/circus-train/pull/188#discussion_r442792595", "createdAt": "2020-06-19T11:43:09Z", "author": {"login": "massdosage"}, "path": "circus-train-integration-tests/pom.xml", "diffHunk": "@@ -14,9 +15,21 @@\n     <hadoop.version>2.8.1</hadoop.version>\n     <!-- override for hadoop-common dependency that is used in this test -->\n     <guava.version>17.0</guava.version>\n+    <log4j-api.version>2.5</log4j-api.version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f37f38ccf0502e6eabeb87780a4d986cec1b7d54"}, "originalPosition": 11}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3895, "cost": 1, "resetAt": "2021-11-12T18:49:56Z"}}}