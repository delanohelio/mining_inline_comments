{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg2Mzk1ODI2", "number": 770, "title": "Issue #765 #768 Bulkdata import metrics and error handling enhancements", "bodyText": "Changes:\n(1) Added error handling for fhir parse exception.\n(2) Added error handling and retry for https and S3 client timeout/reset exception.\n(3) Increase S3 client socket timeout(without activity) to 120 seconds.\n(4) Use the sum of job execution times as the job instance time when calculating rate.\n(5) Skip the already processed lines after the job is restarted.", "createdAt": "2020-03-10T22:31:46Z", "url": "https://github.com/IBM/FHIR/pull/770", "merged": true, "mergeCommit": {"oid": "4bbc80b3266d08cee163c43bfce8033b24bdcfbb"}, "closed": true, "closedAt": "2020-03-11T18:04:04Z", "author": {"login": "albertwang-ibm"}, "timelineItems": {"totalCount": 41, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcMaL5bAH2gAyMzg2Mzk1ODI2OjkwMzY0Y2UwZDI4MWJlMGVhMDI5OWM1ODlhZWY0MjhmZGM3YWYxNjk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcMq5VwgFqTM3Mjk5MjY5MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "90364ce0d281be0ea0299c589aef428fdc7af169", "author": {"user": {"login": "albertwang-ibm", "name": "Albert(Xu) Wang"}}, "url": "https://github.com/IBM/FHIR/commit/90364ce0d281be0ea0299c589aef428fdc7af169", "committedDate": "2020-03-10T22:23:42Z", "message": "issue #765 #768 Bulkdata batchJob metrics and error handling enhancement\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b75adb84d3d4dabc985a23286536b022aaae40df", "author": {"user": {"login": "albertwang-ibm", "name": "Albert(Xu) Wang"}}, "url": "https://github.com/IBM/FHIR/commit/b75adb84d3d4dabc985a23286536b022aaae40df", "committedDate": "2020-03-10T22:25:03Z", "message": "issue #765 adding the changed Constants.java\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3", "author": {"user": {"login": "albertwang-ibm", "name": "Albert(Xu) Wang"}}, "url": "https://github.com/IBM/FHIR/commit/b377ad83612ea30e7361c65d3e07f9b83bb398d3", "committedDate": "2020-03-11T02:42:42Z", "message": "issue #765 use logger instead of system.out for metric\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzE3OTky", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372717992", "createdAt": "2020-03-11T12:37:08Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjozNzowOVrOF01FXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjozNzowOVrOF01FXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDkzOTk5Nw==", "bodyText": "maybe convert the values to constants explaining REQUEST_TIMEOUT_IN_SECONDS or something similar -- 1000 = second", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390939997", "createdAt": "2020-03-11T12:37:09Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -84,8 +85,10 @@ public static AmazonS3 getCosClient(String cosCredentialIbm, String cosApiKeyPro\n             credentials = new BasicAWSCredentials(cosApiKeyProperty, cosSrvinstId);\n         }\n \n-        ClientConfiguration clientConfig = new ClientConfiguration().withRequestTimeout(8000);\n-        clientConfig.setUseTcpKeepAlive(true);\n+        ClientConfiguration clientConfig = new ClientConfiguration()\n+                .withRequestTimeout(10*1000)\n+                .withTcpKeepAlive(true)\n+                .withSocketTimeout(120*1000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 17}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzE4NDcw", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372718470", "createdAt": "2020-03-11T12:37:50Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjozNzo1MFrOF01GzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjozNzo1MFrOF01GzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0MDM2NA==", "bodyText": "interesting... I hadn't thought so much about this case... make sense", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390940364", "createdAt": "2020-03-11T12:37:50Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -134,9 +137,18 @@ public static void listBuckets(AmazonS3 cosClient) {\n         }\n     }\n \n+    /**\n+     * @param resReader - the buffer reader to read FHIR resource from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @return - the number of parsing failures.\n+     * @throws Exception\n+     */\n     private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfLinesToSkip, List<Resource> fhirResources) throws Exception {\n         int exported = 0;\n         int lineRed = 0;\n+        int parseFailures = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzE4Njkw", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372718690", "createdAt": "2020-03-11T12:38:09Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjozODowOVrOF01HeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjozODowOVrOF01HeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0MDUzNw==", "bodyText": "you can switch back to a while loop", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390940537", "createdAt": "2020-03-11T12:38:09Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -134,9 +137,18 @@ public static void listBuckets(AmazonS3 cosClient) {\n         }\n     }\n \n+    /**\n+     * @param resReader - the buffer reader to read FHIR resource from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @return - the number of parsing failures.\n+     * @throws Exception\n+     */\n     private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfLinesToSkip, List<Resource> fhirResources) throws Exception {\n         int exported = 0;\n         int lineRed = 0;\n+        int parseFailures = 0;\n+\n         String resLine = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzIyMTk5", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372722199", "createdAt": "2020-03-11T12:43:19Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo0MzoxOVrOF01SHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo0MzoxOVrOF01SHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0MzI2MA==", "bodyText": "it may be worth logging out the line number as well", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390943260", "createdAt": "2020-03-11T12:43:19Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzI0MjA2", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372724206", "createdAt": "2020-03-11T12:46:17Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo0NjoxN1rOF01YFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo0NjoxN1rOF01YFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NDc5MA==", "bodyText": "why would export result in parse failures? wouldnt it already be valid and stored in the db\nalso why is it reporting back the number of parseFailures? maybe update the method signature to explain the contract and purpose", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390944790", "createdAt": "2020-03-11T12:46:17Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzI0NTY3", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372724567", "createdAt": "2020-03-11T12:46:49Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo0Njo0OVrOF01ZLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo0Njo0OVrOF01ZLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NTA2OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n          \n          \n            \n                public static void cleanupForTransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n          \n      \n    \n    \n  \n\nI don't think we should use the number 4 here", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390945069", "createdAt": "2020-03-11T12:46:49Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 95}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzI2Mjk3", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372726297", "createdAt": "2020-03-11T12:49:21Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo0OToyMlrOF01eVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo0OToyMlrOF01eVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NjM5MA==", "bodyText": "it's cool to see this works in the context of the webapp (I'm sure this simplifies things)", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390946390", "createdAt": "2020-03-11T12:49:22Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java", "diffHunk": "@@ -8,29 +8,46 @@\n \n import java.util.HashMap;\n import java.util.List;\n+import java.util.logging.Logger;\n \n import javax.batch.api.listener.JobListener;\n+import javax.batch.operations.JobOperator;\n+import javax.batch.runtime.BatchRuntime;\n+import javax.batch.runtime.JobExecution;\n import javax.batch.runtime.context.JobContext;\n import javax.inject.Inject;\n \n public class ImportJobListener implements JobListener {\n+    private static final Logger logger = Logger.getLogger(ImportJobListener.class.getName());\n     @Inject\n     JobContext jobContext;\n \n-    private long jobStartTimeInMS, jobEndTimeInMS;\n-\n     public ImportJobListener() {\n \n     }\n \n-    @SuppressWarnings(\"unchecked\")\n+\n+    @SuppressWarnings({\"unchecked\" })\n     @Override\n     public void afterJob() {\n+        // jobExecution.getEndTime() for current execution always returns null, so we use system current time as the end time for current execution.\n+        long currentExecutionEndTimeInMS = System.currentTimeMillis();;\n+\n         // Used for generating response for all the import data resources.\n         List<ImportCheckPointData> partitionSummaries = (List<ImportCheckPointData>)jobContext.getTransientUserData();\n         // Used for generating performance measurement per each resource type.\n         HashMap<String, ImportCheckPointData> importedResourceTypeSummaries = new HashMap<>();\n \n+        JobOperator jobOperator = BatchRuntime.getJobOperator();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzI2ODU3", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372726857", "createdAt": "2020-03-11T12:50:12Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1MDoxMlrOF01gMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1MDoxMlrOF01gMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0Njg2NQ==", "bodyText": "double?  does this need to be a double or is a long just fine?", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390946865", "createdAt": "2020-03-11T12:50:12Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java", "diffHunk": "@@ -49,26 +66,25 @@ public void afterJob() {\n             }\n         }\n \n-        jobEndTimeInMS = System.currentTimeMillis();\n-        double jobProcessingSeconds = (jobEndTimeInMS - jobStartTimeInMS)/1000.0;\n+\n+        double jobProcessingSeconds = (totalJobExecutionMilliSeconds)/1000.0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 57}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzI3MzU3", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372727357", "createdAt": "2020-03-11T12:50:56Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1MDo1NlrOF01hww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1MDo1NlrOF01hww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NzI2Nw==", "bodyText": "is this outputting a report?", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390947267", "createdAt": "2020-03-11T12:50:56Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java", "diffHunk": "@@ -49,26 +66,25 @@ public void afterJob() {\n             }\n         }\n \n-        jobEndTimeInMS = System.currentTimeMillis();\n-        double jobProcessingSeconds = (jobEndTimeInMS - jobStartTimeInMS)/1000.0;\n+\n+        double jobProcessingSeconds = (totalJobExecutionMilliSeconds)/1000.0;\n         jobProcessingSeconds = jobProcessingSeconds < 1 ? 1.0 : jobProcessingSeconds;\n \n-        // Print out the simple metrics to console.\n-        System.out.println(\" ---- Fhir resources imported in \" + jobProcessingSeconds + \"seconds ----\");\n-        System.out.println(\"ResourceType \\t Imported \\t Failed\");\n+        // log the simple metrics.\n+        logger.info(\" ---- Fhir resources imported in \" + jobProcessingSeconds + \"seconds ----\");\n+        logger.info(\"ResourceType \\t Imported \\t Failed\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 65}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzI3OTYx", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372727961", "createdAt": "2020-03-11T12:51:51Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1MTo1MVrOF01jog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1MTo1MVrOF01jog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0Nzc0Ng==", "bodyText": "I suggested a signature change in a prior comment.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        BulkDataUtils.cleanup4TransientUserData(partitionSummaryData, true);\n          \n          \n            \n                        BulkDataUtils.cleanupForTransientUserData(partitionSummaryData, true);", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390947746", "createdAt": "2020-03-11T12:51:51Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportPartitionCollector.java", "diffHunk": "@@ -79,16 +78,7 @@ public Serializable collectPartitionData() throws Exception{\n \n         // If the job is being stopped or in other status except for \"started\", then do cleanup for the partition.\n         if (!batchStatus.equals(BatchStatus.STARTED)) {\n-            if (partitionSummaryData.getInputStream() != null) {\n-                if (partitionSummaryData.getInputStream() instanceof S3ObjectInputStream) {\n-                    ((S3ObjectInputStream)partitionSummaryData.getInputStream()).abort();\n-                }\n-                partitionSummaryData.getInputStream().close();\n-            }\n-\n-            if (partitionSummaryData.getBufferReader() != null) {\n-                partitionSummaryData.getBufferReader().close();\n-            }\n+            BulkDataUtils.cleanup4TransientUserData(partitionSummaryData, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 22}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzI4MDY0", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372728064", "createdAt": "2020-03-11T12:52:01Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1MjowMVrOF01j8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1MjowMVrOF01j8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0NzgyNw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        BulkDataUtils.cleanup4TransientUserData(partitionSummaryData, false);\n          \n          \n            \n                        BulkDataUtils.cleanupForTransientUserData(partitionSummaryData, false);", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390947827", "createdAt": "2020-03-11T12:52:01Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportPartitionCollector.java", "diffHunk": "@@ -152,13 +142,8 @@ public Serializable collectPartitionData() throws Exception{\n                 }\n             }\n \n-            if (partitionSummaryData.getBufferReader() != null) {\n-                partitionSummaryData.getBufferReader().close();\n-            }\n-\n-            if (partitionSummaryData.getInputStream() != null) {\n-                partitionSummaryData.getInputStream().close();\n-            }\n+            // Clean up.\n+            BulkDataUtils.cleanup4TransientUserData(partitionSummaryData, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 38}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzI4NDkw", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372728490", "createdAt": "2020-03-11T12:52:39Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1MjozOVrOF01lUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1MjozOVrOF01lUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODE3Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                logger.warning(\"Failed to import \" + fhirResource.getId() + \" due to error: \" + e.getMessage());\n          \n          \n            \n                                logger.warning(\"Failed to import '\" + fhirResource.getId() + \"' due to error: \" + e.getMessage());", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390948177", "createdAt": "2020-03-11T12:52:39Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ChunkWriter.java", "diffHunk": "@@ -151,7 +151,7 @@ public void writeItems(List<java.lang.Object> arg0) throws Exception {\n                         }\n                     }\n                 } catch (FHIRPersistenceException e) {\n-                    logger.warning(\"Failed to import due to error: \" + e.getMessage());\n+                    logger.warning(\"Failed to import \" + fhirResource.getId() + \" due to error: \" + e.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 15}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzI5MjYx", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372729261", "createdAt": "2020-03-11T12:53:46Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1Mzo0NlrOF01npQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1Mzo0NlrOF01npQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODc3Mw==", "bodyText": "Thinking about this... it's probably worth adding a logger.finer() just in case the code changes and hits default", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390948773", "createdAt": "2020-03-11T12:53:46Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ChunkReader.java", "diffHunk": "@@ -142,17 +140,20 @@ public Object readItem() throws Exception {\n             } else {\n                 logger.finer(\"readItem: Got CosClient successfully!\");\n             }\n-            imported = BulkDataUtils.readFhirResourceFromObjectStore(cosClient, cosBucketName, importPartitionWorkitem,\n-                    numOfLinesToSkip, loadedFhirResources, Constants.IMPORT_IS_REUSE_INPUTSTREAM, chunkData);\n+            numOfParseFailures = BulkDataUtils.readFhirResourceFromObjectStore(cosClient, cosBucketName, importPartitionWorkitem,\n+                    numOfLinesToSkip, loadedFhirResources, chunkData);\n             break;\n         default:\n             break;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 31}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzI5NTA0", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372729504", "createdAt": "2020-03-11T12:54:08Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1NDowOFrOF01oWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1NDowOFrOF01oWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0ODk1Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            cleanup4TransientUserData(transientUserData, true);\n          \n          \n            \n                            cleanupForTransientUserData(transientUserData, true);", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390948952", "createdAt": "2020-03-11T12:54:08Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    throw ex;\n                 }\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n             }\n-        } else {\n-            try (BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n+        } while (retryTimes > 0);\n+\n+        return parseFailures;\n+    }\n+\n+    /**\n+     * @param filePath - file path to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+\n+        try {\n+            if (transientUserData.getBufferReader() == null) {\n+                BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                transientUserData.setBufferReader(resReader);\n+                // Skip the already processed lines after opening the input stream for first read.\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+            } else {\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n             }\n+        } catch (Exception ex) {\n+            // Clean up.\n+            fhirResources.clear();\n+            cleanup4TransientUserData(transientUserData, true);\n+            // Log the error and throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+            logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ex.getMessage());\n+            throw ex;\n         }\n-        return exported;\n+\n+        return parseFailures;\n     }\n \n \n+    /**\n+     * @param dataUrl - URL to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n     public static int readFhirResourceFromHttps(String dataUrl, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n                     InputStream inputStream = new URL(dataUrl).openConnection().getInputStream();\n                     transientUserData.setInputStream(inputStream);\n                     BufferedReader resReader = new BufferedReader(new InputStreamReader(inputStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 239}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzI5NzQ2", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372729746", "createdAt": "2020-03-11T12:54:30Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1NDozMVrOF01pJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1NDozMVrOF01pJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0OTE1Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Retry ...\");\n          \n          \n            \n                                logger.warning(\"readFhirResourceFromLocalFile: Retry ...\");\n          \n      \n    \n    \n  \n\nno need to split string", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390949157", "createdAt": "2020-03-11T12:54:31Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    throw ex;\n                 }\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n             }\n-        } else {\n-            try (BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n+        } while (retryTimes > 0);\n+\n+        return parseFailures;\n+    }\n+\n+    /**\n+     * @param filePath - file path to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+\n+        try {\n+            if (transientUserData.getBufferReader() == null) {\n+                BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                transientUserData.setBufferReader(resReader);\n+                // Skip the already processed lines after opening the input stream for first read.\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+            } else {\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n             }\n+        } catch (Exception ex) {\n+            // Clean up.\n+            fhirResources.clear();\n+            cleanup4TransientUserData(transientUserData, true);\n+            // Log the error and throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+            logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ex.getMessage());\n+            throw ex;\n         }\n-        return exported;\n+\n+        return parseFailures;\n     }\n \n \n+    /**\n+     * @param dataUrl - URL to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n     public static int readFhirResourceFromHttps(String dataUrl, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n                     InputStream inputStream = new URL(dataUrl).openConnection().getInputStream();\n                     transientUserData.setInputStream(inputStream);\n                     BufferedReader resReader = new BufferedReader(new InputStreamReader(inputStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file \" + dataUrl + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromLocalFile: \" + \"Retry ...\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 242}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzI5OTIw", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372729920", "createdAt": "2020-03-11T12:54:47Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1NDo0N1rOF01pnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1NDo0N1rOF01pnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0OTI3OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file \" + dataUrl + \" - \" + ex.getMessage());\n          \n          \n            \n                            logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file [\" + dataUrl + \"] - \" + ex.getMessage());", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390949279", "createdAt": "2020-03-11T12:54:47Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    throw ex;\n                 }\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n             }\n-        } else {\n-            try (BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n+        } while (retryTimes > 0);\n+\n+        return parseFailures;\n+    }\n+\n+    /**\n+     * @param filePath - file path to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+\n+        try {\n+            if (transientUserData.getBufferReader() == null) {\n+                BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                transientUserData.setBufferReader(resReader);\n+                // Skip the already processed lines after opening the input stream for first read.\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+            } else {\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n             }\n+        } catch (Exception ex) {\n+            // Clean up.\n+            fhirResources.clear();\n+            cleanup4TransientUserData(transientUserData, true);\n+            // Log the error and throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+            logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ex.getMessage());\n+            throw ex;\n         }\n-        return exported;\n+\n+        return parseFailures;\n     }\n \n \n+    /**\n+     * @param dataUrl - URL to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n     public static int readFhirResourceFromHttps(String dataUrl, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n                     InputStream inputStream = new URL(dataUrl).openConnection().getInputStream();\n                     transientUserData.setInputStream(inputStream);\n                     BufferedReader resReader = new BufferedReader(new InputStreamReader(inputStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file \" + dataUrl + \" - \" + ex.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 240}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzMwMTI1", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372730125", "createdAt": "2020-03-11T12:55:04Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1NTowNFrOF01qOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1NTowNFrOF01qOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk0OTQzNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file \" + dataUrl + \" - \" + ex.getMessage());\n          \n          \n            \n                                logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file [\" + dataUrl + \"] - \" + ex.getMessage());", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390949434", "createdAt": "2020-03-11T12:55:04Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    throw ex;\n                 }\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n             }\n-        } else {\n-            try (BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n+        } while (retryTimes > 0);\n+\n+        return parseFailures;\n+    }\n+\n+    /**\n+     * @param filePath - file path to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+\n+        try {\n+            if (transientUserData.getBufferReader() == null) {\n+                BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                transientUserData.setBufferReader(resReader);\n+                // Skip the already processed lines after opening the input stream for first read.\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+            } else {\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n             }\n+        } catch (Exception ex) {\n+            // Clean up.\n+            fhirResources.clear();\n+            cleanup4TransientUserData(transientUserData, true);\n+            // Log the error and throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+            logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ex.getMessage());\n+            throw ex;\n         }\n-        return exported;\n+\n+        return parseFailures;\n     }\n \n \n+    /**\n+     * @param dataUrl - URL to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n     public static int readFhirResourceFromHttps(String dataUrl, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n                     InputStream inputStream = new URL(dataUrl).openConnection().getInputStream();\n                     transientUserData.setInputStream(inputStream);\n                     BufferedReader resReader = new BufferedReader(new InputStreamReader(inputStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file \" + dataUrl + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromLocalFile: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    logger.warning(\"readFhirResourceFromHttps: \" + \"Error proccesing file \" + dataUrl + \" - \" + ex.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 245}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzMxNDA5", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372731409", "createdAt": "2020-03-11T12:56:55Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1Njo1NlrOF01uLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1Njo1NlrOF01uLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MDQ0NA==", "bodyText": "what does fhirResources.clear accomplish and why?  I see it's passed into this method.", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390950444", "createdAt": "2020-03-11T12:56:56Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    throw ex;\n                 }\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n             }\n-        } else {\n-            try (BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n+        } while (retryTimes > 0);\n+\n+        return parseFailures;\n+    }\n+\n+    /**\n+     * @param filePath - file path to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+\n+        try {\n+            if (transientUserData.getBufferReader() == null) {\n+                BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                transientUserData.setBufferReader(resReader);\n+                // Skip the already processed lines after opening the input stream for first read.\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+            } else {\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n             }\n+        } catch (Exception ex) {\n+            // Clean up.\n+            fhirResources.clear();\n+            cleanup4TransientUserData(transientUserData, true);\n+            // Log the error and throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+            logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ex.getMessage());\n+            throw ex;\n         }\n-        return exported;\n+\n+        return parseFailures;\n     }\n \n \n+    /**\n+     * @param dataUrl - URL to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n     public static int readFhirResourceFromHttps(String dataUrl, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n                     InputStream inputStream = new URL(dataUrl).openConnection().getInputStream();\n                     transientUserData.setInputStream(inputStream);\n                     BufferedReader resReader = new BufferedReader(new InputStreamReader(inputStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 238}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzMxNjM2", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372731636", "createdAt": "2020-03-11T12:57:14Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1NzoxNFrOF01u0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1NzoxNFrOF01u0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MDYwOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        cleanup4TransientUserData(transientUserData, true);\n          \n          \n            \n                        cleanupForTransientUserData(transientUserData, true);", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390950609", "createdAt": "2020-03-11T12:57:14Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    throw ex;\n                 }\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n             }\n-        } else {\n-            try (BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n+        } while (retryTimes > 0);\n+\n+        return parseFailures;\n+    }\n+\n+    /**\n+     * @param filePath - file path to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+\n+        try {\n+            if (transientUserData.getBufferReader() == null) {\n+                BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                transientUserData.setBufferReader(resReader);\n+                // Skip the already processed lines after opening the input stream for first read.\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+            } else {\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n             }\n+        } catch (Exception ex) {\n+            // Clean up.\n+            fhirResources.clear();\n+            cleanup4TransientUserData(transientUserData, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 197}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzMxOTAw", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372731900", "createdAt": "2020-03-11T12:57:35Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1NzozNVrOF01vfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1NzozNVrOF01vfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MDc4Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            cleanup4TransientUserData(transientUserData, true);\n          \n          \n            \n                            cleanupForTransientUserData(transientUserData, true);", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390950783", "createdAt": "2020-03-11T12:57:35Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 149}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzMyMDcx", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372732071", "createdAt": "2020-03-11T12:57:51Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1Nzo1MVrOF01wBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1Nzo1MVrOF01wBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MDkxNg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n          \n          \n            \n                            logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file [\" + itemName + \"] - \" + ex.getMessage());", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390950916", "createdAt": "2020-03-11T12:57:51Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 150}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzMyMjU4", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372732258", "createdAt": "2020-03-11T12:58:07Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1ODowOFrOF01wjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1ODowOFrOF01wjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MTA1Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n          \n          \n            \n                                logger.warning(\"readFhirResourceFromObjectStore: Retry ...\");", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390951052", "createdAt": "2020-03-11T12:58:08Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 152}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyNzMyNDQ5", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372732449", "createdAt": "2020-03-11T12:58:24Z", "commit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1ODoyNFrOF01xCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxMjo1ODoyNFrOF01xCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MTE3OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ex.getMessage());\n          \n          \n            \n                        logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file [\" + filePath + \"] - \" + ex.getMessage());", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r390951178", "createdAt": "2020-03-11T12:58:24Z", "author": {"login": "prb112"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -145,101 +157,159 @@ private static int getFhirResourceFromBufferReader(BufferedReader resReader, int\n                 if (lineRed <= numOfLinesToSkip) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse '\" + resLine + \"'\");\n+                    parseFailures++;\n+                    continue;\n                 }\n             }\n         } while (resLine != null);\n-        return exported;\n+        return parseFailures;\n     }\n \n-    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n-           int numOfLinesToSkip, List<Resource> fhirResources, boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n-            if (transientUserData.getBufferReader() == null) {\n-                S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-                S3ObjectInputStream s3InStream = item.getObjectContent();\n-                transientUserData.setInputStream(s3InStream);\n-                BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n-                transientUserData.setBufferReader(resReader);\n-            }\n-            try {\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n-            }\n-\n-        } else {\n-            S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n-            try (S3ObjectInputStream s3InStream = item.getObjectContent();\n-                 BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-                // Notify s3 client to abort and prevent the server from keeping on sending data.\n-                s3InStream.abort();\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ioe.getMessage());\n-                exported = 0;\n+    public static void cleanup4TransientUserData(ImportTransientUserData transientUserData, boolean isAbort) throws Exception {\n+        if (transientUserData.getInputStream() != null) {\n+            if (isAbort && transientUserData.getInputStream() instanceof S3ObjectInputStream) {\n+                // For S3 input stream, if the read is not finished successfully, we have to abort it first.\n+                ((S3ObjectInputStream)transientUserData.getInputStream()).abort();\n             }\n+            transientUserData.getInputStream().close();\n+            transientUserData.setInputStream(null);\n         }\n \n-        return exported;\n+        if (transientUserData.getBufferReader() != null) {\n+            transientUserData.getBufferReader().close();\n+            transientUserData.setBufferReader(null);\n+        }\n     }\n \n-\n-    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n-            boolean isReuseInput, ImportTransientUserData transientUserData) {\n-        int exported = 0;\n-        if (isReuseInput) {\n+    /**\n+     * @param cosClient - COS/S3 client.\n+     * @param bucketName - COS/S3 bucket name to read from.\n+     * @param itemName - COS/S3 object name to read from.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromObjectStore(AmazonS3 cosClient, String bucketName, String itemName,\n+           int numOfLinesToSkip, List<Resource> fhirResources, ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+        int retryTimes = Constants.IMPORT_RETRY_TIMES;\n+        do {\n             try {\n                 if (transientUserData.getBufferReader() == null) {\n-                    BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                    S3Object item = cosClient.getObject(new GetObjectRequest(bucketName, itemName));\n+                    S3ObjectInputStream s3InStream = item.getObjectContent();\n+                    transientUserData.setInputStream(s3InStream);\n+                    BufferedReader resReader = new BufferedReader(new InputStreamReader(s3InStream));\n                     transientUserData.setBufferReader(resReader);\n+                    // Skip the already processed lines after opening the input stream for first read.\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+                } else {\n+                    parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n+                }\n+                break;\n+            } catch (Exception ex) {\n+                // Clean up.\n+                fhirResources.clear();\n+                cleanup4TransientUserData(transientUserData, true);\n+                logger.warning(\"readFhirResourceFromObjectStore: \" + \"Error proccesing file \" + itemName + \" - \" + ex.getMessage());\n+                if ((retryTimes--) > 0) {\n+                    logger.warning(\"readFhirResourceFromObjectStore: \" + \"Retry ...\");\n+                } else {\n+                    // Throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+                    throw ex;\n                 }\n-                exported = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n             }\n-        } else {\n-            try (BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath))) {\n-                exported = getFhirResourceFromBufferReader(resReader, numOfLinesToSkip, fhirResources);\n-            } catch (Exception ioe) {\n-                logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ioe.getMessage());\n-                exported = 0;\n+        } while (retryTimes > 0);\n+\n+        return parseFailures;\n+    }\n+\n+    /**\n+     * @param filePath - file path to the ndjson file.\n+     * @param numOfLinesToSkip - number of lines to skip before read.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param transientUserData - transient user data for the chunk.\n+     * @return - number of parsing failures.\n+     * @throws Exception\n+     */\n+    public static int readFhirResourceFromLocalFile(String filePath, int numOfLinesToSkip, List<Resource> fhirResources,\n+            ImportTransientUserData transientUserData) throws Exception {\n+        int parseFailures = 0;\n+\n+        try {\n+            if (transientUserData.getBufferReader() == null) {\n+                BufferedReader resReader = Files.newBufferedReader(Paths.get(filePath));\n+                transientUserData.setBufferReader(resReader);\n+                // Skip the already processed lines after opening the input stream for first read.\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), numOfLinesToSkip, fhirResources);\n+            } else {\n+                parseFailures = getFhirResourceFromBufferReader(transientUserData.getBufferReader(), 0, fhirResources);\n             }\n+        } catch (Exception ex) {\n+            // Clean up.\n+            fhirResources.clear();\n+            cleanup4TransientUserData(transientUserData, true);\n+            // Log the error and throw exception to fail the job, the job can be continued from the current checkpoint after the problem is solved.\n+            logger.warning(\"readFhirResourceFromLocalFile: \" + \"Error proccesing file \" + filePath + \" - \" + ex.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b377ad83612ea30e7361c65d3e07f9b83bb398d3"}, "originalPosition": 199}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "author": {"user": {"login": "albertwang-ibm", "name": "Albert(Xu) Wang"}}, "url": "https://github.com/IBM/FHIR/commit/8ef9fdf0b24d2239ba026f0d9b5fe64ec0d79de6", "committedDate": "2020-03-11T14:11:18Z", "message": "issue #765 #768 updates per review comments\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "48eeec8a4bb62a15813109a7c485ebf36350132f", "author": {"user": {"login": "albertwang-ibm", "name": "Albert(Xu) Wang"}}, "url": "https://github.com/IBM/FHIR/commit/48eeec8a4bb62a15813109a7c485ebf36350132f", "committedDate": "2020-03-11T14:19:36Z", "message": "issue #765 update logging\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "900e9ea2cfee8c7b3b577b47b210a317d1536e68", "author": {"user": {"login": "albertwang-ibm", "name": "Albert(Xu) Wang"}}, "url": "https://github.com/IBM/FHIR/commit/900e9ea2cfee8c7b3b577b47b210a317d1536e68", "committedDate": "2020-03-11T15:23:02Z", "message": "issue #765 add line number and file info for parsing error.\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyODczOTUx", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372873951", "createdAt": "2020-03-11T15:33:26Z", "commit": {"oid": "900e9ea2cfee8c7b3b577b47b210a317d1536e68"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c74a610037b7bec59fc7fba4cd612a831c51293", "author": {"user": {"login": "albertwang-ibm", "name": "Albert(Xu) Wang"}}, "url": "https://github.com/IBM/FHIR/commit/9c74a610037b7bec59fc7fba4cd612a831c51293", "committedDate": "2020-03-11T16:16:32Z", "message": "issue #765 reuse thread local COS/S3 client\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyOTUzMjE5", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372953219", "createdAt": "2020-03-11T17:02:17Z", "commit": {"oid": "9c74a610037b7bec59fc7fba4cd612a831c51293"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzowMjoxN1rOF1ARqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzowMjoxN1rOF1ARqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEyMzM3MA==", "bodyText": "why not log the stack trace?  might it have some useful info for figuring out what is invalid about the resource that failed to parse?", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391123370", "createdAt": "2020-03-11T17:02:17Z", "author": {"login": "lmsurpre"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -134,112 +137,181 @@ public static void listBuckets(AmazonS3 cosClient) {\n         }\n     }\n \n-    private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfLinesToSkip, List<Resource> fhirResources) throws Exception {\n+    /**\n+     * @param resReader - the buffer reader to read FHIR resource from.\n+     * @param numOfProcessedLines - number of the already processed lines.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param isSkipProcessed - if need to skip the processed lines before read.\n+     * @return - the number of parsing failures.\n+     * @throws Exception\n+     */\n+    private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfProcessedLines, List<Resource> fhirResources,\n+            boolean isSkipProcessed, String dataSource) throws Exception {\n         int exported = 0;\n         int lineRed = 0;\n+        int parseFailures = 0;\n+\n         String resLine = null;\n         do {\n             resLine = resReader.readLine();\n             if (resLine != null) {\n                 lineRed++;\n-                if (lineRed <= numOfLinesToSkip) {\n+                if (isSkipProcessed && lineRed <= numOfProcessedLines) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c74a610037b7bec59fc7fba4cd612a831c51293"}, "originalPosition": 61}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyOTU1MDg1", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372955085", "createdAt": "2020-03-11T17:04:32Z", "commit": {"oid": "9c74a610037b7bec59fc7fba4cd612a831c51293"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzowNDozMlrOF1AXMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzowNDozMlrOF1AXMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEyNDc4Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                // Log and skip the invalid FHIR resource.\n          \n          \n            \n                                logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n          \n          \n            \n                                parseFailures++;\n          \n          \n            \n                                logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse line \"\n          \n          \n            \n                                        + (numOfProcessedLines + exported + parseFailures) + \" of [\" + dataSource + \"].\");\n          \n          \n            \n                                // Log and skip the invalid FHIR resource.\n          \n          \n            \n                                parseFailures++;\n          \n          \n            \n                                logger.log(Level.INFO, \"getFhirResourceFromBufferReader: \" + \"Failed to parse line \"\n          \n          \n            \n                                        + (numOfProcessedLines + exported + parseFailures) + \" of [\" + dataSource + \"].\", e);", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391124787", "createdAt": "2020-03-11T17:04:32Z", "author": {"login": "lmsurpre"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java", "diffHunk": "@@ -134,112 +137,181 @@ public static void listBuckets(AmazonS3 cosClient) {\n         }\n     }\n \n-    private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfLinesToSkip, List<Resource> fhirResources) throws Exception {\n+    /**\n+     * @param resReader - the buffer reader to read FHIR resource from.\n+     * @param numOfProcessedLines - number of the already processed lines.\n+     * @param fhirResources - List holds the FHIR resources.\n+     * @param isSkipProcessed - if need to skip the processed lines before read.\n+     * @return - the number of parsing failures.\n+     * @throws Exception\n+     */\n+    private static int getFhirResourceFromBufferReader(BufferedReader resReader, int numOfProcessedLines, List<Resource> fhirResources,\n+            boolean isSkipProcessed, String dataSource) throws Exception {\n         int exported = 0;\n         int lineRed = 0;\n+        int parseFailures = 0;\n+\n         String resLine = null;\n         do {\n             resLine = resReader.readLine();\n             if (resLine != null) {\n                 lineRed++;\n-                if (lineRed <= numOfLinesToSkip) {\n+                if (isSkipProcessed && lineRed <= numOfProcessedLines) {\n                     continue;\n                 }\n-                fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n-                exported++;\n-                if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n-                    break;\n+                try {\n+                    fhirResources.add(FHIRParser.parser(Format.JSON).parse(new StringReader(resLine)));\n+                    exported++;\n+                    if (exported == Constants.IMPORT_NUMOFFHIRRESOURCES_PERREAD) {\n+                        break;\n+                    }\n+                } catch (FHIRParserException e) {\n+                    // Log and skip the invalid FHIR resource.\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + e.getMessage());\n+                    parseFailures++;\n+                    logger.warning(\"getFhirResourceFromBufferReader: \" + \"Failed to parse line \"\n+                            + (numOfProcessedLines + exported + parseFailures) + \" of [\" + dataSource + \"].\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c74a610037b7bec59fc7fba4cd612a831c51293"}, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyOTY0OTk3", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372964997", "createdAt": "2020-03-11T17:16:42Z", "commit": {"oid": "9c74a610037b7bec59fc7fba4cd612a831c51293"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzoxNjo0M1rOF1A2Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzoxNjo0M1rOF1A2Zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzMjc3NQ==", "bodyText": "maybe turn these comments into javadoc?", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391132775", "createdAt": "2020-03-11T17:16:43Z", "author": {"login": "lmsurpre"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/Constants.java", "diffHunk": "@@ -48,6 +48,11 @@\n \n     // Control if push OperationOutcomes to COS/S3.\n     public static final boolean IMPORT_IS_COLLECT_OPERATIONOUTCOMES = false;\n-    // Control if reuse the input stream of the data source across the chunks.\n-    public static final boolean IMPORT_IS_REUSE_INPUTSTREAM = true;\n+    // Retry times when https or amazon s3 client timeout or other error happens, e.g, timeout can happen if the batch write to DB takes\n+    // longer than the socket timeout, set to retry once for now.\n+    public static final int IMPORT_RETRY_TIMES = 1;\n+    public static final int COS_REQUEST_TIMEOUT = 10000;\n+    // Batch writing to DB can take long time which can make the idle COS/S3 client connection timeout, so set the client socket timeout\n+    // to 120 seconds which is the default DB2 timeout.\n+    public static final int COS_SOCKET_TIMEOUT = 120000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c74a610037b7bec59fc7fba4cd612a831c51293"}, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyOTY4MTUz", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372968153", "createdAt": "2020-03-11T17:20:28Z", "commit": {"oid": "9c74a610037b7bec59fc7fba4cd612a831c51293"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzoyMDoyOFrOF1BAUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzoyMDoyOFrOF1BAUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzNTMxMw==", "bodyText": "Just confirming:  ChunkWriter get re-instantiated for each job, right?", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391135313", "createdAt": "2020-03-11T17:20:28Z", "author": {"login": "lmsurpre"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ChunkWriter.java", "diffHunk": "@@ -38,6 +38,7 @@\n  */\n public class ChunkWriter extends AbstractItemWriter {\n     private static final Logger logger = Logger.getLogger(ChunkWriter.class.getName());\n+    AmazonS3 cosClient = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c74a610037b7bec59fc7fba4cd612a831c51293"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyOTcyMDM2", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372972036", "createdAt": "2020-03-11T17:25:19Z", "commit": {"oid": "9c74a610037b7bec59fc7fba4cd612a831c51293"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzoyNToxOVrOF1BMrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzoyNToxOVrOF1BMrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzODQ3Ng==", "bodyText": "which warning is being suppressed?  can we avoid it?", "url": "https://github.com/IBM/FHIR/pull/770#discussion_r391138476", "createdAt": "2020-03-11T17:25:19Z", "author": {"login": "lmsurpre"}, "path": "fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkimport/ImportJobListener.java", "diffHunk": "@@ -8,29 +8,46 @@\n \n import java.util.HashMap;\n import java.util.List;\n+import java.util.logging.Logger;\n \n import javax.batch.api.listener.JobListener;\n+import javax.batch.operations.JobOperator;\n+import javax.batch.runtime.BatchRuntime;\n+import javax.batch.runtime.JobExecution;\n import javax.batch.runtime.context.JobContext;\n import javax.inject.Inject;\n \n public class ImportJobListener implements JobListener {\n+    private static final Logger logger = Logger.getLogger(ImportJobListener.class.getName());\n     @Inject\n     JobContext jobContext;\n \n-    private long jobStartTimeInMS, jobEndTimeInMS;\n-\n     public ImportJobListener() {\n \n     }\n \n-    @SuppressWarnings(\"unchecked\")\n+\n+    @SuppressWarnings({\"unchecked\" })", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9c74a610037b7bec59fc7fba4cd612a831c51293"}, "originalPosition": 26}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "39e7fb85ee8a5f1d88bdaac01fb64921b5abd2bd", "author": {"user": {"login": "albertwang-ibm", "name": "Albert(Xu) Wang"}}, "url": "https://github.com/IBM/FHIR/commit/39e7fb85ee8a5f1d88bdaac01fb64921b5abd2bd", "committedDate": "2020-03-11T17:26:38Z", "message": "Update fhir-bulkimportexport-webapp/src/main/java/com/ibm/fhir/bulkcommon/BulkDataUtils.java\n\nCo-Authored-By: Lee Surprenant <lmsurpre@us.ibm.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyOTc0NTI1", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372974525", "createdAt": "2020-03-11T17:28:22Z", "commit": {"oid": "39e7fb85ee8a5f1d88bdaac01fb64921b5abd2bd"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "16f960cf476556ccb0998fcb24c7632616d28d9a", "author": {"user": {"login": "albertwang-ibm", "name": "Albert(Xu) Wang"}}, "url": "https://github.com/IBM/FHIR/commit/16f960cf476556ccb0998fcb24c7632616d28d9a", "committedDate": "2020-03-11T17:38:39Z", "message": "issue #765 update to logging per review comments\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "05203d7b61438d06d745079822ea59b4d262d3ca", "author": {"user": {"login": "albertwang-ibm", "name": "Albert(Xu) Wang"}}, "url": "https://github.com/IBM/FHIR/commit/05203d7b61438d06d745079822ea59b4d262d3ca", "committedDate": "2020-03-11T17:44:10Z", "message": "issue #765 reduce supresswarning scope per review\n\nSigned-off-by: Albert Wang <xuwang@us.ibm.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyOTkyNjkx", "url": "https://github.com/IBM/FHIR/pull/770#pullrequestreview-372992691", "createdAt": "2020-03-11T17:51:49Z", "commit": {"oid": "05203d7b61438d06d745079822ea59b4d262d3ca"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 504, "cost": 1, "resetAt": "2021-11-01T14:51:55Z"}}}