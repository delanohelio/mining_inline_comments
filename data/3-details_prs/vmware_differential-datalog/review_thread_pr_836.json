{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMxMzU0OTk2", "number": 836, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwNzo1NTowNVrOFCyBoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwODozNjo0NlrOFCzHjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NDYxMDg4OnYy", "diffSide": "RIGHT", "path": "rust/template/differential_datalog/program/worker.rs", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwNzo1NTowNVrOICG-Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxNDo0NDoxNVrOICYMGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA4MjI3NA==", "bodyText": "Why is relaxed consistency sufficient?", "url": "https://github.com/vmware/differential-datalog/pull/836#discussion_r539082274", "createdAt": "2020-12-09T07:55:05Z", "author": {"login": "ryzhyk"}, "path": "rust/template/differential_datalog/program/worker.rs", "diffHunk": "@@ -0,0 +1,853 @@\n+use crate::{\n+    ddval::DDValue,\n+    profile::{get_prof_context, with_prof_context, ProfMsg},\n+    program::{\n+        arrange::{ArrangedCollection, Arrangements},\n+        concatenate_collections,\n+        timestamp::TSAtomic,\n+        ArrId, Dep, Msg, ProgNode, Program, Reply, Update, TS,\n+    },\n+    program::{RelId, Weight},\n+    variable::Variable,\n+};\n+use differential_dataflow::{\n+    input::{Input, InputSession},\n+    logging::DifferentialEvent,\n+    operators::{arrange::TraceAgent, Consolidate, ThresholdTotal},\n+    trace::{\n+        implementations::{ord::OrdValBatch, spine_fueled::Spine},\n+        BatchReader, Cursor, TraceReader,\n+    },\n+    Collection,\n+};\n+use fnv::{FnvBuildHasher, FnvHashMap};\n+use std::{\n+    collections::{BTreeMap, BTreeSet, HashMap},\n+    mem,\n+    rc::Rc,\n+    sync::{\n+        atomic::{AtomicBool, Ordering},\n+        mpsc::SyncSender,\n+        mpsc::{Receiver, Sender, TryRecvError},\n+        Arc, Barrier, Mutex,\n+    },\n+    thread::{self, Thread},\n+    time::Duration,\n+};\n+use timely::{\n+    communication::Allocator,\n+    dataflow::{operators::probe::Handle as ProbeHandle, scopes::Child, Scope},\n+    logging::TimelyEvent,\n+    worker::Worker,\n+};\n+\n+type SessionData = (\n+    FnvHashMap<RelId, InputSession<TS, DDValue, Weight>>,\n+    BTreeMap<\n+        ArrId,\n+        TraceAgent<\n+            Spine<DDValue, DDValue, u32, i32, Rc<OrdValBatch<DDValue, DDValue, u32, i32, u32>>>,\n+        >,\n+    >,\n+);\n+\n+/// A DDlog timely worker\n+pub struct DDlogWorker<'a> {\n+    /// The timely worker instance\n+    worker: &'a mut Worker<Allocator>,\n+    /// The program this worker is executing\n+    program: Arc<Program>,\n+    /// The atomically synchronized timestamp for the transaction\n+    /// frontier\n+    frontier_timestamp: &'a TSAtomic,\n+    /// Peer workers' thread handles, only used by worker 0.\n+    peers: FnvHashMap<usize, Thread>,\n+    /// The progress barrier used for transactions\n+    progress_barrier: Arc<Barrier>,\n+    /// Information on which metrics are enabled and a\n+    /// channel for sending profiling data\n+    profiling: ProfilingData,\n+    /// The current worker's receiver for receiving messages\n+    request_receiver: Receiver<Msg>,\n+    /// The current worker's sender for sending messages\n+    reply_sender: Sender<Reply>,\n+}\n+\n+impl<'a> DDlogWorker<'a> {\n+    /// Create a new ddlog timely worker\n+    #[allow(clippy::too_many_arguments)]\n+    pub(super) fn new(\n+        worker: &'a mut Worker<Allocator>,\n+        program: Arc<Program>,\n+        frontier_timestamp: &'a TSAtomic,\n+        num_workers: usize,\n+        progress_barrier: Arc<Barrier>,\n+        profiling: ProfilingData,\n+        request_receivers: Arc<Mutex<Vec<Option<Receiver<Msg>>>>>,\n+        reply_senders: Arc<Mutex<Vec<Option<Sender<Reply>>>>>,\n+        thread_handle_sender: SyncSender<(usize, Thread)>,\n+        thread_handle_receiver: Arc<Mutex<Receiver<(usize, Thread)>>>,\n+    ) -> Self {\n+        let worker_index = worker.index();\n+\n+        // The hashmap that will contain all worker thread handles\n+        let mut peers: FnvHashMap<usize, Thread> = HashMap::with_capacity_and_hasher(\n+            // Only worker zero will populate this, so all others can create an empty map\n+            if worker_index == 0 { num_workers } else { 0 },\n+            FnvBuildHasher::default(),\n+        );\n+\n+        // If this is worker zero, receive all other worker's thread handles and\n+        // populate the `peers` map with them\n+        if worker_index == 0 {\n+            let thread_handle_recv = thread_handle_receiver\n+                .lock()\n+                .expect(\"failed to lock thread handle receiver\");\n+\n+            for _ in 0..num_workers - 1 {\n+                let (worker, thread_handle) = thread_handle_recv\n+                    .recv()\n+                    .expect(\"failed to receive thread handle from timely worker\");\n+\n+                peers.insert(worker, thread_handle);\n+            }\n+\n+        // Send other all worker's thread handles to worker 0.\n+        } else {\n+            thread_handle_sender\n+                .send((worker_index, thread::current()))\n+                .expect(\"failed to send thread handle for a timely worker\");\n+        }\n+\n+        // Get the request receiver for the current worker\n+        let request_receiver = mem::take(&mut request_receivers.lock().unwrap()[worker_index])\n+            .expect(\"failed to get request receiver for a timely worker\");\n+\n+        // Get the reply sender for the current worker\n+        let reply_sender = mem::take(&mut reply_senders.lock().unwrap()[worker_index])\n+            .expect(\"failed to get reply sender for a timely worker\");\n+\n+        Self {\n+            worker,\n+            program,\n+            frontier_timestamp,\n+            peers,\n+            progress_barrier,\n+            profiling,\n+            request_receiver,\n+            reply_sender,\n+        }\n+    }\n+\n+    /// Returns whether or not the current worker is the leader, worker 0\n+    pub fn is_leader(&self) -> bool {\n+        self.worker_index() == 0\n+    }\n+\n+    /// Get the index of the current worker\n+    pub fn worker_index(&self) -> usize {\n+        self.worker.index()\n+    }\n+\n+    /// Set the current transaction frontier's timestamp\n+    fn set_frontier_timestamp(&self, timestamp: TS) {\n+        self.frontier_timestamp.store(timestamp, Ordering::Relaxed);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85c3c2a377ab52b9c39b6e923fc4e688a941f48c"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTM2NDM3OA==", "bodyText": "I actually changed these to Acquire/Release, I'll push the changes later today", "url": "https://github.com/vmware/differential-datalog/pull/836#discussion_r539364378", "createdAt": "2020-12-09T14:44:15Z", "author": {"login": "Kixiron"}, "path": "rust/template/differential_datalog/program/worker.rs", "diffHunk": "@@ -0,0 +1,853 @@\n+use crate::{\n+    ddval::DDValue,\n+    profile::{get_prof_context, with_prof_context, ProfMsg},\n+    program::{\n+        arrange::{ArrangedCollection, Arrangements},\n+        concatenate_collections,\n+        timestamp::TSAtomic,\n+        ArrId, Dep, Msg, ProgNode, Program, Reply, Update, TS,\n+    },\n+    program::{RelId, Weight},\n+    variable::Variable,\n+};\n+use differential_dataflow::{\n+    input::{Input, InputSession},\n+    logging::DifferentialEvent,\n+    operators::{arrange::TraceAgent, Consolidate, ThresholdTotal},\n+    trace::{\n+        implementations::{ord::OrdValBatch, spine_fueled::Spine},\n+        BatchReader, Cursor, TraceReader,\n+    },\n+    Collection,\n+};\n+use fnv::{FnvBuildHasher, FnvHashMap};\n+use std::{\n+    collections::{BTreeMap, BTreeSet, HashMap},\n+    mem,\n+    rc::Rc,\n+    sync::{\n+        atomic::{AtomicBool, Ordering},\n+        mpsc::SyncSender,\n+        mpsc::{Receiver, Sender, TryRecvError},\n+        Arc, Barrier, Mutex,\n+    },\n+    thread::{self, Thread},\n+    time::Duration,\n+};\n+use timely::{\n+    communication::Allocator,\n+    dataflow::{operators::probe::Handle as ProbeHandle, scopes::Child, Scope},\n+    logging::TimelyEvent,\n+    worker::Worker,\n+};\n+\n+type SessionData = (\n+    FnvHashMap<RelId, InputSession<TS, DDValue, Weight>>,\n+    BTreeMap<\n+        ArrId,\n+        TraceAgent<\n+            Spine<DDValue, DDValue, u32, i32, Rc<OrdValBatch<DDValue, DDValue, u32, i32, u32>>>,\n+        >,\n+    >,\n+);\n+\n+/// A DDlog timely worker\n+pub struct DDlogWorker<'a> {\n+    /// The timely worker instance\n+    worker: &'a mut Worker<Allocator>,\n+    /// The program this worker is executing\n+    program: Arc<Program>,\n+    /// The atomically synchronized timestamp for the transaction\n+    /// frontier\n+    frontier_timestamp: &'a TSAtomic,\n+    /// Peer workers' thread handles, only used by worker 0.\n+    peers: FnvHashMap<usize, Thread>,\n+    /// The progress barrier used for transactions\n+    progress_barrier: Arc<Barrier>,\n+    /// Information on which metrics are enabled and a\n+    /// channel for sending profiling data\n+    profiling: ProfilingData,\n+    /// The current worker's receiver for receiving messages\n+    request_receiver: Receiver<Msg>,\n+    /// The current worker's sender for sending messages\n+    reply_sender: Sender<Reply>,\n+}\n+\n+impl<'a> DDlogWorker<'a> {\n+    /// Create a new ddlog timely worker\n+    #[allow(clippy::too_many_arguments)]\n+    pub(super) fn new(\n+        worker: &'a mut Worker<Allocator>,\n+        program: Arc<Program>,\n+        frontier_timestamp: &'a TSAtomic,\n+        num_workers: usize,\n+        progress_barrier: Arc<Barrier>,\n+        profiling: ProfilingData,\n+        request_receivers: Arc<Mutex<Vec<Option<Receiver<Msg>>>>>,\n+        reply_senders: Arc<Mutex<Vec<Option<Sender<Reply>>>>>,\n+        thread_handle_sender: SyncSender<(usize, Thread)>,\n+        thread_handle_receiver: Arc<Mutex<Receiver<(usize, Thread)>>>,\n+    ) -> Self {\n+        let worker_index = worker.index();\n+\n+        // The hashmap that will contain all worker thread handles\n+        let mut peers: FnvHashMap<usize, Thread> = HashMap::with_capacity_and_hasher(\n+            // Only worker zero will populate this, so all others can create an empty map\n+            if worker_index == 0 { num_workers } else { 0 },\n+            FnvBuildHasher::default(),\n+        );\n+\n+        // If this is worker zero, receive all other worker's thread handles and\n+        // populate the `peers` map with them\n+        if worker_index == 0 {\n+            let thread_handle_recv = thread_handle_receiver\n+                .lock()\n+                .expect(\"failed to lock thread handle receiver\");\n+\n+            for _ in 0..num_workers - 1 {\n+                let (worker, thread_handle) = thread_handle_recv\n+                    .recv()\n+                    .expect(\"failed to receive thread handle from timely worker\");\n+\n+                peers.insert(worker, thread_handle);\n+            }\n+\n+        // Send other all worker's thread handles to worker 0.\n+        } else {\n+            thread_handle_sender\n+                .send((worker_index, thread::current()))\n+                .expect(\"failed to send thread handle for a timely worker\");\n+        }\n+\n+        // Get the request receiver for the current worker\n+        let request_receiver = mem::take(&mut request_receivers.lock().unwrap()[worker_index])\n+            .expect(\"failed to get request receiver for a timely worker\");\n+\n+        // Get the reply sender for the current worker\n+        let reply_sender = mem::take(&mut reply_senders.lock().unwrap()[worker_index])\n+            .expect(\"failed to get reply sender for a timely worker\");\n+\n+        Self {\n+            worker,\n+            program,\n+            frontier_timestamp,\n+            peers,\n+            progress_barrier,\n+            profiling,\n+            request_receiver,\n+            reply_sender,\n+        }\n+    }\n+\n+    /// Returns whether or not the current worker is the leader, worker 0\n+    pub fn is_leader(&self) -> bool {\n+        self.worker_index() == 0\n+    }\n+\n+    /// Get the index of the current worker\n+    pub fn worker_index(&self) -> usize {\n+        self.worker.index()\n+    }\n+\n+    /// Set the current transaction frontier's timestamp\n+    fn set_frontier_timestamp(&self, timestamp: TS) {\n+        self.frontier_timestamp.store(timestamp, Ordering::Relaxed);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA4MjI3NA=="}, "originalCommit": {"oid": "85c3c2a377ab52b9c39b6e923fc4e688a941f48c"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NDc4OTg4OnYy", "diffSide": "RIGHT", "path": "rust/template/differential_datalog/program/mod.rs", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwODozNjo0NlrOICIlTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxNjoxNTo0NVrOICdBoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTEwODY4NA==", "bodyText": "Looks like this causes a couple of problems.  First, HDDlog is no longer Send, which breaks the server_api test: https://gitlab.com/ddlog/differential-datalog-kixiron/-/jobs/887145084\n(This has been fixed in the latest timely: TimelyDataflow/timely-dataflow#296, but we are still using an older version from crates.io).\nSecond, removing the timely thread seems to have altered the error handling path causing the nested panic error in another test: https://gitlab.com/ddlog/differential-datalog-kixiron/-/jobs/887145073\nI need more time to investigate these issues, i.e., find a workaround for the former and debug the latter.", "url": "https://github.com/vmware/differential-datalog/pull/836#discussion_r539108684", "createdAt": "2020-12-09T08:36:46Z", "author": {"login": "ryzhyk"}, "path": "rust/template/differential_datalog/program/mod.rs", "diffHunk": "@@ -640,25 +637,22 @@ pub struct RunningProgram {\n     /// Producer sides of channels used to send commands to workers.\n     /// We use async channels to avoid deadlocks when workers are blocked\n     /// in `step_or_park`.\n-    senders: Vec<mpsc::Sender<Msg>>,\n+    senders: Vec<Sender<Msg>>,\n     /// Channels to receive replies from worker threads. We could use a single\n     /// channel with multiple senders, but use many channels instead to avoid\n     /// deadlocks when one of the workers has died, but `recv` blocks instead\n     /// of failing, since the channel is still considered alive.\n-    reply_recv: Vec<mpsc::Receiver<Reply>>,\n+    reply_recv: Vec<Receiver<Reply>>,\n     relations: FnvHashMap<RelId, RelationInstance>,\n-    /// Join handle of the thread running timely computation.\n-    thread_handle: Option<thread::JoinHandle<Result<(), String>>>,\n-    /// Timely worker threads.\n-    worker_threads: Vec<thread::Thread>,\n+    worker_guards: Option<WorkerGuards<Result<(), String>>>,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85c3c2a377ab52b9c39b6e923fc4e688a941f48c"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTM2NjM4Mg==", "bodyText": "The former can be fixed with an unsafe impl Send for HDDlog {} if you're comfortable with it, the latter is more tricky though", "url": "https://github.com/vmware/differential-datalog/pull/836#discussion_r539366382", "createdAt": "2020-12-09T14:46:37Z", "author": {"login": "Kixiron"}, "path": "rust/template/differential_datalog/program/mod.rs", "diffHunk": "@@ -640,25 +637,22 @@ pub struct RunningProgram {\n     /// Producer sides of channels used to send commands to workers.\n     /// We use async channels to avoid deadlocks when workers are blocked\n     /// in `step_or_park`.\n-    senders: Vec<mpsc::Sender<Msg>>,\n+    senders: Vec<Sender<Msg>>,\n     /// Channels to receive replies from worker threads. We could use a single\n     /// channel with multiple senders, but use many channels instead to avoid\n     /// deadlocks when one of the workers has died, but `recv` blocks instead\n     /// of failing, since the channel is still considered alive.\n-    reply_recv: Vec<mpsc::Receiver<Reply>>,\n+    reply_recv: Vec<Receiver<Reply>>,\n     relations: FnvHashMap<RelId, RelationInstance>,\n-    /// Join handle of the thread running timely computation.\n-    thread_handle: Option<thread::JoinHandle<Result<(), String>>>,\n-    /// Timely worker threads.\n-    worker_threads: Vec<thread::Thread>,\n+    worker_guards: Option<WorkerGuards<Result<(), String>>>,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTEwODY4NA=="}, "originalCommit": {"oid": "85c3c2a377ab52b9c39b6e923fc4e688a941f48c"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ0MzYxNw==", "bodyText": "I think unsafe impl is ok.\nI haven't looked closely into the second issue yet. I suspect it's just a matter of more careful error handling now that we don't have a separate thread to catch panic's.", "url": "https://github.com/vmware/differential-datalog/pull/836#discussion_r539443617", "createdAt": "2020-12-09T16:15:45Z", "author": {"login": "ryzhyk"}, "path": "rust/template/differential_datalog/program/mod.rs", "diffHunk": "@@ -640,25 +637,22 @@ pub struct RunningProgram {\n     /// Producer sides of channels used to send commands to workers.\n     /// We use async channels to avoid deadlocks when workers are blocked\n     /// in `step_or_park`.\n-    senders: Vec<mpsc::Sender<Msg>>,\n+    senders: Vec<Sender<Msg>>,\n     /// Channels to receive replies from worker threads. We could use a single\n     /// channel with multiple senders, but use many channels instead to avoid\n     /// deadlocks when one of the workers has died, but `recv` blocks instead\n     /// of failing, since the channel is still considered alive.\n-    reply_recv: Vec<mpsc::Receiver<Reply>>,\n+    reply_recv: Vec<Receiver<Reply>>,\n     relations: FnvHashMap<RelId, RelationInstance>,\n-    /// Join handle of the thread running timely computation.\n-    thread_handle: Option<thread::JoinHandle<Result<(), String>>>,\n-    /// Timely worker threads.\n-    worker_threads: Vec<thread::Thread>,\n+    worker_guards: Option<WorkerGuards<Result<(), String>>>,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTEwODY4NA=="}, "originalCommit": {"oid": "85c3c2a377ab52b9c39b6e923fc4e688a941f48c"}, "originalPosition": 77}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4686, "cost": 1, "resetAt": "2021-11-13T14:23:39Z"}}}