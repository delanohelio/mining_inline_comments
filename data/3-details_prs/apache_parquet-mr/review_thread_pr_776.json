{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk2ODE2Njg2", "number": 776, "reviewThreads": {"totalCount": 38, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzo0MzowOVrOD2bHDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxNDowNzo1NVrOD-nEig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4MzkzODY5OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzo0MzowOVrOGMLzKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxNDowNzoxM1rOGY0KuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQyOTQxNg==", "bodyText": "Since this is a public method, can we validate pageAAD also?", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r415429416", "createdAt": "2020-04-26T23:43:09Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -90,6 +102,10 @@\n \n   // Update last two bytes with new page ordinal (instead of creating new page AAD from scratch)\n   public static void quickUpdatePageAAD(byte[] pageAAD, short newPageOrdinal) {\n+    if (newPageOrdinal < 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTYxNzg3Mw==", "bodyText": "Ok, I'll add this.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r415617873", "createdAt": "2020-04-27T08:32:57Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -90,6 +102,10 @@\n \n   // Update last two bytes with new page ordinal (instead of creating new page AAD from scratch)\n   public static void quickUpdatePageAAD(byte[] pageAAD, short newPageOrdinal) {\n+    if (newPageOrdinal < 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQyOTQxNg=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY3MzcyMQ==", "bodyText": "I agree on validating the arguments is important. But the proper exception to be thrown for a null is a NullPointerException that would be thrown at line 134 anyway. If you really want to validate the argument for null at the first line of the method I would suggest using java.util.Objects.requireNonNull(Object).", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428673721", "createdAt": "2020-05-21T14:07:13Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -90,6 +102,10 @@\n \n   // Update last two bytes with new page ordinal (instead of creating new page AAD from scratch)\n   public static void quickUpdatePageAAD(byte[] pageAAD, short newPageOrdinal) {\n+    if (newPageOrdinal < 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQyOTQxNg=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NDA2NjUyOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMDo1NzoyMlrOGMMubg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNTo1NTozNVrOGcioTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NDU5MA==", "bodyText": "Should we keep original addRowGroup() intact and just add a new one with InternalFileEncryptor to isolate the change's impact? There is not much duplicate code if doing so and we can refactor the existing code with helper functions.  In the majority use cases, they are non-encryption cases.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r415444590", "createdAt": "2020-04-27T00:57:22Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -463,14 +486,29 @@ ConvertedType convertToConvertedType(LogicalTypeAnnotation logicalTypeAnnotation\n     }\n   }\n \n-  private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block) {\n+  private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTYyMTM5OA==", "bodyText": "encryption is isolated there, with if (null != fileEncryptor)  and if (encryptMetaData) - similar to the if (columnIndexRef != null)  and if (offsetIndexRef != null) in the same function.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r415621398", "createdAt": "2020-04-27T08:38:12Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -463,14 +486,29 @@ ConvertedType convertToConvertedType(LogicalTypeAnnotation logicalTypeAnnotation\n     }\n   }\n \n-  private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block) {\n+  private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NDU5MA=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU3NzM2OA==", "bodyText": "Not all the changes are isolated. Generally, adding 'if/else' will add diverge the code and add the complexity. One other thing is regression thinking. If fileEncryptor is null, which would be most of the case, then it just executes the existing method without change. It would be less error prone.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r422577368", "createdAt": "2020-05-10T03:16:54Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -463,14 +486,29 @@ ConvertedType convertToConvertedType(LogicalTypeAnnotation logicalTypeAnnotation\n     }\n   }\n \n-  private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block) {\n+  private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NDU5MA=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzUwNzMzMg==", "bodyText": "Many functions can be run either with or without encryption. Duplicating them will result in hundreds or thousands of duplicate code lines. This will make code maintenance (changes/fixes)  a headache. Instead, we isolate encryption with if switches, without duplicating the existing code.\nThe same goes for other recent new features (column indexes and bloom filters) - they are isolated with an if switch, instead of code duplication. See if (columnIndexRef != null) and if (offsetIndexRef != null) in this addRowGroup function.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r423507332", "createdAt": "2020-05-12T07:05:58Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -463,14 +486,29 @@ ConvertedType convertToConvertedType(LogicalTypeAnnotation logicalTypeAnnotation\n     }\n   }\n \n-  private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block) {\n+  private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NDU5MA=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4MDY4NQ==", "bodyText": "It is not a blocking comment and I am fine with it. But generally speaking, adding too much nested if/else diverges the code path and causes the complexity for reading. One way to avoid duplicating is to wrap them up in helper functions. I understand column indexes and bloom filters already did that but if we keep adding features like this, the code will become less and less readable.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r432580685", "createdAt": "2020-05-29T15:55:35Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -463,14 +486,29 @@ ConvertedType convertToConvertedType(LogicalTypeAnnotation logicalTypeAnnotation\n     }\n   }\n \n-  private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block) {\n+  private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NDU5MA=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU4NDA3OTg3OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMTowNDo0OFrOGMM0yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNDowNToxMVrOGXiCPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NjIxNg==", "bodyText": "Move comments up", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r415446216", "createdAt": "2020-04-27T01:04:48Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTYyMjg0NA==", "bodyText": "is there a requirement in the code formatting rules in this community to keep comments in separate lines?", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r415622844", "createdAt": "2020-04-27T08:40:18Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NjIxNg=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU3NTMyNg==", "bodyText": "No. I see most of them on up line but a few on the same line. It is not a must.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r422575326", "createdAt": "2020-05-10T02:49:52Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NjIxNg=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyODA2MA==", "bodyText": "Actually, based on parquet-mr README:\n\nGenerally speaking, stick to the Sun Java Code Conventions\n\nBased on the related section both should be fine.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427328060", "createdAt": "2020-05-19T14:05:11Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NjIxNg=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 286}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTE2MzczOnYy", "diffSide": "RIGHT", "path": "parquet-column/src/main/java/org/apache/parquet/internal/column/columnindex/OffsetIndex.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzowMzo1MlrOGXfYLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwODo1MDozMFrOGYBoww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI4NDUyNw==", "bodyText": "Why do we need it as a short instead of keeping it as an int? As per the parquet.thrift spec we never say that we cannot have more pages than 32767 even if it is unlikely to have such many.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427284527", "createdAt": "2020-05-19T13:03:52Z", "author": {"login": "gszadovszky"}, "path": "parquet-column/src/main/java/org/apache/parquet/internal/column/columnindex/OffsetIndex.java", "diffHunk": "@@ -49,6 +49,13 @@\n    * @return the index of the first row in the page\n    */\n   public long getFirstRowIndex(int pageIndex);\n+  \n+  /**\n+   * @param pageIndex\n+   *         the index of the page\n+   * @return the original ordinal of the page in the column chunk\n+   */\n+  public short getPageOrdinal(int pageIndex);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc0NTU3Ng==", "bodyText": "The background discussion is here,\n#776 (comment)\nIn the case of pages, encryption becomes an order (or two orders) of magnitude slower if the pages are small. Basically, the hardware acceleration does not kick in with small pages (and there are additional problems). This is another reason not to allow more than 32K pages in a chunk.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427745576", "createdAt": "2020-05-20T05:14:01Z", "author": {"login": "ggershinsky"}, "path": "parquet-column/src/main/java/org/apache/parquet/internal/column/columnindex/OffsetIndex.java", "diffHunk": "@@ -49,6 +49,13 @@\n    * @return the index of the first row in the page\n    */\n   public long getFirstRowIndex(int pageIndex);\n+  \n+  /**\n+   * @param pageIndex\n+   *         the index of the page\n+   * @return the original ordinal of the page in the column chunk\n+   */\n+  public short getPageOrdinal(int pageIndex);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI4NDUyNw=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzg0NTgyNw==", "bodyText": "Plus - the page headers are also encrypted. These are small, so the hardware acceleration is not applied on them. Having dozens/hundreds of thousands (or more) of page headers will significantly affect the overall encryption time of a file.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427845827", "createdAt": "2020-05-20T08:50:30Z", "author": {"login": "ggershinsky"}, "path": "parquet-column/src/main/java/org/apache/parquet/internal/column/columnindex/OffsetIndex.java", "diffHunk": "@@ -49,6 +49,13 @@\n    * @return the index of the first row in the page\n    */\n   public long getFirstRowIndex(int pageIndex);\n+  \n+  /**\n+   * @param pageIndex\n+   *         the index of the page\n+   * @return the original ordinal of the page in the column chunk\n+   */\n+  public short getPageOrdinal(int pageIndex);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI4NDUyNw=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTIyMzkwOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "isResolved": true, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzoxODoxMlrOGXf-Yw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMToyNToxOFrOGYHCig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5NDMwNw==", "bodyText": "Theoretically we don't give hard limits for the number of row groups, number of columns or the number of pages in the spec. There is a de facto limit that we use thrift lists where the size is an i32 meaning that we should allow java int values here.\nAlso, there was a post commit discussion in a related PR. It is unfortunate that that time parquet-format was already released so I don't know if there is a way to properly fix this issue in the format. Anyway, I would not restrict these values to a short.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427294307", "createdAt": "2020-05-19T13:18:12Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -68,19 +67,32 @@\n \n   public static byte[] createModuleAAD(byte[] fileAAD, ModuleType moduleType, \n       short rowGroupOrdinal, short columnOrdinal, short pageOrdinal) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc0Mzg2MQ==", "bodyText": "The links to the discussion on this,\napache/parquet-format#114 (comment)\napache/parquet-format#114 (comment)\nhttp://mail-archives.apache.org/mod_mbox/parquet-dev/201901.mbox/%3CCAO4re1kM4xGMNT4CGrjvA43t-QgUmUwLMskTJfd8ivgCfF8rSw%40mail.gmail.com%3E\nThe parquet-cpp approach to this is to allow for any number of row groups in files without encryption, and to limit it to 32K in encrypted files,\napache/arrow@0c5168c\n\"While writing files with so many row groups is a bad idea, people will still do it... This .. enables reading the many-row-group files again. Files with encrypted row group metadata with that many row groups cannot be read\"", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427743861", "createdAt": "2020-05-20T05:07:46Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -68,19 +67,32 @@\n \n   public static byte[] createModuleAAD(byte[] fileAAD, ModuleType moduleType, \n       short rowGroupOrdinal, short columnOrdinal, short pageOrdinal) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5NDMwNw=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzg0MzI5MQ==", "bodyText": "Also - like with the page numbers in the previous comment, having too many row groups will adversely affect encryption performance. There are per-rowgroup encryption operations, always performed on small  buffers - therefore, very slow (no hardware acceleration, etc). Having dozens/hundreds of thousands (or more) of them will significantly affect the overall encryption time of a file. Moreover, having lots of row groups might lead to having smaller data pages, which decreases the performance further.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427843291", "createdAt": "2020-05-20T08:46:52Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -68,19 +67,32 @@\n \n   public static byte[] createModuleAAD(byte[] fileAAD, ModuleType moduleType, \n       short rowGroupOrdinal, short columnOrdinal, short pageOrdinal) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5NDMwNw=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkyMjMxMg==", "bodyText": "Still, the variables in this function (and elsewhere) don't have to be short. After looking at the code, it seems ints are better suited for managing and checking these parameters (and for enabling any values in unencrypted files). I'll make this change.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427922312", "createdAt": "2020-05-20T11:01:31Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -68,19 +67,32 @@\n \n   public static byte[] createModuleAAD(byte[] fileAAD, ModuleType moduleType, \n       short rowGroupOrdinal, short columnOrdinal, short pageOrdinal) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5NDMwNw=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkyNTExMQ==", "bodyText": "I understand that large numbers of pages/row groups or columns would lead to significant performance drawbacks but it should not limit what the spec allows otherwise.\nSince it is discussed and approved already, I am fine with using short values for these. What I would suggest adding though is to have the conversion from int to short centralized and and have specific error messages so it is clear that the limit reached is a hard limit for the encryption feature. Also, if we will publish any description/example for the encryption feature these limitations shall be listed there.\nOne more thing: the check of intValue > Short.MAX_VALUE is not complete. In case of intValue is negative the cast may result in a valid positive short value. I would suggest using (short) intValue != intValue instead.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427925111", "createdAt": "2020-05-20T11:07:02Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -68,19 +67,32 @@\n \n   public static byte[] createModuleAAD(byte[] fileAAD, ModuleType moduleType, \n       short rowGroupOrdinal, short columnOrdinal, short pageOrdinal) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5NDMwNw=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkzMjQ3Ng==", "bodyText": "Thanks for the value checking tip, I'll update the code to use it. As for a centralization - I think this function (createModuleAAD) is the right place. In the encryption feature, ordinals are used only for integrity verification - performed via AADs, which are calculated here for both encryption and decryption. Everywhere in the code, the ordinals will be an int. Since the createModuleAAD is called only for encrypted files, an exception will be thrown only for them (if an ordinal exceeds the Short.MAX_VALUE).", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427932476", "createdAt": "2020-05-20T11:21:26Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -68,19 +67,32 @@\n \n   public static byte[] createModuleAAD(byte[] fileAAD, ModuleType moduleType, \n       short rowGroupOrdinal, short columnOrdinal, short pageOrdinal) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5NDMwNw=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkzNDM0Ng==", "bodyText": "Sounds good to me.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427934346", "createdAt": "2020-05-20T11:25:18Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -68,19 +67,32 @@\n \n   public static byte[] createModuleAAD(byte[] fileAAD, ModuleType moduleType, \n       short rowGroupOrdinal, short columnOrdinal, short pageOrdinal) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5NDMwNw=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTI1NzE2OnYy", "diffSide": "RIGHT", "path": "parquet-format-structures/src/main/java/org/apache/parquet/format/BlockCipher.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzoyNTozN1rOGXgTIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzoyNTozN1rOGXgTIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5OTYxOA==", "bodyText": "I would keep throws IOException here. InputStream objects throw IOException so the caller shall be prepared handling these.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427299618", "createdAt": "2020-05-19T13:25:37Z", "author": {"login": "gszadovszky"}, "path": "parquet-format-structures/src/main/java/org/apache/parquet/format/BlockCipher.java", "diffHunk": "@@ -51,19 +49,17 @@\n      * Parquet Modular Encryption specification.\n      * @param AAD - Additional Authenticated Data for the decryption (ignored in case of CTR cipher)\n      * @return plaintext - starts at offset 0 of the output value, and fills up the entire byte array.\n-     * @throws IOException thrown upon any crypto problem encountered during decryption\n      */\n-    public byte[] decrypt(byte[] lengthAndCiphertext, byte[] AAD) throws IOException;\n+    public byte[] decrypt(byte[] lengthAndCiphertext, byte[] AAD);\n \n     /**\n      * Convenience decryption method that reads the length and ciphertext from the input stream.\n      * \n      * @param from Input stream with length and ciphertext.\n      * @param AAD - Additional Authenticated Data for the decryption (ignored in case of CTR cipher)\n      * @return plaintext -  starts at offset 0 of the output, and fills up the entire byte array.\n-     * @throws IOException thrown upon any crypto or IO problem encountered during decryption\n      */\n-    public byte[] decrypt(InputStream from, byte[] AAD) throws IOException;\n+    public byte[] decrypt(InputStream from, byte[] AAD);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTI2MjY4OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzoyNzowMFrOGXgWww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzoyNzowMFrOGXgWww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMwMDU0Nw==", "bodyText": "We should let the IOException thrown out.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427300547", "createdAt": "2020-05-19T13:27:00Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java", "diffHunk": "@@ -98,16 +104,21 @@\n         ((lengthBuffer[0] & 0xff));\n \n     if (ciphertextLength < 1) {\n-      throw new IOException(\"Wrong length of encrypted metadata: \" + ciphertextLength);\n+      throw new ParquetCryptoRuntimeException(\"Wrong length of encrypted metadata: \" + ciphertextLength);\n     }\n \n     byte[] ciphertextBuffer = new byte[ciphertextLength];\n     gotBytes = 0;\n     // Read the encrypted structure contents\n     while (gotBytes < ciphertextLength) {\n-      int n = from.read(ciphertextBuffer, gotBytes, ciphertextLength - gotBytes);\n+      int n;\n+      try {\n+        n = from.read(ciphertextBuffer, gotBytes, ciphertextLength - gotBytes);\n+      } catch (IOException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTI2Mjc3OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzoyNzowMVrOGXgW1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzoyNzowMVrOGXgW1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMwMDU2NA==", "bodyText": "We should let the IOException thrown out.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427300564", "createdAt": "2020-05-19T13:27:01Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java", "diffHunk": "@@ -70,23 +69,30 @@\n       if (null != AAD) cipher.updateAAD(AAD);\n \n       cipher.doFinal(ciphertext, inputOffset, inputLength, plainText, outputOffset);\n-    }  catch (GeneralSecurityException e) {\n-      throw new IOException(\"Failed to decrypt\", e);\n+    }  catch (AEADBadTagException e) {\n+      throw new TagVerificationException(\"GCM tag check failed\", e);\n+    } catch (GeneralSecurityException e) {\n+      throw new ParquetCryptoRuntimeException(\"Failed to decrypt\", e);\n     }\n \n     return plainText;\n   }\n \n   @Override\n-  public byte[] decrypt(InputStream from, byte[] AAD) throws IOException {\n+  public byte[] decrypt(InputStream from, byte[] AAD) {\n     byte[] lengthBuffer = new byte[SIZE_LENGTH];\n     int gotBytes = 0;\n \n     // Read the length of encrypted Thrift structure\n     while (gotBytes < SIZE_LENGTH) {\n-      int n = from.read(lengthBuffer, gotBytes, SIZE_LENGTH - gotBytes);\n+      int n;\n+      try {\n+        n = from.read(lengthBuffer, gotBytes, SIZE_LENGTH - gotBytes);\n+      } catch (IOException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTQ0MTE5OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNDowNzo0OFrOGXiJnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNTo1OTo1OVrOGX8V_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyOTk0OA==", "bodyText": "Is plaintext a usual term for un-encrypted files? I don't really like it but fine if it is commonly used in that sense. (Plaintext files for me are the *.txt files.)", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427329948", "createdAt": "2020-05-19T14:07:48Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc1OTEwMQ==", "bodyText": "In cryptography, plaintext is an opposite of ciphertext (the result of plaintext encryption).", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427759101", "createdAt": "2020-05-20T05:59:59Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyOTk0OA=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 286}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2MTQ0MzQ0OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNDowODoxN1rOGXiLDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNDowODoxN1rOGXiLDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMzMDMxNg==", "bodyText": "I think, ParquetCryptoRuntimeException would fit better here.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427330316", "createdAt": "2020-05-19T14:08:17Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file\n+        fileDecryptor.setPlaintextFile();\n+        // Done to detect files that were not encrypted by mistake\n+        if (!fileDecryptor.plaintextFilesAllowed()) {\n+          throw new IOException(\"Applying decryptor on plaintext file\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 290}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NTQxMTEzOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMjozMjozNFrOGYJTWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMjozMjozNFrOGYJTWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzk3MTQxNg==", "bodyText": "There was no such check in the previous code. Strictly speaking it is a breaking change as a NullPointerException was thrown where an IOException is thrown today.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427971416", "createdAt": "2020-05-20T12:32:34Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file\n+        fileDecryptor.setPlaintextFile();\n+        // Done to detect files that were not encrypted by mistake\n+        if (!fileDecryptor.plaintextFilesAllowed()) {\n+          throw new IOException(\"Applying decryptor on plaintext file\");\n+        }\n+      } else {  // Encrypted file with plaintext footer\n+        // if no fileDecryptor, can still read plaintext columns\n+        fileDecryptor.setFileCryptoMetaData(fileMetaData.getEncryption_algorithm(), false, \n+            fileMetaData.getFooter_signing_key_metadata());\n+        if (fileDecryptor.checkFooterIntegrity()) {\n+          verifyFooterIntegrity(from, fileDecryptor, combinedFooterLength);\n+        }\n+      }\n+    }\n+    \n+    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData, fileDecryptor, encryptedFooter);\n     if (LOG.isDebugEnabled()) LOG.debug(ParquetMetadata.toPrettyJSON(parquetMetadata));\n     return parquetMetadata;\n   }\n+  \n+  public ColumnChunkMetaData buildColumnChunkMetaData(ColumnMetaData metaData, ColumnPath columnPath, PrimitiveType type, String createdBy) {\n+    return ColumnChunkMetaData.get(\n+        columnPath,\n+        type,\n+        fromFormatCodec(metaData.codec),\n+        convertEncodingStats(metaData.getEncoding_stats()),\n+        fromFormatEncodings(metaData.encodings),\n+        fromParquetStatistics(\n+            createdBy,\n+            metaData.statistics,\n+            type),\n+        metaData.data_page_offset,\n+        metaData.dictionary_page_offset,\n+        metaData.num_values,\n+        metaData.total_compressed_size,\n+        metaData.total_uncompressed_size);\n+  }\n \n   public ParquetMetadata fromParquetMetadata(FileMetaData parquetMetadata) throws IOException {\n+    return fromParquetMetadata(parquetMetadata, null, false);\n+  }\n+\n+  public ParquetMetadata fromParquetMetadata(FileMetaData parquetMetadata, \n+      InternalFileDecryptor fileDecryptor, boolean encryptedFooter) throws IOException {\n     MessageType messageType = fromParquetSchema(parquetMetadata.getSchema(), parquetMetadata.getColumn_orders());\n     List<BlockMetaData> blocks = new ArrayList<BlockMetaData>();\n     List<RowGroup> row_groups = parquetMetadata.getRow_groups();\n+    \n     if (row_groups != null) {\n       for (RowGroup rowGroup : row_groups) {\n         BlockMetaData blockMetaData = new BlockMetaData();\n         blockMetaData.setRowCount(rowGroup.getNum_rows());\n         blockMetaData.setTotalByteSize(rowGroup.getTotal_byte_size());\n+        // not set in legacy files\n+        if (rowGroup.isSetOrdinal()) {\n+          blockMetaData.setOrdinal(rowGroup.getOrdinal());\n+        }\n         List<ColumnChunk> columns = rowGroup.getColumns();\n         String filePath = columns.get(0).getFile_path();\n+        short columnOrdinal = -1;\n         for (ColumnChunk columnChunk : columns) {\n+          columnOrdinal++;\n           if ((filePath == null && columnChunk.getFile_path() != null)\n               || (filePath != null && !filePath.equals(columnChunk.getFile_path()))) {\n             throw new ParquetDecodingException(\"all column chunks of the same row group must be in the same file for now\");\n           }\n           ColumnMetaData metaData = columnChunk.meta_data;\n-          ColumnPath path = getPath(metaData);\n-          ColumnChunkMetaData column = ColumnChunkMetaData.get(\n-              path,\n-              messageType.getType(path.toArray()).asPrimitiveType(),\n-              fromFormatCodec(metaData.codec),\n-              convertEncodingStats(metaData.getEncoding_stats()),\n-              fromFormatEncodings(metaData.encodings),\n-              fromParquetStatistics(\n-                  parquetMetadata.getCreated_by(),\n-                  metaData.statistics,\n-                  messageType.getType(path.toArray()).asPrimitiveType()),\n-              metaData.data_page_offset,\n-              metaData.dictionary_page_offset,\n-              metaData.num_values,\n-              metaData.total_compressed_size,\n-              metaData.total_uncompressed_size);\n+          ColumnCryptoMetaData cryptoMetaData = columnChunk.getCrypto_metadata();\n+          ColumnChunkMetaData column = null;\n+          ColumnPath columnPath = null;\n+          boolean encryptedMetadata = false;\n+          \n+          if (null == cryptoMetaData) { // Plaintext column\n+            if (null == metaData) {\n+              throw new IOException(\"ColumnMetaData not set in plaintext column\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 377}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NTQxNTYyOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMjozMzo1NVrOGYJWVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMjozMzo1NVrOGYJWVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzk3MjE4MA==", "bodyText": "These IOExceptions seems to be thrown in cases of encryption related issues. Don't we want to use the specific exception instead?", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427972180", "createdAt": "2020-05-20T12:33:55Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file\n+        fileDecryptor.setPlaintextFile();\n+        // Done to detect files that were not encrypted by mistake\n+        if (!fileDecryptor.plaintextFilesAllowed()) {\n+          throw new IOException(\"Applying decryptor on plaintext file\");\n+        }\n+      } else {  // Encrypted file with plaintext footer\n+        // if no fileDecryptor, can still read plaintext columns\n+        fileDecryptor.setFileCryptoMetaData(fileMetaData.getEncryption_algorithm(), false, \n+            fileMetaData.getFooter_signing_key_metadata());\n+        if (fileDecryptor.checkFooterIntegrity()) {\n+          verifyFooterIntegrity(from, fileDecryptor, combinedFooterLength);\n+        }\n+      }\n+    }\n+    \n+    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData, fileDecryptor, encryptedFooter);\n     if (LOG.isDebugEnabled()) LOG.debug(ParquetMetadata.toPrettyJSON(parquetMetadata));\n     return parquetMetadata;\n   }\n+  \n+  public ColumnChunkMetaData buildColumnChunkMetaData(ColumnMetaData metaData, ColumnPath columnPath, PrimitiveType type, String createdBy) {\n+    return ColumnChunkMetaData.get(\n+        columnPath,\n+        type,\n+        fromFormatCodec(metaData.codec),\n+        convertEncodingStats(metaData.getEncoding_stats()),\n+        fromFormatEncodings(metaData.encodings),\n+        fromParquetStatistics(\n+            createdBy,\n+            metaData.statistics,\n+            type),\n+        metaData.data_page_offset,\n+        metaData.dictionary_page_offset,\n+        metaData.num_values,\n+        metaData.total_compressed_size,\n+        metaData.total_uncompressed_size);\n+  }\n \n   public ParquetMetadata fromParquetMetadata(FileMetaData parquetMetadata) throws IOException {\n+    return fromParquetMetadata(parquetMetadata, null, false);\n+  }\n+\n+  public ParquetMetadata fromParquetMetadata(FileMetaData parquetMetadata, \n+      InternalFileDecryptor fileDecryptor, boolean encryptedFooter) throws IOException {\n     MessageType messageType = fromParquetSchema(parquetMetadata.getSchema(), parquetMetadata.getColumn_orders());\n     List<BlockMetaData> blocks = new ArrayList<BlockMetaData>();\n     List<RowGroup> row_groups = parquetMetadata.getRow_groups();\n+    \n     if (row_groups != null) {\n       for (RowGroup rowGroup : row_groups) {\n         BlockMetaData blockMetaData = new BlockMetaData();\n         blockMetaData.setRowCount(rowGroup.getNum_rows());\n         blockMetaData.setTotalByteSize(rowGroup.getTotal_byte_size());\n+        // not set in legacy files\n+        if (rowGroup.isSetOrdinal()) {\n+          blockMetaData.setOrdinal(rowGroup.getOrdinal());\n+        }\n         List<ColumnChunk> columns = rowGroup.getColumns();\n         String filePath = columns.get(0).getFile_path();\n+        short columnOrdinal = -1;\n         for (ColumnChunk columnChunk : columns) {\n+          columnOrdinal++;\n           if ((filePath == null && columnChunk.getFile_path() != null)\n               || (filePath != null && !filePath.equals(columnChunk.getFile_path()))) {\n             throw new ParquetDecodingException(\"all column chunks of the same row group must be in the same file for now\");\n           }\n           ColumnMetaData metaData = columnChunk.meta_data;\n-          ColumnPath path = getPath(metaData);\n-          ColumnChunkMetaData column = ColumnChunkMetaData.get(\n-              path,\n-              messageType.getType(path.toArray()).asPrimitiveType(),\n-              fromFormatCodec(metaData.codec),\n-              convertEncodingStats(metaData.getEncoding_stats()),\n-              fromFormatEncodings(metaData.encodings),\n-              fromParquetStatistics(\n-                  parquetMetadata.getCreated_by(),\n-                  metaData.statistics,\n-                  messageType.getType(path.toArray()).asPrimitiveType()),\n-              metaData.data_page_offset,\n-              metaData.dictionary_page_offset,\n-              metaData.num_values,\n-              metaData.total_compressed_size,\n-              metaData.total_uncompressed_size);\n+          ColumnCryptoMetaData cryptoMetaData = columnChunk.getCrypto_metadata();\n+          ColumnChunkMetaData column = null;\n+          ColumnPath columnPath = null;\n+          boolean encryptedMetadata = false;\n+          \n+          if (null == cryptoMetaData) { // Plaintext column\n+            if (null == metaData) {\n+              throw new IOException(\"ColumnMetaData not set in plaintext column\");\n+            }\n+            columnPath = getPath(metaData);\n+            if (null != fileDecryptor && !fileDecryptor.plaintextFile()) {\n+              // mark this column as plaintext in encrypted file decryptor\n+              fileDecryptor.setColumnCryptoMetadata(columnPath, false, false, (byte[]) null, columnOrdinal);\n+            }\n+          } else {  // Encrypted column\n+            boolean encryptedWithFooterKey = cryptoMetaData.isSetENCRYPTION_WITH_FOOTER_KEY();\n+            if (encryptedWithFooterKey) { // Column encrypted with footer key\n+              if (!encryptedFooter) {\n+                throw new IOException(\"Column encrypted with footer key in file with plaintext footer\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 388}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NTUxODk4OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMjo1OToxMFrOGYKVJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMjo1OToxMFrOGYKVJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzk4ODI2MQ==", "bodyText": "Please, check your code to not to introduce any trailing whitespaces.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427988261", "createdAt": "2020-05-20T12:59:10Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1465,23 +1674,38 @@ public void writeDataPageV2Header(\n             dataEncoding,\n             rlByteLength, dlByteLength), to);\n   }\n-\n+  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 443}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NTY0NDM4OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMzoyNTo1M1rOGYLhzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwOTowNzo0OFrOGYrv1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAwNzg4NQ==", "bodyText": "Why do we introduce new public methods that are deprecated already?", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428007885", "createdAt": "2020-05-20T13:25:53Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -442,7 +460,13 @@ static ParquetMetadata readSummaryMetadata(Configuration configuration, Path bas\n    */\n   @Deprecated\n   public static final ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException {\n-    return readFooter(configuration, file, NO_FILTER);\n+    return readFooter(configuration, file, getDecryptionProperties(file, configuration));\n+  }\n+\n+  @Deprecated\n+  public static final ParquetMetadata readFooter(Configuration configuration, Path file, ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ1NTA3MQ==", "bodyText": "These are encrypting versions of the existing readFooter functions, already marked as deprecated, but still actively used (eg in parquet-cli and in Spark). The deprecation comment says \"@ deprecated will be removed in 2.0.0\". Since we are not at parquet 2.0 yet, I've marked the encrypting versions of these functions as deprecated too, in order not to forget to handle them when working on parquet-2.0.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428455071", "createdAt": "2020-05-21T05:39:36Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -442,7 +460,13 @@ static ParquetMetadata readSummaryMetadata(Configuration configuration, Path bas\n    */\n   @Deprecated\n   public static final ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException {\n-    return readFooter(configuration, file, NO_FILTER);\n+    return readFooter(configuration, file, getDecryptionProperties(file, configuration));\n+  }\n+\n+  @Deprecated\n+  public static final ParquetMetadata readFooter(Configuration configuration, Path file, ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAwNzg4NQ=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODUzNTc2Nw==", "bodyText": "So, all of these new methods are used inside parquet-mr? If not, then I don't think we need them. If yes, then please, try to refactor the caller part to use the non-deprecated ones instead. If it does not require too much effort.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428535767", "createdAt": "2020-05-21T09:07:48Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -442,7 +460,13 @@ static ParquetMetadata readSummaryMetadata(Configuration configuration, Path bas\n    */\n   @Deprecated\n   public static final ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException {\n-    return readFooter(configuration, file, NO_FILTER);\n+    return readFooter(configuration, file, getDecryptionProperties(file, configuration));\n+  }\n+\n+  @Deprecated\n+  public static final ParquetMetadata readFooter(Configuration configuration, Path file, ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAwNzg4NQ=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NTY0NzQ4OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMzoyNjoyOFrOGYLjoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMzoyNjoyOFrOGYLjoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAwODM1Mw==", "bodyText": "Why do we introduce new public methods that are deprecated already?", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428008353", "createdAt": "2020-05-20T13:26:28Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -499,55 +528,100 @@ public static final ParquetMetadata readFooter(Configuration configuration, File\n    */\n   @Deprecated\n   public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter) throws IOException {\n+    return readFooter(file, filter, null);\n+  }\n+\n+  @Deprecated\n+  public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter, ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NTY2OTUzOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMzozMTowMVrOGYLxXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMzozMTowMVrOGYLxXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAxMTg3MA==", "bodyText": "I would use the specific crypto exception here.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428011870", "createdAt": "2020-05-20T13:31:01Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -499,55 +528,100 @@ public static final ParquetMetadata readFooter(Configuration configuration, File\n    */\n   @Deprecated\n   public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter) throws IOException {\n+    return readFooter(file, filter, null);\n+  }\n+\n+  @Deprecated\n+  public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter, \n+      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n     ParquetReadOptions options;\n     if (file instanceof HadoopInputFile) {\n-      options = HadoopReadOptions.builder(((HadoopInputFile) file).getConfiguration())\n+      HadoopInputFile hadoopFile = (HadoopInputFile) file;\n+      options = HadoopReadOptions.builder(hadoopFile.getConfiguration())\n           .withMetadataFilter(filter).build();\n+      if (null == fileDecryptionProperties) {\n+        fileDecryptionProperties = getDecryptionProperties(hadoopFile.getPath(), hadoopFile.getConfiguration());\n+      }\n     } else {\n       options = ParquetReadOptions.builder().withMetadataFilter(filter).build();\n     }\n \n     try (SeekableInputStream in = file.newStream()) {\n-      return readFooter(file, options, in);\n+      return readFooter(file, options, in, fileDecryptionProperties);\n     }\n   }\n \n-  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, SeekableInputStream f) throws IOException {\n+  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, SeekableInputStream f, \n+      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n     ParquetMetadataConverter converter = new ParquetMetadataConverter(options);\n-    return readFooter(file, options, f, converter);\n+    return readFooter(file, options, f, converter, fileDecryptionProperties);\n   }\n \n-  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, SeekableInputStream f, ParquetMetadataConverter converter) throws IOException {\n+  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, \n+      SeekableInputStream f, ParquetMetadataConverter converter, \n+      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n+\n     long fileLen = file.getLength();\n+    String filePath = file.toString();\n     LOG.debug(\"File length {}\", fileLen);\n+\n     int FOOTER_LENGTH_SIZE = 4;\n     if (fileLen < MAGIC.length + FOOTER_LENGTH_SIZE + MAGIC.length) { // MAGIC + data + footer + footerIndex + MAGIC\n-      throw new RuntimeException(file.toString() + \" is not a Parquet file (too small length: \" + fileLen + \")\");\n+      throw new RuntimeException(filePath + \" is not a Parquet file (length is too low: \" + fileLen + \")\");\n     }\n-    long footerLengthIndex = fileLen - FOOTER_LENGTH_SIZE - MAGIC.length;\n-    LOG.debug(\"reading footer index at {}\", footerLengthIndex);\n \n-    f.seek(footerLengthIndex);\n-    int footerLength = readIntLittleEndian(f);\n+    // Read footer length and magic string - with a single seek\n     byte[] magic = new byte[MAGIC.length];\n+    long fileMetadataLengthIndex = fileLen - magic.length - FOOTER_LENGTH_SIZE;\n+    LOG.debug(\"reading footer index at {}\", fileMetadataLengthIndex);\n+    f.seek(fileMetadataLengthIndex);\n+    int fileMetadataLength = readIntLittleEndian(f);\n     f.readFully(magic);\n-    if (!Arrays.equals(MAGIC, magic)) {\n-      throw new RuntimeException(file.toString() + \" is not a Parquet file. expected magic number at tail \" + Arrays.toString(MAGIC) + \" but found \" + Arrays.toString(magic));\n+\n+    boolean encryptedFooterMode;\n+    if (Arrays.equals(MAGIC, magic)) {\n+      encryptedFooterMode = false;\n+    } else if (Arrays.equals(EFMAGIC, magic)) {\n+      encryptedFooterMode = true;\n+    } else {\n+      throw new RuntimeException(filePath + \" is not a Parquet file. Expected magic number at tail, but found \" + Arrays.toString(magic));\n+    }\n+\n+    long fileMetadataIndex = fileMetadataLengthIndex - fileMetadataLength;\n+    LOG.debug(\"read footer length: {}, footer index: {}\", fileMetadataLength, fileMetadataIndex);\n+    if (fileMetadataIndex < magic.length || fileMetadataIndex >= fileMetadataLengthIndex) {\n+      throw new RuntimeException(\"corrupted file: the footer index is not within the file: \" + fileMetadataIndex);\n     }\n-    long footerIndex = footerLengthIndex - footerLength;\n-    LOG.debug(\"read footer length: {}, footer index: {}\", footerLength, footerIndex);\n-    if (footerIndex < MAGIC.length || footerIndex >= footerLengthIndex) {\n-      throw new RuntimeException(\"corrupted file: the footer index is not within the file: \" + footerIndex);\n+    f.seek(fileMetadataIndex);\n+\n+    InternalFileDecryptor fileDecryptor = null;\n+    if (null != fileDecryptionProperties) {\n+      fileDecryptor  = new InternalFileDecryptor(fileDecryptionProperties);\n     }\n-    f.seek(footerIndex);\n+\n     // Read all the footer bytes in one time to avoid multiple read operations,\n     // since it can be pretty time consuming for a single read operation in HDFS.\n-    ByteBuffer footerBytesBuffer = ByteBuffer.allocate(footerLength);\n+    ByteBuffer footerBytesBuffer = ByteBuffer.allocate(fileMetadataLength);\n     f.readFully(footerBytesBuffer);\n     LOG.debug(\"Finished to read all footer bytes.\");\n     footerBytesBuffer.flip();\n     InputStream footerBytesStream = ByteBufferInputStream.wrap(footerBytesBuffer);\n-    return converter.readParquetMetadata(footerBytesStream, options.getMetadataFilter());\n+\n+    // Regular file, or encrypted file with plaintext footer\n+    if (!encryptedFooterMode) {\n+      return converter.readParquetMetadata(footerBytesStream, options.getMetadataFilter(), fileDecryptor, false, \n+          fileMetadataLength);\n+    }\n+\n+    // Encrypted file with encrypted footer\n+    if (null == fileDecryptor) {\n+      throw new RuntimeException(\"Trying to read file with encrypted footer. No keys available\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 205}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NTgxMjU4OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMzo1ODoyNlrOGYNIGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMTowMDo1OFrOGYuwJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAzNDA3NA==", "bodyText": "I think, it would be better to put the FileDecryptionProperties into ParquetReadOptions. HadoopReadOptions (the extension of ParquetReadOptions) already contains the hadoop conf so you may be able to create FileDecryptionProperties from there if not set.\nThis way you do not need to add new methods/constructors where ParquetReadOptions is already there as an argument.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428034074", "createdAt": "2020-05-20T13:58:26Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -705,22 +797,39 @@ public ParquetFileReader(Configuration conf, Path file, ParquetMetadata footer)\n       paths.put(ColumnPath.get(col.getPath()), col);\n     }\n     this.crc = options.usePageChecksumVerification() ? new CRC32() : null;\n+    this.fileDecryptor = fileMetaData.getFileDecryptor();\n   }\n \n   public ParquetFileReader(InputFile file, ParquetReadOptions options) throws IOException {\n+    this(file, options, null);\n+  }\n+\n+  public ParquetFileReader(InputFile file, ParquetReadOptions options, \n+      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n     this.converter = new ParquetMetadataConverter(options);\n     this.file = file;\n     this.f = file.newStream();\n     this.options = options;\n+    if ((null == fileDecryptionProperties) && (file instanceof HadoopInputFile)) {\n+      HadoopInputFile hadoopFile = (HadoopInputFile) file;\n+      fileDecryptionProperties = getDecryptionProperties(hadoopFile.getPath(), hadoopFile.getConfiguration());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 275}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQ2NTA5NA==", "bodyText": "We need the file Path in order to create the file decryption properties. ParquetReadOptions and HadoopReadOptions don't keep/handle the file paths.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428465094", "createdAt": "2020-05-21T06:14:47Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -705,22 +797,39 @@ public ParquetFileReader(Configuration conf, Path file, ParquetMetadata footer)\n       paths.put(ColumnPath.get(col.getPath()), col);\n     }\n     this.crc = options.usePageChecksumVerification() ? new CRC32() : null;\n+    this.fileDecryptor = fileMetaData.getFileDecryptor();\n   }\n \n   public ParquetFileReader(InputFile file, ParquetReadOptions options) throws IOException {\n+    this(file, options, null);\n+  }\n+\n+  public ParquetFileReader(InputFile file, ParquetReadOptions options, \n+      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n     this.converter = new ParquetMetadataConverter(options);\n     this.file = file;\n     this.f = file.newStream();\n     this.options = options;\n+    if ((null == fileDecryptionProperties) && (file instanceof HadoopInputFile)) {\n+      HadoopInputFile hadoopFile = (HadoopInputFile) file;\n+      fileDecryptionProperties = getDecryptionProperties(hadoopFile.getPath(), hadoopFile.getConfiguration());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAzNDA3NA=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 275}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODUzNzIyMA==", "bodyText": "ParquetReadOptions was created to carry all the required properties for reading a parquet file. If the Path is necessary for the decryption then we might add it to the options as well. If we decide to not to add it still, I would use the options object to carry any other decryption properties.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428537220", "createdAt": "2020-05-21T09:10:58Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -705,22 +797,39 @@ public ParquetFileReader(Configuration conf, Path file, ParquetMetadata footer)\n       paths.put(ColumnPath.get(col.getPath()), col);\n     }\n     this.crc = options.usePageChecksumVerification() ? new CRC32() : null;\n+    this.fileDecryptor = fileMetaData.getFileDecryptor();\n   }\n \n   public ParquetFileReader(InputFile file, ParquetReadOptions options) throws IOException {\n+    this(file, options, null);\n+  }\n+\n+  public ParquetFileReader(InputFile file, ParquetReadOptions options, \n+      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n     this.converter = new ParquetMetadataConverter(options);\n     this.file = file;\n     this.f = file.newStream();\n     this.options = options;\n+    if ((null == fileDecryptionProperties) && (file instanceof HadoopInputFile)) {\n+      HadoopInputFile hadoopFile = (HadoopInputFile) file;\n+      fileDecryptionProperties = getDecryptionProperties(hadoopFile.getPath(), hadoopFile.getConfiguration());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAzNDA3NA=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 275}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU4NDk5OA==", "bodyText": "Sounds good. I'll check what can be done here.\nAlso, will check how the deprecated readFooter functions can be handled for encryption.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428584998", "createdAt": "2020-05-21T11:00:58Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -705,22 +797,39 @@ public ParquetFileReader(Configuration conf, Path file, ParquetMetadata footer)\n       paths.put(ColumnPath.get(col.getPath()), col);\n     }\n     this.crc = options.usePageChecksumVerification() ? new CRC32() : null;\n+    this.fileDecryptor = fileMetaData.getFileDecryptor();\n   }\n \n   public ParquetFileReader(InputFile file, ParquetReadOptions options) throws IOException {\n+    this(file, options, null);\n+  }\n+\n+  public ParquetFileReader(InputFile file, ParquetReadOptions options, \n+      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n     this.converter = new ParquetMetadataConverter(options);\n     this.file = file;\n     this.f = file.newStream();\n     this.options = options;\n+    if ((null == fileDecryptionProperties) && (file instanceof HadoopInputFile)) {\n+      HadoopInputFile hadoopFile = (HadoopInputFile) file;\n+      fileDecryptionProperties = getDecryptionProperties(hadoopFile.getPath(), hadoopFile.getConfiguration());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAzNDA3NA=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 275}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NTg1ODIzOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDowNzoyM1rOGYNkIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDowNzoyM1rOGYNkIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA0MTI0OQ==", "bodyText": "I think, we can replace it.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428041249", "createdAt": "2020-05-20T14:07:23Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1021,18 +1148,44 @@ DictionaryPage readDictionary(ColumnChunkMetaData meta) throws IOException {\n         !meta.getEncodings().contains(Encoding.RLE_DICTIONARY)) {\n       return null;\n     }\n+    /** TODO Gabor - can be replaced with this?:\n+    if (!meta.hasDictionaryPage()) {\n+      return null;\n+    } */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 375}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTAzNjA2OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwOTo0ODoxN1rOGYs3JQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxMzo1ODoyNVrOGb2Z4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU1NDAyMQ==", "bodyText": "@chenjunjiedada, could you please check this?", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428554021", "createdAt": "2020-05-21T09:48:17Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1071,12 +1229,31 @@ public BloomFilterReader getBloomFilterDataReader(BlockMetaData block) {\n    */\n   public BloomFilter readBloomFilter(ColumnChunkMetaData meta) throws IOException {\n     long bloomFilterOffset = meta.getBloomFilterOffset();\n+\n+    if (0 == bloomFilterOffset) { // TODO Junjie - is there a better way to handle this?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 441}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTgyMzg0OQ==", "bodyText": "@ggershinsky, please ensure this is correct and don't keep TODOs in the final code. (We usually don't fix them and they will be there forever.)", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r431823849", "createdAt": "2020-05-28T13:13:12Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1071,12 +1229,31 @@ public BloomFilterReader getBloomFilterDataReader(BlockMetaData block) {\n    */\n   public BloomFilter readBloomFilter(ColumnChunkMetaData meta) throws IOException {\n     long bloomFilterOffset = meta.getBloomFilterOffset();\n+\n+    if (0 == bloomFilterOffset) { // TODO Junjie - is there a better way to handle this?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU1NDAyMQ=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 441}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTg1NjA5Nw==", "bodyText": "yep, this works; also, makes sense to me - we should not proceed to read a bloom filter header (including seeking its offset), if there is no bloom filter in the file; this can be checked via the bloomFilterOffset variable - it can't be 0 if a bloom filter is present; and if no bloom filter, it can be 0 only.\nI'll remove the TODO comment.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r431856097", "createdAt": "2020-05-28T13:58:25Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1071,12 +1229,31 @@ public BloomFilterReader getBloomFilterDataReader(BlockMetaData block) {\n    */\n   public BloomFilter readBloomFilter(ColumnChunkMetaData meta) throws IOException {\n     long bloomFilterOffset = meta.getBloomFilterOffset();\n+\n+    if (0 == bloomFilterOffset) { // TODO Junjie - is there a better way to handle this?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU1NDAyMQ=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 441}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTA1MDE3OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwOTo1Mzo0MlrOGYtAnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwOTo1Mzo0MlrOGYtAnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU1NjQ0Nw==", "bodyText": "I guess, it should be the specific crypto exception instead.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428556447", "createdAt": "2020-05-21T09:53:42Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1095,8 +1272,16 @@ public BloomFilter readBloomFilter(ColumnChunkMetaData meta) throws IOException\n       return null;\n     }\n \n-    byte[] bitset = new byte[numBytes];\n-    f.readFully(bitset);\n+    byte[] bitset;\n+    if (null == bloomFilterDecryptor) {\n+      bitset = new byte[numBytes];\n+      f.readFully(bitset);\n+    } else {\n+      bitset = bloomFilterDecryptor.decrypt(f, bloomFilterBitsetAAD);\n+      if (bitset.length != numBytes) {\n+        throw new IOException(\"Wrong length of decrypted bloom filter bitset\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 482}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTA3NDI2OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMDowMjoyMVrOGYtP8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwODo1OToyMVrOGa_-IQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU2MDM2OA==", "bodyText": "My understanding about this method is that it is invoked inside the ColumnChunkMetaData object when retrieving a value that might be encrypted. Why do we need to call it here?", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428560368", "createdAt": "2020-05-21T10:02:21Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1114,7 +1299,20 @@ public ColumnIndex readColumnIndex(ColumnChunkMetaData column) throws IOExceptio\n       return null;\n     }\n     f.seek(ref.getOffset());\n-    return ParquetMetadataConverter.fromParquetColumnIndex(column.getPrimitiveType(), Util.readColumnIndex(f));\n+\n+    column.decryptIfNeededed();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 494}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk2NDI1Nw==", "bodyText": "found a better place for it, inside the ColumnChunkMetaData object", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r430964257", "createdAt": "2020-05-27T08:59:21Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1114,7 +1299,20 @@ public ColumnIndex readColumnIndex(ColumnChunkMetaData column) throws IOExceptio\n       return null;\n     }\n     f.seek(ref.getOffset());\n-    return ParquetMetadataConverter.fromParquetColumnIndex(column.getPrimitiveType(), Util.readColumnIndex(f));\n+\n+    column.decryptIfNeededed();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU2MDM2OA=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 494}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTA4NTI2OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMDowNjoyM1rOGYtWmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMDowNjoyM1rOGYtWmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU2MjA3NA==", "bodyText": "Same as above.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428562074", "createdAt": "2020-05-21T10:06:23Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1131,7 +1329,19 @@ public OffsetIndex readOffsetIndex(ColumnChunkMetaData column) throws IOExceptio\n       return null;\n     }\n     f.seek(ref.getOffset());\n-    return ParquetMetadataConverter.fromParquetOffsetIndex(Util.readOffsetIndex(f));\n+\n+    column.decryptIfNeededed();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 516}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTEwMTYwOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMDoxMjoxN1rOGYtgsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMDoxMjoxN1rOGYtgsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU2NDY1Nw==", "bodyText": "Just like for MAGIC:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              public static final byte[] EFMAGIC = EF_MAGIC_STR.getBytes(Charset.forName(\"ASCII\"));\n          \n          \n            \n              public static final byte[] EFMAGIC = EF_MAGIC_STR.getBytes(StandardCharsets.US_ASCII);", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428564657", "createdAt": "2020-05-21T10:12:17Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java", "diffHunk": "@@ -90,6 +102,8 @@\n   public static final String PARQUET_METADATA_FILE = \"_metadata\";\n   public static final String MAGIC_STR = \"PAR1\";\n   public static final byte[] MAGIC = MAGIC_STR.getBytes(StandardCharsets.US_ASCII);\n+  public static final String EF_MAGIC_STR = \"PARE\";\n+  public static final byte[] EFMAGIC = EF_MAGIC_STR.getBytes(Charset.forName(\"ASCII\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTM2ODkwOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMTo1NTowNlrOGYwGug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMTo1NTowNlrOGYwGug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYwNzE2Mg==", "bodyText": "blockSize is a bit misleading. It is going to be used as ordinal so what about rowGroupOrdinal or similar?", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428607162", "createdAt": "2020-05-21T11:55:06Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java", "diffHunk": "@@ -772,6 +860,11 @@ public void endBlock() throws IOException {\n     state = state.endBlock();\n     LOG.debug(\"{}: end block\", out.getPos());\n     currentBlock.setRowCount(currentRecordCount);\n+    int blockSize = blocks.size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 208}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTQ2Mjg5OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMjoyOTo1OFrOGYxANA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMjoyOTo1OFrOGYxANA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYyMTg3Ng==", "bodyText": "I don't know why we think that the footer length is an important information but this is the only case where we do not log it. We might want to add it here as well.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428621876", "createdAt": "2020-05-21T12:29:58Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java", "diffHunk": "@@ -1035,20 +1154,89 @@ private static void serializeBloomFilters(\n \n         long offset = out.getPos();\n         column.setBloomFilterOffset(offset);\n-        Util.writeBloomFilterHeader(ParquetMetadataConverter.toBloomFilterHeader(bloomFilter), out);\n-        bloomFilter.writeTo(out);\n+        \n+        BlockCipher.Encryptor bloomFilterEncryptor = null;\n+        byte[] bloomFilterHeaderAAD = null;\n+        byte[] bloomFilterBitsetAAD = null;\n+        if (null != fileEncryptor) {\n+          InternalColumnEncryptionSetup columnEncryptionSetup = fileEncryptor.getColumnSetup(column.getPath(), false, (short) cIndex);\n+          if (columnEncryptionSetup.isEncrypted()) {\n+            bloomFilterEncryptor = columnEncryptionSetup.getMetaDataEncryptor();\n+            short columnOrdinal = columnEncryptionSetup.getOrdinal();\n+            bloomFilterHeaderAAD = AesCipher.createModuleAAD(fileEncryptor.getFileAAD(), ModuleType.BloomFilterHeader, \n+                block.getOrdinal(), columnOrdinal, (short)-1);\n+            bloomFilterBitsetAAD = AesCipher.createModuleAAD(fileEncryptor.getFileAAD(), ModuleType.BloomFilterBitset, \n+                block.getOrdinal(), columnOrdinal, (short)-1);\n+          }\n+        }\n+        \n+        Util.writeBloomFilterHeader(ParquetMetadataConverter.toBloomFilterHeader(bloomFilter), out, \n+            bloomFilterEncryptor, bloomFilterHeaderAAD);\n+        \n+        ByteArrayOutputStream tempOutStream = new ByteArrayOutputStream();\n+        bloomFilter.writeTo(tempOutStream);\n+        byte[] serializedBitset = tempOutStream.toByteArray();\n+        if (null != bloomFilterEncryptor) {\n+          serializedBitset = bloomFilterEncryptor.encrypt(serializedBitset, bloomFilterBitsetAAD);\n+        }\n+        out.write(serializedBitset);\n       }\n     }\n   }\n-\n-  private static void serializeFooter(ParquetMetadata footer, PositionOutputStream out) throws IOException {\n-    long footerIndex = out.getPos();\n+  \n+  private static void serializeFooter(ParquetMetadata footer, PositionOutputStream out,\n+      InternalFileEncryptor fileEncryptor) throws IOException {\n+    \n     ParquetMetadataConverter metadataConverter = new ParquetMetadataConverter();\n-    org.apache.parquet.format.FileMetaData parquetMetadata = metadataConverter.toParquetMetadata(CURRENT_VERSION, footer);\n-    writeFileMetaData(parquetMetadata, out);\n-    LOG.debug(\"{}: footer length = {}\" , out.getPos(), (out.getPos() - footerIndex));\n-    BytesUtils.writeIntLittleEndian(out, (int) (out.getPos() - footerIndex));\n-    out.write(MAGIC);\n+    \n+    // Unencrypted file\n+    if (null == fileEncryptor) {\n+      long footerIndex = out.getPos();\n+      org.apache.parquet.format.FileMetaData parquetMetadata = metadataConverter.toParquetMetadata(CURRENT_VERSION, footer);\n+      writeFileMetaData(parquetMetadata, out);\n+      LOG.debug(\"{}: footer length = {}\" , out.getPos(), (out.getPos() - footerIndex));\n+      BytesUtils.writeIntLittleEndian(out, (int) (out.getPos() - footerIndex));\n+      out.write(MAGIC);\n+      return;\n+    }\n+    \n+    org.apache.parquet.format.FileMetaData parquetMetadata =\n+        metadataConverter.toParquetMetadata(CURRENT_VERSION, footer, fileEncryptor);\n+    \n+    // Encrypted file with plaintext footer \n+    if (!fileEncryptor.isFooterEncrypted()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 381}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTQ4OTU4OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMjozOTo0MlrOGYxRmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMjozOTo0MlrOGYxRmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYyNjMyOQ==", "bodyText": "I think, createEncryptionProperties or similar would be a better naming.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428626329", "createdAt": "2020-05-21T12:39:42Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java", "diffHunk": "@@ -539,4 +544,13 @@ public OutputCommitter getOutputCommitter(TaskAttemptContext context)\n   public synchronized static MemoryManager getMemoryManager() {\n     return memoryManager;\n   }\n+  \n+  private FileEncryptionProperties getEncryptionProperties(Configuration fileHadoopConfig, Path tempFilePath, ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTUwNDMzOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetRecordReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMjo0NToxMlrOGYxbOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QwODo1Njo1MVrOGa_3uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYyODc5Mg==", "bodyText": "It is not clear to me why we need to suppress this exception.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428628792", "createdAt": "2020-05-21T12:45:12Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetRecordReader.java", "diffHunk": "@@ -188,7 +189,11 @@ private void checkDeltaByteArrayProblem(FileMetaData meta, Configuration conf, B\n       // this is okay if not using DELTA_BYTE_ARRAY with the bug\n       Set<Encoding> encodings = new HashSet<Encoding>();\n       for (ColumnChunkMetaData column : block.getColumns()) {\n-        encodings.addAll(column.getEncodings());\n+        try {\n+          encodings.addAll(column.getEncodings());\n+        } catch (KeyAccessDeniedException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDk2MjYxOQ==", "bodyText": "removed the suppression", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r430962619", "createdAt": "2020-05-27T08:56:51Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetRecordReader.java", "diffHunk": "@@ -188,7 +189,11 @@ private void checkDeltaByteArrayProblem(FileMetaData meta, Configuration conf, B\n       // this is okay if not using DELTA_BYTE_ARRAY with the bug\n       Set<Encoding> encodings = new HashSet<Encoding>();\n       for (ColumnChunkMetaData column : block.getColumns()) {\n-        encodings.addAll(column.getEncodings());\n+        try {\n+          encodings.addAll(column.getEncodings());\n+        } catch (KeyAccessDeniedException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYyODc5Mg=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTUxMDMxOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMjo0NzoyMVrOGYxe8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMjo0NzoyMVrOGYxe8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYyOTc0Nw==", "bodyText": "Similarly to another one of my comments. Instead of adding more deprecated methods/constructors, try to use the non-deprecated once in the code. If not used internally, we should not introduce these.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428629747", "createdAt": "2020-05-21T12:47:21Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "diffHunk": "@@ -194,6 +195,23 @@ public ParquetWriter(\n         enableDictionary, validating, writerVersion, conf);\n   }\n \n+  @Deprecated\n+  public ParquetWriter(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTU0Mjc3OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMjo1ODowMFrOGYxzAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMjo1ODowMFrOGYxzAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYzNDg4MA==", "bodyText": "The column path is already part of properties which whole purpose is to save memory. If you need this for EncryptedColumnChunkMetaData, add it there instead.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428634880", "createdAt": "2020-05-21T12:58:00Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -34,6 +47,9 @@\n  * Column meta data for a block stored in the file footer and passed in the InputSplit\n  */\n abstract public class ColumnChunkMetaData {\n+  \n+  protected ColumnPath path;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTU1MDEzOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMzowMDozMVrOGYx3zQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMzowMDozMVrOGYx3zQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYzNjEwOQ==", "bodyText": "The design seems to require this method to be only invoked inside this package. So please, make it package private.\nYou may have an empty implementation here so you don't need to add the empty implementations in the Int/Long classes.\nAlso, a bit too many \"ed\"s:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              abstract public void decryptIfNeededed();\n          \n          \n            \n              abstract public void decryptIfNeeded();", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428636109", "createdAt": "2020-05-21T13:00:31Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -241,6 +280,8 @@ public PrimitiveType getPrimitiveType() {\n    * @return the stats for this column\n    */\n   abstract public Statistics getStatistics();\n+  \n+  abstract public void decryptIfNeededed();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTU4OTU0OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMzoxMzoyMFrOGYyQ0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxMzo0ODoyN1rOGb18FA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0MjUxMg==", "bodyText": "These are accessed only inside the same class so you may keep them private.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428642512", "createdAt": "2020-05-21T13:13:20Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -165,10 +200,10 @@ protected static boolean positiveLongFitsInAnInt(long value) {\n     return (value >= 0) && (value + Integer.MIN_VALUE <= Integer.MAX_VALUE);\n   }\n \n-  private final EncodingStats encodingStats;\n+  protected EncodingStats encodingStats;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkwMjQwMA==", "bodyText": "these fields are accessed by the EncryptedColumnChunkMetaData class, that sets their values after decrypting the column metadata.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r430902400", "createdAt": "2020-05-27T07:11:43Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -165,10 +200,10 @@ protected static boolean positiveLongFitsInAnInt(long value) {\n     return (value >= 0) && (value + Integer.MIN_VALUE <= Integer.MAX_VALUE);\n   }\n \n-  private final EncodingStats encodingStats;\n+  protected EncodingStats encodingStats;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0MjUxMg=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTc1MTk4MQ==", "bodyText": "Yep, but EncryptedColumnChunkMetaData is defined inside the class ColumnChunkMetaData. It makes them access each other's private members. Even the classEncryptedColumnChunkMetaData can be declared private if you don't want to use the type outside of EncryptedColumnChunkMetaData.\nSee Nested Classes for details.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r431751981", "createdAt": "2020-05-28T10:59:16Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -165,10 +200,10 @@ protected static boolean positiveLongFitsInAnInt(long value) {\n     return (value >= 0) && (value + Integer.MIN_VALUE <= Integer.MAX_VALUE);\n   }\n \n-  private final EncodingStats encodingStats;\n+  protected EncodingStats encodingStats;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0MjUxMg=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTgwMjc2Mg==", "bodyText": "hmm, when I change encodingStats to private, I get a compilation error\nThe field ColumnChunkMetaData.encodingStats is not visible\nat line 619,\nthis.encodingStats = shadowColumnChunkMetaData.encodingStats;", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r431802762", "createdAt": "2020-05-28T12:38:36Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -165,10 +200,10 @@ protected static boolean positiveLongFitsInAnInt(long value) {\n     return (value >= 0) && (value + Integer.MIN_VALUE <= Integer.MAX_VALUE);\n   }\n \n-  private final EncodingStats encodingStats;\n+  protected EncodingStats encodingStats;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0MjUxMg=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTgyMTgwMw==", "bodyText": "OK, I got it now. So, these are not nested classes. There are 4 different classes next to each other in the same java file. This is valid since only one class is public but I've never seen such design in production. I've never realized this design in ColumnChunkMetaData. There is no reason why one would put multiple classes in the same file but not nesting them.\nSo, without refactoring this structure I'll accept encodingStats to not being private. But, we can narrow the visibility by using package private (no modifier).", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r431821803", "createdAt": "2020-05-28T13:10:07Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -165,10 +200,10 @@ protected static boolean positiveLongFitsInAnInt(long value) {\n     return (value >= 0) && (value + Integer.MIN_VALUE <= Integer.MAX_VALUE);\n   }\n \n-  private final EncodingStats encodingStats;\n+  protected EncodingStats encodingStats;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0MjUxMg=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTg0ODQ2OA==", "bodyText": "Sure, will change this.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r431848468", "createdAt": "2020-05-28T13:48:27Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -165,10 +200,10 @@ protected static boolean positiveLongFitsInAnInt(long value) {\n     return (value >= 0) && (value + Integer.MIN_VALUE <= Integer.MAX_VALUE);\n   }\n \n-  private final EncodingStats encodingStats;\n+  protected EncodingStats encodingStats;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0MjUxMg=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTYyOTgzOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMzoyNTozN1rOGYyqSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMzoyNTozN1rOGYyqSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0OTAzMg==", "bodyText": "I would expect some comments here why we need this object (to decrypt this metadata lazily instead of simply decrypt it at reading).", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428649032", "createdAt": "2020-05-21T13:25:37Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -141,11 +159,28 @@ public static ColumnChunkMetaData get(\n           totalUncompressedSize);\n     }\n   }\n+  \n+  public static ColumnChunkMetaData getWithEncryptedMetadata(ParquetMetadataConverter parquetMetadataConverter, ColumnPath path, ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTY4MzYxOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestBloomEncryption.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMzo0MDo0MFrOGYzL4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxMDoyMjo1N1rOGcXOgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY1NzYzMw==", "bodyText": "I guess, this test and the one for column indexes were created by copy-pasting the original tests and adding the encryption. Since we already use parameterized testing in both I would suggest keeping the original tests and adding another dimension for plain/encrypted or similar. This way we would have 4 runs for each tests: (V1 with plain), (V1 with encryption), (V2 with plain) and (V2 with encryption).", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428657633", "createdAt": "2020-05-21T13:40:40Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestBloomEncryption.java", "diffHunk": "@@ -0,0 +1,313 @@\n+/*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDkwMTIzNw==", "bodyText": "this will be a part of #782 (it will also remove the TestBloomEncryption.java and TestColumnIndexEncryption.java files).", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r430901237", "createdAt": "2020-05-27T07:09:14Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestBloomEncryption.java", "diffHunk": "@@ -0,0 +1,313 @@\n+/*", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY1NzYzMw=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjM5Mzg1OA==", "bodyText": "Thank you, @gszadovszky , for the suggestion. We've added it to #782 .", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r432393858", "createdAt": "2020-05-29T10:22:57Z", "author": {"login": "andersonm-1"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestBloomEncryption.java", "diffHunk": "@@ -0,0 +1,313 @@\n+/*", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY1NzYzMw=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTY5MzczOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMzo0MzoyMFrOGYzSQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMzo0MzoyMFrOGYzSQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY1OTI2NA==", "bodyText": "I would suggest creating a temporary dir in temporary space instead of having a relative path depending on the current directory. This way it is a bit error prone.\n(There are a couple of ways to create and cleanup temporary directories in junit.)", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428659264", "createdAt": "2020-05-21T13:43:20Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Random;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.StringKeyIdRetriever;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+\n+\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestEncryption {\n+\n+  @Test\n+  public void test() throws Exception {\n+    Configuration conf = new Configuration();\n+    Path root = new Path(\"target/tests/TestEncryption/\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTcwMzc2OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMzo0NjowN1rOGYzYfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMzo0NjowN1rOGYzYfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY2MDg2Mw==", "bodyText": "Consider using try-with-resources. That construct would be less error prone (e.g. closing the writer/reader in case of an exception occurs).", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428660863", "createdAt": "2020-05-21T13:46:07Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Random;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.StringKeyIdRetriever;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+\n+\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestEncryption {\n+\n+  @Test\n+  public void test() throws Exception {\n+    Configuration conf = new Configuration();\n+    Path root = new Path(\"target/tests/TestEncryption/\");\n+    enforceEmptyDir(conf, root);\n+\n+    Random random = new Random();\n+    int numberOfEncryptionModes = 5;\n+    FileEncryptionProperties[] encryptionPropertiesList = new FileEncryptionProperties[numberOfEncryptionModes];\n+    FileDecryptionProperties[] decryptionPropertiesList = new FileDecryptionProperties[numberOfEncryptionModes];\n+\n+    // #0 Unencrypted - make sure null encryption properties don't break regular Parquet\n+    encryptionPropertiesList[0] = null;\n+    decryptionPropertiesList[0] = null;\n+\n+    // #1 Basic encryption setup\n+    byte[] encryptionKey = new byte[16];\n+    random.nextBytes(encryptionKey);\n+    FileEncryptionProperties encryptionProperties = FileEncryptionProperties.builder(encryptionKey).build();\n+    FileDecryptionProperties decryptionProperties = FileDecryptionProperties.builder().withFooterKey(encryptionKey).build();\n+    encryptionPropertiesList[1] = encryptionProperties;\n+    decryptionPropertiesList[1] = decryptionProperties;\n+\n+    // #2 Default algorithm, non-uniform encryption, key metadata, key retriever, AAD prefix\n+    byte[] footerKey = new byte[16];\n+    random.nextBytes(footerKey);\n+    byte[] columnKey0 = new byte[16];\n+    random.nextBytes(columnKey0);\n+    byte[] columnKey1 = new byte[16];\n+    random.nextBytes(columnKey1);\n+    ColumnEncryptionProperties columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    ColumnEncryptionProperties columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    HashMap<ColumnPath, ColumnEncryptionProperties> columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    byte[] AADPrefix = root.getName().getBytes(StandardCharsets.UTF_8);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    StringKeyIdRetriever keyRetriever = new StringKeyIdRetriever();\n+    keyRetriever.putKey(\"fk\", footerKey);\n+    keyRetriever.putKey(\"ck0\", columnKey0);\n+    keyRetriever.putKey(\"ck1\", columnKey1);\n+    decryptionProperties = FileDecryptionProperties.builder()\n+        .withKeyRetriever(keyRetriever)\n+        .build();\n+    encryptionPropertiesList[2] = encryptionProperties;\n+    decryptionPropertiesList[2] = decryptionProperties;\n+\n+    // #3 GCM_CTR algorithm, non-uniform encryption, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withAlgorithm(ParquetCipher.AES_GCM_CTR_V1)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[3] = encryptionProperties;\n+    decryptionPropertiesList[3] = decryptionProperties; // Same decryption properties\n+\n+    // #4  Plaintext footer, default algorithm, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withPlaintextFooter()\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[4] = encryptionProperties;\n+    decryptionPropertiesList[4] = decryptionProperties; // Same decryption properties\n+\n+\n+    MessageType schema = parseMessageType(\n+        \"message test { \"\n+            + \"required binary binary_field; \"\n+            + \"required int32 int32_field; \"\n+            + \"required int64 int64_field; \"\n+            + \"required boolean boolean_field; \"\n+            + \"required float float_field; \"\n+            + \"required double double_field; \"\n+            + \"required fixed_len_byte_array(3) flba_field; \"\n+            + \"required int96 int96_field; \"\n+            + \"} \");\n+    GroupWriteSupport.setSchema(schema, conf);\n+    SimpleGroupFactory f = new SimpleGroupFactory(schema);\n+\n+    for (int encryptionMode = 0; encryptionMode < numberOfEncryptionModes; encryptionMode++) {\n+      System.out.println(\"MODE: \"+encryptionMode);\n+      \n+      Path file = new Path(root, \"m_\" + encryptionMode + \".parquet.encrypted\");\n+      ParquetWriter<Group> writer = new ParquetWriter<Group>(\n+          file,\n+          new GroupWriteSupport(),\n+          UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetWriter.DEFAULT_WRITER_VERSION, conf, \n+          encryptionPropertiesList[encryptionMode]);\n+      for (int i = 0; i < 1000; i++) {\n+        writer.write(\n+            f.newGroup()\n+            .append(\"binary_field\", \"test\" + i)\n+            .append(\"int32_field\", 32)\n+            .append(\"int64_field\", 64l)\n+            .append(\"boolean_field\", true)\n+            .append(\"float_field\", 1.0f)\n+            .append(\"double_field\", 2.0d)\n+            .append(\"flba_field\", \"foo\")\n+            .append(\"int96_field\", Binary.fromConstantByteArray(new byte[12])));\n+      }\n+      writer.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 188}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTcxMjQ5OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMzo0ODozNVrOGYzd9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMzo0ODozNVrOGYzd9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY2MjI2MA==", "bodyText": "Yep, that's one good way I was talking about but you should use temp instead of hardcoding the directory at the beginning.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428662260", "createdAt": "2020-05-21T13:48:35Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Random;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.StringKeyIdRetriever;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+\n+\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestEncryption {\n+\n+  @Test\n+  public void test() throws Exception {\n+    Configuration conf = new Configuration();\n+    Path root = new Path(\"target/tests/TestEncryption/\");\n+    enforceEmptyDir(conf, root);\n+\n+    Random random = new Random();\n+    int numberOfEncryptionModes = 5;\n+    FileEncryptionProperties[] encryptionPropertiesList = new FileEncryptionProperties[numberOfEncryptionModes];\n+    FileDecryptionProperties[] decryptionPropertiesList = new FileDecryptionProperties[numberOfEncryptionModes];\n+\n+    // #0 Unencrypted - make sure null encryption properties don't break regular Parquet\n+    encryptionPropertiesList[0] = null;\n+    decryptionPropertiesList[0] = null;\n+\n+    // #1 Basic encryption setup\n+    byte[] encryptionKey = new byte[16];\n+    random.nextBytes(encryptionKey);\n+    FileEncryptionProperties encryptionProperties = FileEncryptionProperties.builder(encryptionKey).build();\n+    FileDecryptionProperties decryptionProperties = FileDecryptionProperties.builder().withFooterKey(encryptionKey).build();\n+    encryptionPropertiesList[1] = encryptionProperties;\n+    decryptionPropertiesList[1] = decryptionProperties;\n+\n+    // #2 Default algorithm, non-uniform encryption, key metadata, key retriever, AAD prefix\n+    byte[] footerKey = new byte[16];\n+    random.nextBytes(footerKey);\n+    byte[] columnKey0 = new byte[16];\n+    random.nextBytes(columnKey0);\n+    byte[] columnKey1 = new byte[16];\n+    random.nextBytes(columnKey1);\n+    ColumnEncryptionProperties columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    ColumnEncryptionProperties columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    HashMap<ColumnPath, ColumnEncryptionProperties> columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    byte[] AADPrefix = root.getName().getBytes(StandardCharsets.UTF_8);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    StringKeyIdRetriever keyRetriever = new StringKeyIdRetriever();\n+    keyRetriever.putKey(\"fk\", footerKey);\n+    keyRetriever.putKey(\"ck0\", columnKey0);\n+    keyRetriever.putKey(\"ck1\", columnKey1);\n+    decryptionProperties = FileDecryptionProperties.builder()\n+        .withKeyRetriever(keyRetriever)\n+        .build();\n+    encryptionPropertiesList[2] = encryptionProperties;\n+    decryptionPropertiesList[2] = decryptionProperties;\n+\n+    // #3 GCM_CTR algorithm, non-uniform encryption, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withAlgorithm(ParquetCipher.AES_GCM_CTR_V1)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[3] = encryptionProperties;\n+    decryptionPropertiesList[3] = decryptionProperties; // Same decryption properties\n+\n+    // #4  Plaintext footer, default algorithm, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withPlaintextFooter()\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[4] = encryptionProperties;\n+    decryptionPropertiesList[4] = decryptionProperties; // Same decryption properties\n+\n+\n+    MessageType schema = parseMessageType(\n+        \"message test { \"\n+            + \"required binary binary_field; \"\n+            + \"required int32 int32_field; \"\n+            + \"required int64 int64_field; \"\n+            + \"required boolean boolean_field; \"\n+            + \"required float float_field; \"\n+            + \"required double double_field; \"\n+            + \"required fixed_len_byte_array(3) flba_field; \"\n+            + \"required int96 int96_field; \"\n+            + \"} \");\n+    GroupWriteSupport.setSchema(schema, conf);\n+    SimpleGroupFactory f = new SimpleGroupFactory(schema);\n+\n+    for (int encryptionMode = 0; encryptionMode < numberOfEncryptionModes; encryptionMode++) {\n+      System.out.println(\"MODE: \"+encryptionMode);\n+      \n+      Path file = new Path(root, \"m_\" + encryptionMode + \".parquet.encrypted\");\n+      ParquetWriter<Group> writer = new ParquetWriter<Group>(\n+          file,\n+          new GroupWriteSupport(),\n+          UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetWriter.DEFAULT_WRITER_VERSION, conf, \n+          encryptionPropertiesList[encryptionMode]);\n+      for (int i = 0; i < 1000; i++) {\n+        writer.write(\n+            f.newGroup()\n+            .append(\"binary_field\", \"test\" + i)\n+            .append(\"int32_field\", 32)\n+            .append(\"int64_field\", 64l)\n+            .append(\"boolean_field\", true)\n+            .append(\"float_field\", 1.0f)\n+            .append(\"double_field\", 2.0d)\n+            .append(\"flba_field\", \"foo\")\n+            .append(\"int96_field\", Binary.fromConstantByteArray(new byte[12])));\n+      }\n+      writer.close();\n+\n+      FileDecryptionProperties fileDecryptionProperties = decryptionPropertiesList[encryptionMode];\n+      ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file)\n+          .withDecryption(fileDecryptionProperties).withConf(conf).build();\n+      for (int i = 0; i < 1000; i++) {\n+        Group group = null;\n+        group= reader.read();\n+        assertEquals(\"test\" + i, group.getBinary(\"binary_field\", 0).toStringUsingUTF8());\n+        assertEquals(32, group.getInteger(\"int32_field\", 0));\n+        assertEquals(64l, group.getLong(\"int64_field\", 0));\n+        assertEquals(true, group.getBoolean(\"boolean_field\", 0));\n+        assertEquals(1.0f, group.getFloat(\"float_field\", 0), 0.001);\n+        assertEquals(2.0d, group.getDouble(\"double_field\", 0), 0.001);\n+        assertEquals(\"foo\", group.getBinary(\"flba_field\", 0).toStringUsingUTF8());\n+        assertEquals(Binary.fromConstantByteArray(new byte[12]),\n+            group.getInt96(\"int96_field\",0));\n+      }\n+      reader.close();\n+    }\n+    enforceEmptyDir(conf, root);\n+  }\n+\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 213}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTcyMTg2OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMzo1MTowOVrOGYzkEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxNTowMTozNlrOGY2bjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY2MzgyNw==", "bodyText": "I don't know if make to much sense to write the same data 1000 times. Most of our tests are working by generating random data in memory (e.g. a List<Group>) the write it and then test whether we can read back the same data we have in memory.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428663827", "createdAt": "2020-05-21T13:51:09Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Random;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.StringKeyIdRetriever;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+\n+\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestEncryption {\n+\n+  @Test\n+  public void test() throws Exception {\n+    Configuration conf = new Configuration();\n+    Path root = new Path(\"target/tests/TestEncryption/\");\n+    enforceEmptyDir(conf, root);\n+\n+    Random random = new Random();\n+    int numberOfEncryptionModes = 5;\n+    FileEncryptionProperties[] encryptionPropertiesList = new FileEncryptionProperties[numberOfEncryptionModes];\n+    FileDecryptionProperties[] decryptionPropertiesList = new FileDecryptionProperties[numberOfEncryptionModes];\n+\n+    // #0 Unencrypted - make sure null encryption properties don't break regular Parquet\n+    encryptionPropertiesList[0] = null;\n+    decryptionPropertiesList[0] = null;\n+\n+    // #1 Basic encryption setup\n+    byte[] encryptionKey = new byte[16];\n+    random.nextBytes(encryptionKey);\n+    FileEncryptionProperties encryptionProperties = FileEncryptionProperties.builder(encryptionKey).build();\n+    FileDecryptionProperties decryptionProperties = FileDecryptionProperties.builder().withFooterKey(encryptionKey).build();\n+    encryptionPropertiesList[1] = encryptionProperties;\n+    decryptionPropertiesList[1] = decryptionProperties;\n+\n+    // #2 Default algorithm, non-uniform encryption, key metadata, key retriever, AAD prefix\n+    byte[] footerKey = new byte[16];\n+    random.nextBytes(footerKey);\n+    byte[] columnKey0 = new byte[16];\n+    random.nextBytes(columnKey0);\n+    byte[] columnKey1 = new byte[16];\n+    random.nextBytes(columnKey1);\n+    ColumnEncryptionProperties columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    ColumnEncryptionProperties columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    HashMap<ColumnPath, ColumnEncryptionProperties> columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    byte[] AADPrefix = root.getName().getBytes(StandardCharsets.UTF_8);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    StringKeyIdRetriever keyRetriever = new StringKeyIdRetriever();\n+    keyRetriever.putKey(\"fk\", footerKey);\n+    keyRetriever.putKey(\"ck0\", columnKey0);\n+    keyRetriever.putKey(\"ck1\", columnKey1);\n+    decryptionProperties = FileDecryptionProperties.builder()\n+        .withKeyRetriever(keyRetriever)\n+        .build();\n+    encryptionPropertiesList[2] = encryptionProperties;\n+    decryptionPropertiesList[2] = decryptionProperties;\n+\n+    // #3 GCM_CTR algorithm, non-uniform encryption, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withAlgorithm(ParquetCipher.AES_GCM_CTR_V1)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[3] = encryptionProperties;\n+    decryptionPropertiesList[3] = decryptionProperties; // Same decryption properties\n+\n+    // #4  Plaintext footer, default algorithm, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withPlaintextFooter()\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[4] = encryptionProperties;\n+    decryptionPropertiesList[4] = decryptionProperties; // Same decryption properties\n+\n+\n+    MessageType schema = parseMessageType(\n+        \"message test { \"\n+            + \"required binary binary_field; \"\n+            + \"required int32 int32_field; \"\n+            + \"required int64 int64_field; \"\n+            + \"required boolean boolean_field; \"\n+            + \"required float float_field; \"\n+            + \"required double double_field; \"\n+            + \"required fixed_len_byte_array(3) flba_field; \"\n+            + \"required int96 int96_field; \"\n+            + \"} \");\n+    GroupWriteSupport.setSchema(schema, conf);\n+    SimpleGroupFactory f = new SimpleGroupFactory(schema);\n+\n+    for (int encryptionMode = 0; encryptionMode < numberOfEncryptionModes; encryptionMode++) {\n+      System.out.println(\"MODE: \"+encryptionMode);\n+      \n+      Path file = new Path(root, \"m_\" + encryptionMode + \".parquet.encrypted\");\n+      ParquetWriter<Group> writer = new ParquetWriter<Group>(\n+          file,\n+          new GroupWriteSupport(),\n+          UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetWriter.DEFAULT_WRITER_VERSION, conf, \n+          encryptionPropertiesList[encryptionMode]);\n+      for (int i = 0; i < 1000; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODcxMDc5OQ==", "bodyText": "we're working on a better unitest for encryption, in #782 . it should be possible to drop this one.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428710799", "createdAt": "2020-05-21T15:01:36Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Random;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.StringKeyIdRetriever;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+\n+\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestEncryption {\n+\n+  @Test\n+  public void test() throws Exception {\n+    Configuration conf = new Configuration();\n+    Path root = new Path(\"target/tests/TestEncryption/\");\n+    enforceEmptyDir(conf, root);\n+\n+    Random random = new Random();\n+    int numberOfEncryptionModes = 5;\n+    FileEncryptionProperties[] encryptionPropertiesList = new FileEncryptionProperties[numberOfEncryptionModes];\n+    FileDecryptionProperties[] decryptionPropertiesList = new FileDecryptionProperties[numberOfEncryptionModes];\n+\n+    // #0 Unencrypted - make sure null encryption properties don't break regular Parquet\n+    encryptionPropertiesList[0] = null;\n+    decryptionPropertiesList[0] = null;\n+\n+    // #1 Basic encryption setup\n+    byte[] encryptionKey = new byte[16];\n+    random.nextBytes(encryptionKey);\n+    FileEncryptionProperties encryptionProperties = FileEncryptionProperties.builder(encryptionKey).build();\n+    FileDecryptionProperties decryptionProperties = FileDecryptionProperties.builder().withFooterKey(encryptionKey).build();\n+    encryptionPropertiesList[1] = encryptionProperties;\n+    decryptionPropertiesList[1] = decryptionProperties;\n+\n+    // #2 Default algorithm, non-uniform encryption, key metadata, key retriever, AAD prefix\n+    byte[] footerKey = new byte[16];\n+    random.nextBytes(footerKey);\n+    byte[] columnKey0 = new byte[16];\n+    random.nextBytes(columnKey0);\n+    byte[] columnKey1 = new byte[16];\n+    random.nextBytes(columnKey1);\n+    ColumnEncryptionProperties columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    ColumnEncryptionProperties columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    HashMap<ColumnPath, ColumnEncryptionProperties> columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    byte[] AADPrefix = root.getName().getBytes(StandardCharsets.UTF_8);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    StringKeyIdRetriever keyRetriever = new StringKeyIdRetriever();\n+    keyRetriever.putKey(\"fk\", footerKey);\n+    keyRetriever.putKey(\"ck0\", columnKey0);\n+    keyRetriever.putKey(\"ck1\", columnKey1);\n+    decryptionProperties = FileDecryptionProperties.builder()\n+        .withKeyRetriever(keyRetriever)\n+        .build();\n+    encryptionPropertiesList[2] = encryptionProperties;\n+    decryptionPropertiesList[2] = decryptionProperties;\n+\n+    // #3 GCM_CTR algorithm, non-uniform encryption, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withAlgorithm(ParquetCipher.AES_GCM_CTR_V1)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[3] = encryptionProperties;\n+    decryptionPropertiesList[3] = decryptionProperties; // Same decryption properties\n+\n+    // #4  Plaintext footer, default algorithm, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withPlaintextFooter()\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[4] = encryptionProperties;\n+    decryptionPropertiesList[4] = decryptionProperties; // Same decryption properties\n+\n+\n+    MessageType schema = parseMessageType(\n+        \"message test { \"\n+            + \"required binary binary_field; \"\n+            + \"required int32 int32_field; \"\n+            + \"required int64 int64_field; \"\n+            + \"required boolean boolean_field; \"\n+            + \"required float float_field; \"\n+            + \"required double double_field; \"\n+            + \"required fixed_len_byte_array(3) flba_field; \"\n+            + \"required int96 int96_field; \"\n+            + \"} \");\n+    GroupWriteSupport.setSchema(schema, conf);\n+    SimpleGroupFactory f = new SimpleGroupFactory(schema);\n+\n+    for (int encryptionMode = 0; encryptionMode < numberOfEncryptionModes; encryptionMode++) {\n+      System.out.println(\"MODE: \"+encryptionMode);\n+      \n+      Path file = new Path(root, \"m_\" + encryptionMode + \".parquet.encrypted\");\n+      ParquetWriter<Group> writer = new ParquetWriter<Group>(\n+          file,\n+          new GroupWriteSupport(),\n+          UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetWriter.DEFAULT_WRITER_VERSION, conf, \n+          encryptionPropertiesList[encryptionMode]);\n+      for (int i = 0; i < 1000; i++) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY2MzgyNw=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 176}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2OTc4NDQyOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxNDowNzo1NVrOGY0MNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxNDowNzo1NVrOGY0MNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY3NDEwMw==", "bodyText": "I think, writing the actual value instead of referencing a java constant is more informative.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428674103", "createdAt": "2020-05-21T14:07:55Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -78,11 +78,22 @@\n     if (rowGroupOrdinal < 0) {\n       throw new IllegalArgumentException(\"Wrong row group ordinal: \" + rowGroupOrdinal);\n     }\n-    byte[] rowGroupOrdinalBytes = shortToBytesLE(rowGroupOrdinal);\n+    short shortRGOrdinal = (short) rowGroupOrdinal;\n+    if (shortRGOrdinal != rowGroupOrdinal) {\n+      throw new ParquetCryptoRuntimeException(\"Encrypted parquet files can't have \"\n+          + \"more than Short.MAX_VALUE row groups: \" + rowGroupOrdinal);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9761c39a774caac1bde5875b41e3368e745c0b4"}, "originalPosition": 17}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4754, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}