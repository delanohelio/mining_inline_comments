{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIxNTg4NDc5", "number": 793, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwODoyNTozNVrOD-3rqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwODo1ODo1M1rOEBqBRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MjUwNjAxOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/codec/ZstdCodec.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwODoyNTozNVrOGZO63A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwODoyNTozNVrOGZO63A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTExMjAyOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * one because it requires 1) to set up hadoop on local develop machine;\n          \n          \n            \n             * one because it requires 1) to set up hadoop on local development machine;", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r429112028", "createdAt": "2020-05-22T08:25:35Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/codec/ZstdCodec.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop.codec;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * ZSTD compression codec for Parquet.  We do not use the default hadoop\n+ * one because it requires 1) to set up hadoop on local develop machine;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c16585f6bcb747e661b6e077a9fe92fde57612d2"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MjUxMjQxOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/codec/ZstdCodec.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwODoyNzo0NFrOGZO-0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QxNjo0MzoxNFrOGZqTvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTExMzA0MQ==", "bodyText": "Please, also update the documentation in the README.", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r429113041", "createdAt": "2020-05-22T08:27:44Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/codec/ZstdCodec.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop.codec;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * ZSTD compression codec for Parquet.  We do not use the default hadoop\n+ * one because it requires 1) to set up hadoop on local develop machine;\n+ * 2) to upgrade hadoop to the newer version to have ZSTD support which is\n+ * more cumbersome than upgrading parquet version.\n+ *\n+ * This implementation relies on ZSTD JNI(https://github.com/luben/zstd-jni)\n+ * which is already a dependency for Parquet. ZSTD JNI ZstdOutputStream and\n+ * ZstdInputStream use Zstd internally. So no need to create compressor and\n+ * decompressor in ZstdCodec.\n+ */\n+public class ZstdCodec implements Configurable, CompressionCodec {\n+\n+  public final static String PARQUET_COMPRESS_ZSTD_LEVEL = \"parquet.compression.codec.zstd.level\";\n+  public final static int DEFAULT_PARQUET_COMPRESS_ZSTD_LEVEL = 3;\n+  public final static String PARQUET_COMPRESS_ZSTD_WORKERS = \"parquet.compression.codec.zstd.workers\";\n+  public final static int DEFAULTPARQUET_COMPRESS_ZSTD_WORKERS = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c16585f6bcb747e661b6e077a9fe92fde57612d2"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU2MDc2Nw==", "bodyText": "Sure", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r429560767", "createdAt": "2020-05-23T16:43:14Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/codec/ZstdCodec.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop.codec;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * ZSTD compression codec for Parquet.  We do not use the default hadoop\n+ * one because it requires 1) to set up hadoop on local develop machine;\n+ * 2) to upgrade hadoop to the newer version to have ZSTD support which is\n+ * more cumbersome than upgrading parquet version.\n+ *\n+ * This implementation relies on ZSTD JNI(https://github.com/luben/zstd-jni)\n+ * which is already a dependency for Parquet. ZSTD JNI ZstdOutputStream and\n+ * ZstdInputStream use Zstd internally. So no need to create compressor and\n+ * decompressor in ZstdCodec.\n+ */\n+public class ZstdCodec implements Configurable, CompressionCodec {\n+\n+  public final static String PARQUET_COMPRESS_ZSTD_LEVEL = \"parquet.compression.codec.zstd.level\";\n+  public final static int DEFAULT_PARQUET_COMPRESS_ZSTD_LEVEL = 3;\n+  public final static String PARQUET_COMPRESS_ZSTD_WORKERS = \"parquet.compression.codec.zstd.workers\";\n+  public final static int DEFAULTPARQUET_COMPRESS_ZSTD_WORKERS = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTExMzA0MQ=="}, "originalCommit": {"oid": "c16585f6bcb747e661b6e077a9fe92fde57612d2"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3Mjc3MTYzOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstdCodec.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwOTo1NjoxN1rOGZRhzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QxNjo0MzozMVrOGZqTzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1NDc2NA==", "bodyText": "I tried to find the code part where we set the hadoop conf to the codec but could not find it. Please, write a high level test where you set compression level and workers in the hadoop conf and executes a file write via e.g. an MR job.", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r429154764", "createdAt": "2020-05-22T09:56:17Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstdCodec.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.hadoop.codec.ZstdCodec;\n+import org.junit.Assert;\n+import org.junit.Test;  \n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.Random;\n+\n+public class TestZstdCodec {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c16585f6bcb747e661b6e077a9fe92fde57612d2"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU2MDc4Mw==", "bodyText": "Added test for conf setting with MR", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r429560783", "createdAt": "2020-05-23T16:43:31Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstdCodec.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.hadoop.codec.ZstdCodec;\n+import org.junit.Assert;\n+import org.junit.Test;  \n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.Random;\n+\n+public class TestZstdCodec {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1NDc2NA=="}, "originalCommit": {"oid": "c16585f6bcb747e661b6e077a9fe92fde57612d2"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4ODk5OTQ0OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxMDowNTozOFrOGbuW4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxODozNjoxOVrOGcn5hQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTcyNDI1OA==", "bodyText": "In markdown you have to do some extra if you want to force new lines. With a simple linebreak the two lines will be written together. There are some tricks here for enforcing line breaks.\n(You may check the rendered format on github).", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r431724258", "createdAt": "2020-05-28T10:05:38Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -324,9 +324,20 @@ ParquetInputFormat to materialize records. It should be a the descendant class o\n **Property:** `parquet.read.schema`  \n **Description:** The read projection schema.\n \n-\n ## Class: UnmaterializableRecordCounter\n \n **Property:** `parquet.read.bad.record.threshold`  \n **Description:** The percentage of bad records to tolerate.  \n **Default value:** `0`\n+\n+## Class: ZstandardCodec\n+\n+**Property:** `parquet.compression.codec.zstd.level`\n+**Description:** The compression level of ZSTD. The valid range is 1~22. Generally the higher compression level, the higher compression ratio can be achieved, but the writing time will be longer.     ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689b3489ec8177082c5248ecbf869d03eee62bf0"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjExODQ4Ng==", "bodyText": "Thanks for pointing out! I think it is OK to keep them in same line unless you have strong opinion we should have a separate line. I looked at the existing lines above and I see some of them are longer than mines.", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r432118486", "createdAt": "2020-05-28T20:56:31Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -324,9 +324,20 @@ ParquetInputFormat to materialize records. It should be a the descendant class o\n **Property:** `parquet.read.schema`  \n **Description:** The read projection schema.\n \n-\n ## Class: UnmaterializableRecordCounter\n \n **Property:** `parquet.read.bad.record.threshold`  \n **Description:** The percentage of bad records to tolerate.  \n **Default value:** `0`\n+\n+## Class: ZstandardCodec\n+\n+**Property:** `parquet.compression.codec.zstd.level`\n+**Description:** The compression level of ZSTD. The valid range is 1~22. Generally the higher compression level, the higher compression ratio can be achieved, but the writing time will be longer.     ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTcyNDI1OA=="}, "originalCommit": {"oid": "689b3489ec8177082c5248ecbf869d03eee62bf0"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjM3NjQwOQ==", "bodyText": "I was trying to say that Property, Description and Default value should be separate paragraphs. Currently they are not, the Description is rendered just after Property in the same line even if in the markdown they are separated. You should use one of the techniques described under the link to force the new line there.", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r432376409", "createdAt": "2020-05-29T09:47:51Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -324,9 +324,20 @@ ParquetInputFormat to materialize records. It should be a the descendant class o\n **Property:** `parquet.read.schema`  \n **Description:** The read projection schema.\n \n-\n ## Class: UnmaterializableRecordCounter\n \n **Property:** `parquet.read.bad.record.threshold`  \n **Description:** The percentage of bad records to tolerate.  \n **Default value:** `0`\n+\n+## Class: ZstandardCodec\n+\n+**Property:** `parquet.compression.codec.zstd.level`\n+**Description:** The compression level of ZSTD. The valid range is 1~22. Generally the higher compression level, the higher compression ratio can be achieved, but the writing time will be longer.     ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTcyNDI1OA=="}, "originalCommit": {"oid": "689b3489ec8177082c5248ecbf869d03eee62bf0"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjY2NzAxMw==", "bodyText": "I see. I added double space in the end of Property, Description sentences. I checked it in Intellij and it works. I also see the above sections used the trailing double space.", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r432667013", "createdAt": "2020-05-29T18:36:19Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -324,9 +324,20 @@ ParquetInputFormat to materialize records. It should be a the descendant class o\n **Property:** `parquet.read.schema`  \n **Description:** The read projection schema.\n \n-\n ## Class: UnmaterializableRecordCounter\n \n **Property:** `parquet.read.bad.record.threshold`  \n **Description:** The percentage of bad records to tolerate.  \n **Default value:** `0`\n+\n+## Class: ZstandardCodec\n+\n+**Property:** `parquet.compression.codec.zstd.level`\n+**Description:** The compression level of ZSTD. The valid range is 1~22. Generally the higher compression level, the higher compression ratio can be achieved, but the writing time will be longer.     ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTcyNDI1OA=="}, "originalCommit": {"oid": "689b3489ec8177082c5248ecbf869d03eee62bf0"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4OTA0NTcxOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxMDoxOTo0OVrOGbu0aA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNjoxOTo0NVrOGd5seg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTczMTgxNg==", "bodyText": "Thanks for creating an MR test. What I wanted to test with it is if the ZSTD related configuration takes place if set via the configuration. This is not tested currently. I wanted to test it because I am not sure if we have the code in place to pass configuration properties to the codecs.", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r431731816", "createdAt": "2020-05-28T10:19:49Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.RunningJob;\n+import org.apache.hadoop.mapred.TextInputFormat;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+import org.apache.parquet.hadoop.codec.ZstandardCodec;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.mapred.DeprecatedParquetOutputFormat;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.util.Random;\n+\n+public class TestZstandardCodec {\n+\n+  private final Path inputPath = new Path(\"src/test/java/org/apache/parquet/hadoop/example/TestInputOutputFormat.java\");\n+\n+  @Test\n+  public void testZstdCodec() throws IOException {\n+    ZstandardCodec codec = new ZstandardCodec();\n+    Configuration conf = new Configuration();\n+    int[] levels = {1, 4, 7, 10, 13, 16, 19, 22};\n+    int[] dataSizes = {0, 1, 10, 1024, 1024 * 1024};\n+\n+    for (int i = 0; i < levels.length; i++) {\n+      conf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, levels[i]);\n+      codec.setConf(conf);\n+      for (int j = 0; j < dataSizes.length; j++) {\n+        testZstd(codec, dataSizes[j]);\n+      }\n+    }\n+  }\n+\n+  private void testZstd(ZstandardCodec codec, int dataSize) throws IOException {\n+    byte[] data = new byte[dataSize];\n+    (new Random()).nextBytes(data);\n+    BytesInput compressedData = compress(codec,  BytesInput.from(data));\n+    BytesInput decompressedData = decompress(codec, compressedData, data.length);\n+    Assert.assertArrayEquals(data, decompressedData.toByteArray());\n+  }\n+\n+  private BytesInput compress(ZstandardCodec codec, BytesInput bytes) throws IOException {\n+    ByteArrayOutputStream compressedOutBuffer = new ByteArrayOutputStream((int)bytes.size());\n+    CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, null);\n+    bytes.writeAllTo(cos);\n+    cos.close();\n+    return BytesInput.from(compressedOutBuffer);\n+  }\n+\n+  private BytesInput decompress(ZstandardCodec codec, BytesInput bytes, int uncompressedSize) throws IOException {\n+    BytesInput decompressed;\n+    InputStream is = codec.createInputStream(bytes.toInputStream(), null);\n+    decompressed = BytesInput.from(is, uncompressedSize);\n+    is.close();\n+    return decompressed;\n+  }\n+\n+  @Test\n+  public void testZstdConfWithMr() throws Exception {\n+    JobConf jobConf = new JobConf();\n+    Configuration conf = new Configuration();\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 18);\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 4);\n+    RunningJob mapRedJob = runMapReduceJob(CompressionCodecName.ZSTD, jobConf, conf);\n+    assert(mapRedJob.isSuccessful());\n+  }\n+\n+  private RunningJob runMapReduceJob(CompressionCodecName codec, JobConf jobConf, Configuration conf) throws IOException, ClassNotFoundException, InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689b3489ec8177082c5248ecbf869d03eee62bf0"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE0MjQ3NA==", "bodyText": "I set breakpoint and checked it. But I don't know how to get back the level/workers at the MR job. Any ideal how to do that? I also verified it in the standalone Spark application. Do you think that is sufficient?", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r432142474", "createdAt": "2020-05-28T21:46:37Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.RunningJob;\n+import org.apache.hadoop.mapred.TextInputFormat;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+import org.apache.parquet.hadoop.codec.ZstandardCodec;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.mapred.DeprecatedParquetOutputFormat;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.util.Random;\n+\n+public class TestZstandardCodec {\n+\n+  private final Path inputPath = new Path(\"src/test/java/org/apache/parquet/hadoop/example/TestInputOutputFormat.java\");\n+\n+  @Test\n+  public void testZstdCodec() throws IOException {\n+    ZstandardCodec codec = new ZstandardCodec();\n+    Configuration conf = new Configuration();\n+    int[] levels = {1, 4, 7, 10, 13, 16, 19, 22};\n+    int[] dataSizes = {0, 1, 10, 1024, 1024 * 1024};\n+\n+    for (int i = 0; i < levels.length; i++) {\n+      conf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, levels[i]);\n+      codec.setConf(conf);\n+      for (int j = 0; j < dataSizes.length; j++) {\n+        testZstd(codec, dataSizes[j]);\n+      }\n+    }\n+  }\n+\n+  private void testZstd(ZstandardCodec codec, int dataSize) throws IOException {\n+    byte[] data = new byte[dataSize];\n+    (new Random()).nextBytes(data);\n+    BytesInput compressedData = compress(codec,  BytesInput.from(data));\n+    BytesInput decompressedData = decompress(codec, compressedData, data.length);\n+    Assert.assertArrayEquals(data, decompressedData.toByteArray());\n+  }\n+\n+  private BytesInput compress(ZstandardCodec codec, BytesInput bytes) throws IOException {\n+    ByteArrayOutputStream compressedOutBuffer = new ByteArrayOutputStream((int)bytes.size());\n+    CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, null);\n+    bytes.writeAllTo(cos);\n+    cos.close();\n+    return BytesInput.from(compressedOutBuffer);\n+  }\n+\n+  private BytesInput decompress(ZstandardCodec codec, BytesInput bytes, int uncompressedSize) throws IOException {\n+    BytesInput decompressed;\n+    InputStream is = codec.createInputStream(bytes.toInputStream(), null);\n+    decompressed = BytesInput.from(is, uncompressedSize);\n+    is.close();\n+    return decompressed;\n+  }\n+\n+  @Test\n+  public void testZstdConfWithMr() throws Exception {\n+    JobConf jobConf = new JobConf();\n+    Configuration conf = new Configuration();\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 18);\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 4);\n+    RunningJob mapRedJob = runMapReduceJob(CompressionCodecName.ZSTD, jobConf, conf);\n+    assert(mapRedJob.isSuccessful());\n+  }\n+\n+  private RunningJob runMapReduceJob(CompressionCodecName codec, JobConf jobConf, Configuration conf) throws IOException, ClassNotFoundException, InterruptedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTczMTgxNg=="}, "originalCommit": {"oid": "689b3489ec8177082c5248ecbf869d03eee62bf0"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjM4MTcyOA==", "bodyText": "Maybe, if we create two parquet files with different levels (e.g. 1 and 22) but with the exact same data we can expect that the larger level will generate smaller file?\nI just want to ensure that the properties really arrive to the codec. If you think the related test requires too much effort or even not feasible I am fine if you can point me to the code path where the properties are passed through to the codecs.", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r432381728", "createdAt": "2020-05-29T09:57:49Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.RunningJob;\n+import org.apache.hadoop.mapred.TextInputFormat;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+import org.apache.parquet.hadoop.codec.ZstandardCodec;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.mapred.DeprecatedParquetOutputFormat;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.util.Random;\n+\n+public class TestZstandardCodec {\n+\n+  private final Path inputPath = new Path(\"src/test/java/org/apache/parquet/hadoop/example/TestInputOutputFormat.java\");\n+\n+  @Test\n+  public void testZstdCodec() throws IOException {\n+    ZstandardCodec codec = new ZstandardCodec();\n+    Configuration conf = new Configuration();\n+    int[] levels = {1, 4, 7, 10, 13, 16, 19, 22};\n+    int[] dataSizes = {0, 1, 10, 1024, 1024 * 1024};\n+\n+    for (int i = 0; i < levels.length; i++) {\n+      conf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, levels[i]);\n+      codec.setConf(conf);\n+      for (int j = 0; j < dataSizes.length; j++) {\n+        testZstd(codec, dataSizes[j]);\n+      }\n+    }\n+  }\n+\n+  private void testZstd(ZstandardCodec codec, int dataSize) throws IOException {\n+    byte[] data = new byte[dataSize];\n+    (new Random()).nextBytes(data);\n+    BytesInput compressedData = compress(codec,  BytesInput.from(data));\n+    BytesInput decompressedData = decompress(codec, compressedData, data.length);\n+    Assert.assertArrayEquals(data, decompressedData.toByteArray());\n+  }\n+\n+  private BytesInput compress(ZstandardCodec codec, BytesInput bytes) throws IOException {\n+    ByteArrayOutputStream compressedOutBuffer = new ByteArrayOutputStream((int)bytes.size());\n+    CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, null);\n+    bytes.writeAllTo(cos);\n+    cos.close();\n+    return BytesInput.from(compressedOutBuffer);\n+  }\n+\n+  private BytesInput decompress(ZstandardCodec codec, BytesInput bytes, int uncompressedSize) throws IOException {\n+    BytesInput decompressed;\n+    InputStream is = codec.createInputStream(bytes.toInputStream(), null);\n+    decompressed = BytesInput.from(is, uncompressedSize);\n+    is.close();\n+    return decompressed;\n+  }\n+\n+  @Test\n+  public void testZstdConfWithMr() throws Exception {\n+    JobConf jobConf = new JobConf();\n+    Configuration conf = new Configuration();\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 18);\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 4);\n+    RunningJob mapRedJob = runMapReduceJob(CompressionCodecName.ZSTD, jobConf, conf);\n+    assert(mapRedJob.isSuccessful());\n+  }\n+\n+  private RunningJob runMapReduceJob(CompressionCodecName codec, JobConf jobConf, Configuration conf) throws IOException, ClassNotFoundException, InterruptedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTczMTgxNg=="}, "originalCommit": {"oid": "689b3489ec8177082c5248ecbf869d03eee62bf0"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjcwOTI1OA==", "bodyText": "I like the idea to compare the two files with different compression levels. Only thing is that it is not 100% guarantee the higher level always gets higher compression ratio although the chance of that is very low. In our test data, it seems it is OK though. So I will just go ahead to add the file size comparison.", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r432709258", "createdAt": "2020-05-29T20:04:03Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.RunningJob;\n+import org.apache.hadoop.mapred.TextInputFormat;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+import org.apache.parquet.hadoop.codec.ZstandardCodec;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.mapred.DeprecatedParquetOutputFormat;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.util.Random;\n+\n+public class TestZstandardCodec {\n+\n+  private final Path inputPath = new Path(\"src/test/java/org/apache/parquet/hadoop/example/TestInputOutputFormat.java\");\n+\n+  @Test\n+  public void testZstdCodec() throws IOException {\n+    ZstandardCodec codec = new ZstandardCodec();\n+    Configuration conf = new Configuration();\n+    int[] levels = {1, 4, 7, 10, 13, 16, 19, 22};\n+    int[] dataSizes = {0, 1, 10, 1024, 1024 * 1024};\n+\n+    for (int i = 0; i < levels.length; i++) {\n+      conf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, levels[i]);\n+      codec.setConf(conf);\n+      for (int j = 0; j < dataSizes.length; j++) {\n+        testZstd(codec, dataSizes[j]);\n+      }\n+    }\n+  }\n+\n+  private void testZstd(ZstandardCodec codec, int dataSize) throws IOException {\n+    byte[] data = new byte[dataSize];\n+    (new Random()).nextBytes(data);\n+    BytesInput compressedData = compress(codec,  BytesInput.from(data));\n+    BytesInput decompressedData = decompress(codec, compressedData, data.length);\n+    Assert.assertArrayEquals(data, decompressedData.toByteArray());\n+  }\n+\n+  private BytesInput compress(ZstandardCodec codec, BytesInput bytes) throws IOException {\n+    ByteArrayOutputStream compressedOutBuffer = new ByteArrayOutputStream((int)bytes.size());\n+    CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, null);\n+    bytes.writeAllTo(cos);\n+    cos.close();\n+    return BytesInput.from(compressedOutBuffer);\n+  }\n+\n+  private BytesInput decompress(ZstandardCodec codec, BytesInput bytes, int uncompressedSize) throws IOException {\n+    BytesInput decompressed;\n+    InputStream is = codec.createInputStream(bytes.toInputStream(), null);\n+    decompressed = BytesInput.from(is, uncompressedSize);\n+    is.close();\n+    return decompressed;\n+  }\n+\n+  @Test\n+  public void testZstdConfWithMr() throws Exception {\n+    JobConf jobConf = new JobConf();\n+    Configuration conf = new Configuration();\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 18);\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 4);\n+    RunningJob mapRedJob = runMapReduceJob(CompressionCodecName.ZSTD, jobConf, conf);\n+    assert(mapRedJob.isSuccessful());\n+  }\n+\n+  private RunningJob runMapReduceJob(CompressionCodecName codec, JobConf jobConf, Configuration conf) throws IOException, ClassNotFoundException, InterruptedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTczMTgxNg=="}, "originalCommit": {"oid": "689b3489ec8177082c5248ecbf869d03eee62bf0"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzY3NTI1Mg==", "bodyText": "Yes, I know, there is no 100% guarantee in normal circumstances. But if you write the test in a deterministic way (as it should be by e.g. using a fix seed for random generation) then without changing the data and the compression codec it shall be fine. I worth a comment, though about it is not the nicest way of testing the work of the properties but currently that's seems to be the only one.", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r433675252", "createdAt": "2020-06-02T07:30:51Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.RunningJob;\n+import org.apache.hadoop.mapred.TextInputFormat;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+import org.apache.parquet.hadoop.codec.ZstandardCodec;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.mapred.DeprecatedParquetOutputFormat;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.util.Random;\n+\n+public class TestZstandardCodec {\n+\n+  private final Path inputPath = new Path(\"src/test/java/org/apache/parquet/hadoop/example/TestInputOutputFormat.java\");\n+\n+  @Test\n+  public void testZstdCodec() throws IOException {\n+    ZstandardCodec codec = new ZstandardCodec();\n+    Configuration conf = new Configuration();\n+    int[] levels = {1, 4, 7, 10, 13, 16, 19, 22};\n+    int[] dataSizes = {0, 1, 10, 1024, 1024 * 1024};\n+\n+    for (int i = 0; i < levels.length; i++) {\n+      conf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, levels[i]);\n+      codec.setConf(conf);\n+      for (int j = 0; j < dataSizes.length; j++) {\n+        testZstd(codec, dataSizes[j]);\n+      }\n+    }\n+  }\n+\n+  private void testZstd(ZstandardCodec codec, int dataSize) throws IOException {\n+    byte[] data = new byte[dataSize];\n+    (new Random()).nextBytes(data);\n+    BytesInput compressedData = compress(codec,  BytesInput.from(data));\n+    BytesInput decompressedData = decompress(codec, compressedData, data.length);\n+    Assert.assertArrayEquals(data, decompressedData.toByteArray());\n+  }\n+\n+  private BytesInput compress(ZstandardCodec codec, BytesInput bytes) throws IOException {\n+    ByteArrayOutputStream compressedOutBuffer = new ByteArrayOutputStream((int)bytes.size());\n+    CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, null);\n+    bytes.writeAllTo(cos);\n+    cos.close();\n+    return BytesInput.from(compressedOutBuffer);\n+  }\n+\n+  private BytesInput decompress(ZstandardCodec codec, BytesInput bytes, int uncompressedSize) throws IOException {\n+    BytesInput decompressed;\n+    InputStream is = codec.createInputStream(bytes.toInputStream(), null);\n+    decompressed = BytesInput.from(is, uncompressedSize);\n+    is.close();\n+    return decompressed;\n+  }\n+\n+  @Test\n+  public void testZstdConfWithMr() throws Exception {\n+    JobConf jobConf = new JobConf();\n+    Configuration conf = new Configuration();\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 18);\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 4);\n+    RunningJob mapRedJob = runMapReduceJob(CompressionCodecName.ZSTD, jobConf, conf);\n+    assert(mapRedJob.isSuccessful());\n+  }\n+\n+  private RunningJob runMapReduceJob(CompressionCodecName codec, JobConf jobConf, Configuration conf) throws IOException, ClassNotFoundException, InterruptedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTczMTgxNg=="}, "originalCommit": {"oid": "689b3489ec8177082c5248ecbf869d03eee62bf0"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDAwNzE2Mg==", "bodyText": "Thanks.", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r434007162", "createdAt": "2020-06-02T16:19:45Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.RunningJob;\n+import org.apache.hadoop.mapred.TextInputFormat;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+import org.apache.parquet.hadoop.codec.ZstandardCodec;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.mapred.DeprecatedParquetOutputFormat;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.util.Random;\n+\n+public class TestZstandardCodec {\n+\n+  private final Path inputPath = new Path(\"src/test/java/org/apache/parquet/hadoop/example/TestInputOutputFormat.java\");\n+\n+  @Test\n+  public void testZstdCodec() throws IOException {\n+    ZstandardCodec codec = new ZstandardCodec();\n+    Configuration conf = new Configuration();\n+    int[] levels = {1, 4, 7, 10, 13, 16, 19, 22};\n+    int[] dataSizes = {0, 1, 10, 1024, 1024 * 1024};\n+\n+    for (int i = 0; i < levels.length; i++) {\n+      conf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, levels[i]);\n+      codec.setConf(conf);\n+      for (int j = 0; j < dataSizes.length; j++) {\n+        testZstd(codec, dataSizes[j]);\n+      }\n+    }\n+  }\n+\n+  private void testZstd(ZstandardCodec codec, int dataSize) throws IOException {\n+    byte[] data = new byte[dataSize];\n+    (new Random()).nextBytes(data);\n+    BytesInput compressedData = compress(codec,  BytesInput.from(data));\n+    BytesInput decompressedData = decompress(codec, compressedData, data.length);\n+    Assert.assertArrayEquals(data, decompressedData.toByteArray());\n+  }\n+\n+  private BytesInput compress(ZstandardCodec codec, BytesInput bytes) throws IOException {\n+    ByteArrayOutputStream compressedOutBuffer = new ByteArrayOutputStream((int)bytes.size());\n+    CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, null);\n+    bytes.writeAllTo(cos);\n+    cos.close();\n+    return BytesInput.from(compressedOutBuffer);\n+  }\n+\n+  private BytesInput decompress(ZstandardCodec codec, BytesInput bytes, int uncompressedSize) throws IOException {\n+    BytesInput decompressed;\n+    InputStream is = codec.createInputStream(bytes.toInputStream(), null);\n+    decompressed = BytesInput.from(is, uncompressedSize);\n+    is.close();\n+    return decompressed;\n+  }\n+\n+  @Test\n+  public void testZstdConfWithMr() throws Exception {\n+    JobConf jobConf = new JobConf();\n+    Configuration conf = new Configuration();\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 18);\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 4);\n+    RunningJob mapRedJob = runMapReduceJob(CompressionCodecName.ZSTD, jobConf, conf);\n+    assert(mapRedJob.isSuccessful());\n+  }\n+\n+  private RunningJob runMapReduceJob(CompressionCodecName codec, JobConf jobConf, Configuration conf) throws IOException, ClassNotFoundException, InterruptedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTczMTgxNg=="}, "originalCommit": {"oid": "689b3489ec8177082c5248ecbf869d03eee62bf0"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY4OTA0OTE3OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxMDoyMDo0OVrOGbu2jQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMDo1Njo1MFrOGcGbkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTczMjM2NQ==", "bodyText": "You are setting the level twice (and the workers are missing).", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r431732365", "createdAt": "2020-05-28T10:20:49Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.RunningJob;\n+import org.apache.hadoop.mapred.TextInputFormat;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+import org.apache.parquet.hadoop.codec.ZstandardCodec;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.mapred.DeprecatedParquetOutputFormat;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.util.Random;\n+\n+public class TestZstandardCodec {\n+\n+  private final Path inputPath = new Path(\"src/test/java/org/apache/parquet/hadoop/example/TestInputOutputFormat.java\");\n+\n+  @Test\n+  public void testZstdCodec() throws IOException {\n+    ZstandardCodec codec = new ZstandardCodec();\n+    Configuration conf = new Configuration();\n+    int[] levels = {1, 4, 7, 10, 13, 16, 19, 22};\n+    int[] dataSizes = {0, 1, 10, 1024, 1024 * 1024};\n+\n+    for (int i = 0; i < levels.length; i++) {\n+      conf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, levels[i]);\n+      codec.setConf(conf);\n+      for (int j = 0; j < dataSizes.length; j++) {\n+        testZstd(codec, dataSizes[j]);\n+      }\n+    }\n+  }\n+\n+  private void testZstd(ZstandardCodec codec, int dataSize) throws IOException {\n+    byte[] data = new byte[dataSize];\n+    (new Random()).nextBytes(data);\n+    BytesInput compressedData = compress(codec,  BytesInput.from(data));\n+    BytesInput decompressedData = decompress(codec, compressedData, data.length);\n+    Assert.assertArrayEquals(data, decompressedData.toByteArray());\n+  }\n+\n+  private BytesInput compress(ZstandardCodec codec, BytesInput bytes) throws IOException {\n+    ByteArrayOutputStream compressedOutBuffer = new ByteArrayOutputStream((int)bytes.size());\n+    CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, null);\n+    bytes.writeAllTo(cos);\n+    cos.close();\n+    return BytesInput.from(compressedOutBuffer);\n+  }\n+\n+  private BytesInput decompress(ZstandardCodec codec, BytesInput bytes, int uncompressedSize) throws IOException {\n+    BytesInput decompressed;\n+    InputStream is = codec.createInputStream(bytes.toInputStream(), null);\n+    decompressed = BytesInput.from(is, uncompressedSize);\n+    is.close();\n+    return decompressed;\n+  }\n+\n+  @Test\n+  public void testZstdConfWithMr() throws Exception {\n+    JobConf jobConf = new JobConf();\n+    Configuration conf = new Configuration();\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 18);\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 4);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "689b3489ec8177082c5248ecbf869d03eee62bf0"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjExODY3Mw==", "bodyText": "Good catch", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r432118673", "createdAt": "2020-05-28T20:56:50Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.RunningJob;\n+import org.apache.hadoop.mapred.TextInputFormat;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+import org.apache.parquet.hadoop.codec.ZstandardCodec;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.mapred.DeprecatedParquetOutputFormat;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.util.Random;\n+\n+public class TestZstandardCodec {\n+\n+  private final Path inputPath = new Path(\"src/test/java/org/apache/parquet/hadoop/example/TestInputOutputFormat.java\");\n+\n+  @Test\n+  public void testZstdCodec() throws IOException {\n+    ZstandardCodec codec = new ZstandardCodec();\n+    Configuration conf = new Configuration();\n+    int[] levels = {1, 4, 7, 10, 13, 16, 19, 22};\n+    int[] dataSizes = {0, 1, 10, 1024, 1024 * 1024};\n+\n+    for (int i = 0; i < levels.length; i++) {\n+      conf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, levels[i]);\n+      codec.setConf(conf);\n+      for (int j = 0; j < dataSizes.length; j++) {\n+        testZstd(codec, dataSizes[j]);\n+      }\n+    }\n+  }\n+\n+  private void testZstd(ZstandardCodec codec, int dataSize) throws IOException {\n+    byte[] data = new byte[dataSize];\n+    (new Random()).nextBytes(data);\n+    BytesInput compressedData = compress(codec,  BytesInput.from(data));\n+    BytesInput decompressedData = decompress(codec, compressedData, data.length);\n+    Assert.assertArrayEquals(data, decompressedData.toByteArray());\n+  }\n+\n+  private BytesInput compress(ZstandardCodec codec, BytesInput bytes) throws IOException {\n+    ByteArrayOutputStream compressedOutBuffer = new ByteArrayOutputStream((int)bytes.size());\n+    CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, null);\n+    bytes.writeAllTo(cos);\n+    cos.close();\n+    return BytesInput.from(compressedOutBuffer);\n+  }\n+\n+  private BytesInput decompress(ZstandardCodec codec, BytesInput bytes, int uncompressedSize) throws IOException {\n+    BytesInput decompressed;\n+    InputStream is = codec.createInputStream(bytes.toInputStream(), null);\n+    decompressed = BytesInput.from(is, uncompressedSize);\n+    is.close();\n+    return decompressed;\n+  }\n+\n+  @Test\n+  public void testZstdConfWithMr() throws Exception {\n+    JobConf jobConf = new JobConf();\n+    Configuration conf = new Configuration();\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 18);\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, 4);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTczMjM2NQ=="}, "originalCommit": {"oid": "689b3489ec8177082c5248ecbf869d03eee62bf0"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMTcxNDE2OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwODo1NjowMFrOGdoZhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNjoxMDoyNVrOGd5Uhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzcyMzc4Mw==", "bodyText": "Please use the JUnit framework assert functions instead of the assert keyword. The assert keyword is not for unit testing and it is not 100% guaranteed that the assertions are enabled during testing.", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r433723783", "createdAt": "2020-06-02T08:56:00Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.RunningJob;\n+import org.apache.hadoop.mapred.TextInputFormat;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+import org.apache.parquet.hadoop.codec.ZstandardCodec;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.mapred.DeprecatedParquetOutputFormat;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.util.Random;\n+\n+public class TestZstandardCodec {\n+\n+  private final Path inputPath = new Path(\"src/test/java/org/apache/parquet/hadoop/example/TestInputOutputFormat.java\");\n+\n+  @Test\n+  public void testZstdCodec() throws IOException {\n+    ZstandardCodec codec = new ZstandardCodec();\n+    Configuration conf = new Configuration();\n+    int[] levels = {1, 4, 7, 10, 13, 16, 19, 22};\n+    int[] dataSizes = {0, 1, 10, 1024, 1024 * 1024};\n+\n+    for (int i = 0; i < levels.length; i++) {\n+      conf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, levels[i]);\n+      codec.setConf(conf);\n+      for (int j = 0; j < dataSizes.length; j++) {\n+        testZstd(codec, dataSizes[j]);\n+      }\n+    }\n+  }\n+\n+  private void testZstd(ZstandardCodec codec, int dataSize) throws IOException {\n+    byte[] data = new byte[dataSize];\n+    (new Random()).nextBytes(data);\n+    BytesInput compressedData = compress(codec,  BytesInput.from(data));\n+    BytesInput decompressedData = decompress(codec, compressedData, data.length);\n+    Assert.assertArrayEquals(data, decompressedData.toByteArray());\n+  }\n+\n+  private BytesInput compress(ZstandardCodec codec, BytesInput bytes) throws IOException {\n+    ByteArrayOutputStream compressedOutBuffer = new ByteArrayOutputStream((int)bytes.size());\n+    CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, null);\n+    bytes.writeAllTo(cos);\n+    cos.close();\n+    return BytesInput.from(compressedOutBuffer);\n+  }\n+\n+  private BytesInput decompress(ZstandardCodec codec, BytesInput bytes, int uncompressedSize) throws IOException {\n+    BytesInput decompressed;\n+    InputStream is = codec.createInputStream(bytes.toInputStream(), null);\n+    decompressed = BytesInput.from(is, uncompressedSize);\n+    is.close();\n+    return decompressed;\n+  }\n+\n+  @Test\n+  public void testZstdConfWithMr() throws Exception {\n+    long fileSizeLowLevel = runMrWithConf(1);\n+    // Clear the cache so that a new codec can be created with new configuration\n+    CodecFactory.CODEC_BY_NAME.clear();\n+    long fileSizeHighLevel = runMrWithConf(22);\n+    assert (fileSizeLowLevel > fileSizeHighLevel);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7204a18dc8db359cf5412ece717eaa3fcbc9f990"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDAwMTAzMQ==", "bodyText": "Sounds good!", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r434001031", "createdAt": "2020-06-02T16:10:25Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.RunningJob;\n+import org.apache.hadoop.mapred.TextInputFormat;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+import org.apache.parquet.hadoop.codec.ZstandardCodec;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.mapred.DeprecatedParquetOutputFormat;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.util.Random;\n+\n+public class TestZstandardCodec {\n+\n+  private final Path inputPath = new Path(\"src/test/java/org/apache/parquet/hadoop/example/TestInputOutputFormat.java\");\n+\n+  @Test\n+  public void testZstdCodec() throws IOException {\n+    ZstandardCodec codec = new ZstandardCodec();\n+    Configuration conf = new Configuration();\n+    int[] levels = {1, 4, 7, 10, 13, 16, 19, 22};\n+    int[] dataSizes = {0, 1, 10, 1024, 1024 * 1024};\n+\n+    for (int i = 0; i < levels.length; i++) {\n+      conf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, levels[i]);\n+      codec.setConf(conf);\n+      for (int j = 0; j < dataSizes.length; j++) {\n+        testZstd(codec, dataSizes[j]);\n+      }\n+    }\n+  }\n+\n+  private void testZstd(ZstandardCodec codec, int dataSize) throws IOException {\n+    byte[] data = new byte[dataSize];\n+    (new Random()).nextBytes(data);\n+    BytesInput compressedData = compress(codec,  BytesInput.from(data));\n+    BytesInput decompressedData = decompress(codec, compressedData, data.length);\n+    Assert.assertArrayEquals(data, decompressedData.toByteArray());\n+  }\n+\n+  private BytesInput compress(ZstandardCodec codec, BytesInput bytes) throws IOException {\n+    ByteArrayOutputStream compressedOutBuffer = new ByteArrayOutputStream((int)bytes.size());\n+    CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, null);\n+    bytes.writeAllTo(cos);\n+    cos.close();\n+    return BytesInput.from(compressedOutBuffer);\n+  }\n+\n+  private BytesInput decompress(ZstandardCodec codec, BytesInput bytes, int uncompressedSize) throws IOException {\n+    BytesInput decompressed;\n+    InputStream is = codec.createInputStream(bytes.toInputStream(), null);\n+    decompressed = BytesInput.from(is, uncompressedSize);\n+    is.close();\n+    return decompressed;\n+  }\n+\n+  @Test\n+  public void testZstdConfWithMr() throws Exception {\n+    long fileSizeLowLevel = runMrWithConf(1);\n+    // Clear the cache so that a new codec can be created with new configuration\n+    CodecFactory.CODEC_BY_NAME.clear();\n+    long fileSizeHighLevel = runMrWithConf(22);\n+    assert (fileSizeLowLevel > fileSizeHighLevel);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzcyMzc4Mw=="}, "originalCommit": {"oid": "7204a18dc8db359cf5412ece717eaa3fcbc9f990"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMTcxNDgwOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwODo1NjowOVrOGdoZ-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNjoxMDozNFrOGd5U_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzcyMzg5Nw==", "bodyText": "Same as above", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r433723897", "createdAt": "2020-06-02T08:56:09Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.RunningJob;\n+import org.apache.hadoop.mapred.TextInputFormat;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+import org.apache.parquet.hadoop.codec.ZstandardCodec;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.mapred.DeprecatedParquetOutputFormat;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.util.Random;\n+\n+public class TestZstandardCodec {\n+\n+  private final Path inputPath = new Path(\"src/test/java/org/apache/parquet/hadoop/example/TestInputOutputFormat.java\");\n+\n+  @Test\n+  public void testZstdCodec() throws IOException {\n+    ZstandardCodec codec = new ZstandardCodec();\n+    Configuration conf = new Configuration();\n+    int[] levels = {1, 4, 7, 10, 13, 16, 19, 22};\n+    int[] dataSizes = {0, 1, 10, 1024, 1024 * 1024};\n+\n+    for (int i = 0; i < levels.length; i++) {\n+      conf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, levels[i]);\n+      codec.setConf(conf);\n+      for (int j = 0; j < dataSizes.length; j++) {\n+        testZstd(codec, dataSizes[j]);\n+      }\n+    }\n+  }\n+\n+  private void testZstd(ZstandardCodec codec, int dataSize) throws IOException {\n+    byte[] data = new byte[dataSize];\n+    (new Random()).nextBytes(data);\n+    BytesInput compressedData = compress(codec,  BytesInput.from(data));\n+    BytesInput decompressedData = decompress(codec, compressedData, data.length);\n+    Assert.assertArrayEquals(data, decompressedData.toByteArray());\n+  }\n+\n+  private BytesInput compress(ZstandardCodec codec, BytesInput bytes) throws IOException {\n+    ByteArrayOutputStream compressedOutBuffer = new ByteArrayOutputStream((int)bytes.size());\n+    CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, null);\n+    bytes.writeAllTo(cos);\n+    cos.close();\n+    return BytesInput.from(compressedOutBuffer);\n+  }\n+\n+  private BytesInput decompress(ZstandardCodec codec, BytesInput bytes, int uncompressedSize) throws IOException {\n+    BytesInput decompressed;\n+    InputStream is = codec.createInputStream(bytes.toInputStream(), null);\n+    decompressed = BytesInput.from(is, uncompressedSize);\n+    is.close();\n+    return decompressed;\n+  }\n+\n+  @Test\n+  public void testZstdConfWithMr() throws Exception {\n+    long fileSizeLowLevel = runMrWithConf(1);\n+    // Clear the cache so that a new codec can be created with new configuration\n+    CodecFactory.CODEC_BY_NAME.clear();\n+    long fileSizeHighLevel = runMrWithConf(22);\n+    assert (fileSizeLowLevel > fileSizeHighLevel);\n+  }\n+\n+  private long runMrWithConf(int level) throws Exception {\n+    JobConf jobConf = new JobConf();\n+    Configuration conf = new Configuration();\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, level);\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_WORKERS, 4);\n+    Path path = new Path(Files.createTempDirectory(\"zstd\" + level).toAbsolutePath().toString());\n+    RunningJob mapRedJob = runMapReduceJob(CompressionCodecName.ZSTD, jobConf, conf, path);\n+    assert(mapRedJob.isSuccessful());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7204a18dc8db359cf5412ece717eaa3fcbc9f990"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDAwMTE0OA==", "bodyText": "fixed", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r434001148", "createdAt": "2020-06-02T16:10:34Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.RunningJob;\n+import org.apache.hadoop.mapred.TextInputFormat;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+import org.apache.parquet.hadoop.codec.ZstandardCodec;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.mapred.DeprecatedParquetOutputFormat;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.util.Random;\n+\n+public class TestZstandardCodec {\n+\n+  private final Path inputPath = new Path(\"src/test/java/org/apache/parquet/hadoop/example/TestInputOutputFormat.java\");\n+\n+  @Test\n+  public void testZstdCodec() throws IOException {\n+    ZstandardCodec codec = new ZstandardCodec();\n+    Configuration conf = new Configuration();\n+    int[] levels = {1, 4, 7, 10, 13, 16, 19, 22};\n+    int[] dataSizes = {0, 1, 10, 1024, 1024 * 1024};\n+\n+    for (int i = 0; i < levels.length; i++) {\n+      conf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, levels[i]);\n+      codec.setConf(conf);\n+      for (int j = 0; j < dataSizes.length; j++) {\n+        testZstd(codec, dataSizes[j]);\n+      }\n+    }\n+  }\n+\n+  private void testZstd(ZstandardCodec codec, int dataSize) throws IOException {\n+    byte[] data = new byte[dataSize];\n+    (new Random()).nextBytes(data);\n+    BytesInput compressedData = compress(codec,  BytesInput.from(data));\n+    BytesInput decompressedData = decompress(codec, compressedData, data.length);\n+    Assert.assertArrayEquals(data, decompressedData.toByteArray());\n+  }\n+\n+  private BytesInput compress(ZstandardCodec codec, BytesInput bytes) throws IOException {\n+    ByteArrayOutputStream compressedOutBuffer = new ByteArrayOutputStream((int)bytes.size());\n+    CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, null);\n+    bytes.writeAllTo(cos);\n+    cos.close();\n+    return BytesInput.from(compressedOutBuffer);\n+  }\n+\n+  private BytesInput decompress(ZstandardCodec codec, BytesInput bytes, int uncompressedSize) throws IOException {\n+    BytesInput decompressed;\n+    InputStream is = codec.createInputStream(bytes.toInputStream(), null);\n+    decompressed = BytesInput.from(is, uncompressedSize);\n+    is.close();\n+    return decompressed;\n+  }\n+\n+  @Test\n+  public void testZstdConfWithMr() throws Exception {\n+    long fileSizeLowLevel = runMrWithConf(1);\n+    // Clear the cache so that a new codec can be created with new configuration\n+    CodecFactory.CODEC_BY_NAME.clear();\n+    long fileSizeHighLevel = runMrWithConf(22);\n+    assert (fileSizeLowLevel > fileSizeHighLevel);\n+  }\n+\n+  private long runMrWithConf(int level) throws Exception {\n+    JobConf jobConf = new JobConf();\n+    Configuration conf = new Configuration();\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, level);\n+    jobConf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_WORKERS, 4);\n+    Path path = new Path(Files.createTempDirectory(\"zstd\" + level).toAbsolutePath().toString());\n+    RunningJob mapRedJob = runMapReduceJob(CompressionCodecName.ZSTD, jobConf, conf, path);\n+    assert(mapRedJob.isSuccessful());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzcyMzg5Nw=="}, "originalCommit": {"oid": "7204a18dc8db359cf5412ece717eaa3fcbc9f990"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMTcyNDg0OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwODo1ODo1M1rOGdogaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNjoxMjo1NlrOGd5baQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzcyNTU0NQ==", "bodyText": "Please, add some comments that the intent of this test is to verify that the properties are passed through from the config to the codec.", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r433725545", "createdAt": "2020-06-02T08:58:53Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.RunningJob;\n+import org.apache.hadoop.mapred.TextInputFormat;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+import org.apache.parquet.hadoop.codec.ZstandardCodec;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.mapred.DeprecatedParquetOutputFormat;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.util.Random;\n+\n+public class TestZstandardCodec {\n+\n+  private final Path inputPath = new Path(\"src/test/java/org/apache/parquet/hadoop/example/TestInputOutputFormat.java\");\n+\n+  @Test\n+  public void testZstdCodec() throws IOException {\n+    ZstandardCodec codec = new ZstandardCodec();\n+    Configuration conf = new Configuration();\n+    int[] levels = {1, 4, 7, 10, 13, 16, 19, 22};\n+    int[] dataSizes = {0, 1, 10, 1024, 1024 * 1024};\n+\n+    for (int i = 0; i < levels.length; i++) {\n+      conf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, levels[i]);\n+      codec.setConf(conf);\n+      for (int j = 0; j < dataSizes.length; j++) {\n+        testZstd(codec, dataSizes[j]);\n+      }\n+    }\n+  }\n+\n+  private void testZstd(ZstandardCodec codec, int dataSize) throws IOException {\n+    byte[] data = new byte[dataSize];\n+    (new Random()).nextBytes(data);\n+    BytesInput compressedData = compress(codec,  BytesInput.from(data));\n+    BytesInput decompressedData = decompress(codec, compressedData, data.length);\n+    Assert.assertArrayEquals(data, decompressedData.toByteArray());\n+  }\n+\n+  private BytesInput compress(ZstandardCodec codec, BytesInput bytes) throws IOException {\n+    ByteArrayOutputStream compressedOutBuffer = new ByteArrayOutputStream((int)bytes.size());\n+    CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, null);\n+    bytes.writeAllTo(cos);\n+    cos.close();\n+    return BytesInput.from(compressedOutBuffer);\n+  }\n+\n+  private BytesInput decompress(ZstandardCodec codec, BytesInput bytes, int uncompressedSize) throws IOException {\n+    BytesInput decompressed;\n+    InputStream is = codec.createInputStream(bytes.toInputStream(), null);\n+    decompressed = BytesInput.from(is, uncompressedSize);\n+    is.close();\n+    return decompressed;\n+  }\n+\n+  @Test\n+  public void testZstdConfWithMr() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7204a18dc8db359cf5412ece717eaa3fcbc9f990"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDAwMjc5Mw==", "bodyText": "Added. Thanks.", "url": "https://github.com/apache/parquet-mr/pull/793#discussion_r434002793", "createdAt": "2020-06-02T16:12:56Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestZstandardCodec.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.mapred.JobClient;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.OutputCollector;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.RunningJob;\n+import org.apache.hadoop.mapred.TextInputFormat;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+import org.apache.parquet.hadoop.codec.ZstandardCodec;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.mapred.DeprecatedParquetOutputFormat;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.schema.MessageTypeParser;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.Files;\n+import java.util.Random;\n+\n+public class TestZstandardCodec {\n+\n+  private final Path inputPath = new Path(\"src/test/java/org/apache/parquet/hadoop/example/TestInputOutputFormat.java\");\n+\n+  @Test\n+  public void testZstdCodec() throws IOException {\n+    ZstandardCodec codec = new ZstandardCodec();\n+    Configuration conf = new Configuration();\n+    int[] levels = {1, 4, 7, 10, 13, 16, 19, 22};\n+    int[] dataSizes = {0, 1, 10, 1024, 1024 * 1024};\n+\n+    for (int i = 0; i < levels.length; i++) {\n+      conf.setInt(ZstandardCodec.PARQUET_COMPRESS_ZSTD_LEVEL, levels[i]);\n+      codec.setConf(conf);\n+      for (int j = 0; j < dataSizes.length; j++) {\n+        testZstd(codec, dataSizes[j]);\n+      }\n+    }\n+  }\n+\n+  private void testZstd(ZstandardCodec codec, int dataSize) throws IOException {\n+    byte[] data = new byte[dataSize];\n+    (new Random()).nextBytes(data);\n+    BytesInput compressedData = compress(codec,  BytesInput.from(data));\n+    BytesInput decompressedData = decompress(codec, compressedData, data.length);\n+    Assert.assertArrayEquals(data, decompressedData.toByteArray());\n+  }\n+\n+  private BytesInput compress(ZstandardCodec codec, BytesInput bytes) throws IOException {\n+    ByteArrayOutputStream compressedOutBuffer = new ByteArrayOutputStream((int)bytes.size());\n+    CompressionOutputStream cos = codec.createOutputStream(compressedOutBuffer, null);\n+    bytes.writeAllTo(cos);\n+    cos.close();\n+    return BytesInput.from(compressedOutBuffer);\n+  }\n+\n+  private BytesInput decompress(ZstandardCodec codec, BytesInput bytes, int uncompressedSize) throws IOException {\n+    BytesInput decompressed;\n+    InputStream is = codec.createInputStream(bytes.toInputStream(), null);\n+    decompressed = BytesInput.from(is, uncompressedSize);\n+    is.close();\n+    return decompressed;\n+  }\n+\n+  @Test\n+  public void testZstdConfWithMr() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzcyNTU0NQ=="}, "originalCommit": {"oid": "7204a18dc8db359cf5412ece717eaa3fcbc9f990"}, "originalPosition": 96}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4771, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}