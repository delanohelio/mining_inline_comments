{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU4NzA4MDM0", "number": 808, "reviewThreads": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxMDoxMDowMlrOETncdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwODowOTo0N1rOEyOdtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MDA0NjYyOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxMDoxMDowMlrOG5cxqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQxNzowMDo1MVrOG6PiAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjg5MzQ4MA==", "bodyText": "This package name break java naming conventions. It should not contain uppercase. I would suggest using e.g. org.apache.parquet.crypto.propertiesfactory", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r462893480", "createdAt": "2020-07-30T10:10:02Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,252 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.CryptoPropertiesFactoryTests;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9adb75a2356147a204d76c82eb39f43e9ee72b58"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzcyNTA1Nw==", "bodyText": "Sounds good!", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r463725057", "createdAt": "2020-07-31T17:00:51Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,252 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.CryptoPropertiesFactoryTests;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjg5MzQ4MA=="}, "originalCommit": {"oid": "9adb75a2356147a204d76c82eb39f43e9ee72b58"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MDA4NzE4OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxMDoyMjoxNlrOG5dKBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQxODo0Mzo1NlrOG6Sbfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjg5OTcxNw==", "bodyText": "Is it necessary to allow setting the WriteSupport? The concept of the ParquetWriter implementations is to hide all these stuff from the user so it can simply create a ParquetWriter<Group> writer = ExampleParquetWriter.builder(...).with(...) without dealing with the logic required for converting a Group object to writable primitives. Also, allowing to set a simple WriteSupport allows to set one that is not compatible with the Group type breaking the whole logic.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r462899717", "createdAt": "2020-07-30T10:22:16Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java", "diffHunk": "@@ -104,15 +105,19 @@ public Builder withExtraMetaData(Map<String, String> extraMetaData) {\n       return this;\n     }\n \n+    public Builder withWriteSupport(WriteSupport writeSupport) {\n+      this.writeSupport = writeSupport;\n+      return this;\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9adb75a2356147a204d76c82eb39f43e9ee72b58"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc3MjU0Mw==", "bodyText": "If we don't do that, we need to write Builder class to extend ParquetWriter.Builder. It should be a thin wrapper. I think we can do it.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r463772543", "createdAt": "2020-07-31T18:43:56Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java", "diffHunk": "@@ -104,15 +105,19 @@ public Builder withExtraMetaData(Map<String, String> extraMetaData) {\n       return this;\n     }\n \n+    public Builder withWriteSupport(WriteSupport writeSupport) {\n+      this.writeSupport = writeSupport;\n+      return this;\n+    }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjg5OTcxNw=="}, "originalCommit": {"oid": "9adb75a2356147a204d76c82eb39f43e9ee72b58"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MDE0NzAzOnYy", "diffSide": "RIGHT", "path": "parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxMDo0MTo0MlrOG5dviA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQxNjo0OTowNVrOG6PMFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkwOTMyMA==", "bodyText": "You should use the annotation @Override for every method that is overriden.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r462909320", "createdAt": "2020-07-30T10:41:42Z", "author": {"login": "gszadovszky"}, "path": "parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.schema;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * This class decorates the class 'Type' by adding a Map field 'metadata'.\n+ *\n+ * This decoration is needed to add metadata to each column without changing existing class 'MessageType', which is used\n+ * extensively. Here is the example usage to add column metadata to schema with type of 'MessageType'.\n+ *\n+ * MessageType oldSchema = ...\n+ * Map metadata = ...\n+ * List newFields = new ArrayList();\n+ * for (Type field = oldSchema.getFields()) {\n+ *     Type newField = new ExtType(field);\n+ *     newField.setMetadata(metadata);\n+ *     newFields.add(newField);\n+ * }\n+ * MessageType newSchema = new MessageType(oldSchema.getName(), newFields);\n+ *\n+ * The implementation is mostly following decoration pattern. Most of the methods are just thin wrappers of existing\n+ * implementation of PrimitiveType or GroupType.\n+ */\n+public class ExtType<T> extends Type {\n+  private Type type;\n+  private Map<String, T> metadata;\n+\n+  public ExtType(Type type) {\n+    super(type.getName(), type.getRepetition(), type.getOriginalType(), type.getId());\n+    this.type = type;\n+  }\n+\n+  public ExtType(Type type, String name) {\n+    super(name, type.getRepetition(), OriginalType.UINT_64, type.getId());\n+    this.type = new PrimitiveType(type.getRepetition(), type.asPrimitiveType().getPrimitiveTypeName(), name);\n+  }\n+\n+  public Type withId(int id) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9adb75a2356147a204d76c82eb39f43e9ee72b58"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzcxOTQ0Nw==", "bodyText": "Sounds good", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r463719447", "createdAt": "2020-07-31T16:49:05Z", "author": {"login": "shangxinli"}, "path": "parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.schema;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * This class decorates the class 'Type' by adding a Map field 'metadata'.\n+ *\n+ * This decoration is needed to add metadata to each column without changing existing class 'MessageType', which is used\n+ * extensively. Here is the example usage to add column metadata to schema with type of 'MessageType'.\n+ *\n+ * MessageType oldSchema = ...\n+ * Map metadata = ...\n+ * List newFields = new ArrayList();\n+ * for (Type field = oldSchema.getFields()) {\n+ *     Type newField = new ExtType(field);\n+ *     newField.setMetadata(metadata);\n+ *     newFields.add(newField);\n+ * }\n+ * MessageType newSchema = new MessageType(oldSchema.getName(), newFields);\n+ *\n+ * The implementation is mostly following decoration pattern. Most of the methods are just thin wrappers of existing\n+ * implementation of PrimitiveType or GroupType.\n+ */\n+public class ExtType<T> extends Type {\n+  private Type type;\n+  private Map<String, T> metadata;\n+\n+  public ExtType(Type type) {\n+    super(type.getName(), type.getRepetition(), type.getOriginalType(), type.getId());\n+    this.type = type;\n+  }\n+\n+  public ExtType(Type type, String name) {\n+    super(name, type.getRepetition(), OriginalType.UINT_64, type.getId());\n+    this.type = new PrimitiveType(type.getRepetition(), type.asPrimitiveType().getPrimitiveTypeName(), name);\n+  }\n+\n+  public Type withId(int id) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkwOTMyMA=="}, "originalCommit": {"oid": "9adb75a2356147a204d76c82eb39f43e9ee72b58"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MDE1NjAzOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxMDo0NDo0NVrOG5d06A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQxODo0ODowOVrOG6Sihg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkxMDY5Ng==", "bodyText": "Using defaultCharset() would work just as if you would not set any. We usually set the charset to ensure that the result will always be the same on every environment (independently from the default charset). I would suggest using one of the constants of StandardCharsets.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r462910696", "createdAt": "2020-07-30T10:44:45Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.CryptoPropertiesFactoryTests;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.DecryptionKeyRetrieverMock;\n+import org.apache.parquet.crypto.DecryptionPropertiesFactory;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.ParquetCryptoRuntimeException;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.api.WriteSupport.WriteContext;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.ExtType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactory, DecryptionPropertiesFactory {\n+\n+  private static Logger log = LoggerFactory.getLogger(SchemaCryptoPropertiesFactory.class);\n+\n+  public static final String CONF_ENCRYPTION_ALGORITHM = \"parquet.encryption.algorithm\";\n+  public static final String CONF_ENCRYPTION_FOOTER = \"parquet.encrypt.footer\";\n+  private static final byte[] FOOTER_KEY = {0x01, 0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a,\n+    0x0b, 0x0c, 0x0d, 0x0e, 0x0f, 0x10};\n+  private static final byte[] FOOTER_KEY_METADATA = \"footkey\".getBytes(Charset.defaultCharset());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9adb75a2356147a204d76c82eb39f43e9ee72b58"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc3NDM0Mg==", "bodyText": "Fix it.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r463774342", "createdAt": "2020-07-31T18:48:09Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.CryptoPropertiesFactoryTests;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.DecryptionKeyRetrieverMock;\n+import org.apache.parquet.crypto.DecryptionPropertiesFactory;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.ParquetCryptoRuntimeException;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.api.WriteSupport.WriteContext;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.ExtType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactory, DecryptionPropertiesFactory {\n+\n+  private static Logger log = LoggerFactory.getLogger(SchemaCryptoPropertiesFactory.class);\n+\n+  public static final String CONF_ENCRYPTION_ALGORITHM = \"parquet.encryption.algorithm\";\n+  public static final String CONF_ENCRYPTION_FOOTER = \"parquet.encrypt.footer\";\n+  private static final byte[] FOOTER_KEY = {0x01, 0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a,\n+    0x0b, 0x0c, 0x0d, 0x0e, 0x0f, 0x10};\n+  private static final byte[] FOOTER_KEY_METADATA = \"footkey\".getBytes(Charset.defaultCharset());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkxMDY5Ng=="}, "originalCommit": {"oid": "9adb75a2356147a204d76c82eb39f43e9ee72b58"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwODM3MTIwOnYy", "diffSide": "RIGHT", "path": "parquet-column/src/main/java/org/apache/parquet/schema/Type.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxMTowMTo1MFrOG8EsPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxMTowMTo1MFrOG8EsPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY0NDYwNQ==", "bodyText": "I would expect method comments describing the purpose and usage of this metadata. (We should mention that this metadata is for the current parquet-mr runtime only and it won't be serialized to the file.)\nI understand this is the easiest way to add this map to the class but I don't really like it. For example by invoking this setMetadata the caller will not be informed if it overwrites any values already in. I would more like an approach where the user can set/get the metadata one-by-one.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r465644605", "createdAt": "2020-08-05T11:01:50Z", "author": {"login": "gszadovszky"}, "path": "parquet-column/src/main/java/org/apache/parquet/schema/Type.java", "diffHunk": "@@ -363,4 +365,11 @@ void checkContains(Type subType) {\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n \n+   public void setMetadata(Map<String, Object> metadata) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b649eb56682451042c44821edc113e8091bcacfe"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwODQyMTExOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxMToxNTowOVrOG8FMGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNlQxNzozNzozN1rOHYhnfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY1Mjc2Mw==", "bodyText": "nit: -> cryptoMetadatas\nI'm not sure about 's'. I think, data doesn't have plural.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r465652763", "createdAt": "2020-08-05T11:15:09Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,253 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadatas = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b649eb56682451042c44821edc113e8091bcacfe"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQ3ODY1NA==", "bodyText": "resolved", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r495478654", "createdAt": "2020-09-26T17:37:37Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,253 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadatas = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY1Mjc2Mw=="}, "originalCommit": {"oid": "b649eb56682451042c44821edc113e8091bcacfe"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwODQ0MTgzOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxMToyMTowMFrOG8FY8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxMToyMTowMFrOG8FY8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY1NjA0OA==", "bodyText": "I suggest using the already existing constant in PropertiesDrivenCryptoFactory directly.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r465656048", "createdAt": "2020-08-05T11:21:00Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.DecryptionKeyRetrieverMock;\n+import org.apache.parquet.crypto.DecryptionPropertiesFactory;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.ParquetCryptoRuntimeException;\n+import org.apache.parquet.hadoop.api.WriteSupport.WriteContext;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactory, DecryptionPropertiesFactory {\n+\n+  private static Logger log = LoggerFactory.getLogger(SchemaCryptoPropertiesFactory.class);\n+\n+  public static final String CONF_ENCRYPTION_ALGORITHM = \"parquet.encryption.algorithm\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b649eb56682451042c44821edc113e8091bcacfe"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwODQ0Njg4OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxMToyMjo0NlrOG8FcLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxMToyMjo0NlrOG8FcLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY1Njg3Ng==", "bodyText": "Do you mean \"parquet.encryption.plaintext.footer\"? Please, use existing constants.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r465656876", "createdAt": "2020-08-05T11:22:46Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.DecryptionKeyRetrieverMock;\n+import org.apache.parquet.crypto.DecryptionPropertiesFactory;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.ParquetCryptoRuntimeException;\n+import org.apache.parquet.hadoop.api.WriteSupport.WriteContext;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactory, DecryptionPropertiesFactory {\n+\n+  private static Logger log = LoggerFactory.getLogger(SchemaCryptoPropertiesFactory.class);\n+\n+  public static final String CONF_ENCRYPTION_ALGORITHM = \"parquet.encryption.algorithm\";\n+  public static final String CONF_ENCRYPTION_FOOTER = \"parquet.encrypt.footer\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b649eb56682451042c44821edc113e8091bcacfe"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTIyNTQxOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwOTowNToyMFrOHlh1lg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQyMToxMjo1N1rOHm0Cyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExMzc1MA==", "bodyText": "Since the target class is already on the classpath I would use the class object directly and maybe the related conf setter method as well.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509113750", "createdAt": "2020-10-21T09:05:20Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcmCtr() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_V1\");\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionWithFooter() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.setBoolean(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_FOOTER, true);\n+    runTest(conf);\n+  }\n+\n+  private void runTest(Configuration conf ) throws Exception {\n+    conf.set(EncryptionPropertiesFactory.CRYPTO_FACTORY_CLASS_PROPERTY_NAME,\n+      \"org.apache.parquet.crypto.propertiesfactory.SchemaCryptoPropertiesFactory\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ2MDYxOQ==", "bodyText": "changed to SchemaCryptoPropertiesFactory.class.getName()", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r510460619", "createdAt": "2020-10-22T21:12:57Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcmCtr() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_V1\");\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionWithFooter() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.setBoolean(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_FOOTER, true);\n+    runTest(conf);\n+  }\n+\n+  private void runTest(Configuration conf ) throws Exception {\n+    conf.set(EncryptionPropertiesFactory.CRYPTO_FACTORY_CLASS_PROPERTY_NAME,\n+      \"org.apache.parquet.crypto.propertiesfactory.SchemaCryptoPropertiesFactory\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExMzc1MA=="}, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 117}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTIzNjIzOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwOTowNzo1OVrOHlh8Uw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQyMTowNzozN1rOHmz3_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNTQ3NQ==", "bodyText": "I would suggest using the related constant instead.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509115475", "createdAt": "2020-10-21T09:07:59Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1Nzg1NQ==", "bodyText": "fixed", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r510457855", "createdAt": "2020-10-22T21:07:37Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNTQ3NQ=="}, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTIzNjYwOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwOTowODowNVrOHlh8lg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQyMTowNzozNFrOHmz33A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNTU0Mg==", "bodyText": "I would suggest using the related constant instead.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509115542", "createdAt": "2020-10-21T09:08:05Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcmCtr() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_V1\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1NzgyMA==", "bodyText": "fixed", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r510457820", "createdAt": "2020-10-22T21:07:34Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcmCtr() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_V1\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNTU0Mg=="}, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTM2MDU1OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwOTozNzowN1rOHljJEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMjo1Mzo1MFrOHmOtgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzNTEyMg==", "bodyText": "I guess it should be cryptoMetadata", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509135122", "createdAt": "2020-10-21T09:37:07Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0ODk2MQ==", "bodyText": "good catch", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509848961", "createdAt": "2020-10-22T02:53:50Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzNTEyMg=="}, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTM3MDk2OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwOTozOTo0MFrOHljPnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMzo1MTowNlrOHmPm6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzNjc5Ng==", "bodyText": "I don't think it is a good practice to use the column name as a conf key directly. The chance of collisions are pretty high. I would suggest adding a constant prefix.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509136796", "createdAt": "2020-10-21T09:39:40Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.DecryptionKeyRetrieverMock;\n+import org.apache.parquet.crypto.DecryptionPropertiesFactory;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.ParquetCryptoRuntimeException;\n+import org.apache.parquet.hadoop.api.WriteSupport.WriteContext;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.MessageType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactory, DecryptionPropertiesFactory {\n+\n+  private static Logger log = LoggerFactory.getLogger(SchemaCryptoPropertiesFactory.class);\n+\n+  public static final String CONF_ENCRYPTION_ALGORITHM = \"parquet.encryption.algorithm\";\n+  public static final String CONF_ENCRYPTION_FOOTER = \"parquet.encrypt.footer\";\n+  private static final byte[] FOOTER_KEY = {0x01, 0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a,\n+    0x0b, 0x0c, 0x0d, 0x0e, 0x0f, 0x10};\n+  private static final byte[] FOOTER_KEY_METADATA = \"footkey\".getBytes(StandardCharsets.UTF_8);\n+  private static final byte[] COL_KEY = {0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a, 0x0b,\n+    0x0c, 0x0d, 0x0e, 0x0f, 0x10, 0x11};\n+  private static final byte[] COL_KEY_METADATA = \"col\".getBytes(StandardCharsets.UTF_8);\n+\n+  @Override\n+  public FileEncryptionProperties getFileEncryptionProperties(Configuration conf, Path tempFilePath,\n+                                                              WriteContext fileWriteContext) throws ParquetCryptoRuntimeException {\n+    MessageType schema = fileWriteContext.getSchema();\n+    List<String[]> paths = schema.getPaths();\n+    if (paths == null || paths.isEmpty()) {\n+      throw new ParquetCryptoRuntimeException(\"Null or empty fields is found\");\n+    }\n+\n+    Map<ColumnPath, ColumnEncryptionProperties> columnPropertyMap = new HashMap<>();\n+\n+    for (String[] path : paths) {\n+      getColumnEncryptionProperties(path, columnPropertyMap, conf);\n+    }\n+\n+    if (columnPropertyMap.size() == 0) {\n+      log.debug(\"No column is encrypted. Returning null so that Parquet can skip. Empty properties will cause Parquet exception\");\n+      return null;\n+    }\n+\n+    /**\n+     * Why we still need footerKeyMetadata even withEncryptedFooter as false? According to the\n+     * 'Plaintext Footer' section of\n+     * https://github.com/apache/parquet-format/blob/encryption/Encryption.md, the plaintext footer\n+     * is signed in order to prevent tampering with the FileMetaData contents. So footerKeyMetadata\n+     * is always needed. This signature will be verified if parquet-mr code is with parquet-1178.\n+     * Otherwise, it will be ignored.\n+     */\n+    boolean shouldEncryptFooter = getEncryptFooter(conf);\n+    FileEncryptionProperties.Builder encryptionPropertiesBuilder =\n+      FileEncryptionProperties.builder(FOOTER_KEY)\n+        .withFooterKeyMetadata(FOOTER_KEY_METADATA)\n+        .withAlgorithm(getParquetCipherOrDefault(conf))\n+        .withEncryptedColumns(columnPropertyMap);\n+    if (!shouldEncryptFooter) {\n+      encryptionPropertiesBuilder = encryptionPropertiesBuilder.withPlaintextFooter();\n+    }\n+    FileEncryptionProperties encryptionProperties = encryptionPropertiesBuilder.build();\n+    log.info(\n+      \"FileEncryptionProperties is built with, algorithm:{}, footerEncrypted:{}\",\n+      encryptionProperties.getAlgorithm(),\n+      encryptionProperties.encryptedFooter());\n+    return encryptionProperties;\n+  }\n+\n+  private ParquetCipher getParquetCipherOrDefault(Configuration conf) {\n+    String algorithm = conf.get(CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");\n+    log.debug(\"Encryption algorithm is {}\", algorithm);\n+    return ParquetCipher.valueOf(algorithm.toUpperCase());\n+  }\n+\n+  private boolean getEncryptFooter(Configuration conf) {\n+    boolean encryptFooter = conf.getBoolean(CONF_ENCRYPTION_FOOTER, false);\n+    log.debug(\"Encrypt Footer: {}\", encryptFooter);\n+    return encryptFooter;\n+  }\n+\n+  private void getColumnEncryptionProperties(String[] path, Map<ColumnPath, ColumnEncryptionProperties> columnPropertyMap,\n+                                             Configuration conf) throws ParquetCryptoRuntimeException {\n+    String pathName = String.join(\".\", path);\n+    String columnKeyName = conf.get(pathName, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg2MzY1OA==", "bodyText": "fixed", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509863658", "createdAt": "2020-10-22T03:51:06Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.DecryptionKeyRetrieverMock;\n+import org.apache.parquet.crypto.DecryptionPropertiesFactory;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.ParquetCryptoRuntimeException;\n+import org.apache.parquet.hadoop.api.WriteSupport.WriteContext;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.MessageType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactory, DecryptionPropertiesFactory {\n+\n+  private static Logger log = LoggerFactory.getLogger(SchemaCryptoPropertiesFactory.class);\n+\n+  public static final String CONF_ENCRYPTION_ALGORITHM = \"parquet.encryption.algorithm\";\n+  public static final String CONF_ENCRYPTION_FOOTER = \"parquet.encrypt.footer\";\n+  private static final byte[] FOOTER_KEY = {0x01, 0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a,\n+    0x0b, 0x0c, 0x0d, 0x0e, 0x0f, 0x10};\n+  private static final byte[] FOOTER_KEY_METADATA = \"footkey\".getBytes(StandardCharsets.UTF_8);\n+  private static final byte[] COL_KEY = {0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a, 0x0b,\n+    0x0c, 0x0d, 0x0e, 0x0f, 0x10, 0x11};\n+  private static final byte[] COL_KEY_METADATA = \"col\".getBytes(StandardCharsets.UTF_8);\n+\n+  @Override\n+  public FileEncryptionProperties getFileEncryptionProperties(Configuration conf, Path tempFilePath,\n+                                                              WriteContext fileWriteContext) throws ParquetCryptoRuntimeException {\n+    MessageType schema = fileWriteContext.getSchema();\n+    List<String[]> paths = schema.getPaths();\n+    if (paths == null || paths.isEmpty()) {\n+      throw new ParquetCryptoRuntimeException(\"Null or empty fields is found\");\n+    }\n+\n+    Map<ColumnPath, ColumnEncryptionProperties> columnPropertyMap = new HashMap<>();\n+\n+    for (String[] path : paths) {\n+      getColumnEncryptionProperties(path, columnPropertyMap, conf);\n+    }\n+\n+    if (columnPropertyMap.size() == 0) {\n+      log.debug(\"No column is encrypted. Returning null so that Parquet can skip. Empty properties will cause Parquet exception\");\n+      return null;\n+    }\n+\n+    /**\n+     * Why we still need footerKeyMetadata even withEncryptedFooter as false? According to the\n+     * 'Plaintext Footer' section of\n+     * https://github.com/apache/parquet-format/blob/encryption/Encryption.md, the plaintext footer\n+     * is signed in order to prevent tampering with the FileMetaData contents. So footerKeyMetadata\n+     * is always needed. This signature will be verified if parquet-mr code is with parquet-1178.\n+     * Otherwise, it will be ignored.\n+     */\n+    boolean shouldEncryptFooter = getEncryptFooter(conf);\n+    FileEncryptionProperties.Builder encryptionPropertiesBuilder =\n+      FileEncryptionProperties.builder(FOOTER_KEY)\n+        .withFooterKeyMetadata(FOOTER_KEY_METADATA)\n+        .withAlgorithm(getParquetCipherOrDefault(conf))\n+        .withEncryptedColumns(columnPropertyMap);\n+    if (!shouldEncryptFooter) {\n+      encryptionPropertiesBuilder = encryptionPropertiesBuilder.withPlaintextFooter();\n+    }\n+    FileEncryptionProperties encryptionProperties = encryptionPropertiesBuilder.build();\n+    log.info(\n+      \"FileEncryptionProperties is built with, algorithm:{}, footerEncrypted:{}\",\n+      encryptionProperties.getAlgorithm(),\n+      encryptionProperties.encryptedFooter());\n+    return encryptionProperties;\n+  }\n+\n+  private ParquetCipher getParquetCipherOrDefault(Configuration conf) {\n+    String algorithm = conf.get(CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");\n+    log.debug(\"Encryption algorithm is {}\", algorithm);\n+    return ParquetCipher.valueOf(algorithm.toUpperCase());\n+  }\n+\n+  private boolean getEncryptFooter(Configuration conf) {\n+    boolean encryptFooter = conf.getBoolean(CONF_ENCRYPTION_FOOTER, false);\n+    log.debug(\"Encrypt Footer: {}\", encryptFooter);\n+    return encryptFooter;\n+  }\n+\n+  private void getColumnEncryptionProperties(String[] path, Map<ColumnPath, ColumnEncryptionProperties> columnPropertyMap,\n+                                             Configuration conf) throws ParquetCryptoRuntimeException {\n+    String pathName = String.join(\".\", path);\n+    String columnKeyName = conf.get(pathName, null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzNjc5Ng=="}, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTc3NTcwOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxMToyODo1MFrOHlnJGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QwNzozODozMFrOHnCOEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwMDY2Nw==", "bodyText": "This assumes that file.toString() returns the full file path. However, the file is a public abstract interface org.apache.parquet.io.OutputFile, which doesn't have such method, so toString() is up to the implementation; no guarantees it will return the path. Also, new Path(string full_path) is not aware of the right filesystem (?) Maybe can be handled with an upcast to a known implementing class - preferably one that already has a Path getPath() method.\nBut of course, this won't be very general.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509200667", "createdAt": "2020-10-21T11:28:50Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "diffHunk": "@@ -279,6 +279,11 @@ public ParquetWriter(Path file, Configuration conf, WriteSupport<T> writeSupport\n     WriteSupport.WriteContext writeContext = writeSupport.init(conf);\n     MessageType schema = writeContext.getSchema();\n \n+    // encryptionProperties could be built from the implementation of EncryptionPropertiesFactory when it is attached.\n+    if (encryptionProperties == null) {\n+      encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, new Path(file.toString()), writeContext);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwNjI3OQ==", "bodyText": "eg upcasting to HadoopOutputFile. Or even better, adding Path getPath() method to the OutputFile - this should be general enough.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509206279", "createdAt": "2020-10-21T11:38:57Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "diffHunk": "@@ -279,6 +279,11 @@ public ParquetWriter(Path file, Configuration conf, WriteSupport<T> writeSupport\n     WriteSupport.WriteContext writeContext = writeSupport.init(conf);\n     MessageType schema = writeContext.getSchema();\n \n+    // encryptionProperties could be built from the implementation of EncryptionPropertiesFactory when it is attached.\n+    if (encryptionProperties == null) {\n+      encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, new Path(file.toString()), writeContext);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwMDY2Nw=="}, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDI3ODIwMg==", "bodyText": "Good point. I like the idea of adding \"Path getPath()\" better. Since this feature is going to be in a major release. I think\nadding a new method to the interface could be fine.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r510278202", "createdAt": "2020-10-22T15:59:21Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "diffHunk": "@@ -279,6 +279,11 @@ public ParquetWriter(Path file, Configuration conf, WriteSupport<T> writeSupport\n     WriteSupport.WriteContext writeContext = writeSupport.init(conf);\n     MessageType schema = writeContext.getSchema();\n \n+    // encryptionProperties could be built from the implementation of EncryptionPropertiesFactory when it is attached.\n+    if (encryptionProperties == null) {\n+      encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, new Path(file.toString()), writeContext);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwMDY2Nw=="}, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDY5Mjg4Mg==", "bodyText": "looks good. please also replace new Path(file.toString()) with file.getPath()", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r510692882", "createdAt": "2020-10-23T07:38:30Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "diffHunk": "@@ -279,6 +279,11 @@ public ParquetWriter(Path file, Configuration conf, WriteSupport<T> writeSupport\n     WriteSupport.WriteContext writeContext = writeSupport.init(conf);\n     MessageType schema = writeContext.getSchema();\n \n+    // encryptionProperties could be built from the implementation of EncryptionPropertiesFactory when it is attached.\n+    if (encryptionProperties == null) {\n+      encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, new Path(file.toString()), writeContext);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwMDY2Nw=="}, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4OTc4OTY1OnYy", "diffSide": "RIGHT", "path": "parquet-column/src/main/java/org/apache/parquet/schema/Type.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxMTozMjo0NlrOHlnSAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwMjo1MzoxM1rOHmOs6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwMjk0NA==", "bodyText": "probably no need in changing this file", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509202944", "createdAt": "2020-10-21T11:32:46Z", "author": {"login": "ggershinsky"}, "path": "parquet-column/src/main/java/org/apache/parquet/schema/Type.java", "diffHunk": "@@ -362,5 +362,4 @@ void checkContains(Type subType) {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n-\n-}\n+ }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTg0ODgxMQ==", "bodyText": "true", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509848811", "createdAt": "2020-10-22T02:53:13Z", "author": {"login": "shangxinli"}, "path": "parquet-column/src/main/java/org/apache/parquet/schema/Type.java", "diffHunk": "@@ -362,5 +362,4 @@ void checkContains(Type subType) {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n-\n-}\n+ }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwMjk0NA=="}, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNjYwMzUxOnYy", "diffSide": "RIGHT", "path": "parquet-common/src/main/java/org/apache/parquet/io/OutputFile.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTowMTo0MFrOHoGHhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTo1NjozOVrOHoIKdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwNTMxNw==", "bodyText": "I don't know if I overlooked this one before or it is a new change. The module parquet-column should not depend on hadoop. That's why we have the separate module parquet-hadoop. We already have struggling issues that parquet-mr cannot be used without hadoop, let's not make it worse.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511805317", "createdAt": "2020-10-26T09:01:40Z", "author": {"login": "gszadovszky"}, "path": "parquet-common/src/main/java/org/apache/parquet/io/OutputFile.java", "diffHunk": "@@ -31,4 +33,5 @@\n \n   long defaultBlockSize();\n \n+  Path getPath();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0246e50fec13681a1de0407ca660ca923e8a3f99"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgzODgzOA==", "bodyText": "apologies, this is a recent change done due to my suggestion; making common dependent on hadoop is indeed not a good idea. @shangxinli - I've checked the hadoop Path and FileSystem code, and it looks like having a Path class here is not a must. A regular String would do, eg String getPath in this interface (OutputFile).\nThen the ParquetWriter can use the new Path(String) call.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511838838", "createdAt": "2020-10-26T09:56:39Z", "author": {"login": "ggershinsky"}, "path": "parquet-common/src/main/java/org/apache/parquet/io/OutputFile.java", "diffHunk": "@@ -31,4 +33,5 @@\n \n   long defaultBlockSize();\n \n+  Path getPath();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwNTMxNw=="}, "originalCommit": {"oid": "0246e50fec13681a1de0407ca660ca923e8a3f99"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNjYwODYyOnYy", "diffSide": "RIGHT", "path": "parquet-common/pom.xml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTowMjo1MVrOHoGKcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTowMjo1MVrOHoGKcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwNjA2Nw==", "bodyText": "See at OutputFile.getPath.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511806067", "createdAt": "2020-10-26T09:02:51Z", "author": {"login": "gszadovszky"}, "path": "parquet-common/pom.xml", "diffHunk": "@@ -67,6 +67,20 @@\n       <artifactId>audience-annotations</artifactId>\n       <version>0.12.0</version>\n     </dependency>\n+\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-client</artifactId>\n+      <version>${hadoop.version}</version>\n+      <scope>provided</scope>\n+      <exclusions>\n+        <exclusion>\n+          <groupId>org.slf4j</groupId>\n+          <artifactId>slf4j-log4j12</artifactId>\n+        </exclusion>\n+      </exclusions>\n+    </dependency>\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNjYyMzY5OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTowNzowOFrOHoGTzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTowNzowOFrOHoGTzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwODQ2MQ==", "bodyText": "I think it is nicer to use the enum ParquetCypher instead of the parquet-format generated class.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511808461", "createdAt": "2020-10-26T09:07:08Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.EncryptionAlgorithm;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> cryptoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__CTR__V1.getFieldName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNjYyNDA1OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTowNzoxNVrOHoGUCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTowNzoxNVrOHoGUCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwODUyMw==", "bodyText": "I think it is nicer to use the enum ParquetCypher instead of the parquet-format generated class.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511808523", "createdAt": "2020-10-26T09:07:15Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.EncryptionAlgorithm;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> cryptoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__CTR__V1.getFieldName());\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcmCtr() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__V1.getFieldName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNjgzMjQ4OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTo1OTowNFrOHoIQsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTo1OTowNFrOHoIQsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTg0MDQzMw==", "bodyText": "per the previous comment, can be changed to encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, new Path(file.getPath()), writeContext);", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511840433", "createdAt": "2020-10-26T09:59:04Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "diffHunk": "@@ -279,6 +279,11 @@ public ParquetWriter(Path file, Configuration conf, WriteSupport<T> writeSupport\n     WriteSupport.WriteContext writeContext = writeSupport.init(conf);\n     MessageType schema = writeContext.getSchema();\n \n+    // encryptionProperties could be built from the implementation of EncryptionPropertiesFactory when it is attached.\n+    if (encryptionProperties == null) {\n+      encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, file.getPath(), writeContext);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMTAwNzY2OnYy", "diffSide": "RIGHT", "path": "parquet-benchmarks/src/main/java/org/apache/parquet/benchmarks/NestedNullWritingBenchmarks.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwODowODozMVrOHovmjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxMzoyNjoxMVrOHo8CLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTAwNg==", "bodyText": "It's unused.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512485006", "createdAt": "2020-10-27T08:08:31Z", "author": {"login": "gszadovszky"}, "path": "parquet-benchmarks/src/main/java/org/apache/parquet/benchmarks/NestedNullWritingBenchmarks.java", "diffHunk": "@@ -28,6 +28,7 @@\n import java.io.IOException;\n import java.util.Random;\n \n+import org.apache.hadoop.fs.Path;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjY4ODY4Nw==", "bodyText": "fixed", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512688687", "createdAt": "2020-10-27T13:26:11Z", "author": {"login": "shangxinli"}, "path": "parquet-benchmarks/src/main/java/org/apache/parquet/benchmarks/NestedNullWritingBenchmarks.java", "diffHunk": "@@ -28,6 +28,7 @@\n import java.io.IOException;\n import java.util.Random;\n \n+import org.apache.hadoop.fs.Path;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTAwNg=="}, "originalCommit": {"oid": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMTAxMDcxOnYy", "diffSide": "LEFT", "path": "parquet-column/src/main/java/org/apache/parquet/schema/Type.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwODowOToyMlrOHovocg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxMzoyNzo0N1rOHo8HDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTQ5MA==", "bodyText": "nit: Please undo formatting changes in this file.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512485490", "createdAt": "2020-10-27T08:09:22Z", "author": {"login": "gszadovszky"}, "path": "parquet-column/src/main/java/org/apache/parquet/schema/Type.java", "diffHunk": "@@ -362,5 +362,4 @@ void checkContains(Type subType) {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjY4OTkzNQ==", "bodyText": "fixed", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512689935", "createdAt": "2020-10-27T13:27:47Z", "author": {"login": "shangxinli"}, "path": "parquet-column/src/main/java/org/apache/parquet/schema/Type.java", "diffHunk": "@@ -362,5 +362,4 @@ void checkContains(Type subType) {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n-", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTQ5MA=="}, "originalCommit": {"oid": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMTAxMTEzOnYy", "diffSide": "RIGHT", "path": "parquet-common/pom.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwODowOToyOFrOHovosQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxMzoyODo0MVrOHo8Jtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTU1Mw==", "bodyText": "nit: Please undo formatting changes in this file.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512485553", "createdAt": "2020-10-27T08:09:28Z", "author": {"login": "gszadovszky"}, "path": "parquet-common/pom.xml", "diffHunk": "@@ -67,6 +67,7 @@\n       <artifactId>audience-annotations</artifactId>\n       <version>0.12.0</version>\n     </dependency>\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjY5MDYxNQ==", "bodyText": "fixed", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512690615", "createdAt": "2020-10-27T13:28:41Z", "author": {"login": "shangxinli"}, "path": "parquet-common/pom.xml", "diffHunk": "@@ -67,6 +67,7 @@\n       <artifactId>audience-annotations</artifactId>\n       <version>0.12.0</version>\n     </dependency>\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTU1Mw=="}, "originalCommit": {"oid": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMTAxMjM4OnYy", "diffSide": "LEFT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwODowOTo0N1rOHovpZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwODowOTo0N1rOHovpZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTczNQ==", "bodyText": "nit: Please undo formatting changes in this file.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512485735", "createdAt": "2020-10-27T08:09:47Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java", "diffHunk": "@@ -113,6 +113,5 @@ protected Builder self() {\n     protected WriteSupport<Group> getWriteSupport(Configuration conf) {\n       return new GroupWriteSupport(type, extraMetaData);\n     }\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4786, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}