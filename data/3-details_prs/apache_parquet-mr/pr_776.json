{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk2ODE2Njg2", "number": 776, "title": "PARQUET-1229: Parquet MR encryption", "bodyText": "Make sure you have checked all steps below.\nJira\n\n My PR addresses the following Parquet Jira issues and references them in the PR title. For example, \"PARQUET-1234: My Parquet PR\"\n\nhttps://issues.apache.org/jira/browse/PARQUET-XXX\nIn case you are adding a dependency, check if the license complies with the ASF 3rd Party License Policy.\n\n\n\nTests\n\n My PR adds the following unit tests OR does not need testing for this extremely good reason:\n\nCommits\n\n My commits all reference Jira issues in their subject lines. In addition, my commits follow the guidelines from \"How to write a good git commit message\":\n\nSubject is separated from body by a blank line\nSubject is limited to 50 characters (not including Jira issue reference)\nSubject does not end with a period\nSubject uses the imperative mood (\"add\", not \"adding\")\nBody wraps at 72 characters\nBody explains \"what\" and \"why\", not \"how\"\n\n\n\nDocumentation\n\n In case of new functionality, my PR adds documentation that describes how to use it.\n\nAll the public functions and the classes in the PR contain Javadoc that explain what it does", "createdAt": "2020-04-01T07:43:37Z", "url": "https://github.com/apache/parquet-mr/pull/776", "merged": true, "mergeCommit": {"oid": "06b5372ddf0f1108c2109cc9011b2497a136830f"}, "closed": true, "closedAt": "2020-05-29T10:07:04Z", "author": {"login": "ggershinsky"}, "timelineItems": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcTZeBsABqjMxODg0NzI0NTU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcl_kuogFqTQyMDgyMDMxOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "247a2a54687cd8948b90356c0f5145083ab30088", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/247a2a54687cd8948b90356c0f5145083ab30088", "committedDate": "2020-04-01T10:34:06Z", "message": "clean up"}, "afterCommit": {"oid": "45b917f7bf52cf41367c2ea7f6d2ec27e2ab0359", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/45b917f7bf52cf41367c2ea7f6d2ec27e2ab0359", "committedDate": "2020-04-01T15:30:20Z", "message": "code format clean up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d2b93707fa04d9c66f9ef5914d4c60f76b7829b2", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/d2b93707fa04d9c66f9ef5914d4c60f76b7829b2", "committedDate": "2020-04-22T10:17:43Z", "message": "mr encryption - initial push"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "96d00dc60efc5b64ead41863273abda7b47a5805", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/96d00dc60efc5b64ead41863273abda7b47a5805", "committedDate": "2020-04-22T10:17:43Z", "message": "PFR fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4eff3ba1190da37a6905c39e00fa1eedcc7ded67", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/4eff3ba1190da37a6905c39e00fa1eedcc7ded67", "committedDate": "2020-04-22T10:17:43Z", "message": "PFR fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b2bcffaba1f47b085e2ccb8e8675ee3aa79f9a5", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/6b2bcffaba1f47b085e2ccb8e8675ee3aa79f9a5", "committedDate": "2020-04-22T10:17:43Z", "message": "format fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "af87bcf0ebf52dd38c4ee0cf6c77397880e059d5", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/af87bcf0ebf52dd38c4ee0cf6c77397880e059d5", "committedDate": "2020-04-22T10:32:02Z", "message": "javadoc annotations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c8aa709738e538edd524e5dcaba2dd482065f23e", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/c8aa709738e538edd524e5dcaba2dd482065f23e", "committedDate": "2020-04-22T10:32:02Z", "message": "code format clean up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "297ed1b12a78f196cf49a9a0b40d9215177f7351", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/297ed1b12a78f196cf49a9a0b40d9215177f7351", "committedDate": "2020-04-22T10:32:02Z", "message": "use crypto exception instead of IOexception"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "40484c6950702cc34484d056fba2434c6a990b2a", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/40484c6950702cc34484d056fba2434c6a990b2a", "committedDate": "2020-04-22T10:32:02Z", "message": "remove hidden column exception"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c6b0cde5cff24eccc3ec3175ecb2ef694e1b9f36", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/c6b0cde5cff24eccc3ec3175ecb2ef694e1b9f36", "committedDate": "2020-04-22T10:32:02Z", "message": "fix prettyJSON exception"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fe7ab01b890d50fe6a3455053fdb8f9031a5dd1e", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/fe7ab01b890d50fe6a3455053fdb8f9031a5dd1e", "committedDate": "2020-04-20T11:17:25Z", "message": "fix prettyJSON exception"}, "afterCommit": {"oid": "c6b0cde5cff24eccc3ec3175ecb2ef694e1b9f36", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/c6b0cde5cff24eccc3ec3175ecb2ef694e1b9f36", "committedDate": "2020-04-22T10:32:02Z", "message": "fix prettyJSON exception"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cacea5ea9e7ce759992e73e51aa535154e96fdaf", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/cacea5ea9e7ce759992e73e51aa535154e96fdaf", "committedDate": "2020-04-22T12:39:45Z", "message": "bloom encryption test fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0245db23f5e8b8aca6b1c8520a1164b08eb4fba5", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/0245db23f5e8b8aca6b1c8520a1164b08eb4fba5", "committedDate": "2020-04-22T12:54:31Z", "message": "indentation fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/fdb1d59461a06bbbc418d75a0dec257d1e11013d", "committedDate": "2020-04-23T12:54:34Z", "message": "remove travis-before_install-encryption (format master fetch)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwNTYzNDg3", "url": "https://github.com/apache/parquet-mr/pull/776#pullrequestreview-400563487", "createdAt": "2020-04-26T23:43:09Z", "commit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzo0MzowOVrOGMLzKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQyMzo0MzowOVrOGMLzKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQyOTQxNg==", "bodyText": "Since this is a public method, can we validate pageAAD also?", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r415429416", "createdAt": "2020-04-26T23:43:09Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -90,6 +102,10 @@\n \n   // Update last two bytes with new page ordinal (instead of creating new page AAD from scratch)\n   public static void quickUpdatePageAAD(byte[] pageAAD, short newPageOrdinal) {\n+    if (newPageOrdinal < 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 54}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwNTczMTcw", "url": "https://github.com/apache/parquet-mr/pull/776#pullrequestreview-400573170", "createdAt": "2020-04-27T00:57:22Z", "commit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMDo1NzoyMlrOGMMubg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMDo1NzoyMlrOGMMubg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NDU5MA==", "bodyText": "Should we keep original addRowGroup() intact and just add a new one with InternalFileEncryptor to isolate the change's impact? There is not much duplicate code if doing so and we can refactor the existing code with helper functions.  In the majority use cases, they are non-encryption cases.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r415444590", "createdAt": "2020-04-27T00:57:22Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -463,14 +486,29 @@ ConvertedType convertToConvertedType(LogicalTypeAnnotation logicalTypeAnnotation\n     }\n   }\n \n-  private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block) {\n+  private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGroups, BlockMetaData block,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 75}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwNTc0Mzg1", "url": "https://github.com/apache/parquet-mr/pull/776#pullrequestreview-400574385", "createdAt": "2020-04-27T01:04:48Z", "commit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMTowNDo0OFrOGMM0yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMTowNDo0OFrOGMM0yA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NjIxNg==", "bodyText": "Move comments up", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r415446216", "createdAt": "2020-04-27T01:04:48Z", "author": {"login": "shangxinli"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 286}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0NDIyMDQz", "url": "https://github.com/apache/parquet-mr/pull/776#pullrequestreview-414422043", "createdAt": "2020-05-19T13:03:52Z", "commit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxMzowMzo1MlrOGXfYLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xOVQxNDowODoxN1rOGXiLDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI4NDUyNw==", "bodyText": "Why do we need it as a short instead of keeping it as an int? As per the parquet.thrift spec we never say that we cannot have more pages than 32767 even if it is unlikely to have such many.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427284527", "createdAt": "2020-05-19T13:03:52Z", "author": {"login": "gszadovszky"}, "path": "parquet-column/src/main/java/org/apache/parquet/internal/column/columnindex/OffsetIndex.java", "diffHunk": "@@ -49,6 +49,13 @@\n    * @return the index of the first row in the page\n    */\n   public long getFirstRowIndex(int pageIndex);\n+  \n+  /**\n+   * @param pageIndex\n+   *         the index of the page\n+   * @return the original ordinal of the page in the column chunk\n+   */\n+  public short getPageOrdinal(int pageIndex);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5NDMwNw==", "bodyText": "Theoretically we don't give hard limits for the number of row groups, number of columns or the number of pages in the spec. There is a de facto limit that we use thrift lists where the size is an i32 meaning that we should allow java int values here.\nAlso, there was a post commit discussion in a related PR. It is unfortunate that that time parquet-format was already released so I don't know if there is a way to properly fix this issue in the format. Anyway, I would not restrict these values to a short.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427294307", "createdAt": "2020-05-19T13:18:12Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -68,19 +67,32 @@\n \n   public static byte[] createModuleAAD(byte[] fileAAD, ModuleType moduleType, \n       short rowGroupOrdinal, short columnOrdinal, short pageOrdinal) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzI5OTYxOA==", "bodyText": "I would keep throws IOException here. InputStream objects throw IOException so the caller shall be prepared handling these.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427299618", "createdAt": "2020-05-19T13:25:37Z", "author": {"login": "gszadovszky"}, "path": "parquet-format-structures/src/main/java/org/apache/parquet/format/BlockCipher.java", "diffHunk": "@@ -51,19 +49,17 @@\n      * Parquet Modular Encryption specification.\n      * @param AAD - Additional Authenticated Data for the decryption (ignored in case of CTR cipher)\n      * @return plaintext - starts at offset 0 of the output value, and fills up the entire byte array.\n-     * @throws IOException thrown upon any crypto problem encountered during decryption\n      */\n-    public byte[] decrypt(byte[] lengthAndCiphertext, byte[] AAD) throws IOException;\n+    public byte[] decrypt(byte[] lengthAndCiphertext, byte[] AAD);\n \n     /**\n      * Convenience decryption method that reads the length and ciphertext from the input stream.\n      * \n      * @param from Input stream with length and ciphertext.\n      * @param AAD - Additional Authenticated Data for the decryption (ignored in case of CTR cipher)\n      * @return plaintext -  starts at offset 0 of the output, and fills up the entire byte array.\n-     * @throws IOException thrown upon any crypto or IO problem encountered during decryption\n      */\n-    public byte[] decrypt(InputStream from, byte[] AAD) throws IOException;\n+    public byte[] decrypt(InputStream from, byte[] AAD);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMwMDU0Nw==", "bodyText": "We should let the IOException thrown out.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427300547", "createdAt": "2020-05-19T13:27:00Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java", "diffHunk": "@@ -98,16 +104,21 @@\n         ((lengthBuffer[0] & 0xff));\n \n     if (ciphertextLength < 1) {\n-      throw new IOException(\"Wrong length of encrypted metadata: \" + ciphertextLength);\n+      throw new ParquetCryptoRuntimeException(\"Wrong length of encrypted metadata: \" + ciphertextLength);\n     }\n \n     byte[] ciphertextBuffer = new byte[ciphertextLength];\n     gotBytes = 0;\n     // Read the encrypted structure contents\n     while (gotBytes < ciphertextLength) {\n-      int n = from.read(ciphertextBuffer, gotBytes, ciphertextLength - gotBytes);\n+      int n;\n+      try {\n+        n = from.read(ciphertextBuffer, gotBytes, ciphertextLength - gotBytes);\n+      } catch (IOException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMwMDU2NA==", "bodyText": "We should let the IOException thrown out.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427300564", "createdAt": "2020-05-19T13:27:01Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesGcmDecryptor.java", "diffHunk": "@@ -70,23 +69,30 @@\n       if (null != AAD) cipher.updateAAD(AAD);\n \n       cipher.doFinal(ciphertext, inputOffset, inputLength, plainText, outputOffset);\n-    }  catch (GeneralSecurityException e) {\n-      throw new IOException(\"Failed to decrypt\", e);\n+    }  catch (AEADBadTagException e) {\n+      throw new TagVerificationException(\"GCM tag check failed\", e);\n+    } catch (GeneralSecurityException e) {\n+      throw new ParquetCryptoRuntimeException(\"Failed to decrypt\", e);\n     }\n \n     return plainText;\n   }\n \n   @Override\n-  public byte[] decrypt(InputStream from, byte[] AAD) throws IOException {\n+  public byte[] decrypt(InputStream from, byte[] AAD) {\n     byte[] lengthBuffer = new byte[SIZE_LENGTH];\n     int gotBytes = 0;\n \n     // Read the length of encrypted Thrift structure\n     while (gotBytes < SIZE_LENGTH) {\n-      int n = from.read(lengthBuffer, gotBytes, SIZE_LENGTH - gotBytes);\n+      int n;\n+      try {\n+        n = from.read(lengthBuffer, gotBytes, SIZE_LENGTH - gotBytes);\n+      } catch (IOException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyODA2MA==", "bodyText": "Actually, based on parquet-mr README:\n\nGenerally speaking, stick to the Sun Java Code Conventions\n\nBased on the related section both should be fine.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427328060", "createdAt": "2020-05-19T14:05:11Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ0NjIxNg=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMyOTk0OA==", "bodyText": "Is plaintext a usual term for un-encrypted files? I don't really like it but fine if it is commonly used in that sense. (Plaintext files for me are the *.txt files.)", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427329948", "createdAt": "2020-05-19T14:07:48Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzMzMDMxNg==", "bodyText": "I think, ParquetCryptoRuntimeException would fit better here.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427330316", "createdAt": "2020-05-19T14:08:17Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file\n+        fileDecryptor.setPlaintextFile();\n+        // Done to detect files that were not encrypted by mistake\n+        if (!fileDecryptor.plaintextFilesAllowed()) {\n+          throw new IOException(\"Applying decryptor on plaintext file\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 290}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c9761c39a774caac1bde5875b41e3368e745c0b4", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/c9761c39a774caac1bde5875b41e3368e745c0b4", "committedDate": "2020-05-20T13:35:51Z", "message": "address initial comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE1Mjg0MTgw", "url": "https://github.com/apache/parquet-mr/pull/776#pullrequestreview-415284180", "createdAt": "2020-05-20T12:32:34Z", "commit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxMjozMjozNFrOGYJTWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNDowNzoyM1rOGYNkIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzk3MTQxNg==", "bodyText": "There was no such check in the previous code. Strictly speaking it is a breaking change as a NullPointerException was thrown where an IOException is thrown today.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427971416", "createdAt": "2020-05-20T12:32:34Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file\n+        fileDecryptor.setPlaintextFile();\n+        // Done to detect files that were not encrypted by mistake\n+        if (!fileDecryptor.plaintextFilesAllowed()) {\n+          throw new IOException(\"Applying decryptor on plaintext file\");\n+        }\n+      } else {  // Encrypted file with plaintext footer\n+        // if no fileDecryptor, can still read plaintext columns\n+        fileDecryptor.setFileCryptoMetaData(fileMetaData.getEncryption_algorithm(), false, \n+            fileMetaData.getFooter_signing_key_metadata());\n+        if (fileDecryptor.checkFooterIntegrity()) {\n+          verifyFooterIntegrity(from, fileDecryptor, combinedFooterLength);\n+        }\n+      }\n+    }\n+    \n+    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData, fileDecryptor, encryptedFooter);\n     if (LOG.isDebugEnabled()) LOG.debug(ParquetMetadata.toPrettyJSON(parquetMetadata));\n     return parquetMetadata;\n   }\n+  \n+  public ColumnChunkMetaData buildColumnChunkMetaData(ColumnMetaData metaData, ColumnPath columnPath, PrimitiveType type, String createdBy) {\n+    return ColumnChunkMetaData.get(\n+        columnPath,\n+        type,\n+        fromFormatCodec(metaData.codec),\n+        convertEncodingStats(metaData.getEncoding_stats()),\n+        fromFormatEncodings(metaData.encodings),\n+        fromParquetStatistics(\n+            createdBy,\n+            metaData.statistics,\n+            type),\n+        metaData.data_page_offset,\n+        metaData.dictionary_page_offset,\n+        metaData.num_values,\n+        metaData.total_compressed_size,\n+        metaData.total_uncompressed_size);\n+  }\n \n   public ParquetMetadata fromParquetMetadata(FileMetaData parquetMetadata) throws IOException {\n+    return fromParquetMetadata(parquetMetadata, null, false);\n+  }\n+\n+  public ParquetMetadata fromParquetMetadata(FileMetaData parquetMetadata, \n+      InternalFileDecryptor fileDecryptor, boolean encryptedFooter) throws IOException {\n     MessageType messageType = fromParquetSchema(parquetMetadata.getSchema(), parquetMetadata.getColumn_orders());\n     List<BlockMetaData> blocks = new ArrayList<BlockMetaData>();\n     List<RowGroup> row_groups = parquetMetadata.getRow_groups();\n+    \n     if (row_groups != null) {\n       for (RowGroup rowGroup : row_groups) {\n         BlockMetaData blockMetaData = new BlockMetaData();\n         blockMetaData.setRowCount(rowGroup.getNum_rows());\n         blockMetaData.setTotalByteSize(rowGroup.getTotal_byte_size());\n+        // not set in legacy files\n+        if (rowGroup.isSetOrdinal()) {\n+          blockMetaData.setOrdinal(rowGroup.getOrdinal());\n+        }\n         List<ColumnChunk> columns = rowGroup.getColumns();\n         String filePath = columns.get(0).getFile_path();\n+        short columnOrdinal = -1;\n         for (ColumnChunk columnChunk : columns) {\n+          columnOrdinal++;\n           if ((filePath == null && columnChunk.getFile_path() != null)\n               || (filePath != null && !filePath.equals(columnChunk.getFile_path()))) {\n             throw new ParquetDecodingException(\"all column chunks of the same row group must be in the same file for now\");\n           }\n           ColumnMetaData metaData = columnChunk.meta_data;\n-          ColumnPath path = getPath(metaData);\n-          ColumnChunkMetaData column = ColumnChunkMetaData.get(\n-              path,\n-              messageType.getType(path.toArray()).asPrimitiveType(),\n-              fromFormatCodec(metaData.codec),\n-              convertEncodingStats(metaData.getEncoding_stats()),\n-              fromFormatEncodings(metaData.encodings),\n-              fromParquetStatistics(\n-                  parquetMetadata.getCreated_by(),\n-                  metaData.statistics,\n-                  messageType.getType(path.toArray()).asPrimitiveType()),\n-              metaData.data_page_offset,\n-              metaData.dictionary_page_offset,\n-              metaData.num_values,\n-              metaData.total_compressed_size,\n-              metaData.total_uncompressed_size);\n+          ColumnCryptoMetaData cryptoMetaData = columnChunk.getCrypto_metadata();\n+          ColumnChunkMetaData column = null;\n+          ColumnPath columnPath = null;\n+          boolean encryptedMetadata = false;\n+          \n+          if (null == cryptoMetaData) { // Plaintext column\n+            if (null == metaData) {\n+              throw new IOException(\"ColumnMetaData not set in plaintext column\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 377}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzk3MjE4MA==", "bodyText": "These IOExceptions seems to be thrown in cases of encryption related issues. Don't we want to use the specific exception instead?", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427972180", "createdAt": "2020-05-20T12:33:55Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1185,70 +1275,189 @@ static long getOffset(ColumnChunk columnChunk) {\n     return offset;\n   }\n \n+  private static void verifyFooterIntegrity(InputStream from, InternalFileDecryptor fileDecryptor, \n+      int combinedFooterLength) throws IOException {\n+    \n+    byte[] nonce = new byte[AesCipher.NONCE_LENGTH];\n+    from.read(nonce);\n+    byte[] gcmTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    from.read(gcmTag);\n+    \n+    AesGcmEncryptor footerSigner =  fileDecryptor.createSignedFooterEncryptor();\n+    \n+    byte[] footerAndSignature = ((ByteBufferInputStream) from).slice(0).array();\n+    int footerSignatureLength = AesCipher.NONCE_LENGTH + AesCipher.GCM_TAG_LENGTH;\n+    byte[] serializedFooter = new byte[combinedFooterLength - footerSignatureLength];\n+    System.arraycopy(footerAndSignature, 0, serializedFooter, 0, serializedFooter.length);\n+\n+    byte[] signedFooterAAD = AesCipher.createFooterAAD(fileDecryptor.getFileAAD());\n+    byte[] encryptedFooterBytes = footerSigner.encrypt(false, serializedFooter, nonce, signedFooterAAD);\n+    byte[] calculatedTag = new byte[AesCipher.GCM_TAG_LENGTH];\n+    System.arraycopy(encryptedFooterBytes, encryptedFooterBytes.length - AesCipher.GCM_TAG_LENGTH, \n+        calculatedTag, 0, AesCipher.GCM_TAG_LENGTH);\n+    if (!Arrays.equals(gcmTag, calculatedTag)) {\n+      throw new TagVerificationException(\"Signature mismatch in plaintext footer\");\n+    }\n+  }\n+\n   public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter) throws IOException {\n+    return readParquetMetadata(from, filter, null, false, 0);\n+  }\n+\n+  public ParquetMetadata readParquetMetadata(final InputStream from, MetadataFilter filter,\n+      final InternalFileDecryptor fileDecryptor, final boolean encryptedFooter, \n+      final int combinedFooterLength) throws IOException {\n+    \n+    final BlockCipher.Decryptor footerDecryptor = (encryptedFooter? fileDecryptor.fetchFooterDecryptor() : null);\n+    final byte[] encryptedFooterAAD = (encryptedFooter? AesCipher.createFooterAAD(fileDecryptor.getFileAAD()) : null);\n+    \n     FileMetaData fileMetaData = filter.accept(new MetadataFilterVisitor<FileMetaData, IOException>() {\n       @Override\n       public FileMetaData visit(NoFilter filter) throws IOException {\n-        return readFileMetaData(from);\n+        return readFileMetaData(from, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(SkipMetadataFilter filter) throws IOException {\n-        return readFileMetaData(from, true);\n+        return readFileMetaData(from, true, footerDecryptor, encryptedFooterAAD);\n       }\n \n       @Override\n       public FileMetaData visit(OffsetMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByStart(readFileMetaData(from), filter);\n+        return filterFileMetaDataByStart(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n \n       @Override\n       public FileMetaData visit(RangeMetadataFilter filter) throws IOException {\n-        return filterFileMetaDataByMidpoint(readFileMetaData(from), filter);\n+        return filterFileMetaDataByMidpoint(readFileMetaData(from, footerDecryptor, encryptedFooterAAD), filter);\n       }\n     });\n     LOG.debug(\"{}\", fileMetaData);\n-    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData);\n+    \n+    if (!encryptedFooter && null != fileDecryptor) {\n+      if (!fileMetaData.isSetEncryption_algorithm()) { // Plaintext file\n+        fileDecryptor.setPlaintextFile();\n+        // Done to detect files that were not encrypted by mistake\n+        if (!fileDecryptor.plaintextFilesAllowed()) {\n+          throw new IOException(\"Applying decryptor on plaintext file\");\n+        }\n+      } else {  // Encrypted file with plaintext footer\n+        // if no fileDecryptor, can still read plaintext columns\n+        fileDecryptor.setFileCryptoMetaData(fileMetaData.getEncryption_algorithm(), false, \n+            fileMetaData.getFooter_signing_key_metadata());\n+        if (fileDecryptor.checkFooterIntegrity()) {\n+          verifyFooterIntegrity(from, fileDecryptor, combinedFooterLength);\n+        }\n+      }\n+    }\n+    \n+    ParquetMetadata parquetMetadata = fromParquetMetadata(fileMetaData, fileDecryptor, encryptedFooter);\n     if (LOG.isDebugEnabled()) LOG.debug(ParquetMetadata.toPrettyJSON(parquetMetadata));\n     return parquetMetadata;\n   }\n+  \n+  public ColumnChunkMetaData buildColumnChunkMetaData(ColumnMetaData metaData, ColumnPath columnPath, PrimitiveType type, String createdBy) {\n+    return ColumnChunkMetaData.get(\n+        columnPath,\n+        type,\n+        fromFormatCodec(metaData.codec),\n+        convertEncodingStats(metaData.getEncoding_stats()),\n+        fromFormatEncodings(metaData.encodings),\n+        fromParquetStatistics(\n+            createdBy,\n+            metaData.statistics,\n+            type),\n+        metaData.data_page_offset,\n+        metaData.dictionary_page_offset,\n+        metaData.num_values,\n+        metaData.total_compressed_size,\n+        metaData.total_uncompressed_size);\n+  }\n \n   public ParquetMetadata fromParquetMetadata(FileMetaData parquetMetadata) throws IOException {\n+    return fromParquetMetadata(parquetMetadata, null, false);\n+  }\n+\n+  public ParquetMetadata fromParquetMetadata(FileMetaData parquetMetadata, \n+      InternalFileDecryptor fileDecryptor, boolean encryptedFooter) throws IOException {\n     MessageType messageType = fromParquetSchema(parquetMetadata.getSchema(), parquetMetadata.getColumn_orders());\n     List<BlockMetaData> blocks = new ArrayList<BlockMetaData>();\n     List<RowGroup> row_groups = parquetMetadata.getRow_groups();\n+    \n     if (row_groups != null) {\n       for (RowGroup rowGroup : row_groups) {\n         BlockMetaData blockMetaData = new BlockMetaData();\n         blockMetaData.setRowCount(rowGroup.getNum_rows());\n         blockMetaData.setTotalByteSize(rowGroup.getTotal_byte_size());\n+        // not set in legacy files\n+        if (rowGroup.isSetOrdinal()) {\n+          blockMetaData.setOrdinal(rowGroup.getOrdinal());\n+        }\n         List<ColumnChunk> columns = rowGroup.getColumns();\n         String filePath = columns.get(0).getFile_path();\n+        short columnOrdinal = -1;\n         for (ColumnChunk columnChunk : columns) {\n+          columnOrdinal++;\n           if ((filePath == null && columnChunk.getFile_path() != null)\n               || (filePath != null && !filePath.equals(columnChunk.getFile_path()))) {\n             throw new ParquetDecodingException(\"all column chunks of the same row group must be in the same file for now\");\n           }\n           ColumnMetaData metaData = columnChunk.meta_data;\n-          ColumnPath path = getPath(metaData);\n-          ColumnChunkMetaData column = ColumnChunkMetaData.get(\n-              path,\n-              messageType.getType(path.toArray()).asPrimitiveType(),\n-              fromFormatCodec(metaData.codec),\n-              convertEncodingStats(metaData.getEncoding_stats()),\n-              fromFormatEncodings(metaData.encodings),\n-              fromParquetStatistics(\n-                  parquetMetadata.getCreated_by(),\n-                  metaData.statistics,\n-                  messageType.getType(path.toArray()).asPrimitiveType()),\n-              metaData.data_page_offset,\n-              metaData.dictionary_page_offset,\n-              metaData.num_values,\n-              metaData.total_compressed_size,\n-              metaData.total_uncompressed_size);\n+          ColumnCryptoMetaData cryptoMetaData = columnChunk.getCrypto_metadata();\n+          ColumnChunkMetaData column = null;\n+          ColumnPath columnPath = null;\n+          boolean encryptedMetadata = false;\n+          \n+          if (null == cryptoMetaData) { // Plaintext column\n+            if (null == metaData) {\n+              throw new IOException(\"ColumnMetaData not set in plaintext column\");\n+            }\n+            columnPath = getPath(metaData);\n+            if (null != fileDecryptor && !fileDecryptor.plaintextFile()) {\n+              // mark this column as plaintext in encrypted file decryptor\n+              fileDecryptor.setColumnCryptoMetadata(columnPath, false, false, (byte[]) null, columnOrdinal);\n+            }\n+          } else {  // Encrypted column\n+            boolean encryptedWithFooterKey = cryptoMetaData.isSetENCRYPTION_WITH_FOOTER_KEY();\n+            if (encryptedWithFooterKey) { // Column encrypted with footer key\n+              if (!encryptedFooter) {\n+                throw new IOException(\"Column encrypted with footer key in file with plaintext footer\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 388}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzk4ODI2MQ==", "bodyText": "Please, check your code to not to introduce any trailing whitespaces.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r427988261", "createdAt": "2020-05-20T12:59:10Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java", "diffHunk": "@@ -1465,23 +1674,38 @@ public void writeDataPageV2Header(\n             dataEncoding,\n             rlByteLength, dlByteLength), to);\n   }\n-\n+  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 443}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAwNzg4NQ==", "bodyText": "Why do we introduce new public methods that are deprecated already?", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428007885", "createdAt": "2020-05-20T13:25:53Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -442,7 +460,13 @@ static ParquetMetadata readSummaryMetadata(Configuration configuration, Path bas\n    */\n   @Deprecated\n   public static final ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException {\n-    return readFooter(configuration, file, NO_FILTER);\n+    return readFooter(configuration, file, getDecryptionProperties(file, configuration));\n+  }\n+\n+  @Deprecated\n+  public static final ParquetMetadata readFooter(Configuration configuration, Path file, ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAwODM1Mw==", "bodyText": "Why do we introduce new public methods that are deprecated already?", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428008353", "createdAt": "2020-05-20T13:26:28Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -499,55 +528,100 @@ public static final ParquetMetadata readFooter(Configuration configuration, File\n    */\n   @Deprecated\n   public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter) throws IOException {\n+    return readFooter(file, filter, null);\n+  }\n+\n+  @Deprecated\n+  public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter, ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAxMTg3MA==", "bodyText": "I would use the specific crypto exception here.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428011870", "createdAt": "2020-05-20T13:31:01Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -499,55 +528,100 @@ public static final ParquetMetadata readFooter(Configuration configuration, File\n    */\n   @Deprecated\n   public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter) throws IOException {\n+    return readFooter(file, filter, null);\n+  }\n+\n+  @Deprecated\n+  public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter, \n+      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n     ParquetReadOptions options;\n     if (file instanceof HadoopInputFile) {\n-      options = HadoopReadOptions.builder(((HadoopInputFile) file).getConfiguration())\n+      HadoopInputFile hadoopFile = (HadoopInputFile) file;\n+      options = HadoopReadOptions.builder(hadoopFile.getConfiguration())\n           .withMetadataFilter(filter).build();\n+      if (null == fileDecryptionProperties) {\n+        fileDecryptionProperties = getDecryptionProperties(hadoopFile.getPath(), hadoopFile.getConfiguration());\n+      }\n     } else {\n       options = ParquetReadOptions.builder().withMetadataFilter(filter).build();\n     }\n \n     try (SeekableInputStream in = file.newStream()) {\n-      return readFooter(file, options, in);\n+      return readFooter(file, options, in, fileDecryptionProperties);\n     }\n   }\n \n-  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, SeekableInputStream f) throws IOException {\n+  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, SeekableInputStream f, \n+      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n     ParquetMetadataConverter converter = new ParquetMetadataConverter(options);\n-    return readFooter(file, options, f, converter);\n+    return readFooter(file, options, f, converter, fileDecryptionProperties);\n   }\n \n-  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, SeekableInputStream f, ParquetMetadataConverter converter) throws IOException {\n+  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, \n+      SeekableInputStream f, ParquetMetadataConverter converter, \n+      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n+\n     long fileLen = file.getLength();\n+    String filePath = file.toString();\n     LOG.debug(\"File length {}\", fileLen);\n+\n     int FOOTER_LENGTH_SIZE = 4;\n     if (fileLen < MAGIC.length + FOOTER_LENGTH_SIZE + MAGIC.length) { // MAGIC + data + footer + footerIndex + MAGIC\n-      throw new RuntimeException(file.toString() + \" is not a Parquet file (too small length: \" + fileLen + \")\");\n+      throw new RuntimeException(filePath + \" is not a Parquet file (length is too low: \" + fileLen + \")\");\n     }\n-    long footerLengthIndex = fileLen - FOOTER_LENGTH_SIZE - MAGIC.length;\n-    LOG.debug(\"reading footer index at {}\", footerLengthIndex);\n \n-    f.seek(footerLengthIndex);\n-    int footerLength = readIntLittleEndian(f);\n+    // Read footer length and magic string - with a single seek\n     byte[] magic = new byte[MAGIC.length];\n+    long fileMetadataLengthIndex = fileLen - magic.length - FOOTER_LENGTH_SIZE;\n+    LOG.debug(\"reading footer index at {}\", fileMetadataLengthIndex);\n+    f.seek(fileMetadataLengthIndex);\n+    int fileMetadataLength = readIntLittleEndian(f);\n     f.readFully(magic);\n-    if (!Arrays.equals(MAGIC, magic)) {\n-      throw new RuntimeException(file.toString() + \" is not a Parquet file. expected magic number at tail \" + Arrays.toString(MAGIC) + \" but found \" + Arrays.toString(magic));\n+\n+    boolean encryptedFooterMode;\n+    if (Arrays.equals(MAGIC, magic)) {\n+      encryptedFooterMode = false;\n+    } else if (Arrays.equals(EFMAGIC, magic)) {\n+      encryptedFooterMode = true;\n+    } else {\n+      throw new RuntimeException(filePath + \" is not a Parquet file. Expected magic number at tail, but found \" + Arrays.toString(magic));\n+    }\n+\n+    long fileMetadataIndex = fileMetadataLengthIndex - fileMetadataLength;\n+    LOG.debug(\"read footer length: {}, footer index: {}\", fileMetadataLength, fileMetadataIndex);\n+    if (fileMetadataIndex < magic.length || fileMetadataIndex >= fileMetadataLengthIndex) {\n+      throw new RuntimeException(\"corrupted file: the footer index is not within the file: \" + fileMetadataIndex);\n     }\n-    long footerIndex = footerLengthIndex - footerLength;\n-    LOG.debug(\"read footer length: {}, footer index: {}\", footerLength, footerIndex);\n-    if (footerIndex < MAGIC.length || footerIndex >= footerLengthIndex) {\n-      throw new RuntimeException(\"corrupted file: the footer index is not within the file: \" + footerIndex);\n+    f.seek(fileMetadataIndex);\n+\n+    InternalFileDecryptor fileDecryptor = null;\n+    if (null != fileDecryptionProperties) {\n+      fileDecryptor  = new InternalFileDecryptor(fileDecryptionProperties);\n     }\n-    f.seek(footerIndex);\n+\n     // Read all the footer bytes in one time to avoid multiple read operations,\n     // since it can be pretty time consuming for a single read operation in HDFS.\n-    ByteBuffer footerBytesBuffer = ByteBuffer.allocate(footerLength);\n+    ByteBuffer footerBytesBuffer = ByteBuffer.allocate(fileMetadataLength);\n     f.readFully(footerBytesBuffer);\n     LOG.debug(\"Finished to read all footer bytes.\");\n     footerBytesBuffer.flip();\n     InputStream footerBytesStream = ByteBufferInputStream.wrap(footerBytesBuffer);\n-    return converter.readParquetMetadata(footerBytesStream, options.getMetadataFilter());\n+\n+    // Regular file, or encrypted file with plaintext footer\n+    if (!encryptedFooterMode) {\n+      return converter.readParquetMetadata(footerBytesStream, options.getMetadataFilter(), fileDecryptor, false, \n+          fileMetadataLength);\n+    }\n+\n+    // Encrypted file with encrypted footer\n+    if (null == fileDecryptor) {\n+      throw new RuntimeException(\"Trying to read file with encrypted footer. No keys available\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 205}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAzNDA3NA==", "bodyText": "I think, it would be better to put the FileDecryptionProperties into ParquetReadOptions. HadoopReadOptions (the extension of ParquetReadOptions) already contains the hadoop conf so you may be able to create FileDecryptionProperties from there if not set.\nThis way you do not need to add new methods/constructors where ParquetReadOptions is already there as an argument.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428034074", "createdAt": "2020-05-20T13:58:26Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -705,22 +797,39 @@ public ParquetFileReader(Configuration conf, Path file, ParquetMetadata footer)\n       paths.put(ColumnPath.get(col.getPath()), col);\n     }\n     this.crc = options.usePageChecksumVerification() ? new CRC32() : null;\n+    this.fileDecryptor = fileMetaData.getFileDecryptor();\n   }\n \n   public ParquetFileReader(InputFile file, ParquetReadOptions options) throws IOException {\n+    this(file, options, null);\n+  }\n+\n+  public ParquetFileReader(InputFile file, ParquetReadOptions options, \n+      FileDecryptionProperties fileDecryptionProperties) throws IOException {\n     this.converter = new ParquetMetadataConverter(options);\n     this.file = file;\n     this.f = file.newStream();\n     this.options = options;\n+    if ((null == fileDecryptionProperties) && (file instanceof HadoopInputFile)) {\n+      HadoopInputFile hadoopFile = (HadoopInputFile) file;\n+      fileDecryptionProperties = getDecryptionProperties(hadoopFile.getPath(), hadoopFile.getConfiguration());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 275}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA0MTI0OQ==", "bodyText": "I think, we can replace it.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428041249", "createdAt": "2020-05-20T14:07:23Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1021,18 +1148,44 @@ DictionaryPage readDictionary(ColumnChunkMetaData meta) throws IOException {\n         !meta.getEncodings().contains(Encoding.RLE_DICTIONARY)) {\n       return null;\n     }\n+    /** TODO Gabor - can be replaced with this?:\n+    if (!meta.hasDictionaryPage()) {\n+      return null;\n+    } */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 375}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE2MDE2Mjg5", "url": "https://github.com/apache/parquet-mr/pull/776#pullrequestreview-416016289", "createdAt": "2020-05-21T09:48:17Z", "commit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQwOTo0ODoxN1rOGYs3JQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxMzo1MTowOVrOGYzkEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU1NDAyMQ==", "bodyText": "@chenjunjiedada, could you please check this?", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428554021", "createdAt": "2020-05-21T09:48:17Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1071,12 +1229,31 @@ public BloomFilterReader getBloomFilterDataReader(BlockMetaData block) {\n    */\n   public BloomFilter readBloomFilter(ColumnChunkMetaData meta) throws IOException {\n     long bloomFilterOffset = meta.getBloomFilterOffset();\n+\n+    if (0 == bloomFilterOffset) { // TODO Junjie - is there a better way to handle this?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 441}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU1NjQ0Nw==", "bodyText": "I guess, it should be the specific crypto exception instead.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428556447", "createdAt": "2020-05-21T09:53:42Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1095,8 +1272,16 @@ public BloomFilter readBloomFilter(ColumnChunkMetaData meta) throws IOException\n       return null;\n     }\n \n-    byte[] bitset = new byte[numBytes];\n-    f.readFully(bitset);\n+    byte[] bitset;\n+    if (null == bloomFilterDecryptor) {\n+      bitset = new byte[numBytes];\n+      f.readFully(bitset);\n+    } else {\n+      bitset = bloomFilterDecryptor.decrypt(f, bloomFilterBitsetAAD);\n+      if (bitset.length != numBytes) {\n+        throw new IOException(\"Wrong length of decrypted bloom filter bitset\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 482}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU2MDM2OA==", "bodyText": "My understanding about this method is that it is invoked inside the ColumnChunkMetaData object when retrieving a value that might be encrypted. Why do we need to call it here?", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428560368", "createdAt": "2020-05-21T10:02:21Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1114,7 +1299,20 @@ public ColumnIndex readColumnIndex(ColumnChunkMetaData column) throws IOExceptio\n       return null;\n     }\n     f.seek(ref.getOffset());\n-    return ParquetMetadataConverter.fromParquetColumnIndex(column.getPrimitiveType(), Util.readColumnIndex(f));\n+\n+    column.decryptIfNeededed();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 494}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU2MjA3NA==", "bodyText": "Same as above.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428562074", "createdAt": "2020-05-21T10:06:23Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java", "diffHunk": "@@ -1131,7 +1329,19 @@ public OffsetIndex readOffsetIndex(ColumnChunkMetaData column) throws IOExceptio\n       return null;\n     }\n     f.seek(ref.getOffset());\n-    return ParquetMetadataConverter.fromParquetOffsetIndex(Util.readOffsetIndex(f));\n+\n+    column.decryptIfNeededed();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 516}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU2NDY1Nw==", "bodyText": "Just like for MAGIC:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              public static final byte[] EFMAGIC = EF_MAGIC_STR.getBytes(Charset.forName(\"ASCII\"));\n          \n          \n            \n              public static final byte[] EFMAGIC = EF_MAGIC_STR.getBytes(StandardCharsets.US_ASCII);", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428564657", "createdAt": "2020-05-21T10:12:17Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java", "diffHunk": "@@ -90,6 +102,8 @@\n   public static final String PARQUET_METADATA_FILE = \"_metadata\";\n   public static final String MAGIC_STR = \"PAR1\";\n   public static final byte[] MAGIC = MAGIC_STR.getBytes(StandardCharsets.US_ASCII);\n+  public static final String EF_MAGIC_STR = \"PARE\";\n+  public static final byte[] EFMAGIC = EF_MAGIC_STR.getBytes(Charset.forName(\"ASCII\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYwNzE2Mg==", "bodyText": "blockSize is a bit misleading. It is going to be used as ordinal so what about rowGroupOrdinal or similar?", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428607162", "createdAt": "2020-05-21T11:55:06Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java", "diffHunk": "@@ -772,6 +860,11 @@ public void endBlock() throws IOException {\n     state = state.endBlock();\n     LOG.debug(\"{}: end block\", out.getPos());\n     currentBlock.setRowCount(currentRecordCount);\n+    int blockSize = blocks.size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 208}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYyMTg3Ng==", "bodyText": "I don't know why we think that the footer length is an important information but this is the only case where we do not log it. We might want to add it here as well.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428621876", "createdAt": "2020-05-21T12:29:58Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java", "diffHunk": "@@ -1035,20 +1154,89 @@ private static void serializeBloomFilters(\n \n         long offset = out.getPos();\n         column.setBloomFilterOffset(offset);\n-        Util.writeBloomFilterHeader(ParquetMetadataConverter.toBloomFilterHeader(bloomFilter), out);\n-        bloomFilter.writeTo(out);\n+        \n+        BlockCipher.Encryptor bloomFilterEncryptor = null;\n+        byte[] bloomFilterHeaderAAD = null;\n+        byte[] bloomFilterBitsetAAD = null;\n+        if (null != fileEncryptor) {\n+          InternalColumnEncryptionSetup columnEncryptionSetup = fileEncryptor.getColumnSetup(column.getPath(), false, (short) cIndex);\n+          if (columnEncryptionSetup.isEncrypted()) {\n+            bloomFilterEncryptor = columnEncryptionSetup.getMetaDataEncryptor();\n+            short columnOrdinal = columnEncryptionSetup.getOrdinal();\n+            bloomFilterHeaderAAD = AesCipher.createModuleAAD(fileEncryptor.getFileAAD(), ModuleType.BloomFilterHeader, \n+                block.getOrdinal(), columnOrdinal, (short)-1);\n+            bloomFilterBitsetAAD = AesCipher.createModuleAAD(fileEncryptor.getFileAAD(), ModuleType.BloomFilterBitset, \n+                block.getOrdinal(), columnOrdinal, (short)-1);\n+          }\n+        }\n+        \n+        Util.writeBloomFilterHeader(ParquetMetadataConverter.toBloomFilterHeader(bloomFilter), out, \n+            bloomFilterEncryptor, bloomFilterHeaderAAD);\n+        \n+        ByteArrayOutputStream tempOutStream = new ByteArrayOutputStream();\n+        bloomFilter.writeTo(tempOutStream);\n+        byte[] serializedBitset = tempOutStream.toByteArray();\n+        if (null != bloomFilterEncryptor) {\n+          serializedBitset = bloomFilterEncryptor.encrypt(serializedBitset, bloomFilterBitsetAAD);\n+        }\n+        out.write(serializedBitset);\n       }\n     }\n   }\n-\n-  private static void serializeFooter(ParquetMetadata footer, PositionOutputStream out) throws IOException {\n-    long footerIndex = out.getPos();\n+  \n+  private static void serializeFooter(ParquetMetadata footer, PositionOutputStream out,\n+      InternalFileEncryptor fileEncryptor) throws IOException {\n+    \n     ParquetMetadataConverter metadataConverter = new ParquetMetadataConverter();\n-    org.apache.parquet.format.FileMetaData parquetMetadata = metadataConverter.toParquetMetadata(CURRENT_VERSION, footer);\n-    writeFileMetaData(parquetMetadata, out);\n-    LOG.debug(\"{}: footer length = {}\" , out.getPos(), (out.getPos() - footerIndex));\n-    BytesUtils.writeIntLittleEndian(out, (int) (out.getPos() - footerIndex));\n-    out.write(MAGIC);\n+    \n+    // Unencrypted file\n+    if (null == fileEncryptor) {\n+      long footerIndex = out.getPos();\n+      org.apache.parquet.format.FileMetaData parquetMetadata = metadataConverter.toParquetMetadata(CURRENT_VERSION, footer);\n+      writeFileMetaData(parquetMetadata, out);\n+      LOG.debug(\"{}: footer length = {}\" , out.getPos(), (out.getPos() - footerIndex));\n+      BytesUtils.writeIntLittleEndian(out, (int) (out.getPos() - footerIndex));\n+      out.write(MAGIC);\n+      return;\n+    }\n+    \n+    org.apache.parquet.format.FileMetaData parquetMetadata =\n+        metadataConverter.toParquetMetadata(CURRENT_VERSION, footer, fileEncryptor);\n+    \n+    // Encrypted file with plaintext footer \n+    if (!fileEncryptor.isFooterEncrypted()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 381}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYyNjMyOQ==", "bodyText": "I think, createEncryptionProperties or similar would be a better naming.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428626329", "createdAt": "2020-05-21T12:39:42Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java", "diffHunk": "@@ -539,4 +544,13 @@ public OutputCommitter getOutputCommitter(TaskAttemptContext context)\n   public synchronized static MemoryManager getMemoryManager() {\n     return memoryManager;\n   }\n+  \n+  private FileEncryptionProperties getEncryptionProperties(Configuration fileHadoopConfig, Path tempFilePath, ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYyODc5Mg==", "bodyText": "It is not clear to me why we need to suppress this exception.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428628792", "createdAt": "2020-05-21T12:45:12Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetRecordReader.java", "diffHunk": "@@ -188,7 +189,11 @@ private void checkDeltaByteArrayProblem(FileMetaData meta, Configuration conf, B\n       // this is okay if not using DELTA_BYTE_ARRAY with the bug\n       Set<Encoding> encodings = new HashSet<Encoding>();\n       for (ColumnChunkMetaData column : block.getColumns()) {\n-        encodings.addAll(column.getEncodings());\n+        try {\n+          encodings.addAll(column.getEncodings());\n+        } catch (KeyAccessDeniedException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYyOTc0Nw==", "bodyText": "Similarly to another one of my comments. Instead of adding more deprecated methods/constructors, try to use the non-deprecated once in the code. If not used internally, we should not introduce these.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428629747", "createdAt": "2020-05-21T12:47:21Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "diffHunk": "@@ -194,6 +195,23 @@ public ParquetWriter(\n         enableDictionary, validating, writerVersion, conf);\n   }\n \n+  @Deprecated\n+  public ParquetWriter(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYzNDg4MA==", "bodyText": "The column path is already part of properties which whole purpose is to save memory. If you need this for EncryptedColumnChunkMetaData, add it there instead.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428634880", "createdAt": "2020-05-21T12:58:00Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -34,6 +47,9 @@\n  * Column meta data for a block stored in the file footer and passed in the InputSplit\n  */\n abstract public class ColumnChunkMetaData {\n+  \n+  protected ColumnPath path;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODYzNjEwOQ==", "bodyText": "The design seems to require this method to be only invoked inside this package. So please, make it package private.\nYou may have an empty implementation here so you don't need to add the empty implementations in the Int/Long classes.\nAlso, a bit too many \"ed\"s:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              abstract public void decryptIfNeededed();\n          \n          \n            \n              abstract public void decryptIfNeeded();", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428636109", "createdAt": "2020-05-21T13:00:31Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -241,6 +280,8 @@ public PrimitiveType getPrimitiveType() {\n    * @return the stats for this column\n    */\n   abstract public Statistics getStatistics();\n+  \n+  abstract public void decryptIfNeededed();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0MjUxMg==", "bodyText": "These are accessed only inside the same class so you may keep them private.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428642512", "createdAt": "2020-05-21T13:13:20Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -165,10 +200,10 @@ protected static boolean positiveLongFitsInAnInt(long value) {\n     return (value >= 0) && (value + Integer.MIN_VALUE <= Integer.MAX_VALUE);\n   }\n \n-  private final EncodingStats encodingStats;\n+  protected EncodingStats encodingStats;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY0OTAzMg==", "bodyText": "I would expect some comments here why we need this object (to decrypt this metadata lazily instead of simply decrypt it at reading).", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428649032", "createdAt": "2020-05-21T13:25:37Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java", "diffHunk": "@@ -141,11 +159,28 @@ public static ColumnChunkMetaData get(\n           totalUncompressedSize);\n     }\n   }\n+  \n+  public static ColumnChunkMetaData getWithEncryptedMetadata(ParquetMetadataConverter parquetMetadataConverter, ColumnPath path, ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY1NzYzMw==", "bodyText": "I guess, this test and the one for column indexes were created by copy-pasting the original tests and adding the encryption. Since we already use parameterized testing in both I would suggest keeping the original tests and adding another dimension for plain/encrypted or similar. This way we would have 4 runs for each tests: (V1 with plain), (V1 with encryption), (V2 with plain) and (V2 with encryption).", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428657633", "createdAt": "2020-05-21T13:40:40Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestBloomEncryption.java", "diffHunk": "@@ -0,0 +1,313 @@\n+/*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY1OTI2NA==", "bodyText": "I would suggest creating a temporary dir in temporary space instead of having a relative path depending on the current directory. This way it is a bit error prone.\n(There are a couple of ways to create and cleanup temporary directories in junit.)", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428659264", "createdAt": "2020-05-21T13:43:20Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Random;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.StringKeyIdRetriever;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+\n+\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestEncryption {\n+\n+  @Test\n+  public void test() throws Exception {\n+    Configuration conf = new Configuration();\n+    Path root = new Path(\"target/tests/TestEncryption/\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY2MDg2Mw==", "bodyText": "Consider using try-with-resources. That construct would be less error prone (e.g. closing the writer/reader in case of an exception occurs).", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428660863", "createdAt": "2020-05-21T13:46:07Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Random;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.StringKeyIdRetriever;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+\n+\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestEncryption {\n+\n+  @Test\n+  public void test() throws Exception {\n+    Configuration conf = new Configuration();\n+    Path root = new Path(\"target/tests/TestEncryption/\");\n+    enforceEmptyDir(conf, root);\n+\n+    Random random = new Random();\n+    int numberOfEncryptionModes = 5;\n+    FileEncryptionProperties[] encryptionPropertiesList = new FileEncryptionProperties[numberOfEncryptionModes];\n+    FileDecryptionProperties[] decryptionPropertiesList = new FileDecryptionProperties[numberOfEncryptionModes];\n+\n+    // #0 Unencrypted - make sure null encryption properties don't break regular Parquet\n+    encryptionPropertiesList[0] = null;\n+    decryptionPropertiesList[0] = null;\n+\n+    // #1 Basic encryption setup\n+    byte[] encryptionKey = new byte[16];\n+    random.nextBytes(encryptionKey);\n+    FileEncryptionProperties encryptionProperties = FileEncryptionProperties.builder(encryptionKey).build();\n+    FileDecryptionProperties decryptionProperties = FileDecryptionProperties.builder().withFooterKey(encryptionKey).build();\n+    encryptionPropertiesList[1] = encryptionProperties;\n+    decryptionPropertiesList[1] = decryptionProperties;\n+\n+    // #2 Default algorithm, non-uniform encryption, key metadata, key retriever, AAD prefix\n+    byte[] footerKey = new byte[16];\n+    random.nextBytes(footerKey);\n+    byte[] columnKey0 = new byte[16];\n+    random.nextBytes(columnKey0);\n+    byte[] columnKey1 = new byte[16];\n+    random.nextBytes(columnKey1);\n+    ColumnEncryptionProperties columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    ColumnEncryptionProperties columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    HashMap<ColumnPath, ColumnEncryptionProperties> columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    byte[] AADPrefix = root.getName().getBytes(StandardCharsets.UTF_8);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    StringKeyIdRetriever keyRetriever = new StringKeyIdRetriever();\n+    keyRetriever.putKey(\"fk\", footerKey);\n+    keyRetriever.putKey(\"ck0\", columnKey0);\n+    keyRetriever.putKey(\"ck1\", columnKey1);\n+    decryptionProperties = FileDecryptionProperties.builder()\n+        .withKeyRetriever(keyRetriever)\n+        .build();\n+    encryptionPropertiesList[2] = encryptionProperties;\n+    decryptionPropertiesList[2] = decryptionProperties;\n+\n+    // #3 GCM_CTR algorithm, non-uniform encryption, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withAlgorithm(ParquetCipher.AES_GCM_CTR_V1)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[3] = encryptionProperties;\n+    decryptionPropertiesList[3] = decryptionProperties; // Same decryption properties\n+\n+    // #4  Plaintext footer, default algorithm, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withPlaintextFooter()\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[4] = encryptionProperties;\n+    decryptionPropertiesList[4] = decryptionProperties; // Same decryption properties\n+\n+\n+    MessageType schema = parseMessageType(\n+        \"message test { \"\n+            + \"required binary binary_field; \"\n+            + \"required int32 int32_field; \"\n+            + \"required int64 int64_field; \"\n+            + \"required boolean boolean_field; \"\n+            + \"required float float_field; \"\n+            + \"required double double_field; \"\n+            + \"required fixed_len_byte_array(3) flba_field; \"\n+            + \"required int96 int96_field; \"\n+            + \"} \");\n+    GroupWriteSupport.setSchema(schema, conf);\n+    SimpleGroupFactory f = new SimpleGroupFactory(schema);\n+\n+    for (int encryptionMode = 0; encryptionMode < numberOfEncryptionModes; encryptionMode++) {\n+      System.out.println(\"MODE: \"+encryptionMode);\n+      \n+      Path file = new Path(root, \"m_\" + encryptionMode + \".parquet.encrypted\");\n+      ParquetWriter<Group> writer = new ParquetWriter<Group>(\n+          file,\n+          new GroupWriteSupport(),\n+          UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetWriter.DEFAULT_WRITER_VERSION, conf, \n+          encryptionPropertiesList[encryptionMode]);\n+      for (int i = 0; i < 1000; i++) {\n+        writer.write(\n+            f.newGroup()\n+            .append(\"binary_field\", \"test\" + i)\n+            .append(\"int32_field\", 32)\n+            .append(\"int64_field\", 64l)\n+            .append(\"boolean_field\", true)\n+            .append(\"float_field\", 1.0f)\n+            .append(\"double_field\", 2.0d)\n+            .append(\"flba_field\", \"foo\")\n+            .append(\"int96_field\", Binary.fromConstantByteArray(new byte[12])));\n+      }\n+      writer.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 188}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY2MjI2MA==", "bodyText": "Yep, that's one good way I was talking about but you should use temp instead of hardcoding the directory at the beginning.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428662260", "createdAt": "2020-05-21T13:48:35Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Random;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.StringKeyIdRetriever;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+\n+\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestEncryption {\n+\n+  @Test\n+  public void test() throws Exception {\n+    Configuration conf = new Configuration();\n+    Path root = new Path(\"target/tests/TestEncryption/\");\n+    enforceEmptyDir(conf, root);\n+\n+    Random random = new Random();\n+    int numberOfEncryptionModes = 5;\n+    FileEncryptionProperties[] encryptionPropertiesList = new FileEncryptionProperties[numberOfEncryptionModes];\n+    FileDecryptionProperties[] decryptionPropertiesList = new FileDecryptionProperties[numberOfEncryptionModes];\n+\n+    // #0 Unencrypted - make sure null encryption properties don't break regular Parquet\n+    encryptionPropertiesList[0] = null;\n+    decryptionPropertiesList[0] = null;\n+\n+    // #1 Basic encryption setup\n+    byte[] encryptionKey = new byte[16];\n+    random.nextBytes(encryptionKey);\n+    FileEncryptionProperties encryptionProperties = FileEncryptionProperties.builder(encryptionKey).build();\n+    FileDecryptionProperties decryptionProperties = FileDecryptionProperties.builder().withFooterKey(encryptionKey).build();\n+    encryptionPropertiesList[1] = encryptionProperties;\n+    decryptionPropertiesList[1] = decryptionProperties;\n+\n+    // #2 Default algorithm, non-uniform encryption, key metadata, key retriever, AAD prefix\n+    byte[] footerKey = new byte[16];\n+    random.nextBytes(footerKey);\n+    byte[] columnKey0 = new byte[16];\n+    random.nextBytes(columnKey0);\n+    byte[] columnKey1 = new byte[16];\n+    random.nextBytes(columnKey1);\n+    ColumnEncryptionProperties columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    ColumnEncryptionProperties columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    HashMap<ColumnPath, ColumnEncryptionProperties> columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    byte[] AADPrefix = root.getName().getBytes(StandardCharsets.UTF_8);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    StringKeyIdRetriever keyRetriever = new StringKeyIdRetriever();\n+    keyRetriever.putKey(\"fk\", footerKey);\n+    keyRetriever.putKey(\"ck0\", columnKey0);\n+    keyRetriever.putKey(\"ck1\", columnKey1);\n+    decryptionProperties = FileDecryptionProperties.builder()\n+        .withKeyRetriever(keyRetriever)\n+        .build();\n+    encryptionPropertiesList[2] = encryptionProperties;\n+    decryptionPropertiesList[2] = decryptionProperties;\n+\n+    // #3 GCM_CTR algorithm, non-uniform encryption, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withAlgorithm(ParquetCipher.AES_GCM_CTR_V1)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[3] = encryptionProperties;\n+    decryptionPropertiesList[3] = decryptionProperties; // Same decryption properties\n+\n+    // #4  Plaintext footer, default algorithm, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withPlaintextFooter()\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[4] = encryptionProperties;\n+    decryptionPropertiesList[4] = decryptionProperties; // Same decryption properties\n+\n+\n+    MessageType schema = parseMessageType(\n+        \"message test { \"\n+            + \"required binary binary_field; \"\n+            + \"required int32 int32_field; \"\n+            + \"required int64 int64_field; \"\n+            + \"required boolean boolean_field; \"\n+            + \"required float float_field; \"\n+            + \"required double double_field; \"\n+            + \"required fixed_len_byte_array(3) flba_field; \"\n+            + \"required int96 int96_field; \"\n+            + \"} \");\n+    GroupWriteSupport.setSchema(schema, conf);\n+    SimpleGroupFactory f = new SimpleGroupFactory(schema);\n+\n+    for (int encryptionMode = 0; encryptionMode < numberOfEncryptionModes; encryptionMode++) {\n+      System.out.println(\"MODE: \"+encryptionMode);\n+      \n+      Path file = new Path(root, \"m_\" + encryptionMode + \".parquet.encrypted\");\n+      ParquetWriter<Group> writer = new ParquetWriter<Group>(\n+          file,\n+          new GroupWriteSupport(),\n+          UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetWriter.DEFAULT_WRITER_VERSION, conf, \n+          encryptionPropertiesList[encryptionMode]);\n+      for (int i = 0; i < 1000; i++) {\n+        writer.write(\n+            f.newGroup()\n+            .append(\"binary_field\", \"test\" + i)\n+            .append(\"int32_field\", 32)\n+            .append(\"int64_field\", 64l)\n+            .append(\"boolean_field\", true)\n+            .append(\"float_field\", 1.0f)\n+            .append(\"double_field\", 2.0d)\n+            .append(\"flba_field\", \"foo\")\n+            .append(\"int96_field\", Binary.fromConstantByteArray(new byte[12])));\n+      }\n+      writer.close();\n+\n+      FileDecryptionProperties fileDecryptionProperties = decryptionPropertiesList[encryptionMode];\n+      ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), file)\n+          .withDecryption(fileDecryptionProperties).withConf(conf).build();\n+      for (int i = 0; i < 1000; i++) {\n+        Group group = null;\n+        group= reader.read();\n+        assertEquals(\"test\" + i, group.getBinary(\"binary_field\", 0).toStringUsingUTF8());\n+        assertEquals(32, group.getInteger(\"int32_field\", 0));\n+        assertEquals(64l, group.getLong(\"int64_field\", 0));\n+        assertEquals(true, group.getBoolean(\"boolean_field\", 0));\n+        assertEquals(1.0f, group.getFloat(\"float_field\", 0), 0.001);\n+        assertEquals(2.0d, group.getDouble(\"double_field\", 0), 0.001);\n+        assertEquals(\"foo\", group.getBinary(\"flba_field\", 0).toStringUsingUTF8());\n+        assertEquals(Binary.fromConstantByteArray(new byte[12]),\n+            group.getInt96(\"int96_field\",0));\n+      }\n+      reader.close();\n+    }\n+    enforceEmptyDir(conf, root);\n+  }\n+\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY2MzgyNw==", "bodyText": "I don't know if make to much sense to write the same data 1000 times. Most of our tests are working by generating random data in memory (e.g. a List<Group>) the write it and then test whether we can read back the same data we have in memory.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428663827", "createdAt": "2020-05-21T13:51:09Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestEncryption.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.apache.parquet.hadoop.TestUtils.enforceEmptyDir;\n+import static org.apache.parquet.hadoop.metadata.CompressionCodecName.UNCOMPRESSED;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.Random;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.StringKeyIdRetriever;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroupFactory;\n+\n+\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestEncryption {\n+\n+  @Test\n+  public void test() throws Exception {\n+    Configuration conf = new Configuration();\n+    Path root = new Path(\"target/tests/TestEncryption/\");\n+    enforceEmptyDir(conf, root);\n+\n+    Random random = new Random();\n+    int numberOfEncryptionModes = 5;\n+    FileEncryptionProperties[] encryptionPropertiesList = new FileEncryptionProperties[numberOfEncryptionModes];\n+    FileDecryptionProperties[] decryptionPropertiesList = new FileDecryptionProperties[numberOfEncryptionModes];\n+\n+    // #0 Unencrypted - make sure null encryption properties don't break regular Parquet\n+    encryptionPropertiesList[0] = null;\n+    decryptionPropertiesList[0] = null;\n+\n+    // #1 Basic encryption setup\n+    byte[] encryptionKey = new byte[16];\n+    random.nextBytes(encryptionKey);\n+    FileEncryptionProperties encryptionProperties = FileEncryptionProperties.builder(encryptionKey).build();\n+    FileDecryptionProperties decryptionProperties = FileDecryptionProperties.builder().withFooterKey(encryptionKey).build();\n+    encryptionPropertiesList[1] = encryptionProperties;\n+    decryptionPropertiesList[1] = decryptionProperties;\n+\n+    // #2 Default algorithm, non-uniform encryption, key metadata, key retriever, AAD prefix\n+    byte[] footerKey = new byte[16];\n+    random.nextBytes(footerKey);\n+    byte[] columnKey0 = new byte[16];\n+    random.nextBytes(columnKey0);\n+    byte[] columnKey1 = new byte[16];\n+    random.nextBytes(columnKey1);\n+    ColumnEncryptionProperties columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    ColumnEncryptionProperties columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    HashMap<ColumnPath, ColumnEncryptionProperties> columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    byte[] AADPrefix = root.getName().getBytes(StandardCharsets.UTF_8);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    StringKeyIdRetriever keyRetriever = new StringKeyIdRetriever();\n+    keyRetriever.putKey(\"fk\", footerKey);\n+    keyRetriever.putKey(\"ck0\", columnKey0);\n+    keyRetriever.putKey(\"ck1\", columnKey1);\n+    decryptionProperties = FileDecryptionProperties.builder()\n+        .withKeyRetriever(keyRetriever)\n+        .build();\n+    encryptionPropertiesList[2] = encryptionProperties;\n+    decryptionPropertiesList[2] = decryptionProperties;\n+\n+    // #3 GCM_CTR algorithm, non-uniform encryption, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withAlgorithm(ParquetCipher.AES_GCM_CTR_V1)\n+        .withFooterKeyID(\"fk\")\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[3] = encryptionProperties;\n+    decryptionPropertiesList[3] = decryptionProperties; // Same decryption properties\n+\n+    // #4  Plaintext footer, default algorithm, key metadata, key retriever, AAD\n+    columnProperties0 = ColumnEncryptionProperties.builder(\"binary_field\")\n+        .withKey(columnKey0)\n+        .withKeyID(\"ck0\")\n+        .build();\n+    columnProperties1 = ColumnEncryptionProperties.builder(\"int32_field\")\n+        .withKey(columnKey1)\n+        .withKeyID(\"ck1\")\n+        .build();\n+    columnPropertiesMap = new HashMap<ColumnPath, ColumnEncryptionProperties>();\n+    columnPropertiesMap.put(columnProperties0.getPath(), columnProperties0);\n+    columnPropertiesMap.put(columnProperties1.getPath(), columnProperties1);\n+    encryptionProperties = FileEncryptionProperties.builder(footerKey)\n+        .withFooterKeyID(\"fk\")\n+        .withPlaintextFooter()\n+        .withAADPrefix(AADPrefix)\n+        .withEncryptedColumns(columnPropertiesMap)\n+        .build();\n+    encryptionPropertiesList[4] = encryptionProperties;\n+    decryptionPropertiesList[4] = decryptionProperties; // Same decryption properties\n+\n+\n+    MessageType schema = parseMessageType(\n+        \"message test { \"\n+            + \"required binary binary_field; \"\n+            + \"required int32 int32_field; \"\n+            + \"required int64 int64_field; \"\n+            + \"required boolean boolean_field; \"\n+            + \"required float float_field; \"\n+            + \"required double double_field; \"\n+            + \"required fixed_len_byte_array(3) flba_field; \"\n+            + \"required int96 int96_field; \"\n+            + \"} \");\n+    GroupWriteSupport.setSchema(schema, conf);\n+    SimpleGroupFactory f = new SimpleGroupFactory(schema);\n+\n+    for (int encryptionMode = 0; encryptionMode < numberOfEncryptionModes; encryptionMode++) {\n+      System.out.println(\"MODE: \"+encryptionMode);\n+      \n+      Path file = new Path(root, \"m_\" + encryptionMode + \".parquet.encrypted\");\n+      ParquetWriter<Group> writer = new ParquetWriter<Group>(\n+          file,\n+          new GroupWriteSupport(),\n+          UNCOMPRESSED, 1024, 1024, 512, true, false, ParquetWriter.DEFAULT_WRITER_VERSION, conf, \n+          encryptionPropertiesList[encryptionMode]);\n+      for (int i = 0; i < 1000; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 176}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE2MTc0ODU3", "url": "https://github.com/apache/parquet-mr/pull/776#pullrequestreview-416174857", "createdAt": "2020-05-21T14:07:13Z", "commit": {"oid": "c9761c39a774caac1bde5875b41e3368e745c0b4"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxNDowNzoxM1rOGY0KuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxNDowNzo1NVrOGY0MNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY3MzcyMQ==", "bodyText": "I agree on validating the arguments is important. But the proper exception to be thrown for a null is a NullPointerException that would be thrown at line 134 anyway. If you really want to validate the argument for null at the first line of the method I would suggest using java.util.Objects.requireNonNull(Object).", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428673721", "createdAt": "2020-05-21T14:07:13Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -90,6 +102,10 @@\n \n   // Update last two bytes with new page ordinal (instead of creating new page AAD from scratch)\n   public static void quickUpdatePageAAD(byte[] pageAAD, short newPageOrdinal) {\n+    if (newPageOrdinal < 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQyOTQxNg=="}, "originalCommit": {"oid": "fdb1d59461a06bbbc418d75a0dec257d1e11013d"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODY3NDEwMw==", "bodyText": "I think, writing the actual value instead of referencing a java constant is more informative.", "url": "https://github.com/apache/parquet-mr/pull/776#discussion_r428674103", "createdAt": "2020-05-21T14:07:55Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/crypto/AesCipher.java", "diffHunk": "@@ -78,11 +78,22 @@\n     if (rowGroupOrdinal < 0) {\n       throw new IllegalArgumentException(\"Wrong row group ordinal: \" + rowGroupOrdinal);\n     }\n-    byte[] rowGroupOrdinalBytes = shortToBytesLE(rowGroupOrdinal);\n+    short shortRGOrdinal = (short) rowGroupOrdinal;\n+    if (shortRGOrdinal != rowGroupOrdinal) {\n+      throw new ParquetCryptoRuntimeException(\"Encrypted parquet files can't have \"\n+          + \"more than Short.MAX_VALUE row groups: \" + rowGroupOrdinal);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c9761c39a774caac1bde5875b41e3368e745c0b4"}, "originalPosition": 17}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a8cc10ec25eac5d3d21ec96129951fcd63a0a51", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/7a8cc10ec25eac5d3d21ec96129951fcd63a0a51", "committedDate": "2020-05-21T14:53:09Z", "message": "Update parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java\n\nCo-authored-by: Gabor Szadovszky <gabor@apache.org>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dded1bbfe9a588f7e74a6d80065c382151101844", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/dded1bbfe9a588f7e74a6d80065c382151101844", "committedDate": "2020-05-21T14:56:29Z", "message": "Update parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java\n\nCo-authored-by: Gabor Szadovszky <gabor@apache.org>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4a325b70b4deb434c9589edd209b1a0d22a1162a", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/4a325b70b4deb434c9589edd209b1a0d22a1162a", "committedDate": "2020-05-27T11:37:08Z", "message": "address review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3ed0b96643c097236ae66dbcde019da3331a13e", "author": {"user": {"login": "ggershinsky", "name": null}}, "url": "https://github.com/apache/parquet-mr/commit/f3ed0b96643c097236ae66dbcde019da3331a13e", "committedDate": "2020-05-29T05:57:53Z", "message": "field scope, rm comment"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIwODIwMzE4", "url": "https://github.com/apache/parquet-mr/pull/776#pullrequestreview-420820318", "createdAt": "2020-05-29T10:05:25Z", "commit": {"oid": "f3ed0b96643c097236ae66dbcde019da3331a13e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2194, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}