{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk5Mjg2NTg4", "number": 781, "reviewThreads": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwODoxOToyOVrODvc2LQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxMjo0ODo0NVrODyDmHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMDgyMjg1OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwODoxOToyOVrOGB4S-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNzozMjoyOVrOGCOjRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDYyNDEyMw==", "bodyText": "Why did you change it to 8MB? It says the default is 0 so it seems to me that this example can be anything.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r404624123", "createdAt": "2020-04-07T08:19:29Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java", "diffHunk": "@@ -90,7 +90,7 @@\n  *\n  * # Maximum size (in bytes) allowed as padding to align row groups\n  * # This is also the minimum size of a row group. Default: 0\n- * parquet.writer.max-padding=2097152 # 2 MB\n+ * parquet.writer.max-padding=8388608 # 8 MB", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDk4ODc0Mg==", "bodyText": "The default value equals to 8MB here: \n  \n    \n      parquet-mr/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java\n    \n    \n         Line 55\n      in\n      d00b2f1\n    \n    \n    \n    \n\n        \n          \n           public static final int MAX_PADDING_SIZE_DEFAULT = 8 * 1024 * 1024; // 8MB \n        \n    \n  \n\n\nI should also change the comment above to Default: 8388608", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r404988742", "createdAt": "2020-04-07T17:32:29Z", "author": {"login": "garawalid"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java", "diffHunk": "@@ -90,7 +90,7 @@\n  *\n  * # Maximum size (in bytes) allowed as padding to align row groups\n  * # This is also the minimum size of a row group. Default: 0\n- * parquet.writer.max-padding=2097152 # 2 MB\n+ * parquet.writer.max-padding=8388608 # 8 MB", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDYyNDEyMw=="}, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMTEyNTYyOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwOTozNjozM1rOGB7Rrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxOTozNTo1OVrOGCTEUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY3Mjk0Mw==", "bodyText": "One may use ParquetInputFormat and ParquetOutputFormat to set the related properties in the hadoop conf. It is required because the MR/Spark (etc.) jobs are transferring the related data via the Configuration object so the workers will know how to read/write the parquet files. So, I would not say they have a higher precedence, they are more utilities to help in the configuration.\nIn your example 1024 wins because it is set later than the 512.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r404672943", "createdAt": "2020-04-07T09:36:33Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA2MjczNw==", "bodyText": "You are right! I'll delete the note", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r405062737", "createdAt": "2020-04-07T19:35:59Z", "author": {"login": "garawalid"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY3Mjk0Mw=="}, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMTI0NTE0OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMDowODowN1rOGB8cvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzozNjoxNlrOGCZ5yQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY5MjE1OQ==", "bodyText": "I would format all the default values like this\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            **Default value:** 134217728 (128 MB)\n          \n          \n            \n            **Default value:** `134217728` (128 MB)", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r404692159", "createdAt": "2020-04-07T10:08:07Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  \n+**Default value:** 134217728 (128 MB)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE3NDcyOQ==", "bodyText": "I agree!", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r405174729", "createdAt": "2020-04-07T23:36:16Z", "author": {"login": "garawalid"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  \n+**Default value:** 134217728 (128 MB)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY5MjE1OQ=="}, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMTI1MDE1OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMDowOToyNVrOGB8f8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxOTozMDo1NVrOGCS5og==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY5Mjk3OQ==", "bodyText": "For me it is not clear what would the leading '_' mean in _metadata and _common_metadata. Was it formatting? (Then, it did not work :) )", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r404692979", "createdAt": "2020-04-07T10:09:25Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA2MDAwMg==", "bodyText": "The file name generated within the same directory as parquet files. I'll clarify it", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r405060002", "createdAt": "2020-04-07T19:30:55Z", "author": {"login": "garawalid"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY5Mjk3OQ=="}, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMTMwMjU0OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMDoyMzo0M1rOGB9Acg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQwOToyMDozNFrOGCmhTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcwMTI5OA==", "bodyText": "I know this is written in the comments but it is confusing. Block size is set to match the row-group size with the block size of the storage. For example let the row-group size the same as the block size on HDFS so it is easier to distribute the data among the workers (every worker gets a row-group which is exactly one block on HDFS).\nMeanwhile, it is not that easy as we need padding because we cannot write exact amount of data in a row group (we have to sync the columns). See the code here for more details.\n(You can see that we also tries to get the default block size from the storage layer itself and combine with this value.)\nIt relates to memory only because at writing you need at least this amount of memory because we keep the whole row-group in memory until it is written as a whole.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r404701298", "createdAt": "2020-04-07T10:23:43Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTExNzMxMg==", "bodyText": "Thanks @gszadovszky for the explanation. What about this description?\nThe block size in bytes. This property depends on the file system:\n\n\nIf the file system (FS) used supports blocks like HDFS, the block size will be the maximum between the default block size of FS and this property. And the row group size will be equals to this property.\nblock_size = max(default_fs_block_size, parquet.block.size)\nrow_group_size = parquet.block.size`\n\n\nIf the file system used doesn't support blocks, then this property will define the row group size.\n\n\nNote that larger values of row group size will improve the IO when reading but consume more memory when writing", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r405117312", "createdAt": "2020-04-07T21:14:46Z", "author": {"login": "garawalid"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcwMTI5OA=="}, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTM4MTQ1Mg==", "bodyText": "perfect :)", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r405381452", "createdAt": "2020-04-08T09:20:34Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcwMTI5OA=="}, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMTMxMTAzOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMDoyNTo1NlrOGB9Fig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMDoyNTo1NlrOGB9Fig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcwMjYwMg==", "bodyText": "We have more. See CompressionCodecName.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r404702602", "createdAt": "2020-04-07T10:25:56Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  \n+**Default value:** 134217728 (128 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size`  \n+**Description:** The page size in bytes is for compression. When reading, each page can be decompressed independently.  \n+A block is composed of pages. The page is the smallest unit that must be read fully to access a single record.  \n+If this value is too small, the compression will deteriorate.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.compression`  \n+**Description:** The compression algorithm used to compress pages. This property supersedes `mapred.output.compress*`.  \n+It can be `uncompressed`, `snappy`, `gzip` or `lzo`.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMTMxNDQ5OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMDoyNzowMlrOGB9H1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMDoyNzowMlrOGB9H1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcwMzE5MQ==", "bodyText": "Same as above.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r404703191", "createdAt": "2020-04-07T10:27:02Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  \n+**Default value:** 134217728 (128 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size`  \n+**Description:** The page size in bytes is for compression. When reading, each page can be decompressed independently.  \n+A block is composed of pages. The page is the smallest unit that must be read fully to access a single record.  \n+If this value is too small, the compression will deteriorate.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.compression`  \n+**Description:** The compression algorithm used to compress pages. This property supersedes `mapred.output.compress*`.  \n+It can be `uncompressed`, `snappy`, `gzip` or `lzo`.  \n+If `parquet.compression` is not set, the following properties are checked:  \n+ * mapred.output.compress=true\n+ * mapred.output.compression.codec=org.apache.hadoop.io.compress.SomeCodec\n+\n+Note that custom codecs are explicitly disallowed. Only one of Snappy, GZip or LZO is accepted.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMTQwNDIyOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMDo1Mjo1NVrOGB9-7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMTowMDowMVrOGDpORg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcxNzI5Mw==", "bodyText": "These are not about the size of the pages. We have a size limit for pages but checking the actual size after every value added is not good for performance. So, we check only after some values. The frequency of these checks can be modified by these properties. See the code for more details.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r404717293", "createdAt": "2020-04-07T10:52:55Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  \n+**Default value:** 134217728 (128 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size`  \n+**Description:** The page size in bytes is for compression. When reading, each page can be decompressed independently.  \n+A block is composed of pages. The page is the smallest unit that must be read fully to access a single record.  \n+If this value is too small, the compression will deteriorate.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.compression`  \n+**Description:** The compression algorithm used to compress pages. This property supersedes `mapred.output.compress*`.  \n+It can be `uncompressed`, `snappy`, `gzip` or `lzo`.  \n+If `parquet.compression` is not set, the following properties are checked:  \n+ * mapred.output.compress=true\n+ * mapred.output.compression.codec=org.apache.hadoop.io.compress.SomeCodec\n+\n+Note that custom codecs are explicitly disallowed. Only one of Snappy, GZip or LZO is accepted.  \n+**Default value:** uncompressed\n+\n+---\n+\n+**Property:** `parquet.write.support.class`  \n+**Description:** The write support class to convert the records written to the OutputFormat into the events accepted by the record consumer.  \n+Usually provided by a specific ParquetOutputFormat subclass.  \n+\n+---\n+\n+**Property:** `parquet.enable.dictionary`  \n+**Description:** Whether to enable or disable dictionary encoding. If it is true, then the dictionary encoding is enabled for all columns.  \n+If it is false, then the dictionary encoding is disabled for all columns.  \n+It is possible to enable or disable the encoding for some columns by specifying the column name in the property.  \n+Note that all configurations of this property will be combined (See the following example).  \n+**Default value:** true  \n+**Example**\n+```java\n+conf.set(\"parquet.enable.dictionary\", true); // Enable dictionary encoding for all columns\n+conf.set(\"parquet.enable.dictionary#column.path\", false); // Disable dictionary encoding for 'column.path'\n+// The final configuration will be: Enable dictionary encoding for all columns except 'column.path'\n+```\n+\n+---\n+\n+**Property:** `parquet.dictionary.page.size`  \n+**Description:** The dictionary page size works like the page size but for dictionary.  \n+There is one dictionary page per column per row group when dictionary encoding is used.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.validation`  \n+**Description:** Whether to turn on validation using the schema.  \n+**Default value:** false\n+\n+---\n+\n+**Property:** `parquet.writer.version`  \n+**Description:** The writer version. It can be either `PARQUET_1_0` or `PARQUET_2_0`.  \n+`PARQUET_1_0` and `PARQUET_2_0` refer to DataPageHeaderV1 and DataPageHeaderV2.  \n+The v1 pages store levels uncompressed while v1 pages compress levels with the data.  \n+For more details, see the the [thrift definition](https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift).  \n+**Default value:** PARQUET_1_0\n+\n+---\n+\n+**Property:** `parquet.memory.pool.ratio`  \n+**Description:** The memory manager balances the allocation size of each Parquet writer by resize them averagely.  \n+If the sum of each writer's allocation size is less than the total memory pool, the memory manager keeps their original value.  \n+If the sum exceeds, it decreases the allocation size of each writer by this ratio.  \n+This property should be between 0 and 1.  \n+**Default value:** 0.95\n+\n+---\n+\n+**Property:** `parquet.memory.min.chunk.size`  \n+**Description:** The minimum allocation size in byte per Parquet writer. If the allocation size is less than the minimum, the memory manager will fail with an exception.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.writer.max-padding`  \n+**Description:** The maximum size in bytes allowed as padding to align row groups. This is also the minimum size of a row group.  \n+**Default value:** 8388608 (8 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.min`  \n+**Description:** The minimum number of row per page.  \n+**Default value:** 100\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.max`  \n+**Description:** The maximum number of row per page.  \n+**Default value:** 10000", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ3NDMxMA==", "bodyText": "Thanks for the clarification, now it makes sense :)", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r406474310", "createdAt": "2020-04-09T21:00:01Z", "author": {"login": "garawalid"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  \n+**Default value:** 134217728 (128 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size`  \n+**Description:** The page size in bytes is for compression. When reading, each page can be decompressed independently.  \n+A block is composed of pages. The page is the smallest unit that must be read fully to access a single record.  \n+If this value is too small, the compression will deteriorate.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.compression`  \n+**Description:** The compression algorithm used to compress pages. This property supersedes `mapred.output.compress*`.  \n+It can be `uncompressed`, `snappy`, `gzip` or `lzo`.  \n+If `parquet.compression` is not set, the following properties are checked:  \n+ * mapred.output.compress=true\n+ * mapred.output.compression.codec=org.apache.hadoop.io.compress.SomeCodec\n+\n+Note that custom codecs are explicitly disallowed. Only one of Snappy, GZip or LZO is accepted.  \n+**Default value:** uncompressed\n+\n+---\n+\n+**Property:** `parquet.write.support.class`  \n+**Description:** The write support class to convert the records written to the OutputFormat into the events accepted by the record consumer.  \n+Usually provided by a specific ParquetOutputFormat subclass.  \n+\n+---\n+\n+**Property:** `parquet.enable.dictionary`  \n+**Description:** Whether to enable or disable dictionary encoding. If it is true, then the dictionary encoding is enabled for all columns.  \n+If it is false, then the dictionary encoding is disabled for all columns.  \n+It is possible to enable or disable the encoding for some columns by specifying the column name in the property.  \n+Note that all configurations of this property will be combined (See the following example).  \n+**Default value:** true  \n+**Example**\n+```java\n+conf.set(\"parquet.enable.dictionary\", true); // Enable dictionary encoding for all columns\n+conf.set(\"parquet.enable.dictionary#column.path\", false); // Disable dictionary encoding for 'column.path'\n+// The final configuration will be: Enable dictionary encoding for all columns except 'column.path'\n+```\n+\n+---\n+\n+**Property:** `parquet.dictionary.page.size`  \n+**Description:** The dictionary page size works like the page size but for dictionary.  \n+There is one dictionary page per column per row group when dictionary encoding is used.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.validation`  \n+**Description:** Whether to turn on validation using the schema.  \n+**Default value:** false\n+\n+---\n+\n+**Property:** `parquet.writer.version`  \n+**Description:** The writer version. It can be either `PARQUET_1_0` or `PARQUET_2_0`.  \n+`PARQUET_1_0` and `PARQUET_2_0` refer to DataPageHeaderV1 and DataPageHeaderV2.  \n+The v1 pages store levels uncompressed while v1 pages compress levels with the data.  \n+For more details, see the the [thrift definition](https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift).  \n+**Default value:** PARQUET_1_0\n+\n+---\n+\n+**Property:** `parquet.memory.pool.ratio`  \n+**Description:** The memory manager balances the allocation size of each Parquet writer by resize them averagely.  \n+If the sum of each writer's allocation size is less than the total memory pool, the memory manager keeps their original value.  \n+If the sum exceeds, it decreases the allocation size of each writer by this ratio.  \n+This property should be between 0 and 1.  \n+**Default value:** 0.95\n+\n+---\n+\n+**Property:** `parquet.memory.min.chunk.size`  \n+**Description:** The minimum allocation size in byte per Parquet writer. If the allocation size is less than the minimum, the memory manager will fail with an exception.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.writer.max-padding`  \n+**Description:** The maximum size in bytes allowed as padding to align row groups. This is also the minimum size of a row group.  \n+**Default value:** 8388608 (8 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.min`  \n+**Description:** The minimum number of row per page.  \n+**Default value:** 100\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.max`  \n+**Description:** The maximum number of row per page.  \n+**Default value:** 10000", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcxNzI5Mw=="}, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 159}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMTQzMTIxOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMTowMDo0MVrOGB-Pxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMTowMDo0MVrOGB-Pxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcyMTYwNw==", "bodyText": "It would be better to add the comment in a separate line so no horizontal scroll bar will take place and the whole example would be displayed on the same screen.\n(Similarly for the other examples)", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r404721607", "createdAt": "2020-04-07T11:00:41Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  \n+**Default value:** 134217728 (128 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size`  \n+**Description:** The page size in bytes is for compression. When reading, each page can be decompressed independently.  \n+A block is composed of pages. The page is the smallest unit that must be read fully to access a single record.  \n+If this value is too small, the compression will deteriorate.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.compression`  \n+**Description:** The compression algorithm used to compress pages. This property supersedes `mapred.output.compress*`.  \n+It can be `uncompressed`, `snappy`, `gzip` or `lzo`.  \n+If `parquet.compression` is not set, the following properties are checked:  \n+ * mapred.output.compress=true\n+ * mapred.output.compression.codec=org.apache.hadoop.io.compress.SomeCodec\n+\n+Note that custom codecs are explicitly disallowed. Only one of Snappy, GZip or LZO is accepted.  \n+**Default value:** uncompressed\n+\n+---\n+\n+**Property:** `parquet.write.support.class`  \n+**Description:** The write support class to convert the records written to the OutputFormat into the events accepted by the record consumer.  \n+Usually provided by a specific ParquetOutputFormat subclass.  \n+\n+---\n+\n+**Property:** `parquet.enable.dictionary`  \n+**Description:** Whether to enable or disable dictionary encoding. If it is true, then the dictionary encoding is enabled for all columns.  \n+If it is false, then the dictionary encoding is disabled for all columns.  \n+It is possible to enable or disable the encoding for some columns by specifying the column name in the property.  \n+Note that all configurations of this property will be combined (See the following example).  \n+**Default value:** true  \n+**Example**\n+```java\n+conf.set(\"parquet.enable.dictionary\", true); // Enable dictionary encoding for all columns\n+conf.set(\"parquet.enable.dictionary#column.path\", false); // Disable dictionary encoding for 'column.path'\n+// The final configuration will be: Enable dictionary encoding for all columns except 'column.path'\n+```\n+\n+---\n+\n+**Property:** `parquet.dictionary.page.size`  \n+**Description:** The dictionary page size works like the page size but for dictionary.  \n+There is one dictionary page per column per row group when dictionary encoding is used.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.validation`  \n+**Description:** Whether to turn on validation using the schema.  \n+**Default value:** false\n+\n+---\n+\n+**Property:** `parquet.writer.version`  \n+**Description:** The writer version. It can be either `PARQUET_1_0` or `PARQUET_2_0`.  \n+`PARQUET_1_0` and `PARQUET_2_0` refer to DataPageHeaderV1 and DataPageHeaderV2.  \n+The v1 pages store levels uncompressed while v1 pages compress levels with the data.  \n+For more details, see the the [thrift definition](https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift).  \n+**Default value:** PARQUET_1_0\n+\n+---\n+\n+**Property:** `parquet.memory.pool.ratio`  \n+**Description:** The memory manager balances the allocation size of each Parquet writer by resize them averagely.  \n+If the sum of each writer's allocation size is less than the total memory pool, the memory manager keeps their original value.  \n+If the sum exceeds, it decreases the allocation size of each writer by this ratio.  \n+This property should be between 0 and 1.  \n+**Default value:** 0.95\n+\n+---\n+\n+**Property:** `parquet.memory.min.chunk.size`  \n+**Description:** The minimum allocation size in byte per Parquet writer. If the allocation size is less than the minimum, the memory manager will fail with an exception.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.writer.max-padding`  \n+**Description:** The maximum size in bytes allowed as padding to align row groups. This is also the minimum size of a row group.  \n+**Default value:** 8388608 (8 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.min`  \n+**Description:** The minimum number of row per page.  \n+**Default value:** 100\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.max`  \n+**Description:** The maximum number of row per page.  \n+**Default value:** 10000\n+\n+---\n+\n+**Property:** `parquet.page.size.check.estimate`  \n+**Description:** If it is true, the column writer estimates the size of the next page.  \n+It prevents issues with rows that vary significantly in size.  \n+**Default value:** true\n+\n+---\n+\n+**Property:** `parquet.columnindex.truncate.length`  \n+**Description:** The [column index](https://github.com/apache/parquet-format/blob/master/PageIndex.md) containing min/max and null count values for the pages in a column chunk.  \n+This property is the length to be used for truncating binary values if possible in a binary column index.  \n+**Default value:** 64\n+\n+---\n+\n+**Property:** `parquet.statistics.truncate.length`  \n+**Description:** The length which the min/max binary values in row groups tried to be truncated to.  \n+**Default value:** 2147483647\n+\n+---\n+\n+**Property:** `parquet.bloom.filter.enabled`  \n+**Description:** Whether to enable writing bloom filter.  \n+If it is true, the bloom filter will be enable for all columns. If it is false, it will be disabled for all columns.  \n+It is also possible to enable it for some columns by specifying the column name within the property followed by #.  \n+**Default value:** false  \n+**Example:**  \n+```java\n+conf.set(\"parquet.bloom.filter.enabled\", true); // Enable the bloom filter for all columns\n+conf.set(\"parquet.bloom.filter.enabled#column.path\", false); // Disable the bloom filter for the column 'column.path'\n+// The bloom filter will be enabled for all columns except 'column.path'\n+```\n+\n+---\n+\n+**Property:** `parquet.bloom.filter.expected.ndv`  \n+**Description:** The expected number of distinct values in a column, it is used to compute the optimal size of the bloom filter.  \n+Note that if this property is not enabled, the bloom filter will use the maximum size.  \n+If this is property is set without a the column name, all columns will have a boom filter with the same expected number of distinct values.  \n+If this property is set for a column, then no need to enable the bloom filter with `parquet.bloom.filter.enabled` property.  \n+**Example:**\n+```java\n+conf.set(\"parquet.bloom.filter.expected.ndv#column.path\", 200) // The bloom filter will be enabled for 'column.path' with expected number of distinct values equals to 200\n+```\n+```java\n+conf.set(\"parquet.bloom.filter.expected.ndv\", 200) // Enable bloom filter for all columns with expected number of distinct values equals to 200", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 207}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMTQzMzU4OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMTowMToyOVrOGB-RVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMTowMToyOVrOGB-RVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcyMjAwNQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Note that if this property is not enabled, the bloom filter will use the maximum size.  \n          \n          \n            \n            Note that if this property is not set, the bloom filter will use the maximum size.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r404722005", "createdAt": "2020-04-07T11:01:29Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  \n+**Default value:** 134217728 (128 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size`  \n+**Description:** The page size in bytes is for compression. When reading, each page can be decompressed independently.  \n+A block is composed of pages. The page is the smallest unit that must be read fully to access a single record.  \n+If this value is too small, the compression will deteriorate.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.compression`  \n+**Description:** The compression algorithm used to compress pages. This property supersedes `mapred.output.compress*`.  \n+It can be `uncompressed`, `snappy`, `gzip` or `lzo`.  \n+If `parquet.compression` is not set, the following properties are checked:  \n+ * mapred.output.compress=true\n+ * mapred.output.compression.codec=org.apache.hadoop.io.compress.SomeCodec\n+\n+Note that custom codecs are explicitly disallowed. Only one of Snappy, GZip or LZO is accepted.  \n+**Default value:** uncompressed\n+\n+---\n+\n+**Property:** `parquet.write.support.class`  \n+**Description:** The write support class to convert the records written to the OutputFormat into the events accepted by the record consumer.  \n+Usually provided by a specific ParquetOutputFormat subclass.  \n+\n+---\n+\n+**Property:** `parquet.enable.dictionary`  \n+**Description:** Whether to enable or disable dictionary encoding. If it is true, then the dictionary encoding is enabled for all columns.  \n+If it is false, then the dictionary encoding is disabled for all columns.  \n+It is possible to enable or disable the encoding for some columns by specifying the column name in the property.  \n+Note that all configurations of this property will be combined (See the following example).  \n+**Default value:** true  \n+**Example**\n+```java\n+conf.set(\"parquet.enable.dictionary\", true); // Enable dictionary encoding for all columns\n+conf.set(\"parquet.enable.dictionary#column.path\", false); // Disable dictionary encoding for 'column.path'\n+// The final configuration will be: Enable dictionary encoding for all columns except 'column.path'\n+```\n+\n+---\n+\n+**Property:** `parquet.dictionary.page.size`  \n+**Description:** The dictionary page size works like the page size but for dictionary.  \n+There is one dictionary page per column per row group when dictionary encoding is used.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.validation`  \n+**Description:** Whether to turn on validation using the schema.  \n+**Default value:** false\n+\n+---\n+\n+**Property:** `parquet.writer.version`  \n+**Description:** The writer version. It can be either `PARQUET_1_0` or `PARQUET_2_0`.  \n+`PARQUET_1_0` and `PARQUET_2_0` refer to DataPageHeaderV1 and DataPageHeaderV2.  \n+The v1 pages store levels uncompressed while v1 pages compress levels with the data.  \n+For more details, see the the [thrift definition](https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift).  \n+**Default value:** PARQUET_1_0\n+\n+---\n+\n+**Property:** `parquet.memory.pool.ratio`  \n+**Description:** The memory manager balances the allocation size of each Parquet writer by resize them averagely.  \n+If the sum of each writer's allocation size is less than the total memory pool, the memory manager keeps their original value.  \n+If the sum exceeds, it decreases the allocation size of each writer by this ratio.  \n+This property should be between 0 and 1.  \n+**Default value:** 0.95\n+\n+---\n+\n+**Property:** `parquet.memory.min.chunk.size`  \n+**Description:** The minimum allocation size in byte per Parquet writer. If the allocation size is less than the minimum, the memory manager will fail with an exception.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.writer.max-padding`  \n+**Description:** The maximum size in bytes allowed as padding to align row groups. This is also the minimum size of a row group.  \n+**Default value:** 8388608 (8 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.min`  \n+**Description:** The minimum number of row per page.  \n+**Default value:** 100\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.max`  \n+**Description:** The maximum number of row per page.  \n+**Default value:** 10000\n+\n+---\n+\n+**Property:** `parquet.page.size.check.estimate`  \n+**Description:** If it is true, the column writer estimates the size of the next page.  \n+It prevents issues with rows that vary significantly in size.  \n+**Default value:** true\n+\n+---\n+\n+**Property:** `parquet.columnindex.truncate.length`  \n+**Description:** The [column index](https://github.com/apache/parquet-format/blob/master/PageIndex.md) containing min/max and null count values for the pages in a column chunk.  \n+This property is the length to be used for truncating binary values if possible in a binary column index.  \n+**Default value:** 64\n+\n+---\n+\n+**Property:** `parquet.statistics.truncate.length`  \n+**Description:** The length which the min/max binary values in row groups tried to be truncated to.  \n+**Default value:** 2147483647\n+\n+---\n+\n+**Property:** `parquet.bloom.filter.enabled`  \n+**Description:** Whether to enable writing bloom filter.  \n+If it is true, the bloom filter will be enable for all columns. If it is false, it will be disabled for all columns.  \n+It is also possible to enable it for some columns by specifying the column name within the property followed by #.  \n+**Default value:** false  \n+**Example:**  \n+```java\n+conf.set(\"parquet.bloom.filter.enabled\", true); // Enable the bloom filter for all columns\n+conf.set(\"parquet.bloom.filter.enabled#column.path\", false); // Disable the bloom filter for the column 'column.path'\n+// The bloom filter will be enabled for all columns except 'column.path'\n+```\n+\n+---\n+\n+**Property:** `parquet.bloom.filter.expected.ndv`  \n+**Description:** The expected number of distinct values in a column, it is used to compute the optimal size of the bloom filter.  \n+Note that if this property is not enabled, the bloom filter will use the maximum size.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 199}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMTQ1NjYxOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMTowODo0NFrOGB-fzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMzowNDo1OVrOGCZRdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcyNTcxMQ==", "bodyText": "This is not true. NDV is usually specific for a column, it would not make sense to allow a common value for all. So, we expect the user to set the values column by column and there is no option to set them all at once.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r404725711", "createdAt": "2020-04-07T11:08:44Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  \n+**Default value:** 134217728 (128 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size`  \n+**Description:** The page size in bytes is for compression. When reading, each page can be decompressed independently.  \n+A block is composed of pages. The page is the smallest unit that must be read fully to access a single record.  \n+If this value is too small, the compression will deteriorate.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.compression`  \n+**Description:** The compression algorithm used to compress pages. This property supersedes `mapred.output.compress*`.  \n+It can be `uncompressed`, `snappy`, `gzip` or `lzo`.  \n+If `parquet.compression` is not set, the following properties are checked:  \n+ * mapred.output.compress=true\n+ * mapred.output.compression.codec=org.apache.hadoop.io.compress.SomeCodec\n+\n+Note that custom codecs are explicitly disallowed. Only one of Snappy, GZip or LZO is accepted.  \n+**Default value:** uncompressed\n+\n+---\n+\n+**Property:** `parquet.write.support.class`  \n+**Description:** The write support class to convert the records written to the OutputFormat into the events accepted by the record consumer.  \n+Usually provided by a specific ParquetOutputFormat subclass.  \n+\n+---\n+\n+**Property:** `parquet.enable.dictionary`  \n+**Description:** Whether to enable or disable dictionary encoding. If it is true, then the dictionary encoding is enabled for all columns.  \n+If it is false, then the dictionary encoding is disabled for all columns.  \n+It is possible to enable or disable the encoding for some columns by specifying the column name in the property.  \n+Note that all configurations of this property will be combined (See the following example).  \n+**Default value:** true  \n+**Example**\n+```java\n+conf.set(\"parquet.enable.dictionary\", true); // Enable dictionary encoding for all columns\n+conf.set(\"parquet.enable.dictionary#column.path\", false); // Disable dictionary encoding for 'column.path'\n+// The final configuration will be: Enable dictionary encoding for all columns except 'column.path'\n+```\n+\n+---\n+\n+**Property:** `parquet.dictionary.page.size`  \n+**Description:** The dictionary page size works like the page size but for dictionary.  \n+There is one dictionary page per column per row group when dictionary encoding is used.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.validation`  \n+**Description:** Whether to turn on validation using the schema.  \n+**Default value:** false\n+\n+---\n+\n+**Property:** `parquet.writer.version`  \n+**Description:** The writer version. It can be either `PARQUET_1_0` or `PARQUET_2_0`.  \n+`PARQUET_1_0` and `PARQUET_2_0` refer to DataPageHeaderV1 and DataPageHeaderV2.  \n+The v1 pages store levels uncompressed while v1 pages compress levels with the data.  \n+For more details, see the the [thrift definition](https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift).  \n+**Default value:** PARQUET_1_0\n+\n+---\n+\n+**Property:** `parquet.memory.pool.ratio`  \n+**Description:** The memory manager balances the allocation size of each Parquet writer by resize them averagely.  \n+If the sum of each writer's allocation size is less than the total memory pool, the memory manager keeps their original value.  \n+If the sum exceeds, it decreases the allocation size of each writer by this ratio.  \n+This property should be between 0 and 1.  \n+**Default value:** 0.95\n+\n+---\n+\n+**Property:** `parquet.memory.min.chunk.size`  \n+**Description:** The minimum allocation size in byte per Parquet writer. If the allocation size is less than the minimum, the memory manager will fail with an exception.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.writer.max-padding`  \n+**Description:** The maximum size in bytes allowed as padding to align row groups. This is also the minimum size of a row group.  \n+**Default value:** 8388608 (8 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.min`  \n+**Description:** The minimum number of row per page.  \n+**Default value:** 100\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.max`  \n+**Description:** The maximum number of row per page.  \n+**Default value:** 10000\n+\n+---\n+\n+**Property:** `parquet.page.size.check.estimate`  \n+**Description:** If it is true, the column writer estimates the size of the next page.  \n+It prevents issues with rows that vary significantly in size.  \n+**Default value:** true\n+\n+---\n+\n+**Property:** `parquet.columnindex.truncate.length`  \n+**Description:** The [column index](https://github.com/apache/parquet-format/blob/master/PageIndex.md) containing min/max and null count values for the pages in a column chunk.  \n+This property is the length to be used for truncating binary values if possible in a binary column index.  \n+**Default value:** 64\n+\n+---\n+\n+**Property:** `parquet.statistics.truncate.length`  \n+**Description:** The length which the min/max binary values in row groups tried to be truncated to.  \n+**Default value:** 2147483647\n+\n+---\n+\n+**Property:** `parquet.bloom.filter.enabled`  \n+**Description:** Whether to enable writing bloom filter.  \n+If it is true, the bloom filter will be enable for all columns. If it is false, it will be disabled for all columns.  \n+It is also possible to enable it for some columns by specifying the column name within the property followed by #.  \n+**Default value:** false  \n+**Example:**  \n+```java\n+conf.set(\"parquet.bloom.filter.enabled\", true); // Enable the bloom filter for all columns\n+conf.set(\"parquet.bloom.filter.enabled#column.path\", false); // Disable the bloom filter for the column 'column.path'\n+// The bloom filter will be enabled for all columns except 'column.path'\n+```\n+\n+---\n+\n+**Property:** `parquet.bloom.filter.expected.ndv`  \n+**Description:** The expected number of distinct values in a column, it is used to compute the optimal size of the bloom filter.  \n+Note that if this property is not enabled, the bloom filter will use the maximum size.  \n+If this is property is set without a the column name, all columns will have a boom filter with the same expected number of distinct values.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 200}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE2NDQwNA==", "bodyText": "Ok! I'll update it", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r405164404", "createdAt": "2020-04-07T23:04:59Z", "author": {"login": "garawalid"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  \n+**Default value:** 134217728 (128 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size`  \n+**Description:** The page size in bytes is for compression. When reading, each page can be decompressed independently.  \n+A block is composed of pages. The page is the smallest unit that must be read fully to access a single record.  \n+If this value is too small, the compression will deteriorate.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.compression`  \n+**Description:** The compression algorithm used to compress pages. This property supersedes `mapred.output.compress*`.  \n+It can be `uncompressed`, `snappy`, `gzip` or `lzo`.  \n+If `parquet.compression` is not set, the following properties are checked:  \n+ * mapred.output.compress=true\n+ * mapred.output.compression.codec=org.apache.hadoop.io.compress.SomeCodec\n+\n+Note that custom codecs are explicitly disallowed. Only one of Snappy, GZip or LZO is accepted.  \n+**Default value:** uncompressed\n+\n+---\n+\n+**Property:** `parquet.write.support.class`  \n+**Description:** The write support class to convert the records written to the OutputFormat into the events accepted by the record consumer.  \n+Usually provided by a specific ParquetOutputFormat subclass.  \n+\n+---\n+\n+**Property:** `parquet.enable.dictionary`  \n+**Description:** Whether to enable or disable dictionary encoding. If it is true, then the dictionary encoding is enabled for all columns.  \n+If it is false, then the dictionary encoding is disabled for all columns.  \n+It is possible to enable or disable the encoding for some columns by specifying the column name in the property.  \n+Note that all configurations of this property will be combined (See the following example).  \n+**Default value:** true  \n+**Example**\n+```java\n+conf.set(\"parquet.enable.dictionary\", true); // Enable dictionary encoding for all columns\n+conf.set(\"parquet.enable.dictionary#column.path\", false); // Disable dictionary encoding for 'column.path'\n+// The final configuration will be: Enable dictionary encoding for all columns except 'column.path'\n+```\n+\n+---\n+\n+**Property:** `parquet.dictionary.page.size`  \n+**Description:** The dictionary page size works like the page size but for dictionary.  \n+There is one dictionary page per column per row group when dictionary encoding is used.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.validation`  \n+**Description:** Whether to turn on validation using the schema.  \n+**Default value:** false\n+\n+---\n+\n+**Property:** `parquet.writer.version`  \n+**Description:** The writer version. It can be either `PARQUET_1_0` or `PARQUET_2_0`.  \n+`PARQUET_1_0` and `PARQUET_2_0` refer to DataPageHeaderV1 and DataPageHeaderV2.  \n+The v1 pages store levels uncompressed while v1 pages compress levels with the data.  \n+For more details, see the the [thrift definition](https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift).  \n+**Default value:** PARQUET_1_0\n+\n+---\n+\n+**Property:** `parquet.memory.pool.ratio`  \n+**Description:** The memory manager balances the allocation size of each Parquet writer by resize them averagely.  \n+If the sum of each writer's allocation size is less than the total memory pool, the memory manager keeps their original value.  \n+If the sum exceeds, it decreases the allocation size of each writer by this ratio.  \n+This property should be between 0 and 1.  \n+**Default value:** 0.95\n+\n+---\n+\n+**Property:** `parquet.memory.min.chunk.size`  \n+**Description:** The minimum allocation size in byte per Parquet writer. If the allocation size is less than the minimum, the memory manager will fail with an exception.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.writer.max-padding`  \n+**Description:** The maximum size in bytes allowed as padding to align row groups. This is also the minimum size of a row group.  \n+**Default value:** 8388608 (8 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.min`  \n+**Description:** The minimum number of row per page.  \n+**Default value:** 100\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.max`  \n+**Description:** The maximum number of row per page.  \n+**Default value:** 10000\n+\n+---\n+\n+**Property:** `parquet.page.size.check.estimate`  \n+**Description:** If it is true, the column writer estimates the size of the next page.  \n+It prevents issues with rows that vary significantly in size.  \n+**Default value:** true\n+\n+---\n+\n+**Property:** `parquet.columnindex.truncate.length`  \n+**Description:** The [column index](https://github.com/apache/parquet-format/blob/master/PageIndex.md) containing min/max and null count values for the pages in a column chunk.  \n+This property is the length to be used for truncating binary values if possible in a binary column index.  \n+**Default value:** 64\n+\n+---\n+\n+**Property:** `parquet.statistics.truncate.length`  \n+**Description:** The length which the min/max binary values in row groups tried to be truncated to.  \n+**Default value:** 2147483647\n+\n+---\n+\n+**Property:** `parquet.bloom.filter.enabled`  \n+**Description:** Whether to enable writing bloom filter.  \n+If it is true, the bloom filter will be enable for all columns. If it is false, it will be disabled for all columns.  \n+It is also possible to enable it for some columns by specifying the column name within the property followed by #.  \n+**Default value:** false  \n+**Example:**  \n+```java\n+conf.set(\"parquet.bloom.filter.enabled\", true); // Enable the bloom filter for all columns\n+conf.set(\"parquet.bloom.filter.enabled#column.path\", false); // Disable the bloom filter for the column 'column.path'\n+// The bloom filter will be enabled for all columns except 'column.path'\n+```\n+\n+---\n+\n+**Property:** `parquet.bloom.filter.expected.ndv`  \n+**Description:** The expected number of distinct values in a column, it is used to compute the optimal size of the bloom filter.  \n+Note that if this property is not enabled, the bloom filter will use the maximum size.  \n+If this is property is set without a the column name, all columns will have a boom filter with the same expected number of distinct values.  ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcyNTcxMQ=="}, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 200}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMTQ2MDQzOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMTowOTo0M1rOGB-iDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMTowOTo0M1rOGB-iDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcyNjI4Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            **Description:** The maximum number of row per page.  \n          \n          \n            \n            **Description:** The maximum number of rows per page.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r404726287", "createdAt": "2020-04-07T11:09:43Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  \n+**Default value:** 134217728 (128 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size`  \n+**Description:** The page size in bytes is for compression. When reading, each page can be decompressed independently.  \n+A block is composed of pages. The page is the smallest unit that must be read fully to access a single record.  \n+If this value is too small, the compression will deteriorate.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.compression`  \n+**Description:** The compression algorithm used to compress pages. This property supersedes `mapred.output.compress*`.  \n+It can be `uncompressed`, `snappy`, `gzip` or `lzo`.  \n+If `parquet.compression` is not set, the following properties are checked:  \n+ * mapred.output.compress=true\n+ * mapred.output.compression.codec=org.apache.hadoop.io.compress.SomeCodec\n+\n+Note that custom codecs are explicitly disallowed. Only one of Snappy, GZip or LZO is accepted.  \n+**Default value:** uncompressed\n+\n+---\n+\n+**Property:** `parquet.write.support.class`  \n+**Description:** The write support class to convert the records written to the OutputFormat into the events accepted by the record consumer.  \n+Usually provided by a specific ParquetOutputFormat subclass.  \n+\n+---\n+\n+**Property:** `parquet.enable.dictionary`  \n+**Description:** Whether to enable or disable dictionary encoding. If it is true, then the dictionary encoding is enabled for all columns.  \n+If it is false, then the dictionary encoding is disabled for all columns.  \n+It is possible to enable or disable the encoding for some columns by specifying the column name in the property.  \n+Note that all configurations of this property will be combined (See the following example).  \n+**Default value:** true  \n+**Example**\n+```java\n+conf.set(\"parquet.enable.dictionary\", true); // Enable dictionary encoding for all columns\n+conf.set(\"parquet.enable.dictionary#column.path\", false); // Disable dictionary encoding for 'column.path'\n+// The final configuration will be: Enable dictionary encoding for all columns except 'column.path'\n+```\n+\n+---\n+\n+**Property:** `parquet.dictionary.page.size`  \n+**Description:** The dictionary page size works like the page size but for dictionary.  \n+There is one dictionary page per column per row group when dictionary encoding is used.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.validation`  \n+**Description:** Whether to turn on validation using the schema.  \n+**Default value:** false\n+\n+---\n+\n+**Property:** `parquet.writer.version`  \n+**Description:** The writer version. It can be either `PARQUET_1_0` or `PARQUET_2_0`.  \n+`PARQUET_1_0` and `PARQUET_2_0` refer to DataPageHeaderV1 and DataPageHeaderV2.  \n+The v1 pages store levels uncompressed while v1 pages compress levels with the data.  \n+For more details, see the the [thrift definition](https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift).  \n+**Default value:** PARQUET_1_0\n+\n+---\n+\n+**Property:** `parquet.memory.pool.ratio`  \n+**Description:** The memory manager balances the allocation size of each Parquet writer by resize them averagely.  \n+If the sum of each writer's allocation size is less than the total memory pool, the memory manager keeps their original value.  \n+If the sum exceeds, it decreases the allocation size of each writer by this ratio.  \n+This property should be between 0 and 1.  \n+**Default value:** 0.95\n+\n+---\n+\n+**Property:** `parquet.memory.min.chunk.size`  \n+**Description:** The minimum allocation size in byte per Parquet writer. If the allocation size is less than the minimum, the memory manager will fail with an exception.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.writer.max-padding`  \n+**Description:** The maximum size in bytes allowed as padding to align row groups. This is also the minimum size of a row group.  \n+**Default value:** 8388608 (8 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.min`  \n+**Description:** The minimum number of row per page.  \n+**Default value:** 100\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.max`  \n+**Description:** The maximum number of row per page.  \n+**Default value:** 10000\n+\n+---\n+\n+**Property:** `parquet.page.size.check.estimate`  \n+**Description:** If it is true, the column writer estimates the size of the next page.  \n+It prevents issues with rows that vary significantly in size.  \n+**Default value:** true\n+\n+---\n+\n+**Property:** `parquet.columnindex.truncate.length`  \n+**Description:** The [column index](https://github.com/apache/parquet-format/blob/master/PageIndex.md) containing min/max and null count values for the pages in a column chunk.  \n+This property is the length to be used for truncating binary values if possible in a binary column index.  \n+**Default value:** 64\n+\n+---\n+\n+**Property:** `parquet.statistics.truncate.length`  \n+**Description:** The length which the min/max binary values in row groups tried to be truncated to.  \n+**Default value:** 2147483647\n+\n+---\n+\n+**Property:** `parquet.bloom.filter.enabled`  \n+**Description:** Whether to enable writing bloom filter.  \n+If it is true, the bloom filter will be enable for all columns. If it is false, it will be disabled for all columns.  \n+It is also possible to enable it for some columns by specifying the column name within the property followed by #.  \n+**Default value:** false  \n+**Example:**  \n+```java\n+conf.set(\"parquet.bloom.filter.enabled\", true); // Enable the bloom filter for all columns\n+conf.set(\"parquet.bloom.filter.enabled#column.path\", false); // Disable the bloom filter for the column 'column.path'\n+// The bloom filter will be enabled for all columns except 'column.path'\n+```\n+\n+---\n+\n+**Property:** `parquet.bloom.filter.expected.ndv`  \n+**Description:** The expected number of distinct values in a column, it is used to compute the optimal size of the bloom filter.  \n+Note that if this property is not enabled, the bloom filter will use the maximum size.  \n+If this is property is set without a the column name, all columns will have a boom filter with the same expected number of distinct values.  \n+If this property is set for a column, then no need to enable the bloom filter with `parquet.bloom.filter.enabled` property.  \n+**Example:**\n+```java\n+conf.set(\"parquet.bloom.filter.expected.ndv#column.path\", 200) // The bloom filter will be enabled for 'column.path' with expected number of distinct values equals to 200\n+```\n+```java\n+conf.set(\"parquet.bloom.filter.expected.ndv\", 200) // Enable bloom filter for all columns with expected number of distinct values equals to 200\n+```\n+\n+---\n+\n+**Property:** `parquet.bloom.filter.max.bytes`  \n+**Description:** The maximum number of bytes for a bloom filter bitset.  \n+**Default value:** 1024*1024 (1MB)\n+\n+---\n+\n+**Property:** `parquet.page.row.count.limit`  \n+**Description:** The maximum number of row per page.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 219}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMTQ4MzE4OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMToxNjoyMFrOGB-v3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMToxNjoyMFrOGB-v3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcyOTgyMg==", "bodyText": "Might worth mentioning that it shall be the descendant class of org.apache.parquet.hadoop.api.WriteSupport.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r404729822", "createdAt": "2020-04-07T11:16:20Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  \n+**Default value:** 134217728 (128 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size`  \n+**Description:** The page size in bytes is for compression. When reading, each page can be decompressed independently.  \n+A block is composed of pages. The page is the smallest unit that must be read fully to access a single record.  \n+If this value is too small, the compression will deteriorate.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.compression`  \n+**Description:** The compression algorithm used to compress pages. This property supersedes `mapred.output.compress*`.  \n+It can be `uncompressed`, `snappy`, `gzip` or `lzo`.  \n+If `parquet.compression` is not set, the following properties are checked:  \n+ * mapred.output.compress=true\n+ * mapred.output.compression.codec=org.apache.hadoop.io.compress.SomeCodec\n+\n+Note that custom codecs are explicitly disallowed. Only one of Snappy, GZip or LZO is accepted.  \n+**Default value:** uncompressed\n+\n+---\n+\n+**Property:** `parquet.write.support.class`  \n+**Description:** The write support class to convert the records written to the OutputFormat into the events accepted by the record consumer.  \n+Usually provided by a specific ParquetOutputFormat subclass.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMTQ4OTY4OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMToxODowMVrOGB-zmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMToxODowMVrOGB-zmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDczMDc3Nw==", "bodyText": "Might worth mentioning that it shall be the descendant class of org.apache.parquet.hadoop.api.ReadSupport.\nAlso add something similar as at the read support but in the other way around.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r404730777", "createdAt": "2020-04-07T11:18:01Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -0,0 +1,310 @@\n+<!--\n+  ~ Licensed to the Apache Software Foundation (ASF) under one\n+  ~ or more contributor license agreements.  See the NOTICE file\n+  ~ distributed with this work for additional information\n+  ~ regarding copyright ownership.  The ASF licenses this file\n+  ~ to you under the Apache License, Version 2.0 (the\n+  ~ \"License\"); you may not use this file except in compliance\n+  ~ with the License.  You may obtain a copy of the License at\n+  ~\n+  ~   http://www.apache.org/licenses/LICENSE-2.0\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing,\n+  ~ software distributed under the License is distributed on an\n+  ~ \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+  ~ KIND, either express or implied.  See the License for the\n+  ~ specific language governing permissions and limitations\n+  ~ under the License.\n+  -->\n+  \n+# Hadoop integration\n+\n+**Todo:** Add a description\n+\n+## Properties\n+\n+It's possible to configure the [ParquetInputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java) / [ParquetOutputFormat](https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java) with Hadoop config or programmatically with setters.\n+\n+**Note:** Properties passed through InputOutputParquetFormat have a high precedence than properties passed via Hadoop Configuration.\n+\n+**Example:**\n+```java\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.Job;\n+\n+Configuration conf = new Configuration();\n+conf.set(\"parquet.block.size\",\"512\");\n+\n+Job writeJob = new Job(conf);\n+ParquetOutputFormat.setBlockSize(writeJob, 1024);\n+\n+```\n+\n+## Class: ParquetOutputFormat\n+\n+**Property:** `parquet.summary.metadata.level`  \n+**Description:** Write summary files. If this property is set to `all`, write both summary file with row group info (_metadata) and summary file without (_common_metadata).  \n+If it is `common_only`, write only the summary file without the row group info.  \n+If it is `none`, don't write summary files.  \n+**Default value:** all\n+\n+---\n+\n+**Property:** `parquet.enable.summary-metadata`  \n+**Description:** This property is deprecated, use `parquet.summary.metadata.level` instead.  \n+If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If it is `false`, it is similar to `NONE`.  \n+**Default value:** true  \n+\n+---\n+\n+**Property:** `parquet.block.size`  \n+**Description:** The block size in bytes is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n+Larger values will improve the IO when reading but consume more memory when writing.  \n+**Default value:** 134217728 (128 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size`  \n+**Description:** The page size in bytes is for compression. When reading, each page can be decompressed independently.  \n+A block is composed of pages. The page is the smallest unit that must be read fully to access a single record.  \n+If this value is too small, the compression will deteriorate.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.compression`  \n+**Description:** The compression algorithm used to compress pages. This property supersedes `mapred.output.compress*`.  \n+It can be `uncompressed`, `snappy`, `gzip` or `lzo`.  \n+If `parquet.compression` is not set, the following properties are checked:  \n+ * mapred.output.compress=true\n+ * mapred.output.compression.codec=org.apache.hadoop.io.compress.SomeCodec\n+\n+Note that custom codecs are explicitly disallowed. Only one of Snappy, GZip or LZO is accepted.  \n+**Default value:** uncompressed\n+\n+---\n+\n+**Property:** `parquet.write.support.class`  \n+**Description:** The write support class to convert the records written to the OutputFormat into the events accepted by the record consumer.  \n+Usually provided by a specific ParquetOutputFormat subclass.  \n+\n+---\n+\n+**Property:** `parquet.enable.dictionary`  \n+**Description:** Whether to enable or disable dictionary encoding. If it is true, then the dictionary encoding is enabled for all columns.  \n+If it is false, then the dictionary encoding is disabled for all columns.  \n+It is possible to enable or disable the encoding for some columns by specifying the column name in the property.  \n+Note that all configurations of this property will be combined (See the following example).  \n+**Default value:** true  \n+**Example**\n+```java\n+conf.set(\"parquet.enable.dictionary\", true); // Enable dictionary encoding for all columns\n+conf.set(\"parquet.enable.dictionary#column.path\", false); // Disable dictionary encoding for 'column.path'\n+// The final configuration will be: Enable dictionary encoding for all columns except 'column.path'\n+```\n+\n+---\n+\n+**Property:** `parquet.dictionary.page.size`  \n+**Description:** The dictionary page size works like the page size but for dictionary.  \n+There is one dictionary page per column per row group when dictionary encoding is used.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.validation`  \n+**Description:** Whether to turn on validation using the schema.  \n+**Default value:** false\n+\n+---\n+\n+**Property:** `parquet.writer.version`  \n+**Description:** The writer version. It can be either `PARQUET_1_0` or `PARQUET_2_0`.  \n+`PARQUET_1_0` and `PARQUET_2_0` refer to DataPageHeaderV1 and DataPageHeaderV2.  \n+The v1 pages store levels uncompressed while v1 pages compress levels with the data.  \n+For more details, see the the [thrift definition](https://github.com/apache/parquet-format/blob/master/src/main/thrift/parquet.thrift).  \n+**Default value:** PARQUET_1_0\n+\n+---\n+\n+**Property:** `parquet.memory.pool.ratio`  \n+**Description:** The memory manager balances the allocation size of each Parquet writer by resize them averagely.  \n+If the sum of each writer's allocation size is less than the total memory pool, the memory manager keeps their original value.  \n+If the sum exceeds, it decreases the allocation size of each writer by this ratio.  \n+This property should be between 0 and 1.  \n+**Default value:** 0.95\n+\n+---\n+\n+**Property:** `parquet.memory.min.chunk.size`  \n+**Description:** The minimum allocation size in byte per Parquet writer. If the allocation size is less than the minimum, the memory manager will fail with an exception.  \n+**Default value:** 1048576 (1 MB)\n+\n+---\n+\n+**Property:** `parquet.writer.max-padding`  \n+**Description:** The maximum size in bytes allowed as padding to align row groups. This is also the minimum size of a row group.  \n+**Default value:** 8388608 (8 MB)\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.min`  \n+**Description:** The minimum number of row per page.  \n+**Default value:** 100\n+\n+---\n+\n+**Property:** `parquet.page.size.row.check.max`  \n+**Description:** The maximum number of row per page.  \n+**Default value:** 10000\n+\n+---\n+\n+**Property:** `parquet.page.size.check.estimate`  \n+**Description:** If it is true, the column writer estimates the size of the next page.  \n+It prevents issues with rows that vary significantly in size.  \n+**Default value:** true\n+\n+---\n+\n+**Property:** `parquet.columnindex.truncate.length`  \n+**Description:** The [column index](https://github.com/apache/parquet-format/blob/master/PageIndex.md) containing min/max and null count values for the pages in a column chunk.  \n+This property is the length to be used for truncating binary values if possible in a binary column index.  \n+**Default value:** 64\n+\n+---\n+\n+**Property:** `parquet.statistics.truncate.length`  \n+**Description:** The length which the min/max binary values in row groups tried to be truncated to.  \n+**Default value:** 2147483647\n+\n+---\n+\n+**Property:** `parquet.bloom.filter.enabled`  \n+**Description:** Whether to enable writing bloom filter.  \n+If it is true, the bloom filter will be enable for all columns. If it is false, it will be disabled for all columns.  \n+It is also possible to enable it for some columns by specifying the column name within the property followed by #.  \n+**Default value:** false  \n+**Example:**  \n+```java\n+conf.set(\"parquet.bloom.filter.enabled\", true); // Enable the bloom filter for all columns\n+conf.set(\"parquet.bloom.filter.enabled#column.path\", false); // Disable the bloom filter for the column 'column.path'\n+// The bloom filter will be enabled for all columns except 'column.path'\n+```\n+\n+---\n+\n+**Property:** `parquet.bloom.filter.expected.ndv`  \n+**Description:** The expected number of distinct values in a column, it is used to compute the optimal size of the bloom filter.  \n+Note that if this property is not enabled, the bloom filter will use the maximum size.  \n+If this is property is set without a the column name, all columns will have a boom filter with the same expected number of distinct values.  \n+If this property is set for a column, then no need to enable the bloom filter with `parquet.bloom.filter.enabled` property.  \n+**Example:**\n+```java\n+conf.set(\"parquet.bloom.filter.expected.ndv#column.path\", 200) // The bloom filter will be enabled for 'column.path' with expected number of distinct values equals to 200\n+```\n+```java\n+conf.set(\"parquet.bloom.filter.expected.ndv\", 200) // Enable bloom filter for all columns with expected number of distinct values equals to 200\n+```\n+\n+---\n+\n+**Property:** `parquet.bloom.filter.max.bytes`  \n+**Description:** The maximum number of bytes for a bloom filter bitset.  \n+**Default value:** 1024*1024 (1MB)\n+\n+---\n+\n+**Property:** `parquet.page.row.count.limit`  \n+**Description:** The maximum number of row per page.  \n+**Default value:** 20000\n+\n+---\n+\n+**Property:** `parquet.page.write-checksum.enabled`  \n+**Description:** Whether to write out page level checksums.  \n+**Default value:** true\n+\n+## Class: ParquetInputFormat\n+\n+**Property:** `parquet.read.support.class`  \n+**Description:** The read support class.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "686d4c1da21cffd5b888e54bc0332eda1a5b71f0"}, "originalPosition": 231}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzODA0MjU0OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxMjoyMTo1MFrOGF3IHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxMjoyMTo1MFrOGF3IHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODc5OTI2MQ==", "bodyText": "nit:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - If the file system (FS) used supports blocks like HDFS, the block size will be the maximum between the default block size of FS and this property. And the row group size will be equals to this property.  \n          \n          \n            \n            - If the file system (FS) used supports blocks like HDFS, the block size will be the maximum between the default block size of FS and this property. And the row group size will be equal to this property.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r408799261", "createdAt": "2020-04-15T12:21:50Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -58,9 +58,15 @@ If it is `true`, it similar to `parquet.summary.metadata.level` with `all`. If i\n ---\n \n **Property:** `parquet.block.size`  \n-**Description:** The block size in bytes.\n-is the size of a row group being buffered in memory. This limits the memory usage when writing.  \n-Larger values will improve the IO when reading but consume more memory when writing.  \n+**Description:** The block size in bytes. This property depends on the file system:\n+\n+- If the file system (FS) used supports blocks like HDFS, the block size will be the maximum between the default block size of FS and this property. And the row group size will be equals to this property.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f5a218b83b66123bfd17319faebe29c17055674"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzODExNDQ2OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxMjo0MToxMlrOGF30BQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQyMToxNDoxMVrOGJYxQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODgxMDUwMQ==", "bodyText": "It is not that simple and parquet.page.size.row.check.min shall be updated as well.\nSo, the frequency of the checks will be between parquet.page.size.row.check.min and parquet.page.size.row.check.max. You may influence the checking frequency by setting these numbers. (Higher frequency results more precise page sizes, lower frequency improves performance.) See the code for more details.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r408810501", "createdAt": "2020-04-15T12:41:12Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -158,7 +164,7 @@ This property should be between 0 and 1.\n ---\n \n **Property:** `parquet.page.size.row.check.max`  \n-**Description:** The maximum number of rows per page.  \n+**Description:** The frequency of checks of the page size limit. In other words, we perform the checking after each `parquet.page.size.row.check.max` rows.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f5a218b83b66123bfd17319faebe29c17055674"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQ5NjE5NA==", "bodyText": "Thanks @gszadovszky for the clarification, I'll duplicate the description for both parquet.page.size.row.check.min and parquet.page.size.row.check.max properties.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r412496194", "createdAt": "2020-04-21T21:14:11Z", "author": {"login": "garawalid"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -158,7 +164,7 @@ This property should be between 0 and 1.\n ---\n \n **Property:** `parquet.page.size.row.check.max`  \n-**Description:** The maximum number of rows per page.  \n+**Description:** The frequency of checks of the page size limit. In other words, we perform the checking after each `parquet.page.size.row.check.max` rows.  ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODgxMDUwMQ=="}, "originalCommit": {"oid": "3f5a218b83b66123bfd17319faebe29c17055674"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzODEyNTg1OnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxMjo0NDowM1rOGF368A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQyMToxMjozMVrOGJYtQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODgxMjI3Mg==", "bodyText": "We shall suggest using the new one.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r408812272", "createdAt": "2020-04-15T12:44:03Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -230,23 +236,28 @@ conf.set(\"parquet.bloom.filter.expected.ndv#column.path\", 200)\n ## Class: ParquetInputFormat\n \n **Property:** `parquet.read.support.class`  \n-**Description:** The read support class.\n+**Description:** The read support class that is used in\n+ParquetInputFormat to materialize records. It should be a the descendant class of `org.apache.parquet.hadoop.api.ReadSupport`\n \n ---\n \n **Property:** `parquet.read.filter`  \n-**Description:** **Todo**\n+**Description:** The filter class name that implements `org.apache.parquet.filter.UnboundRecordFilter`. This class is for the old filter API in the package `org.apache.parquet.filter`, it filters records during record assembly.\n \n ---\n \n-**Property:** `parquet.strict.typing`  \n-**Description:** Whether to enable type checking for conflicting schema.  \n-**Default value:** `true`\n+ **Property:** `parquet.private.read.filter.predicate`  \n+ **Description:** The filter class used in the new filter API in the package `org.apache.parquet.filter2.predicate`\n+ Note that this class should implements `org.apache.parquet.filter2..FilterPredicate` and the value of this property should be a gzip compressed base64 encoded java serialized object.  \n+ The new filter API can filter records or filter entire row groups of records without reading them at all.\n+\n+**Note:** User should either use the old filter API (`parquet.read.filter`) or the new one (`parquet.private.read.filter.predicate`).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f5a218b83b66123bfd17319faebe29c17055674"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQ5NTE3MQ==", "bodyText": "I agree!", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r412495171", "createdAt": "2020-04-21T21:12:31Z", "author": {"login": "garawalid"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -230,23 +236,28 @@ conf.set(\"parquet.bloom.filter.expected.ndv#column.path\", 200)\n ## Class: ParquetInputFormat\n \n **Property:** `parquet.read.support.class`  \n-**Description:** The read support class.\n+**Description:** The read support class that is used in\n+ParquetInputFormat to materialize records. It should be a the descendant class of `org.apache.parquet.hadoop.api.ReadSupport`\n \n ---\n \n **Property:** `parquet.read.filter`  \n-**Description:** **Todo**\n+**Description:** The filter class name that implements `org.apache.parquet.filter.UnboundRecordFilter`. This class is for the old filter API in the package `org.apache.parquet.filter`, it filters records during record assembly.\n \n ---\n \n-**Property:** `parquet.strict.typing`  \n-**Description:** Whether to enable type checking for conflicting schema.  \n-**Default value:** `true`\n+ **Property:** `parquet.private.read.filter.predicate`  \n+ **Description:** The filter class used in the new filter API in the package `org.apache.parquet.filter2.predicate`\n+ Note that this class should implements `org.apache.parquet.filter2..FilterPredicate` and the value of this property should be a gzip compressed base64 encoded java serialized object.  \n+ The new filter API can filter records or filter entire row groups of records without reading them at all.\n+\n+**Note:** User should either use the old filter API (`parquet.read.filter`) or the new one (`parquet.private.read.filter.predicate`).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODgxMjI3Mg=="}, "originalCommit": {"oid": "3f5a218b83b66123bfd17319faebe29c17055674"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzODE0MzAxOnYy", "diffSide": "RIGHT", "path": "parquet-hadoop/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQxMjo0ODo0NVrOGF4Fyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQyMToxNjo1MFrOGJY25w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODgxNTA1MA==", "bodyText": "It is not a class but a serialized java object structure. But I am not sure if we would like to describe it in details. What is more interesting to the user is that this value shall not be set manually, the method ParquetInputFormat.setFilterPredicate shall be used instead.\nWe might also mention the property parquet.private.read.filter.predicate.human.readable that contains the filter in a human readable format for the users to help understanding the configuration. (The human readable format is not used for anything else.)", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r408815050", "createdAt": "2020-04-15T12:48:45Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -230,23 +236,28 @@ conf.set(\"parquet.bloom.filter.expected.ndv#column.path\", 200)\n ## Class: ParquetInputFormat\n \n **Property:** `parquet.read.support.class`  \n-**Description:** The read support class.\n+**Description:** The read support class that is used in\n+ParquetInputFormat to materialize records. It should be a the descendant class of `org.apache.parquet.hadoop.api.ReadSupport`\n \n ---\n \n **Property:** `parquet.read.filter`  \n-**Description:** **Todo**\n+**Description:** The filter class name that implements `org.apache.parquet.filter.UnboundRecordFilter`. This class is for the old filter API in the package `org.apache.parquet.filter`, it filters records during record assembly.\n \n ---\n \n-**Property:** `parquet.strict.typing`  \n-**Description:** Whether to enable type checking for conflicting schema.  \n-**Default value:** `true`\n+ **Property:** `parquet.private.read.filter.predicate`  \n+ **Description:** The filter class used in the new filter API in the package `org.apache.parquet.filter2.predicate`\n+ Note that this class should implements `org.apache.parquet.filter2..FilterPredicate` and the value of this property should be a gzip compressed base64 encoded java serialized object.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f5a218b83b66123bfd17319faebe29c17055674"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMjQ5NzYzOQ==", "bodyText": "I think it's okay if we keep the details of the object. After all, we will suggest using the setFilterPredicate method.", "url": "https://github.com/apache/parquet-mr/pull/781#discussion_r412497639", "createdAt": "2020-04-21T21:16:50Z", "author": {"login": "garawalid"}, "path": "parquet-hadoop/README.md", "diffHunk": "@@ -230,23 +236,28 @@ conf.set(\"parquet.bloom.filter.expected.ndv#column.path\", 200)\n ## Class: ParquetInputFormat\n \n **Property:** `parquet.read.support.class`  \n-**Description:** The read support class.\n+**Description:** The read support class that is used in\n+ParquetInputFormat to materialize records. It should be a the descendant class of `org.apache.parquet.hadoop.api.ReadSupport`\n \n ---\n \n **Property:** `parquet.read.filter`  \n-**Description:** **Todo**\n+**Description:** The filter class name that implements `org.apache.parquet.filter.UnboundRecordFilter`. This class is for the old filter API in the package `org.apache.parquet.filter`, it filters records during record assembly.\n \n ---\n \n-**Property:** `parquet.strict.typing`  \n-**Description:** Whether to enable type checking for conflicting schema.  \n-**Default value:** `true`\n+ **Property:** `parquet.private.read.filter.predicate`  \n+ **Description:** The filter class used in the new filter API in the package `org.apache.parquet.filter2.predicate`\n+ Note that this class should implements `org.apache.parquet.filter2..FilterPredicate` and the value of this property should be a gzip compressed base64 encoded java serialized object.  ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODgxNTA1MA=="}, "originalCommit": {"oid": "3f5a218b83b66123bfd17319faebe29c17055674"}, "originalPosition": 81}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4762, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}