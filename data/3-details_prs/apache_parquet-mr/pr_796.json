{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMzNDQ4OTMx", "number": 796, "title": "Parquet-1872: Add TransCompression command to parquet-tools", "bodyText": "Make sure you have checked all steps below.\nJira\n\n My PR addresses the following Parquet Jira issues and references them in the PR title. For example, \"PARQUET-1234: My Parquet PR\"\n\nhttps://issues.apache.org/jira/browse/PARQUET-XXX\nIn case you are adding a dependency, check if the license complies with the ASF 3rd Party License Policy.\n\n\n\nTests\n\n My PR adds the following unit tests OR does not need testing for this extremely good reason:\n\nCommits\n\n My commits all reference Jira issues in their subject lines. In addition, my commits follow the guidelines from \"How to write a good git commit message\":\n\nSubject is separated from body by a blank line\nSubject is limited to 50 characters (not including Jira issue reference)\nSubject does not end with a period\nSubject uses the imperative mood (\"add\", not \"adding\")\nBody wraps at 72 characters\nBody explains \"what\" and \"why\", not \"how\"\n\n\n\nDocumentation\n\n In case of new functionality, my PR adds documentation that describes how to use it.\n\nAll the public functions and the classes in the PR contain Javadoc that explain what it does", "createdAt": "2020-06-12T03:54:59Z", "url": "https://github.com/apache/parquet-mr/pull/796", "merged": true, "mergeCommit": {"oid": "e4988f3489663b99ff35a8573bab5522d5e6dcf8"}, "closed": true, "closedAt": "2020-06-23T09:40:05Z", "author": {"login": "shangxinli"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcr5z-TgBqjM0NTAzODc3NzA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcuCL3RAFqTQzNTYxNDAzOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "df5406e9969e54002e54543fa5943e79616416d8", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/df5406e9969e54002e54543fa5943e79616416d8", "committedDate": "2020-06-12T03:51:59Z", "message": "Parquet-1872: Add TransCompression command to parquet-tools"}, "afterCommit": {"oid": "228431e6b67c7553367b4a1e8f30befb319a4024", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/228431e6b67c7553367b4a1e8f30befb319a4024", "committedDate": "2020-06-16T18:44:36Z", "message": "Parquet-1872: Add TransCompression command to parquet-tools"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d0cf386e7b51b1a2488df7c07edab509e288cd8e", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/d0cf386e7b51b1a2488df7c07edab509e288cd8e", "committedDate": "2020-06-16T22:37:41Z", "message": "Parquet-1872: Add TransCompression command to parquet-tools"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "843660490620f9acd572c76328658b5a74e8fc3e", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/843660490620f9acd572c76328658b5a74e8fc3e", "committedDate": "2020-06-16T22:37:21Z", "message": "Add cli command"}, "afterCommit": {"oid": "d0cf386e7b51b1a2488df7c07edab509e288cd8e", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/d0cf386e7b51b1a2488df7c07edab509e288cd8e", "committedDate": "2020-06-16T22:37:41Z", "message": "Parquet-1872: Add TransCompression command to parquet-tools"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzMDM0OTc5", "url": "https://github.com/apache/parquet-mr/pull/796#pullrequestreview-433034979", "createdAt": "2020-06-18T08:02:53Z", "commit": {"oid": "d0cf386e7b51b1a2488df7c07edab509e288cd8e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwODowMjo1M1rOGlkFgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxMzozMDo0MVrOGlvZKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjA0MTczMQ==", "bodyText": "You may add the writer to the resource opening code part just next to the reader (by separating them with ';').", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442041731", "createdAt": "2020-06-18T08:02:53Z", "author": {"login": "gszadovszky"}, "path": "parquet-cli/src/main/java/org/apache/parquet/cli/commands/TransCompressionCommand.java", "diffHunk": "@@ -0,0 +1,302 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.cli.commands;\n+\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.Parameters;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.cli.BaseCommand;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.impl.ColumnReadStoreImpl;\n+import org.apache.parquet.column.page.DictionaryPage;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.column.statistics.Statistics;\n+import org.apache.parquet.compression.CompressionCodecFactory;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputCompressor;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputDecompressor;\n+import org.apache.parquet.format.DataPageHeader;\n+import org.apache.parquet.format.DataPageHeaderV2;\n+import org.apache.parquet.format.DictionaryPageHeader;\n+import org.apache.parquet.format.PageHeader;\n+import org.apache.parquet.format.Util;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetFileWriter;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopCodecs;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.InputFile;\n+import org.apache.parquet.io.ParquetEncodingException;\n+import org.apache.parquet.io.api.Converter;\n+import org.apache.parquet.io.api.GroupConverter;\n+import org.apache.parquet.io.api.PrimitiveConverter;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.slf4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+\n+@Parameters(commandDescription=\"Translate the compression from one to another\")\n+public class TransCompressionCommand extends BaseCommand {\n+\n+  public TransCompressionCommand(Logger console) {\n+    super(console);\n+  }\n+\n+  @Parameter(description = \"<input parquet file path>\")\n+  String input;\n+\n+  @Parameter(description = \"<output parquet file path>\")\n+  String output;\n+\n+  @Parameter(description = \"<new compression codec\")\n+  String codec;\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public int run() throws IOException {\n+    Preconditions.checkArgument(input != null && output != null,\n+      \"Both input and output parquet file paths are required.\");\n+\n+    Preconditions.checkArgument(codec != null,\n+      \"The codec cannot be null\");\n+\n+    Path inPath = new Path(input);\n+    Path outPath = new Path(output);\n+    CompressionCodecName codecName = CompressionCodecName.valueOf(codec);\n+\n+    ParquetMetadata metaData = ParquetFileReader.readFooter(getConf(), inPath, NO_FILTER);\n+    MessageType schema = metaData.getFileMetaData().getSchema();\n+\n+    try (TransParquetFileReader reader = new TransParquetFileReader(HadoopInputFile.fromPath(inPath, getConf()), HadoopReadOptions.builder(getConf()).build())) {\n+      ParquetFileWriter writer = new ParquetFileWriter(getConf(), schema, outPath, ParquetFileWriter.Mode.CREATE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0cf386e7b51b1a2488df7c07edab509e288cd8e"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIwMzMwMw==", "bodyText": "We stopped writing page statistics in purpose. It is never used in any of the implementations I aware of and using it would require to read every page headers which does not perform well. That's why we introduced column indexes. So, I would suggest writing page level statistics only if the original file has them.", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442203303", "createdAt": "2020-06-18T12:54:14Z", "author": {"login": "gszadovszky"}, "path": "parquet-cli/src/main/java/org/apache/parquet/cli/commands/TransCompressionCommand.java", "diffHunk": "@@ -0,0 +1,302 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.cli.commands;\n+\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.Parameters;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.cli.BaseCommand;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.impl.ColumnReadStoreImpl;\n+import org.apache.parquet.column.page.DictionaryPage;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.column.statistics.Statistics;\n+import org.apache.parquet.compression.CompressionCodecFactory;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputCompressor;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputDecompressor;\n+import org.apache.parquet.format.DataPageHeader;\n+import org.apache.parquet.format.DataPageHeaderV2;\n+import org.apache.parquet.format.DictionaryPageHeader;\n+import org.apache.parquet.format.PageHeader;\n+import org.apache.parquet.format.Util;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetFileWriter;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopCodecs;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.InputFile;\n+import org.apache.parquet.io.ParquetEncodingException;\n+import org.apache.parquet.io.api.Converter;\n+import org.apache.parquet.io.api.GroupConverter;\n+import org.apache.parquet.io.api.PrimitiveConverter;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.slf4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+\n+@Parameters(commandDescription=\"Translate the compression from one to another\")\n+public class TransCompressionCommand extends BaseCommand {\n+\n+  public TransCompressionCommand(Logger console) {\n+    super(console);\n+  }\n+\n+  @Parameter(description = \"<input parquet file path>\")\n+  String input;\n+\n+  @Parameter(description = \"<output parquet file path>\")\n+  String output;\n+\n+  @Parameter(description = \"<new compression codec\")\n+  String codec;\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public int run() throws IOException {\n+    Preconditions.checkArgument(input != null && output != null,\n+      \"Both input and output parquet file paths are required.\");\n+\n+    Preconditions.checkArgument(codec != null,\n+      \"The codec cannot be null\");\n+\n+    Path inPath = new Path(input);\n+    Path outPath = new Path(output);\n+    CompressionCodecName codecName = CompressionCodecName.valueOf(codec);\n+\n+    ParquetMetadata metaData = ParquetFileReader.readFooter(getConf(), inPath, NO_FILTER);\n+    MessageType schema = metaData.getFileMetaData().getSchema();\n+\n+    try (TransParquetFileReader reader = new TransParquetFileReader(HadoopInputFile.fromPath(inPath, getConf()), HadoopReadOptions.builder(getConf()).build())) {\n+      ParquetFileWriter writer = new ParquetFileWriter(getConf(), schema, outPath, ParquetFileWriter.Mode.CREATE);\n+      writer.start();\n+      processBlocks(reader, writer, metaData, schema, metaData.getFileMetaData().getCreatedBy(), codecName);\n+      writer.end(metaData.getFileMetaData().getKeyValueMetaData());\n+    }\n+    return 0;\n+  }\n+\n+  @Override\n+  public List<String> getExamples() {\n+    return Lists.newArrayList(\n+        \"# Translate the compression from one to another\",\n+        \" input.parquet output.parquet ZSTD\"\n+    );\n+  }\n+  private void processBlocks(TransParquetFileReader reader, ParquetFileWriter writer, ParquetMetadata meta, MessageType schema,\n+                             String createdBy, CompressionCodecName codecName) throws IOException {\n+    int blockIndex = 0;\n+    PageReadStore store = reader.readNextRowGroup();\n+    while (store != null) {\n+      writer.startBlock(store.getRowCount());\n+      BlockMetaData blockMetaData = meta.getBlocks().get(blockIndex);\n+      List<ColumnChunkMetaData> columnsInOrder = blockMetaData.getColumns();\n+      Map<ColumnPath, ColumnDescriptor> descriptorsMap = schema.getColumns().stream().collect(\n+        Collectors.toMap(x -> ColumnPath.get(x.getPath()), x -> x));\n+      for (int i = 0; i < columnsInOrder.size(); i += 1) {\n+        ColumnChunkMetaData chunk = columnsInOrder.get(i);\n+        ColumnReadStoreImpl crstore = new ColumnReadStoreImpl(store, new DumpGroupConverter(), schema, createdBy);\n+        ColumnDescriptor columnDescriptor = descriptorsMap.get(chunk.getPath());\n+        writer.startColumn(columnDescriptor, crstore.getColumnReader(columnDescriptor).getTotalValueCount(), codecName);\n+        processChunk(reader, writer, chunk, createdBy, codecName);\n+        writer.endColumn();\n+      }\n+      writer.endBlock();\n+      store = reader.readNextRowGroup();\n+      blockIndex++;\n+    }\n+  }\n+\n+  private void processChunk(TransParquetFileReader reader, ParquetFileWriter writer, ColumnChunkMetaData chunk,\n+                            String createdBy, CompressionCodecName codecName) throws IOException {\n+    CompressionCodecFactory codecFactory = HadoopCodecs.newFactory(0);\n+    BytesInputDecompressor decompressor = codecFactory.getDecompressor(chunk.getCodec());\n+    BytesInputCompressor compressor = codecFactory.getCompressor(codecName);\n+    ColumnIndex columnIndex = reader.readColumnIndex(chunk);\n+    OffsetIndex offsetIndex = reader.readOffsetIndex(chunk);\n+\n+    reader.setStreamPosition(chunk.getStartingPos());\n+    DictionaryPage dictionaryPage = null;\n+    long readValues = 0;\n+    Statistics statistics = null;\n+    ParquetMetadataConverter converter = new ParquetMetadataConverter();\n+    int pageIndex = 0;\n+    long totalChunkValues = chunk.getValueCount();\n+    while (readValues < totalChunkValues) {\n+      PageHeader pageHeader = reader.readPageHeader();\n+      byte[] pageLoad;\n+      switch (pageHeader.type) {\n+        case DICTIONARY_PAGE:\n+          if (dictionaryPage != null) {\n+            throw new IOException(\"has more than one dictionary page in column chunk\");\n+          }\n+          DictionaryPageHeader dictPageHeader = pageHeader.dictionary_page_header;\n+          pageLoad = translatePageLoad(reader, true, compressor, decompressor, pageHeader.getCompressed_page_size(), pageHeader.getUncompressed_page_size());\n+          writer.writeDictionaryPage(new DictionaryPage(BytesInput.from(pageLoad),\n+            pageHeader.getUncompressed_page_size(),\n+            converter.getEncoding(dictPageHeader.getEncoding())));\n+          break;\n+        case DATA_PAGE:\n+          DataPageHeader headerV1 = pageHeader.data_page_header;\n+          pageLoad = translatePageLoad(reader, true, compressor, decompressor, pageHeader.getCompressed_page_size(), pageHeader.getUncompressed_page_size());\n+          statistics = convertStatistics(createdBy, chunk.getPrimitiveType(), headerV1.getStatistics(), columnIndex, pageIndex, converter);\n+          readValues += headerV1.getNum_values();\n+          if (offsetIndex != null) {\n+            long rowCount = 1 + offsetIndex.getLastRowIndex(pageIndex, totalChunkValues) - offsetIndex.getFirstRowIndex(pageIndex);\n+            writer.writeDataPage(toIntWithCheck(headerV1.getNum_values()),\n+              pageHeader.uncompressed_page_size,\n+              BytesInput.from(pageLoad),\n+              statistics,\n+              toIntWithCheck(rowCount),\n+              converter.getEncoding(headerV1.getRepetition_level_encoding()),\n+              converter.getEncoding(headerV1.getDefinition_level_encoding()),\n+              converter.getEncoding(headerV1.getEncoding()));\n+          } else {\n+            writer.writeDataPage(toIntWithCheck(headerV1.getNum_values()),\n+              pageHeader.uncompressed_page_size,\n+              BytesInput.from(pageLoad),\n+              statistics,\n+              converter.getEncoding(headerV1.getRepetition_level_encoding()),\n+              converter.getEncoding(headerV1.getDefinition_level_encoding()),\n+              converter.getEncoding(headerV1.getEncoding()));\n+          }\n+          pageIndex++;\n+          break;\n+        case DATA_PAGE_V2:\n+          DataPageHeaderV2 headerV2 = pageHeader.data_page_header_v2;\n+          int rlLength = headerV2.getRepetition_levels_byte_length();\n+          BytesInput rlLevels = readBlock(rlLength, reader);\n+          int dlLength = headerV2.getDefinition_levels_byte_length();\n+          BytesInput dlLevels = readBlock(dlLength, reader);\n+          int payLoadLength = pageHeader.getCompressed_page_size() - rlLength - dlLength;\n+          int rawDataLength = pageHeader.getUncompressed_page_size() - rlLength - dlLength;\n+          pageLoad = translatePageLoad(reader, headerV2.is_compressed, compressor, decompressor, payLoadLength, rawDataLength);\n+          statistics = convertStatistics(createdBy, chunk.getPrimitiveType(), headerV2.getStatistics(), columnIndex, pageIndex, converter);\n+          readValues += headerV2.getNum_values();\n+          writer.writeDataPageV2(headerV2.getNum_rows(),\n+            headerV2.getNum_nulls(),\n+            headerV2.getNum_values(),\n+            rlLevels,\n+            dlLevels,\n+            converter.getEncoding(headerV2.getEncoding()),\n+            BytesInput.from(pageLoad),\n+            pageHeader.uncompressed_page_size - rlLength - dlLength,\n+            statistics);\n+          pageIndex++;\n+          break;\n+        default:\n+          break;\n+      }\n+    }\n+  }\n+\n+  private Statistics convertStatistics(String createdBy, PrimitiveType type, org.apache.parquet.format.Statistics pageStatistics,\n+                                       ColumnIndex columnIndex, int pageIndex, ParquetMetadataConverter converter) throws IOException {\n+    if (pageStatistics != null) {\n+      return converter.fromParquetStatistics(createdBy, pageStatistics, type);\n+    } else if (columnIndex != null) {\n+      if (columnIndex.getNullPages() == null) {\n+        throw new IOException(\"columnIndex has null variable 'nullPages' which indicates corrupted data for type: \" +  type.getName());\n+      }\n+      if (pageIndex > columnIndex.getNullPages().size()) {\n+        throw new IOException(\"There are more pages \" + pageIndex + \" found in the column than in the columnIndex \" + columnIndex.getNullPages().size());\n+      }\n+      org.apache.parquet.column.statistics.Statistics.Builder statsBuilder = org.apache.parquet.column.statistics.Statistics.getBuilderForReading(type);\n+      statsBuilder.withNumNulls(columnIndex.getNullCounts().get(pageIndex));\n+\n+      if (!columnIndex.getNullPages().get(pageIndex)) {\n+        statsBuilder.withMin(columnIndex.getMinValues().get(pageIndex).array().clone());\n+        statsBuilder.withMax(columnIndex.getMaxValues().get(pageIndex).array().clone());\n+      }\n+      return statsBuilder.build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0cf386e7b51b1a2488df7c07edab509e288cd8e"}, "originalPosition": 245}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIxMjgwNA==", "bodyText": "I think, Dummy would be a better naming instead of Dump.", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442212804", "createdAt": "2020-06-18T13:09:39Z", "author": {"login": "gszadovszky"}, "path": "parquet-cli/src/main/java/org/apache/parquet/cli/commands/TransCompressionCommand.java", "diffHunk": "@@ -0,0 +1,302 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.cli.commands;\n+\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.Parameters;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.cli.BaseCommand;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.impl.ColumnReadStoreImpl;\n+import org.apache.parquet.column.page.DictionaryPage;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.column.statistics.Statistics;\n+import org.apache.parquet.compression.CompressionCodecFactory;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputCompressor;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputDecompressor;\n+import org.apache.parquet.format.DataPageHeader;\n+import org.apache.parquet.format.DataPageHeaderV2;\n+import org.apache.parquet.format.DictionaryPageHeader;\n+import org.apache.parquet.format.PageHeader;\n+import org.apache.parquet.format.Util;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetFileWriter;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopCodecs;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.InputFile;\n+import org.apache.parquet.io.ParquetEncodingException;\n+import org.apache.parquet.io.api.Converter;\n+import org.apache.parquet.io.api.GroupConverter;\n+import org.apache.parquet.io.api.PrimitiveConverter;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.slf4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+\n+@Parameters(commandDescription=\"Translate the compression from one to another\")\n+public class TransCompressionCommand extends BaseCommand {\n+\n+  public TransCompressionCommand(Logger console) {\n+    super(console);\n+  }\n+\n+  @Parameter(description = \"<input parquet file path>\")\n+  String input;\n+\n+  @Parameter(description = \"<output parquet file path>\")\n+  String output;\n+\n+  @Parameter(description = \"<new compression codec\")\n+  String codec;\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public int run() throws IOException {\n+    Preconditions.checkArgument(input != null && output != null,\n+      \"Both input and output parquet file paths are required.\");\n+\n+    Preconditions.checkArgument(codec != null,\n+      \"The codec cannot be null\");\n+\n+    Path inPath = new Path(input);\n+    Path outPath = new Path(output);\n+    CompressionCodecName codecName = CompressionCodecName.valueOf(codec);\n+\n+    ParquetMetadata metaData = ParquetFileReader.readFooter(getConf(), inPath, NO_FILTER);\n+    MessageType schema = metaData.getFileMetaData().getSchema();\n+\n+    try (TransParquetFileReader reader = new TransParquetFileReader(HadoopInputFile.fromPath(inPath, getConf()), HadoopReadOptions.builder(getConf()).build())) {\n+      ParquetFileWriter writer = new ParquetFileWriter(getConf(), schema, outPath, ParquetFileWriter.Mode.CREATE);\n+      writer.start();\n+      processBlocks(reader, writer, metaData, schema, metaData.getFileMetaData().getCreatedBy(), codecName);\n+      writer.end(metaData.getFileMetaData().getKeyValueMetaData());\n+    }\n+    return 0;\n+  }\n+\n+  @Override\n+  public List<String> getExamples() {\n+    return Lists.newArrayList(\n+        \"# Translate the compression from one to another\",\n+        \" input.parquet output.parquet ZSTD\"\n+    );\n+  }\n+  private void processBlocks(TransParquetFileReader reader, ParquetFileWriter writer, ParquetMetadata meta, MessageType schema,\n+                             String createdBy, CompressionCodecName codecName) throws IOException {\n+    int blockIndex = 0;\n+    PageReadStore store = reader.readNextRowGroup();\n+    while (store != null) {\n+      writer.startBlock(store.getRowCount());\n+      BlockMetaData blockMetaData = meta.getBlocks().get(blockIndex);\n+      List<ColumnChunkMetaData> columnsInOrder = blockMetaData.getColumns();\n+      Map<ColumnPath, ColumnDescriptor> descriptorsMap = schema.getColumns().stream().collect(\n+        Collectors.toMap(x -> ColumnPath.get(x.getPath()), x -> x));\n+      for (int i = 0; i < columnsInOrder.size(); i += 1) {\n+        ColumnChunkMetaData chunk = columnsInOrder.get(i);\n+        ColumnReadStoreImpl crstore = new ColumnReadStoreImpl(store, new DumpGroupConverter(), schema, createdBy);\n+        ColumnDescriptor columnDescriptor = descriptorsMap.get(chunk.getPath());\n+        writer.startColumn(columnDescriptor, crstore.getColumnReader(columnDescriptor).getTotalValueCount(), codecName);\n+        processChunk(reader, writer, chunk, createdBy, codecName);\n+        writer.endColumn();\n+      }\n+      writer.endBlock();\n+      store = reader.readNextRowGroup();\n+      blockIndex++;\n+    }\n+  }\n+\n+  private void processChunk(TransParquetFileReader reader, ParquetFileWriter writer, ColumnChunkMetaData chunk,\n+                            String createdBy, CompressionCodecName codecName) throws IOException {\n+    CompressionCodecFactory codecFactory = HadoopCodecs.newFactory(0);\n+    BytesInputDecompressor decompressor = codecFactory.getDecompressor(chunk.getCodec());\n+    BytesInputCompressor compressor = codecFactory.getCompressor(codecName);\n+    ColumnIndex columnIndex = reader.readColumnIndex(chunk);\n+    OffsetIndex offsetIndex = reader.readOffsetIndex(chunk);\n+\n+    reader.setStreamPosition(chunk.getStartingPos());\n+    DictionaryPage dictionaryPage = null;\n+    long readValues = 0;\n+    Statistics statistics = null;\n+    ParquetMetadataConverter converter = new ParquetMetadataConverter();\n+    int pageIndex = 0;\n+    long totalChunkValues = chunk.getValueCount();\n+    while (readValues < totalChunkValues) {\n+      PageHeader pageHeader = reader.readPageHeader();\n+      byte[] pageLoad;\n+      switch (pageHeader.type) {\n+        case DICTIONARY_PAGE:\n+          if (dictionaryPage != null) {\n+            throw new IOException(\"has more than one dictionary page in column chunk\");\n+          }\n+          DictionaryPageHeader dictPageHeader = pageHeader.dictionary_page_header;\n+          pageLoad = translatePageLoad(reader, true, compressor, decompressor, pageHeader.getCompressed_page_size(), pageHeader.getUncompressed_page_size());\n+          writer.writeDictionaryPage(new DictionaryPage(BytesInput.from(pageLoad),\n+            pageHeader.getUncompressed_page_size(),\n+            converter.getEncoding(dictPageHeader.getEncoding())));\n+          break;\n+        case DATA_PAGE:\n+          DataPageHeader headerV1 = pageHeader.data_page_header;\n+          pageLoad = translatePageLoad(reader, true, compressor, decompressor, pageHeader.getCompressed_page_size(), pageHeader.getUncompressed_page_size());\n+          statistics = convertStatistics(createdBy, chunk.getPrimitiveType(), headerV1.getStatistics(), columnIndex, pageIndex, converter);\n+          readValues += headerV1.getNum_values();\n+          if (offsetIndex != null) {\n+            long rowCount = 1 + offsetIndex.getLastRowIndex(pageIndex, totalChunkValues) - offsetIndex.getFirstRowIndex(pageIndex);\n+            writer.writeDataPage(toIntWithCheck(headerV1.getNum_values()),\n+              pageHeader.uncompressed_page_size,\n+              BytesInput.from(pageLoad),\n+              statistics,\n+              toIntWithCheck(rowCount),\n+              converter.getEncoding(headerV1.getRepetition_level_encoding()),\n+              converter.getEncoding(headerV1.getDefinition_level_encoding()),\n+              converter.getEncoding(headerV1.getEncoding()));\n+          } else {\n+            writer.writeDataPage(toIntWithCheck(headerV1.getNum_values()),\n+              pageHeader.uncompressed_page_size,\n+              BytesInput.from(pageLoad),\n+              statistics,\n+              converter.getEncoding(headerV1.getRepetition_level_encoding()),\n+              converter.getEncoding(headerV1.getDefinition_level_encoding()),\n+              converter.getEncoding(headerV1.getEncoding()));\n+          }\n+          pageIndex++;\n+          break;\n+        case DATA_PAGE_V2:\n+          DataPageHeaderV2 headerV2 = pageHeader.data_page_header_v2;\n+          int rlLength = headerV2.getRepetition_levels_byte_length();\n+          BytesInput rlLevels = readBlock(rlLength, reader);\n+          int dlLength = headerV2.getDefinition_levels_byte_length();\n+          BytesInput dlLevels = readBlock(dlLength, reader);\n+          int payLoadLength = pageHeader.getCompressed_page_size() - rlLength - dlLength;\n+          int rawDataLength = pageHeader.getUncompressed_page_size() - rlLength - dlLength;\n+          pageLoad = translatePageLoad(reader, headerV2.is_compressed, compressor, decompressor, payLoadLength, rawDataLength);\n+          statistics = convertStatistics(createdBy, chunk.getPrimitiveType(), headerV2.getStatistics(), columnIndex, pageIndex, converter);\n+          readValues += headerV2.getNum_values();\n+          writer.writeDataPageV2(headerV2.getNum_rows(),\n+            headerV2.getNum_nulls(),\n+            headerV2.getNum_values(),\n+            rlLevels,\n+            dlLevels,\n+            converter.getEncoding(headerV2.getEncoding()),\n+            BytesInput.from(pageLoad),\n+            pageHeader.uncompressed_page_size - rlLength - dlLength,\n+            statistics);\n+          pageIndex++;\n+          break;\n+        default:\n+          break;\n+      }\n+    }\n+  }\n+\n+  private Statistics convertStatistics(String createdBy, PrimitiveType type, org.apache.parquet.format.Statistics pageStatistics,\n+                                       ColumnIndex columnIndex, int pageIndex, ParquetMetadataConverter converter) throws IOException {\n+    if (pageStatistics != null) {\n+      return converter.fromParquetStatistics(createdBy, pageStatistics, type);\n+    } else if (columnIndex != null) {\n+      if (columnIndex.getNullPages() == null) {\n+        throw new IOException(\"columnIndex has null variable 'nullPages' which indicates corrupted data for type: \" +  type.getName());\n+      }\n+      if (pageIndex > columnIndex.getNullPages().size()) {\n+        throw new IOException(\"There are more pages \" + pageIndex + \" found in the column than in the columnIndex \" + columnIndex.getNullPages().size());\n+      }\n+      org.apache.parquet.column.statistics.Statistics.Builder statsBuilder = org.apache.parquet.column.statistics.Statistics.getBuilderForReading(type);\n+      statsBuilder.withNumNulls(columnIndex.getNullCounts().get(pageIndex));\n+\n+      if (!columnIndex.getNullPages().get(pageIndex)) {\n+        statsBuilder.withMin(columnIndex.getMinValues().get(pageIndex).array().clone());\n+        statsBuilder.withMax(columnIndex.getMaxValues().get(pageIndex).array().clone());\n+      }\n+      return statsBuilder.build();\n+    } else {\n+      return null;\n+    }\n+  }\n+\n+  private byte[] translatePageLoad(TransParquetFileReader reader, boolean isCompressed, BytesInputCompressor compressor,\n+                                   BytesInputDecompressor decompressor, int payloadLength, int rawDataLength) throws IOException {\n+    BytesInput data = readBlock(payloadLength, reader);\n+    if (isCompressed) {\n+      data = decompressor.decompress(data, rawDataLength);\n+    }\n+    BytesInput newCompressedData = compressor.compress(data);\n+    return newCompressedData.toByteArray();\n+  }\n+\n+  private BytesInput readBlock(int length, TransParquetFileReader reader) throws IOException {\n+    byte[] data = new byte[length];\n+    reader.blockRead(data);\n+    return BytesInput.from(data);\n+  }\n+\n+  private int toIntWithCheck(long size) {\n+    if ((int)size != size) {\n+      throw new ParquetEncodingException(\"size is bigger than \" + Integer.MAX_VALUE + \" bytes: \" + size);\n+    }\n+    return (int)size;\n+  }\n+\n+  private static final class DumpGroupConverter extends GroupConverter {\n+    @Override public void start() {}\n+    @Override public void end() {}\n+    @Override public Converter getConverter(int fieldIndex) { return new DumpConverter(); }\n+  }\n+\n+  private static final class DumpConverter extends PrimitiveConverter {\n+    @Override public GroupConverter asGroupConverter() { return new DumpGroupConverter(); }\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0cf386e7b51b1a2488df7c07edab509e288cd8e"}, "originalPosition": 282}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIxNTMzNg==", "bodyText": "Creating a byte array can be expensive if there are many pages. I would suggest maintaining a cache array that is reused and created only if a block is larger than the cache.", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442215336", "createdAt": "2020-06-18T13:13:35Z", "author": {"login": "gszadovszky"}, "path": "parquet-cli/src/main/java/org/apache/parquet/cli/commands/TransCompressionCommand.java", "diffHunk": "@@ -0,0 +1,302 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.cli.commands;\n+\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.Parameters;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.cli.BaseCommand;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.impl.ColumnReadStoreImpl;\n+import org.apache.parquet.column.page.DictionaryPage;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.column.statistics.Statistics;\n+import org.apache.parquet.compression.CompressionCodecFactory;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputCompressor;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputDecompressor;\n+import org.apache.parquet.format.DataPageHeader;\n+import org.apache.parquet.format.DataPageHeaderV2;\n+import org.apache.parquet.format.DictionaryPageHeader;\n+import org.apache.parquet.format.PageHeader;\n+import org.apache.parquet.format.Util;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetFileWriter;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopCodecs;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.InputFile;\n+import org.apache.parquet.io.ParquetEncodingException;\n+import org.apache.parquet.io.api.Converter;\n+import org.apache.parquet.io.api.GroupConverter;\n+import org.apache.parquet.io.api.PrimitiveConverter;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.slf4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+\n+@Parameters(commandDescription=\"Translate the compression from one to another\")\n+public class TransCompressionCommand extends BaseCommand {\n+\n+  public TransCompressionCommand(Logger console) {\n+    super(console);\n+  }\n+\n+  @Parameter(description = \"<input parquet file path>\")\n+  String input;\n+\n+  @Parameter(description = \"<output parquet file path>\")\n+  String output;\n+\n+  @Parameter(description = \"<new compression codec\")\n+  String codec;\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public int run() throws IOException {\n+    Preconditions.checkArgument(input != null && output != null,\n+      \"Both input and output parquet file paths are required.\");\n+\n+    Preconditions.checkArgument(codec != null,\n+      \"The codec cannot be null\");\n+\n+    Path inPath = new Path(input);\n+    Path outPath = new Path(output);\n+    CompressionCodecName codecName = CompressionCodecName.valueOf(codec);\n+\n+    ParquetMetadata metaData = ParquetFileReader.readFooter(getConf(), inPath, NO_FILTER);\n+    MessageType schema = metaData.getFileMetaData().getSchema();\n+\n+    try (TransParquetFileReader reader = new TransParquetFileReader(HadoopInputFile.fromPath(inPath, getConf()), HadoopReadOptions.builder(getConf()).build())) {\n+      ParquetFileWriter writer = new ParquetFileWriter(getConf(), schema, outPath, ParquetFileWriter.Mode.CREATE);\n+      writer.start();\n+      processBlocks(reader, writer, metaData, schema, metaData.getFileMetaData().getCreatedBy(), codecName);\n+      writer.end(metaData.getFileMetaData().getKeyValueMetaData());\n+    }\n+    return 0;\n+  }\n+\n+  @Override\n+  public List<String> getExamples() {\n+    return Lists.newArrayList(\n+        \"# Translate the compression from one to another\",\n+        \" input.parquet output.parquet ZSTD\"\n+    );\n+  }\n+  private void processBlocks(TransParquetFileReader reader, ParquetFileWriter writer, ParquetMetadata meta, MessageType schema,\n+                             String createdBy, CompressionCodecName codecName) throws IOException {\n+    int blockIndex = 0;\n+    PageReadStore store = reader.readNextRowGroup();\n+    while (store != null) {\n+      writer.startBlock(store.getRowCount());\n+      BlockMetaData blockMetaData = meta.getBlocks().get(blockIndex);\n+      List<ColumnChunkMetaData> columnsInOrder = blockMetaData.getColumns();\n+      Map<ColumnPath, ColumnDescriptor> descriptorsMap = schema.getColumns().stream().collect(\n+        Collectors.toMap(x -> ColumnPath.get(x.getPath()), x -> x));\n+      for (int i = 0; i < columnsInOrder.size(); i += 1) {\n+        ColumnChunkMetaData chunk = columnsInOrder.get(i);\n+        ColumnReadStoreImpl crstore = new ColumnReadStoreImpl(store, new DumpGroupConverter(), schema, createdBy);\n+        ColumnDescriptor columnDescriptor = descriptorsMap.get(chunk.getPath());\n+        writer.startColumn(columnDescriptor, crstore.getColumnReader(columnDescriptor).getTotalValueCount(), codecName);\n+        processChunk(reader, writer, chunk, createdBy, codecName);\n+        writer.endColumn();\n+      }\n+      writer.endBlock();\n+      store = reader.readNextRowGroup();\n+      blockIndex++;\n+    }\n+  }\n+\n+  private void processChunk(TransParquetFileReader reader, ParquetFileWriter writer, ColumnChunkMetaData chunk,\n+                            String createdBy, CompressionCodecName codecName) throws IOException {\n+    CompressionCodecFactory codecFactory = HadoopCodecs.newFactory(0);\n+    BytesInputDecompressor decompressor = codecFactory.getDecompressor(chunk.getCodec());\n+    BytesInputCompressor compressor = codecFactory.getCompressor(codecName);\n+    ColumnIndex columnIndex = reader.readColumnIndex(chunk);\n+    OffsetIndex offsetIndex = reader.readOffsetIndex(chunk);\n+\n+    reader.setStreamPosition(chunk.getStartingPos());\n+    DictionaryPage dictionaryPage = null;\n+    long readValues = 0;\n+    Statistics statistics = null;\n+    ParquetMetadataConverter converter = new ParquetMetadataConverter();\n+    int pageIndex = 0;\n+    long totalChunkValues = chunk.getValueCount();\n+    while (readValues < totalChunkValues) {\n+      PageHeader pageHeader = reader.readPageHeader();\n+      byte[] pageLoad;\n+      switch (pageHeader.type) {\n+        case DICTIONARY_PAGE:\n+          if (dictionaryPage != null) {\n+            throw new IOException(\"has more than one dictionary page in column chunk\");\n+          }\n+          DictionaryPageHeader dictPageHeader = pageHeader.dictionary_page_header;\n+          pageLoad = translatePageLoad(reader, true, compressor, decompressor, pageHeader.getCompressed_page_size(), pageHeader.getUncompressed_page_size());\n+          writer.writeDictionaryPage(new DictionaryPage(BytesInput.from(pageLoad),\n+            pageHeader.getUncompressed_page_size(),\n+            converter.getEncoding(dictPageHeader.getEncoding())));\n+          break;\n+        case DATA_PAGE:\n+          DataPageHeader headerV1 = pageHeader.data_page_header;\n+          pageLoad = translatePageLoad(reader, true, compressor, decompressor, pageHeader.getCompressed_page_size(), pageHeader.getUncompressed_page_size());\n+          statistics = convertStatistics(createdBy, chunk.getPrimitiveType(), headerV1.getStatistics(), columnIndex, pageIndex, converter);\n+          readValues += headerV1.getNum_values();\n+          if (offsetIndex != null) {\n+            long rowCount = 1 + offsetIndex.getLastRowIndex(pageIndex, totalChunkValues) - offsetIndex.getFirstRowIndex(pageIndex);\n+            writer.writeDataPage(toIntWithCheck(headerV1.getNum_values()),\n+              pageHeader.uncompressed_page_size,\n+              BytesInput.from(pageLoad),\n+              statistics,\n+              toIntWithCheck(rowCount),\n+              converter.getEncoding(headerV1.getRepetition_level_encoding()),\n+              converter.getEncoding(headerV1.getDefinition_level_encoding()),\n+              converter.getEncoding(headerV1.getEncoding()));\n+          } else {\n+            writer.writeDataPage(toIntWithCheck(headerV1.getNum_values()),\n+              pageHeader.uncompressed_page_size,\n+              BytesInput.from(pageLoad),\n+              statistics,\n+              converter.getEncoding(headerV1.getRepetition_level_encoding()),\n+              converter.getEncoding(headerV1.getDefinition_level_encoding()),\n+              converter.getEncoding(headerV1.getEncoding()));\n+          }\n+          pageIndex++;\n+          break;\n+        case DATA_PAGE_V2:\n+          DataPageHeaderV2 headerV2 = pageHeader.data_page_header_v2;\n+          int rlLength = headerV2.getRepetition_levels_byte_length();\n+          BytesInput rlLevels = readBlock(rlLength, reader);\n+          int dlLength = headerV2.getDefinition_levels_byte_length();\n+          BytesInput dlLevels = readBlock(dlLength, reader);\n+          int payLoadLength = pageHeader.getCompressed_page_size() - rlLength - dlLength;\n+          int rawDataLength = pageHeader.getUncompressed_page_size() - rlLength - dlLength;\n+          pageLoad = translatePageLoad(reader, headerV2.is_compressed, compressor, decompressor, payLoadLength, rawDataLength);\n+          statistics = convertStatistics(createdBy, chunk.getPrimitiveType(), headerV2.getStatistics(), columnIndex, pageIndex, converter);\n+          readValues += headerV2.getNum_values();\n+          writer.writeDataPageV2(headerV2.getNum_rows(),\n+            headerV2.getNum_nulls(),\n+            headerV2.getNum_values(),\n+            rlLevels,\n+            dlLevels,\n+            converter.getEncoding(headerV2.getEncoding()),\n+            BytesInput.from(pageLoad),\n+            pageHeader.uncompressed_page_size - rlLength - dlLength,\n+            statistics);\n+          pageIndex++;\n+          break;\n+        default:\n+          break;\n+      }\n+    }\n+  }\n+\n+  private Statistics convertStatistics(String createdBy, PrimitiveType type, org.apache.parquet.format.Statistics pageStatistics,\n+                                       ColumnIndex columnIndex, int pageIndex, ParquetMetadataConverter converter) throws IOException {\n+    if (pageStatistics != null) {\n+      return converter.fromParquetStatistics(createdBy, pageStatistics, type);\n+    } else if (columnIndex != null) {\n+      if (columnIndex.getNullPages() == null) {\n+        throw new IOException(\"columnIndex has null variable 'nullPages' which indicates corrupted data for type: \" +  type.getName());\n+      }\n+      if (pageIndex > columnIndex.getNullPages().size()) {\n+        throw new IOException(\"There are more pages \" + pageIndex + \" found in the column than in the columnIndex \" + columnIndex.getNullPages().size());\n+      }\n+      org.apache.parquet.column.statistics.Statistics.Builder statsBuilder = org.apache.parquet.column.statistics.Statistics.getBuilderForReading(type);\n+      statsBuilder.withNumNulls(columnIndex.getNullCounts().get(pageIndex));\n+\n+      if (!columnIndex.getNullPages().get(pageIndex)) {\n+        statsBuilder.withMin(columnIndex.getMinValues().get(pageIndex).array().clone());\n+        statsBuilder.withMax(columnIndex.getMaxValues().get(pageIndex).array().clone());\n+      }\n+      return statsBuilder.build();\n+    } else {\n+      return null;\n+    }\n+  }\n+\n+  private byte[] translatePageLoad(TransParquetFileReader reader, boolean isCompressed, BytesInputCompressor compressor,\n+                                   BytesInputDecompressor decompressor, int payloadLength, int rawDataLength) throws IOException {\n+    BytesInput data = readBlock(payloadLength, reader);\n+    if (isCompressed) {\n+      data = decompressor.decompress(data, rawDataLength);\n+    }\n+    BytesInput newCompressedData = compressor.compress(data);\n+    return newCompressedData.toByteArray();\n+  }\n+\n+  private BytesInput readBlock(int length, TransParquetFileReader reader) throws IOException {\n+    byte[] data = new byte[length];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0cf386e7b51b1a2488df7c07edab509e288cd8e"}, "originalPosition": 262}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIyMTg2Mg==", "bodyText": "You are not testing OffsetIndex.getOffset(int) which is a big problem. These offsets are to reference the exact locations of the pages in the file. It is not trivial to test it but if you were it would fail because you are not rewriting it in the new file. Every other data in the column/offset indexes shall be the same in the source and the target file but these values shall be recalculated as the sizes of the pages will be different after compressed by another codec so the positions shift in the file.", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442221862", "createdAt": "2020-06-18T13:23:26Z", "author": {"login": "gszadovszky"}, "path": "parquet-cli/src/test/java/org/apache/parquet/cli/commands/TransCompressionCommandTest.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.cli.commands;\n+\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.example.data.simple.convert.GroupRecordConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.ExampleParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.ColumnIOFactory;\n+import org.apache.parquet.io.MessageColumnIO;\n+import org.apache.parquet.io.RecordReader;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TransCompressionCommandTest extends ParquetFileTest  {\n+\n+  private Configuration conf = new Configuration();\n+  private Map<String, String> extraMeta\n+    = ImmutableMap.of(\"key1\", \"value1\", \"key2\", \"value2\");\n+\n+  @Test\n+  public void testTransCompression() throws Exception {\n+    String[] codecs = {\"UNCOMPRESSED\", \"SNAPPY\", \"GZIP\", \"ZSTD\"};\n+    for (int i = 0; i < codecs.length; i++) {\n+      for (int j = 0; j <codecs.length; j++) {\n+        // Same codec for both are considered as valid test case\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_2_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, 64);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE * 100);\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testSpeed() throws Exception {\n+    String inputFile = createParquetFile(\"input\", \"GZIP\", 100000,\n+    ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+    String outputFile = createTempFile(\"output_trans\");\n+\n+    long start = System.currentTimeMillis();\n+    TransCompressionCommand command = new TransCompressionCommand(createLogger());\n+    command.setConf(new Configuration());\n+    command.input = inputFile;\n+    command.output = outputFile;\n+    command.codec = \"ZSTD\";\n+    command.run();\n+    long durationTrans = System.currentTimeMillis() - start;\n+\n+    outputFile = createTempFile(\"output_record\");\n+    start = System.currentTimeMillis();\n+    convertRecordByRecord(CompressionCodecName.valueOf(\"ZSTD\"), new Path(inputFile), new Path(outputFile));\n+    long durationRecord = System.currentTimeMillis() - start;\n+\n+    // The TransCompressionCommand is ~5 times faster than translating record by record\n+    Assert.assertTrue(durationTrans < durationRecord);\n+  }\n+\n+  private void testInternal(String srcCodec, String destCodec, ParquetProperties.WriterVersion writerVersion, int pageSize) throws Exception {\n+    int numRecord = 1000;\n+    String inputFile = createParquetFile(\"input\", srcCodec, numRecord, writerVersion, pageSize);\n+    String outputFile = createTempFile(\"output_trans\");\n+\n+    TransCompressionCommand command = new TransCompressionCommand(createLogger());\n+    command.setConf(new Configuration());\n+    command.input = inputFile;\n+    command.output = outputFile;\n+    command.codec = destCodec;\n+    command.run();\n+\n+    validateColumns(inputFile, numRecord);\n+    validMeta(inputFile, outputFile);\n+    validColumnIndex(inputFile, outputFile);\n+  }\n+\n+  private void convertRecordByRecord(CompressionCodecName codecName, Path inpath, Path outpath) throws Exception {\n+    ParquetMetadata metaData = ParquetFileReader.readFooter(conf, inpath, NO_FILTER);\n+    MessageType schema = metaData.getFileMetaData().getSchema();\n+    HadoopInputFile inputFile = HadoopInputFile.fromPath(inpath, conf);\n+    ParquetReadOptions readOptions = HadoopReadOptions.builder(conf).build();\n+\n+    conf.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString());\n+    ExampleParquetWriter.Builder builder = ExampleParquetWriter.builder(outpath).withConf(conf).withCompressionCodec(codecName);\n+\n+    ParquetWriter parquetWriter = builder.build();\n+\n+    PageReadStore pages;\n+    ParquetFileReader reader = new ParquetFileReader(inputFile, readOptions);\n+\n+    while ((pages = reader.readNextRowGroup()) != null) {\n+      long rows = pages.getRowCount();\n+      MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);\n+      RecordReader recordReader = columnIO.getRecordReader(pages, new GroupRecordConverter(schema));\n+\n+      for (int i = 0; i < rows; i++) {\n+        SimpleGroup simpleGroup = (SimpleGroup) recordReader.read();\n+        parquetWriter.write(simpleGroup);\n+      }\n+    }\n+\n+    parquetWriter.close();\n+  }\n+\n+  private void validateColumns(String inputFile, int numRecord) throws IOException {\n+    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(inputFile)).withConf(conf).build();\n+    for (int i = 0; i < numRecord; i++) {\n+      Group group = reader.read();\n+      assertTrue(group.getLong(\"DocId\", 0) < 1000);\n+      assertEquals(group.getBinary(\"Name\", 0).length(), 100);\n+      assertEquals(group.getBinary(\"Gender\", 0).length(), 100);\n+      Group subGroup = group.getGroup(\"Links\", 0);\n+      assertEquals(subGroup.getBinary(\"Backward\", 0).length(), 100);\n+      assertEquals(subGroup.getBinary(\"Forward\", 0).length(), 100);\n+    }\n+    reader.close();\n+  }\n+\n+  private void validMeta(String inputFile, String outFile) throws Exception {\n+    ParquetMetadata inMetaData = ParquetFileReader.readFooter(conf, new Path(inputFile), NO_FILTER);\n+    ParquetMetadata outMetaData = ParquetFileReader.readFooter(conf, new Path(outFile), NO_FILTER);\n+    Assert.assertEquals(inMetaData.getFileMetaData().getSchema(), outMetaData.getFileMetaData().getSchema());\n+    Assert.assertEquals(inMetaData.getFileMetaData().getKeyValueMetaData(), outMetaData.getFileMetaData().getKeyValueMetaData());\n+  }\n+\n+  private void validColumnIndex(String inputFile, String outFile) throws Exception {\n+    ParquetMetadata inMetaData = ParquetFileReader.readFooter(conf, new Path(inputFile), NO_FILTER);\n+    ParquetMetadata outMetaData = ParquetFileReader.readFooter(conf, new Path(outFile), NO_FILTER);\n+    Assert.assertEquals(inMetaData.getBlocks().size(), outMetaData.getBlocks().size());\n+    try (ParquetFileReader inReader = new ParquetFileReader(HadoopInputFile.fromPath(new Path(inputFile), conf), HadoopReadOptions.builder(conf).build());\n+         ParquetFileReader outReader = new ParquetFileReader(HadoopInputFile.fromPath(new Path(outFile), conf), HadoopReadOptions.builder(conf).build())) {\n+      for (int i = 0; i < inMetaData.getBlocks().size(); i++) {\n+        BlockMetaData inBlockMetaData = inMetaData.getBlocks().get(i);\n+        BlockMetaData outBlockMetaData = outMetaData.getBlocks().get(i);\n+        Assert.assertEquals(inBlockMetaData.getColumns().size(), outBlockMetaData.getColumns().size());\n+        for (int j = 0; j < inBlockMetaData.getColumns().size(); j++) {\n+          ColumnChunkMetaData inChunk = inBlockMetaData.getColumns().get(j);\n+          ColumnIndex inColumnIndex = inReader.readColumnIndex(inChunk);\n+          OffsetIndex inOffsetIndex = inReader.readOffsetIndex(inChunk);\n+          ColumnChunkMetaData outChunk = outBlockMetaData.getColumns().get(j);\n+          ColumnIndex outColumnIndex = outReader.readColumnIndex(outChunk);\n+          OffsetIndex outOffsetIndex = outReader.readOffsetIndex(outChunk);\n+          if (inColumnIndex != null) {\n+            Assert.assertEquals(inColumnIndex.getBoundaryOrder(), outColumnIndex.getBoundaryOrder());\n+            Assert.assertEquals(inColumnIndex.getMaxValues(), outColumnIndex.getMaxValues());\n+            Assert.assertEquals(inColumnIndex.getMinValues(), outColumnIndex.getMinValues());\n+            Assert.assertEquals(inColumnIndex.getNullCounts(), outColumnIndex.getNullCounts());\n+          }\n+          if (inOffsetIndex != null) {\n+            Assert.assertEquals(inOffsetIndex.getPageCount(), outOffsetIndex.getPageCount());\n+            for (int k = 0; k < inOffsetIndex.getPageCount(); k++) {\n+              Assert.assertEquals(inOffsetIndex.getFirstRowIndex(k), outOffsetIndex.getFirstRowIndex(k));\n+              Assert.assertEquals(inOffsetIndex.getLastRowIndex(k, inChunk.getValueCount()),\n+                outOffsetIndex.getLastRowIndex(k, outChunk.getValueCount()));\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0cf386e7b51b1a2488df7c07edab509e288cd8e"}, "originalPosition": 207}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIyNTg1OA==", "bodyText": "I think, you should check the output file instead.", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442225858", "createdAt": "2020-06-18T13:29:09Z", "author": {"login": "gszadovszky"}, "path": "parquet-cli/src/test/java/org/apache/parquet/cli/commands/TransCompressionCommandTest.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.cli.commands;\n+\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.example.data.simple.convert.GroupRecordConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.ExampleParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.ColumnIOFactory;\n+import org.apache.parquet.io.MessageColumnIO;\n+import org.apache.parquet.io.RecordReader;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TransCompressionCommandTest extends ParquetFileTest  {\n+\n+  private Configuration conf = new Configuration();\n+  private Map<String, String> extraMeta\n+    = ImmutableMap.of(\"key1\", \"value1\", \"key2\", \"value2\");\n+\n+  @Test\n+  public void testTransCompression() throws Exception {\n+    String[] codecs = {\"UNCOMPRESSED\", \"SNAPPY\", \"GZIP\", \"ZSTD\"};\n+    for (int i = 0; i < codecs.length; i++) {\n+      for (int j = 0; j <codecs.length; j++) {\n+        // Same codec for both are considered as valid test case\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_2_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, 64);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE * 100);\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testSpeed() throws Exception {\n+    String inputFile = createParquetFile(\"input\", \"GZIP\", 100000,\n+    ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+    String outputFile = createTempFile(\"output_trans\");\n+\n+    long start = System.currentTimeMillis();\n+    TransCompressionCommand command = new TransCompressionCommand(createLogger());\n+    command.setConf(new Configuration());\n+    command.input = inputFile;\n+    command.output = outputFile;\n+    command.codec = \"ZSTD\";\n+    command.run();\n+    long durationTrans = System.currentTimeMillis() - start;\n+\n+    outputFile = createTempFile(\"output_record\");\n+    start = System.currentTimeMillis();\n+    convertRecordByRecord(CompressionCodecName.valueOf(\"ZSTD\"), new Path(inputFile), new Path(outputFile));\n+    long durationRecord = System.currentTimeMillis() - start;\n+\n+    // The TransCompressionCommand is ~5 times faster than translating record by record\n+    Assert.assertTrue(durationTrans < durationRecord);\n+  }\n+\n+  private void testInternal(String srcCodec, String destCodec, ParquetProperties.WriterVersion writerVersion, int pageSize) throws Exception {\n+    int numRecord = 1000;\n+    String inputFile = createParquetFile(\"input\", srcCodec, numRecord, writerVersion, pageSize);\n+    String outputFile = createTempFile(\"output_trans\");\n+\n+    TransCompressionCommand command = new TransCompressionCommand(createLogger());\n+    command.setConf(new Configuration());\n+    command.input = inputFile;\n+    command.output = outputFile;\n+    command.codec = destCodec;\n+    command.run();\n+\n+    validateColumns(inputFile, numRecord);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0cf386e7b51b1a2488df7c07edab509e288cd8e"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIyNjk4Nw==", "bodyText": "I don't like these tests. You should test for exact values. You may either keep the generated data in memory so you can match the file content with it or you may read both the source and the target file in the same time and match the values.", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442226987", "createdAt": "2020-06-18T13:30:41Z", "author": {"login": "gszadovszky"}, "path": "parquet-cli/src/test/java/org/apache/parquet/cli/commands/TransCompressionCommandTest.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.cli.commands;\n+\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.example.data.simple.convert.GroupRecordConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.ExampleParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.ColumnIOFactory;\n+import org.apache.parquet.io.MessageColumnIO;\n+import org.apache.parquet.io.RecordReader;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TransCompressionCommandTest extends ParquetFileTest  {\n+\n+  private Configuration conf = new Configuration();\n+  private Map<String, String> extraMeta\n+    = ImmutableMap.of(\"key1\", \"value1\", \"key2\", \"value2\");\n+\n+  @Test\n+  public void testTransCompression() throws Exception {\n+    String[] codecs = {\"UNCOMPRESSED\", \"SNAPPY\", \"GZIP\", \"ZSTD\"};\n+    for (int i = 0; i < codecs.length; i++) {\n+      for (int j = 0; j <codecs.length; j++) {\n+        // Same codec for both are considered as valid test case\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_2_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, 64);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE * 100);\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testSpeed() throws Exception {\n+    String inputFile = createParquetFile(\"input\", \"GZIP\", 100000,\n+    ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+    String outputFile = createTempFile(\"output_trans\");\n+\n+    long start = System.currentTimeMillis();\n+    TransCompressionCommand command = new TransCompressionCommand(createLogger());\n+    command.setConf(new Configuration());\n+    command.input = inputFile;\n+    command.output = outputFile;\n+    command.codec = \"ZSTD\";\n+    command.run();\n+    long durationTrans = System.currentTimeMillis() - start;\n+\n+    outputFile = createTempFile(\"output_record\");\n+    start = System.currentTimeMillis();\n+    convertRecordByRecord(CompressionCodecName.valueOf(\"ZSTD\"), new Path(inputFile), new Path(outputFile));\n+    long durationRecord = System.currentTimeMillis() - start;\n+\n+    // The TransCompressionCommand is ~5 times faster than translating record by record\n+    Assert.assertTrue(durationTrans < durationRecord);\n+  }\n+\n+  private void testInternal(String srcCodec, String destCodec, ParquetProperties.WriterVersion writerVersion, int pageSize) throws Exception {\n+    int numRecord = 1000;\n+    String inputFile = createParquetFile(\"input\", srcCodec, numRecord, writerVersion, pageSize);\n+    String outputFile = createTempFile(\"output_trans\");\n+\n+    TransCompressionCommand command = new TransCompressionCommand(createLogger());\n+    command.setConf(new Configuration());\n+    command.input = inputFile;\n+    command.output = outputFile;\n+    command.codec = destCodec;\n+    command.run();\n+\n+    validateColumns(inputFile, numRecord);\n+    validMeta(inputFile, outputFile);\n+    validColumnIndex(inputFile, outputFile);\n+  }\n+\n+  private void convertRecordByRecord(CompressionCodecName codecName, Path inpath, Path outpath) throws Exception {\n+    ParquetMetadata metaData = ParquetFileReader.readFooter(conf, inpath, NO_FILTER);\n+    MessageType schema = metaData.getFileMetaData().getSchema();\n+    HadoopInputFile inputFile = HadoopInputFile.fromPath(inpath, conf);\n+    ParquetReadOptions readOptions = HadoopReadOptions.builder(conf).build();\n+\n+    conf.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString());\n+    ExampleParquetWriter.Builder builder = ExampleParquetWriter.builder(outpath).withConf(conf).withCompressionCodec(codecName);\n+\n+    ParquetWriter parquetWriter = builder.build();\n+\n+    PageReadStore pages;\n+    ParquetFileReader reader = new ParquetFileReader(inputFile, readOptions);\n+\n+    while ((pages = reader.readNextRowGroup()) != null) {\n+      long rows = pages.getRowCount();\n+      MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);\n+      RecordReader recordReader = columnIO.getRecordReader(pages, new GroupRecordConverter(schema));\n+\n+      for (int i = 0; i < rows; i++) {\n+        SimpleGroup simpleGroup = (SimpleGroup) recordReader.read();\n+        parquetWriter.write(simpleGroup);\n+      }\n+    }\n+\n+    parquetWriter.close();\n+  }\n+\n+  private void validateColumns(String inputFile, int numRecord) throws IOException {\n+    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(inputFile)).withConf(conf).build();\n+    for (int i = 0; i < numRecord; i++) {\n+      Group group = reader.read();\n+      assertTrue(group.getLong(\"DocId\", 0) < 1000);\n+      assertEquals(group.getBinary(\"Name\", 0).length(), 100);\n+      assertEquals(group.getBinary(\"Gender\", 0).length(), 100);\n+      Group subGroup = group.getGroup(\"Links\", 0);\n+      assertEquals(subGroup.getBinary(\"Backward\", 0).length(), 100);\n+      assertEquals(subGroup.getBinary(\"Forward\", 0).length(), 100);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0cf386e7b51b1a2488df7c07edab509e288cd8e"}, "originalPosition": 166}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "64827a3ee339614b432c7424c13c03fbc8e91413", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/64827a3ee339614b432c7424c13c03fbc8e91413", "committedDate": "2020-06-20T06:18:19Z", "message": "Address feedback"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b695d4b7936e8694d638eee5fe7714282bc8ba82", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/b695d4b7936e8694d638eee5fe7714282bc8ba82", "committedDate": "2020-06-20T06:17:30Z", "message": "Fix a leftover"}, "afterCommit": {"oid": "64827a3ee339614b432c7424c13c03fbc8e91413", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/64827a3ee339614b432c7424c13c03fbc8e91413", "committedDate": "2020-06-20T06:18:19Z", "message": "Address feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0Nzg4OTkx", "url": "https://github.com/apache/parquet-mr/pull/796#pullrequestreview-434788991", "createdAt": "2020-06-22T10:16:26Z", "commit": {"oid": "64827a3ee339614b432c7424c13c03fbc8e91413"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxMDoxNjoyNlrOGm6hXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxMDoxNjoyNlrOGm6hXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzQ1Nzg4NA==", "bodyText": "Good catch!", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r443457884", "createdAt": "2020-06-22T10:16:26Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/H1SeekableInputStream.java", "diffHunk": "@@ -53,7 +53,7 @@ public void readFully(byte[] bytes) throws IOException {\n \n   @Override\n   public void readFully(byte[] bytes, int start, int len) throws IOException {\n-    stream.readFully(bytes);\n+    stream.readFully(bytes, start, len);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64827a3ee339614b432c7424c13c03fbc8e91413"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0ODc0MDcw", "url": "https://github.com/apache/parquet-mr/pull/796#pullrequestreview-434874070", "createdAt": "2020-06-22T12:28:20Z", "commit": {"oid": "64827a3ee339614b432c7424c13c03fbc8e91413"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxMjoyODoyMFrOGm-cug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQxMjoyODoyMFrOGm-cug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzUyMjIzNA==", "bodyText": "Instead of creating a Random instance or retrieving a ThreadLocalRandom instance for each generated values I would suggest creating one Random instance in TestDocs in the constructor and use it for all the values to be generated in that instance. Also suggest to use a hard-coded seed for Random to ensure the test is deterministic.", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r443522234", "createdAt": "2020-06-22T12:28:20Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/util/CompressionConveterTest.java", "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop.util;\n+\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.example.data.simple.convert.GroupRecordConverter;\n+import org.apache.parquet.format.DataPageHeader;\n+import org.apache.parquet.format.DataPageHeaderV2;\n+import org.apache.parquet.format.DictionaryPageHeader;\n+import org.apache.parquet.format.PageHeader;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetFileWriter;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.ExampleParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.CompressionConverter.TransParquetFileReader;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.ColumnIOFactory;\n+import org.apache.parquet.io.MessageColumnIO;\n+import org.apache.parquet.io.RecordReader;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class CompressionConveterTest {\n+\n+  private Configuration conf = new Configuration();\n+  private Map<String, String> extraMeta\n+    = ImmutableMap.of(\"key1\", \"value1\", \"key2\", \"value2\");\n+  private CompressionConverter compressionConverter = new CompressionConverter();\n+\n+  @Test\n+  public void testTransCompression() throws Exception {\n+    String[] codecs = {\"UNCOMPRESSED\", \"SNAPPY\", \"GZIP\", \"ZSTD\"};\n+    for (int i = 0; i < codecs.length; i++) {\n+      for (int j = 0; j <codecs.length; j++) {\n+        // Same codec for both are considered as valid test case\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_2_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, 64);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE * 100);\n+      }\n+    }\n+  }\n+\n+  private void testInternal(String srcCodec, String destCodec, ParquetProperties.WriterVersion writerVersion, int pageSize) throws Exception {\n+    int numRecord = 1000;\n+    TestDocs testDocs = new TestDocs(numRecord);\n+    String inputFile = createParquetFile(conf, extraMeta, numRecord, \"input\", srcCodec, writerVersion, pageSize, testDocs);\n+    String outputFile = createTempFile(\"output_trans\");\n+\n+    convertCompression(conf, inputFile, outputFile, destCodec);\n+\n+    validateColumns(outputFile, numRecord, testDocs);\n+    validMeta(inputFile, outputFile);\n+    validColumnIndex(inputFile, outputFile);\n+  }\n+\n+  private void convertCompression(Configuration conf, String inputFile, String outputFile, String codec) throws IOException {\n+    Path inPath = new Path(inputFile);\n+    Path outPath = new Path(outputFile);\n+    CompressionCodecName codecName = CompressionCodecName.valueOf(codec);\n+\n+    ParquetMetadata metaData = ParquetFileReader.readFooter(conf, inPath, NO_FILTER);\n+    MessageType schema = metaData.getFileMetaData().getSchema();\n+    ParquetFileWriter writer = new ParquetFileWriter(conf, schema, outPath, ParquetFileWriter.Mode.CREATE);\n+    writer.start();\n+\n+    try (TransParquetFileReader reader = new TransParquetFileReader(HadoopInputFile.fromPath(inPath, conf), HadoopReadOptions.builder(conf).build())) {\n+      compressionConverter.processBlocks(reader, writer, metaData, schema, metaData.getFileMetaData().getCreatedBy(), codecName);\n+    } finally {\n+      writer.end(metaData.getFileMetaData().getKeyValueMetaData());\n+    }\n+  }\n+\n+  private void validateColumns(String file, int numRecord, TestDocs testDocs) throws IOException {\n+    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(file)).withConf(conf).build();\n+    for (int i = 0; i < numRecord; i++) {\n+      Group group = reader.read();\n+      assertTrue(group.getLong(\"DocId\", 0) == testDocs.docId[i]);\n+      assertArrayEquals(group.getBinary(\"Name\", 0).getBytes(), testDocs.name[i].getBytes());\n+      assertArrayEquals(group.getBinary(\"Gender\", 0).getBytes(), testDocs.gender[i].getBytes());\n+      Group subGroup = group.getGroup(\"Links\", 0);\n+      assertArrayEquals(subGroup.getBinary(\"Backward\", 0).getBytes(), testDocs.linkBackward[i].getBytes());\n+      assertArrayEquals(subGroup.getBinary(\"Forward\", 0).getBytes(), testDocs.linkForward[i].getBytes());\n+    }\n+    reader.close();\n+  }\n+\n+  private void validMeta(String inputFile, String outFile) throws Exception {\n+    ParquetMetadata inMetaData = ParquetFileReader.readFooter(conf, new Path(inputFile), NO_FILTER);\n+    ParquetMetadata outMetaData = ParquetFileReader.readFooter(conf, new Path(outFile), NO_FILTER);\n+    Assert.assertEquals(inMetaData.getFileMetaData().getSchema(), outMetaData.getFileMetaData().getSchema());\n+    Assert.assertEquals(inMetaData.getFileMetaData().getKeyValueMetaData(), outMetaData.getFileMetaData().getKeyValueMetaData());\n+  }\n+\n+  private void validColumnIndex(String inputFile, String outFile) throws Exception {\n+    ParquetMetadata inMetaData = ParquetFileReader.readFooter(conf, new Path(inputFile), NO_FILTER);\n+    ParquetMetadata outMetaData = ParquetFileReader.readFooter(conf, new Path(outFile), NO_FILTER);\n+    Assert.assertEquals(inMetaData.getBlocks().size(), outMetaData.getBlocks().size());\n+    try (TransParquetFileReader inReader = new TransParquetFileReader(HadoopInputFile.fromPath(new Path(inputFile), conf), HadoopReadOptions.builder(conf).build());\n+         TransParquetFileReader outReader = new TransParquetFileReader(HadoopInputFile.fromPath(new Path(outFile), conf), HadoopReadOptions.builder(conf).build())) {\n+      for (int i = 0; i < inMetaData.getBlocks().size(); i++) {\n+        BlockMetaData inBlockMetaData = inMetaData.getBlocks().get(i);\n+        BlockMetaData outBlockMetaData = outMetaData.getBlocks().get(i);\n+        Assert.assertEquals(inBlockMetaData.getColumns().size(), outBlockMetaData.getColumns().size());\n+        for (int j = 0; j < inBlockMetaData.getColumns().size(); j++) {\n+          ColumnChunkMetaData inChunk = inBlockMetaData.getColumns().get(j);\n+          ColumnIndex inColumnIndex = inReader.readColumnIndex(inChunk);\n+          OffsetIndex inOffsetIndex = inReader.readOffsetIndex(inChunk);\n+          ColumnChunkMetaData outChunk = outBlockMetaData.getColumns().get(j);\n+          ColumnIndex outColumnIndex = outReader.readColumnIndex(outChunk);\n+          OffsetIndex outOffsetIndex = outReader.readOffsetIndex(outChunk);\n+          if (inColumnIndex != null) {\n+            Assert.assertEquals(inColumnIndex.getBoundaryOrder(), outColumnIndex.getBoundaryOrder());\n+            Assert.assertEquals(inColumnIndex.getMaxValues(), outColumnIndex.getMaxValues());\n+            Assert.assertEquals(inColumnIndex.getMinValues(), outColumnIndex.getMinValues());\n+            Assert.assertEquals(inColumnIndex.getNullCounts(), outColumnIndex.getNullCounts());\n+          }\n+          if (inOffsetIndex != null) {\n+            List<Long> inOffsets = getOffsets(inReader, inChunk);\n+            List<Long> outOffsets = getOffsets(outReader, outChunk);\n+            Assert.assertEquals(inOffsets.size(), outOffsets.size());\n+            Assert.assertEquals(inOffsets.size(), inOffsetIndex.getPageCount());\n+            Assert.assertEquals(inOffsetIndex.getPageCount(), outOffsetIndex.getPageCount());\n+            for (int k = 0; k < inOffsetIndex.getPageCount(); k++) {\n+              Assert.assertEquals(inOffsetIndex.getFirstRowIndex(k), outOffsetIndex.getFirstRowIndex(k));\n+              Assert.assertEquals(inOffsetIndex.getLastRowIndex(k, inChunk.getValueCount()),\n+                outOffsetIndex.getLastRowIndex(k, outChunk.getValueCount()));\n+              Assert.assertEquals(inOffsetIndex.getOffset(k), (long)inOffsets.get(k));\n+              Assert.assertEquals(outOffsetIndex.getOffset(k), (long)outOffsets.get(k));\n+            }\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private List<Long> getOffsets(TransParquetFileReader reader, ColumnChunkMetaData chunk) throws IOException {\n+    List<Long> offsets = new ArrayList<>();\n+    reader.setStreamPosition(chunk.getStartingPos());\n+    long readValues = 0;\n+    long totalChunkValues = chunk.getValueCount();\n+    while (readValues < totalChunkValues) {\n+      long curOffset = reader.getPos();\n+      PageHeader pageHeader = reader.readPageHeader();\n+      switch (pageHeader.type) {\n+        case DICTIONARY_PAGE:\n+          compressionConverter.readBlock(pageHeader.getCompressed_page_size(), reader);\n+          break;\n+        case DATA_PAGE:\n+          DataPageHeader headerV1 = pageHeader.data_page_header;\n+          offsets.add(curOffset);\n+          compressionConverter.readBlock(pageHeader.getCompressed_page_size(), reader);\n+          readValues += headerV1.getNum_values();\n+          break;\n+        case DATA_PAGE_V2:\n+          DataPageHeaderV2 headerV2 = pageHeader.data_page_header_v2;\n+          offsets.add(curOffset);\n+          int rlLength = headerV2.getRepetition_levels_byte_length();\n+          compressionConverter.readBlock(rlLength, reader);\n+          int dlLength = headerV2.getDefinition_levels_byte_length();\n+          compressionConverter.readBlock(dlLength, reader);\n+          int payLoadLength = pageHeader.getCompressed_page_size() - rlLength - dlLength;\n+          compressionConverter.readBlock(payLoadLength, reader);\n+          readValues += headerV2.getNum_values();\n+          break;\n+        default:\n+          throw new IOException(\"Not recognized page type\");\n+      }\n+    }\n+    return offsets;\n+  }\n+\n+  private String createParquetFile(Configuration conf, Map<String, String> extraMeta, int numRecord, String prefix, String codec,\n+                                         ParquetProperties.WriterVersion writerVersion, int pageSize, TestDocs testDocs) throws IOException {\n+    MessageType schema = new MessageType(\"schema\",\n+      new PrimitiveType(REQUIRED, INT64, \"DocId\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Name\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Gender\"),\n+      new GroupType(OPTIONAL, \"Links\",\n+        new PrimitiveType(REPEATED, BINARY, \"Backward\"),\n+        new PrimitiveType(REPEATED, BINARY, \"Forward\")));\n+\n+    conf.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString());\n+\n+    String file = createTempFile(prefix);\n+    ExampleParquetWriter.Builder builder = ExampleParquetWriter.builder(new Path(file))\n+      .withConf(conf)\n+      .withWriterVersion(writerVersion)\n+      .withExtraMetaData(extraMeta)\n+      .withDictionaryEncoding(\"DocId\", true)\n+      .withValidation(true)\n+      .enablePageWriteChecksum()\n+      .withPageSize(pageSize)\n+      .withCompressionCodec(CompressionCodecName.valueOf(codec));\n+    try (ParquetWriter writer = builder.build()) {\n+      for (int i = 0; i < numRecord; i++) {\n+        SimpleGroup g = new SimpleGroup(schema);\n+        g.add(\"DocId\", testDocs.docId[i]);\n+        g.add(\"Name\", testDocs.name[i]);\n+        g.add(\"Gender\", testDocs.gender[i]);\n+        Group links = g.addGroup(\"Links\");\n+        links.add(0, testDocs.linkBackward[i]);\n+        links.add(1, testDocs.linkForward[i]);\n+        writer.write(g);\n+      }\n+    }\n+\n+    return file;\n+  }\n+\n+  private static long getLong() {\n+    return ThreadLocalRandom.current().nextLong(1000);\n+  }\n+\n+  private String getString() {\n+    char[] chars = {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'x', 'z', 'y'};\n+    StringBuilder sb = new StringBuilder();\n+    for (int i = 0; i < 100; i++) {\n+      sb.append(chars[new Random().nextInt(10)]);\n+    }\n+    return sb.toString();\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64827a3ee339614b432c7424c13c03fbc8e91413"}, "originalPosition": 274}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "99d1a08975b83d0df93c730edc2d7b007535fd70", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/99d1a08975b83d0df93c730edc2d7b007535fd70", "committedDate": "2020-06-22T23:19:28Z", "message": "Address more feedbacks"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1NjE0MDM4", "url": "https://github.com/apache/parquet-mr/pull/796#pullrequestreview-435614038", "createdAt": "2020-06-23T09:39:22Z", "commit": {"oid": "99d1a08975b83d0df93c730edc2d7b007535fd70"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2209, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}