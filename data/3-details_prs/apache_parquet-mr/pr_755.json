{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcyODM2Njky", "number": 755, "title": "PARQUET-1791: Add 'prune' command to parquet-tools", "bodyText": "Make sure you have checked all steps below.\nJira\n\n My PR addresses the following Parquet Jira issues and references them in the PR title. For example, \"PARQUET-1234: My Parquet PR\"\n\nhttps://issues.apache.org/jira/browse/PARQUET-XXX\nIn case you are adding a dependency, check if the license complies with the ASF 3rd Party License Policy.\n\n\n\nTests\n\n My PR adds the following unit tests OR does not need testing for this extremely good reason:\n\nCommits\n\n My commits all reference Jira issues in their subject lines. In addition, my commits follow the guidelines from \"How to write a good git commit message\":\n\nSubject is separated from body by a blank line\nSubject is limited to 50 characters (not including Jira issue reference)\nSubject does not end with a period\nSubject uses the imperative mood (\"add\", not \"adding\")\nBody wraps at 72 characters\nBody explains \"what\" and \"why\", not \"how\"\n\n\n\nDocumentation\n\n In case of new functionality, my PR adds documentation that describes how to use it.\n\nAll the public functions and the classes in the PR contain Javadoc that explain what it does", "createdAt": "2020-02-09T14:51:24Z", "url": "https://github.com/apache/parquet-mr/pull/755", "merged": true, "mergeCommit": {"oid": "a0c9e696d424d4289653cba5d6b1e70b25ae96be"}, "closed": true, "closedAt": "2020-02-25T09:53:14Z", "author": {"login": "shangxinli"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcC498AAFqTM1NTc0ODE1OQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcHZb7YgFqTM2MzIyNzIyMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU1NzQ4MTU5", "url": "https://github.com/apache/parquet-mr/pull/755#pullrequestreview-355748159", "createdAt": "2020-02-10T08:15:55Z", "commit": {"oid": "964f41d2c94554623a0927dd5bc5c459d4edffed"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQwODoxNTo1NVrOFndBpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMFQwODoxNTo1NVrOFndBpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjkxNDM0Mw==", "bodyText": "Please, remove this line.", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r376914343", "createdAt": "2020-02-10T08:15:55Z", "author": {"login": "gszadovszky"}, "path": "parquet-tools/src/main/java/org/apache/parquet/tools/command/PruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetFileWriter;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.hadoop.metadata.FileMetaData;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.Type;\n+import org.apache.parquet.tools.Main;\n+\n+import java.io.PrintWriter;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.HashSet;\n+\n+public class PruneColumnsCommand extends ArgsOnlyCommand {\n+\n+  public static final String[] USAGE = new String[] {\n+    \"<input> <output> [<column> ...]\",\n+\n+    \"where <input> is the source parquet file\",\n+    \"    <output> is the destination parquet file,\" +\n+    \"    [<column> ...] are the columns in the case senstive dot format\" +\n+      \" to be pruned, for example a.b.c\"\n+  };\n+\n+  /**\n+   * Biggest number of columns we can prune.\n+   */\n+  private static final int MAX_COL_NUM = 100;\n+\n+  private Configuration conf;\n+\n+  public PruneColumnsCommand() {\n+    super(3, MAX_COL_NUM + 1);\n+\n+    conf = new Configuration();\n+  }\n+\n+  @Override\n+  public String[] getUsageDescription() {\n+    return USAGE;\n+  }\n+\n+  @Override\n+  public String getCommandDescription() {\n+    return \"Prune column(s) in a Parquet file and save it to a new file. \" +\n+      \"The columns left are not changed.\";\n+  }\n+\n+  @Override\n+  public void execute(CommandLine options) throws Exception {\n+    List<String> args = options.getArgList();\n+    Path inputFile = new Path(args.get(0));\n+    Path outputFile = new Path(args.get(1));\n+    List<String> cols = args.subList(2, args.size());\n+\n+    Set<ColumnPath> prunePaths = convertToColumnPaths(cols);\n+\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, inputFile, ParquetMetadataConverter.NO_FILTER);\n+    FileMetaData metaData = pmd.getFileMetaData();\n+\n+    ParquetFileWriter writer = new ParquetFileWriter(conf,\n+      pruneColumnsInSchema(metaData.getSchema(), prunePaths), outputFile, ParquetFileWriter.Mode.CREATE);\n+\n+    writer.start();\n+    writer.appendFile(HadoopInputFile.fromPath(inputFile, conf));\n+    writer.end(metaData.getKeyValueMetaData());\n+  }\n+\n+  private MessageType pruneColumnsInSchema(MessageType schema, Set<ColumnPath> prunePaths) {\n+\n+    List<Type> fields = schema.getFields();\n+    List<String> currentPath = new ArrayList<>();\n+    List<Type> prunedFields = pruneColumnsInFields(fields, currentPath, prunePaths);\n+    MessageType newSchema = new MessageType(schema.getName(), prunedFields);\n+    return newSchema;\n+\n+    //return schema;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "964f41d2c94554623a0927dd5bc5c459d4edffed"}, "originalPosition": 106}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b6ee2c1ea36ef18bf2b26e0817258bfa52faec04", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/b6ee2c1ea36ef18bf2b26e0817258bfa52faec04", "committedDate": "2020-02-17T06:22:14Z", "message": "Add unit tests"}, "afterCommit": {"oid": "6bef646243f867c1b208b92e2997917366716d69", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/6bef646243f867c1b208b92e2997917366716d69", "committedDate": "2020-02-17T06:24:46Z", "message": "PARQUET-1791: Add 'prune' command to parquet-tools"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU5NzM0MjM2", "url": "https://github.com/apache/parquet-mr/pull/755#pullrequestreview-359734236", "createdAt": "2020-02-17T13:30:04Z", "commit": {"oid": "6bef646243f867c1b208b92e2997917366716d69"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QxMzozMDowNVrOFqkdsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QxMzozMjo1OFrOFqkjjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDE4MTkzOA==", "bodyText": "Checking in binary parquet files is not really future proof. It is usually a better idea to create the input file in the test and remove it at the end. (There are many examples in the tests to do so.)", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r380181938", "createdAt": "2020-02-17T13:30:05Z", "author": {"login": "gszadovszky"}, "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bef646243f867c1b208b92e2997917366716d69"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDE4MjU4MA==", "bodyText": "This path depends on the path where the maven command is executed from. Maybe, that's why it is failing on Travis.", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r380182580", "createdAt": "2020-02-17T13:31:18Z", "author": {"login": "gszadovszky"}, "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {\n+\n+  private Configuration conf = new Configuration();\n+  private PruneColumnsCommand command = new PruneColumnsCommand();\n+  private Random rand = new Random();\n+\n+  @Test\n+  public void testPruneOneColumn() throws Exception {\n+    // File test.parquet has 3 columns: \"name\", \"age\", \"gender\"\n+    String inputFile = \"src/test/java/org/apache/parquet/tools/data/test.parquet\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bef646243f867c1b208b92e2997917366716d69"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDE4MzQzOA==", "bodyText": "It would also be great to verify the actual data. Maybe, you may use the dump command?", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r380183438", "createdAt": "2020-02-17T13:32:58Z", "author": {"login": "gszadovszky"}, "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {\n+\n+  private Configuration conf = new Configuration();\n+  private PruneColumnsCommand command = new PruneColumnsCommand();\n+  private Random rand = new Random();\n+\n+  @Test\n+  public void testPruneOneColumn() throws Exception {\n+    // File test.parquet has 3 columns: \"name\", \"age\", \"gender\"\n+    String inputFile = \"src/test/java/org/apache/parquet/tools/data/test.parquet\";\n+    String outputFile =  rand.nextInt() + \".parquet\";\n+    String cargs[] = {inputFile, outputFile, \"gender\"};\n+    executeCommandLine(cargs);\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 2);\n+    assertEquals(fields.get(0).getName(), \"name\");\n+    assertEquals(fields.get(1).getName(), \"age\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6bef646243f867c1b208b92e2997917366716d69"}, "originalPosition": 58}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e4f368c086ba739235a738c602586ac8826b6c4c", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/e4f368c086ba739235a738c602586ac8826b6c4c", "committedDate": "2020-02-18T03:48:08Z", "message": "PARQUET-1791: Add 'prune' command to parquet-tools"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8fc7a46c30460bd8aac2c262ddfa2db1fa5836cd", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/8fc7a46c30460bd8aac2c262ddfa2db1fa5836cd", "committedDate": "2020-02-18T03:25:22Z", "message": "Fix unit tests"}, "afterCommit": {"oid": "e4f368c086ba739235a738c602586ac8826b6c4c", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/e4f368c086ba739235a738c602586ac8826b6c4c", "committedDate": "2020-02-18T03:48:08Z", "message": "PARQUET-1791: Add 'prune' command to parquet-tools"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwOTg5Mzcw", "url": "https://github.com/apache/parquet-mr/pull/755#pullrequestreview-360989370", "createdAt": "2020-02-19T10:15:21Z", "commit": {"oid": "e4f368c086ba739235a738c602586ac8826b6c4c"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQxMDoxNToyMVrOFribZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOVQxMDozMDozNlrOFri8SA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTE5NzE1Nw==", "bodyText": "I would prefer using creating the file in a temporary space instead. (See e.g. java.nio.file.Files.createTempFile)\nCreating a file int the hard-coded \"target/\" relative path is a bit error prone. What happens if the mvn command is executed from outside of the project structure?", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r381197157", "createdAt": "2020-02-19T10:15:21Z", "author": {"login": "gszadovszky"}, "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {\n+\n+  private static final String root = \"target/\";\n+  private final int numRecord = 1000;\n+  private PruneColumnsCommand command = new PruneColumnsCommand();\n+  private Configuration conf = new Configuration();\n+  private Random rand = new Random();\n+\n+  @Test\n+  public void testPruneOneColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove column\n+    String cargs[] = {inputFile, outputFile, \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Links\");\n+    List<Type> subFields = fields.get(2).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneMultiColumns() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove columns\n+    String cargs[] = {inputFile, outputFile, \"Name\", \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 2);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Links\");\n+    List<Type> subFields = fields.get(1).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Name\", \"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"no_exist\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  @Test\n+  public void testPruneNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove nested column\n+    String cargs[] = {inputFile, outputFile, \"Links.Backward\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 4);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+    assertEquals(fields.get(3).getName(), \"Links\");\n+    List<Type> subFields = fields.get(3).asGroupType().getFields();\n+    assertEquals(subFields.size(), 1);\n+    assertEquals(subFields.get(0).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links.Backward\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneNestedParentColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove parent column. All of it's children will be removed.\n+    String cargs[] = {inputFile, outputFile, \"Links\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"Links.Not_exists\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  private void executeCommandLine(String[] cargs) throws Exception {\n+    CommandLineParser parser = new PosixParser();\n+    CommandLine cmd = parser.parse(new Options(), cargs, command.supportsExtraArgs());\n+    command.execute(cmd);\n+  }\n+\n+  private void validateColumns(String inputFile, List<String> prunePaths) throws IOException {\n+    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(inputFile)).withConf(conf).build();\n+    for (int i = 0; i < numRecord; i++) {\n+      Group group = reader.read();\n+      if (!prunePaths.contains(\"DocId\")) {\n+        assertEquals(1l, group.getLong(\"DocId\", 0));\n+      }\n+      if (!prunePaths.contains(\"Name\")) {\n+        assertEquals(\"foo\", group.getBinary(\"Name\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Gender\")) {\n+        assertEquals(\"male\", group.getBinary(\"Gender\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Links\")) {\n+        Group subGroup = group.getGroup(\"Links\", 0);\n+        if (!prunePaths.contains(\"Links.Backward\")) {\n+          assertEquals(2l, subGroup.getLong(\"Backward\", 0));\n+        }\n+        if (!prunePaths.contains(\"Links.Forward\")) {\n+          assertEquals(3l, subGroup.getLong(\"Forward\", 0));\n+        }\n+      }\n+    }\n+    reader.close();\n+  }\n+\n+  private String createParquetFile() throws IOException {\n+    MessageType schema = new MessageType(\"schema\",\n+      new PrimitiveType(REQUIRED, INT64, \"DocId\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Name\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Gender\"),\n+      new GroupType(OPTIONAL, \"Links\",\n+        new PrimitiveType(REPEATED, INT64, \"Backward\"),\n+        new PrimitiveType(REPEATED, INT64, \"Forward\")));\n+\n+    conf.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString());\n+\n+    Path file = new Path(root, + rand.nextInt(100000) + \"input.parquet\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4f368c086ba739235a738c602586ac8826b6c4c"}, "originalPosition": 232}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTE5ODM4MA==", "bodyText": "nit: Using the builders in org.apache.parquet.schema.Types would result a nicer and more readable code.", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r381198380", "createdAt": "2020-02-19T10:17:38Z", "author": {"login": "gszadovszky"}, "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {\n+\n+  private static final String root = \"target/\";\n+  private final int numRecord = 1000;\n+  private PruneColumnsCommand command = new PruneColumnsCommand();\n+  private Configuration conf = new Configuration();\n+  private Random rand = new Random();\n+\n+  @Test\n+  public void testPruneOneColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove column\n+    String cargs[] = {inputFile, outputFile, \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Links\");\n+    List<Type> subFields = fields.get(2).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneMultiColumns() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove columns\n+    String cargs[] = {inputFile, outputFile, \"Name\", \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 2);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Links\");\n+    List<Type> subFields = fields.get(1).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Name\", \"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"no_exist\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  @Test\n+  public void testPruneNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove nested column\n+    String cargs[] = {inputFile, outputFile, \"Links.Backward\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 4);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+    assertEquals(fields.get(3).getName(), \"Links\");\n+    List<Type> subFields = fields.get(3).asGroupType().getFields();\n+    assertEquals(subFields.size(), 1);\n+    assertEquals(subFields.get(0).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links.Backward\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneNestedParentColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove parent column. All of it's children will be removed.\n+    String cargs[] = {inputFile, outputFile, \"Links\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"Links.Not_exists\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  private void executeCommandLine(String[] cargs) throws Exception {\n+    CommandLineParser parser = new PosixParser();\n+    CommandLine cmd = parser.parse(new Options(), cargs, command.supportsExtraArgs());\n+    command.execute(cmd);\n+  }\n+\n+  private void validateColumns(String inputFile, List<String> prunePaths) throws IOException {\n+    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(inputFile)).withConf(conf).build();\n+    for (int i = 0; i < numRecord; i++) {\n+      Group group = reader.read();\n+      if (!prunePaths.contains(\"DocId\")) {\n+        assertEquals(1l, group.getLong(\"DocId\", 0));\n+      }\n+      if (!prunePaths.contains(\"Name\")) {\n+        assertEquals(\"foo\", group.getBinary(\"Name\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Gender\")) {\n+        assertEquals(\"male\", group.getBinary(\"Gender\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Links\")) {\n+        Group subGroup = group.getGroup(\"Links\", 0);\n+        if (!prunePaths.contains(\"Links.Backward\")) {\n+          assertEquals(2l, subGroup.getLong(\"Backward\", 0));\n+        }\n+        if (!prunePaths.contains(\"Links.Forward\")) {\n+          assertEquals(3l, subGroup.getLong(\"Forward\", 0));\n+        }\n+      }\n+    }\n+    reader.close();\n+  }\n+\n+  private String createParquetFile() throws IOException {\n+    MessageType schema = new MessageType(\"schema\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4f368c086ba739235a738c602586ac8826b6c4c"}, "originalPosition": 222}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTIwMDI5OQ==", "bodyText": "nit: Using org.apache.parquet.hadoop.example.ExampleParquetWriter.builder makes creating the writer easier and more readable.", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r381200299", "createdAt": "2020-02-19T10:20:59Z", "author": {"login": "gszadovszky"}, "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {\n+\n+  private static final String root = \"target/\";\n+  private final int numRecord = 1000;\n+  private PruneColumnsCommand command = new PruneColumnsCommand();\n+  private Configuration conf = new Configuration();\n+  private Random rand = new Random();\n+\n+  @Test\n+  public void testPruneOneColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove column\n+    String cargs[] = {inputFile, outputFile, \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Links\");\n+    List<Type> subFields = fields.get(2).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneMultiColumns() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove columns\n+    String cargs[] = {inputFile, outputFile, \"Name\", \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 2);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Links\");\n+    List<Type> subFields = fields.get(1).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Name\", \"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"no_exist\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  @Test\n+  public void testPruneNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove nested column\n+    String cargs[] = {inputFile, outputFile, \"Links.Backward\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 4);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+    assertEquals(fields.get(3).getName(), \"Links\");\n+    List<Type> subFields = fields.get(3).asGroupType().getFields();\n+    assertEquals(subFields.size(), 1);\n+    assertEquals(subFields.get(0).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links.Backward\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneNestedParentColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove parent column. All of it's children will be removed.\n+    String cargs[] = {inputFile, outputFile, \"Links\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"Links.Not_exists\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  private void executeCommandLine(String[] cargs) throws Exception {\n+    CommandLineParser parser = new PosixParser();\n+    CommandLine cmd = parser.parse(new Options(), cargs, command.supportsExtraArgs());\n+    command.execute(cmd);\n+  }\n+\n+  private void validateColumns(String inputFile, List<String> prunePaths) throws IOException {\n+    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(inputFile)).withConf(conf).build();\n+    for (int i = 0; i < numRecord; i++) {\n+      Group group = reader.read();\n+      if (!prunePaths.contains(\"DocId\")) {\n+        assertEquals(1l, group.getLong(\"DocId\", 0));\n+      }\n+      if (!prunePaths.contains(\"Name\")) {\n+        assertEquals(\"foo\", group.getBinary(\"Name\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Gender\")) {\n+        assertEquals(\"male\", group.getBinary(\"Gender\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Links\")) {\n+        Group subGroup = group.getGroup(\"Links\", 0);\n+        if (!prunePaths.contains(\"Links.Backward\")) {\n+          assertEquals(2l, subGroup.getLong(\"Backward\", 0));\n+        }\n+        if (!prunePaths.contains(\"Links.Forward\")) {\n+          assertEquals(3l, subGroup.getLong(\"Forward\", 0));\n+        }\n+      }\n+    }\n+    reader.close();\n+  }\n+\n+  private String createParquetFile() throws IOException {\n+    MessageType schema = new MessageType(\"schema\",\n+      new PrimitiveType(REQUIRED, INT64, \"DocId\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Name\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Gender\"),\n+      new GroupType(OPTIONAL, \"Links\",\n+        new PrimitiveType(REPEATED, INT64, \"Backward\"),\n+        new PrimitiveType(REPEATED, INT64, \"Forward\")));\n+\n+    conf.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString());\n+\n+    Path file = new Path(root, + rand.nextInt(100000) + \"input.parquet\");\n+\n+    ParquetWriter<Group> writer = new ParquetWriter<>(file, new GroupWriteSupport(), ParquetWriter.DEFAULT_COMPRESSION_CODEC_NAME,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4f368c086ba739235a738c602586ac8826b6c4c"}, "originalPosition": 234}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTIwMDk1OQ==", "bodyText": "I would prefer using a try-with-resources construct instead.", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r381200959", "createdAt": "2020-02-19T10:22:11Z", "author": {"login": "gszadovszky"}, "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {\n+\n+  private static final String root = \"target/\";\n+  private final int numRecord = 1000;\n+  private PruneColumnsCommand command = new PruneColumnsCommand();\n+  private Configuration conf = new Configuration();\n+  private Random rand = new Random();\n+\n+  @Test\n+  public void testPruneOneColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove column\n+    String cargs[] = {inputFile, outputFile, \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Links\");\n+    List<Type> subFields = fields.get(2).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneMultiColumns() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove columns\n+    String cargs[] = {inputFile, outputFile, \"Name\", \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 2);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Links\");\n+    List<Type> subFields = fields.get(1).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Name\", \"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"no_exist\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  @Test\n+  public void testPruneNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove nested column\n+    String cargs[] = {inputFile, outputFile, \"Links.Backward\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 4);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+    assertEquals(fields.get(3).getName(), \"Links\");\n+    List<Type> subFields = fields.get(3).asGroupType().getFields();\n+    assertEquals(subFields.size(), 1);\n+    assertEquals(subFields.get(0).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links.Backward\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneNestedParentColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove parent column. All of it's children will be removed.\n+    String cargs[] = {inputFile, outputFile, \"Links\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Gender\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Links\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsNestedColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+    String cargs[] = {inputFile, outputFile, \"Links.Not_exists\"};\n+    executeCommandLine(cargs);\n+  }\n+\n+  private void executeCommandLine(String[] cargs) throws Exception {\n+    CommandLineParser parser = new PosixParser();\n+    CommandLine cmd = parser.parse(new Options(), cargs, command.supportsExtraArgs());\n+    command.execute(cmd);\n+  }\n+\n+  private void validateColumns(String inputFile, List<String> prunePaths) throws IOException {\n+    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(inputFile)).withConf(conf).build();\n+    for (int i = 0; i < numRecord; i++) {\n+      Group group = reader.read();\n+      if (!prunePaths.contains(\"DocId\")) {\n+        assertEquals(1l, group.getLong(\"DocId\", 0));\n+      }\n+      if (!prunePaths.contains(\"Name\")) {\n+        assertEquals(\"foo\", group.getBinary(\"Name\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Gender\")) {\n+        assertEquals(\"male\", group.getBinary(\"Gender\", 0).toStringUsingUTF8());\n+      }\n+      if (!prunePaths.contains(\"Links\")) {\n+        Group subGroup = group.getGroup(\"Links\", 0);\n+        if (!prunePaths.contains(\"Links.Backward\")) {\n+          assertEquals(2l, subGroup.getLong(\"Backward\", 0));\n+        }\n+        if (!prunePaths.contains(\"Links.Forward\")) {\n+          assertEquals(3l, subGroup.getLong(\"Forward\", 0));\n+        }\n+      }\n+    }\n+    reader.close();\n+  }\n+\n+  private String createParquetFile() throws IOException {\n+    MessageType schema = new MessageType(\"schema\",\n+      new PrimitiveType(REQUIRED, INT64, \"DocId\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Name\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Gender\"),\n+      new GroupType(OPTIONAL, \"Links\",\n+        new PrimitiveType(REPEATED, INT64, \"Backward\"),\n+        new PrimitiveType(REPEATED, INT64, \"Forward\")));\n+\n+    conf.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString());\n+\n+    Path file = new Path(root, + rand.nextInt(100000) + \"input.parquet\");\n+\n+    ParquetWriter<Group> writer = new ParquetWriter<>(file, new GroupWriteSupport(), ParquetWriter.DEFAULT_COMPRESSION_CODEC_NAME,\n+      ParquetWriter.DEFAULT_BLOCK_SIZE, ParquetWriter.DEFAULT_PAGE_SIZE, 1024,\n+      ParquetWriter.DEFAULT_IS_DICTIONARY_ENABLED, ParquetWriter.DEFAULT_IS_VALIDATING_ENABLED,\n+      ParquetProperties.WriterVersion.PARQUET_1_0, conf);\n+\n+    for (int i = 0; i < numRecord; i++) {\n+      SimpleGroup g = new SimpleGroup(schema);\n+      g.add(\"DocId\", 1l);\n+      g.add(\"Name\", \"foo\");\n+      g.add(\"Gender\", \"male\");\n+      Group links = g.addGroup(\"Links\");\n+      links.add(0, 2l);\n+      links.add(1, 3l);\n+      writer.write(g);\n+    }\n+\n+    writer.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4f368c086ba739235a738c602586ac8826b6c4c"}, "originalPosition": 250}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MTIwNTU3Ng==", "bodyText": "Let's check the following scenario.\nUser wants to prune the columns a, b and c from several parquet files. The schema of the files are not the same because they evolved (e.g. column c is added after some files were already created or column a is used for partitioning so many files do not contain it).\nIf you think the previous scenario is valid then the tool should not throw an exception in case a column to be pruned is not in the file. Meanwhile, the user might mistyped the column name and it may not notify. So, it is your choice how you should handle this case. :)", "url": "https://github.com/apache/parquet-mr/pull/755#discussion_r381205576", "createdAt": "2020-02-19T10:30:36Z", "author": {"login": "gszadovszky"}, "path": "parquet-tools/src/test/java/org/apache/parquet/tools/command/TestPruneColumnsCommand.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.tools.command;\n+\n+import org.apache.commons.cli.CommandLine;\n+import org.apache.commons.cli.CommandLineParser;\n+import org.apache.commons.cli.Options;\n+import org.apache.commons.cli.PosixParser;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Random;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestPruneColumnsCommand {\n+\n+  private static final String root = \"target/\";\n+  private final int numRecord = 1000;\n+  private PruneColumnsCommand command = new PruneColumnsCommand();\n+  private Configuration conf = new Configuration();\n+  private Random rand = new Random();\n+\n+  @Test\n+  public void testPruneOneColumn() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove column\n+    String cargs[] = {inputFile, outputFile, \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 3);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Name\");\n+    assertEquals(fields.get(2).getName(), \"Links\");\n+    List<Type> subFields = fields.get(2).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test\n+  public void testPruneMultiColumns() throws Exception {\n+    // Create Parquet file\n+    String inputFile = createParquetFile();\n+    String outputFile = getParquetFileName();\n+\n+    // Remove columns\n+    String cargs[] = {inputFile, outputFile, \"Name\", \"Gender\"};\n+    executeCommandLine(cargs);\n+\n+    // Verify the schema are not changed for the columns not pruned\n+    ParquetMetadata pmd = ParquetFileReader.readFooter(conf, new Path(outputFile), ParquetMetadataConverter.NO_FILTER);\n+    MessageType schema = pmd.getFileMetaData().getSchema();\n+    List<Type> fields = schema.getFields();\n+    assertEquals(fields.size(), 2);\n+    assertEquals(fields.get(0).getName(), \"DocId\");\n+    assertEquals(fields.get(1).getName(), \"Links\");\n+    List<Type> subFields = fields.get(1).asGroupType().getFields();\n+    assertEquals(subFields.size(), 2);\n+    assertEquals(subFields.get(0).getName(), \"Backward\");\n+    assertEquals(subFields.get(1).getName(), \"Forward\");\n+\n+    // Verify the data are not changed for the columns not pruned\n+    List<String> prunePaths = Arrays.asList(\"Name\", \"Gender\");\n+    validateColumns(inputFile, prunePaths);\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNotExistsColumn() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4f368c086ba739235a738c602586ac8826b6c4c"}, "originalPosition": 120}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fda592ed4ad0ca6c469472cb89c6966c59a2d7fa", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/fda592ed4ad0ca6c469472cb89c6966c59a2d7fa", "committedDate": "2020-02-21T04:57:36Z", "message": "Address feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "75386f91dd6127447876ab9a80271edfadf5c739", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/75386f91dd6127447876ab9a80271edfadf5c739", "committedDate": "2020-02-21T04:59:32Z", "message": "Remove unused imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c1a524198faadddd2854db9cfa1db8e532e38f5d", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/c1a524198faadddd2854db9cfa1db8e532e38f5d", "committedDate": "2020-02-21T16:21:20Z", "message": "Fix failing test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYzMjI3MjIz", "url": "https://github.com/apache/parquet-mr/pull/755#pullrequestreview-363227223", "createdAt": "2020-02-24T08:41:41Z", "commit": {"oid": "c1a524198faadddd2854db9cfa1db8e532e38f5d"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2159, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}