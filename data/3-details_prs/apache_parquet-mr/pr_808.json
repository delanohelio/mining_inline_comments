{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU4NzA4MDM0", "number": 808, "title": "PARQUET-1396: Example of using EncryptionPropertiesFactory and DecryptionPropertiesFactory", "bodyText": "\u2026ncryption\nMake sure you have checked all steps below.\nJira\n\n My PR addresses the following Parquet Jira issues and references them in the PR title. For example, \"PARQUET-1234: My Parquet PR\"\n\nhttps://issues.apache.org/jira/browse/PARQUET-XXX\nIn case you are adding a dependency, check if the license complies with the ASF 3rd Party License Policy.\n\n\n\nTests\n\n My PR adds the following unit tests OR does not need testing for this extremely good reason:\n\nCommits\n\n My commits all reference Jira issues in their subject lines. In addition, my commits follow the guidelines from \"How to write a good git commit message\":\n\nSubject is separated from body by a blank line\nSubject is limited to 50 characters (not including Jira issue reference)\nSubject does not end with a period\nSubject uses the imperative mood (\"add\", not \"adding\")\nBody wraps at 72 characters\nBody explains \"what\" and \"why\", not \"how\"\n\n\n\nDocumentation\n\n In case of new functionality, my PR adds documentation that describes how to use it.\n\nAll the public functions and the classes in the PR contain Javadoc that explain what it does", "createdAt": "2020-07-29T21:20:25Z", "url": "https://github.com/apache/parquet-mr/pull/808", "merged": true, "mergeCommit": {"oid": "5c6916c23cb2b9c225ea80328550ee0e11aee225"}, "closed": true, "closedAt": "2020-11-13T21:22:30Z", "author": {"login": "shangxinli"}, "timelineItems": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc59tDVgFqTQ1ODI2NjQyMA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdWq8lCgFqTUxNzg1NzQ2Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4MjY2NDIw", "url": "https://github.com/apache/parquet-mr/pull/808#pullrequestreview-458266420", "createdAt": "2020-07-30T10:10:02Z", "commit": {"oid": "9adb75a2356147a204d76c82eb39f43e9ee72b58"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxMDoxMDowMlrOG5cxqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxMDo0NDo0NVrOG5d06A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjg5MzQ4MA==", "bodyText": "This package name break java naming conventions. It should not contain uppercase. I would suggest using e.g. org.apache.parquet.crypto.propertiesfactory", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r462893480", "createdAt": "2020-07-30T10:10:02Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,252 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.CryptoPropertiesFactoryTests;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9adb75a2356147a204d76c82eb39f43e9ee72b58"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mjg5OTcxNw==", "bodyText": "Is it necessary to allow setting the WriteSupport? The concept of the ParquetWriter implementations is to hide all these stuff from the user so it can simply create a ParquetWriter<Group> writer = ExampleParquetWriter.builder(...).with(...) without dealing with the logic required for converting a Group object to writable primitives. Also, allowing to set a simple WriteSupport allows to set one that is not compatible with the Group type breaking the whole logic.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r462899717", "createdAt": "2020-07-30T10:22:16Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java", "diffHunk": "@@ -104,15 +105,19 @@ public Builder withExtraMetaData(Map<String, String> extraMetaData) {\n       return this;\n     }\n \n+    public Builder withWriteSupport(WriteSupport writeSupport) {\n+      this.writeSupport = writeSupport;\n+      return this;\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9adb75a2356147a204d76c82eb39f43e9ee72b58"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkwOTMyMA==", "bodyText": "You should use the annotation @Override for every method that is overriden.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r462909320", "createdAt": "2020-07-30T10:41:42Z", "author": {"login": "gszadovszky"}, "path": "parquet-column/src/main/java/org/apache/parquet/schema/ExtType.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.schema;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * This class decorates the class 'Type' by adding a Map field 'metadata'.\n+ *\n+ * This decoration is needed to add metadata to each column without changing existing class 'MessageType', which is used\n+ * extensively. Here is the example usage to add column metadata to schema with type of 'MessageType'.\n+ *\n+ * MessageType oldSchema = ...\n+ * Map metadata = ...\n+ * List newFields = new ArrayList();\n+ * for (Type field = oldSchema.getFields()) {\n+ *     Type newField = new ExtType(field);\n+ *     newField.setMetadata(metadata);\n+ *     newFields.add(newField);\n+ * }\n+ * MessageType newSchema = new MessageType(oldSchema.getName(), newFields);\n+ *\n+ * The implementation is mostly following decoration pattern. Most of the methods are just thin wrappers of existing\n+ * implementation of PrimitiveType or GroupType.\n+ */\n+public class ExtType<T> extends Type {\n+  private Type type;\n+  private Map<String, T> metadata;\n+\n+  public ExtType(Type type) {\n+    super(type.getName(), type.getRepetition(), type.getOriginalType(), type.getId());\n+    this.type = type;\n+  }\n+\n+  public ExtType(Type type, String name) {\n+    super(name, type.getRepetition(), OriginalType.UINT_64, type.getId());\n+    this.type = new PrimitiveType(type.getRepetition(), type.asPrimitiveType().getPrimitiveTypeName(), name);\n+  }\n+\n+  public Type withId(int id) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9adb75a2356147a204d76c82eb39f43e9ee72b58"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjkxMDY5Ng==", "bodyText": "Using defaultCharset() would work just as if you would not set any. We usually set the charset to ensure that the result will always be the same on every environment (independently from the default charset). I would suggest using one of the constants of StandardCharsets.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r462910696", "createdAt": "2020-07-30T10:44:45Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/CryptoPropertiesFactoryTests/SchemaCryptoPropertiesFactory.java", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.CryptoPropertiesFactoryTests;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.DecryptionKeyRetrieverMock;\n+import org.apache.parquet.crypto.DecryptionPropertiesFactory;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.ParquetCryptoRuntimeException;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.api.WriteSupport.WriteContext;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.ExtType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactory, DecryptionPropertiesFactory {\n+\n+  private static Logger log = LoggerFactory.getLogger(SchemaCryptoPropertiesFactory.class);\n+\n+  public static final String CONF_ENCRYPTION_ALGORITHM = \"parquet.encryption.algorithm\";\n+  public static final String CONF_ENCRYPTION_FOOTER = \"parquet.encrypt.footer\";\n+  private static final byte[] FOOTER_KEY = {0x01, 0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a,\n+    0x0b, 0x0c, 0x0d, 0x0e, 0x0f, 0x10};\n+  private static final byte[] FOOTER_KEY_METADATA = \"footkey\".getBytes(Charset.defaultCharset());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9adb75a2356147a204d76c82eb39f43e9ee72b58"}, "originalPosition": 55}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4459a335c84300437bef3f912c6628f1407de0ed", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/4459a335c84300437bef3f912c6628f1407de0ed", "committedDate": "2020-08-01T00:14:50Z", "message": "fix a leftover"}, "afterCommit": {"oid": "ed946b260730ab2456cbc8c940c528522e02138c", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/ed946b260730ab2456cbc8c940c528522e02138c", "committedDate": "2020-08-01T00:15:22Z", "message": "Address feedbacks"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxNTU2MDEx", "url": "https://github.com/apache/parquet-mr/pull/808#pullrequestreview-461556011", "createdAt": "2020-08-05T11:01:50Z", "commit": {"oid": "b649eb56682451042c44821edc113e8091bcacfe"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxMTowMTo1MFrOG8EsPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxMToyMjo0NlrOG8FcLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY0NDYwNQ==", "bodyText": "I would expect method comments describing the purpose and usage of this metadata. (We should mention that this metadata is for the current parquet-mr runtime only and it won't be serialized to the file.)\nI understand this is the easiest way to add this map to the class but I don't really like it. For example by invoking this setMetadata the caller will not be informed if it overwrites any values already in. I would more like an approach where the user can set/get the metadata one-by-one.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r465644605", "createdAt": "2020-08-05T11:01:50Z", "author": {"login": "gszadovszky"}, "path": "parquet-column/src/main/java/org/apache/parquet/schema/Type.java", "diffHunk": "@@ -363,4 +365,11 @@ void checkContains(Type subType) {\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n \n+   public void setMetadata(Map<String, Object> metadata) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b649eb56682451042c44821edc113e8091bcacfe"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY1Mjc2Mw==", "bodyText": "nit: -> cryptoMetadatas\nI'm not sure about 's'. I think, data doesn't have plural.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r465652763", "createdAt": "2020-08-05T11:15:09Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,253 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadatas = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b649eb56682451042c44821edc113e8091bcacfe"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY1NjA0OA==", "bodyText": "I suggest using the already existing constant in PropertiesDrivenCryptoFactory directly.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r465656048", "createdAt": "2020-08-05T11:21:00Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.DecryptionKeyRetrieverMock;\n+import org.apache.parquet.crypto.DecryptionPropertiesFactory;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.ParquetCryptoRuntimeException;\n+import org.apache.parquet.hadoop.api.WriteSupport.WriteContext;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactory, DecryptionPropertiesFactory {\n+\n+  private static Logger log = LoggerFactory.getLogger(SchemaCryptoPropertiesFactory.class);\n+\n+  public static final String CONF_ENCRYPTION_ALGORITHM = \"parquet.encryption.algorithm\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b649eb56682451042c44821edc113e8091bcacfe"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTY1Njg3Ng==", "bodyText": "Do you mean \"parquet.encryption.plaintext.footer\"? Please, use existing constants.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r465656876", "createdAt": "2020-08-05T11:22:46Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.DecryptionKeyRetrieverMock;\n+import org.apache.parquet.crypto.DecryptionPropertiesFactory;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.ParquetCryptoRuntimeException;\n+import org.apache.parquet.hadoop.api.WriteSupport.WriteContext;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.Type;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactory, DecryptionPropertiesFactory {\n+\n+  private static Logger log = LoggerFactory.getLogger(SchemaCryptoPropertiesFactory.class);\n+\n+  public static final String CONF_ENCRYPTION_ALGORITHM = \"parquet.encryption.algorithm\";\n+  public static final String CONF_ENCRYPTION_FOOTER = \"parquet.encrypt.footer\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b649eb56682451042c44821edc113e8091bcacfe"}, "originalPosition": 50}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f51b3d2b05fb57564ad6e2fdce1c94d1126a4d81", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/f51b3d2b05fb57564ad6e2fdce1c94d1126a4d81", "committedDate": "2020-09-26T20:51:43Z", "message": "change metadata"}, "afterCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/94872443620c93f96ba4b7c182c9e0a1eca75992", "committedDate": "2020-09-26T20:52:52Z", "message": "Use Configuration to pass the setting"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEzNDYxNzA2", "url": "https://github.com/apache/parquet-mr/pull/808#pullrequestreview-513461706", "createdAt": "2020-10-21T09:05:20Z", "commit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwOTowNToyMFrOHlh1lg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwOTozOTo0MFrOHljPnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExMzc1MA==", "bodyText": "Since the target class is already on the classpath I would use the class object directly and maybe the related conf setter method as well.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509113750", "createdAt": "2020-10-21T09:05:20Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcmCtr() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_V1\");\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionWithFooter() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.setBoolean(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_FOOTER, true);\n+    runTest(conf);\n+  }\n+\n+  private void runTest(Configuration conf ) throws Exception {\n+    conf.set(EncryptionPropertiesFactory.CRYPTO_FACTORY_CLASS_PROPERTY_NAME,\n+      \"org.apache.parquet.crypto.propertiesfactory.SchemaCryptoPropertiesFactory\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNTQ3NQ==", "bodyText": "I would suggest using the related constant instead.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509115475", "createdAt": "2020-10-21T09:07:59Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTExNTU0Mg==", "bodyText": "I would suggest using the related constant instead.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509115542", "createdAt": "2020-10-21T09:08:05Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcmCtr() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_V1\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzNTEyMg==", "bodyText": "I guess it should be cryptoMetadata", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509135122", "createdAt": "2020-10-21T09:37:07Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> crytoMetadata = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzNjc5Ng==", "bodyText": "I don't think it is a good practice to use the column name as a conf key directly. The chance of collisions are pretty high. I would suggest adding a constant prefix.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509136796", "createdAt": "2020-10-21T09:39:40Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaCryptoPropertiesFactory.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.crypto.ColumnEncryptionProperties;\n+import org.apache.parquet.crypto.DecryptionKeyRetrieverMock;\n+import org.apache.parquet.crypto.DecryptionPropertiesFactory;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.crypto.FileDecryptionProperties;\n+import org.apache.parquet.crypto.FileEncryptionProperties;\n+import org.apache.parquet.crypto.ParquetCipher;\n+import org.apache.parquet.crypto.ParquetCryptoRuntimeException;\n+import org.apache.parquet.hadoop.api.WriteSupport.WriteContext;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.MessageType;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.nio.charset.StandardCharsets;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+public class SchemaCryptoPropertiesFactory implements EncryptionPropertiesFactory, DecryptionPropertiesFactory {\n+\n+  private static Logger log = LoggerFactory.getLogger(SchemaCryptoPropertiesFactory.class);\n+\n+  public static final String CONF_ENCRYPTION_ALGORITHM = \"parquet.encryption.algorithm\";\n+  public static final String CONF_ENCRYPTION_FOOTER = \"parquet.encrypt.footer\";\n+  private static final byte[] FOOTER_KEY = {0x01, 0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a,\n+    0x0b, 0x0c, 0x0d, 0x0e, 0x0f, 0x10};\n+  private static final byte[] FOOTER_KEY_METADATA = \"footkey\".getBytes(StandardCharsets.UTF_8);\n+  private static final byte[] COL_KEY = {0x02, 0x03, 0x4, 0x05, 0x06, 0x07, 0x08, 0x09, 0x0a, 0x0b,\n+    0x0c, 0x0d, 0x0e, 0x0f, 0x10, 0x11};\n+  private static final byte[] COL_KEY_METADATA = \"col\".getBytes(StandardCharsets.UTF_8);\n+\n+  @Override\n+  public FileEncryptionProperties getFileEncryptionProperties(Configuration conf, Path tempFilePath,\n+                                                              WriteContext fileWriteContext) throws ParquetCryptoRuntimeException {\n+    MessageType schema = fileWriteContext.getSchema();\n+    List<String[]> paths = schema.getPaths();\n+    if (paths == null || paths.isEmpty()) {\n+      throw new ParquetCryptoRuntimeException(\"Null or empty fields is found\");\n+    }\n+\n+    Map<ColumnPath, ColumnEncryptionProperties> columnPropertyMap = new HashMap<>();\n+\n+    for (String[] path : paths) {\n+      getColumnEncryptionProperties(path, columnPropertyMap, conf);\n+    }\n+\n+    if (columnPropertyMap.size() == 0) {\n+      log.debug(\"No column is encrypted. Returning null so that Parquet can skip. Empty properties will cause Parquet exception\");\n+      return null;\n+    }\n+\n+    /**\n+     * Why we still need footerKeyMetadata even withEncryptedFooter as false? According to the\n+     * 'Plaintext Footer' section of\n+     * https://github.com/apache/parquet-format/blob/encryption/Encryption.md, the plaintext footer\n+     * is signed in order to prevent tampering with the FileMetaData contents. So footerKeyMetadata\n+     * is always needed. This signature will be verified if parquet-mr code is with parquet-1178.\n+     * Otherwise, it will be ignored.\n+     */\n+    boolean shouldEncryptFooter = getEncryptFooter(conf);\n+    FileEncryptionProperties.Builder encryptionPropertiesBuilder =\n+      FileEncryptionProperties.builder(FOOTER_KEY)\n+        .withFooterKeyMetadata(FOOTER_KEY_METADATA)\n+        .withAlgorithm(getParquetCipherOrDefault(conf))\n+        .withEncryptedColumns(columnPropertyMap);\n+    if (!shouldEncryptFooter) {\n+      encryptionPropertiesBuilder = encryptionPropertiesBuilder.withPlaintextFooter();\n+    }\n+    FileEncryptionProperties encryptionProperties = encryptionPropertiesBuilder.build();\n+    log.info(\n+      \"FileEncryptionProperties is built with, algorithm:{}, footerEncrypted:{}\",\n+      encryptionProperties.getAlgorithm(),\n+      encryptionProperties.encryptedFooter());\n+    return encryptionProperties;\n+  }\n+\n+  private ParquetCipher getParquetCipherOrDefault(Configuration conf) {\n+    String algorithm = conf.get(CONF_ENCRYPTION_ALGORITHM, \"AES_GCM_CTR_V1\");\n+    log.debug(\"Encryption algorithm is {}\", algorithm);\n+    return ParquetCipher.valueOf(algorithm.toUpperCase());\n+  }\n+\n+  private boolean getEncryptFooter(Configuration conf) {\n+    boolean encryptFooter = conf.getBoolean(CONF_ENCRYPTION_FOOTER, false);\n+    log.debug(\"Encrypt Footer: {}\", encryptFooter);\n+    return encryptFooter;\n+  }\n+\n+  private void getColumnEncryptionProperties(String[] path, Map<ColumnPath, ColumnEncryptionProperties> columnPropertyMap,\n+                                             Configuration conf) throws ParquetCryptoRuntimeException {\n+    String pathName = String.join(\".\", path);\n+    String columnKeyName = conf.get(pathName, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 116}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEzNTc0NjQz", "url": "https://github.com/apache/parquet-mr/pull/808#pullrequestreview-513574643", "createdAt": "2020-10-21T11:28:50Z", "commit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxMToyODo1MFrOHlnJGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxMTozMjo0NlrOHlnSAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwMDY2Nw==", "bodyText": "This assumes that file.toString() returns the full file path. However, the file is a public abstract interface org.apache.parquet.io.OutputFile, which doesn't have such method, so toString() is up to the implementation; no guarantees it will return the path. Also, new Path(string full_path) is not aware of the right filesystem (?) Maybe can be handled with an upcast to a known implementing class - preferably one that already has a Path getPath() method.\nBut of course, this won't be very general.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509200667", "createdAt": "2020-10-21T11:28:50Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "diffHunk": "@@ -279,6 +279,11 @@ public ParquetWriter(Path file, Configuration conf, WriteSupport<T> writeSupport\n     WriteSupport.WriteContext writeContext = writeSupport.init(conf);\n     MessageType schema = writeContext.getSchema();\n \n+    // encryptionProperties could be built from the implementation of EncryptionPropertiesFactory when it is attached.\n+    if (encryptionProperties == null) {\n+      encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, new Path(file.toString()), writeContext);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTIwMjk0NA==", "bodyText": "probably no need in changing this file", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r509202944", "createdAt": "2020-10-21T11:32:46Z", "author": {"login": "ggershinsky"}, "path": "parquet-column/src/main/java/org/apache/parquet/schema/Type.java", "diffHunk": "@@ -362,5 +362,4 @@ void checkContains(Type subType) {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n-\n-}\n+ }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "94872443620c93f96ba4b7c182c9e0a1eca75992"}, "originalPosition": 6}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4be2d141f1df419ae53e320f01c80e1945f25fa1", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/4be2d141f1df419ae53e320f01c80e1945f25fa1", "committedDate": "2020-10-22T03:46:38Z", "message": "Parquet-1396: Example of using EncryptionPropertiesFactory and DecryptionPropertiesFactory"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7b68b0b0e901a1d6b6a7676d60df7507cc267380", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/7b68b0b0e901a1d6b6a7676d60df7507cc267380", "committedDate": "2020-10-22T03:47:12Z", "message": "Address feedbacks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "621437f72d46a95747826d6e02bb710d981de7d3", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/621437f72d46a95747826d6e02bb710d981de7d3", "committedDate": "2020-10-22T03:47:12Z", "message": "Remove ExtType and add metadata to Type directly"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aea692ac3abb37bfbd0b79984cfc7a861d428825", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/aea692ac3abb37bfbd0b79984cfc7a861d428825", "committedDate": "2020-10-22T03:47:13Z", "message": "Use Configuration to pass the setting"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6e1db8dea8b96ea3d6f30d0ee08ff65f69c21684", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/6e1db8dea8b96ea3d6f30d0ee08ff65f69c21684", "committedDate": "2020-10-22T03:35:05Z", "message": "Address feedback"}, "afterCommit": {"oid": "306796e0b6eedcd9bfda63b2b498223d72c599bf", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/306796e0b6eedcd9bfda63b2b498223d72c599bf", "committedDate": "2020-10-22T03:47:13Z", "message": "Address feedback"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f2bed4e9196c628d06f94fd438196e1b8325dbd2", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/f2bed4e9196c628d06f94fd438196e1b8325dbd2", "committedDate": "2020-10-22T16:57:39Z", "message": "Address more feedbacks"}, "afterCommit": {"oid": "26a4cf4ab43ab345461c2fddc21bdb942f5d4a35", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/26a4cf4ab43ab345461c2fddc21bdb942f5d4a35", "committedDate": "2020-10-22T16:58:08Z", "message": "Address feedback"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "26a4cf4ab43ab345461c2fddc21bdb942f5d4a35", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/26a4cf4ab43ab345461c2fddc21bdb942f5d4a35", "committedDate": "2020-10-22T16:58:08Z", "message": "Address feedback"}, "afterCommit": {"oid": "9589c7593be119591cf802e3d37924d937d7ff50", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/9589c7593be119591cf802e3d37924d937d7ff50", "committedDate": "2020-10-22T18:40:45Z", "message": "Address feedback"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b2839f7bd3e53e3cafa34dc5b4bbfe85e2493bab", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/b2839f7bd3e53e3cafa34dc5b4bbfe85e2493bab", "committedDate": "2020-10-22T20:03:45Z", "message": "remove unused imports"}, "afterCommit": {"oid": "c056ee1db7832d7b36b98363ab668306a272e302", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/c056ee1db7832d7b36b98363ab668306a272e302", "committedDate": "2020-10-22T20:04:26Z", "message": "Address feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0246e50fec13681a1de0407ca660ca923e8a3f99", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/0246e50fec13681a1de0407ca660ca923e8a3f99", "committedDate": "2020-10-22T21:14:13Z", "message": "Address feedback"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e08d0a3e3ae0c9efba16485e6f2e8867671f8f5c", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/e08d0a3e3ae0c9efba16485e6f2e8867671f8f5c", "committedDate": "2020-10-22T21:13:45Z", "message": "Address more feedback"}, "afterCommit": {"oid": "0246e50fec13681a1de0407ca660ca923e8a3f99", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/0246e50fec13681a1de0407ca660ca923e8a3f99", "committedDate": "2020-10-22T21:14:13Z", "message": "Address feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7", "committedDate": "2020-10-23T13:34:48Z", "message": "Replace file.toString() with file.getPath()"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2NTc1NDkw", "url": "https://github.com/apache/parquet-mr/pull/808#pullrequestreview-516575490", "createdAt": "2020-10-26T09:01:39Z", "commit": {"oid": "0246e50fec13681a1de0407ca660ca923e8a3f99"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTowMTo0MFrOHoGHhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTowNzoxNVrOHoGUCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwNTMxNw==", "bodyText": "I don't know if I overlooked this one before or it is a new change. The module parquet-column should not depend on hadoop. That's why we have the separate module parquet-hadoop. We already have struggling issues that parquet-mr cannot be used without hadoop, let's not make it worse.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511805317", "createdAt": "2020-10-26T09:01:40Z", "author": {"login": "gszadovszky"}, "path": "parquet-common/src/main/java/org/apache/parquet/io/OutputFile.java", "diffHunk": "@@ -31,4 +33,5 @@\n \n   long defaultBlockSize();\n \n+  Path getPath();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0246e50fec13681a1de0407ca660ca923e8a3f99"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwNjA2Nw==", "bodyText": "See at OutputFile.getPath.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511806067", "createdAt": "2020-10-26T09:02:51Z", "author": {"login": "gszadovszky"}, "path": "parquet-common/pom.xml", "diffHunk": "@@ -67,6 +67,20 @@\n       <artifactId>audience-annotations</artifactId>\n       <version>0.12.0</version>\n     </dependency>\n+\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-client</artifactId>\n+      <version>${hadoop.version}</version>\n+      <scope>provided</scope>\n+      <exclusions>\n+        <exclusion>\n+          <groupId>org.slf4j</groupId>\n+          <artifactId>slf4j-log4j12</artifactId>\n+        </exclusion>\n+      </exclusions>\n+    </dependency>\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwODQ2MQ==", "bodyText": "I think it is nicer to use the enum ParquetCypher instead of the parquet-format generated class.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511808461", "createdAt": "2020-10-26T09:07:08Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.EncryptionAlgorithm;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> cryptoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__CTR__V1.getFieldName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwODUyMw==", "bodyText": "I think it is nicer to use the enum ParquetCypher instead of the parquet-format generated class.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511808523", "createdAt": "2020-10-26T09:07:15Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/test/java/org/apache/parquet/crypto/propertiesfactory/SchemaControlEncryptionTest.java", "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.crypto.propertiesfactory;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.crypto.EncryptionPropertiesFactory;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.format.EncryptionAlgorithm;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.api.WriteSupport;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertEquals;\n+\n+public class SchemaControlEncryptionTest {\n+\n+  private final static Log LOG = LogFactory.getLog(SchemaControlEncryptionTest.class);\n+  private final static int numRecord = 1000;\n+  private Random rnd = new Random(5);\n+  \n+  // In the test We use a map to tell WriteSupport which columns to be encrypted with what key. In real use cases, people\n+  // can find whatever easy way to do so basing on how do they get these information, for example people can choose to \n+  // store in HMS, or other metastore. \n+  private Map<String, Map<String, Object>> cryptoMetadata = new HashMap<>();\n+  private Map<String, Object[]> testData = new HashMap<>();\n+\n+  @Before\n+  public void generateTestData() {\n+    String[] names = new String[numRecord];\n+    Long[] ages = new Long[numRecord];\n+    String[] linkedInWebs = new String[numRecord];\n+    String[] twitterWebs = new String[numRecord];\n+    for (int i = 0; i < numRecord; i++) {\n+      names[i] = getString();\n+      ages[i] = getLong();\n+      linkedInWebs[i] = getString();\n+      twitterWebs[i] = getString();\n+    }\n+\n+    testData.put(\"Name\", names);\n+    testData.put(\"Age\", ages);\n+    testData.put(\"LinkedIn\", linkedInWebs);\n+    testData.put(\"Twitter\", twitterWebs);\n+  }\n+\n+  @Test\n+  public void testEncryptionDefault() throws Exception {\n+    Configuration conf = new Configuration();\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcm() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__CTR__V1.getFieldName());\n+    runTest(conf);\n+  }\n+\n+  @Test\n+  public void testEncryptionGcmCtr() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(SchemaCryptoPropertiesFactory.CONF_ENCRYPTION_ALGORITHM, EncryptionAlgorithm._Fields.AES__GCM__V1.getFieldName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7"}, "originalPosition": 105}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2NjIwMjYx", "url": "https://github.com/apache/parquet-mr/pull/808#pullrequestreview-516620261", "createdAt": "2020-10-26T09:59:04Z", "commit": {"oid": "2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTo1OTowNFrOHoIQsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwOTo1OTowNFrOHoIQsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTg0MDQzMw==", "bodyText": "per the previous comment, can be changed to encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, new Path(file.getPath()), writeContext);", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r511840433", "createdAt": "2020-10-26T09:59:04Z", "author": {"login": "ggershinsky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java", "diffHunk": "@@ -279,6 +279,11 @@ public ParquetWriter(Path file, Configuration conf, WriteSupport<T> writeSupport\n     WriteSupport.WriteContext writeContext = writeSupport.init(conf);\n     MessageType schema = writeContext.getSchema();\n \n+    // encryptionProperties could be built from the implementation of EncryptionPropertiesFactory when it is attached.\n+    if (encryptionProperties == null) {\n+      encryptionProperties = ParquetOutputFormat.createEncryptionProperties(conf, file.getPath(), writeContext);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f0cf800f5641d6f3c078a0de5a13857ea5a8aa7"}, "originalPosition": 6}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "76d1b6872ffa38eff4d43e441419a9f362711934", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/76d1b6872ffa38eff4d43e441419a9f362711934", "committedDate": "2020-10-26T14:53:10Z", "message": "Address feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2ODkyODIx", "url": "https://github.com/apache/parquet-mr/pull/808#pullrequestreview-516892821", "createdAt": "2020-10-26T15:24:18Z", "commit": {"oid": "76d1b6872ffa38eff4d43e441419a9f362711934"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/cd897c42a242fbfc65e79022fbd0cd3d213a1e5b", "committedDate": "2020-10-26T19:58:48Z", "message": "fix build error"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE3NDM0MTkw", "url": "https://github.com/apache/parquet-mr/pull/808#pullrequestreview-517434190", "createdAt": "2020-10-27T08:08:30Z", "commit": {"oid": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwODowODozMVrOHovmjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwODowOTo0N1rOHovpZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTAwNg==", "bodyText": "It's unused.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512485006", "createdAt": "2020-10-27T08:08:31Z", "author": {"login": "gszadovszky"}, "path": "parquet-benchmarks/src/main/java/org/apache/parquet/benchmarks/NestedNullWritingBenchmarks.java", "diffHunk": "@@ -28,6 +28,7 @@\n import java.io.IOException;\n import java.util.Random;\n \n+import org.apache.hadoop.fs.Path;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTQ5MA==", "bodyText": "nit: Please undo formatting changes in this file.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512485490", "createdAt": "2020-10-27T08:09:22Z", "author": {"login": "gszadovszky"}, "path": "parquet-column/src/main/java/org/apache/parquet/schema/Type.java", "diffHunk": "@@ -362,5 +362,4 @@ void checkContains(Type subType) {\n    * @return the converted tree\n    */\n    abstract <T> T convert(List<GroupType> path, TypeConverter<T> converter);\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTU1Mw==", "bodyText": "nit: Please undo formatting changes in this file.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512485553", "createdAt": "2020-10-27T08:09:28Z", "author": {"login": "gszadovszky"}, "path": "parquet-common/pom.xml", "diffHunk": "@@ -67,6 +67,7 @@\n       <artifactId>audience-annotations</artifactId>\n       <version>0.12.0</version>\n     </dependency>\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ4NTczNQ==", "bodyText": "nit: Please undo formatting changes in this file.", "url": "https://github.com/apache/parquet-mr/pull/808#discussion_r512485735", "createdAt": "2020-10-27T08:09:47Z", "author": {"login": "gszadovszky"}, "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/example/ExampleParquetWriter.java", "diffHunk": "@@ -113,6 +113,5 @@ protected Builder self() {\n     protected WriteSupport<Group> getWriteSupport(Configuration conf) {\n       return new GroupWriteSupport(type, extraMetaData);\n     }\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd897c42a242fbfc65e79022fbd0cd3d213a1e5b"}, "originalPosition": 4}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "475b2228847263d11972539104c11fe31cb4c1fa", "author": {"user": {"login": "shangxinli", "name": "Xinli Shang"}}, "url": "https://github.com/apache/parquet-mr/commit/475b2228847263d11972539104c11fe31cb4c1fa", "committedDate": "2020-10-27T13:31:43Z", "message": "Address more feedbacks"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE3ODU3NDY2", "url": "https://github.com/apache/parquet-mr/pull/808#pullrequestreview-517857466", "createdAt": "2020-10-27T15:45:45Z", "commit": {"oid": "475b2228847263d11972539104c11fe31cb4c1fa"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2218, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}