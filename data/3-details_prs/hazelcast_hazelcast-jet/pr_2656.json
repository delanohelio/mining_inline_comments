{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIwMzc5MTE4", "number": 2656, "title": "Amazon Kinesis source & sink", "bodyText": "Amazon Kinesis Data Streams source.\nFixes #2735\nBreaking changes?\n\nAPI\nclient protocol format\nserialized form\nsnapshot format\n\nChecklist:\n\n Labels and Milestone set\n Added a line in hazelcast-jet-distribution/src/root/release_notes.txt (for any non-trivial fix/enhancement/feature)\n New public APIs have @Nonnull/@Nullable annotations\n New public APIs have @since tags in Javadoc", "createdAt": "2020-11-13T06:27:25Z", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656", "merged": true, "mergeCommit": {"oid": "6e92a177347a3204d4f0763f3614babf733adcf9"}, "closed": true, "closedAt": "2021-01-15T12:25:33Z", "author": {"login": "jbartok"}, "timelineItems": {"totalCount": 124, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdv3DZhAFqTU2NzAzNTYyOA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdwXswRAH2gAyNTIwMzc5MTE4Ojg3ZGZjZTdmZTcyMDZjZWFkYmQ3NWE3ZTM0OWI4YzliN2EyOWVkYTc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY3MDM1NjI4", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#pullrequestreview-567035628", "createdAt": "2021-01-13T09:29:49Z", "commit": {"oid": "71e88f662a3d4c2021925b9b8cea8c23271e1bcd"}, "state": "COMMENTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xM1QwOToyOTo0OVrOISmr7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xM1QxOTo0NjoyNlrOIS_fug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjM3OTExOA==", "bodyText": "1.15.1 is out", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r556379118", "createdAt": "2021-01-13T09:29:49Z", "author": {"login": "gurbuzali"}, "path": "extensions/kinesis/pom.xml", "diffHunk": "@@ -0,0 +1,112 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+  ~ Copyright 2020 Hazelcast Inc.\n+  ~\n+  ~ Licensed under the Hazelcast Community License (the \"License\");\n+  ~ you may not use this file except in compliance with the License.\n+  ~ You may obtain a copy of the License at\n+  ~\n+  ~ http://hazelcast.com/hazelcast-community-license\n+  ~\n+  ~ Unless required by applicable law or agreed to in writing, software\n+  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n+  ~ WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+  ~ See the License for the specific language governing permissions and\n+  ~ limitations under the License.\n+  -->\n+\n+<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n+         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n+         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n+    <modelVersion>4.0.0</modelVersion>\n+    <packaging>jar</packaging>\n+    <name>hazelcast-jet-kinesis</name>\n+    <description>Amazon Kinesis Data Streams producer/consumer support for Hazelcast Jet</description>\n+    <url>http://www.hazelcast.com/</url>\n+\n+    <artifactId>hazelcast-jet-kinesis</artifactId>\n+\n+    <parent>\n+        <groupId>com.hazelcast.jet</groupId>\n+        <artifactId>hazelcast-jet-extensions</artifactId>\n+        <version>4.4-SNAPSHOT</version>\n+    </parent>\n+\n+    <properties>\n+        <aws.kinesis.sdk.version>1.11.934</aws.kinesis.sdk.version>\n+        <testcontainers.version>1.15.0-rc2</testcontainers.version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "71e88f662a3d4c2021925b9b8cea8c23271e1bcd"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjQ2Mzk2NQ==", "bodyText": "can we rename this to something ending with Future. I think we can also drop the describeStream prefix for this and other fields. The only thing we are really interested is the shard-count I guess", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r556463965", "createdAt": "2021-01-13T11:50:37Z", "author": {"login": "gurbuzali"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/ShardCountMonitor.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.DescribeStreamSummaryResult;\n+import com.hazelcast.jet.retry.RetryStrategy;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+public class ShardCountMonitor extends AbstractShardWorker {\n+\n+    /**\n+     * DescribeStreamSummary operations are limited to 20 per second, per account.\n+     */\n+    private static final int DESCRIBE_STREAM_OPERATIONS_ALLOWED_PER_SECOND = 20;\n+\n+    /**\n+     * We don't want to issue describe stream operations at the peak allowed rate.\n+     */\n+    private static final double RATIO_OF_DESCRIBE_STREAM_RATE_UTILIZED = 0.1;\n+\n+    private final AtomicInteger shardCount;\n+    private final RandomizedRateTracker descriteStreamRateTracker;\n+    private final RetryTracker describeStreamRetryTracker;\n+\n+    private Future<DescribeStreamSummaryResult> describeStreamResult;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "812bec6d345b083efca7ade5606565263803a944"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjQ3MDU1Mg==", "bodyText": "can be used context.totalParallelism()", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r556470552", "createdAt": "2021-01-13T12:03:22Z", "author": {"login": "gurbuzali"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java", "diffHunk": "@@ -0,0 +1,502 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsResultEntry;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.internal.metrics.Probe;\n+import com.hazelcast.internal.metrics.ProbeUnit;\n+import com.hazelcast.internal.util.counters.Counter;\n+import com.hazelcast.internal.util.counters.SwCounter;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.core.Inbox;\n+import com.hazelcast.jet.core.Outbox;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.Watermark;\n+import com.hazelcast.jet.kinesis.KinesisSinks;\n+import com.hazelcast.jet.retry.RetryStrategy;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+import static java.lang.System.nanoTime;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.concurrent.TimeUnit.NANOSECONDS;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+public class KinesisSinkP<T> implements Processor {\n+\n+    /**\n+     * Each shard can ingest a maximum of a 1000 records per second.\n+     */\n+    private static final int MAX_RECORD_PER_SHARD_PER_SECOND = 1000;\n+\n+    /**\n+     * PutRecords requests are limited to 500 records.\n+     */\n+    private static final int MAX_RECORDS_IN_REQUEST = 500;\n+\n+    /**\n+     * Since we are using PutRecords for its batching effect, we don't want\n+     * the batch size to be so small as to negate all benefits.\n+     */\n+    private static final int MIN_RECORDS_IN_REQUEST = 10;\n+\n+    /**\n+     * The maximum allowed size of all the records in a PutRecords request,\n+     * including partition keys is 5M.\n+     */\n+    private static final int MAX_REQUEST_SIZE_IN_BYTES = 5 * 1024 * 1024;\n+\n+    /**\n+     * Kinesis always uses UTF-8 to encode strings.\n+     */\n+    private static final Charset KINESIS_STRING_ENCODING = StandardCharsets.UTF_8;\n+\n+    @Nonnull\n+    private final AmazonKinesisAsync kinesis;\n+    @Nonnull\n+    private final String stream;\n+    @Nullable\n+    private final ShardCountMonitor monitor;\n+    @Nonnull\n+    private final AtomicInteger shardCountProvider;\n+    @Nonnull\n+    private final Buffer<T> buffer;\n+\n+    @Probe(name = KinesisSinks.BATCH_SIZE_METRIC, unit = ProbeUnit.COUNT)\n+    private final Counter batchSizeMetric;\n+    @Probe(name = KinesisSinks.THROTTLING_SLEEP_METRIC, unit = ProbeUnit.MS)\n+    private final Counter sleepMetric = SwCounter.newSwCounter();\n+\n+    private ILogger logger;\n+    private KinesisHelper helper;\n+    private int shardCount;\n+    private int sinkCount;\n+\n+    private Future<PutRecordsResult> sendResult;\n+    private long nextSendTime = nanoTime();\n+    private final RetryTracker sendRetryTracker;\n+\n+    private final ThroughputController throughputController = new ThroughputController();\n+\n+    public KinesisSinkP(\n+            @Nonnull AmazonKinesisAsync kinesis,\n+            @Nonnull String stream,\n+            @Nonnull FunctionEx<T, String> keyFn,\n+            @Nonnull FunctionEx<T, byte[]> valueFn,\n+            @Nullable ShardCountMonitor monitor,\n+            @Nonnull AtomicInteger shardCountProvider,\n+            @Nonnull RetryStrategy retryStrategy\n+            ) {\n+        this.kinesis = kinesis;\n+        this.stream = stream;\n+        this.monitor = monitor;\n+        this.shardCountProvider = shardCountProvider;\n+        this.buffer = new Buffer<>(keyFn, valueFn);\n+        this.batchSizeMetric = SwCounter.newSwCounter(buffer.getCapacity());\n+        this.sendRetryTracker = new RetryTracker(retryStrategy);\n+    }\n+\n+    @Override\n+    public boolean isCooperative() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void init(@Nonnull Outbox outbox, @Nonnull Context context) {\n+        logger = context.logger();\n+        sinkCount = context.memberCount() * context.localParallelism();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "812bec6d345b083efca7ade5606565263803a944"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjUzNzE1Mw==", "bodyText": "we could use Arrays.setAll to initialize the arry instead of creating a stream for it\n            this.entries = new BufferEntry[MAX_RECORDS_IN_REQUEST];\n            Arrays.setAll(entries, value -> new BufferEntry());", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r556537153", "createdAt": "2021-01-13T13:57:04Z", "author": {"login": "gurbuzali"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java", "diffHunk": "@@ -0,0 +1,502 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsResultEntry;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.internal.metrics.Probe;\n+import com.hazelcast.internal.metrics.ProbeUnit;\n+import com.hazelcast.internal.util.counters.Counter;\n+import com.hazelcast.internal.util.counters.SwCounter;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.core.Inbox;\n+import com.hazelcast.jet.core.Outbox;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.Watermark;\n+import com.hazelcast.jet.kinesis.KinesisSinks;\n+import com.hazelcast.jet.retry.RetryStrategy;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+import static java.lang.System.nanoTime;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.concurrent.TimeUnit.NANOSECONDS;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+public class KinesisSinkP<T> implements Processor {\n+\n+    /**\n+     * Each shard can ingest a maximum of a 1000 records per second.\n+     */\n+    private static final int MAX_RECORD_PER_SHARD_PER_SECOND = 1000;\n+\n+    /**\n+     * PutRecords requests are limited to 500 records.\n+     */\n+    private static final int MAX_RECORDS_IN_REQUEST = 500;\n+\n+    /**\n+     * Since we are using PutRecords for its batching effect, we don't want\n+     * the batch size to be so small as to negate all benefits.\n+     */\n+    private static final int MIN_RECORDS_IN_REQUEST = 10;\n+\n+    /**\n+     * The maximum allowed size of all the records in a PutRecords request,\n+     * including partition keys is 5M.\n+     */\n+    private static final int MAX_REQUEST_SIZE_IN_BYTES = 5 * 1024 * 1024;\n+\n+    /**\n+     * Kinesis always uses UTF-8 to encode strings.\n+     */\n+    private static final Charset KINESIS_STRING_ENCODING = StandardCharsets.UTF_8;\n+\n+    @Nonnull\n+    private final AmazonKinesisAsync kinesis;\n+    @Nonnull\n+    private final String stream;\n+    @Nullable\n+    private final ShardCountMonitor monitor;\n+    @Nonnull\n+    private final AtomicInteger shardCountProvider;\n+    @Nonnull\n+    private final Buffer<T> buffer;\n+\n+    @Probe(name = KinesisSinks.BATCH_SIZE_METRIC, unit = ProbeUnit.COUNT)\n+    private final Counter batchSizeMetric;\n+    @Probe(name = KinesisSinks.THROTTLING_SLEEP_METRIC, unit = ProbeUnit.MS)\n+    private final Counter sleepMetric = SwCounter.newSwCounter();\n+\n+    private ILogger logger;\n+    private KinesisHelper helper;\n+    private int shardCount;\n+    private int sinkCount;\n+\n+    private Future<PutRecordsResult> sendResult;\n+    private long nextSendTime = nanoTime();\n+    private final RetryTracker sendRetryTracker;\n+\n+    private final ThroughputController throughputController = new ThroughputController();\n+\n+    public KinesisSinkP(\n+            @Nonnull AmazonKinesisAsync kinesis,\n+            @Nonnull String stream,\n+            @Nonnull FunctionEx<T, String> keyFn,\n+            @Nonnull FunctionEx<T, byte[]> valueFn,\n+            @Nullable ShardCountMonitor monitor,\n+            @Nonnull AtomicInteger shardCountProvider,\n+            @Nonnull RetryStrategy retryStrategy\n+            ) {\n+        this.kinesis = kinesis;\n+        this.stream = stream;\n+        this.monitor = monitor;\n+        this.shardCountProvider = shardCountProvider;\n+        this.buffer = new Buffer<>(keyFn, valueFn);\n+        this.batchSizeMetric = SwCounter.newSwCounter(buffer.getCapacity());\n+        this.sendRetryTracker = new RetryTracker(retryStrategy);\n+    }\n+\n+    @Override\n+    public boolean isCooperative() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void init(@Nonnull Outbox outbox, @Nonnull Context context) {\n+        logger = context.logger();\n+        sinkCount = context.memberCount() * context.localParallelism();\n+        helper = new KinesisHelper(kinesis, stream);\n+    }\n+\n+    @Override\n+    public boolean tryProcessWatermark(@Nonnull Watermark watermark) {\n+        return true; //watermark ignored\n+    }\n+\n+    @Override\n+    public void process(int ordinal, @Nonnull Inbox inbox) {\n+        if (monitor != null) {\n+            monitor.run();\n+        }\n+\n+        updateThroughputLimitations();\n+\n+        if (sendResult != null) {\n+            checkIfSendingFinished();\n+        }\n+        if (sendResult == null) {\n+            initSending(inbox);\n+        }\n+    }\n+\n+    @Override\n+    public boolean complete() {\n+        if (sendResult != null) {\n+            checkIfSendingFinished();\n+        }\n+        if (sendResult == null) {\n+            if (buffer.isEmpty()) {\n+                return true;\n+            }\n+            initSending(null);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public boolean saveToSnapshot() {\n+        if (sendResult != null) {\n+            checkIfSendingFinished();\n+        }\n+        return sendResult == null;\n+    }\n+\n+    private void updateThroughputLimitations() {\n+        int newShardCount = shardCountProvider.get();\n+        if (newShardCount > 0 && shardCount != newShardCount) {\n+            buffer.setCapacity(throughputController.computeBatchSize(newShardCount, sinkCount));\n+            batchSizeMetric.set(buffer.getCapacity());\n+\n+            shardCount = newShardCount;\n+        }\n+    }\n+\n+    private void initSending(@Nullable Inbox inbox) {\n+        if (inbox != null) {\n+            bufferFromInbox(inbox);\n+        }\n+        attemptToDispatchBufferContent();\n+    }\n+\n+    private void bufferFromInbox(@Nonnull Inbox inbox) {\n+        if (buffer.isFull()) {\n+            return;\n+        }\n+\n+        while (true) {\n+            T t = (T) inbox.peek();\n+            if (t == null) {\n+                //no more items in inbox\n+                return;\n+            }\n+\n+            boolean canBeBuffered = buffer.add(t);\n+            if (canBeBuffered) {\n+                inbox.remove();\n+            } else {\n+                //no more room in buffer\n+                return;\n+            }\n+        }\n+    }\n+\n+    private void attemptToDispatchBufferContent() {\n+        if (buffer.isEmpty()) {\n+            return;\n+        }\n+\n+        long currentTime = nanoTime();\n+        if (currentTime < nextSendTime) {\n+            return;\n+        }\n+\n+        List<PutRecordsRequestEntry> entries = buffer.content();\n+        sendResult = helper.putRecordsAsync(entries);\n+        nextSendTime = currentTime;\n+    }\n+\n+    private void checkIfSendingFinished() {\n+        if (sendResult.isDone()) {\n+            PutRecordsResult result;\n+            try {\n+                result = helper.readResult(this.sendResult);\n+            } catch (ProvisionedThroughputExceededException pte) {\n+                dealWithThroughputExceeded(\"Data throughput rate exceeded. Backing off and retrying in %d ms\");\n+                return;\n+            } catch (SdkClientException sce) {\n+                dealWithSendFailure(sce);\n+                return;\n+            } catch (Throwable t) {\n+                throw rethrow(t);\n+            } finally {\n+                sendResult = null;\n+            }\n+\n+            pruneSentFromBuffer(result);\n+            if (result.getFailedRecordCount() > 0) {\n+                dealWithThroughputExceeded(\"Failed to send \" + result.getFailedRecordCount() + \" (out of \" +\n+                        result.getRecords().size() + \") record(s) to stream '\" + stream +\n+                        \"'. Sending will be retried in %d ms, message reordering is likely.\");\n+            } else {\n+                long sleepTimeNanos = throughputController.markSuccess();\n+                this.nextSendTime += sleepTimeNanos;\n+                this.sleepMetric.set(NANOSECONDS.toMillis(sleepTimeNanos));\n+                sendRetryTracker.reset();\n+            }\n+        }\n+    }\n+\n+    private void dealWithSendFailure(@Nonnull Exception failure) {\n+        sendRetryTracker.attemptFailed();\n+        if (sendRetryTracker.shouldTryAgain()) {\n+            long timeoutMillis = sendRetryTracker.getNextWaitTimeMillis();\n+            logger.warning(String.format(\"Failed to send records, will retry in %d ms. Cause: %s\",\n+                    timeoutMillis, failure.getMessage()));\n+            nextSendTime = System.nanoTime() + MILLISECONDS.toNanos(timeoutMillis);\n+        } else {\n+            throw rethrow(failure);\n+        }\n+\n+    }\n+\n+    private void dealWithThroughputExceeded(@Nonnull String message) {\n+        long sleepTimeNanos = throughputController.markFailure();\n+        this.nextSendTime += sleepTimeNanos;\n+        this.sleepMetric.set(NANOSECONDS.toMillis(sleepTimeNanos));\n+        logger.warning(String.format(message, NANOSECONDS.toMillis(sleepTimeNanos)));\n+    }\n+\n+    private void pruneSentFromBuffer(@Nullable PutRecordsResult result) {\n+        if (result == null) {\n+            return;\n+        }\n+\n+        List<PutRecordsResultEntry> resultEntries = result.getRecords();\n+        if (result.getFailedRecordCount() > 0) {\n+            for (int i = resultEntries.size() - 1; i >= 0; i--) {\n+                PutRecordsResultEntry resultEntry = resultEntries.get(i);\n+                if (resultEntry.getErrorCode() == null) {\n+                    buffer.remove(i);\n+                }\n+            }\n+        } else {\n+            buffer.remove(0, resultEntries.size());\n+        }\n+    }\n+\n+    private static class Buffer<T> {\n+\n+        private final FunctionEx<T, String> keyFn;\n+        private final FunctionEx<T, byte[]> valueFn;\n+\n+        private final BufferEntry[] entries;\n+        private int entryCount;\n+        private int totalEntrySize;\n+        private int capacity;\n+\n+        Buffer(FunctionEx<T, String> keyFn, FunctionEx<T, byte[]> valueFn) {\n+            this.keyFn = keyFn;\n+            this.valueFn = valueFn;\n+            this.entries = initEntries();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "812bec6d345b083efca7ade5606565263803a944"}, "originalPosition": 321}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjYwNzQxOA==", "bodyText": "buffer.remove(index) moves the entry at the given index to the end and shifts all the entries after the index. And we do it for each successful entry one by one. I suggest to add a retainFailedEntries method to Buffer, something like this:\n        public void retainFailedEntries(List<PutRecordsResultEntry> records) {\n            int startIndex = 0;\n            for (int index = 0; index < records.size(); index++) {\n                if (records.get(index).getErrorCode() != null) {\n                    swap(startIndex++, index);\n                } else {\n                    totalEntrySize -= entries[index].encodedSize;\n                    entryCount--;\n                }\n            }\n        }\n\n        private void swap(int a, int b) {\n            BufferEntry temp = entries[a];\n            entries[a] = entries[b];\n            entries[b] = temp;\n        }\n\nThe method iterates over the records and if failed moves it to the beginning of the array by swapping entries.\nAlso the below remove(index, count) can be replaced with just clear I guess. It is called when there is no failed record which means the record count should be equal to the entryCount of the buffer. In this case current implementation already delegates to clear.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r556607418", "createdAt": "2021-01-13T15:30:29Z", "author": {"login": "gurbuzali"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java", "diffHunk": "@@ -0,0 +1,502 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsResultEntry;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.internal.metrics.Probe;\n+import com.hazelcast.internal.metrics.ProbeUnit;\n+import com.hazelcast.internal.util.counters.Counter;\n+import com.hazelcast.internal.util.counters.SwCounter;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.core.Inbox;\n+import com.hazelcast.jet.core.Outbox;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.Watermark;\n+import com.hazelcast.jet.kinesis.KinesisSinks;\n+import com.hazelcast.jet.retry.RetryStrategy;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+import static java.lang.System.nanoTime;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.concurrent.TimeUnit.NANOSECONDS;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+public class KinesisSinkP<T> implements Processor {\n+\n+    /**\n+     * Each shard can ingest a maximum of a 1000 records per second.\n+     */\n+    private static final int MAX_RECORD_PER_SHARD_PER_SECOND = 1000;\n+\n+    /**\n+     * PutRecords requests are limited to 500 records.\n+     */\n+    private static final int MAX_RECORDS_IN_REQUEST = 500;\n+\n+    /**\n+     * Since we are using PutRecords for its batching effect, we don't want\n+     * the batch size to be so small as to negate all benefits.\n+     */\n+    private static final int MIN_RECORDS_IN_REQUEST = 10;\n+\n+    /**\n+     * The maximum allowed size of all the records in a PutRecords request,\n+     * including partition keys is 5M.\n+     */\n+    private static final int MAX_REQUEST_SIZE_IN_BYTES = 5 * 1024 * 1024;\n+\n+    /**\n+     * Kinesis always uses UTF-8 to encode strings.\n+     */\n+    private static final Charset KINESIS_STRING_ENCODING = StandardCharsets.UTF_8;\n+\n+    @Nonnull\n+    private final AmazonKinesisAsync kinesis;\n+    @Nonnull\n+    private final String stream;\n+    @Nullable\n+    private final ShardCountMonitor monitor;\n+    @Nonnull\n+    private final AtomicInteger shardCountProvider;\n+    @Nonnull\n+    private final Buffer<T> buffer;\n+\n+    @Probe(name = KinesisSinks.BATCH_SIZE_METRIC, unit = ProbeUnit.COUNT)\n+    private final Counter batchSizeMetric;\n+    @Probe(name = KinesisSinks.THROTTLING_SLEEP_METRIC, unit = ProbeUnit.MS)\n+    private final Counter sleepMetric = SwCounter.newSwCounter();\n+\n+    private ILogger logger;\n+    private KinesisHelper helper;\n+    private int shardCount;\n+    private int sinkCount;\n+\n+    private Future<PutRecordsResult> sendResult;\n+    private long nextSendTime = nanoTime();\n+    private final RetryTracker sendRetryTracker;\n+\n+    private final ThroughputController throughputController = new ThroughputController();\n+\n+    public KinesisSinkP(\n+            @Nonnull AmazonKinesisAsync kinesis,\n+            @Nonnull String stream,\n+            @Nonnull FunctionEx<T, String> keyFn,\n+            @Nonnull FunctionEx<T, byte[]> valueFn,\n+            @Nullable ShardCountMonitor monitor,\n+            @Nonnull AtomicInteger shardCountProvider,\n+            @Nonnull RetryStrategy retryStrategy\n+            ) {\n+        this.kinesis = kinesis;\n+        this.stream = stream;\n+        this.monitor = monitor;\n+        this.shardCountProvider = shardCountProvider;\n+        this.buffer = new Buffer<>(keyFn, valueFn);\n+        this.batchSizeMetric = SwCounter.newSwCounter(buffer.getCapacity());\n+        this.sendRetryTracker = new RetryTracker(retryStrategy);\n+    }\n+\n+    @Override\n+    public boolean isCooperative() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void init(@Nonnull Outbox outbox, @Nonnull Context context) {\n+        logger = context.logger();\n+        sinkCount = context.memberCount() * context.localParallelism();\n+        helper = new KinesisHelper(kinesis, stream);\n+    }\n+\n+    @Override\n+    public boolean tryProcessWatermark(@Nonnull Watermark watermark) {\n+        return true; //watermark ignored\n+    }\n+\n+    @Override\n+    public void process(int ordinal, @Nonnull Inbox inbox) {\n+        if (monitor != null) {\n+            monitor.run();\n+        }\n+\n+        updateThroughputLimitations();\n+\n+        if (sendResult != null) {\n+            checkIfSendingFinished();\n+        }\n+        if (sendResult == null) {\n+            initSending(inbox);\n+        }\n+    }\n+\n+    @Override\n+    public boolean complete() {\n+        if (sendResult != null) {\n+            checkIfSendingFinished();\n+        }\n+        if (sendResult == null) {\n+            if (buffer.isEmpty()) {\n+                return true;\n+            }\n+            initSending(null);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public boolean saveToSnapshot() {\n+        if (sendResult != null) {\n+            checkIfSendingFinished();\n+        }\n+        return sendResult == null;\n+    }\n+\n+    private void updateThroughputLimitations() {\n+        int newShardCount = shardCountProvider.get();\n+        if (newShardCount > 0 && shardCount != newShardCount) {\n+            buffer.setCapacity(throughputController.computeBatchSize(newShardCount, sinkCount));\n+            batchSizeMetric.set(buffer.getCapacity());\n+\n+            shardCount = newShardCount;\n+        }\n+    }\n+\n+    private void initSending(@Nullable Inbox inbox) {\n+        if (inbox != null) {\n+            bufferFromInbox(inbox);\n+        }\n+        attemptToDispatchBufferContent();\n+    }\n+\n+    private void bufferFromInbox(@Nonnull Inbox inbox) {\n+        if (buffer.isFull()) {\n+            return;\n+        }\n+\n+        while (true) {\n+            T t = (T) inbox.peek();\n+            if (t == null) {\n+                //no more items in inbox\n+                return;\n+            }\n+\n+            boolean canBeBuffered = buffer.add(t);\n+            if (canBeBuffered) {\n+                inbox.remove();\n+            } else {\n+                //no more room in buffer\n+                return;\n+            }\n+        }\n+    }\n+\n+    private void attemptToDispatchBufferContent() {\n+        if (buffer.isEmpty()) {\n+            return;\n+        }\n+\n+        long currentTime = nanoTime();\n+        if (currentTime < nextSendTime) {\n+            return;\n+        }\n+\n+        List<PutRecordsRequestEntry> entries = buffer.content();\n+        sendResult = helper.putRecordsAsync(entries);\n+        nextSendTime = currentTime;\n+    }\n+\n+    private void checkIfSendingFinished() {\n+        if (sendResult.isDone()) {\n+            PutRecordsResult result;\n+            try {\n+                result = helper.readResult(this.sendResult);\n+            } catch (ProvisionedThroughputExceededException pte) {\n+                dealWithThroughputExceeded(\"Data throughput rate exceeded. Backing off and retrying in %d ms\");\n+                return;\n+            } catch (SdkClientException sce) {\n+                dealWithSendFailure(sce);\n+                return;\n+            } catch (Throwable t) {\n+                throw rethrow(t);\n+            } finally {\n+                sendResult = null;\n+            }\n+\n+            pruneSentFromBuffer(result);\n+            if (result.getFailedRecordCount() > 0) {\n+                dealWithThroughputExceeded(\"Failed to send \" + result.getFailedRecordCount() + \" (out of \" +\n+                        result.getRecords().size() + \") record(s) to stream '\" + stream +\n+                        \"'. Sending will be retried in %d ms, message reordering is likely.\");\n+            } else {\n+                long sleepTimeNanos = throughputController.markSuccess();\n+                this.nextSendTime += sleepTimeNanos;\n+                this.sleepMetric.set(NANOSECONDS.toMillis(sleepTimeNanos));\n+                sendRetryTracker.reset();\n+            }\n+        }\n+    }\n+\n+    private void dealWithSendFailure(@Nonnull Exception failure) {\n+        sendRetryTracker.attemptFailed();\n+        if (sendRetryTracker.shouldTryAgain()) {\n+            long timeoutMillis = sendRetryTracker.getNextWaitTimeMillis();\n+            logger.warning(String.format(\"Failed to send records, will retry in %d ms. Cause: %s\",\n+                    timeoutMillis, failure.getMessage()));\n+            nextSendTime = System.nanoTime() + MILLISECONDS.toNanos(timeoutMillis);\n+        } else {\n+            throw rethrow(failure);\n+        }\n+\n+    }\n+\n+    private void dealWithThroughputExceeded(@Nonnull String message) {\n+        long sleepTimeNanos = throughputController.markFailure();\n+        this.nextSendTime += sleepTimeNanos;\n+        this.sleepMetric.set(NANOSECONDS.toMillis(sleepTimeNanos));\n+        logger.warning(String.format(message, NANOSECONDS.toMillis(sleepTimeNanos)));\n+    }\n+\n+    private void pruneSentFromBuffer(@Nullable PutRecordsResult result) {\n+        if (result == null) {\n+            return;\n+        }\n+\n+        List<PutRecordsResultEntry> resultEntries = result.getRecords();\n+        if (result.getFailedRecordCount() > 0) {\n+            for (int i = resultEntries.size() - 1; i >= 0; i--) {\n+                PutRecordsResultEntry resultEntry = resultEntries.get(i);\n+                if (resultEntry.getErrorCode() == null) {\n+                    buffer.remove(i);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "812bec6d345b083efca7ade5606565263803a944"}, "originalPosition": 300}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjYyMTUwNQ==", "bodyText": "again having a Future at the end of the field name can help the code reader", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r556621505", "createdAt": "2021-01-13T15:47:28Z", "author": {"login": "gurbuzali"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java", "diffHunk": "@@ -0,0 +1,502 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsResultEntry;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.internal.metrics.Probe;\n+import com.hazelcast.internal.metrics.ProbeUnit;\n+import com.hazelcast.internal.util.counters.Counter;\n+import com.hazelcast.internal.util.counters.SwCounter;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.core.Inbox;\n+import com.hazelcast.jet.core.Outbox;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.Watermark;\n+import com.hazelcast.jet.kinesis.KinesisSinks;\n+import com.hazelcast.jet.retry.RetryStrategy;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+import static java.lang.System.nanoTime;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.concurrent.TimeUnit.NANOSECONDS;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+public class KinesisSinkP<T> implements Processor {\n+\n+    /**\n+     * Each shard can ingest a maximum of a 1000 records per second.\n+     */\n+    private static final int MAX_RECORD_PER_SHARD_PER_SECOND = 1000;\n+\n+    /**\n+     * PutRecords requests are limited to 500 records.\n+     */\n+    private static final int MAX_RECORDS_IN_REQUEST = 500;\n+\n+    /**\n+     * Since we are using PutRecords for its batching effect, we don't want\n+     * the batch size to be so small as to negate all benefits.\n+     */\n+    private static final int MIN_RECORDS_IN_REQUEST = 10;\n+\n+    /**\n+     * The maximum allowed size of all the records in a PutRecords request,\n+     * including partition keys is 5M.\n+     */\n+    private static final int MAX_REQUEST_SIZE_IN_BYTES = 5 * 1024 * 1024;\n+\n+    /**\n+     * Kinesis always uses UTF-8 to encode strings.\n+     */\n+    private static final Charset KINESIS_STRING_ENCODING = StandardCharsets.UTF_8;\n+\n+    @Nonnull\n+    private final AmazonKinesisAsync kinesis;\n+    @Nonnull\n+    private final String stream;\n+    @Nullable\n+    private final ShardCountMonitor monitor;\n+    @Nonnull\n+    private final AtomicInteger shardCountProvider;\n+    @Nonnull\n+    private final Buffer<T> buffer;\n+\n+    @Probe(name = KinesisSinks.BATCH_SIZE_METRIC, unit = ProbeUnit.COUNT)\n+    private final Counter batchSizeMetric;\n+    @Probe(name = KinesisSinks.THROTTLING_SLEEP_METRIC, unit = ProbeUnit.MS)\n+    private final Counter sleepMetric = SwCounter.newSwCounter();\n+\n+    private ILogger logger;\n+    private KinesisHelper helper;\n+    private int shardCount;\n+    private int sinkCount;\n+\n+    private Future<PutRecordsResult> sendResult;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "812bec6d345b083efca7ade5606565263803a944"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc2MjIwOA==", "bodyText": "we need to change the content method too, in order to apply above logic.\n.limit(Math.min(entryCount, capacity)) -> .limit(entryCount)\nthis means if capacity changes just before we send a batch, we'll not take it into effect for this batch but it'll be used when filling the buffer for the next batch.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r556762208", "createdAt": "2021-01-13T19:05:29Z", "author": {"login": "gurbuzali"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java", "diffHunk": "@@ -0,0 +1,502 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsResultEntry;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.internal.metrics.Probe;\n+import com.hazelcast.internal.metrics.ProbeUnit;\n+import com.hazelcast.internal.util.counters.Counter;\n+import com.hazelcast.internal.util.counters.SwCounter;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.core.Inbox;\n+import com.hazelcast.jet.core.Outbox;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.Watermark;\n+import com.hazelcast.jet.kinesis.KinesisSinks;\n+import com.hazelcast.jet.retry.RetryStrategy;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+import static java.lang.System.nanoTime;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.concurrent.TimeUnit.NANOSECONDS;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+public class KinesisSinkP<T> implements Processor {\n+\n+    /**\n+     * Each shard can ingest a maximum of a 1000 records per second.\n+     */\n+    private static final int MAX_RECORD_PER_SHARD_PER_SECOND = 1000;\n+\n+    /**\n+     * PutRecords requests are limited to 500 records.\n+     */\n+    private static final int MAX_RECORDS_IN_REQUEST = 500;\n+\n+    /**\n+     * Since we are using PutRecords for its batching effect, we don't want\n+     * the batch size to be so small as to negate all benefits.\n+     */\n+    private static final int MIN_RECORDS_IN_REQUEST = 10;\n+\n+    /**\n+     * The maximum allowed size of all the records in a PutRecords request,\n+     * including partition keys is 5M.\n+     */\n+    private static final int MAX_REQUEST_SIZE_IN_BYTES = 5 * 1024 * 1024;\n+\n+    /**\n+     * Kinesis always uses UTF-8 to encode strings.\n+     */\n+    private static final Charset KINESIS_STRING_ENCODING = StandardCharsets.UTF_8;\n+\n+    @Nonnull\n+    private final AmazonKinesisAsync kinesis;\n+    @Nonnull\n+    private final String stream;\n+    @Nullable\n+    private final ShardCountMonitor monitor;\n+    @Nonnull\n+    private final AtomicInteger shardCountProvider;\n+    @Nonnull\n+    private final Buffer<T> buffer;\n+\n+    @Probe(name = KinesisSinks.BATCH_SIZE_METRIC, unit = ProbeUnit.COUNT)\n+    private final Counter batchSizeMetric;\n+    @Probe(name = KinesisSinks.THROTTLING_SLEEP_METRIC, unit = ProbeUnit.MS)\n+    private final Counter sleepMetric = SwCounter.newSwCounter();\n+\n+    private ILogger logger;\n+    private KinesisHelper helper;\n+    private int shardCount;\n+    private int sinkCount;\n+\n+    private Future<PutRecordsResult> sendResult;\n+    private long nextSendTime = nanoTime();\n+    private final RetryTracker sendRetryTracker;\n+\n+    private final ThroughputController throughputController = new ThroughputController();\n+\n+    public KinesisSinkP(\n+            @Nonnull AmazonKinesisAsync kinesis,\n+            @Nonnull String stream,\n+            @Nonnull FunctionEx<T, String> keyFn,\n+            @Nonnull FunctionEx<T, byte[]> valueFn,\n+            @Nullable ShardCountMonitor monitor,\n+            @Nonnull AtomicInteger shardCountProvider,\n+            @Nonnull RetryStrategy retryStrategy\n+            ) {\n+        this.kinesis = kinesis;\n+        this.stream = stream;\n+        this.monitor = monitor;\n+        this.shardCountProvider = shardCountProvider;\n+        this.buffer = new Buffer<>(keyFn, valueFn);\n+        this.batchSizeMetric = SwCounter.newSwCounter(buffer.getCapacity());\n+        this.sendRetryTracker = new RetryTracker(retryStrategy);\n+    }\n+\n+    @Override\n+    public boolean isCooperative() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void init(@Nonnull Outbox outbox, @Nonnull Context context) {\n+        logger = context.logger();\n+        sinkCount = context.memberCount() * context.localParallelism();\n+        helper = new KinesisHelper(kinesis, stream);\n+    }\n+\n+    @Override\n+    public boolean tryProcessWatermark(@Nonnull Watermark watermark) {\n+        return true; //watermark ignored\n+    }\n+\n+    @Override\n+    public void process(int ordinal, @Nonnull Inbox inbox) {\n+        if (monitor != null) {\n+            monitor.run();\n+        }\n+\n+        updateThroughputLimitations();\n+\n+        if (sendResult != null) {\n+            checkIfSendingFinished();\n+        }\n+        if (sendResult == null) {\n+            initSending(inbox);\n+        }\n+    }\n+\n+    @Override\n+    public boolean complete() {\n+        if (sendResult != null) {\n+            checkIfSendingFinished();\n+        }\n+        if (sendResult == null) {\n+            if (buffer.isEmpty()) {\n+                return true;\n+            }\n+            initSending(null);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public boolean saveToSnapshot() {\n+        if (sendResult != null) {\n+            checkIfSendingFinished();\n+        }\n+        return sendResult == null;\n+    }\n+\n+    private void updateThroughputLimitations() {\n+        int newShardCount = shardCountProvider.get();\n+        if (newShardCount > 0 && shardCount != newShardCount) {\n+            buffer.setCapacity(throughputController.computeBatchSize(newShardCount, sinkCount));\n+            batchSizeMetric.set(buffer.getCapacity());\n+\n+            shardCount = newShardCount;\n+        }\n+    }\n+\n+    private void initSending(@Nullable Inbox inbox) {\n+        if (inbox != null) {\n+            bufferFromInbox(inbox);\n+        }\n+        attemptToDispatchBufferContent();\n+    }\n+\n+    private void bufferFromInbox(@Nonnull Inbox inbox) {\n+        if (buffer.isFull()) {\n+            return;\n+        }\n+\n+        while (true) {\n+            T t = (T) inbox.peek();\n+            if (t == null) {\n+                //no more items in inbox\n+                return;\n+            }\n+\n+            boolean canBeBuffered = buffer.add(t);\n+            if (canBeBuffered) {\n+                inbox.remove();\n+            } else {\n+                //no more room in buffer\n+                return;\n+            }\n+        }\n+    }\n+\n+    private void attemptToDispatchBufferContent() {\n+        if (buffer.isEmpty()) {\n+            return;\n+        }\n+\n+        long currentTime = nanoTime();\n+        if (currentTime < nextSendTime) {\n+            return;\n+        }\n+\n+        List<PutRecordsRequestEntry> entries = buffer.content();\n+        sendResult = helper.putRecordsAsync(entries);\n+        nextSendTime = currentTime;\n+    }\n+\n+    private void checkIfSendingFinished() {\n+        if (sendResult.isDone()) {\n+            PutRecordsResult result;\n+            try {\n+                result = helper.readResult(this.sendResult);\n+            } catch (ProvisionedThroughputExceededException pte) {\n+                dealWithThroughputExceeded(\"Data throughput rate exceeded. Backing off and retrying in %d ms\");\n+                return;\n+            } catch (SdkClientException sce) {\n+                dealWithSendFailure(sce);\n+                return;\n+            } catch (Throwable t) {\n+                throw rethrow(t);\n+            } finally {\n+                sendResult = null;\n+            }\n+\n+            pruneSentFromBuffer(result);\n+            if (result.getFailedRecordCount() > 0) {\n+                dealWithThroughputExceeded(\"Failed to send \" + result.getFailedRecordCount() + \" (out of \" +\n+                        result.getRecords().size() + \") record(s) to stream '\" + stream +\n+                        \"'. Sending will be retried in %d ms, message reordering is likely.\");\n+            } else {\n+                long sleepTimeNanos = throughputController.markSuccess();\n+                this.nextSendTime += sleepTimeNanos;\n+                this.sleepMetric.set(NANOSECONDS.toMillis(sleepTimeNanos));\n+                sendRetryTracker.reset();\n+            }\n+        }\n+    }\n+\n+    private void dealWithSendFailure(@Nonnull Exception failure) {\n+        sendRetryTracker.attemptFailed();\n+        if (sendRetryTracker.shouldTryAgain()) {\n+            long timeoutMillis = sendRetryTracker.getNextWaitTimeMillis();\n+            logger.warning(String.format(\"Failed to send records, will retry in %d ms. Cause: %s\",\n+                    timeoutMillis, failure.getMessage()));\n+            nextSendTime = System.nanoTime() + MILLISECONDS.toNanos(timeoutMillis);\n+        } else {\n+            throw rethrow(failure);\n+        }\n+\n+    }\n+\n+    private void dealWithThroughputExceeded(@Nonnull String message) {\n+        long sleepTimeNanos = throughputController.markFailure();\n+        this.nextSendTime += sleepTimeNanos;\n+        this.sleepMetric.set(NANOSECONDS.toMillis(sleepTimeNanos));\n+        logger.warning(String.format(message, NANOSECONDS.toMillis(sleepTimeNanos)));\n+    }\n+\n+    private void pruneSentFromBuffer(@Nullable PutRecordsResult result) {\n+        if (result == null) {\n+            return;\n+        }\n+\n+        List<PutRecordsResultEntry> resultEntries = result.getRecords();\n+        if (result.getFailedRecordCount() > 0) {\n+            for (int i = resultEntries.size() - 1; i >= 0; i--) {\n+                PutRecordsResultEntry resultEntry = resultEntries.get(i);\n+                if (resultEntry.getErrorCode() == null) {\n+                    buffer.remove(i);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjYwNzQxOA=="}, "originalCommit": {"oid": "812bec6d345b083efca7ade5606565263803a944"}, "originalPosition": 300}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc3MjQ4OA==", "bodyText": "we encode the key to UTF8 just to get the byte length of key, can't we use the unicodeCharsInKey*3. the actual byte length will be less than that amount most of the time. we don't need to be that accurate I think and this is on the hot path of the sink, it encodes each and every one of the passed key.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r556772488", "createdAt": "2021-01-13T19:23:14Z", "author": {"login": "gurbuzali"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java", "diffHunk": "@@ -0,0 +1,502 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsResultEntry;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.internal.metrics.Probe;\n+import com.hazelcast.internal.metrics.ProbeUnit;\n+import com.hazelcast.internal.util.counters.Counter;\n+import com.hazelcast.internal.util.counters.SwCounter;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.core.Inbox;\n+import com.hazelcast.jet.core.Outbox;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.Watermark;\n+import com.hazelcast.jet.kinesis.KinesisSinks;\n+import com.hazelcast.jet.retry.RetryStrategy;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+import static java.lang.System.nanoTime;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.concurrent.TimeUnit.NANOSECONDS;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+public class KinesisSinkP<T> implements Processor {\n+\n+    /**\n+     * Each shard can ingest a maximum of a 1000 records per second.\n+     */\n+    private static final int MAX_RECORD_PER_SHARD_PER_SECOND = 1000;\n+\n+    /**\n+     * PutRecords requests are limited to 500 records.\n+     */\n+    private static final int MAX_RECORDS_IN_REQUEST = 500;\n+\n+    /**\n+     * Since we are using PutRecords for its batching effect, we don't want\n+     * the batch size to be so small as to negate all benefits.\n+     */\n+    private static final int MIN_RECORDS_IN_REQUEST = 10;\n+\n+    /**\n+     * The maximum allowed size of all the records in a PutRecords request,\n+     * including partition keys is 5M.\n+     */\n+    private static final int MAX_REQUEST_SIZE_IN_BYTES = 5 * 1024 * 1024;\n+\n+    /**\n+     * Kinesis always uses UTF-8 to encode strings.\n+     */\n+    private static final Charset KINESIS_STRING_ENCODING = StandardCharsets.UTF_8;\n+\n+    @Nonnull\n+    private final AmazonKinesisAsync kinesis;\n+    @Nonnull\n+    private final String stream;\n+    @Nullable\n+    private final ShardCountMonitor monitor;\n+    @Nonnull\n+    private final AtomicInteger shardCountProvider;\n+    @Nonnull\n+    private final Buffer<T> buffer;\n+\n+    @Probe(name = KinesisSinks.BATCH_SIZE_METRIC, unit = ProbeUnit.COUNT)\n+    private final Counter batchSizeMetric;\n+    @Probe(name = KinesisSinks.THROTTLING_SLEEP_METRIC, unit = ProbeUnit.MS)\n+    private final Counter sleepMetric = SwCounter.newSwCounter();\n+\n+    private ILogger logger;\n+    private KinesisHelper helper;\n+    private int shardCount;\n+    private int sinkCount;\n+\n+    private Future<PutRecordsResult> sendResult;\n+    private long nextSendTime = nanoTime();\n+    private final RetryTracker sendRetryTracker;\n+\n+    private final ThroughputController throughputController = new ThroughputController();\n+\n+    public KinesisSinkP(\n+            @Nonnull AmazonKinesisAsync kinesis,\n+            @Nonnull String stream,\n+            @Nonnull FunctionEx<T, String> keyFn,\n+            @Nonnull FunctionEx<T, byte[]> valueFn,\n+            @Nullable ShardCountMonitor monitor,\n+            @Nonnull AtomicInteger shardCountProvider,\n+            @Nonnull RetryStrategy retryStrategy\n+            ) {\n+        this.kinesis = kinesis;\n+        this.stream = stream;\n+        this.monitor = monitor;\n+        this.shardCountProvider = shardCountProvider;\n+        this.buffer = new Buffer<>(keyFn, valueFn);\n+        this.batchSizeMetric = SwCounter.newSwCounter(buffer.getCapacity());\n+        this.sendRetryTracker = new RetryTracker(retryStrategy);\n+    }\n+\n+    @Override\n+    public boolean isCooperative() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void init(@Nonnull Outbox outbox, @Nonnull Context context) {\n+        logger = context.logger();\n+        sinkCount = context.memberCount() * context.localParallelism();\n+        helper = new KinesisHelper(kinesis, stream);\n+    }\n+\n+    @Override\n+    public boolean tryProcessWatermark(@Nonnull Watermark watermark) {\n+        return true; //watermark ignored\n+    }\n+\n+    @Override\n+    public void process(int ordinal, @Nonnull Inbox inbox) {\n+        if (monitor != null) {\n+            monitor.run();\n+        }\n+\n+        updateThroughputLimitations();\n+\n+        if (sendResult != null) {\n+            checkIfSendingFinished();\n+        }\n+        if (sendResult == null) {\n+            initSending(inbox);\n+        }\n+    }\n+\n+    @Override\n+    public boolean complete() {\n+        if (sendResult != null) {\n+            checkIfSendingFinished();\n+        }\n+        if (sendResult == null) {\n+            if (buffer.isEmpty()) {\n+                return true;\n+            }\n+            initSending(null);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public boolean saveToSnapshot() {\n+        if (sendResult != null) {\n+            checkIfSendingFinished();\n+        }\n+        return sendResult == null;\n+    }\n+\n+    private void updateThroughputLimitations() {\n+        int newShardCount = shardCountProvider.get();\n+        if (newShardCount > 0 && shardCount != newShardCount) {\n+            buffer.setCapacity(throughputController.computeBatchSize(newShardCount, sinkCount));\n+            batchSizeMetric.set(buffer.getCapacity());\n+\n+            shardCount = newShardCount;\n+        }\n+    }\n+\n+    private void initSending(@Nullable Inbox inbox) {\n+        if (inbox != null) {\n+            bufferFromInbox(inbox);\n+        }\n+        attemptToDispatchBufferContent();\n+    }\n+\n+    private void bufferFromInbox(@Nonnull Inbox inbox) {\n+        if (buffer.isFull()) {\n+            return;\n+        }\n+\n+        while (true) {\n+            T t = (T) inbox.peek();\n+            if (t == null) {\n+                //no more items in inbox\n+                return;\n+            }\n+\n+            boolean canBeBuffered = buffer.add(t);\n+            if (canBeBuffered) {\n+                inbox.remove();\n+            } else {\n+                //no more room in buffer\n+                return;\n+            }\n+        }\n+    }\n+\n+    private void attemptToDispatchBufferContent() {\n+        if (buffer.isEmpty()) {\n+            return;\n+        }\n+\n+        long currentTime = nanoTime();\n+        if (currentTime < nextSendTime) {\n+            return;\n+        }\n+\n+        List<PutRecordsRequestEntry> entries = buffer.content();\n+        sendResult = helper.putRecordsAsync(entries);\n+        nextSendTime = currentTime;\n+    }\n+\n+    private void checkIfSendingFinished() {\n+        if (sendResult.isDone()) {\n+            PutRecordsResult result;\n+            try {\n+                result = helper.readResult(this.sendResult);\n+            } catch (ProvisionedThroughputExceededException pte) {\n+                dealWithThroughputExceeded(\"Data throughput rate exceeded. Backing off and retrying in %d ms\");\n+                return;\n+            } catch (SdkClientException sce) {\n+                dealWithSendFailure(sce);\n+                return;\n+            } catch (Throwable t) {\n+                throw rethrow(t);\n+            } finally {\n+                sendResult = null;\n+            }\n+\n+            pruneSentFromBuffer(result);\n+            if (result.getFailedRecordCount() > 0) {\n+                dealWithThroughputExceeded(\"Failed to send \" + result.getFailedRecordCount() + \" (out of \" +\n+                        result.getRecords().size() + \") record(s) to stream '\" + stream +\n+                        \"'. Sending will be retried in %d ms, message reordering is likely.\");\n+            } else {\n+                long sleepTimeNanos = throughputController.markSuccess();\n+                this.nextSendTime += sleepTimeNanos;\n+                this.sleepMetric.set(NANOSECONDS.toMillis(sleepTimeNanos));\n+                sendRetryTracker.reset();\n+            }\n+        }\n+    }\n+\n+    private void dealWithSendFailure(@Nonnull Exception failure) {\n+        sendRetryTracker.attemptFailed();\n+        if (sendRetryTracker.shouldTryAgain()) {\n+            long timeoutMillis = sendRetryTracker.getNextWaitTimeMillis();\n+            logger.warning(String.format(\"Failed to send records, will retry in %d ms. Cause: %s\",\n+                    timeoutMillis, failure.getMessage()));\n+            nextSendTime = System.nanoTime() + MILLISECONDS.toNanos(timeoutMillis);\n+        } else {\n+            throw rethrow(failure);\n+        }\n+\n+    }\n+\n+    private void dealWithThroughputExceeded(@Nonnull String message) {\n+        long sleepTimeNanos = throughputController.markFailure();\n+        this.nextSendTime += sleepTimeNanos;\n+        this.sleepMetric.set(NANOSECONDS.toMillis(sleepTimeNanos));\n+        logger.warning(String.format(message, NANOSECONDS.toMillis(sleepTimeNanos)));\n+    }\n+\n+    private void pruneSentFromBuffer(@Nullable PutRecordsResult result) {\n+        if (result == null) {\n+            return;\n+        }\n+\n+        List<PutRecordsResultEntry> resultEntries = result.getRecords();\n+        if (result.getFailedRecordCount() > 0) {\n+            for (int i = resultEntries.size() - 1; i >= 0; i--) {\n+                PutRecordsResultEntry resultEntry = resultEntries.get(i);\n+                if (resultEntry.getErrorCode() == null) {\n+                    buffer.remove(i);\n+                }\n+            }\n+        } else {\n+            buffer.remove(0, resultEntries.size());\n+        }\n+    }\n+\n+    private static class Buffer<T> {\n+\n+        private final FunctionEx<T, String> keyFn;\n+        private final FunctionEx<T, byte[]> valueFn;\n+\n+        private final BufferEntry[] entries;\n+        private int entryCount;\n+        private int totalEntrySize;\n+        private int capacity;\n+\n+        Buffer(FunctionEx<T, String> keyFn, FunctionEx<T, byte[]> valueFn) {\n+            this.keyFn = keyFn;\n+            this.valueFn = valueFn;\n+            this.entries = initEntries();\n+            this.capacity = entries.length;\n+        }\n+\n+        public int getCapacity() {\n+            return capacity;\n+        }\n+\n+        void setCapacity(int capacity) {\n+            if (capacity < 0 || capacity > entries.length) {\n+                throw new IllegalArgumentException(\"Capacity limited to [0, \" + entries.length + \")\");\n+            }\n+            this.capacity = capacity;\n+        }\n+\n+        boolean add(T item) {\n+            if (isFull()) {\n+                return false;\n+            }\n+\n+            String key = keyFn.apply(item);\n+            if (key.isEmpty()) {\n+                throw new JetException(\"Key empty\");\n+            }\n+            int unicodeCharsInKey = key.length();\n+            if (unicodeCharsInKey > KinesisSinks.MAXIMUM_KEY_LENGTH) {\n+                throw new JetException(\"Key too long\");\n+            }\n+            int keyLength = getKeyLength(key);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "812bec6d345b083efca7ade5606565263803a944"}, "originalPosition": 349}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc3MzYwNA==", "bodyText": "or we can just use unicodeCharsInKey as byte length without multiplying it with 3. If the encoded length is greater then it'll fail when we actually send the entry to kinesis anyway.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r556773604", "createdAt": "2021-01-13T19:25:14Z", "author": {"login": "gurbuzali"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java", "diffHunk": "@@ -0,0 +1,502 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsResultEntry;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.internal.metrics.Probe;\n+import com.hazelcast.internal.metrics.ProbeUnit;\n+import com.hazelcast.internal.util.counters.Counter;\n+import com.hazelcast.internal.util.counters.SwCounter;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.core.Inbox;\n+import com.hazelcast.jet.core.Outbox;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.Watermark;\n+import com.hazelcast.jet.kinesis.KinesisSinks;\n+import com.hazelcast.jet.retry.RetryStrategy;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+import static java.lang.System.nanoTime;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.concurrent.TimeUnit.NANOSECONDS;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+public class KinesisSinkP<T> implements Processor {\n+\n+    /**\n+     * Each shard can ingest a maximum of a 1000 records per second.\n+     */\n+    private static final int MAX_RECORD_PER_SHARD_PER_SECOND = 1000;\n+\n+    /**\n+     * PutRecords requests are limited to 500 records.\n+     */\n+    private static final int MAX_RECORDS_IN_REQUEST = 500;\n+\n+    /**\n+     * Since we are using PutRecords for its batching effect, we don't want\n+     * the batch size to be so small as to negate all benefits.\n+     */\n+    private static final int MIN_RECORDS_IN_REQUEST = 10;\n+\n+    /**\n+     * The maximum allowed size of all the records in a PutRecords request,\n+     * including partition keys is 5M.\n+     */\n+    private static final int MAX_REQUEST_SIZE_IN_BYTES = 5 * 1024 * 1024;\n+\n+    /**\n+     * Kinesis always uses UTF-8 to encode strings.\n+     */\n+    private static final Charset KINESIS_STRING_ENCODING = StandardCharsets.UTF_8;\n+\n+    @Nonnull\n+    private final AmazonKinesisAsync kinesis;\n+    @Nonnull\n+    private final String stream;\n+    @Nullable\n+    private final ShardCountMonitor monitor;\n+    @Nonnull\n+    private final AtomicInteger shardCountProvider;\n+    @Nonnull\n+    private final Buffer<T> buffer;\n+\n+    @Probe(name = KinesisSinks.BATCH_SIZE_METRIC, unit = ProbeUnit.COUNT)\n+    private final Counter batchSizeMetric;\n+    @Probe(name = KinesisSinks.THROTTLING_SLEEP_METRIC, unit = ProbeUnit.MS)\n+    private final Counter sleepMetric = SwCounter.newSwCounter();\n+\n+    private ILogger logger;\n+    private KinesisHelper helper;\n+    private int shardCount;\n+    private int sinkCount;\n+\n+    private Future<PutRecordsResult> sendResult;\n+    private long nextSendTime = nanoTime();\n+    private final RetryTracker sendRetryTracker;\n+\n+    private final ThroughputController throughputController = new ThroughputController();\n+\n+    public KinesisSinkP(\n+            @Nonnull AmazonKinesisAsync kinesis,\n+            @Nonnull String stream,\n+            @Nonnull FunctionEx<T, String> keyFn,\n+            @Nonnull FunctionEx<T, byte[]> valueFn,\n+            @Nullable ShardCountMonitor monitor,\n+            @Nonnull AtomicInteger shardCountProvider,\n+            @Nonnull RetryStrategy retryStrategy\n+            ) {\n+        this.kinesis = kinesis;\n+        this.stream = stream;\n+        this.monitor = monitor;\n+        this.shardCountProvider = shardCountProvider;\n+        this.buffer = new Buffer<>(keyFn, valueFn);\n+        this.batchSizeMetric = SwCounter.newSwCounter(buffer.getCapacity());\n+        this.sendRetryTracker = new RetryTracker(retryStrategy);\n+    }\n+\n+    @Override\n+    public boolean isCooperative() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void init(@Nonnull Outbox outbox, @Nonnull Context context) {\n+        logger = context.logger();\n+        sinkCount = context.memberCount() * context.localParallelism();\n+        helper = new KinesisHelper(kinesis, stream);\n+    }\n+\n+    @Override\n+    public boolean tryProcessWatermark(@Nonnull Watermark watermark) {\n+        return true; //watermark ignored\n+    }\n+\n+    @Override\n+    public void process(int ordinal, @Nonnull Inbox inbox) {\n+        if (monitor != null) {\n+            monitor.run();\n+        }\n+\n+        updateThroughputLimitations();\n+\n+        if (sendResult != null) {\n+            checkIfSendingFinished();\n+        }\n+        if (sendResult == null) {\n+            initSending(inbox);\n+        }\n+    }\n+\n+    @Override\n+    public boolean complete() {\n+        if (sendResult != null) {\n+            checkIfSendingFinished();\n+        }\n+        if (sendResult == null) {\n+            if (buffer.isEmpty()) {\n+                return true;\n+            }\n+            initSending(null);\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    public boolean saveToSnapshot() {\n+        if (sendResult != null) {\n+            checkIfSendingFinished();\n+        }\n+        return sendResult == null;\n+    }\n+\n+    private void updateThroughputLimitations() {\n+        int newShardCount = shardCountProvider.get();\n+        if (newShardCount > 0 && shardCount != newShardCount) {\n+            buffer.setCapacity(throughputController.computeBatchSize(newShardCount, sinkCount));\n+            batchSizeMetric.set(buffer.getCapacity());\n+\n+            shardCount = newShardCount;\n+        }\n+    }\n+\n+    private void initSending(@Nullable Inbox inbox) {\n+        if (inbox != null) {\n+            bufferFromInbox(inbox);\n+        }\n+        attemptToDispatchBufferContent();\n+    }\n+\n+    private void bufferFromInbox(@Nonnull Inbox inbox) {\n+        if (buffer.isFull()) {\n+            return;\n+        }\n+\n+        while (true) {\n+            T t = (T) inbox.peek();\n+            if (t == null) {\n+                //no more items in inbox\n+                return;\n+            }\n+\n+            boolean canBeBuffered = buffer.add(t);\n+            if (canBeBuffered) {\n+                inbox.remove();\n+            } else {\n+                //no more room in buffer\n+                return;\n+            }\n+        }\n+    }\n+\n+    private void attemptToDispatchBufferContent() {\n+        if (buffer.isEmpty()) {\n+            return;\n+        }\n+\n+        long currentTime = nanoTime();\n+        if (currentTime < nextSendTime) {\n+            return;\n+        }\n+\n+        List<PutRecordsRequestEntry> entries = buffer.content();\n+        sendResult = helper.putRecordsAsync(entries);\n+        nextSendTime = currentTime;\n+    }\n+\n+    private void checkIfSendingFinished() {\n+        if (sendResult.isDone()) {\n+            PutRecordsResult result;\n+            try {\n+                result = helper.readResult(this.sendResult);\n+            } catch (ProvisionedThroughputExceededException pte) {\n+                dealWithThroughputExceeded(\"Data throughput rate exceeded. Backing off and retrying in %d ms\");\n+                return;\n+            } catch (SdkClientException sce) {\n+                dealWithSendFailure(sce);\n+                return;\n+            } catch (Throwable t) {\n+                throw rethrow(t);\n+            } finally {\n+                sendResult = null;\n+            }\n+\n+            pruneSentFromBuffer(result);\n+            if (result.getFailedRecordCount() > 0) {\n+                dealWithThroughputExceeded(\"Failed to send \" + result.getFailedRecordCount() + \" (out of \" +\n+                        result.getRecords().size() + \") record(s) to stream '\" + stream +\n+                        \"'. Sending will be retried in %d ms, message reordering is likely.\");\n+            } else {\n+                long sleepTimeNanos = throughputController.markSuccess();\n+                this.nextSendTime += sleepTimeNanos;\n+                this.sleepMetric.set(NANOSECONDS.toMillis(sleepTimeNanos));\n+                sendRetryTracker.reset();\n+            }\n+        }\n+    }\n+\n+    private void dealWithSendFailure(@Nonnull Exception failure) {\n+        sendRetryTracker.attemptFailed();\n+        if (sendRetryTracker.shouldTryAgain()) {\n+            long timeoutMillis = sendRetryTracker.getNextWaitTimeMillis();\n+            logger.warning(String.format(\"Failed to send records, will retry in %d ms. Cause: %s\",\n+                    timeoutMillis, failure.getMessage()));\n+            nextSendTime = System.nanoTime() + MILLISECONDS.toNanos(timeoutMillis);\n+        } else {\n+            throw rethrow(failure);\n+        }\n+\n+    }\n+\n+    private void dealWithThroughputExceeded(@Nonnull String message) {\n+        long sleepTimeNanos = throughputController.markFailure();\n+        this.nextSendTime += sleepTimeNanos;\n+        this.sleepMetric.set(NANOSECONDS.toMillis(sleepTimeNanos));\n+        logger.warning(String.format(message, NANOSECONDS.toMillis(sleepTimeNanos)));\n+    }\n+\n+    private void pruneSentFromBuffer(@Nullable PutRecordsResult result) {\n+        if (result == null) {\n+            return;\n+        }\n+\n+        List<PutRecordsResultEntry> resultEntries = result.getRecords();\n+        if (result.getFailedRecordCount() > 0) {\n+            for (int i = resultEntries.size() - 1; i >= 0; i--) {\n+                PutRecordsResultEntry resultEntry = resultEntries.get(i);\n+                if (resultEntry.getErrorCode() == null) {\n+                    buffer.remove(i);\n+                }\n+            }\n+        } else {\n+            buffer.remove(0, resultEntries.size());\n+        }\n+    }\n+\n+    private static class Buffer<T> {\n+\n+        private final FunctionEx<T, String> keyFn;\n+        private final FunctionEx<T, byte[]> valueFn;\n+\n+        private final BufferEntry[] entries;\n+        private int entryCount;\n+        private int totalEntrySize;\n+        private int capacity;\n+\n+        Buffer(FunctionEx<T, String> keyFn, FunctionEx<T, byte[]> valueFn) {\n+            this.keyFn = keyFn;\n+            this.valueFn = valueFn;\n+            this.entries = initEntries();\n+            this.capacity = entries.length;\n+        }\n+\n+        public int getCapacity() {\n+            return capacity;\n+        }\n+\n+        void setCapacity(int capacity) {\n+            if (capacity < 0 || capacity > entries.length) {\n+                throw new IllegalArgumentException(\"Capacity limited to [0, \" + entries.length + \")\");\n+            }\n+            this.capacity = capacity;\n+        }\n+\n+        boolean add(T item) {\n+            if (isFull()) {\n+                return false;\n+            }\n+\n+            String key = keyFn.apply(item);\n+            if (key.isEmpty()) {\n+                throw new JetException(\"Key empty\");\n+            }\n+            int unicodeCharsInKey = key.length();\n+            if (unicodeCharsInKey > KinesisSinks.MAXIMUM_KEY_LENGTH) {\n+                throw new JetException(\"Key too long\");\n+            }\n+            int keyLength = getKeyLength(key);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc3MjQ4OA=="}, "originalCommit": {"oid": "812bec6d345b083efca7ade5606565263803a944"}, "originalPosition": 349}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc3OTc0MA==", "bodyText": "we can create a noop ShardCountMonitor and pass it for the other processors instead of a null value to eliminate the null check for each run. We can also move the AtomicInteger shardCount = new AtomicInteger(); into ShardCountMonitor and access the shardCount through it.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r556779740", "createdAt": "2021-01-13T19:36:13Z", "author": {"login": "gurbuzali"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkPSupplier.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.ProcessorSupplier;\n+import com.hazelcast.jet.retry.RetryStrategy;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.util.Collection;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+public class KinesisSinkPSupplier<T> implements ProcessorSupplier {\n+\n+    private static final long serialVersionUID = 1L;\n+\n+    @Nonnull\n+    private final AwsConfig awsConfig;\n+    @Nonnull\n+    private final String stream;\n+    @Nonnull\n+    private final FunctionEx<T, String> keyFn;\n+    @Nonnull\n+    private final FunctionEx<T, byte[]> valueFn;\n+    @Nonnull\n+    private final RetryStrategy retryStrategy;\n+\n+    private transient AmazonKinesisAsync client;\n+    private transient int memberCount;\n+    private transient ILogger logger;\n+\n+    public KinesisSinkPSupplier(\n+            @Nonnull AwsConfig awsConfig,\n+            @Nonnull String stream,\n+            @Nonnull FunctionEx<T, String> keyFn,\n+            @Nonnull FunctionEx<T, byte[]> valueFn,\n+            @Nonnull RetryStrategy retryStrategy\n+    ) {\n+        this.awsConfig = awsConfig;\n+        this.stream = stream;\n+        this.keyFn = keyFn;\n+        this.valueFn = valueFn;\n+        this.retryStrategy = retryStrategy;\n+    }\n+\n+    @Override\n+    public void init(@Nonnull Context context) {\n+        this.memberCount = context.memberCount();\n+        this.logger = context.logger();\n+        this.client = awsConfig.buildClient();\n+    }\n+\n+    @Nonnull\n+    @Override\n+    public Collection<? extends Processor> get(int count) {\n+        AtomicInteger shardCount = new AtomicInteger();\n+\n+        ShardCountMonitor shardCountMonitor = new ShardCountMonitor(\n+                shardCount,\n+                memberCount,\n+                client,\n+                stream,\n+                retryStrategy,\n+                logger\n+        );\n+\n+        return IntStream.range(0, count)\n+                .mapToObj(i -> new KinesisSinkP<>(\n+                        client,\n+                        stream,\n+                        keyFn,\n+                        valueFn,\n+                        i == 0 ? shardCountMonitor : null,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "812bec6d345b083efca7ade5606565263803a944"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc4NTU5NA==", "bodyText": "I've just implemented this, so leaving the comment as an explanation", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r556785594", "createdAt": "2021-01-13T19:46:26Z", "author": {"login": "gurbuzali"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkPSupplier.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.ProcessorSupplier;\n+import com.hazelcast.jet.retry.RetryStrategy;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.util.Collection;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.IntStream;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+public class KinesisSinkPSupplier<T> implements ProcessorSupplier {\n+\n+    private static final long serialVersionUID = 1L;\n+\n+    @Nonnull\n+    private final AwsConfig awsConfig;\n+    @Nonnull\n+    private final String stream;\n+    @Nonnull\n+    private final FunctionEx<T, String> keyFn;\n+    @Nonnull\n+    private final FunctionEx<T, byte[]> valueFn;\n+    @Nonnull\n+    private final RetryStrategy retryStrategy;\n+\n+    private transient AmazonKinesisAsync client;\n+    private transient int memberCount;\n+    private transient ILogger logger;\n+\n+    public KinesisSinkPSupplier(\n+            @Nonnull AwsConfig awsConfig,\n+            @Nonnull String stream,\n+            @Nonnull FunctionEx<T, String> keyFn,\n+            @Nonnull FunctionEx<T, byte[]> valueFn,\n+            @Nonnull RetryStrategy retryStrategy\n+    ) {\n+        this.awsConfig = awsConfig;\n+        this.stream = stream;\n+        this.keyFn = keyFn;\n+        this.valueFn = valueFn;\n+        this.retryStrategy = retryStrategy;\n+    }\n+\n+    @Override\n+    public void init(@Nonnull Context context) {\n+        this.memberCount = context.memberCount();\n+        this.logger = context.logger();\n+        this.client = awsConfig.buildClient();\n+    }\n+\n+    @Nonnull\n+    @Override\n+    public Collection<? extends Processor> get(int count) {\n+        AtomicInteger shardCount = new AtomicInteger();\n+\n+        ShardCountMonitor shardCountMonitor = new ShardCountMonitor(\n+                shardCount,\n+                memberCount,\n+                client,\n+                stream,\n+                retryStrategy,\n+                logger\n+        );\n+\n+        return IntStream.range(0, count)\n+                .mapToObj(i -> new KinesisSinkP<>(\n+                        client,\n+                        stream,\n+                        keyFn,\n+                        valueFn,\n+                        i == 0 ? shardCountMonitor : null,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njc3OTc0MA=="}, "originalCommit": {"oid": "812bec6d345b083efca7ade5606565263803a944"}, "originalPosition": 93}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "16ec9635baf33d43de2211d28c66694d0f9d0a5d", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/16ec9635baf33d43de2211d28c66694d0f9d0a5d", "committedDate": "2021-01-14T07:54:56Z", "message": "Address review concerns"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a90f59803307037f4106ca02a0c4e1aba21ab552", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/a90f59803307037f4106ca02a0c4e1aba21ab552", "committedDate": "2021-01-14T09:32:19Z", "message": "Fix bug, improve tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzNDM4NzAz", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#pullrequestreview-553438703", "createdAt": "2020-12-16T08:07:02Z", "commit": {"oid": "17d6e220ad5dc7870e7199811ac6ae0441719d32"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwODowNzowM1rOIG4pCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNFQxMDozODowM1rOITe1Yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA5MDM3Ng==", "bodyText": "This class might be easier with IdentifiedDataSerializable", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r544090376", "createdAt": "2020-12-16T08:07:03Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSourceP.java", "diffHunk": "@@ -0,0 +1,323 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.Shard;\n+import com.hazelcast.jet.Traverser;\n+import com.hazelcast.jet.Traversers;\n+import com.hazelcast.jet.core.AbstractProcessor;\n+import com.hazelcast.jet.core.BroadcastKey;\n+import com.hazelcast.jet.core.EventTimeMapper;\n+import com.hazelcast.jet.core.EventTimePolicy;\n+import com.hazelcast.jet.retry.RetryStrategy;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Objects;\n+import java.util.Queue;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static com.hazelcast.jet.Traversers.traverseStream;\n+import static com.hazelcast.jet.Util.entry;\n+import static com.hazelcast.jet.core.BroadcastKey.broadcastKey;\n+import static com.hazelcast.jet.impl.util.Util.toLocalTime;\n+import static com.hazelcast.jet.kinesis.impl.KinesisHelper.shardBelongsToRange;\n+\n+public class KinesisSourceP extends AbstractProcessor {\n+\n+    @Nonnull\n+    private final AmazonKinesisAsync kinesis;\n+    @Nonnull\n+    private final String stream;\n+    @Nonnull\n+    private final EventTimeMapper<? super Entry<String, byte[]>> eventTimeMapper;\n+    @Nonnull\n+    private final HashRange hashRange;\n+    @Nonnull\n+    private final ShardStates shardStates = new ShardStates();\n+    @Nonnull\n+    private final Queue<Shard> shardQueue;\n+    @Nullable\n+    private final RangeMonitor monitor;\n+    @Nonnull\n+    private final List<ShardReader> shardReaders = new ArrayList<>();\n+    @Nonnull\n+    private final RetryStrategy retryStrategy;\n+\n+    private int id;\n+    private ILogger logger;\n+\n+    private Traverser<Object> traverser = Traversers.empty();\n+    private Traverser<Entry<BroadcastKey<String>, Object[]>> snapshotTraverser;\n+\n+    private int nextReader;\n+\n+    public KinesisSourceP(\n+            @Nonnull AmazonKinesisAsync kinesis,\n+            @Nonnull String stream,\n+            @Nonnull EventTimePolicy<? super Entry<String, byte[]>> eventTimePolicy,\n+            @Nonnull HashRange hashRange,\n+            @Nonnull Queue<Shard> shardQueue,\n+            @Nullable RangeMonitor monitor,\n+            @Nonnull RetryStrategy retryStrategy\n+            ) {\n+        this.kinesis = Objects.requireNonNull(kinesis, \"kinesis\");\n+        this.stream = Objects.requireNonNull(stream, \"stream\");\n+        this.eventTimeMapper = new EventTimeMapper<>(eventTimePolicy);\n+        this.hashRange = Objects.requireNonNull(hashRange, \"hashRange\");\n+        this.shardQueue = shardQueue;\n+        this.monitor = monitor;\n+        this.retryStrategy = retryStrategy;\n+    }\n+\n+    @Override\n+    protected void init(@Nonnull Context context) throws Exception {\n+        super.init(context);\n+\n+        logger = context.logger();\n+        id = context.globalProcessorIndex();\n+\n+        logger.info(\"Processor \" + id + \" handles \" + hashRange);\n+    }\n+\n+    @Override\n+    public boolean complete() {\n+        if (!emitFromTraverser(traverser)) {\n+            return false;\n+        }\n+\n+        runMonitor();\n+        checkForNewShards();\n+        runReaders();\n+\n+        return false;\n+    }\n+\n+    private void runMonitor() {\n+        if (monitor != null) {\n+            monitor.run();\n+        }\n+    }\n+\n+    private void checkForNewShards() {\n+        Shard shard = shardQueue.poll();\n+        if (shard != null) {\n+            addShardReader(shard);\n+        }\n+    }\n+\n+    private void runReaders() {\n+        if (!shardReaders.isEmpty()) {\n+            long currentTime = System.nanoTime();\n+            for (int i = 0; i < shardReaders.size(); i++) {\n+                int currentReader = nextReader;\n+                ShardReader reader = shardReaders.get(currentReader);\n+                nextReader = incrCircular(currentReader, shardReaders.size());\n+\n+                ShardReader.Result result = reader.probe(currentTime);\n+                if (ShardReader.Result.HAS_DATA.equals(result)) {\n+                    traverser = reader.clearData()\n+                            .flatMap(record -> eventTimeMapper.flatMapEvent(\n+                                    entry(record.getPartitionKey(), record.getData().array()), //todo: shady?\n+                                    currentReader,\n+                                    record.getApproximateArrivalTimestamp().getTime()\n+                            ));\n+                    Long watermark = eventTimeMapper.getWatermark(currentReader);\n+                    watermark = watermark < 0 ? null : watermark;\n+                    shardStates.update(reader.getShard(), reader.getLastSeenSeqNo(), watermark);\n+                    emitFromTraverser(traverser);\n+                    return;\n+                } else if (ShardReader.Result.CLOSED.equals(result)) {\n+                    Shard shard = reader.getShard();\n+                    logger.info(\"Shard \" + shard.getShardId() + \" of stream \" + stream + \" closed\");\n+                    shardStates.close(shard);\n+                    removeShardReader(currentReader);\n+                    nextReader = 0;\n+                    return;\n+                }\n+            }\n+        }\n+\n+        traverser = eventTimeMapper.flatMapIdle();\n+        emitFromTraverser(traverser);\n+    }\n+\n+    @Override\n+    public boolean saveToSnapshot() {\n+        if (!emitFromTraverser(traverser)) {\n+            return false;\n+        }\n+\n+        if (snapshotTraverser == null) {\n+            snapshotTraverser = traverseStream(shardStates.snapshotEntries())\n+                    .onFirstNull(() -> {\n+                        snapshotTraverser = null;\n+                        if (getLogger().isFinestEnabled()) {\n+                            getLogger().finest(\"Finished saving snapshot. Saved shard states: \" + shardStates);\n+                        }\n+                    });\n+        }\n+        return emitFromTraverserToSnapshot(snapshotTraverser);\n+    }\n+\n+    @Override\n+    protected void restoreFromSnapshot(@Nonnull Object key, @Nonnull Object value) {\n+        String shardId = ((BroadcastKey<String>) key).key();\n+\n+        Object[] shardState = (Object[]) value;\n+        String startingHashKey = ShardStates.startingHashKey(shardState);\n+        shardBelongsToRange(startingHashKey, hashRange);\n+        if (shardBelongsToRange(startingHashKey, hashRange)) {\n+            boolean closed = ShardStates.closed(shardState);\n+            String seqNo = ShardStates.lastSeenSeqNo(shardState);\n+            Long watermark = ShardStates.watermark(shardState);\n+            shardStates.update(shardId, startingHashKey, closed, seqNo, watermark);\n+        }\n+    }\n+\n+    private void addShardReader(Shard shard) {\n+        String shardId = shard.getShardId();\n+        Object[] shardState = shardStates.get(shardId);\n+        if (!ShardStates.closed(shardState)) {\n+            int readerIndex = shardReaders.size();\n+\n+            String lastSeenSeqNo = ShardStates.lastSeenSeqNo(shardState);\n+            shardReaders.add(initShardReader(shard, lastSeenSeqNo));\n+\n+            eventTimeMapper.addPartitions(1);\n+\n+            Long watermark = ShardStates.watermark(shardState);\n+            if (watermark != null) {\n+                eventTimeMapper.restoreWatermark(readerIndex, watermark);\n+            }\n+        }\n+    }\n+\n+    private void removeShardReader(int index) {\n+        shardReaders.remove(index);\n+        eventTimeMapper.removePartition(index);\n+    }\n+\n+    @Nonnull\n+    private ShardReader initShardReader(Shard shard, String lastSeenSeqNo) {\n+        logger.info(\"Shard \" + shard.getShardId() + \" of stream \" + stream + \" assigned to processor instance \" + id);\n+        return new ShardReader(kinesis, stream, shard, lastSeenSeqNo, retryStrategy, logger);\n+    }\n+\n+    private static int incrCircular(int v, int limit) {\n+        v++;\n+        if (v == limit) {\n+            v = 0;\n+        }\n+        return v;\n+    }\n+\n+    private static class ShardStates {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17d6e220ad5dc7870e7199811ac6ae0441719d32"}, "originalPosition": 236}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA5MDg1MA==", "bodyText": "We should also check for expired shards and add them to the queue so that processors can remove the entry for them for the snapshot.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r544090850", "createdAt": "2020-12-16T08:07:51Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/RangeMonitor.java", "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ListShardsResult;\n+import com.amazonaws.services.kinesis.model.Shard;\n+import com.hazelcast.jet.retry.RetryStrategy;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.Future;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+import static com.hazelcast.jet.kinesis.impl.KinesisHelper.shardBelongsToRange;\n+import static java.util.concurrent.TimeUnit.MILLISECONDS;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+import static java.util.stream.Collectors.joining;\n+import static java.util.stream.Collectors.toCollection;\n+import static java.util.stream.Collectors.toList;\n+\n+public class RangeMonitor extends AbstractShardWorker {\n+\n+    /**\n+     * ListStreams operations are limited to 100 per second, per data stream\n+     */\n+    private static final int SHARD_LISTINGS_ALLOWED_PER_SECOND = 100;\n+\n+    /**\n+     * We don't want to issue shard listing requests at the peak allowed rate.\n+     */\n+    private static final double RATIO_OF_SHARD_LISTING_RATE_UTILIZED = 0.1;\n+\n+    private final Set<String> knownShards = new HashSet<>();\n+    private final HashRange coveredRange;\n+    private final HashRange[] rangePartitions;\n+    private final Queue<Shard>[] shardQueues;\n+    private final RandomizedRateTracker listShardsRateTracker;\n+    private final RetryTracker listShardRetryTracker;\n+\n+    private String nextToken;\n+    private Future<ListShardsResult> listShardResult;\n+    private long nextListShardsTime;\n+\n+    public RangeMonitor(\n+            int totalInstances,\n+            AmazonKinesisAsync kinesis,\n+            String stream,\n+            HashRange coveredRange,\n+            HashRange[] rangePartitions,\n+            Queue<Shard>[] shardQueues,\n+            RetryStrategy retryStrategy,\n+            ILogger logger\n+    ) {\n+        super(kinesis, stream, logger);\n+        this.coveredRange = coveredRange;\n+        this.rangePartitions = rangePartitions;\n+        this.shardQueues = shardQueues;\n+        this.listShardRetryTracker = new RetryTracker(retryStrategy);\n+        this.listShardsRateTracker = initRandomizedTracker(totalInstances);\n+        this.nextListShardsTime = System.nanoTime() + listShardsRateTracker.next();\n+    }\n+\n+    public void run() {\n+        if (listShardResult == null) {\n+            initShardListing();\n+        } else {\n+            checkForNewShards();\n+        }\n+    }\n+\n+    private void initShardListing() {\n+        long currentTime = System.nanoTime();\n+        if (currentTime < nextListShardsTime) {\n+            return;\n+        }\n+        listShardResult = helper.listShardsAsync(nextToken);\n+        nextListShardsTime = currentTime + listShardsRateTracker.next();\n+    }\n+\n+    private void checkForNewShards() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17d6e220ad5dc7870e7199811ac6ae0441719d32"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzI4OTg1MQ==", "bodyText": "Asssignment of processors isn't based on \"record keys\". It seems as if one shard could be read by multiple processors based on the record key.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r557289851", "createdAt": "2021-01-14T10:23:15Z", "author": {"login": "viliam-durina"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,482 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. Shards help break the\n+stream's data flow into independent substreams, which can be processed\n+in parallel. Shards preserve the order of the data items they ingest\n+while ordering among different shards' items is undefined. One shard\n+provides a capacity of 1MiB/sec data input and 2MiB/sec data output. One\n+shard can support up to 1000 record publications per second. You will\n+specify the number of shards needed when you create a data stream. For\n+example, you can create a data stream with two shards. This data stream\n+has a throughput of 2MiB/sec data input and 4MiB/sec data output and\n+allows up to 2000 record publications per second. You can monitor\n+shard-level metrics in Kinesis and add or remove shards from your data\n+stream dynamically as your data throughput changes by resharding the\n+data stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1 MiB.\n+\n+A **partition key** is used to assign records to different shards of a\n+data stream. Items with the same partition key always belong to the same\n+shard. Since shards preserve the order of the items they ingest, the\n+ordering of records with the same partition key is also preserved. The\n+partition key is specified by your data producer while adding data to\n+KDS.\n+\n+A **sequence number** is a unique identifier for each record within its\n+shard. Sequence numbers are assigned by KDS when a data producer\n+publishes data into it. They can be used as offsets of the ordered\n+series of records of a shard.\n+\n+## APIs\n+\n+Amazon offers various choices of libraries that can be used to interact\n+with KDS:\n+\n+* **Kinesis Client Library (KCL)** and **Kinesis Producer Library\n+  (KPL)** are high-level libraries that are easy to use because they\n+  abstract away many concerns. They manage their threading policy, hide\n+  away the REST-based nature of Kinesis behind asynchronous constructs,\n+  balance load, handle failures, and react to resharding. However, all\n+  this convenience makes them unsuitable for building Jet connectors,\n+  where we need the most control possible to make choices that are\n+  suitable to Jet's architecture.\n+* **Amazon Kinesis Data Streams API** via **AWS SDK for Java** is the\n+  lower-level library that allows sufficient control when interacting\n+  with Kinesis. It consists of a simple set of [REST-based\n+  operations](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Operations.html).\n+  Every other concern mentioned above, when discussing the high-level\n+  libraries, has to be handled explicitly. This is the library used in\n+  the Jet source and sink implementations.\n+\n+## Quotas\n+\n+Amazon Kinesis Data Streams enforces quite a few quotas and limits,\n+which our sources and sinks need to comply with:\n+\n+* A single shard can ingest up to 1 MiB of data per second (including\n+  partition keys) or 1,000 records per second for writes.\n+* The maximum size of the data payload of a record is 1 MiB.\n+* The\n+  [GetRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html)\n+  operation can retrieve up to 10 MiB of data per call from a single\n+  shard and up to 10,000 records per call.\n+* Each shard can support up to 5 GetRecords operations per second.\n+* Each shard can support up to a maximum total data read rate of 2 MiB\n+  per second.\n+* The\n+  [ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+  operation has a limit of 100 transactions per second, per data stream.\n+  Each such transaction is able to return at most 100 shards, and if the\n+  stream has more, then multiple transactions need to be used for a full\n+  listing. (For details, see the [discovery](#discovery) section.)\n+* The\n+  [PutRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)\n+  operation can write at most 500 records into the stream. Each record\n+  in the request can be as large as 1MiB, up to a limit of 5MiB for the\n+  entire request, including partition keys. Each shard can support\n+  writes up to 1,000 records per second, up to a maximum data write\n+  total of 1 MiB per second.\n+* The\n+  [DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+  operation has a limit of 20 transactions per second per account. (For\n+  details, see the [adaptive throughput](#adaptive-throughput) section.)\n+\n+## Source\n+\n+The Kinesis source is a _streaming_, _distributed_, and _fault-tolerant_\n+data source for Jet. It supports both the _at-least-once_ and\n+_exactly-once_ [processing\n+guarantees](../architecture/fault-tolerance.md#processing-guarantee-is-a-shared-concern).\n+\n+### Distribution\n+\n+Being a distributed source, it has multiple instances running in each\n+Jet cluster member. Each instance is responsible for reading from zero,\n+one or more KDS [shards](#shard). The assignment of shards to individual\n+source instances is based on partitioning the space of Kinesis [record\n+keys](#partition-key).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a90f59803307037f4106ca02a0c4e1aba21ab552"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzI5OTA0Mg==", "bodyText": "Are we sure that timestamps within one shard are strictly time-ordered? Are they assigned by a single machine in AWS. If, inside AWS, some other machine takes over after a failed machine, could it have delayed clock? If this isn't explicitly specified by AWS, I wouldn't relay on 0 allowed lag.\nAlso, when a processor starts to consume a new shard, i guess it will happen with a delay after that shard was created. It might already contain data - but those will be rendered late.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r557299042", "createdAt": "2021-01-14T10:38:03Z", "author": {"login": "viliam-durina"}, "path": "site/docs/api/sources-sinks.md", "diffHunk": "@@ -894,7 +894,128 @@ The Kafka sink and source are based on version 2.2.0, this means Kafka\n connector will work with any client and broker having version equal to\n or greater than 1.0.0.\n \n-###\u00a0JMS\n+### Amazon Kinesis\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. All\n+data items passing through it, called _records_, are assigned a\n+_partition key_. As the name suggests, partition keys group related\n+records together. Records with the same partition key are also ordered.\n+Partition keys are grouped into _shards_, the base throughput unit of\n+KDS. The input and output rates of shards is limited. Streams can be\n+resharded at any time.\n+\n+To read from Kinesis, the only requirement is to provide a KDS stream\n+name. (Kinesis does not handle deserialization itself, it only provides\n+serialized binary data.)\n+\n+```java\n+Pipeline p = Pipeline.create();\n+p.readFrom(KinesisSources.kinesis(STREAM).build())\n+  .withNativeTimestamps(0)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a90f59803307037f4106ca02a0c4e1aba21ab552"}, "originalPosition": 78}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "54df21cb47313a9a6b0be3a9e2f641f8be906745", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/54df21cb47313a9a6b0be3a9e2f641f8be906745", "committedDate": "2021-01-14T10:41:22Z", "message": "Swith from kinesis to bundle SDK jar"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "50bf585df328f5c2cafc536e83961f8a498642eb", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/50bf585df328f5c2cafc536e83961f8a498642eb", "committedDate": "2021-01-14T10:47:04Z", "message": "Remove explicit CBOR dependency"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "418d181fa9757a68b7c67d4d1fa24f1b167836e8", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/418d181fa9757a68b7c67d4d1fa24f1b167836e8", "committedDate": "2021-01-14T11:08:03Z", "message": "Address review concerns"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY4MTU0MTc2", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#pullrequestreview-568154176", "createdAt": "2021-01-14T11:39:54Z", "commit": {"oid": "418d181fa9757a68b7c67d4d1fa24f1b167836e8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNFQxMTozOTo1NFrOITg88g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNFQxMTozOTo1NFrOITg88g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzMzMzc0Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            title: 018 - Kinesis Connectors\n          \n          \n            \n            description: Sources and sinks for Amazon Kinesis Data Streams\n          \n          \n            \n            title: 018 - Kinesis Connector\n          \n          \n            \n            description: Source and Sink for Amazon Kinesis Data Streams", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r557333746", "createdAt": "2021-01-14T11:39:54Z", "author": {"login": "viliam-durina"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,481 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "418d181fa9757a68b7c67d4d1fa24f1b167836e8"}, "originalPosition": 3}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "44e8cddd58c9107738f5e4cd1ee9c86fd540025f", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/44e8cddd58c9107738f5e4cd1ee9c86fd540025f", "committedDate": "2021-01-14T11:42:01Z", "message": "Fix some typos, grammar"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY4MTY1NzQ4", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#pullrequestreview-568165748", "createdAt": "2021-01-14T11:57:19Z", "commit": {"oid": "418d181fa9757a68b7c67d4d1fa24f1b167836e8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNFQxMTo1NzoxOVrOIThggA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNFQxMTo1NzoxOVrOIThggA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzM0Mjg0OA==", "bodyText": "Isn't it always the case that if firstDetection == true, then trackingInfo was null above? If yes, you can apply this suggestion:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if (trackingInfo == null) {\n          \n          \n            \n                            int owner = findOwner(new BigInteger(shard.getHashKeyRange().getStartingHashKey()));\n          \n          \n            \n                            trackingInfo = new TrackingInfo(owner, currentTimeMs);\n          \n          \n            \n                            info.put(shardId, trackingInfo);\n          \n          \n            \n                        }\n          \n          \n            \n                        boolean firstDetection = trackingInfo.markDetection(currentTimeMs);\n          \n          \n            \n                        if (firstDetection) {\n          \n          \n            \n                            if (newShards.isEmpty()) {\n          \n          \n            \n                                newShards = new HashMap<>();\n          \n          \n            \n                            }\n          \n          \n            \n                            newShards.put(shard, trackingInfo.getOwner());\n          \n          \n            \n                        }\n          \n          \n            \n                        if (trackingInfo == null) {\n          \n          \n            \n                            int owner = findOwner(new BigInteger(shard.getHashKeyRange().getStartingHashKey()));\n          \n          \n            \n                            trackingInfo = new TrackingInfo(owner, currentTimeMs);\n          \n          \n            \n                            info.put(shardId, trackingInfo);\n          \n          \n            \n                            if (newShards.isEmpty()) {\n          \n          \n            \n                                newShards = new HashMap<>();\n          \n          \n            \n                            }\n          \n          \n            \n                            newShards.put(shard, trackingInfo.getOwner());\n          \n          \n            \n                        }\n          \n          \n            \n                        trackingInfo.markDetection(currentTimeMs);\n          \n      \n    \n    \n  \n\nFrom TrackingInfo you can then remove the boolean detected field.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r557342848", "createdAt": "2021-01-14T11:57:19Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/ShardTracker.java", "diffHunk": "@@ -0,0 +1,132 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.services.kinesis.model.Shard;\n+import com.hazelcast.jet.JetException;\n+\n+import java.math.BigInteger;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static java.util.concurrent.TimeUnit.MINUTES;\n+\n+public class ShardTracker {\n+\n+    /**\n+     * We consider shards to be expired if they haven't been detected at all\n+     * (neither as OPEN, nor as CLOSED) since at least this amount of time.\n+     */\n+    static final long EXPIRATION_MS = MINUTES.toMillis(10);\n+\n+    private final Map<String, TrackingInfo> info = new HashMap<>();\n+    private final HashRange[] rangePartitions;\n+\n+    public ShardTracker(HashRange[] rangePartitions) {\n+        this.rangePartitions = rangePartitions;\n+    }\n+\n+    public void addUndetected(String shardId, BigInteger startingHashKey, long currentTimeMs) {\n+        assert !info.containsKey(shardId);\n+        info.put(shardId, new TrackingInfo(findOwner(startingHashKey), currentTimeMs));\n+    }\n+\n+    public Map<Shard, Integer> markDetections(Set<Shard> shards, long currentTimeMs) {\n+        Map<Shard, Integer> newShards = Collections.emptyMap();\n+        for (Shard shard : shards) {\n+            String shardId = shard.getShardId();\n+            TrackingInfo trackingInfo = info.get(shardId);\n+            if (trackingInfo == null) {\n+                int owner = findOwner(new BigInteger(shard.getHashKeyRange().getStartingHashKey()));\n+                trackingInfo = new TrackingInfo(owner, currentTimeMs);\n+                info.put(shardId, trackingInfo);\n+            }\n+            boolean firstDetection = trackingInfo.markDetection(currentTimeMs);\n+            if (firstDetection) {\n+                if (newShards.isEmpty()) {\n+                    newShards = new HashMap<>();\n+                }\n+                newShards.put(shard, trackingInfo.getOwner());\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "418d181fa9757a68b7c67d4d1fa24f1b167836e8"}, "originalPosition": 66}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "065d68e4b0f9fb19d2e751c5d2cec9e1d13c5bf1", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/065d68e4b0f9fb19d2e751c5d2cec9e1d13c5bf1", "committedDate": "2021-01-14T12:20:06Z", "message": "Simplify ShardQueue's API"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9de35804c8ea5a01e224c5d88a45fad0d6b7a8b2", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/9de35804c8ea5a01e224c5d88a45fad0d6b7a8b2", "committedDate": "2021-01-14T12:20:27Z", "message": "Fix style"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8fc46f1fcf11fff5eb3bf0d31f9527cdef7596ab", "author": {"user": {"login": "olukas", "name": "Ondrej Lukas"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/8fc46f1fcf11fff5eb3bf0d31f9527cdef7596ab", "committedDate": "2021-01-14T14:59:00Z", "message": "add jobsStartedBeforeStreamExists test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fade2cd4ba3a9e9e6e28922769e52a220548ad98", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/fade2cd4ba3a9e9e6e28922769e52a220548ad98", "committedDate": "2021-01-14T17:44:25Z", "message": "Fix license text"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3f394e373a6d852f34173ddf4c86c6a57a7bfb4b", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/3f394e373a6d852f34173ddf4c86c6a57a7bfb4b", "committedDate": "2021-01-14T18:53:32Z", "message": "Update NOTICE file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "217afb255a6ad42ae31b8cadb5600b5003044de3", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/217afb255a6ad42ae31b8cadb5600b5003044de3", "committedDate": "2021-01-15T06:47:54Z", "message": "Update site/docs/design-docs/018-kinesis-connectors.md\n\nCo-authored-by: Viliam Durina <viliam-durina@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "65faee6fe918dd39b6c6c08fac3d91f613f6d786", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/65faee6fe918dd39b6c6c08fac3d91f613f6d786", "committedDate": "2021-01-15T07:46:42Z", "message": "Improve documentation, fix a bug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d6b9f99ddca5844fa9e35b25006d744fcdc0d20b", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/d6b9f99ddca5844fa9e35b25006d744fcdc0d20b", "committedDate": "2021-01-15T08:49:09Z", "message": "Update extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java\n\nCo-authored-by: Viliam Durina <viliam-durina@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cc8584e312a4af135c8b48c0c7439d4ed1053482", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/cc8584e312a4af135c8b48c0c7439d4ed1053482", "committedDate": "2021-01-15T09:37:52Z", "message": "Improve documentation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ee0191823a64cad40ce9b9be4eebcf45b731969", "author": {"user": {"login": "olukas", "name": "Ondrej Lukas"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/2ee0191823a64cad40ce9b9be4eebcf45b731969", "committedDate": "2021-01-15T09:48:17Z", "message": "Add possibility to run KinesisIntegrationTest with real backend"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53f046fd5b38720995a080824d6071b6d9b13ddb", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/53f046fd5b38720995a080824d6071b6d9b13ddb", "committedDate": "2021-01-15T10:29:24Z", "message": "Change from assembly to shade plugin, relocate com.amazonaws"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1205c555116b00a19767ee911c1fb1bde1b94e89", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/1205c555116b00a19767ee911c1fb1bde1b94e89", "committedDate": "2021-01-15T11:26:33Z", "message": "Add missing dependency"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY5MTk0NTc2", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#pullrequestreview-569194576", "createdAt": "2021-01-15T11:52:35Z", "commit": {"oid": "1205c555116b00a19767ee911c1fb1bde1b94e89"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "87dfce7fe7206ceadbd75a7e349b8c9b7a29eda7", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/87dfce7fe7206ceadbd75a7e349b8c9b7a29eda7", "committedDate": "2021-01-15T12:02:18Z", "message": "Update NOTICE file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "46f9aeb60c565be5110f544c43ae6d306a9d46a1", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/46f9aeb60c565be5110f544c43ae6d306a9d46a1", "committedDate": "2020-11-19T11:33:28Z", "message": "Define Kinesis module and integration test framework"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "09be7f6c83942108c4b47f29764d552bfd8dd2e5", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/09be7f6c83942108c4b47f29764d552bfd8dd2e5", "committedDate": "2020-11-19T11:33:29Z", "message": "Basic source and test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f4086c697eeb1381470d435616a731e27097b375", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/f4086c697eeb1381470d435616a731e27097b375", "committedDate": "2020-11-19T11:33:29Z", "message": "Make checkstyle happy"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5533f122e6a8d5ee5d40789cc65b491ccc91f825", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/5533f122e6a8d5ee5d40789cc65b491ccc91f825", "committedDate": "2020-11-19T11:33:29Z", "message": "Rename jackson.jr.version because it's used for more than Jackson jr"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8588a02a57359e1e9a8b3acb9404ffd62e7bf5b2", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/8588a02a57359e1e9a8b3acb9404ffd62e7bf5b2", "committedDate": "2020-11-19T11:33:29Z", "message": "Improve the source"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "73e4f9b56ae1ed082db707115214de2e5d7fe618", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/73e4f9b56ae1ed082db707115214de2e5d7fe618", "committedDate": "2020-11-19T11:33:30Z", "message": "Improve GetRecords limit/quota handling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7e0e3369eb96e1c8a79d83297217be27b0e42a24", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/7e0e3369eb96e1c8a79d83297217be27b0e42a24", "committedDate": "2020-11-19T11:33:30Z", "message": "First working version of the sink"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e43b86e769fdb8c22ed808950622131b86be2cd4", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/e43b86e769fdb8c22ed808950622131b86be2cd4", "committedDate": "2020-11-19T11:33:30Z", "message": "Recycle data objects in the sink"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f495d9652d5320651463eb5274f15af44106e3fe", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/f495d9652d5320651463eb5274f15af44106e3fe", "committedDate": "2020-11-19T11:33:30Z", "message": "Make checkstyle happy"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa5f33ae7ca980c3a0511e211c4f813c3932e0e1", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/aa5f33ae7ca980c3a0511e211c4f813c3932e0e1", "committedDate": "2020-11-19T11:33:31Z", "message": "First working version of merge handling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b750e9a2e61075020c42c0fa430c0dc4b656e336", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/b750e9a2e61075020c42c0fa430c0dc4b656e336", "committedDate": "2020-11-19T11:33:31Z", "message": "Finish merge handling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db1c69552a8e9fc6d63e97566a8821100cda4230", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/db1c69552a8e9fc6d63e97566a8821100cda4230", "committedDate": "2020-11-19T11:33:31Z", "message": "Handle splits, improve source"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "04fec118830987a0924fa564f83d55dc5955151f", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/04fec118830987a0924fa564f83d55dc5955151f", "committedDate": "2020-11-19T11:33:31Z", "message": "Improve exception handling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "71d3c18448b8369c9ace1a67d50931de4457f12b", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/71d3c18448b8369c9ace1a67d50931de4457f12b", "committedDate": "2020-11-19T11:33:32Z", "message": "Improve various aspects"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4d488eec9666cb52c0e49625166d9d525a80cfb4", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/4d488eec9666cb52c0e49625166d9d525a80cfb4", "committedDate": "2020-11-19T11:33:32Z", "message": "Improve test cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4de2d506e7f27ca4866b42892ed9c983995c9d2d", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/4de2d506e7f27ca4866b42892ed9c983995c9d2d", "committedDate": "2020-11-19T11:33:32Z", "message": "Improve tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0343ef6cea59c2faef852008eb4f73d594bb1e66", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/0343ef6cea59c2faef852008eb4f73d594bb1e66", "committedDate": "2020-11-19T11:33:32Z", "message": "Fix version name"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a877e0314e92fe545e5cd50f65b494690e1e81c", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/1a877e0314e92fe545e5cd50f65b494690e1e81c", "committedDate": "2020-11-19T11:33:33Z", "message": "Fix hostname used in tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c6cbf0b627a1bb84577b7ca404486d66e7296fe6", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/c6cbf0b627a1bb84577b7ca404486d66e7296fe6", "committedDate": "2020-11-19T11:33:33Z", "message": "Add timestamp handling"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fb6aa66caf1afcb19706faf5fd029ea74021c889", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/fb6aa66caf1afcb19706faf5fd029ea74021c889", "committedDate": "2020-11-19T11:22:16Z", "message": "Add timestamp handling"}, "afterCommit": {"oid": "c6cbf0b627a1bb84577b7ca404486d66e7296fe6", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/c6cbf0b627a1bb84577b7ca404486d66e7296fe6", "committedDate": "2020-11-19T11:33:33Z", "message": "Add timestamp handling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5123b8bce803483a7df547a975a131cd1962e7cc", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/5123b8bce803483a7df547a975a131cd1962e7cc", "committedDate": "2020-11-19T11:54:24Z", "message": "Add exception handling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "60210677a91b60121237606301d59bb9988dfc9e", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/60210677a91b60121237606301d59bb9988dfc9e", "committedDate": "2020-11-20T08:49:05Z", "message": "Improve failure handling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "424328c77710e5f9841ebd5a432e05a0800e6797", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/424328c77710e5f9841ebd5a432e05a0800e6797", "committedDate": "2020-11-20T09:09:01Z", "message": "Merge branch 'master' into kinesis"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a94b91e2e9984853cc768963a8147f3c150a4b97", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/a94b91e2e9984853cc768963a8147f3c150a4b97", "committedDate": "2020-11-23T13:41:06Z", "message": "Make sink cooperative"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3b641d8a258793aa7f6ea64ed211d34f1eb6af9d", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/3b641d8a258793aa7f6ea64ed211d34f1eb6af9d", "committedDate": "2020-11-25T11:33:57Z", "message": "Fix shard monitoring issue"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2d266c70b69fb4d4a8d61c6d33348b1399e9dca0", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/2d266c70b69fb4d4a8d61c6d33348b1399e9dca0", "committedDate": "2020-11-25T11:34:04Z", "message": "Merge branch 'master' into kinesis"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM4NTg3OTEz", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#pullrequestreview-538587913", "createdAt": "2020-11-25T15:02:22Z", "commit": {"oid": "2d266c70b69fb4d4a8d61c6d33348b1399e9dca0"}, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQxNTowMjoyMlrOH53bFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQxNjozMzo1M1rOH57asw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDQzODkzNA==", "bodyText": "Shouldn't we shutdown the clients in the close method here?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r530438934", "createdAt": "2020-11-25T15:02:22Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkPSupplier.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.ProcessorSupplier;\n+\n+import javax.annotation.Nonnull;\n+import java.util.Collection;\n+import java.util.stream.IntStream;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+public class KinesisSinkPSupplier<T> implements ProcessorSupplier {\n+\n+    private static final long serialVersionUID = 1L;\n+\n+    /**\n+     * We don't want to create an AWS client for each processor instance,\n+     * because they aren't light. We also can't do the other extreme, have a\n+     * single AWS client shared by all processors, because there would be a lot\n+     * of contention, causing problems. So we use shared clients but use them\n+     * for a limited number of processor instances, specified by this constant.\n+     */\n+    private static final int PROCESSORS_PER_CLIENT = 12; //todo: find optimal value on real backend\n+\n+    @Nonnull\n+    private final AwsConfig awsConfig;\n+    @Nonnull\n+    private final String stream;\n+    @Nonnull\n+    private final FunctionEx<T, String> keyFn;\n+    @Nonnull\n+    private final FunctionEx<T, byte[]> valueFn;\n+\n+    private transient AmazonKinesisAsync[] clients;\n+\n+    public KinesisSinkPSupplier(\n+            @Nonnull AwsConfig awsConfig,\n+            @Nonnull String stream,\n+            @Nonnull FunctionEx<T, String> keyFn,\n+            @Nonnull FunctionEx<T, byte[]> valueFn\n+    ) {\n+        this.awsConfig = awsConfig;\n+        this.stream = stream;\n+        this.keyFn = keyFn;\n+        this.valueFn = valueFn;\n+    }\n+\n+    @Override\n+    public void init(@Nonnull Context context) {\n+        int localParallelism = context.localParallelism();\n+        this.clients = IntStream.range(0, (int) Math.ceil((double) localParallelism / PROCESSORS_PER_CLIENT))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d266c70b69fb4d4a8d61c6d33348b1399e9dca0"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDQ0NDA4NQ==", "bodyText": "This will cause double logging. When we re-throw the exception, there's no need to log it.\nSame below.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r530444085", "createdAt": "2020-11-25T15:09:33Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisHelper.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.AmazonKinesisException;\n+import com.amazonaws.services.kinesis.model.DescribeStreamSummaryRequest;\n+import com.amazonaws.services.kinesis.model.ExpiredNextTokenException;\n+import com.amazonaws.services.kinesis.model.GetRecordsRequest;\n+import com.amazonaws.services.kinesis.model.GetRecordsResult;\n+import com.amazonaws.services.kinesis.model.GetShardIteratorResult;\n+import com.amazonaws.services.kinesis.model.InvalidArgumentException;\n+import com.amazonaws.services.kinesis.model.LimitExceededException;\n+import com.amazonaws.services.kinesis.model.ListShardsRequest;\n+import com.amazonaws.services.kinesis.model.ListShardsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequest;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.ResourceInUseException;\n+import com.amazonaws.services.kinesis.model.ResourceNotFoundException;\n+import com.amazonaws.services.kinesis.model.Shard;\n+import com.amazonaws.services.kinesis.model.ShardFilter;\n+import com.amazonaws.services.kinesis.model.ShardFilterType;\n+import com.amazonaws.services.kinesis.model.StreamDescriptionSummary;\n+import com.amazonaws.services.kinesis.model.StreamStatus;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.math.BigInteger;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+\n+public class KinesisHelper {\n+\n+    private static final int SLEEP_DURATION = 250;\n+\n+    private final AmazonKinesisAsync kinesis;\n+    private final String stream;\n+\n+    private final ILogger logger;\n+\n+    public KinesisHelper(AmazonKinesisAsync kinesis, String stream, ILogger logger) {\n+        this.kinesis = kinesis;\n+        this.stream = stream;\n+        this.logger = logger;\n+    }\n+\n+    public static boolean shardActive(@Nonnull Shard shard) {\n+        String endingSequenceNumber = shard.getSequenceNumberRange().getEndingSequenceNumber();\n+        return endingSequenceNumber == null;\n+        //need to rely on this hack, because shard filters don't seem to work, on the mock at least ...\n+    }\n+\n+    public static boolean shardBelongsToRange(@Nonnull Shard shard, @Nonnull HashRange range) {\n+        String startingHashKey = shard.getHashKeyRange().getStartingHashKey();\n+        return range.contains(new BigInteger(startingHashKey));\n+    }\n+\n+    public void waitForStreamToActivate() {\n+        while (true) {\n+            StreamStatus status = callSafely(this::getStreamStatus);\n+            switch (status) {\n+                case ACTIVE:\n+                    return;\n+                case CREATING:\n+                case UPDATING:\n+                    logger.info(\"Waiting for stream \" + stream + \" to become active...\");\n+                    waitABit();\n+                    break;\n+                case DELETING:\n+                    throw new JetException(\"Stream is being deleted\");\n+                default:\n+                    throw new JetException(\"Programming error, unhandled stream status: \" + status);\n+            }\n+        }\n+    }\n+\n+    public void waitForStreamToDisappear() {\n+        while (true) {\n+            List<String> streams = callSafely(this::listStreams);\n+            if (streams.isEmpty()) {\n+                return;\n+            } else {\n+                logger.info(\"Waiting for stream \" + stream + \" to disappear...\");\n+                waitABit();\n+            }\n+        }\n+    }\n+\n+    private List<String> listStreams() {\n+        return kinesis.listStreams().getStreamNames();\n+    }\n+\n+    private StreamStatus getStreamStatus() {\n+        DescribeStreamSummaryRequest request = new DescribeStreamSummaryRequest();\n+        request.setStreamName(stream);\n+\n+        StreamDescriptionSummary description = kinesis.describeStreamSummary(request).getStreamDescriptionSummary();\n+        String statusString = description.getStreamStatus();\n+\n+        return StreamStatus.valueOf(statusString);\n+    }\n+\n+    public List<Shard> listShards(Predicate<? super Shard> filter) {\n+        return callSafely(this::listShards).stream()\n+                .filter(filter)\n+                .collect(Collectors.toList());\n+    }\n+\n+    private List<Shard> listShards() throws AmazonKinesisException {\n+        List<Shard> shards = new ArrayList<>();\n+        String nextToken = null;\n+        do {\n+            ListShardsRequest request = listShardsRequest(nextToken);\n+            ListShardsResult response = kinesis.listShards(request);\n+            shards.addAll(response.getShards());\n+            nextToken = response.getNextToken();\n+        } while (nextToken != null);\n+        return shards;\n+    }\n+\n+    private ListShardsRequest listShardsRequest(@Nullable String nextToken) {\n+        ListShardsRequest request = new ListShardsRequest();\n+        if (nextToken == null) {\n+            request.setStreamName(stream);\n+        } else {\n+            request.setNextToken(nextToken);\n+        }\n+        request.setShardFilter(new ShardFilter().withType(ShardFilterType.AT_LATEST));\n+        return request;\n+    }\n+\n+    public Future<ListShardsResult> listShardsAsync(String nextToken) {\n+        ListShardsRequest request = listShardsRequest(nextToken);\n+        return kinesis.listShardsAsync(request);\n+    }\n+\n+    public Future<GetShardIteratorResult> getShardIteratorAsync(Shard shard) {\n+        return kinesis.getShardIteratorAsync(\n+                stream,\n+                shard.getShardId(),\n+                \"AT_SEQUENCE_NUMBER\",\n+                shard.getSequenceNumberRange().getStartingSequenceNumber()\n+        ); //todo: proper starting sequence number will be provided from offsets restored from Jet snapshots\n+    }\n+\n+    public Future<GetRecordsResult> getRecordsAsync(String shardIterator) {\n+        GetRecordsRequest request = new GetRecordsRequest();\n+        request.setShardIterator(shardIterator);\n+        return kinesis.getRecordsAsync(request);\n+    }\n+\n+    public Future<PutRecordsResult> putRecordsAsync(Collection<PutRecordsRequestEntry> entries) {\n+        PutRecordsRequest request = new PutRecordsRequest();\n+        request.setRecords(entries);\n+        request.setStreamName(stream);\n+        return kinesis.putRecordsAsync(request);\n+    }\n+\n+    private <T> T callSafely(Callable<T> callable) {\n+        while (true) {\n+            try {\n+                return callable.call();\n+            } catch (LimitExceededException lee) {\n+                String message = \"The requested resource exceeds the maximum number allowed, or the number of \" +\n+                        \"concurrent stream requests exceeds the maximum number allowed. Will retry.\";\n+                logger.warning(message, lee);\n+            } catch (ExpiredNextTokenException ente) {\n+                String message = \"The pagination token passed to the operation is expired. Will retry.\";\n+                logger.warning(message, ente);\n+            } catch (ResourceInUseException riue) {\n+                String message = \"The resource is not available for this operation. For successful operation, the \" +\n+                        \"resource must be in the <code>ACTIVE</code> state. Will retry.\";\n+                logger.warning(message, riue);\n+            } catch (ResourceNotFoundException rnfe) {\n+                String message = \"The requested resource could not be found. The stream might not be specified correctly.\";\n+                logger.severe(message, rnfe);\n+                throw new JetException(message, rnfe);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d266c70b69fb4d4a8d61c6d33348b1399e9dca0"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDQ0NDYwNw==", "bodyText": "HTML in error messages?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r530444607", "createdAt": "2020-11-25T15:10:13Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisHelper.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.AmazonKinesisException;\n+import com.amazonaws.services.kinesis.model.DescribeStreamSummaryRequest;\n+import com.amazonaws.services.kinesis.model.ExpiredNextTokenException;\n+import com.amazonaws.services.kinesis.model.GetRecordsRequest;\n+import com.amazonaws.services.kinesis.model.GetRecordsResult;\n+import com.amazonaws.services.kinesis.model.GetShardIteratorResult;\n+import com.amazonaws.services.kinesis.model.InvalidArgumentException;\n+import com.amazonaws.services.kinesis.model.LimitExceededException;\n+import com.amazonaws.services.kinesis.model.ListShardsRequest;\n+import com.amazonaws.services.kinesis.model.ListShardsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequest;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.ResourceInUseException;\n+import com.amazonaws.services.kinesis.model.ResourceNotFoundException;\n+import com.amazonaws.services.kinesis.model.Shard;\n+import com.amazonaws.services.kinesis.model.ShardFilter;\n+import com.amazonaws.services.kinesis.model.ShardFilterType;\n+import com.amazonaws.services.kinesis.model.StreamDescriptionSummary;\n+import com.amazonaws.services.kinesis.model.StreamStatus;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.math.BigInteger;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+\n+public class KinesisHelper {\n+\n+    private static final int SLEEP_DURATION = 250;\n+\n+    private final AmazonKinesisAsync kinesis;\n+    private final String stream;\n+\n+    private final ILogger logger;\n+\n+    public KinesisHelper(AmazonKinesisAsync kinesis, String stream, ILogger logger) {\n+        this.kinesis = kinesis;\n+        this.stream = stream;\n+        this.logger = logger;\n+    }\n+\n+    public static boolean shardActive(@Nonnull Shard shard) {\n+        String endingSequenceNumber = shard.getSequenceNumberRange().getEndingSequenceNumber();\n+        return endingSequenceNumber == null;\n+        //need to rely on this hack, because shard filters don't seem to work, on the mock at least ...\n+    }\n+\n+    public static boolean shardBelongsToRange(@Nonnull Shard shard, @Nonnull HashRange range) {\n+        String startingHashKey = shard.getHashKeyRange().getStartingHashKey();\n+        return range.contains(new BigInteger(startingHashKey));\n+    }\n+\n+    public void waitForStreamToActivate() {\n+        while (true) {\n+            StreamStatus status = callSafely(this::getStreamStatus);\n+            switch (status) {\n+                case ACTIVE:\n+                    return;\n+                case CREATING:\n+                case UPDATING:\n+                    logger.info(\"Waiting for stream \" + stream + \" to become active...\");\n+                    waitABit();\n+                    break;\n+                case DELETING:\n+                    throw new JetException(\"Stream is being deleted\");\n+                default:\n+                    throw new JetException(\"Programming error, unhandled stream status: \" + status);\n+            }\n+        }\n+    }\n+\n+    public void waitForStreamToDisappear() {\n+        while (true) {\n+            List<String> streams = callSafely(this::listStreams);\n+            if (streams.isEmpty()) {\n+                return;\n+            } else {\n+                logger.info(\"Waiting for stream \" + stream + \" to disappear...\");\n+                waitABit();\n+            }\n+        }\n+    }\n+\n+    private List<String> listStreams() {\n+        return kinesis.listStreams().getStreamNames();\n+    }\n+\n+    private StreamStatus getStreamStatus() {\n+        DescribeStreamSummaryRequest request = new DescribeStreamSummaryRequest();\n+        request.setStreamName(stream);\n+\n+        StreamDescriptionSummary description = kinesis.describeStreamSummary(request).getStreamDescriptionSummary();\n+        String statusString = description.getStreamStatus();\n+\n+        return StreamStatus.valueOf(statusString);\n+    }\n+\n+    public List<Shard> listShards(Predicate<? super Shard> filter) {\n+        return callSafely(this::listShards).stream()\n+                .filter(filter)\n+                .collect(Collectors.toList());\n+    }\n+\n+    private List<Shard> listShards() throws AmazonKinesisException {\n+        List<Shard> shards = new ArrayList<>();\n+        String nextToken = null;\n+        do {\n+            ListShardsRequest request = listShardsRequest(nextToken);\n+            ListShardsResult response = kinesis.listShards(request);\n+            shards.addAll(response.getShards());\n+            nextToken = response.getNextToken();\n+        } while (nextToken != null);\n+        return shards;\n+    }\n+\n+    private ListShardsRequest listShardsRequest(@Nullable String nextToken) {\n+        ListShardsRequest request = new ListShardsRequest();\n+        if (nextToken == null) {\n+            request.setStreamName(stream);\n+        } else {\n+            request.setNextToken(nextToken);\n+        }\n+        request.setShardFilter(new ShardFilter().withType(ShardFilterType.AT_LATEST));\n+        return request;\n+    }\n+\n+    public Future<ListShardsResult> listShardsAsync(String nextToken) {\n+        ListShardsRequest request = listShardsRequest(nextToken);\n+        return kinesis.listShardsAsync(request);\n+    }\n+\n+    public Future<GetShardIteratorResult> getShardIteratorAsync(Shard shard) {\n+        return kinesis.getShardIteratorAsync(\n+                stream,\n+                shard.getShardId(),\n+                \"AT_SEQUENCE_NUMBER\",\n+                shard.getSequenceNumberRange().getStartingSequenceNumber()\n+        ); //todo: proper starting sequence number will be provided from offsets restored from Jet snapshots\n+    }\n+\n+    public Future<GetRecordsResult> getRecordsAsync(String shardIterator) {\n+        GetRecordsRequest request = new GetRecordsRequest();\n+        request.setShardIterator(shardIterator);\n+        return kinesis.getRecordsAsync(request);\n+    }\n+\n+    public Future<PutRecordsResult> putRecordsAsync(Collection<PutRecordsRequestEntry> entries) {\n+        PutRecordsRequest request = new PutRecordsRequest();\n+        request.setRecords(entries);\n+        request.setStreamName(stream);\n+        return kinesis.putRecordsAsync(request);\n+    }\n+\n+    private <T> T callSafely(Callable<T> callable) {\n+        while (true) {\n+            try {\n+                return callable.call();\n+            } catch (LimitExceededException lee) {\n+                String message = \"The requested resource exceeds the maximum number allowed, or the number of \" +\n+                        \"concurrent stream requests exceeds the maximum number allowed. Will retry.\";\n+                logger.warning(message, lee);\n+            } catch (ExpiredNextTokenException ente) {\n+                String message = \"The pagination token passed to the operation is expired. Will retry.\";\n+                logger.warning(message, ente);\n+            } catch (ResourceInUseException riue) {\n+                String message = \"The resource is not available for this operation. For successful operation, the \" +\n+                        \"resource must be in the <code>ACTIVE</code> state. Will retry.\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d266c70b69fb4d4a8d61c6d33348b1399e9dca0"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDQ0NjYwNw==", "bodyText": "If we throw the cause, we won't see where the exception was caught. The cause has a stack trace from another thread.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        throw e.getCause();\n          \n          \n            \n                        throw e;", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r530446607", "createdAt": "2020-11-25T15:12:56Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisHelper.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.AmazonKinesisException;\n+import com.amazonaws.services.kinesis.model.DescribeStreamSummaryRequest;\n+import com.amazonaws.services.kinesis.model.ExpiredNextTokenException;\n+import com.amazonaws.services.kinesis.model.GetRecordsRequest;\n+import com.amazonaws.services.kinesis.model.GetRecordsResult;\n+import com.amazonaws.services.kinesis.model.GetShardIteratorResult;\n+import com.amazonaws.services.kinesis.model.InvalidArgumentException;\n+import com.amazonaws.services.kinesis.model.LimitExceededException;\n+import com.amazonaws.services.kinesis.model.ListShardsRequest;\n+import com.amazonaws.services.kinesis.model.ListShardsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequest;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.ResourceInUseException;\n+import com.amazonaws.services.kinesis.model.ResourceNotFoundException;\n+import com.amazonaws.services.kinesis.model.Shard;\n+import com.amazonaws.services.kinesis.model.ShardFilter;\n+import com.amazonaws.services.kinesis.model.ShardFilterType;\n+import com.amazonaws.services.kinesis.model.StreamDescriptionSummary;\n+import com.amazonaws.services.kinesis.model.StreamStatus;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.math.BigInteger;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+\n+public class KinesisHelper {\n+\n+    private static final int SLEEP_DURATION = 250;\n+\n+    private final AmazonKinesisAsync kinesis;\n+    private final String stream;\n+\n+    private final ILogger logger;\n+\n+    public KinesisHelper(AmazonKinesisAsync kinesis, String stream, ILogger logger) {\n+        this.kinesis = kinesis;\n+        this.stream = stream;\n+        this.logger = logger;\n+    }\n+\n+    public static boolean shardActive(@Nonnull Shard shard) {\n+        String endingSequenceNumber = shard.getSequenceNumberRange().getEndingSequenceNumber();\n+        return endingSequenceNumber == null;\n+        //need to rely on this hack, because shard filters don't seem to work, on the mock at least ...\n+    }\n+\n+    public static boolean shardBelongsToRange(@Nonnull Shard shard, @Nonnull HashRange range) {\n+        String startingHashKey = shard.getHashKeyRange().getStartingHashKey();\n+        return range.contains(new BigInteger(startingHashKey));\n+    }\n+\n+    public void waitForStreamToActivate() {\n+        while (true) {\n+            StreamStatus status = callSafely(this::getStreamStatus);\n+            switch (status) {\n+                case ACTIVE:\n+                    return;\n+                case CREATING:\n+                case UPDATING:\n+                    logger.info(\"Waiting for stream \" + stream + \" to become active...\");\n+                    waitABit();\n+                    break;\n+                case DELETING:\n+                    throw new JetException(\"Stream is being deleted\");\n+                default:\n+                    throw new JetException(\"Programming error, unhandled stream status: \" + status);\n+            }\n+        }\n+    }\n+\n+    public void waitForStreamToDisappear() {\n+        while (true) {\n+            List<String> streams = callSafely(this::listStreams);\n+            if (streams.isEmpty()) {\n+                return;\n+            } else {\n+                logger.info(\"Waiting for stream \" + stream + \" to disappear...\");\n+                waitABit();\n+            }\n+        }\n+    }\n+\n+    private List<String> listStreams() {\n+        return kinesis.listStreams().getStreamNames();\n+    }\n+\n+    private StreamStatus getStreamStatus() {\n+        DescribeStreamSummaryRequest request = new DescribeStreamSummaryRequest();\n+        request.setStreamName(stream);\n+\n+        StreamDescriptionSummary description = kinesis.describeStreamSummary(request).getStreamDescriptionSummary();\n+        String statusString = description.getStreamStatus();\n+\n+        return StreamStatus.valueOf(statusString);\n+    }\n+\n+    public List<Shard> listShards(Predicate<? super Shard> filter) {\n+        return callSafely(this::listShards).stream()\n+                .filter(filter)\n+                .collect(Collectors.toList());\n+    }\n+\n+    private List<Shard> listShards() throws AmazonKinesisException {\n+        List<Shard> shards = new ArrayList<>();\n+        String nextToken = null;\n+        do {\n+            ListShardsRequest request = listShardsRequest(nextToken);\n+            ListShardsResult response = kinesis.listShards(request);\n+            shards.addAll(response.getShards());\n+            nextToken = response.getNextToken();\n+        } while (nextToken != null);\n+        return shards;\n+    }\n+\n+    private ListShardsRequest listShardsRequest(@Nullable String nextToken) {\n+        ListShardsRequest request = new ListShardsRequest();\n+        if (nextToken == null) {\n+            request.setStreamName(stream);\n+        } else {\n+            request.setNextToken(nextToken);\n+        }\n+        request.setShardFilter(new ShardFilter().withType(ShardFilterType.AT_LATEST));\n+        return request;\n+    }\n+\n+    public Future<ListShardsResult> listShardsAsync(String nextToken) {\n+        ListShardsRequest request = listShardsRequest(nextToken);\n+        return kinesis.listShardsAsync(request);\n+    }\n+\n+    public Future<GetShardIteratorResult> getShardIteratorAsync(Shard shard) {\n+        return kinesis.getShardIteratorAsync(\n+                stream,\n+                shard.getShardId(),\n+                \"AT_SEQUENCE_NUMBER\",\n+                shard.getSequenceNumberRange().getStartingSequenceNumber()\n+        ); //todo: proper starting sequence number will be provided from offsets restored from Jet snapshots\n+    }\n+\n+    public Future<GetRecordsResult> getRecordsAsync(String shardIterator) {\n+        GetRecordsRequest request = new GetRecordsRequest();\n+        request.setShardIterator(shardIterator);\n+        return kinesis.getRecordsAsync(request);\n+    }\n+\n+    public Future<PutRecordsResult> putRecordsAsync(Collection<PutRecordsRequestEntry> entries) {\n+        PutRecordsRequest request = new PutRecordsRequest();\n+        request.setRecords(entries);\n+        request.setStreamName(stream);\n+        return kinesis.putRecordsAsync(request);\n+    }\n+\n+    private <T> T callSafely(Callable<T> callable) {\n+        while (true) {\n+            try {\n+                return callable.call();\n+            } catch (LimitExceededException lee) {\n+                String message = \"The requested resource exceeds the maximum number allowed, or the number of \" +\n+                        \"concurrent stream requests exceeds the maximum number allowed. Will retry.\";\n+                logger.warning(message, lee);\n+            } catch (ExpiredNextTokenException ente) {\n+                String message = \"The pagination token passed to the operation is expired. Will retry.\";\n+                logger.warning(message, ente);\n+            } catch (ResourceInUseException riue) {\n+                String message = \"The resource is not available for this operation. For successful operation, the \" +\n+                        \"resource must be in the <code>ACTIVE</code> state. Will retry.\";\n+                logger.warning(message, riue);\n+            } catch (ResourceNotFoundException rnfe) {\n+                String message = \"The requested resource could not be found. The stream might not be specified correctly.\";\n+                logger.severe(message, rnfe);\n+                throw new JetException(message, rnfe);\n+            } catch (InvalidArgumentException iae) {\n+                String message = \"A specified parameter exceeds its restrictions, is not supported, or can't be used.\";\n+                logger.severe(message, iae);\n+                throw new JetException(message, iae);\n+            } catch (SdkClientException sce) {\n+                String message = \"Amazon SDK failure, ignoring and retrying.\";\n+                logger.warning(message, sce);\n+            } catch (Exception e) {\n+                throw rethrow(e);\n+            }\n+\n+            waitABit();\n+        }\n+    }\n+\n+    public <T> T readResult(Future<T> future) throws Throwable {\n+        try {\n+            return future.get();\n+        } catch (InterruptedException e) {\n+            Thread.currentThread().interrupt();\n+            throw new JetException(\"Interrupted while waiting for results\");\n+        } catch (ExecutionException e) {\n+            throw e.getCause();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d266c70b69fb4d4a8d61c6d33348b1399e9dca0"}, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDQ4NzEyNg==", "bodyText": "If not deleting from the end, the old code doesn't move entries, but just swaps the removed and last entry, which is wrong. It also doesn't decrement entryCount\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        if (index < 0 || index >= entryCount) {\n          \n          \n            \n                            throw new IllegalArgumentException(\"Index needs to be between 0 and \" + entryCount);\n          \n          \n            \n                        }\n          \n          \n            \n            \n          \n          \n            \n                        totalEntrySize -= entries[index].encodedSize;\n          \n          \n            \n            \n          \n          \n            \n                        int lastIndex = entryCount - 1;\n          \n          \n            \n                        if (index < lastIndex) {\n          \n          \n            \n                            BufferEntry tmp = entries[index];\n          \n          \n            \n                            entries[index] = entries[lastIndex];\n          \n          \n            \n                            entries[lastIndex] = tmp;\n          \n          \n            \n                        } else {\n          \n          \n            \n                            entryCount--;\n          \n          \n            \n                        }\n          \n          \n            \n                        if (index < 0 || index >= entryCount) {\n          \n          \n            \n                            throw new IndexOutOfBoundsException(\"index=\" + index + \", size=\" + entryCount);\n          \n          \n            \n                        }\n          \n          \n            \n                        totalEntrySize -= entries[index].encodedSize;\n          \n          \n            \n                        entryCount--;\n          \n          \n            \n                        if (index < entryCount) {\n          \n          \n            \n                            BufferEntry tmp = entries[index];\n          \n          \n            \n                            System.arraycopy(entries, index + 1, entries, index, entryCount - index);\n          \n          \n            \n                            entries[entryCount] = tmp;\n          \n          \n            \n                        }", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r530487126", "createdAt": "2020-11-25T16:08:27Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsResultEntry;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.core.Inbox;\n+import com.hazelcast.jet.core.Outbox;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.Watermark;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+\n+public class KinesisSinkP<T> implements Processor {\n+\n+    //todo; Each shard can support writes up to 1,000 records per second, up to a\n+    // maximum data write total of 1 MiB per second. Right now we don't pre-check\n+    // this, because we don't check which item goes to what shard, we just rely\n+    // failure handling with exponential backoff. Would be complicated to improve\n+    // on, but can we affort not to?\n+\n+    /**\n+     * PutRecords requests are limited to 500 records.\n+     */\n+    private static final int MAX_RECORDS_IN_REQUEST = 500;\n+\n+    private static final long PAUSE_AFTER_FAILURE = 1000L; //todo: exponential backoff\n+\n+    /**\n+     * Each record, when encoded as a byte array, is limited to 1M,\n+     * including the partition key (Unicode String).\n+     */\n+    private static final int MAX_RECORD_SIZE_IN_BYTES = 1024 * 1024;\n+\n+    /**\n+     * The maximum allowed size of all the records in a PutRecords request,\n+     * including partition keys is 5M.\n+     */\n+    private static final int MAX_REQUEST_SIZE_IN_BYTES = 5 * 1024 * 1024;\n+\n+    /**\n+     * The number of Unicode characters making up keys is limited to a maximum\n+     * of 256.\n+     */\n+    private static final int MAX_UNICODE_CHARS_IN_KEY = 256;\n+\n+    private final AmazonKinesisAsync kinesis;\n+    private final String stream;\n+\n+    private final Buffer<T> buffer;\n+\n+    private ILogger logger;\n+    private KinesisHelper helper;\n+\n+    private State state = State.READY_TO_SEND;\n+    private long nextSendTime;\n+    private Future<PutRecordsResult> sendResult;\n+\n+    public KinesisSinkP(\n+            AmazonKinesisAsync kinesis,\n+            @Nonnull String stream,\n+            @Nonnull FunctionEx<T, String> keyFn,\n+            @Nonnull FunctionEx<T, byte[]> valueFn\n+    ) {\n+        this.kinesis = kinesis;\n+        this.stream = stream;\n+        this.buffer = new Buffer<>(keyFn, valueFn);\n+    }\n+\n+    @Override\n+    public boolean isCooperative() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void init(@Nonnull Outbox outbox, @Nonnull Context context) {\n+        logger = context.logger();\n+        helper = new KinesisHelper(kinesis, stream, logger);\n+        helper.waitForStreamToActivate();\n+    }\n+\n+    @Override\n+    public boolean tryProcessWatermark(@Nonnull Watermark watermark) {\n+        return true; //watermark ignored\n+    }\n+\n+    @Override\n+    public void process(int ordinal, @Nonnull Inbox inbox) {\n+        switch (state) {\n+            case READY_TO_SEND:\n+                handleReadyToSend(inbox);\n+                return;\n+            case SENDING_IN_PROGRESS:\n+                handleSendingInProgress();\n+                return;\n+            default:\n+                throw new RuntimeException(\"Programming error, unhandled state: \" + state);\n+        }\n+\n+    }\n+\n+    private void bufferFromInbox(@Nonnull Inbox inbox) {\n+        if (buffer.isFull()) {\n+            return;\n+        }\n+\n+        while (true) {\n+            T t = (T) inbox.peek();\n+            if (t == null) {\n+                //no more items in inbox\n+                return;\n+            }\n+\n+            boolean canBeBuffered = buffer.add(t);\n+            if (canBeBuffered) {\n+                inbox.remove();\n+            } else {\n+                //no more room in buffer\n+                return;\n+            }\n+        }\n+    }\n+\n+    private void handleReadyToSend(@Nonnull Inbox inbox) {\n+        bufferFromInbox(inbox);\n+        if (attemptToDispatchBufferContent()) {\n+            state = State.SENDING_IN_PROGRESS;\n+        }\n+    }\n+\n+    private boolean attemptToDispatchBufferContent() {\n+        if (buffer.isEmpty() || System.currentTimeMillis() < nextSendTime) {\n+            return false;\n+        }\n+\n+        List<PutRecordsRequestEntry> entries = buffer.content();\n+        sendResult = helper.putRecordsAsync(entries);\n+\n+        nextSendTime = System.currentTimeMillis(); //todo: add some wait here?\n+        return true;\n+    }\n+\n+    private void handleSendingInProgress() {\n+        if (sendResult.isDone()) {\n+            try {\n+                PutRecordsResult result = helper.readResult(this.sendResult);\n+                pruneSentFromBuffer(result);\n+                if (result.getFailedRecordCount() > 0) {\n+                    dealWithSendFailure(\"Sending only partially successful. Retry sending failed items (ordering\" +\n+                            \" will be affected). \");\n+                } else {\n+                    dealWithSendSuccessful();\n+                }\n+            } catch (ProvisionedThroughputExceededException pte) {\n+                dealWithSendFailure(\"Data throughput rate exceeded. Backing off and retrying.\");\n+            } catch (SdkClientException sce) {\n+                dealWithSendFailure(\"Failed reading records, retrying. Cause: \" + sce.getMessage());\n+            } catch (Throwable t) {\n+                throw rethrow(t);\n+            }\n+        }\n+    }\n+\n+    private void dealWithSendSuccessful() {\n+        state = State.READY_TO_SEND;\n+    }\n+\n+    private void dealWithSendFailure(@Nonnull String message) {\n+        logger.warning(message);\n+        nextSendTime = System.currentTimeMillis() + PAUSE_AFTER_FAILURE;\n+        state = State.READY_TO_SEND;\n+    }\n+\n+    private void pruneSentFromBuffer(@Nullable PutRecordsResult result) {\n+        if (result == null) {\n+            return;\n+        }\n+\n+        int failCount = result.getFailedRecordCount();\n+        if (failCount > 0) {\n+            logger.warning(\"Failed sending \" + failCount + \" records to stream \" + stream + \". \" +\n+                    \"Sending them will be retried, but reordering might be unavoidable.\");\n+            List<PutRecordsResultEntry> resultEntries = result.getRecords();\n+            for (int i = resultEntries.size() - 1; i >= 0; i--) {\n+                PutRecordsResultEntry resultEntry = resultEntries.get(i);\n+                if (resultEntry.getErrorCode() == null) {\n+                    buffer.remove(i);\n+                }\n+            }\n+        } else {\n+            buffer.clear();\n+        }\n+    }\n+\n+    private enum State {\n+        /**\n+         * Ready to send data to Kinesis, if available.\n+         */\n+        READY_TO_SEND,\n+\n+        /**\n+         * Data has been sent to Kinesis, waiting for a reply.\n+         */\n+        SENDING_IN_PROGRESS,\n+    }\n+\n+    private static class Buffer<T> {\n+\n+        private final FunctionEx<T, String> keyFn;\n+        private final FunctionEx<T, byte[]> valueFn;\n+\n+        private final BufferEntry[] entries;\n+        private int entryCount;\n+        private int totalEntrySize;\n+\n+        Buffer(FunctionEx<T, String> keyFn, FunctionEx<T, byte[]> valueFn) {\n+            this.keyFn = keyFn;\n+            this.valueFn = valueFn;\n+            this.entries = initEntries();\n+        }\n+\n+        boolean add(T item) {\n+            if (entryCount == entries.length) {\n+                return false;\n+            }\n+\n+            String key = keyFn.apply(item);\n+            int unicodeCharsInKey = key.length();\n+            if (unicodeCharsInKey > MAX_UNICODE_CHARS_IN_KEY) {\n+                throw new IllegalArgumentException(\"Key of \" + item + \" too long\");\n+            }\n+            int keyLength = getKeyLength(key);\n+\n+            byte[] value = valueFn.apply(item);\n+            int itemLength = value.length + keyLength;\n+            if (itemLength > MAX_RECORD_SIZE_IN_BYTES) {\n+                throw new IllegalArgumentException(\"Item \" + item + \" encoded length (key + payload) is too big\");\n+            }\n+\n+            if (totalEntrySize + itemLength > MAX_REQUEST_SIZE_IN_BYTES) {\n+                return false;\n+            } else {\n+                totalEntrySize += itemLength;\n+\n+                BufferEntry entry = entries[entryCount++];\n+                entry.set(key, value, itemLength);\n+\n+                return true;\n+            }\n+        }\n+\n+        public void remove(int index) { //todo: test it, at least manually\n+            if (index < 0 || index >= entryCount) {\n+                throw new IllegalArgumentException(\"Index needs to be between 0 and \" + entryCount);\n+            }\n+\n+            totalEntrySize -= entries[index].encodedSize;\n+\n+            int lastIndex = entryCount - 1;\n+            if (index < lastIndex) {\n+                BufferEntry tmp = entries[index];\n+                entries[index] = entries[lastIndex];\n+                entries[lastIndex] = tmp;\n+            } else {\n+                entryCount--;\n+            }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d266c70b69fb4d4a8d61c6d33348b1399e9dca0"}, "originalPosition": 295}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDQ4OTQ4Nw==", "bodyText": "In general we should use nanoTime to measure elapsed time.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r530489487", "createdAt": "2020-11-25T16:11:55Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsResultEntry;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.core.Inbox;\n+import com.hazelcast.jet.core.Outbox;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.Watermark;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+\n+public class KinesisSinkP<T> implements Processor {\n+\n+    //todo; Each shard can support writes up to 1,000 records per second, up to a\n+    // maximum data write total of 1 MiB per second. Right now we don't pre-check\n+    // this, because we don't check which item goes to what shard, we just rely\n+    // failure handling with exponential backoff. Would be complicated to improve\n+    // on, but can we affort not to?\n+\n+    /**\n+     * PutRecords requests are limited to 500 records.\n+     */\n+    private static final int MAX_RECORDS_IN_REQUEST = 500;\n+\n+    private static final long PAUSE_AFTER_FAILURE = 1000L; //todo: exponential backoff\n+\n+    /**\n+     * Each record, when encoded as a byte array, is limited to 1M,\n+     * including the partition key (Unicode String).\n+     */\n+    private static final int MAX_RECORD_SIZE_IN_BYTES = 1024 * 1024;\n+\n+    /**\n+     * The maximum allowed size of all the records in a PutRecords request,\n+     * including partition keys is 5M.\n+     */\n+    private static final int MAX_REQUEST_SIZE_IN_BYTES = 5 * 1024 * 1024;\n+\n+    /**\n+     * The number of Unicode characters making up keys is limited to a maximum\n+     * of 256.\n+     */\n+    private static final int MAX_UNICODE_CHARS_IN_KEY = 256;\n+\n+    private final AmazonKinesisAsync kinesis;\n+    private final String stream;\n+\n+    private final Buffer<T> buffer;\n+\n+    private ILogger logger;\n+    private KinesisHelper helper;\n+\n+    private State state = State.READY_TO_SEND;\n+    private long nextSendTime;\n+    private Future<PutRecordsResult> sendResult;\n+\n+    public KinesisSinkP(\n+            AmazonKinesisAsync kinesis,\n+            @Nonnull String stream,\n+            @Nonnull FunctionEx<T, String> keyFn,\n+            @Nonnull FunctionEx<T, byte[]> valueFn\n+    ) {\n+        this.kinesis = kinesis;\n+        this.stream = stream;\n+        this.buffer = new Buffer<>(keyFn, valueFn);\n+    }\n+\n+    @Override\n+    public boolean isCooperative() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void init(@Nonnull Outbox outbox, @Nonnull Context context) {\n+        logger = context.logger();\n+        helper = new KinesisHelper(kinesis, stream, logger);\n+        helper.waitForStreamToActivate();\n+    }\n+\n+    @Override\n+    public boolean tryProcessWatermark(@Nonnull Watermark watermark) {\n+        return true; //watermark ignored\n+    }\n+\n+    @Override\n+    public void process(int ordinal, @Nonnull Inbox inbox) {\n+        switch (state) {\n+            case READY_TO_SEND:\n+                handleReadyToSend(inbox);\n+                return;\n+            case SENDING_IN_PROGRESS:\n+                handleSendingInProgress();\n+                return;\n+            default:\n+                throw new RuntimeException(\"Programming error, unhandled state: \" + state);\n+        }\n+\n+    }\n+\n+    private void bufferFromInbox(@Nonnull Inbox inbox) {\n+        if (buffer.isFull()) {\n+            return;\n+        }\n+\n+        while (true) {\n+            T t = (T) inbox.peek();\n+            if (t == null) {\n+                //no more items in inbox\n+                return;\n+            }\n+\n+            boolean canBeBuffered = buffer.add(t);\n+            if (canBeBuffered) {\n+                inbox.remove();\n+            } else {\n+                //no more room in buffer\n+                return;\n+            }\n+        }\n+    }\n+\n+    private void handleReadyToSend(@Nonnull Inbox inbox) {\n+        bufferFromInbox(inbox);\n+        if (attemptToDispatchBufferContent()) {\n+            state = State.SENDING_IN_PROGRESS;\n+        }\n+    }\n+\n+    private boolean attemptToDispatchBufferContent() {\n+        if (buffer.isEmpty() || System.currentTimeMillis() < nextSendTime) {\n+            return false;\n+        }\n+\n+        List<PutRecordsRequestEntry> entries = buffer.content();\n+        sendResult = helper.putRecordsAsync(entries);\n+\n+        nextSendTime = System.currentTimeMillis(); //todo: add some wait here?\n+        return true;\n+    }\n+\n+    private void handleSendingInProgress() {\n+        if (sendResult.isDone()) {\n+            try {\n+                PutRecordsResult result = helper.readResult(this.sendResult);\n+                pruneSentFromBuffer(result);\n+                if (result.getFailedRecordCount() > 0) {\n+                    dealWithSendFailure(\"Sending only partially successful. Retry sending failed items (ordering\" +\n+                            \" will be affected). \");\n+                } else {\n+                    dealWithSendSuccessful();\n+                }\n+            } catch (ProvisionedThroughputExceededException pte) {\n+                dealWithSendFailure(\"Data throughput rate exceeded. Backing off and retrying.\");\n+            } catch (SdkClientException sce) {\n+                dealWithSendFailure(\"Failed reading records, retrying. Cause: \" + sce.getMessage());\n+            } catch (Throwable t) {\n+                throw rethrow(t);\n+            }\n+        }\n+    }\n+\n+    private void dealWithSendSuccessful() {\n+        state = State.READY_TO_SEND;\n+    }\n+\n+    private void dealWithSendFailure(@Nonnull String message) {\n+        logger.warning(message);\n+        nextSendTime = System.currentTimeMillis() + PAUSE_AFTER_FAILURE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d266c70b69fb4d4a8d61c6d33348b1399e9dca0"}, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDQ5Mjg2Mg==", "bodyText": "Code was correct, but it's a long reading ;)\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    if (buffer.isFull()) {\n          \n          \n            \n                        return;\n          \n          \n            \n                    }\n          \n          \n            \n            \n          \n          \n            \n                    while (true) {\n          \n          \n            \n                        T t = (T) inbox.peek();\n          \n          \n            \n                        if (t == null) {\n          \n          \n            \n                            //no more items in inbox\n          \n          \n            \n                            return;\n          \n          \n            \n                        }\n          \n          \n            \n            \n          \n          \n            \n                        boolean canBeBuffered = buffer.add(t);\n          \n          \n            \n                        if (canBeBuffered) {\n          \n          \n            \n                            inbox.remove();\n          \n          \n            \n                        } else {\n          \n          \n            \n                            //no more room in buffer\n          \n          \n            \n                            return;\n          \n          \n            \n                        }\n          \n          \n            \n                    }\n          \n          \n            \n                    for (T t; (t = (T) inbox.peek()) != null && buffer.add(t); ) {\n          \n          \n            \n                        inbox.remove();\n          \n          \n            \n                    }", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r530492862", "createdAt": "2020-11-25T16:16:52Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsResultEntry;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.core.Inbox;\n+import com.hazelcast.jet.core.Outbox;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.Watermark;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+\n+public class KinesisSinkP<T> implements Processor {\n+\n+    //todo; Each shard can support writes up to 1,000 records per second, up to a\n+    // maximum data write total of 1 MiB per second. Right now we don't pre-check\n+    // this, because we don't check which item goes to what shard, we just rely\n+    // failure handling with exponential backoff. Would be complicated to improve\n+    // on, but can we affort not to?\n+\n+    /**\n+     * PutRecords requests are limited to 500 records.\n+     */\n+    private static final int MAX_RECORDS_IN_REQUEST = 500;\n+\n+    private static final long PAUSE_AFTER_FAILURE = 1000L; //todo: exponential backoff\n+\n+    /**\n+     * Each record, when encoded as a byte array, is limited to 1M,\n+     * including the partition key (Unicode String).\n+     */\n+    private static final int MAX_RECORD_SIZE_IN_BYTES = 1024 * 1024;\n+\n+    /**\n+     * The maximum allowed size of all the records in a PutRecords request,\n+     * including partition keys is 5M.\n+     */\n+    private static final int MAX_REQUEST_SIZE_IN_BYTES = 5 * 1024 * 1024;\n+\n+    /**\n+     * The number of Unicode characters making up keys is limited to a maximum\n+     * of 256.\n+     */\n+    private static final int MAX_UNICODE_CHARS_IN_KEY = 256;\n+\n+    private final AmazonKinesisAsync kinesis;\n+    private final String stream;\n+\n+    private final Buffer<T> buffer;\n+\n+    private ILogger logger;\n+    private KinesisHelper helper;\n+\n+    private State state = State.READY_TO_SEND;\n+    private long nextSendTime;\n+    private Future<PutRecordsResult> sendResult;\n+\n+    public KinesisSinkP(\n+            AmazonKinesisAsync kinesis,\n+            @Nonnull String stream,\n+            @Nonnull FunctionEx<T, String> keyFn,\n+            @Nonnull FunctionEx<T, byte[]> valueFn\n+    ) {\n+        this.kinesis = kinesis;\n+        this.stream = stream;\n+        this.buffer = new Buffer<>(keyFn, valueFn);\n+    }\n+\n+    @Override\n+    public boolean isCooperative() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void init(@Nonnull Outbox outbox, @Nonnull Context context) {\n+        logger = context.logger();\n+        helper = new KinesisHelper(kinesis, stream, logger);\n+        helper.waitForStreamToActivate();\n+    }\n+\n+    @Override\n+    public boolean tryProcessWatermark(@Nonnull Watermark watermark) {\n+        return true; //watermark ignored\n+    }\n+\n+    @Override\n+    public void process(int ordinal, @Nonnull Inbox inbox) {\n+        switch (state) {\n+            case READY_TO_SEND:\n+                handleReadyToSend(inbox);\n+                return;\n+            case SENDING_IN_PROGRESS:\n+                handleSendingInProgress();\n+                return;\n+            default:\n+                throw new RuntimeException(\"Programming error, unhandled state: \" + state);\n+        }\n+\n+    }\n+\n+    private void bufferFromInbox(@Nonnull Inbox inbox) {\n+        if (buffer.isFull()) {\n+            return;\n+        }\n+\n+        while (true) {\n+            T t = (T) inbox.peek();\n+            if (t == null) {\n+                //no more items in inbox\n+                return;\n+            }\n+\n+            boolean canBeBuffered = buffer.add(t);\n+            if (canBeBuffered) {\n+                inbox.remove();\n+            } else {\n+                //no more room in buffer\n+                return;\n+            }\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d266c70b69fb4d4a8d61c6d33348b1399e9dca0"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDQ5NDc4MA==", "bodyText": "Should initialize to MIN_VALUE, it won't start sending until 1.1.1970 otherwise :-)\nEspecially if we switch to nanoTime.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r530494780", "createdAt": "2020-11-25T16:19:35Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsResultEntry;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.core.Inbox;\n+import com.hazelcast.jet.core.Outbox;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.Watermark;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+\n+public class KinesisSinkP<T> implements Processor {\n+\n+    //todo; Each shard can support writes up to 1,000 records per second, up to a\n+    // maximum data write total of 1 MiB per second. Right now we don't pre-check\n+    // this, because we don't check which item goes to what shard, we just rely\n+    // failure handling with exponential backoff. Would be complicated to improve\n+    // on, but can we affort not to?\n+\n+    /**\n+     * PutRecords requests are limited to 500 records.\n+     */\n+    private static final int MAX_RECORDS_IN_REQUEST = 500;\n+\n+    private static final long PAUSE_AFTER_FAILURE = 1000L; //todo: exponential backoff\n+\n+    /**\n+     * Each record, when encoded as a byte array, is limited to 1M,\n+     * including the partition key (Unicode String).\n+     */\n+    private static final int MAX_RECORD_SIZE_IN_BYTES = 1024 * 1024;\n+\n+    /**\n+     * The maximum allowed size of all the records in a PutRecords request,\n+     * including partition keys is 5M.\n+     */\n+    private static final int MAX_REQUEST_SIZE_IN_BYTES = 5 * 1024 * 1024;\n+\n+    /**\n+     * The number of Unicode characters making up keys is limited to a maximum\n+     * of 256.\n+     */\n+    private static final int MAX_UNICODE_CHARS_IN_KEY = 256;\n+\n+    private final AmazonKinesisAsync kinesis;\n+    private final String stream;\n+\n+    private final Buffer<T> buffer;\n+\n+    private ILogger logger;\n+    private KinesisHelper helper;\n+\n+    private State state = State.READY_TO_SEND;\n+    private long nextSendTime;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d266c70b69fb4d4a8d61c6d33348b1399e9dca0"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDQ5NjM3Ng==", "bodyText": "This is unnecessary now. It can stay in the past unless we see a reason to postpone next sending.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r530496376", "createdAt": "2020-11-25T16:21:55Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsResultEntry;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.core.Inbox;\n+import com.hazelcast.jet.core.Outbox;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.Watermark;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+\n+public class KinesisSinkP<T> implements Processor {\n+\n+    //todo; Each shard can support writes up to 1,000 records per second, up to a\n+    // maximum data write total of 1 MiB per second. Right now we don't pre-check\n+    // this, because we don't check which item goes to what shard, we just rely\n+    // failure handling with exponential backoff. Would be complicated to improve\n+    // on, but can we affort not to?\n+\n+    /**\n+     * PutRecords requests are limited to 500 records.\n+     */\n+    private static final int MAX_RECORDS_IN_REQUEST = 500;\n+\n+    private static final long PAUSE_AFTER_FAILURE = 1000L; //todo: exponential backoff\n+\n+    /**\n+     * Each record, when encoded as a byte array, is limited to 1M,\n+     * including the partition key (Unicode String).\n+     */\n+    private static final int MAX_RECORD_SIZE_IN_BYTES = 1024 * 1024;\n+\n+    /**\n+     * The maximum allowed size of all the records in a PutRecords request,\n+     * including partition keys is 5M.\n+     */\n+    private static final int MAX_REQUEST_SIZE_IN_BYTES = 5 * 1024 * 1024;\n+\n+    /**\n+     * The number of Unicode characters making up keys is limited to a maximum\n+     * of 256.\n+     */\n+    private static final int MAX_UNICODE_CHARS_IN_KEY = 256;\n+\n+    private final AmazonKinesisAsync kinesis;\n+    private final String stream;\n+\n+    private final Buffer<T> buffer;\n+\n+    private ILogger logger;\n+    private KinesisHelper helper;\n+\n+    private State state = State.READY_TO_SEND;\n+    private long nextSendTime;\n+    private Future<PutRecordsResult> sendResult;\n+\n+    public KinesisSinkP(\n+            AmazonKinesisAsync kinesis,\n+            @Nonnull String stream,\n+            @Nonnull FunctionEx<T, String> keyFn,\n+            @Nonnull FunctionEx<T, byte[]> valueFn\n+    ) {\n+        this.kinesis = kinesis;\n+        this.stream = stream;\n+        this.buffer = new Buffer<>(keyFn, valueFn);\n+    }\n+\n+    @Override\n+    public boolean isCooperative() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void init(@Nonnull Outbox outbox, @Nonnull Context context) {\n+        logger = context.logger();\n+        helper = new KinesisHelper(kinesis, stream, logger);\n+        helper.waitForStreamToActivate();\n+    }\n+\n+    @Override\n+    public boolean tryProcessWatermark(@Nonnull Watermark watermark) {\n+        return true; //watermark ignored\n+    }\n+\n+    @Override\n+    public void process(int ordinal, @Nonnull Inbox inbox) {\n+        switch (state) {\n+            case READY_TO_SEND:\n+                handleReadyToSend(inbox);\n+                return;\n+            case SENDING_IN_PROGRESS:\n+                handleSendingInProgress();\n+                return;\n+            default:\n+                throw new RuntimeException(\"Programming error, unhandled state: \" + state);\n+        }\n+\n+    }\n+\n+    private void bufferFromInbox(@Nonnull Inbox inbox) {\n+        if (buffer.isFull()) {\n+            return;\n+        }\n+\n+        while (true) {\n+            T t = (T) inbox.peek();\n+            if (t == null) {\n+                //no more items in inbox\n+                return;\n+            }\n+\n+            boolean canBeBuffered = buffer.add(t);\n+            if (canBeBuffered) {\n+                inbox.remove();\n+            } else {\n+                //no more room in buffer\n+                return;\n+            }\n+        }\n+    }\n+\n+    private void handleReadyToSend(@Nonnull Inbox inbox) {\n+        bufferFromInbox(inbox);\n+        if (attemptToDispatchBufferContent()) {\n+            state = State.SENDING_IN_PROGRESS;\n+        }\n+    }\n+\n+    private boolean attemptToDispatchBufferContent() {\n+        if (buffer.isEmpty() || System.currentTimeMillis() < nextSendTime) {\n+            return false;\n+        }\n+\n+        List<PutRecordsRequestEntry> entries = buffer.content();\n+        sendResult = helper.putRecordsAsync(entries);\n+\n+        nextSendTime = System.currentTimeMillis(); //todo: add some wait here?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d266c70b69fb4d4a8d61c6d33348b1399e9dca0"}, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDQ5OTUyOA==", "bodyText": "In the current code, if sending in progress was just done, we won't send another batch immediately. We'll wait for a next call. The ProcessorTasklet will think no progress was made, so it will back off a little, unnecessarily.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    switch (state) {\n          \n          \n            \n                        case READY_TO_SEND:\n          \n          \n            \n                            handleReadyToSend(inbox);\n          \n          \n            \n                            return;\n          \n          \n            \n                        case SENDING_IN_PROGRESS:\n          \n          \n            \n                            handleSendingInProgress();\n          \n          \n            \n                            return;\n          \n          \n            \n                        default:\n          \n          \n            \n                            throw new RuntimeException(\"Programming error, unhandled state: \" + state);\n          \n          \n            \n                    }\n          \n          \n            \n                    if (state == State.SENDING_IN_PROGRESS) {\n          \n          \n            \n                        handleSendingInProgress();\n          \n          \n            \n                    }\n          \n          \n            \n                    if (state == State.READY_TO_SEND) {\n          \n          \n            \n                        handleReadyToSend(inbox);\n          \n          \n            \n                    }\n          \n      \n    \n    \n  \n\nNote there's no else before the second if - the state might change after the first call.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r530499528", "createdAt": "2020-11-25T16:26:29Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsResultEntry;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.core.Inbox;\n+import com.hazelcast.jet.core.Outbox;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.Watermark;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+\n+public class KinesisSinkP<T> implements Processor {\n+\n+    //todo; Each shard can support writes up to 1,000 records per second, up to a\n+    // maximum data write total of 1 MiB per second. Right now we don't pre-check\n+    // this, because we don't check which item goes to what shard, we just rely\n+    // failure handling with exponential backoff. Would be complicated to improve\n+    // on, but can we affort not to?\n+\n+    /**\n+     * PutRecords requests are limited to 500 records.\n+     */\n+    private static final int MAX_RECORDS_IN_REQUEST = 500;\n+\n+    private static final long PAUSE_AFTER_FAILURE = 1000L; //todo: exponential backoff\n+\n+    /**\n+     * Each record, when encoded as a byte array, is limited to 1M,\n+     * including the partition key (Unicode String).\n+     */\n+    private static final int MAX_RECORD_SIZE_IN_BYTES = 1024 * 1024;\n+\n+    /**\n+     * The maximum allowed size of all the records in a PutRecords request,\n+     * including partition keys is 5M.\n+     */\n+    private static final int MAX_REQUEST_SIZE_IN_BYTES = 5 * 1024 * 1024;\n+\n+    /**\n+     * The number of Unicode characters making up keys is limited to a maximum\n+     * of 256.\n+     */\n+    private static final int MAX_UNICODE_CHARS_IN_KEY = 256;\n+\n+    private final AmazonKinesisAsync kinesis;\n+    private final String stream;\n+\n+    private final Buffer<T> buffer;\n+\n+    private ILogger logger;\n+    private KinesisHelper helper;\n+\n+    private State state = State.READY_TO_SEND;\n+    private long nextSendTime;\n+    private Future<PutRecordsResult> sendResult;\n+\n+    public KinesisSinkP(\n+            AmazonKinesisAsync kinesis,\n+            @Nonnull String stream,\n+            @Nonnull FunctionEx<T, String> keyFn,\n+            @Nonnull FunctionEx<T, byte[]> valueFn\n+    ) {\n+        this.kinesis = kinesis;\n+        this.stream = stream;\n+        this.buffer = new Buffer<>(keyFn, valueFn);\n+    }\n+\n+    @Override\n+    public boolean isCooperative() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void init(@Nonnull Outbox outbox, @Nonnull Context context) {\n+        logger = context.logger();\n+        helper = new KinesisHelper(kinesis, stream, logger);\n+        helper.waitForStreamToActivate();\n+    }\n+\n+    @Override\n+    public boolean tryProcessWatermark(@Nonnull Watermark watermark) {\n+        return true; //watermark ignored\n+    }\n+\n+    @Override\n+    public void process(int ordinal, @Nonnull Inbox inbox) {\n+        switch (state) {\n+            case READY_TO_SEND:\n+                handleReadyToSend(inbox);\n+                return;\n+            case SENDING_IN_PROGRESS:\n+                handleSendingInProgress();\n+                return;\n+            default:\n+                throw new RuntimeException(\"Programming error, unhandled state: \" + state);\n+        }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d266c70b69fb4d4a8d61c6d33348b1399e9dca0"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDUwMzA4MQ==", "bodyText": "We must override the complete() method and ensure that the in-progress items are finished. Currently, the job will be terminated before the last request completes. And if it doesn't complete successfully, the items will be lost.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r530503081", "createdAt": "2020-11-25T16:31:46Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsResultEntry;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.core.Inbox;\n+import com.hazelcast.jet.core.Outbox;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.Watermark;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+\n+public class KinesisSinkP<T> implements Processor {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d266c70b69fb4d4a8d61c6d33348b1399e9dca0"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDUwNDM3MQ==", "bodyText": "For example, if the inbox has 501 items in it, we'll send it in at least 2 batches: 500 items in the first and 1 in the second. The reason is that no more items are added to the inbox until it's made empty by the process method.\nThe solution would be to fill up the buffer while waiting for the response. But I think we won't be able to reuse the ByteBuffers the way we do now. Also retries would need to be handled in another way.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r530504371", "createdAt": "2020-11-25T16:33:53Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSinkP.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.PutRecordsRequestEntry;\n+import com.amazonaws.services.kinesis.model.PutRecordsResult;\n+import com.amazonaws.services.kinesis.model.PutRecordsResultEntry;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.core.Inbox;\n+import com.hazelcast.jet.core.Outbox;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.Watermark;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.concurrent.Future;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+\n+public class KinesisSinkP<T> implements Processor {\n+\n+    //todo; Each shard can support writes up to 1,000 records per second, up to a\n+    // maximum data write total of 1 MiB per second. Right now we don't pre-check\n+    // this, because we don't check which item goes to what shard, we just rely\n+    // failure handling with exponential backoff. Would be complicated to improve\n+    // on, but can we affort not to?\n+\n+    /**\n+     * PutRecords requests are limited to 500 records.\n+     */\n+    private static final int MAX_RECORDS_IN_REQUEST = 500;\n+\n+    private static final long PAUSE_AFTER_FAILURE = 1000L; //todo: exponential backoff\n+\n+    /**\n+     * Each record, when encoded as a byte array, is limited to 1M,\n+     * including the partition key (Unicode String).\n+     */\n+    private static final int MAX_RECORD_SIZE_IN_BYTES = 1024 * 1024;\n+\n+    /**\n+     * The maximum allowed size of all the records in a PutRecords request,\n+     * including partition keys is 5M.\n+     */\n+    private static final int MAX_REQUEST_SIZE_IN_BYTES = 5 * 1024 * 1024;\n+\n+    /**\n+     * The number of Unicode characters making up keys is limited to a maximum\n+     * of 256.\n+     */\n+    private static final int MAX_UNICODE_CHARS_IN_KEY = 256;\n+\n+    private final AmazonKinesisAsync kinesis;\n+    private final String stream;\n+\n+    private final Buffer<T> buffer;\n+\n+    private ILogger logger;\n+    private KinesisHelper helper;\n+\n+    private State state = State.READY_TO_SEND;\n+    private long nextSendTime;\n+    private Future<PutRecordsResult> sendResult;\n+\n+    public KinesisSinkP(\n+            AmazonKinesisAsync kinesis,\n+            @Nonnull String stream,\n+            @Nonnull FunctionEx<T, String> keyFn,\n+            @Nonnull FunctionEx<T, byte[]> valueFn\n+    ) {\n+        this.kinesis = kinesis;\n+        this.stream = stream;\n+        this.buffer = new Buffer<>(keyFn, valueFn);\n+    }\n+\n+    @Override\n+    public boolean isCooperative() {\n+        return true;\n+    }\n+\n+    @Override\n+    public void init(@Nonnull Outbox outbox, @Nonnull Context context) {\n+        logger = context.logger();\n+        helper = new KinesisHelper(kinesis, stream, logger);\n+        helper.waitForStreamToActivate();\n+    }\n+\n+    @Override\n+    public boolean tryProcessWatermark(@Nonnull Watermark watermark) {\n+        return true; //watermark ignored\n+    }\n+\n+    @Override\n+    public void process(int ordinal, @Nonnull Inbox inbox) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d266c70b69fb4d4a8d61c6d33348b1399e9dca0"}, "originalPosition": 117}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6463e2631d1a155984e553b8c221f080135b9394", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/6463e2631d1a155984e553b8c221f080135b9394", "committedDate": "2020-11-25T16:50:21Z", "message": "Grammar, remove double logging"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fb266950d88d91d177b3ab05de7ec25e4c181a8b", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/fb266950d88d91d177b3ab05de7ec25e4c181a8b", "committedDate": "2020-11-26T09:36:31Z", "message": "Address review concerns"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "49ebee2691e0221bcb044ab990192a3bffd6d7e6", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/49ebee2691e0221bcb044ab990192a3bffd6d7e6", "committedDate": "2020-11-26T09:36:46Z", "message": "Merge remote-tracking branch 'origin/kinesis' into kinesis"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5c321cd75e22181553597815b51eccd3efd71544", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/5c321cd75e22181553597815b51eccd3efd71544", "committedDate": "2020-11-26T09:57:39Z", "message": "Merge branch 'master' into kinesis"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4173407054222ba04b96b03e797caf73650579db", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/4173407054222ba04b96b03e797caf73650579db", "committedDate": "2020-11-26T09:58:26Z", "message": "Fix jackson version"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d6eb91bab7a1bf86f775e78041b9bd883190de57", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/d6eb91bab7a1bf86f775e78041b9bd883190de57", "committedDate": "2020-11-30T14:25:19Z", "message": "Minor changes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwOTA3MzQ5", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#pullrequestreview-540907349", "createdAt": "2020-11-30T13:29:38Z", "commit": {"oid": "4173407054222ba04b96b03e797caf73650579db"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxMzoyOTozOFrOH77HXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoxNjoyM1rOH79BmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjU5NjU3NQ==", "bodyText": "Javadoc of Collectors.toSet() says:\n\nThere are no guarantees on the type, mutability, serializability, or thread-safety of the Set returned; if more control over the returned Set is required, use toCollection(Supplier).", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r532596575", "createdAt": "2020-11-30T13:29:38Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/RangeMonitor.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ListShardsResult;\n+import com.amazonaws.services.kinesis.model.Shard;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.Future;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+import static com.hazelcast.jet.kinesis.impl.KinesisHelper.shardBelongsToRange;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+import static java.util.stream.Collectors.joining;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toSet;\n+\n+public class RangeMonitor extends AbstractShardWorker {\n+\n+    /**\n+     * ListStreams operations are limited to 100 per second, per data stream\n+     */\n+    private static final int SHARD_LISTINGS_ALLOWED_PER_SECOND = 100;\n+\n+    /**\n+     * We don't want to issue shard listing requests at the peak allowed rate.\n+     */\n+    private static final double PERCENTAGE_OF_SHARD_LISTING_RATE_UTILIZED = 0.1;\n+\n+    /**\n+     * Failure usually happens due to the over-utilization of resources and/or\n+     * crossing of various limits. Even if we retry the operation it is a good\n+     * idea to add some waits (decrease the rate) in order to alleviate the\n+     * problem.\n+     */\n+    private static final long PAUSE_AFTER_FAILURE = SECONDS.toNanos(1); //todo: exponential backoff\n+\n+    //todo: never removing from the set of known shards, because I have to read from all shards, not\n+    // just the active ones and I have to not read from shards that are closed and I have read from them already...\n+\n+    private final HashRange hashRange;\n+    private final Set<String> knownShards;\n+    private final RandomizedRateTracker listShardsRateTracker;\n+    private final ILogger logger;\n+    private final List<Shard> newShards = new ArrayList<>();\n+\n+    private State state = State.READY_TO_LIST_SHARDS;\n+    private String nextToken;\n+    private Future<ListShardsResult> listShardResult;\n+    private long nextListShardsTime;\n+\n+    public RangeMonitor(\n+            int totalInstances,\n+            AmazonKinesisAsync kinesis,\n+            String stream,\n+            HashRange hashRange,\n+            Collection<Shard> knownShards,\n+            ILogger logger\n+    ) {\n+        super(kinesis, stream, logger);\n+        this.logger = logger;\n+        this.hashRange = hashRange;\n+        this.knownShards = knownShards.stream().map(Shard::getShardId).collect(toSet());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4173407054222ba04b96b03e797caf73650579db"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYwMTc4OQ==", "bodyText": "This class seems a bit complex to me, unnecessarily.\nInstead of the Result enum we could return a collection that would be empty or not. Currently we expect that after returning a NEW_SHARDS, the caller is expected to call getNewShards(), and then the next call to run() switches the state to next state.\nWe can also get rid of the state: if listShardResult is not null, wait until it's done. If it's null, wait until it's time and issue a new request.\nThe run method would be better named probe: it will not block, but return the list of new shards, if it has some.\nThis is a suggestion, it's a matter of style, the current code is correct as far as I can tell. IMO it will be easier to read.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r532601789", "createdAt": "2020-11-30T13:37:52Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/RangeMonitor.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ListShardsResult;\n+import com.amazonaws.services.kinesis.model.Shard;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.Future;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+import static com.hazelcast.jet.kinesis.impl.KinesisHelper.shardBelongsToRange;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+import static java.util.stream.Collectors.joining;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toSet;\n+\n+public class RangeMonitor extends AbstractShardWorker {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4173407054222ba04b96b03e797caf73650579db"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYwMjM1Mw==", "bodyText": "We could move the catch clause just after helper.readResult. It will be clearer where the error occurs.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r532602353", "createdAt": "2020-11-30T13:38:48Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/RangeMonitor.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ListShardsResult;\n+import com.amazonaws.services.kinesis.model.Shard;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.Future;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+import static com.hazelcast.jet.kinesis.impl.KinesisHelper.shardBelongsToRange;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+import static java.util.stream.Collectors.joining;\n+import static java.util.stream.Collectors.toList;\n+import static java.util.stream.Collectors.toSet;\n+\n+public class RangeMonitor extends AbstractShardWorker {\n+\n+    /**\n+     * ListStreams operations are limited to 100 per second, per data stream\n+     */\n+    private static final int SHARD_LISTINGS_ALLOWED_PER_SECOND = 100;\n+\n+    /**\n+     * We don't want to issue shard listing requests at the peak allowed rate.\n+     */\n+    private static final double PERCENTAGE_OF_SHARD_LISTING_RATE_UTILIZED = 0.1;\n+\n+    /**\n+     * Failure usually happens due to the over-utilization of resources and/or\n+     * crossing of various limits. Even if we retry the operation it is a good\n+     * idea to add some waits (decrease the rate) in order to alleviate the\n+     * problem.\n+     */\n+    private static final long PAUSE_AFTER_FAILURE = SECONDS.toNanos(1); //todo: exponential backoff\n+\n+    //todo: never removing from the set of known shards, because I have to read from all shards, not\n+    // just the active ones and I have to not read from shards that are closed and I have read from them already...\n+\n+    private final HashRange hashRange;\n+    private final Set<String> knownShards;\n+    private final RandomizedRateTracker listShardsRateTracker;\n+    private final ILogger logger;\n+    private final List<Shard> newShards = new ArrayList<>();\n+\n+    private State state = State.READY_TO_LIST_SHARDS;\n+    private String nextToken;\n+    private Future<ListShardsResult> listShardResult;\n+    private long nextListShardsTime;\n+\n+    public RangeMonitor(\n+            int totalInstances,\n+            AmazonKinesisAsync kinesis,\n+            String stream,\n+            HashRange hashRange,\n+            Collection<Shard> knownShards,\n+            ILogger logger\n+    ) {\n+        super(kinesis, stream, logger);\n+        this.logger = logger;\n+        this.hashRange = hashRange;\n+        this.knownShards = knownShards.stream().map(Shard::getShardId).collect(toSet());\n+        this.listShardsRateTracker = initRandomizedTracker(totalInstances);\n+        this.nextListShardsTime = System.nanoTime() + listShardsRateTracker.next();\n+    }\n+\n+    public Result run() {\n+        switch (state) {\n+            case READY_TO_LIST_SHARDS:\n+                return handleReadyToListShards();\n+            case WAITING_FOR_SHARD_LIST:\n+                return handleWaitingForShardList();\n+            case NEW_SHARDS_FOUND:\n+                return handleNewShardsFound();\n+            default:\n+                throw new RuntimeException(\"Programming error, unhandled state: \" + state);\n+        }\n+    }\n+\n+    private Result handleReadyToListShards() {\n+        if (System.nanoTime() < nextListShardsTime) {\n+            return Result.NOTHING;\n+        }\n+\n+        listShardResult = helper.listShardsAsync(nextToken);\n+        state = State.WAITING_FOR_SHARD_LIST;\n+\n+        nextListShardsTime = System.nanoTime() + listShardsRateTracker.next();\n+\n+        return Result.NOTHING;\n+    }\n+\n+    private Result handleWaitingForShardList() {\n+        if (listShardResult.isDone()) {\n+            try {\n+                ListShardsResult result = helper.readResult(listShardResult);\n+                nextToken = result.getNextToken();\n+\n+                List<Shard> shards = result.getShards();\n+\n+                List<Shard> unknownShards = shards.stream()\n+                        .filter(shard -> shardBelongsToRange(shard, hashRange))\n+                        .filter(shard -> !knownShards.contains(shard.getShardId())).collect(toList());\n+\n+                if (unknownShards.isEmpty()) {\n+                    state = State.READY_TO_LIST_SHARDS;\n+                    return Result.NOTHING;\n+                } else {\n+                    knownShards.addAll(unknownShards.stream().map(Shard::getShardId).collect(toList()));\n+                    newShards.addAll(unknownShards);\n+                    logger.info(\"New shards detected: \" +\n+                            unknownShards.stream().map(Shard::getShardId).collect(joining(\", \")));\n+                    state = State.NEW_SHARDS_FOUND;\n+                    return Result.NEW_SHARDS;\n+                }\n+            } catch (SdkClientException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4173407054222ba04b96b03e797caf73650579db"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYxNDM3MA==", "bodyText": "Both rangeMonitor and readers check System.nanoTime. It's quite expensive operation if run on the hot path. It might be better to read it once in complete and pass the current value to these methods.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r532614370", "createdAt": "2020-11-30T13:56:58Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSourceP.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.Record;\n+import com.amazonaws.services.kinesis.model.Shard;\n+import com.hazelcast.jet.Traverser;\n+import com.hazelcast.jet.Traversers;\n+import com.hazelcast.jet.core.AbstractProcessor;\n+import com.hazelcast.jet.core.EventTimeMapper;\n+import com.hazelcast.jet.core.EventTimePolicy;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+\n+import static com.hazelcast.jet.Util.entry;\n+import static com.hazelcast.jet.kinesis.impl.KinesisHelper.shardBelongsToRange;\n+\n+public class KinesisSourceP extends AbstractProcessor {\n+\n+    @Nonnull\n+    private final AmazonKinesisAsync kinesis;\n+    @Nonnull\n+    private final String stream;\n+    @Nonnull\n+    private final EventTimeMapper<? super Map.Entry<String, byte[]>> eventTimeMapper;\n+    @Nonnull\n+    private final HashRange hashRange;\n+\n+    private int id;\n+    private ILogger logger;\n+\n+    private Traverser<Object> traverser = Traversers.empty();\n+\n+    private KinesisHelper helper;\n+    private RangeMonitor rangeMonitor;\n+    private List<ShardReader> shardReaders = new ArrayList<>();\n+    private int nextReader;\n+\n+    public KinesisSourceP(\n+            @Nonnull AmazonKinesisAsync kinesis,\n+            @Nonnull String stream,\n+            @Nonnull EventTimePolicy<? super Map.Entry<String, byte[]>> eventTimePolicy,\n+            @Nonnull HashRange hashRange\n+    ) {\n+        this.kinesis = Objects.requireNonNull(kinesis, \"kinesis\");\n+        this.stream = Objects.requireNonNull(stream, \"stream\");\n+        this.eventTimeMapper = new EventTimeMapper<>(eventTimePolicy);\n+        this.hashRange = Objects.requireNonNull(hashRange, \"hashRange\");\n+    }\n+\n+    @Override\n+    protected void init(@Nonnull Context context) throws Exception {\n+        super.init(context);\n+\n+        logger = context.logger();\n+        id = context.globalProcessorIndex();\n+\n+        helper = new KinesisHelper(kinesis, stream, logger);\n+\n+        logger.info(\"Processor \" + id + \" handles \" + hashRange);\n+\n+        helper.waitForStreamToActivate();\n+        List<Shard> shardsInRange = helper.listShards(\n+                (Predicate<? super Shard>) shard -> shardBelongsToRange(shard, hashRange));\n+        rangeMonitor = new RangeMonitor(context.totalParallelism(), kinesis, stream, hashRange, shardsInRange, logger);\n+        addShardReaders(shardsInRange);\n+    }\n+\n+    @Override\n+    public boolean complete() {\n+        if (!emitFromTraverser(traverser)) {\n+            return false;\n+        }\n+\n+        runMonitor();\n+        runReaders();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4173407054222ba04b96b03e797caf73650579db"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYyMTEyNA==", "bodyText": "What about using ArrayDeque? The linked list uses more memory and has more dereferencing. It allows for random removals, but we don't need that, we only remove from one and and add to the other.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r532621124", "createdAt": "2020-11-30T14:06:45Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/ShardReader.java", "diffHunk": "@@ -0,0 +1,278 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ExpiredIteratorException;\n+import com.amazonaws.services.kinesis.model.GetRecordsResult;\n+import com.amazonaws.services.kinesis.model.GetShardIteratorResult;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.Record;\n+import com.amazonaws.services.kinesis.model.Shard;\n+import com.hazelcast.logging.ILogger;\n+\n+import java.util.LinkedList;\n+import java.util.concurrent.Future;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+class ShardReader extends AbstractShardWorker {\n+\n+    /* Kinesis allows for a maximum of 5 GetRecords operations per second. */\n+    private static final int GET_RECORD_OPS_PER_SECOND = 5;\n+\n+    /* Even though GetRecords operations are limited to 5 per second, if one\n+     * such operation happens to return too much data, following operations will\n+     * throw ProvisionedThroughputExceededException. In such cases we need to\n+     * wait a bit longer than for regular rate limiting.\n+     *\n+     * Relevant section from AWS documentation:\n+     *\n+     * \"The size of the data returned by GetRecords varies depending on the\n+     * utilization of the shard. The maximum size of data that GetRecords can\n+     * return is 10 MiB. If a call returns this amount of data, subsequent calls\n+     * made within the next 5 seconds throw\n+     * ProvisionedThroughputExceededException. If there is insufficient\n+     * provisioned throughput on the stream, subsequent calls made within the\n+     * next 1 second throw ProvisionedThroughputExceededException. GetRecords\n+     * doesn't return any data when it throws an exception. For this reason, we\n+     * recommend that you wait 1 second between calls to GetRecords. However,\n+     * it's possible that the application will get exceptions for longer than\n+     * 1 second.\"\n+     *\n+     * We also need to add this extra wait whenever we encounter other unexpected\n+     * failures.\n+     * */\n+    private static final long PAUSE_AFTER_FAILURE = SECONDS.toNanos(1); //todo: exponential backoff\n+\n+    /**\n+     * Maximum number of records returned by this reader in a single batch. Is\n+     * limited due to being used from a cooperative processor. Should not pose\n+     * a performance bottleneck, because while available data is being processed\n+     * the asynchronous request for more will already be issued in the background.\n+     */\n+    private static final int DATA_BATCH_SIZE = 100;\n+\n+    private final Shard shard;\n+    private final RandomizedRateTracker getRecordsRateTracker =\n+            new RandomizedRateTracker(1000, GET_RECORD_OPS_PER_SECOND);\n+\n+    private State state = State.NO_SHARD_ITERATOR;\n+    private String shardIterator;\n+    private Future<GetShardIteratorResult> shardIteratorResult;\n+    private Future<GetRecordsResult> recordsResult;\n+    private long nextGetRecordsTime = System.nanoTime();\n+\n+    private final LinkedList<Record> data = new LinkedList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4173407054222ba04b96b03e797caf73650579db"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYyNDU5Mg==", "bodyText": "There's a bit too much copying: we copy the results from GetRecordsResult to LinkedList<Record> in handleWaitingForRecords. Then from the linked list to an array in getData(). I think neither of this is necessary - can't we directly return the List<Record> from the GetRecordsResult in getData? Why do we need the batching here?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r532624592", "createdAt": "2020-11-30T14:11:45Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/ShardReader.java", "diffHunk": "@@ -0,0 +1,278 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.SdkClientException;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.ExpiredIteratorException;\n+import com.amazonaws.services.kinesis.model.GetRecordsResult;\n+import com.amazonaws.services.kinesis.model.GetShardIteratorResult;\n+import com.amazonaws.services.kinesis.model.ProvisionedThroughputExceededException;\n+import com.amazonaws.services.kinesis.model.Record;\n+import com.amazonaws.services.kinesis.model.Shard;\n+import com.hazelcast.logging.ILogger;\n+\n+import java.util.LinkedList;\n+import java.util.concurrent.Future;\n+\n+import static com.hazelcast.jet.impl.util.ExceptionUtil.rethrow;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+class ShardReader extends AbstractShardWorker {\n+\n+    /* Kinesis allows for a maximum of 5 GetRecords operations per second. */\n+    private static final int GET_RECORD_OPS_PER_SECOND = 5;\n+\n+    /* Even though GetRecords operations are limited to 5 per second, if one\n+     * such operation happens to return too much data, following operations will\n+     * throw ProvisionedThroughputExceededException. In such cases we need to\n+     * wait a bit longer than for regular rate limiting.\n+     *\n+     * Relevant section from AWS documentation:\n+     *\n+     * \"The size of the data returned by GetRecords varies depending on the\n+     * utilization of the shard. The maximum size of data that GetRecords can\n+     * return is 10 MiB. If a call returns this amount of data, subsequent calls\n+     * made within the next 5 seconds throw\n+     * ProvisionedThroughputExceededException. If there is insufficient\n+     * provisioned throughput on the stream, subsequent calls made within the\n+     * next 1 second throw ProvisionedThroughputExceededException. GetRecords\n+     * doesn't return any data when it throws an exception. For this reason, we\n+     * recommend that you wait 1 second between calls to GetRecords. However,\n+     * it's possible that the application will get exceptions for longer than\n+     * 1 second.\"\n+     *\n+     * We also need to add this extra wait whenever we encounter other unexpected\n+     * failures.\n+     * */\n+    private static final long PAUSE_AFTER_FAILURE = SECONDS.toNanos(1); //todo: exponential backoff\n+\n+    /**\n+     * Maximum number of records returned by this reader in a single batch. Is\n+     * limited due to being used from a cooperative processor. Should not pose\n+     * a performance bottleneck, because while available data is being processed\n+     * the asynchronous request for more will already be issued in the background.\n+     */\n+    private static final int DATA_BATCH_SIZE = 100;\n+\n+    private final Shard shard;\n+    private final RandomizedRateTracker getRecordsRateTracker =\n+            new RandomizedRateTracker(1000, GET_RECORD_OPS_PER_SECOND);\n+\n+    private State state = State.NO_SHARD_ITERATOR;\n+    private String shardIterator;\n+    private Future<GetShardIteratorResult> shardIteratorResult;\n+    private Future<GetRecordsResult> recordsResult;\n+    private long nextGetRecordsTime = System.nanoTime();\n+\n+    private final LinkedList<Record> data = new LinkedList<>();\n+\n+    ShardReader(AmazonKinesisAsync kinesis, String stream, Shard shard, ILogger logger) {\n+        super(kinesis, stream, logger);\n+        this.shard = shard;\n+    }\n+\n+    public Result run() {\n+        switch (state) {\n+            case NO_SHARD_ITERATOR:\n+                return handleNoShardIterator();\n+            case WAITING_FOR_SHARD_ITERATOR:\n+                return handleWaitingForShardIterator();\n+            case NEED_TO_REQUEST_RECORDS:\n+                return handleNeedToRequestRecords();\n+            case WAITING_FOR_RECORDS:\n+                return handleWaitingForRecords();\n+            case HAS_DATA_NEED_TO_REQUEST_RECORDS:\n+                return handleHasDataNeedToRequestRecords();\n+            case HAS_DATA:\n+                return handleHasData();\n+            case SHARD_CLOSED:\n+                return handleShardClosed();\n+            default:\n+                throw new RuntimeException(\"Programming error, unhandled state: \" + state);\n+        }\n+    }\n+\n+    private Result handleNoShardIterator() {\n+        shardIteratorResult = helper.getShardIteratorAsync(shard);\n+        state = State.WAITING_FOR_SHARD_ITERATOR;\n+        return Result.NOTHING;\n+    }\n+\n+    private Result handleWaitingForShardIterator() {\n+        if (shardIteratorResult.isDone()) {\n+            try {\n+                shardIterator = helper.readResult(shardIteratorResult).getShardIterator();\n+                state = State.NEED_TO_REQUEST_RECORDS;\n+            } catch (SdkClientException sce) {\n+                logger.warning(\"Failed retrieving shard iterator, retrying. Cause: \" + sce.getMessage());\n+                state = State.NO_SHARD_ITERATOR;\n+                return Result.NOTHING;\n+            } catch (Throwable t) {\n+                throw rethrow(t);\n+            }\n+        }\n+        return Result.NOTHING;\n+    }\n+\n+    private Result handleNeedToRequestRecords() {\n+        if (attemptToSendGetRecordsRequest()) {\n+            state = State.WAITING_FOR_RECORDS;\n+        }\n+\n+        return Result.NOTHING;\n+    }\n+\n+    private Result handleWaitingForRecords() {\n+        if (recordsResult.isDone()) {\n+            try {\n+                GetRecordsResult result = helper.readResult(recordsResult);\n+                shardIterator = result.getNextShardIterator();\n+                data.addAll(result.getRecords());\n+                if (shardIterator == null) {\n+                    state = State.SHARD_CLOSED;\n+                    return data.size() > 0 ? Result.HAS_DATA : Result.CLOSED;\n+                } else if (data.size() > 0) {\n+                    state = State.HAS_DATA_NEED_TO_REQUEST_RECORDS;\n+                    return Result.HAS_DATA;\n+                } else {\n+                    state = State.NEED_TO_REQUEST_RECORDS;\n+                    return Result.NOTHING;\n+                }\n+            } catch (ProvisionedThroughputExceededException pte) {\n+                return dealWithReadRecordFailure(\"Data throughput rate exceeded. Backing off and retrying.\");\n+            } catch (ExpiredIteratorException eie) {\n+                return dealWithReadRecordFailure(\"Record iterator expired. Retrying.\");\n+            } catch (SdkClientException sce) {\n+                return dealWithReadRecordFailure(\"Failed reading records, retrying. Cause: \" + sce.getMessage());\n+            } catch (Throwable t) {\n+                throw rethrow(t);\n+            }\n+        } else {\n+            return Result.NOTHING;\n+        }\n+    }\n+\n+    private Result dealWithReadRecordFailure(String message) {\n+        logger.warning(message);\n+        nextGetRecordsTime = System.nanoTime() + PAUSE_AFTER_FAILURE;\n+        state = State.NEED_TO_REQUEST_RECORDS;\n+        return Result.NOTHING;\n+    }\n+\n+    private Result handleHasDataNeedToRequestRecords() {\n+        if (attemptToSendGetRecordsRequest()) {\n+            state = data.size() > 0 ? State.HAS_DATA : State.WAITING_FOR_RECORDS;\n+        }\n+\n+        return data.size() > 0 ? Result.HAS_DATA : Result.NOTHING;\n+    }\n+\n+    private Result handleHasData() {\n+        state = data.size() > 0 ? State.HAS_DATA : State.WAITING_FOR_RECORDS;\n+        return data.size() > 0 ? Result.HAS_DATA : Result.NOTHING;\n+    }\n+\n+    private Result handleShardClosed() {\n+        return data.size() > 0 ? Result.HAS_DATA : Result.CLOSED;\n+    }\n+\n+    private boolean attemptToSendGetRecordsRequest() {\n+        if (System.nanoTime() < nextGetRecordsTime) {\n+            return false;\n+        }\n+\n+        recordsResult = helper.getRecordsAsync(shardIterator);\n+\n+        nextGetRecordsTime = System.nanoTime() + getRecordsRateTracker.next();\n+        return true;\n+    }\n+\n+    public Shard getShard() {\n+        return shard;\n+    }\n+\n+    public Record[] getData() {\n+        if (data.isEmpty()) {\n+            throw new IllegalStateException(\"Can't ask for data when none is available\");\n+        }\n+\n+        Record[] records = new Record[Math.min(data.size(), DATA_BATCH_SIZE)];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4173407054222ba04b96b03e797caf73650579db"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYyNzg2NA==", "bodyText": "I think we should also save the list of closed shards until we're sure they will be no longer returned from ListShards.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r532627864", "createdAt": "2020-11-30T14:16:23Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSourceP.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.Record;\n+import com.amazonaws.services.kinesis.model.Shard;\n+import com.hazelcast.jet.Traverser;\n+import com.hazelcast.jet.Traversers;\n+import com.hazelcast.jet.core.AbstractProcessor;\n+import com.hazelcast.jet.core.EventTimeMapper;\n+import com.hazelcast.jet.core.EventTimePolicy;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+\n+import static com.hazelcast.jet.Util.entry;\n+import static com.hazelcast.jet.kinesis.impl.KinesisHelper.shardBelongsToRange;\n+\n+public class KinesisSourceP extends AbstractProcessor {\n+\n+    @Nonnull\n+    private final AmazonKinesisAsync kinesis;\n+    @Nonnull\n+    private final String stream;\n+    @Nonnull\n+    private final EventTimeMapper<? super Map.Entry<String, byte[]>> eventTimeMapper;\n+    @Nonnull\n+    private final HashRange hashRange;\n+\n+    private int id;\n+    private ILogger logger;\n+\n+    private Traverser<Object> traverser = Traversers.empty();\n+\n+    private KinesisHelper helper;\n+    private RangeMonitor rangeMonitor;\n+    private List<ShardReader> shardReaders = new ArrayList<>();\n+    private int nextReader;\n+\n+    public KinesisSourceP(\n+            @Nonnull AmazonKinesisAsync kinesis,\n+            @Nonnull String stream,\n+            @Nonnull EventTimePolicy<? super Map.Entry<String, byte[]>> eventTimePolicy,\n+            @Nonnull HashRange hashRange\n+    ) {\n+        this.kinesis = Objects.requireNonNull(kinesis, \"kinesis\");\n+        this.stream = Objects.requireNonNull(stream, \"stream\");\n+        this.eventTimeMapper = new EventTimeMapper<>(eventTimePolicy);\n+        this.hashRange = Objects.requireNonNull(hashRange, \"hashRange\");\n+    }\n+\n+    @Override\n+    protected void init(@Nonnull Context context) throws Exception {\n+        super.init(context);\n+\n+        logger = context.logger();\n+        id = context.globalProcessorIndex();\n+\n+        helper = new KinesisHelper(kinesis, stream, logger);\n+\n+        logger.info(\"Processor \" + id + \" handles \" + hashRange);\n+\n+        helper.waitForStreamToActivate();\n+        List<Shard> shardsInRange = helper.listShards(\n+                (Predicate<? super Shard>) shard -> shardBelongsToRange(shard, hashRange));\n+        rangeMonitor = new RangeMonitor(context.totalParallelism(), kinesis, stream, hashRange, shardsInRange, logger);\n+        addShardReaders(shardsInRange);\n+    }\n+\n+    @Override\n+    public boolean complete() {\n+        if (!emitFromTraverser(traverser)) {\n+            return false;\n+        }\n+\n+        runMonitor();\n+        runReaders();\n+        return false;\n+    }\n+\n+    private void runMonitor() {\n+        RangeMonitor.Result result = rangeMonitor.run();\n+        if (RangeMonitor.Result.NEW_SHARDS.equals(result)) {\n+            Collection<Shard> shards = rangeMonitor.getNewShards();\n+            addShardReaders(shards);\n+        }\n+    }\n+\n+    private void runReaders() {\n+        for (int i = 0; i < shardReaders.size(); i++) {\n+            int currentReader = nextReader;\n+            ShardReader reader = shardReaders.get(currentReader);\n+            nextReader = incrCircular(currentReader, shardReaders.size());\n+\n+            ShardReader.Result result = reader.run();\n+            if (ShardReader.Result.HAS_DATA.equals(result)) {\n+                Record[] records = reader.getData();\n+                traverser = Traversers.traverseArray(records)\n+                        .flatMap(record -> eventTimeMapper.flatMapEvent(\n+                                entry(record.getPartitionKey(), record.getData().array()), //todo: shady?\n+                                currentReader,\n+                                record.getApproximateArrivalTimestamp().getTime()\n+                        ));\n+                emitFromTraverser(traverser);\n+                return;\n+            } else if (ShardReader.Result.CLOSED.equals(result)) {\n+                Shard shard = reader.getShard();\n+                logger.info(\"Shard \" + shard.getShardId() + \" of stream \" + stream + \" closed\");\n+                removeShardReader(currentReader);\n+                nextReader = 0;\n+                return;\n+            }\n+        }\n+\n+        traverser = eventTimeMapper.flatMapIdle();\n+        emitFromTraverser(traverser);\n+    }\n+\n+    @Override\n+    public boolean saveToSnapshot() {\n+        if (!emitFromTraverser(traverser)) {\n+            return false;\n+        }\n+\n+        //todo: actual snapshot saving; we will be saving the sequence numbers of last seen messages, per shard", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4173407054222ba04b96b03e797caf73650579db"}, "originalPosition": 146}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d9bdc0bfb4c64f3bbb705e4482a25852a3537603", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/d9bdc0bfb4c64f3bbb705e4482a25852a3537603", "committedDate": "2020-12-02T08:04:49Z", "message": "Addressing review concerns"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5b80af7073a6df7b2092d516f53977491f4592af", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/5b80af7073a6df7b2092d516f53977491f4592af", "committedDate": "2020-12-02T08:17:14Z", "message": "Addressing review concerns"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d14a50669e46ee2e2d4ff17ad4548c602b60a623", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/d14a50669e46ee2e2d4ff17ad4548c602b60a623", "committedDate": "2020-12-02T09:11:39Z", "message": "Addressing review concerns"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "68a2ec1c6102a7c00b2fbb68f734b74f23b22f66", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/68a2ec1c6102a7c00b2fbb68f734b74f23b22f66", "committedDate": "2020-12-02T10:22:20Z", "message": "Addressing review concerns"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b82c039e261697a2d4dd648d26038d4f7df8f74d", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/b82c039e261697a2d4dd648d26038d4f7df8f74d", "committedDate": "2020-12-03T11:38:42Z", "message": "Make source fault tolerant, works for static streams"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3d194ea99f91de326de1fc6e7da0716d9f7e854", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/f3d194ea99f91de326de1fc6e7da0716d9f7e854", "committedDate": "2020-12-03T17:00:24Z", "message": "Attempt to fix test failure"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f88b08c5be1c416c8927635dfa4c24e0cbdd5618", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/f88b08c5be1c416c8927635dfa4c24e0cbdd5618", "committedDate": "2020-12-06T08:47:59Z", "message": "Poll shard metadata only once per member"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "84c9f6a97aef379f85e71f296a352ee8e4cd096a", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/84c9f6a97aef379f85e71f296a352ee8e4cd096a", "committedDate": "2020-12-07T08:08:21Z", "message": "Make sure source fault tolerance works"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8ed63c124c1db7fb971e588a5f041fab9caeb169", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/8ed63c124c1db7fb971e588a5f041fab9caeb169", "committedDate": "2020-12-07T10:04:30Z", "message": "Add small fix, until it shows up on master"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e69c4caf4101f0c02d8347070f61eff0215ff308", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/e69c4caf4101f0c02d8347070f61eff0215ff308", "committedDate": "2020-12-07T10:08:03Z", "message": "Add small fix, until it shows up on master"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "717ab4c22cb2fde38cb421afbb772f8fcc537c4a", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/717ab4c22cb2fde38cb421afbb772f8fcc537c4a", "committedDate": "2020-12-07T11:14:37Z", "message": "Make sink fault tolerant (at-least once), do various changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a008524e9c769d4599c3e51d31df33c1116a2957", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/a008524e9c769d4599c3e51d31df33c1116a2957", "committedDate": "2020-12-07T11:43:07Z", "message": "Simplify builders"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "81e717c74ead5caaae12902a238d5b978cb40920", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/81e717c74ead5caaae12902a238d5b978cb40920", "committedDate": "2020-12-08T10:14:25Z", "message": "Add exponential backoff to retries"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3e054c8074d7fdad7501d7749b344ace3b66f07a", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/3e054c8074d7fdad7501d7749b344ace3b66f07a", "committedDate": "2020-12-08T12:38:16Z", "message": "Modify retry strategy configuration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8513d3e0be701205b0360e2cacfef61d86a1e9b6", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/8513d3e0be701205b0360e2cacfef61d86a1e9b6", "committedDate": "2020-12-08T12:45:06Z", "message": "Make checkstyle happy"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "719836d2ef067e94827627abd4d0bf6de7b87425", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/719836d2ef067e94827627abd4d0bf6de7b87425", "committedDate": "2020-12-09T08:58:01Z", "message": "Merge branch 'master' into kinesis"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6126832abda4dbc0ba2081b386fb8e30110206e1", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/6126832abda4dbc0ba2081b386fb8e30110206e1", "committedDate": "2020-12-14T13:19:04Z", "message": "Control throughput in the sink"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "efac18ab93dd99266d84c00159ffdd3cba7746e7", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/efac18ab93dd99266d84c00159ffdd3cba7746e7", "committedDate": "2020-12-15T07:38:43Z", "message": "Merge branch 'master' into kinesis"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "77f0995d8cfd04973956aa9761bd1593f53499bd", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/77f0995d8cfd04973956aa9761bd1593f53499bd", "committedDate": "2020-12-15T11:16:00Z", "message": "Add some more tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dcb527e2ac4d88dee8471bed5fb339d18fd8224f", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/dcb527e2ac4d88dee8471bed5fb339d18fd8224f", "committedDate": "2020-12-15T14:13:14Z", "message": "Add some more tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "17d6e220ad5dc7870e7199811ac6ae0441719d32", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/17d6e220ad5dc7870e7199811ac6ae0441719d32", "committedDate": "2020-12-15T21:00:10Z", "message": "Merge branch 'master' into kinesis"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUyODkxNjQ2", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#pullrequestreview-552891646", "createdAt": "2020-12-15T21:18:27Z", "commit": {"oid": "17d6e220ad5dc7870e7199811ac6ae0441719d32"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQyMToxODoyN1rOIGgY3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQyMToxODoyN1rOIGgY3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzY5MzAyMQ==", "bodyText": "Is this based on some testing or is it a recommendation by Amazon? It should be mentioned here.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r543693021", "createdAt": "2020-12-15T21:18:27Z", "author": {"login": "viliam-durina"}, "path": "extensions/kinesis/src/main/java/com/hazelcast/jet/kinesis/impl/KinesisSourcePSupplier.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package com.hazelcast.jet.kinesis.impl;\n+\n+import com.amazonaws.services.kinesis.AmazonKinesis;\n+import com.amazonaws.services.kinesis.AmazonKinesisAsync;\n+import com.amazonaws.services.kinesis.model.Shard;\n+import com.hazelcast.jet.core.EventTimePolicy;\n+import com.hazelcast.jet.core.Processor;\n+import com.hazelcast.jet.core.ProcessorSupplier;\n+import com.hazelcast.jet.retry.RetryStrategy;\n+import com.hazelcast.logging.ILogger;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.IntStream;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+public class KinesisSourcePSupplier implements ProcessorSupplier {\n+\n+    private static final long serialVersionUID = 1L;\n+\n+    /**\n+     * We don't want to create an AWS client for each processor instance,\n+     * because they aren't light. We also can't do the other extreme, have a\n+     * single AWS client shared by all processors, because there would be a lot\n+     * of contention, causing problems. So we use shared clients but use them\n+     * for a limited number of processor instances, specified by this constant.\n+     */\n+    private static final int PROCESSORS_PER_CLIENT = 12;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "17d6e220ad5dc7870e7199811ac6ae0441719d32"}, "originalPosition": 49}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2fbdddf90dc98808c27727c52a2fe1f8cf2d72bc", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/2fbdddf90dc98808c27727c52a2fe1f8cf2d72bc", "committedDate": "2020-12-16T09:13:57Z", "message": "Remove duplicate line"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "20728c2996537f1dd9e33bfa490d44190de7b0da", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/20728c2996537f1dd9e33bfa490d44190de7b0da", "committedDate": "2020-12-16T10:51:57Z", "message": "Add eviction of expired shards"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "50c4bea194ce4b246769479fa0d8873aff83679f", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/50c4bea194ce4b246769479fa0d8873aff83679f", "committedDate": "2020-12-16T11:03:05Z", "message": "Add some TODOs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "82c9c6bbc0f0f626d333b24d3575dd905920467e", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/82c9c6bbc0f0f626d333b24d3575dd905920467e", "committedDate": "2020-12-21T10:01:26Z", "message": "Write the first version of the TDD"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "92c41d2d329b4c912024828b41a9676216106193", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/92c41d2d329b4c912024828b41a9676216106193", "committedDate": "2020-12-21T10:03:56Z", "message": "Merge branch 'master' into kinesis"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a9ebfb30ac9ec1b0b1b7a1d580190c9e4650b727", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/a9ebfb30ac9ec1b0b1b7a1d580190c9e4650b727", "committedDate": "2020-12-18T15:41:25Z", "message": "Write the first version of the TDD (UNFINISHED)"}, "afterCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/92c41d2d329b4c912024828b41a9676216106193", "committedDate": "2020-12-21T10:03:56Z", "message": "Merge branch 'master' into kinesis"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a44d4f00721c06579a2a38aa38ca4de82039cc27", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/a44d4f00721c06579a2a38aa38ca4de82039cc27", "committedDate": "2020-12-22T08:12:26Z", "message": "Fix EventTimeMapper related bug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03f59952e010cc05e7da566bfce9387e0b184966", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/03f59952e010cc05e7da566bfce9387e0b184966", "committedDate": "2020-12-22T08:12:45Z", "message": "Add Kinesis to the distribution"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ecc730e39c211d217e4fa83faf0df5d9ff042f07", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/ecc730e39c211d217e4fa83faf0df5d9ff042f07", "committedDate": "2020-12-22T10:52:09Z", "message": "Fix bugs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e90966bca3c5cee2752b7f7c02b2d988965d3604", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/e90966bca3c5cee2752b7f7c02b2d988965d3604", "committedDate": "2020-12-22T10:52:37Z", "message": "Add javadoc"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU2OTY0MjY1", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#pullrequestreview-556964265", "createdAt": "2020-12-22T10:09:25Z", "commit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "state": "COMMENTED", "comments": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxMDowOToyNVrOIJ1olw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxMjo1NTowMVrOIJ6OIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE4NjgzOQ==", "bodyText": "This sounds like you have direct control over mapping keys to shards, but the assignment is pseudorandom.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547186839", "createdAt": "2020-12-22T10:09:25Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE4NzQ1Mw==", "bodyText": "So there's no guarantee on the sequence number monotonicity?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547187453", "createdAt": "2020-12-22T10:10:43Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.\n+\n+A **sequence number** is a unique identifier for each record. Sequence\n+numbers are assigned by KDS when a data producer publishes data into it.\n+Sequence numbers for the same partition key generally increase over", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE5NDgxNQ==", "bodyText": "At what time period are these restrictions applied? Every second?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547194815", "createdAt": "2020-12-22T10:25:13Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.\n+\n+A **sequence number** is a unique identifier for each record. Sequence\n+numbers are assigned by KDS when a data producer publishes data into it.\n+Sequence numbers for the same partition key generally increase over\n+time.\n+\n+## APIs\n+\n+Amazon offers various choices of libraries that can be used to interact\n+with KDS:\n+\n+* **Kinesis Client Library (KCL)** and **Kinesis Producer Library\n+  (KPL)** are high-level libraries that are easy to use because they\n+  abstract away many concerns. They manage their threading policy, hide\n+  away the REST-based nature of Kinesis behind asynchronous constructs,\n+  balance load, handle failures, and react to resharding. However, all\n+  this convenience makes them unsuitable for building Jet connectors,\n+  where we need the most control possible to make choices that are\n+  suitable to Jet's architecture.\n+* **Amazon Kinesis Data Streams API** via **AWS SDK for Java** is the\n+  lower-level library that allows sufficient control when interacting\n+  with Kinesis. It consists of a simple set of [REST-based\n+  operations](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Operations.html).\n+  Every other concern mentioned above, when discussing the high-level\n+  libraries, has to be handled explicitly. This is the library used in\n+  the Jet source and sink implementations.\n+\n+## Quotas\n+\n+Amazon Kinesis Data Streams enforces quite a few quotas and limits,\n+which our sources and sinks need to comply with:\n+\n+* A single shard can ingest up to 1 MB of data per second (including", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI0OTk1MA==", "bodyText": "Can this be written more precisely? For example, is there a clear linearization point (a unique point in global time) separating the activation of parent and children?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547249950", "createdAt": "2020-12-22T12:27:57Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.\n+\n+A **sequence number** is a unique identifier for each record. Sequence\n+numbers are assigned by KDS when a data producer publishes data into it.\n+Sequence numbers for the same partition key generally increase over\n+time.\n+\n+## APIs\n+\n+Amazon offers various choices of libraries that can be used to interact\n+with KDS:\n+\n+* **Kinesis Client Library (KCL)** and **Kinesis Producer Library\n+  (KPL)** are high-level libraries that are easy to use because they\n+  abstract away many concerns. They manage their threading policy, hide\n+  away the REST-based nature of Kinesis behind asynchronous constructs,\n+  balance load, handle failures, and react to resharding. However, all\n+  this convenience makes them unsuitable for building Jet connectors,\n+  where we need the most control possible to make choices that are\n+  suitable to Jet's architecture.\n+* **Amazon Kinesis Data Streams API** via **AWS SDK for Java** is the\n+  lower-level library that allows sufficient control when interacting\n+  with Kinesis. It consists of a simple set of [REST-based\n+  operations](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Operations.html).\n+  Every other concern mentioned above, when discussing the high-level\n+  libraries, has to be handled explicitly. This is the library used in\n+  the Jet source and sink implementations.\n+\n+## Quotas\n+\n+Amazon Kinesis Data Streams enforces quite a few quotas and limits,\n+which our sources and sinks need to comply with:\n+\n+* A single shard can ingest up to 1 MB of data per second (including\n+  partition keys) or 1,000 records per second for writes.\n+* The maximum size of the data payload of a record is 1 MB.\n+* The\n+  [GetRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html)\n+  operation can retrieve up to 10 MB of data per call from a single\n+  shard and up to 10,000 records per call.\n+* Each shard can support up to 5 GetRecords operations per second.\n+* Each shard can support up to a maximum total data read rate of 2 MB\n+  per second.\n+* The\n+  [ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+  operation has a limit of 100 transactions per second, per data stream.\n+  Each such transaction is able to return at most 100 shards, and if the\n+  stream has more, then multiple transactions need to be used for a full\n+  listing. (For details, see the [discovery](#discovery) section.)\n+* The\n+  [PutRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)\n+  operation can write at most 500 records into the stream. Each record\n+  in the request can be as large as 1MB, up to a limit of 5MB for the\n+  entire request, including partition keys. Each shard can support\n+  writes up to 1,000 records per second, up to a maximum data write\n+  total of 1 MiB per second.\n+* The\n+  [DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+  operation has a limit of 20 transactions per second per account. (For\n+  details, see the [adaptive throughput](#adaptive-throughput) section.)\n+\n+## Source\n+\n+The Kinesis source is a _streaming_, _distributed_, and _fault-tolerant_\n+data source for Jet. It supports both the _at-least-once_ and\n+_exactly-once_ [processing\n+guarantees](../architecture/fault-tolerance.md#processing-guarantee-is-a-shared-concern).\n+\n+### Distribution\n+\n+Being a distributed source, it has multiple instances running in each\n+Jet cluster member. Each instance is responsible for reading from zero,\n+one or more KDS [shards](#shard). The assignment of shards to individual\n+source instances is based on partitioning the space of Kinesis [record\n+keys](#partition-key).\n+\n+Record keys, or partition keys as Kinesis calls them, are Unicode\n+strings, with a maximum length limit of 256 characters. The stream uses\n+the MD5 hash function to map these strings to 128-bit integer values.\n+The universe of these values is thus the [0 .. 2^128) range. Each\n+Kinesis shard has a continuous chunk of this range assigned to it,\n+called the shard's hash range. The stream assigns a record to a shard if\n+the record's partition key hashes into the shard's range.\n+\n+In the Jet Kinesis source, we use similar logic for assigning shards to\n+source instances. Each of our sources gets a part of the hash range\n+assigned to it. We say that a source owns a specific shard if and only\n+if the shard's hash range's starting point is inside the source's hash\n+range. Any similar range matching logic would work, as long as it's\n+non-ambiguous.\n+\n+### Discovery\n+\n+Reading records from shards assigned to them is only part of the\n+responsibility of sources. Sources also need a way to discover currently\n+active shards in the stream to take responsibility for them. Moreover,\n+this discovery process can't just happen once, on start-up, because\n+streams are dynamic, shards can be closed, and new shards can pop-up at\n+any time. For details, see the [resharding section](#resharding).\n+\n+Continuously monitoring the set of active shards in the stream is the\n+responsibility of **one** of the local source instances in each Jet\n+cluster member. This is an optimization. If all sources would run the\n+discovery, they would still obtain the same data, just with a multiplied\n+effort and cost. Monitoring means continuously polling the stream for\n+the list of all shards in it.\n+\n+Monitoring needs to take care not to cross the rate limit imposed by\n+Kinesis on this operation. For details, see the [quotas\n+section](#quotas).\n+\n+### Resharding\n+\n+Kinesis supports resharding, which lets you adjust the number of shards\n+in your stream to adapt to changes in data flow rate through the stream.\n+(Amazon charges on a per-shard basis, that's why it's desirable to have\n+the smallest amount of shards possible.)\n+\n+There are two types of resharding operations: shard **split** and shard\n+**merge**. In a shard split, you divide a single shard into two adjacent\n+shards. In a shard merge, you combine two adjacent shards into a single\n+shard. By \"adjacent\", we mean that one's hash range starts where the\n+other one's ends.\n+\n+Splitting increases the number of shards in your stream and therefore\n+increases the data capacity (and cost) of the stream. Similarly, merging\n+reduces the number of shards in your stream and therefore decreases the\n+data capacity (and cost).\n+\n+Resharding is always pairwise in the sense that you cannot split into\n+more than two shards in a single operation, and you cannot merge more\n+than two shards in a single operation. The shard or pair of shards that\n+the resharding operation acts on are called parent shards. The shard or\n+pair of shards that result from the resharding operation are called\n+child shards.\n+\n+When child shards, resulting from a split or merge, activate, their", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI1MTE3Mg==", "bodyText": "With preventReordering introduced, Jet would prevent this, though.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547251172", "createdAt": "2020-12-22T12:30:52Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.\n+\n+A **sequence number** is a unique identifier for each record. Sequence\n+numbers are assigned by KDS when a data producer publishes data into it.\n+Sequence numbers for the same partition key generally increase over\n+time.\n+\n+## APIs\n+\n+Amazon offers various choices of libraries that can be used to interact\n+with KDS:\n+\n+* **Kinesis Client Library (KCL)** and **Kinesis Producer Library\n+  (KPL)** are high-level libraries that are easy to use because they\n+  abstract away many concerns. They manage their threading policy, hide\n+  away the REST-based nature of Kinesis behind asynchronous constructs,\n+  balance load, handle failures, and react to resharding. However, all\n+  this convenience makes them unsuitable for building Jet connectors,\n+  where we need the most control possible to make choices that are\n+  suitable to Jet's architecture.\n+* **Amazon Kinesis Data Streams API** via **AWS SDK for Java** is the\n+  lower-level library that allows sufficient control when interacting\n+  with Kinesis. It consists of a simple set of [REST-based\n+  operations](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Operations.html).\n+  Every other concern mentioned above, when discussing the high-level\n+  libraries, has to be handled explicitly. This is the library used in\n+  the Jet source and sink implementations.\n+\n+## Quotas\n+\n+Amazon Kinesis Data Streams enforces quite a few quotas and limits,\n+which our sources and sinks need to comply with:\n+\n+* A single shard can ingest up to 1 MB of data per second (including\n+  partition keys) or 1,000 records per second for writes.\n+* The maximum size of the data payload of a record is 1 MB.\n+* The\n+  [GetRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html)\n+  operation can retrieve up to 10 MB of data per call from a single\n+  shard and up to 10,000 records per call.\n+* Each shard can support up to 5 GetRecords operations per second.\n+* Each shard can support up to a maximum total data read rate of 2 MB\n+  per second.\n+* The\n+  [ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+  operation has a limit of 100 transactions per second, per data stream.\n+  Each such transaction is able to return at most 100 shards, and if the\n+  stream has more, then multiple transactions need to be used for a full\n+  listing. (For details, see the [discovery](#discovery) section.)\n+* The\n+  [PutRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)\n+  operation can write at most 500 records into the stream. Each record\n+  in the request can be as large as 1MB, up to a limit of 5MB for the\n+  entire request, including partition keys. Each shard can support\n+  writes up to 1,000 records per second, up to a maximum data write\n+  total of 1 MiB per second.\n+* The\n+  [DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+  operation has a limit of 20 transactions per second per account. (For\n+  details, see the [adaptive throughput](#adaptive-throughput) section.)\n+\n+## Source\n+\n+The Kinesis source is a _streaming_, _distributed_, and _fault-tolerant_\n+data source for Jet. It supports both the _at-least-once_ and\n+_exactly-once_ [processing\n+guarantees](../architecture/fault-tolerance.md#processing-guarantee-is-a-shared-concern).\n+\n+### Distribution\n+\n+Being a distributed source, it has multiple instances running in each\n+Jet cluster member. Each instance is responsible for reading from zero,\n+one or more KDS [shards](#shard). The assignment of shards to individual\n+source instances is based on partitioning the space of Kinesis [record\n+keys](#partition-key).\n+\n+Record keys, or partition keys as Kinesis calls them, are Unicode\n+strings, with a maximum length limit of 256 characters. The stream uses\n+the MD5 hash function to map these strings to 128-bit integer values.\n+The universe of these values is thus the [0 .. 2^128) range. Each\n+Kinesis shard has a continuous chunk of this range assigned to it,\n+called the shard's hash range. The stream assigns a record to a shard if\n+the record's partition key hashes into the shard's range.\n+\n+In the Jet Kinesis source, we use similar logic for assigning shards to\n+source instances. Each of our sources gets a part of the hash range\n+assigned to it. We say that a source owns a specific shard if and only\n+if the shard's hash range's starting point is inside the source's hash\n+range. Any similar range matching logic would work, as long as it's\n+non-ambiguous.\n+\n+### Discovery\n+\n+Reading records from shards assigned to them is only part of the\n+responsibility of sources. Sources also need a way to discover currently\n+active shards in the stream to take responsibility for them. Moreover,\n+this discovery process can't just happen once, on start-up, because\n+streams are dynamic, shards can be closed, and new shards can pop-up at\n+any time. For details, see the [resharding section](#resharding).\n+\n+Continuously monitoring the set of active shards in the stream is the\n+responsibility of **one** of the local source instances in each Jet\n+cluster member. This is an optimization. If all sources would run the\n+discovery, they would still obtain the same data, just with a multiplied\n+effort and cost. Monitoring means continuously polling the stream for\n+the list of all shards in it.\n+\n+Monitoring needs to take care not to cross the rate limit imposed by\n+Kinesis on this operation. For details, see the [quotas\n+section](#quotas).\n+\n+### Resharding\n+\n+Kinesis supports resharding, which lets you adjust the number of shards\n+in your stream to adapt to changes in data flow rate through the stream.\n+(Amazon charges on a per-shard basis, that's why it's desirable to have\n+the smallest amount of shards possible.)\n+\n+There are two types of resharding operations: shard **split** and shard\n+**merge**. In a shard split, you divide a single shard into two adjacent\n+shards. In a shard merge, you combine two adjacent shards into a single\n+shard. By \"adjacent\", we mean that one's hash range starts where the\n+other one's ends.\n+\n+Splitting increases the number of shards in your stream and therefore\n+increases the data capacity (and cost) of the stream. Similarly, merging\n+reduces the number of shards in your stream and therefore decreases the\n+data capacity (and cost).\n+\n+Resharding is always pairwise in the sense that you cannot split into\n+more than two shards in a single operation, and you cannot merge more\n+than two shards in a single operation. The shard or pair of shards that\n+the resharding operation acts on are called parent shards. The shard or\n+pair of shards that result from the resharding operation are called\n+child shards.\n+\n+When child shards, resulting from a split or merge, activate, their\n+parents get deactivated and will no longer get data inserted into them.\n+From that point onward, data goes into the children.\n+\n+### Read Order\n+\n+Resharding does not suspend the stream's dataflow, while it's going on.\n+Data continues to be ingested into the stream, and at some point, it\n+just stops being put into the parent shards and starts being put into\n+the child shards.\n+\n+The Kinesis Jet source would need to make sure that it finishes reading\n+from parents before reading from their children. However, this is not\n+possible since the children might end up being owned by an entirely\n+different instance of the source than their parents (for example, in a\n+split), possibly located in an entirely different Jet cluster member.\n+\n+Moreover, it's not enough to finish reading from the parent before\n+reading from the children. Even if that were achieved, data from parents\n+might overtake data from children further down the Jet pipeline, in its\n+various parallel components. A Kinesis source would need to make sure", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI1MjY0OQ==", "bodyText": "This works only if the sequence numbers aren't just \"generally increasing over time\". So is there actually a stricter guarantee for the sequence?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547252649", "createdAt": "2020-12-22T12:34:30Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.\n+\n+A **sequence number** is a unique identifier for each record. Sequence\n+numbers are assigned by KDS when a data producer publishes data into it.\n+Sequence numbers for the same partition key generally increase over\n+time.\n+\n+## APIs\n+\n+Amazon offers various choices of libraries that can be used to interact\n+with KDS:\n+\n+* **Kinesis Client Library (KCL)** and **Kinesis Producer Library\n+  (KPL)** are high-level libraries that are easy to use because they\n+  abstract away many concerns. They manage their threading policy, hide\n+  away the REST-based nature of Kinesis behind asynchronous constructs,\n+  balance load, handle failures, and react to resharding. However, all\n+  this convenience makes them unsuitable for building Jet connectors,\n+  where we need the most control possible to make choices that are\n+  suitable to Jet's architecture.\n+* **Amazon Kinesis Data Streams API** via **AWS SDK for Java** is the\n+  lower-level library that allows sufficient control when interacting\n+  with Kinesis. It consists of a simple set of [REST-based\n+  operations](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Operations.html).\n+  Every other concern mentioned above, when discussing the high-level\n+  libraries, has to be handled explicitly. This is the library used in\n+  the Jet source and sink implementations.\n+\n+## Quotas\n+\n+Amazon Kinesis Data Streams enforces quite a few quotas and limits,\n+which our sources and sinks need to comply with:\n+\n+* A single shard can ingest up to 1 MB of data per second (including\n+  partition keys) or 1,000 records per second for writes.\n+* The maximum size of the data payload of a record is 1 MB.\n+* The\n+  [GetRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html)\n+  operation can retrieve up to 10 MB of data per call from a single\n+  shard and up to 10,000 records per call.\n+* Each shard can support up to 5 GetRecords operations per second.\n+* Each shard can support up to a maximum total data read rate of 2 MB\n+  per second.\n+* The\n+  [ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+  operation has a limit of 100 transactions per second, per data stream.\n+  Each such transaction is able to return at most 100 shards, and if the\n+  stream has more, then multiple transactions need to be used for a full\n+  listing. (For details, see the [discovery](#discovery) section.)\n+* The\n+  [PutRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)\n+  operation can write at most 500 records into the stream. Each record\n+  in the request can be as large as 1MB, up to a limit of 5MB for the\n+  entire request, including partition keys. Each shard can support\n+  writes up to 1,000 records per second, up to a maximum data write\n+  total of 1 MiB per second.\n+* The\n+  [DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+  operation has a limit of 20 transactions per second per account. (For\n+  details, see the [adaptive throughput](#adaptive-throughput) section.)\n+\n+## Source\n+\n+The Kinesis source is a _streaming_, _distributed_, and _fault-tolerant_\n+data source for Jet. It supports both the _at-least-once_ and\n+_exactly-once_ [processing\n+guarantees](../architecture/fault-tolerance.md#processing-guarantee-is-a-shared-concern).\n+\n+### Distribution\n+\n+Being a distributed source, it has multiple instances running in each\n+Jet cluster member. Each instance is responsible for reading from zero,\n+one or more KDS [shards](#shard). The assignment of shards to individual\n+source instances is based on partitioning the space of Kinesis [record\n+keys](#partition-key).\n+\n+Record keys, or partition keys as Kinesis calls them, are Unicode\n+strings, with a maximum length limit of 256 characters. The stream uses\n+the MD5 hash function to map these strings to 128-bit integer values.\n+The universe of these values is thus the [0 .. 2^128) range. Each\n+Kinesis shard has a continuous chunk of this range assigned to it,\n+called the shard's hash range. The stream assigns a record to a shard if\n+the record's partition key hashes into the shard's range.\n+\n+In the Jet Kinesis source, we use similar logic for assigning shards to\n+source instances. Each of our sources gets a part of the hash range\n+assigned to it. We say that a source owns a specific shard if and only\n+if the shard's hash range's starting point is inside the source's hash\n+range. Any similar range matching logic would work, as long as it's\n+non-ambiguous.\n+\n+### Discovery\n+\n+Reading records from shards assigned to them is only part of the\n+responsibility of sources. Sources also need a way to discover currently\n+active shards in the stream to take responsibility for them. Moreover,\n+this discovery process can't just happen once, on start-up, because\n+streams are dynamic, shards can be closed, and new shards can pop-up at\n+any time. For details, see the [resharding section](#resharding).\n+\n+Continuously monitoring the set of active shards in the stream is the\n+responsibility of **one** of the local source instances in each Jet\n+cluster member. This is an optimization. If all sources would run the\n+discovery, they would still obtain the same data, just with a multiplied\n+effort and cost. Monitoring means continuously polling the stream for\n+the list of all shards in it.\n+\n+Monitoring needs to take care not to cross the rate limit imposed by\n+Kinesis on this operation. For details, see the [quotas\n+section](#quotas).\n+\n+### Resharding\n+\n+Kinesis supports resharding, which lets you adjust the number of shards\n+in your stream to adapt to changes in data flow rate through the stream.\n+(Amazon charges on a per-shard basis, that's why it's desirable to have\n+the smallest amount of shards possible.)\n+\n+There are two types of resharding operations: shard **split** and shard\n+**merge**. In a shard split, you divide a single shard into two adjacent\n+shards. In a shard merge, you combine two adjacent shards into a single\n+shard. By \"adjacent\", we mean that one's hash range starts where the\n+other one's ends.\n+\n+Splitting increases the number of shards in your stream and therefore\n+increases the data capacity (and cost) of the stream. Similarly, merging\n+reduces the number of shards in your stream and therefore decreases the\n+data capacity (and cost).\n+\n+Resharding is always pairwise in the sense that you cannot split into\n+more than two shards in a single operation, and you cannot merge more\n+than two shards in a single operation. The shard or pair of shards that\n+the resharding operation acts on are called parent shards. The shard or\n+pair of shards that result from the resharding operation are called\n+child shards.\n+\n+When child shards, resulting from a split or merge, activate, their\n+parents get deactivated and will no longer get data inserted into them.\n+From that point onward, data goes into the children.\n+\n+### Read Order\n+\n+Resharding does not suspend the stream's dataflow, while it's going on.\n+Data continues to be ingested into the stream, and at some point, it\n+just stops being put into the parent shards and starts being put into\n+the child shards.\n+\n+The Kinesis Jet source would need to make sure that it finishes reading\n+from parents before reading from their children. However, this is not\n+possible since the children might end up being owned by an entirely\n+different instance of the source than their parents (for example, in a\n+split), possibly located in an entirely different Jet cluster member.\n+\n+Moreover, it's not enough to finish reading from the parent before\n+reading from the children. Even if that were achieved, data from parents\n+might overtake data from children further down the Jet pipeline, in its\n+various parallel components. A Kinesis source would need to make sure\n+that it has read all data from the parents and that data has fully\n+passed through the Jet pipeline before starting to read from the\n+children. Only then could it provide the same ordering as KDS while\n+resharding.\n+\n+This is currently not possible in Jet. Hopefully, future versions will\n+address the problem. Users of the Kinesis source need to be aware that\n+some data reordering might occur on resharding.\n+\n+### Fault Tolerance\n+\n+The Kinesis Jet source supports pipelines with both at-least-once and\n+exactly-once processing guarantees. It achieves this by saving KDS", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 219}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI1MzI0Mg==", "bodyText": "What happens when the shards are different after a restart?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547253242", "createdAt": "2020-12-22T12:35:46Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.\n+\n+A **sequence number** is a unique identifier for each record. Sequence\n+numbers are assigned by KDS when a data producer publishes data into it.\n+Sequence numbers for the same partition key generally increase over\n+time.\n+\n+## APIs\n+\n+Amazon offers various choices of libraries that can be used to interact\n+with KDS:\n+\n+* **Kinesis Client Library (KCL)** and **Kinesis Producer Library\n+  (KPL)** are high-level libraries that are easy to use because they\n+  abstract away many concerns. They manage their threading policy, hide\n+  away the REST-based nature of Kinesis behind asynchronous constructs,\n+  balance load, handle failures, and react to resharding. However, all\n+  this convenience makes them unsuitable for building Jet connectors,\n+  where we need the most control possible to make choices that are\n+  suitable to Jet's architecture.\n+* **Amazon Kinesis Data Streams API** via **AWS SDK for Java** is the\n+  lower-level library that allows sufficient control when interacting\n+  with Kinesis. It consists of a simple set of [REST-based\n+  operations](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Operations.html).\n+  Every other concern mentioned above, when discussing the high-level\n+  libraries, has to be handled explicitly. This is the library used in\n+  the Jet source and sink implementations.\n+\n+## Quotas\n+\n+Amazon Kinesis Data Streams enforces quite a few quotas and limits,\n+which our sources and sinks need to comply with:\n+\n+* A single shard can ingest up to 1 MB of data per second (including\n+  partition keys) or 1,000 records per second for writes.\n+* The maximum size of the data payload of a record is 1 MB.\n+* The\n+  [GetRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html)\n+  operation can retrieve up to 10 MB of data per call from a single\n+  shard and up to 10,000 records per call.\n+* Each shard can support up to 5 GetRecords operations per second.\n+* Each shard can support up to a maximum total data read rate of 2 MB\n+  per second.\n+* The\n+  [ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+  operation has a limit of 100 transactions per second, per data stream.\n+  Each such transaction is able to return at most 100 shards, and if the\n+  stream has more, then multiple transactions need to be used for a full\n+  listing. (For details, see the [discovery](#discovery) section.)\n+* The\n+  [PutRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)\n+  operation can write at most 500 records into the stream. Each record\n+  in the request can be as large as 1MB, up to a limit of 5MB for the\n+  entire request, including partition keys. Each shard can support\n+  writes up to 1,000 records per second, up to a maximum data write\n+  total of 1 MiB per second.\n+* The\n+  [DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+  operation has a limit of 20 transactions per second per account. (For\n+  details, see the [adaptive throughput](#adaptive-throughput) section.)\n+\n+## Source\n+\n+The Kinesis source is a _streaming_, _distributed_, and _fault-tolerant_\n+data source for Jet. It supports both the _at-least-once_ and\n+_exactly-once_ [processing\n+guarantees](../architecture/fault-tolerance.md#processing-guarantee-is-a-shared-concern).\n+\n+### Distribution\n+\n+Being a distributed source, it has multiple instances running in each\n+Jet cluster member. Each instance is responsible for reading from zero,\n+one or more KDS [shards](#shard). The assignment of shards to individual\n+source instances is based on partitioning the space of Kinesis [record\n+keys](#partition-key).\n+\n+Record keys, or partition keys as Kinesis calls them, are Unicode\n+strings, with a maximum length limit of 256 characters. The stream uses\n+the MD5 hash function to map these strings to 128-bit integer values.\n+The universe of these values is thus the [0 .. 2^128) range. Each\n+Kinesis shard has a continuous chunk of this range assigned to it,\n+called the shard's hash range. The stream assigns a record to a shard if\n+the record's partition key hashes into the shard's range.\n+\n+In the Jet Kinesis source, we use similar logic for assigning shards to\n+source instances. Each of our sources gets a part of the hash range\n+assigned to it. We say that a source owns a specific shard if and only\n+if the shard's hash range's starting point is inside the source's hash\n+range. Any similar range matching logic would work, as long as it's\n+non-ambiguous.\n+\n+### Discovery\n+\n+Reading records from shards assigned to them is only part of the\n+responsibility of sources. Sources also need a way to discover currently\n+active shards in the stream to take responsibility for them. Moreover,\n+this discovery process can't just happen once, on start-up, because\n+streams are dynamic, shards can be closed, and new shards can pop-up at\n+any time. For details, see the [resharding section](#resharding).\n+\n+Continuously monitoring the set of active shards in the stream is the\n+responsibility of **one** of the local source instances in each Jet\n+cluster member. This is an optimization. If all sources would run the\n+discovery, they would still obtain the same data, just with a multiplied\n+effort and cost. Monitoring means continuously polling the stream for\n+the list of all shards in it.\n+\n+Monitoring needs to take care not to cross the rate limit imposed by\n+Kinesis on this operation. For details, see the [quotas\n+section](#quotas).\n+\n+### Resharding\n+\n+Kinesis supports resharding, which lets you adjust the number of shards\n+in your stream to adapt to changes in data flow rate through the stream.\n+(Amazon charges on a per-shard basis, that's why it's desirable to have\n+the smallest amount of shards possible.)\n+\n+There are two types of resharding operations: shard **split** and shard\n+**merge**. In a shard split, you divide a single shard into two adjacent\n+shards. In a shard merge, you combine two adjacent shards into a single\n+shard. By \"adjacent\", we mean that one's hash range starts where the\n+other one's ends.\n+\n+Splitting increases the number of shards in your stream and therefore\n+increases the data capacity (and cost) of the stream. Similarly, merging\n+reduces the number of shards in your stream and therefore decreases the\n+data capacity (and cost).\n+\n+Resharding is always pairwise in the sense that you cannot split into\n+more than two shards in a single operation, and you cannot merge more\n+than two shards in a single operation. The shard or pair of shards that\n+the resharding operation acts on are called parent shards. The shard or\n+pair of shards that result from the resharding operation are called\n+child shards.\n+\n+When child shards, resulting from a split or merge, activate, their\n+parents get deactivated and will no longer get data inserted into them.\n+From that point onward, data goes into the children.\n+\n+### Read Order\n+\n+Resharding does not suspend the stream's dataflow, while it's going on.\n+Data continues to be ingested into the stream, and at some point, it\n+just stops being put into the parent shards and starts being put into\n+the child shards.\n+\n+The Kinesis Jet source would need to make sure that it finishes reading\n+from parents before reading from their children. However, this is not\n+possible since the children might end up being owned by an entirely\n+different instance of the source than their parents (for example, in a\n+split), possibly located in an entirely different Jet cluster member.\n+\n+Moreover, it's not enough to finish reading from the parent before\n+reading from the children. Even if that were achieved, data from parents\n+might overtake data from children further down the Jet pipeline, in its\n+various parallel components. A Kinesis source would need to make sure\n+that it has read all data from the parents and that data has fully\n+passed through the Jet pipeline before starting to read from the\n+children. Only then could it provide the same ordering as KDS while\n+resharding.\n+\n+This is currently not possible in Jet. Hopefully, future versions will\n+address the problem. Users of the Kinesis source need to be aware that\n+some data reordering might occur on resharding.\n+\n+### Fault Tolerance\n+\n+The Kinesis Jet source supports pipelines with both at-least-once and\n+exactly-once processing guarantees. It achieves this by saving KDS\n+offsets into its snapshots and starting data read from saved offsets\n+when restarted.\n+\n+The offsets are saved on a per-shard basis, and on restart, each source\n+instance receives all saved offsets for all shards, so it can function\n+properly regardless of how shards are assigned to sources after the\n+restart.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 226}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI1NDkwMQ==", "bodyText": "How was this default decided on? My intuition is that retrying indefinitely is the best default.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547254901", "createdAt": "2020-12-22T12:39:18Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.\n+\n+A **sequence number** is a unique identifier for each record. Sequence\n+numbers are assigned by KDS when a data producer publishes data into it.\n+Sequence numbers for the same partition key generally increase over\n+time.\n+\n+## APIs\n+\n+Amazon offers various choices of libraries that can be used to interact\n+with KDS:\n+\n+* **Kinesis Client Library (KCL)** and **Kinesis Producer Library\n+  (KPL)** are high-level libraries that are easy to use because they\n+  abstract away many concerns. They manage their threading policy, hide\n+  away the REST-based nature of Kinesis behind asynchronous constructs,\n+  balance load, handle failures, and react to resharding. However, all\n+  this convenience makes them unsuitable for building Jet connectors,\n+  where we need the most control possible to make choices that are\n+  suitable to Jet's architecture.\n+* **Amazon Kinesis Data Streams API** via **AWS SDK for Java** is the\n+  lower-level library that allows sufficient control when interacting\n+  with Kinesis. It consists of a simple set of [REST-based\n+  operations](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Operations.html).\n+  Every other concern mentioned above, when discussing the high-level\n+  libraries, has to be handled explicitly. This is the library used in\n+  the Jet source and sink implementations.\n+\n+## Quotas\n+\n+Amazon Kinesis Data Streams enforces quite a few quotas and limits,\n+which our sources and sinks need to comply with:\n+\n+* A single shard can ingest up to 1 MB of data per second (including\n+  partition keys) or 1,000 records per second for writes.\n+* The maximum size of the data payload of a record is 1 MB.\n+* The\n+  [GetRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html)\n+  operation can retrieve up to 10 MB of data per call from a single\n+  shard and up to 10,000 records per call.\n+* Each shard can support up to 5 GetRecords operations per second.\n+* Each shard can support up to a maximum total data read rate of 2 MB\n+  per second.\n+* The\n+  [ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+  operation has a limit of 100 transactions per second, per data stream.\n+  Each such transaction is able to return at most 100 shards, and if the\n+  stream has more, then multiple transactions need to be used for a full\n+  listing. (For details, see the [discovery](#discovery) section.)\n+* The\n+  [PutRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)\n+  operation can write at most 500 records into the stream. Each record\n+  in the request can be as large as 1MB, up to a limit of 5MB for the\n+  entire request, including partition keys. Each shard can support\n+  writes up to 1,000 records per second, up to a maximum data write\n+  total of 1 MiB per second.\n+* The\n+  [DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+  operation has a limit of 20 transactions per second per account. (For\n+  details, see the [adaptive throughput](#adaptive-throughput) section.)\n+\n+## Source\n+\n+The Kinesis source is a _streaming_, _distributed_, and _fault-tolerant_\n+data source for Jet. It supports both the _at-least-once_ and\n+_exactly-once_ [processing\n+guarantees](../architecture/fault-tolerance.md#processing-guarantee-is-a-shared-concern).\n+\n+### Distribution\n+\n+Being a distributed source, it has multiple instances running in each\n+Jet cluster member. Each instance is responsible for reading from zero,\n+one or more KDS [shards](#shard). The assignment of shards to individual\n+source instances is based on partitioning the space of Kinesis [record\n+keys](#partition-key).\n+\n+Record keys, or partition keys as Kinesis calls them, are Unicode\n+strings, with a maximum length limit of 256 characters. The stream uses\n+the MD5 hash function to map these strings to 128-bit integer values.\n+The universe of these values is thus the [0 .. 2^128) range. Each\n+Kinesis shard has a continuous chunk of this range assigned to it,\n+called the shard's hash range. The stream assigns a record to a shard if\n+the record's partition key hashes into the shard's range.\n+\n+In the Jet Kinesis source, we use similar logic for assigning shards to\n+source instances. Each of our sources gets a part of the hash range\n+assigned to it. We say that a source owns a specific shard if and only\n+if the shard's hash range's starting point is inside the source's hash\n+range. Any similar range matching logic would work, as long as it's\n+non-ambiguous.\n+\n+### Discovery\n+\n+Reading records from shards assigned to them is only part of the\n+responsibility of sources. Sources also need a way to discover currently\n+active shards in the stream to take responsibility for them. Moreover,\n+this discovery process can't just happen once, on start-up, because\n+streams are dynamic, shards can be closed, and new shards can pop-up at\n+any time. For details, see the [resharding section](#resharding).\n+\n+Continuously monitoring the set of active shards in the stream is the\n+responsibility of **one** of the local source instances in each Jet\n+cluster member. This is an optimization. If all sources would run the\n+discovery, they would still obtain the same data, just with a multiplied\n+effort and cost. Monitoring means continuously polling the stream for\n+the list of all shards in it.\n+\n+Monitoring needs to take care not to cross the rate limit imposed by\n+Kinesis on this operation. For details, see the [quotas\n+section](#quotas).\n+\n+### Resharding\n+\n+Kinesis supports resharding, which lets you adjust the number of shards\n+in your stream to adapt to changes in data flow rate through the stream.\n+(Amazon charges on a per-shard basis, that's why it's desirable to have\n+the smallest amount of shards possible.)\n+\n+There are two types of resharding operations: shard **split** and shard\n+**merge**. In a shard split, you divide a single shard into two adjacent\n+shards. In a shard merge, you combine two adjacent shards into a single\n+shard. By \"adjacent\", we mean that one's hash range starts where the\n+other one's ends.\n+\n+Splitting increases the number of shards in your stream and therefore\n+increases the data capacity (and cost) of the stream. Similarly, merging\n+reduces the number of shards in your stream and therefore decreases the\n+data capacity (and cost).\n+\n+Resharding is always pairwise in the sense that you cannot split into\n+more than two shards in a single operation, and you cannot merge more\n+than two shards in a single operation. The shard or pair of shards that\n+the resharding operation acts on are called parent shards. The shard or\n+pair of shards that result from the resharding operation are called\n+child shards.\n+\n+When child shards, resulting from a split or merge, activate, their\n+parents get deactivated and will no longer get data inserted into them.\n+From that point onward, data goes into the children.\n+\n+### Read Order\n+\n+Resharding does not suspend the stream's dataflow, while it's going on.\n+Data continues to be ingested into the stream, and at some point, it\n+just stops being put into the parent shards and starts being put into\n+the child shards.\n+\n+The Kinesis Jet source would need to make sure that it finishes reading\n+from parents before reading from their children. However, this is not\n+possible since the children might end up being owned by an entirely\n+different instance of the source than their parents (for example, in a\n+split), possibly located in an entirely different Jet cluster member.\n+\n+Moreover, it's not enough to finish reading from the parent before\n+reading from the children. Even if that were achieved, data from parents\n+might overtake data from children further down the Jet pipeline, in its\n+various parallel components. A Kinesis source would need to make sure\n+that it has read all data from the parents and that data has fully\n+passed through the Jet pipeline before starting to read from the\n+children. Only then could it provide the same ordering as KDS while\n+resharding.\n+\n+This is currently not possible in Jet. Hopefully, future versions will\n+address the problem. Users of the Kinesis source need to be aware that\n+some data reordering might occur on resharding.\n+\n+### Fault Tolerance\n+\n+The Kinesis Jet source supports pipelines with both at-least-once and\n+exactly-once processing guarantees. It achieves this by saving KDS\n+offsets into its snapshots and starting data read from saved offsets\n+when restarted.\n+\n+The offsets are saved on a per-shard basis, and on restart, each source\n+instance receives all saved offsets for all shards, so it can function\n+properly regardless of how shards are assigned to sources after the\n+restart.\n+\n+### Watermarks\n+\n+The Kinesis source can provide native watermarks because the [record\n+data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Record.html)\n+has a field that can be turned towards this purpose\n+(`ApproximateArrivalTimestamp`). However, it should be pointed out that\n+these watermarks are \"native\" only from Jet's point of view. They are\n+KDS ingestion times, i.e., whenever a KDS producer managed to push said\n+record into the data stream. We have no way of knowing what's the real\n+event time of a record.\n+\n+Watermarks are also saved to and recovered from snapshots.\n+\n+### Metrics\n+\n+When receiving record batches, the [data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html#API_GetRecords_ResponseSyntax)\n+contains a field called `MillisBehindLatest` defined as following:\n+\n+> The number of milliseconds the GetRecords response is from the\n+> stream's tip, indicating how far behind the current time the consumer\n+> is. A value of zero indicates that record processing caught up, and\n+> there are no new records to process at this moment.\n+\n+This value can be useful for monitoring so the sources can publish it\n+as a per processor instance metric.\n+\n+### Code Example\n+\n+A typical example of setting up a Kinesis source in Jet would look like\n+this:\n+\n+```java\n+KinesisSources.kinesis(\"myStream\")\n+  .withRegion(\"us-east-1\")\n+  .withEndpoint(\"http://localhost:12345\")\n+  .withCredentials(\"accesskey\", \"secretkey\")\n+  .withRetryStrategy(RetryStrategies.indefinitely(250))\n+  .build();\n+```\n+\n+The only mandatory property is the Kinesis `stream name`. The others are\n+optional and can be specified via a fluent builder.\n+\n+If `region` is not specified, then _us-east-1_ will be used by default.\n+\n+If `endpoint` is not specified, then the region's default endpoint will\n+be used.\n+\n+If `credentials` aren't specified, then the [Default Credential Provider\n+Chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html#credentials-default)\n+will be followed.\n+\n+If `retry strategy` is not specified, then a default of our definition", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 282}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI1NTE4Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The Kinesis sink is a _distributed_, and _fault-tolerant_ data sink for\n          \n          \n            \n            The Kinesis sink is a _distributed_, _fault-tolerant_ data sink for", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547255187", "createdAt": "2020-12-22T12:39:53Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.\n+\n+A **sequence number** is a unique identifier for each record. Sequence\n+numbers are assigned by KDS when a data producer publishes data into it.\n+Sequence numbers for the same partition key generally increase over\n+time.\n+\n+## APIs\n+\n+Amazon offers various choices of libraries that can be used to interact\n+with KDS:\n+\n+* **Kinesis Client Library (KCL)** and **Kinesis Producer Library\n+  (KPL)** are high-level libraries that are easy to use because they\n+  abstract away many concerns. They manage their threading policy, hide\n+  away the REST-based nature of Kinesis behind asynchronous constructs,\n+  balance load, handle failures, and react to resharding. However, all\n+  this convenience makes them unsuitable for building Jet connectors,\n+  where we need the most control possible to make choices that are\n+  suitable to Jet's architecture.\n+* **Amazon Kinesis Data Streams API** via **AWS SDK for Java** is the\n+  lower-level library that allows sufficient control when interacting\n+  with Kinesis. It consists of a simple set of [REST-based\n+  operations](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Operations.html).\n+  Every other concern mentioned above, when discussing the high-level\n+  libraries, has to be handled explicitly. This is the library used in\n+  the Jet source and sink implementations.\n+\n+## Quotas\n+\n+Amazon Kinesis Data Streams enforces quite a few quotas and limits,\n+which our sources and sinks need to comply with:\n+\n+* A single shard can ingest up to 1 MB of data per second (including\n+  partition keys) or 1,000 records per second for writes.\n+* The maximum size of the data payload of a record is 1 MB.\n+* The\n+  [GetRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html)\n+  operation can retrieve up to 10 MB of data per call from a single\n+  shard and up to 10,000 records per call.\n+* Each shard can support up to 5 GetRecords operations per second.\n+* Each shard can support up to a maximum total data read rate of 2 MB\n+  per second.\n+* The\n+  [ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+  operation has a limit of 100 transactions per second, per data stream.\n+  Each such transaction is able to return at most 100 shards, and if the\n+  stream has more, then multiple transactions need to be used for a full\n+  listing. (For details, see the [discovery](#discovery) section.)\n+* The\n+  [PutRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)\n+  operation can write at most 500 records into the stream. Each record\n+  in the request can be as large as 1MB, up to a limit of 5MB for the\n+  entire request, including partition keys. Each shard can support\n+  writes up to 1,000 records per second, up to a maximum data write\n+  total of 1 MiB per second.\n+* The\n+  [DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+  operation has a limit of 20 transactions per second per account. (For\n+  details, see the [adaptive throughput](#adaptive-throughput) section.)\n+\n+## Source\n+\n+The Kinesis source is a _streaming_, _distributed_, and _fault-tolerant_\n+data source for Jet. It supports both the _at-least-once_ and\n+_exactly-once_ [processing\n+guarantees](../architecture/fault-tolerance.md#processing-guarantee-is-a-shared-concern).\n+\n+### Distribution\n+\n+Being a distributed source, it has multiple instances running in each\n+Jet cluster member. Each instance is responsible for reading from zero,\n+one or more KDS [shards](#shard). The assignment of shards to individual\n+source instances is based on partitioning the space of Kinesis [record\n+keys](#partition-key).\n+\n+Record keys, or partition keys as Kinesis calls them, are Unicode\n+strings, with a maximum length limit of 256 characters. The stream uses\n+the MD5 hash function to map these strings to 128-bit integer values.\n+The universe of these values is thus the [0 .. 2^128) range. Each\n+Kinesis shard has a continuous chunk of this range assigned to it,\n+called the shard's hash range. The stream assigns a record to a shard if\n+the record's partition key hashes into the shard's range.\n+\n+In the Jet Kinesis source, we use similar logic for assigning shards to\n+source instances. Each of our sources gets a part of the hash range\n+assigned to it. We say that a source owns a specific shard if and only\n+if the shard's hash range's starting point is inside the source's hash\n+range. Any similar range matching logic would work, as long as it's\n+non-ambiguous.\n+\n+### Discovery\n+\n+Reading records from shards assigned to them is only part of the\n+responsibility of sources. Sources also need a way to discover currently\n+active shards in the stream to take responsibility for them. Moreover,\n+this discovery process can't just happen once, on start-up, because\n+streams are dynamic, shards can be closed, and new shards can pop-up at\n+any time. For details, see the [resharding section](#resharding).\n+\n+Continuously monitoring the set of active shards in the stream is the\n+responsibility of **one** of the local source instances in each Jet\n+cluster member. This is an optimization. If all sources would run the\n+discovery, they would still obtain the same data, just with a multiplied\n+effort and cost. Monitoring means continuously polling the stream for\n+the list of all shards in it.\n+\n+Monitoring needs to take care not to cross the rate limit imposed by\n+Kinesis on this operation. For details, see the [quotas\n+section](#quotas).\n+\n+### Resharding\n+\n+Kinesis supports resharding, which lets you adjust the number of shards\n+in your stream to adapt to changes in data flow rate through the stream.\n+(Amazon charges on a per-shard basis, that's why it's desirable to have\n+the smallest amount of shards possible.)\n+\n+There are two types of resharding operations: shard **split** and shard\n+**merge**. In a shard split, you divide a single shard into two adjacent\n+shards. In a shard merge, you combine two adjacent shards into a single\n+shard. By \"adjacent\", we mean that one's hash range starts where the\n+other one's ends.\n+\n+Splitting increases the number of shards in your stream and therefore\n+increases the data capacity (and cost) of the stream. Similarly, merging\n+reduces the number of shards in your stream and therefore decreases the\n+data capacity (and cost).\n+\n+Resharding is always pairwise in the sense that you cannot split into\n+more than two shards in a single operation, and you cannot merge more\n+than two shards in a single operation. The shard or pair of shards that\n+the resharding operation acts on are called parent shards. The shard or\n+pair of shards that result from the resharding operation are called\n+child shards.\n+\n+When child shards, resulting from a split or merge, activate, their\n+parents get deactivated and will no longer get data inserted into them.\n+From that point onward, data goes into the children.\n+\n+### Read Order\n+\n+Resharding does not suspend the stream's dataflow, while it's going on.\n+Data continues to be ingested into the stream, and at some point, it\n+just stops being put into the parent shards and starts being put into\n+the child shards.\n+\n+The Kinesis Jet source would need to make sure that it finishes reading\n+from parents before reading from their children. However, this is not\n+possible since the children might end up being owned by an entirely\n+different instance of the source than their parents (for example, in a\n+split), possibly located in an entirely different Jet cluster member.\n+\n+Moreover, it's not enough to finish reading from the parent before\n+reading from the children. Even if that were achieved, data from parents\n+might overtake data from children further down the Jet pipeline, in its\n+various parallel components. A Kinesis source would need to make sure\n+that it has read all data from the parents and that data has fully\n+passed through the Jet pipeline before starting to read from the\n+children. Only then could it provide the same ordering as KDS while\n+resharding.\n+\n+This is currently not possible in Jet. Hopefully, future versions will\n+address the problem. Users of the Kinesis source need to be aware that\n+some data reordering might occur on resharding.\n+\n+### Fault Tolerance\n+\n+The Kinesis Jet source supports pipelines with both at-least-once and\n+exactly-once processing guarantees. It achieves this by saving KDS\n+offsets into its snapshots and starting data read from saved offsets\n+when restarted.\n+\n+The offsets are saved on a per-shard basis, and on restart, each source\n+instance receives all saved offsets for all shards, so it can function\n+properly regardless of how shards are assigned to sources after the\n+restart.\n+\n+### Watermarks\n+\n+The Kinesis source can provide native watermarks because the [record\n+data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Record.html)\n+has a field that can be turned towards this purpose\n+(`ApproximateArrivalTimestamp`). However, it should be pointed out that\n+these watermarks are \"native\" only from Jet's point of view. They are\n+KDS ingestion times, i.e., whenever a KDS producer managed to push said\n+record into the data stream. We have no way of knowing what's the real\n+event time of a record.\n+\n+Watermarks are also saved to and recovered from snapshots.\n+\n+### Metrics\n+\n+When receiving record batches, the [data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html#API_GetRecords_ResponseSyntax)\n+contains a field called `MillisBehindLatest` defined as following:\n+\n+> The number of milliseconds the GetRecords response is from the\n+> stream's tip, indicating how far behind the current time the consumer\n+> is. A value of zero indicates that record processing caught up, and\n+> there are no new records to process at this moment.\n+\n+This value can be useful for monitoring so the sources can publish it\n+as a per processor instance metric.\n+\n+### Code Example\n+\n+A typical example of setting up a Kinesis source in Jet would look like\n+this:\n+\n+```java\n+KinesisSources.kinesis(\"myStream\")\n+  .withRegion(\"us-east-1\")\n+  .withEndpoint(\"http://localhost:12345\")\n+  .withCredentials(\"accesskey\", \"secretkey\")\n+  .withRetryStrategy(RetryStrategies.indefinitely(250))\n+  .build();\n+```\n+\n+The only mandatory property is the Kinesis `stream name`. The others are\n+optional and can be specified via a fluent builder.\n+\n+If `region` is not specified, then _us-east-1_ will be used by default.\n+\n+If `endpoint` is not specified, then the region's default endpoint will\n+be used.\n+\n+If `credentials` aren't specified, then the [Default Credential Provider\n+Chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html#credentials-default)\n+will be followed.\n+\n+If `retry strategy` is not specified, then a default of our definition\n+will be used (maximum 10 retries, with exponential backoff limited to\n+a maximum of 3 seconds). A source's retry strategy applies to failures\n+of reading records from or listing shards of a stream.\n+\n+The actual source created will be of type\n+`StreamSource<Map.Entry<String, byte[]>>`, so basically a stream of\n+partition key - record data blob pairs.\n+\n+## Sink\n+\n+The Kinesis sink is a _distributed_, and _fault-tolerant_ data sink for", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 293}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI1NjIwOQ==", "bodyText": "1 MB? 1 MiB?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547256209", "createdAt": "2020-12-22T12:42:16Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.\n+\n+A **sequence number** is a unique identifier for each record. Sequence\n+numbers are assigned by KDS when a data producer publishes data into it.\n+Sequence numbers for the same partition key generally increase over\n+time.\n+\n+## APIs\n+\n+Amazon offers various choices of libraries that can be used to interact\n+with KDS:\n+\n+* **Kinesis Client Library (KCL)** and **Kinesis Producer Library\n+  (KPL)** are high-level libraries that are easy to use because they\n+  abstract away many concerns. They manage their threading policy, hide\n+  away the REST-based nature of Kinesis behind asynchronous constructs,\n+  balance load, handle failures, and react to resharding. However, all\n+  this convenience makes them unsuitable for building Jet connectors,\n+  where we need the most control possible to make choices that are\n+  suitable to Jet's architecture.\n+* **Amazon Kinesis Data Streams API** via **AWS SDK for Java** is the\n+  lower-level library that allows sufficient control when interacting\n+  with Kinesis. It consists of a simple set of [REST-based\n+  operations](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Operations.html).\n+  Every other concern mentioned above, when discussing the high-level\n+  libraries, has to be handled explicitly. This is the library used in\n+  the Jet source and sink implementations.\n+\n+## Quotas\n+\n+Amazon Kinesis Data Streams enforces quite a few quotas and limits,\n+which our sources and sinks need to comply with:\n+\n+* A single shard can ingest up to 1 MB of data per second (including\n+  partition keys) or 1,000 records per second for writes.\n+* The maximum size of the data payload of a record is 1 MB.\n+* The\n+  [GetRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html)\n+  operation can retrieve up to 10 MB of data per call from a single\n+  shard and up to 10,000 records per call.\n+* Each shard can support up to 5 GetRecords operations per second.\n+* Each shard can support up to a maximum total data read rate of 2 MB\n+  per second.\n+* The\n+  [ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+  operation has a limit of 100 transactions per second, per data stream.\n+  Each such transaction is able to return at most 100 shards, and if the\n+  stream has more, then multiple transactions need to be used for a full\n+  listing. (For details, see the [discovery](#discovery) section.)\n+* The\n+  [PutRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)\n+  operation can write at most 500 records into the stream. Each record\n+  in the request can be as large as 1MB, up to a limit of 5MB for the\n+  entire request, including partition keys. Each shard can support\n+  writes up to 1,000 records per second, up to a maximum data write\n+  total of 1 MiB per second.\n+* The\n+  [DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+  operation has a limit of 20 transactions per second per account. (For\n+  details, see the [adaptive throughput](#adaptive-throughput) section.)\n+\n+## Source\n+\n+The Kinesis source is a _streaming_, _distributed_, and _fault-tolerant_\n+data source for Jet. It supports both the _at-least-once_ and\n+_exactly-once_ [processing\n+guarantees](../architecture/fault-tolerance.md#processing-guarantee-is-a-shared-concern).\n+\n+### Distribution\n+\n+Being a distributed source, it has multiple instances running in each\n+Jet cluster member. Each instance is responsible for reading from zero,\n+one or more KDS [shards](#shard). The assignment of shards to individual\n+source instances is based on partitioning the space of Kinesis [record\n+keys](#partition-key).\n+\n+Record keys, or partition keys as Kinesis calls them, are Unicode\n+strings, with a maximum length limit of 256 characters. The stream uses\n+the MD5 hash function to map these strings to 128-bit integer values.\n+The universe of these values is thus the [0 .. 2^128) range. Each\n+Kinesis shard has a continuous chunk of this range assigned to it,\n+called the shard's hash range. The stream assigns a record to a shard if\n+the record's partition key hashes into the shard's range.\n+\n+In the Jet Kinesis source, we use similar logic for assigning shards to\n+source instances. Each of our sources gets a part of the hash range\n+assigned to it. We say that a source owns a specific shard if and only\n+if the shard's hash range's starting point is inside the source's hash\n+range. Any similar range matching logic would work, as long as it's\n+non-ambiguous.\n+\n+### Discovery\n+\n+Reading records from shards assigned to them is only part of the\n+responsibility of sources. Sources also need a way to discover currently\n+active shards in the stream to take responsibility for them. Moreover,\n+this discovery process can't just happen once, on start-up, because\n+streams are dynamic, shards can be closed, and new shards can pop-up at\n+any time. For details, see the [resharding section](#resharding).\n+\n+Continuously monitoring the set of active shards in the stream is the\n+responsibility of **one** of the local source instances in each Jet\n+cluster member. This is an optimization. If all sources would run the\n+discovery, they would still obtain the same data, just with a multiplied\n+effort and cost. Monitoring means continuously polling the stream for\n+the list of all shards in it.\n+\n+Monitoring needs to take care not to cross the rate limit imposed by\n+Kinesis on this operation. For details, see the [quotas\n+section](#quotas).\n+\n+### Resharding\n+\n+Kinesis supports resharding, which lets you adjust the number of shards\n+in your stream to adapt to changes in data flow rate through the stream.\n+(Amazon charges on a per-shard basis, that's why it's desirable to have\n+the smallest amount of shards possible.)\n+\n+There are two types of resharding operations: shard **split** and shard\n+**merge**. In a shard split, you divide a single shard into two adjacent\n+shards. In a shard merge, you combine two adjacent shards into a single\n+shard. By \"adjacent\", we mean that one's hash range starts where the\n+other one's ends.\n+\n+Splitting increases the number of shards in your stream and therefore\n+increases the data capacity (and cost) of the stream. Similarly, merging\n+reduces the number of shards in your stream and therefore decreases the\n+data capacity (and cost).\n+\n+Resharding is always pairwise in the sense that you cannot split into\n+more than two shards in a single operation, and you cannot merge more\n+than two shards in a single operation. The shard or pair of shards that\n+the resharding operation acts on are called parent shards. The shard or\n+pair of shards that result from the resharding operation are called\n+child shards.\n+\n+When child shards, resulting from a split or merge, activate, their\n+parents get deactivated and will no longer get data inserted into them.\n+From that point onward, data goes into the children.\n+\n+### Read Order\n+\n+Resharding does not suspend the stream's dataflow, while it's going on.\n+Data continues to be ingested into the stream, and at some point, it\n+just stops being put into the parent shards and starts being put into\n+the child shards.\n+\n+The Kinesis Jet source would need to make sure that it finishes reading\n+from parents before reading from their children. However, this is not\n+possible since the children might end up being owned by an entirely\n+different instance of the source than their parents (for example, in a\n+split), possibly located in an entirely different Jet cluster member.\n+\n+Moreover, it's not enough to finish reading from the parent before\n+reading from the children. Even if that were achieved, data from parents\n+might overtake data from children further down the Jet pipeline, in its\n+various parallel components. A Kinesis source would need to make sure\n+that it has read all data from the parents and that data has fully\n+passed through the Jet pipeline before starting to read from the\n+children. Only then could it provide the same ordering as KDS while\n+resharding.\n+\n+This is currently not possible in Jet. Hopefully, future versions will\n+address the problem. Users of the Kinesis source need to be aware that\n+some data reordering might occur on resharding.\n+\n+### Fault Tolerance\n+\n+The Kinesis Jet source supports pipelines with both at-least-once and\n+exactly-once processing guarantees. It achieves this by saving KDS\n+offsets into its snapshots and starting data read from saved offsets\n+when restarted.\n+\n+The offsets are saved on a per-shard basis, and on restart, each source\n+instance receives all saved offsets for all shards, so it can function\n+properly regardless of how shards are assigned to sources after the\n+restart.\n+\n+### Watermarks\n+\n+The Kinesis source can provide native watermarks because the [record\n+data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Record.html)\n+has a field that can be turned towards this purpose\n+(`ApproximateArrivalTimestamp`). However, it should be pointed out that\n+these watermarks are \"native\" only from Jet's point of view. They are\n+KDS ingestion times, i.e., whenever a KDS producer managed to push said\n+record into the data stream. We have no way of knowing what's the real\n+event time of a record.\n+\n+Watermarks are also saved to and recovered from snapshots.\n+\n+### Metrics\n+\n+When receiving record batches, the [data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html#API_GetRecords_ResponseSyntax)\n+contains a field called `MillisBehindLatest` defined as following:\n+\n+> The number of milliseconds the GetRecords response is from the\n+> stream's tip, indicating how far behind the current time the consumer\n+> is. A value of zero indicates that record processing caught up, and\n+> there are no new records to process at this moment.\n+\n+This value can be useful for monitoring so the sources can publish it\n+as a per processor instance metric.\n+\n+### Code Example\n+\n+A typical example of setting up a Kinesis source in Jet would look like\n+this:\n+\n+```java\n+KinesisSources.kinesis(\"myStream\")\n+  .withRegion(\"us-east-1\")\n+  .withEndpoint(\"http://localhost:12345\")\n+  .withCredentials(\"accesskey\", \"secretkey\")\n+  .withRetryStrategy(RetryStrategies.indefinitely(250))\n+  .build();\n+```\n+\n+The only mandatory property is the Kinesis `stream name`. The others are\n+optional and can be specified via a fluent builder.\n+\n+If `region` is not specified, then _us-east-1_ will be used by default.\n+\n+If `endpoint` is not specified, then the region's default endpoint will\n+be used.\n+\n+If `credentials` aren't specified, then the [Default Credential Provider\n+Chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html#credentials-default)\n+will be followed.\n+\n+If `retry strategy` is not specified, then a default of our definition\n+will be used (maximum 10 retries, with exponential backoff limited to\n+a maximum of 3 seconds). A source's retry strategy applies to failures\n+of reading records from or listing shards of a stream.\n+\n+The actual source created will be of type\n+`StreamSource<Map.Entry<String, byte[]>>`, so basically a stream of\n+partition key - record data blob pairs.\n+\n+## Sink\n+\n+The Kinesis sink is a _distributed_, and _fault-tolerant_ data sink for\n+Jet. It supports both _streaming_ and _batching_ pipelines. The\n+fault-tolerance guarantee it can offer is only _at-least-once_ since\n+Kinesis does not offer transaction support.\n+\n+### Distribution\n+\n+Being a distributed sink, it has multiple instances running in each Jet\n+cluster member. When used in a pipeline, this sink forces its incoming\n+edges to be _distributed_ and _partitioned_. The partition keys used by\n+the edges are the same as the Kinesis [partition keys](#partition-key).\n+This ensures that all data with the same partition key will end up in\n+the same global sink instance and the same shard.\n+\n+### Flow Control\n+\n+Writing data into a Kinesis Data Stream is governed by multiple\n+limitations:\n+\n+* no more than 500 records can be written in one batch\n+* each record must contain no more than 1M of data", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 313}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI1ODYyMQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * until failures repeat, it keeps quickly increasing the sleep delays\n          \n          \n            \n            * as long as failures repeat, it keeps quickly increasing the sleep delays", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547258621", "createdAt": "2020-12-22T12:47:28Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.\n+\n+A **sequence number** is a unique identifier for each record. Sequence\n+numbers are assigned by KDS when a data producer publishes data into it.\n+Sequence numbers for the same partition key generally increase over\n+time.\n+\n+## APIs\n+\n+Amazon offers various choices of libraries that can be used to interact\n+with KDS:\n+\n+* **Kinesis Client Library (KCL)** and **Kinesis Producer Library\n+  (KPL)** are high-level libraries that are easy to use because they\n+  abstract away many concerns. They manage their threading policy, hide\n+  away the REST-based nature of Kinesis behind asynchronous constructs,\n+  balance load, handle failures, and react to resharding. However, all\n+  this convenience makes them unsuitable for building Jet connectors,\n+  where we need the most control possible to make choices that are\n+  suitable to Jet's architecture.\n+* **Amazon Kinesis Data Streams API** via **AWS SDK for Java** is the\n+  lower-level library that allows sufficient control when interacting\n+  with Kinesis. It consists of a simple set of [REST-based\n+  operations](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Operations.html).\n+  Every other concern mentioned above, when discussing the high-level\n+  libraries, has to be handled explicitly. This is the library used in\n+  the Jet source and sink implementations.\n+\n+## Quotas\n+\n+Amazon Kinesis Data Streams enforces quite a few quotas and limits,\n+which our sources and sinks need to comply with:\n+\n+* A single shard can ingest up to 1 MB of data per second (including\n+  partition keys) or 1,000 records per second for writes.\n+* The maximum size of the data payload of a record is 1 MB.\n+* The\n+  [GetRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html)\n+  operation can retrieve up to 10 MB of data per call from a single\n+  shard and up to 10,000 records per call.\n+* Each shard can support up to 5 GetRecords operations per second.\n+* Each shard can support up to a maximum total data read rate of 2 MB\n+  per second.\n+* The\n+  [ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+  operation has a limit of 100 transactions per second, per data stream.\n+  Each such transaction is able to return at most 100 shards, and if the\n+  stream has more, then multiple transactions need to be used for a full\n+  listing. (For details, see the [discovery](#discovery) section.)\n+* The\n+  [PutRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)\n+  operation can write at most 500 records into the stream. Each record\n+  in the request can be as large as 1MB, up to a limit of 5MB for the\n+  entire request, including partition keys. Each shard can support\n+  writes up to 1,000 records per second, up to a maximum data write\n+  total of 1 MiB per second.\n+* The\n+  [DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+  operation has a limit of 20 transactions per second per account. (For\n+  details, see the [adaptive throughput](#adaptive-throughput) section.)\n+\n+## Source\n+\n+The Kinesis source is a _streaming_, _distributed_, and _fault-tolerant_\n+data source for Jet. It supports both the _at-least-once_ and\n+_exactly-once_ [processing\n+guarantees](../architecture/fault-tolerance.md#processing-guarantee-is-a-shared-concern).\n+\n+### Distribution\n+\n+Being a distributed source, it has multiple instances running in each\n+Jet cluster member. Each instance is responsible for reading from zero,\n+one or more KDS [shards](#shard). The assignment of shards to individual\n+source instances is based on partitioning the space of Kinesis [record\n+keys](#partition-key).\n+\n+Record keys, or partition keys as Kinesis calls them, are Unicode\n+strings, with a maximum length limit of 256 characters. The stream uses\n+the MD5 hash function to map these strings to 128-bit integer values.\n+The universe of these values is thus the [0 .. 2^128) range. Each\n+Kinesis shard has a continuous chunk of this range assigned to it,\n+called the shard's hash range. The stream assigns a record to a shard if\n+the record's partition key hashes into the shard's range.\n+\n+In the Jet Kinesis source, we use similar logic for assigning shards to\n+source instances. Each of our sources gets a part of the hash range\n+assigned to it. We say that a source owns a specific shard if and only\n+if the shard's hash range's starting point is inside the source's hash\n+range. Any similar range matching logic would work, as long as it's\n+non-ambiguous.\n+\n+### Discovery\n+\n+Reading records from shards assigned to them is only part of the\n+responsibility of sources. Sources also need a way to discover currently\n+active shards in the stream to take responsibility for them. Moreover,\n+this discovery process can't just happen once, on start-up, because\n+streams are dynamic, shards can be closed, and new shards can pop-up at\n+any time. For details, see the [resharding section](#resharding).\n+\n+Continuously monitoring the set of active shards in the stream is the\n+responsibility of **one** of the local source instances in each Jet\n+cluster member. This is an optimization. If all sources would run the\n+discovery, they would still obtain the same data, just with a multiplied\n+effort and cost. Monitoring means continuously polling the stream for\n+the list of all shards in it.\n+\n+Monitoring needs to take care not to cross the rate limit imposed by\n+Kinesis on this operation. For details, see the [quotas\n+section](#quotas).\n+\n+### Resharding\n+\n+Kinesis supports resharding, which lets you adjust the number of shards\n+in your stream to adapt to changes in data flow rate through the stream.\n+(Amazon charges on a per-shard basis, that's why it's desirable to have\n+the smallest amount of shards possible.)\n+\n+There are two types of resharding operations: shard **split** and shard\n+**merge**. In a shard split, you divide a single shard into two adjacent\n+shards. In a shard merge, you combine two adjacent shards into a single\n+shard. By \"adjacent\", we mean that one's hash range starts where the\n+other one's ends.\n+\n+Splitting increases the number of shards in your stream and therefore\n+increases the data capacity (and cost) of the stream. Similarly, merging\n+reduces the number of shards in your stream and therefore decreases the\n+data capacity (and cost).\n+\n+Resharding is always pairwise in the sense that you cannot split into\n+more than two shards in a single operation, and you cannot merge more\n+than two shards in a single operation. The shard or pair of shards that\n+the resharding operation acts on are called parent shards. The shard or\n+pair of shards that result from the resharding operation are called\n+child shards.\n+\n+When child shards, resulting from a split or merge, activate, their\n+parents get deactivated and will no longer get data inserted into them.\n+From that point onward, data goes into the children.\n+\n+### Read Order\n+\n+Resharding does not suspend the stream's dataflow, while it's going on.\n+Data continues to be ingested into the stream, and at some point, it\n+just stops being put into the parent shards and starts being put into\n+the child shards.\n+\n+The Kinesis Jet source would need to make sure that it finishes reading\n+from parents before reading from their children. However, this is not\n+possible since the children might end up being owned by an entirely\n+different instance of the source than their parents (for example, in a\n+split), possibly located in an entirely different Jet cluster member.\n+\n+Moreover, it's not enough to finish reading from the parent before\n+reading from the children. Even if that were achieved, data from parents\n+might overtake data from children further down the Jet pipeline, in its\n+various parallel components. A Kinesis source would need to make sure\n+that it has read all data from the parents and that data has fully\n+passed through the Jet pipeline before starting to read from the\n+children. Only then could it provide the same ordering as KDS while\n+resharding.\n+\n+This is currently not possible in Jet. Hopefully, future versions will\n+address the problem. Users of the Kinesis source need to be aware that\n+some data reordering might occur on resharding.\n+\n+### Fault Tolerance\n+\n+The Kinesis Jet source supports pipelines with both at-least-once and\n+exactly-once processing guarantees. It achieves this by saving KDS\n+offsets into its snapshots and starting data read from saved offsets\n+when restarted.\n+\n+The offsets are saved on a per-shard basis, and on restart, each source\n+instance receives all saved offsets for all shards, so it can function\n+properly regardless of how shards are assigned to sources after the\n+restart.\n+\n+### Watermarks\n+\n+The Kinesis source can provide native watermarks because the [record\n+data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Record.html)\n+has a field that can be turned towards this purpose\n+(`ApproximateArrivalTimestamp`). However, it should be pointed out that\n+these watermarks are \"native\" only from Jet's point of view. They are\n+KDS ingestion times, i.e., whenever a KDS producer managed to push said\n+record into the data stream. We have no way of knowing what's the real\n+event time of a record.\n+\n+Watermarks are also saved to and recovered from snapshots.\n+\n+### Metrics\n+\n+When receiving record batches, the [data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html#API_GetRecords_ResponseSyntax)\n+contains a field called `MillisBehindLatest` defined as following:\n+\n+> The number of milliseconds the GetRecords response is from the\n+> stream's tip, indicating how far behind the current time the consumer\n+> is. A value of zero indicates that record processing caught up, and\n+> there are no new records to process at this moment.\n+\n+This value can be useful for monitoring so the sources can publish it\n+as a per processor instance metric.\n+\n+### Code Example\n+\n+A typical example of setting up a Kinesis source in Jet would look like\n+this:\n+\n+```java\n+KinesisSources.kinesis(\"myStream\")\n+  .withRegion(\"us-east-1\")\n+  .withEndpoint(\"http://localhost:12345\")\n+  .withCredentials(\"accesskey\", \"secretkey\")\n+  .withRetryStrategy(RetryStrategies.indefinitely(250))\n+  .build();\n+```\n+\n+The only mandatory property is the Kinesis `stream name`. The others are\n+optional and can be specified via a fluent builder.\n+\n+If `region` is not specified, then _us-east-1_ will be used by default.\n+\n+If `endpoint` is not specified, then the region's default endpoint will\n+be used.\n+\n+If `credentials` aren't specified, then the [Default Credential Provider\n+Chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html#credentials-default)\n+will be followed.\n+\n+If `retry strategy` is not specified, then a default of our definition\n+will be used (maximum 10 retries, with exponential backoff limited to\n+a maximum of 3 seconds). A source's retry strategy applies to failures\n+of reading records from or listing shards of a stream.\n+\n+The actual source created will be of type\n+`StreamSource<Map.Entry<String, byte[]>>`, so basically a stream of\n+partition key - record data blob pairs.\n+\n+## Sink\n+\n+The Kinesis sink is a _distributed_, and _fault-tolerant_ data sink for\n+Jet. It supports both _streaming_ and _batching_ pipelines. The\n+fault-tolerance guarantee it can offer is only _at-least-once_ since\n+Kinesis does not offer transaction support.\n+\n+### Distribution\n+\n+Being a distributed sink, it has multiple instances running in each Jet\n+cluster member. When used in a pipeline, this sink forces its incoming\n+edges to be _distributed_ and _partitioned_. The partition keys used by\n+the edges are the same as the Kinesis [partition keys](#partition-key).\n+This ensures that all data with the same partition key will end up in\n+the same global sink instance and the same shard.\n+\n+### Flow Control\n+\n+Writing data into a Kinesis Data Stream is governed by multiple\n+limitations:\n+\n+* no more than 500 records can be written in one batch\n+* each record must contain no more than 1M of data\n+* each batch must contain no more than 5M of data\n+* each shard can ingest no more than 1,000 records per second\n+\n+While most of these limitations are simple to enforce, the shard\n+ingestion rate is not. Different partition keys get assigned to a shard\n+based on a hashing function, so partition keys going into the same shard\n+can be written by different sink instances. Currently, Jet has no\n+capability for computing and coordinating such a per shard rate among\n+all it's distributed sink instances.\n+\n+The sink takes a different approach to comply with this limitation. It\n+allows for the rate to be tripped (i.e., it doesn't attempt to prevent\n+it from happening), but once it gets tripped, sinks try to slow down the\n+amount of data they write to keep the rate violation as an occasional,\n+rare event and not a continuous storm.\n+\n+The source achieves this flow control in two ways:\n+\n+* by decreasing the send batch size; the default is the maximum of 500,\n+  which it will reduce, if necessary, as low as 10 records/batch\n+* by adding a delay between two subsequent send actions (which can be as\n+  little as 100ms, a reasonable value in case of Kinesis and as much as\n+  10 seconds, which is a lot, but would occur only in an unreasonably\n+  sized stream, as far as shard count is concerned - ultimately the\n+  owner of the stream is responsible for setting up enough shards to be\n+  able to handle his data rates)\n+\n+The flow control process is _adaptive_ in the sense that:\n+\n+* it kicks in only when batches start failing due to shard ingestion\n+  rates being tripped\n+* until failures repeat, it keeps quickly increasing the sleep delays", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 345}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI2MDA2Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This unfortunate fact originates in the way how KDS handles shard\n          \n          \n            \n            This fact originates in the way how KDS handles shard", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547260062", "createdAt": "2020-12-22T12:50:44Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.\n+\n+A **sequence number** is a unique identifier for each record. Sequence\n+numbers are assigned by KDS when a data producer publishes data into it.\n+Sequence numbers for the same partition key generally increase over\n+time.\n+\n+## APIs\n+\n+Amazon offers various choices of libraries that can be used to interact\n+with KDS:\n+\n+* **Kinesis Client Library (KCL)** and **Kinesis Producer Library\n+  (KPL)** are high-level libraries that are easy to use because they\n+  abstract away many concerns. They manage their threading policy, hide\n+  away the REST-based nature of Kinesis behind asynchronous constructs,\n+  balance load, handle failures, and react to resharding. However, all\n+  this convenience makes them unsuitable for building Jet connectors,\n+  where we need the most control possible to make choices that are\n+  suitable to Jet's architecture.\n+* **Amazon Kinesis Data Streams API** via **AWS SDK for Java** is the\n+  lower-level library that allows sufficient control when interacting\n+  with Kinesis. It consists of a simple set of [REST-based\n+  operations](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Operations.html).\n+  Every other concern mentioned above, when discussing the high-level\n+  libraries, has to be handled explicitly. This is the library used in\n+  the Jet source and sink implementations.\n+\n+## Quotas\n+\n+Amazon Kinesis Data Streams enforces quite a few quotas and limits,\n+which our sources and sinks need to comply with:\n+\n+* A single shard can ingest up to 1 MB of data per second (including\n+  partition keys) or 1,000 records per second for writes.\n+* The maximum size of the data payload of a record is 1 MB.\n+* The\n+  [GetRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html)\n+  operation can retrieve up to 10 MB of data per call from a single\n+  shard and up to 10,000 records per call.\n+* Each shard can support up to 5 GetRecords operations per second.\n+* Each shard can support up to a maximum total data read rate of 2 MB\n+  per second.\n+* The\n+  [ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+  operation has a limit of 100 transactions per second, per data stream.\n+  Each such transaction is able to return at most 100 shards, and if the\n+  stream has more, then multiple transactions need to be used for a full\n+  listing. (For details, see the [discovery](#discovery) section.)\n+* The\n+  [PutRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)\n+  operation can write at most 500 records into the stream. Each record\n+  in the request can be as large as 1MB, up to a limit of 5MB for the\n+  entire request, including partition keys. Each shard can support\n+  writes up to 1,000 records per second, up to a maximum data write\n+  total of 1 MiB per second.\n+* The\n+  [DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+  operation has a limit of 20 transactions per second per account. (For\n+  details, see the [adaptive throughput](#adaptive-throughput) section.)\n+\n+## Source\n+\n+The Kinesis source is a _streaming_, _distributed_, and _fault-tolerant_\n+data source for Jet. It supports both the _at-least-once_ and\n+_exactly-once_ [processing\n+guarantees](../architecture/fault-tolerance.md#processing-guarantee-is-a-shared-concern).\n+\n+### Distribution\n+\n+Being a distributed source, it has multiple instances running in each\n+Jet cluster member. Each instance is responsible for reading from zero,\n+one or more KDS [shards](#shard). The assignment of shards to individual\n+source instances is based on partitioning the space of Kinesis [record\n+keys](#partition-key).\n+\n+Record keys, or partition keys as Kinesis calls them, are Unicode\n+strings, with a maximum length limit of 256 characters. The stream uses\n+the MD5 hash function to map these strings to 128-bit integer values.\n+The universe of these values is thus the [0 .. 2^128) range. Each\n+Kinesis shard has a continuous chunk of this range assigned to it,\n+called the shard's hash range. The stream assigns a record to a shard if\n+the record's partition key hashes into the shard's range.\n+\n+In the Jet Kinesis source, we use similar logic for assigning shards to\n+source instances. Each of our sources gets a part of the hash range\n+assigned to it. We say that a source owns a specific shard if and only\n+if the shard's hash range's starting point is inside the source's hash\n+range. Any similar range matching logic would work, as long as it's\n+non-ambiguous.\n+\n+### Discovery\n+\n+Reading records from shards assigned to them is only part of the\n+responsibility of sources. Sources also need a way to discover currently\n+active shards in the stream to take responsibility for them. Moreover,\n+this discovery process can't just happen once, on start-up, because\n+streams are dynamic, shards can be closed, and new shards can pop-up at\n+any time. For details, see the [resharding section](#resharding).\n+\n+Continuously monitoring the set of active shards in the stream is the\n+responsibility of **one** of the local source instances in each Jet\n+cluster member. This is an optimization. If all sources would run the\n+discovery, they would still obtain the same data, just with a multiplied\n+effort and cost. Monitoring means continuously polling the stream for\n+the list of all shards in it.\n+\n+Monitoring needs to take care not to cross the rate limit imposed by\n+Kinesis on this operation. For details, see the [quotas\n+section](#quotas).\n+\n+### Resharding\n+\n+Kinesis supports resharding, which lets you adjust the number of shards\n+in your stream to adapt to changes in data flow rate through the stream.\n+(Amazon charges on a per-shard basis, that's why it's desirable to have\n+the smallest amount of shards possible.)\n+\n+There are two types of resharding operations: shard **split** and shard\n+**merge**. In a shard split, you divide a single shard into two adjacent\n+shards. In a shard merge, you combine two adjacent shards into a single\n+shard. By \"adjacent\", we mean that one's hash range starts where the\n+other one's ends.\n+\n+Splitting increases the number of shards in your stream and therefore\n+increases the data capacity (and cost) of the stream. Similarly, merging\n+reduces the number of shards in your stream and therefore decreases the\n+data capacity (and cost).\n+\n+Resharding is always pairwise in the sense that you cannot split into\n+more than two shards in a single operation, and you cannot merge more\n+than two shards in a single operation. The shard or pair of shards that\n+the resharding operation acts on are called parent shards. The shard or\n+pair of shards that result from the resharding operation are called\n+child shards.\n+\n+When child shards, resulting from a split or merge, activate, their\n+parents get deactivated and will no longer get data inserted into them.\n+From that point onward, data goes into the children.\n+\n+### Read Order\n+\n+Resharding does not suspend the stream's dataflow, while it's going on.\n+Data continues to be ingested into the stream, and at some point, it\n+just stops being put into the parent shards and starts being put into\n+the child shards.\n+\n+The Kinesis Jet source would need to make sure that it finishes reading\n+from parents before reading from their children. However, this is not\n+possible since the children might end up being owned by an entirely\n+different instance of the source than their parents (for example, in a\n+split), possibly located in an entirely different Jet cluster member.\n+\n+Moreover, it's not enough to finish reading from the parent before\n+reading from the children. Even if that were achieved, data from parents\n+might overtake data from children further down the Jet pipeline, in its\n+various parallel components. A Kinesis source would need to make sure\n+that it has read all data from the parents and that data has fully\n+passed through the Jet pipeline before starting to read from the\n+children. Only then could it provide the same ordering as KDS while\n+resharding.\n+\n+This is currently not possible in Jet. Hopefully, future versions will\n+address the problem. Users of the Kinesis source need to be aware that\n+some data reordering might occur on resharding.\n+\n+### Fault Tolerance\n+\n+The Kinesis Jet source supports pipelines with both at-least-once and\n+exactly-once processing guarantees. It achieves this by saving KDS\n+offsets into its snapshots and starting data read from saved offsets\n+when restarted.\n+\n+The offsets are saved on a per-shard basis, and on restart, each source\n+instance receives all saved offsets for all shards, so it can function\n+properly regardless of how shards are assigned to sources after the\n+restart.\n+\n+### Watermarks\n+\n+The Kinesis source can provide native watermarks because the [record\n+data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Record.html)\n+has a field that can be turned towards this purpose\n+(`ApproximateArrivalTimestamp`). However, it should be pointed out that\n+these watermarks are \"native\" only from Jet's point of view. They are\n+KDS ingestion times, i.e., whenever a KDS producer managed to push said\n+record into the data stream. We have no way of knowing what's the real\n+event time of a record.\n+\n+Watermarks are also saved to and recovered from snapshots.\n+\n+### Metrics\n+\n+When receiving record batches, the [data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html#API_GetRecords_ResponseSyntax)\n+contains a field called `MillisBehindLatest` defined as following:\n+\n+> The number of milliseconds the GetRecords response is from the\n+> stream's tip, indicating how far behind the current time the consumer\n+> is. A value of zero indicates that record processing caught up, and\n+> there are no new records to process at this moment.\n+\n+This value can be useful for monitoring so the sources can publish it\n+as a per processor instance metric.\n+\n+### Code Example\n+\n+A typical example of setting up a Kinesis source in Jet would look like\n+this:\n+\n+```java\n+KinesisSources.kinesis(\"myStream\")\n+  .withRegion(\"us-east-1\")\n+  .withEndpoint(\"http://localhost:12345\")\n+  .withCredentials(\"accesskey\", \"secretkey\")\n+  .withRetryStrategy(RetryStrategies.indefinitely(250))\n+  .build();\n+```\n+\n+The only mandatory property is the Kinesis `stream name`. The others are\n+optional and can be specified via a fluent builder.\n+\n+If `region` is not specified, then _us-east-1_ will be used by default.\n+\n+If `endpoint` is not specified, then the region's default endpoint will\n+be used.\n+\n+If `credentials` aren't specified, then the [Default Credential Provider\n+Chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html#credentials-default)\n+will be followed.\n+\n+If `retry strategy` is not specified, then a default of our definition\n+will be used (maximum 10 retries, with exponential backoff limited to\n+a maximum of 3 seconds). A source's retry strategy applies to failures\n+of reading records from or listing shards of a stream.\n+\n+The actual source created will be of type\n+`StreamSource<Map.Entry<String, byte[]>>`, so basically a stream of\n+partition key - record data blob pairs.\n+\n+## Sink\n+\n+The Kinesis sink is a _distributed_, and _fault-tolerant_ data sink for\n+Jet. It supports both _streaming_ and _batching_ pipelines. The\n+fault-tolerance guarantee it can offer is only _at-least-once_ since\n+Kinesis does not offer transaction support.\n+\n+### Distribution\n+\n+Being a distributed sink, it has multiple instances running in each Jet\n+cluster member. When used in a pipeline, this sink forces its incoming\n+edges to be _distributed_ and _partitioned_. The partition keys used by\n+the edges are the same as the Kinesis [partition keys](#partition-key).\n+This ensures that all data with the same partition key will end up in\n+the same global sink instance and the same shard.\n+\n+### Flow Control\n+\n+Writing data into a Kinesis Data Stream is governed by multiple\n+limitations:\n+\n+* no more than 500 records can be written in one batch\n+* each record must contain no more than 1M of data\n+* each batch must contain no more than 5M of data\n+* each shard can ingest no more than 1,000 records per second\n+\n+While most of these limitations are simple to enforce, the shard\n+ingestion rate is not. Different partition keys get assigned to a shard\n+based on a hashing function, so partition keys going into the same shard\n+can be written by different sink instances. Currently, Jet has no\n+capability for computing and coordinating such a per shard rate among\n+all it's distributed sink instances.\n+\n+The sink takes a different approach to comply with this limitation. It\n+allows for the rate to be tripped (i.e., it doesn't attempt to prevent\n+it from happening), but once it gets tripped, sinks try to slow down the\n+amount of data they write to keep the rate violation as an occasional,\n+rare event and not a continuous storm.\n+\n+The source achieves this flow control in two ways:\n+\n+* by decreasing the send batch size; the default is the maximum of 500,\n+  which it will reduce, if necessary, as low as 10 records/batch\n+* by adding a delay between two subsequent send actions (which can be as\n+  little as 100ms, a reasonable value in case of Kinesis and as much as\n+  10 seconds, which is a lot, but would occur only in an unreasonably\n+  sized stream, as far as shard count is concerned - ultimately the\n+  owner of the stream is responsible for setting up enough shards to be\n+  able to handle his data rates)\n+\n+The flow control process is _adaptive_ in the sense that:\n+\n+* it kicks in only when batches start failing due to shard ingestion\n+  rates being tripped\n+* until failures repeat, it keeps quickly increasing the sleep delays\n+  to stop them from happening\n+* once failures stop, it slowly decreases the sleep delays until they\n+  are eliminated (i.e., the data volume spike was only temporary) or\n+  until failures start happening again\n+\n+Under normal circumstances, if there are enough shards in the stream and\n+their data ingestion rate covers the data flow, this whole flow control\n+process stays shut off. The sink publishes data with the lowest possible\n+latency.\n+\n+### Discovery\n+\n+As we've seen in the [flow control](#flow-control) section, one element\n+used to control the throughput is batch size. Under normal conditions,\n+the sink uses the default/maximum batch size of 500. When flow control\n+kicks a new batch size is picked as a function of the number of open\n+shards in the stream.\n+\n+For this to happen, the sinks need to have relatively up-to-date\n+information about the count of open shards. The sink achieves this by\n+using a mechanism very similar to the [discovery process employed by the\n+source](#discovery). The only real difference is that the sinks use the\n+[DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+operation instead of the\n+[ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+one.\n+\n+### Write Order\n+\n+Under normal circumstances, the Kinesis sink preserves the order of\n+items belonging to the same partition key. However, when the [flow\n+control](#flow-control) mechanism kicks in, the ordering might be lost\n+on occasion.\n+\n+This unfortunate fact originates in the way how KDS handles shard", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 380}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI2MDQ5Ng==", "bodyText": "I'd remove from \"This is very unfortunate\" till the end of the paragraph.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547260496", "createdAt": "2020-12-22T12:51:46Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.\n+\n+A **sequence number** is a unique identifier for each record. Sequence\n+numbers are assigned by KDS when a data producer publishes data into it.\n+Sequence numbers for the same partition key generally increase over\n+time.\n+\n+## APIs\n+\n+Amazon offers various choices of libraries that can be used to interact\n+with KDS:\n+\n+* **Kinesis Client Library (KCL)** and **Kinesis Producer Library\n+  (KPL)** are high-level libraries that are easy to use because they\n+  abstract away many concerns. They manage their threading policy, hide\n+  away the REST-based nature of Kinesis behind asynchronous constructs,\n+  balance load, handle failures, and react to resharding. However, all\n+  this convenience makes them unsuitable for building Jet connectors,\n+  where we need the most control possible to make choices that are\n+  suitable to Jet's architecture.\n+* **Amazon Kinesis Data Streams API** via **AWS SDK for Java** is the\n+  lower-level library that allows sufficient control when interacting\n+  with Kinesis. It consists of a simple set of [REST-based\n+  operations](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Operations.html).\n+  Every other concern mentioned above, when discussing the high-level\n+  libraries, has to be handled explicitly. This is the library used in\n+  the Jet source and sink implementations.\n+\n+## Quotas\n+\n+Amazon Kinesis Data Streams enforces quite a few quotas and limits,\n+which our sources and sinks need to comply with:\n+\n+* A single shard can ingest up to 1 MB of data per second (including\n+  partition keys) or 1,000 records per second for writes.\n+* The maximum size of the data payload of a record is 1 MB.\n+* The\n+  [GetRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html)\n+  operation can retrieve up to 10 MB of data per call from a single\n+  shard and up to 10,000 records per call.\n+* Each shard can support up to 5 GetRecords operations per second.\n+* Each shard can support up to a maximum total data read rate of 2 MB\n+  per second.\n+* The\n+  [ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+  operation has a limit of 100 transactions per second, per data stream.\n+  Each such transaction is able to return at most 100 shards, and if the\n+  stream has more, then multiple transactions need to be used for a full\n+  listing. (For details, see the [discovery](#discovery) section.)\n+* The\n+  [PutRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)\n+  operation can write at most 500 records into the stream. Each record\n+  in the request can be as large as 1MB, up to a limit of 5MB for the\n+  entire request, including partition keys. Each shard can support\n+  writes up to 1,000 records per second, up to a maximum data write\n+  total of 1 MiB per second.\n+* The\n+  [DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+  operation has a limit of 20 transactions per second per account. (For\n+  details, see the [adaptive throughput](#adaptive-throughput) section.)\n+\n+## Source\n+\n+The Kinesis source is a _streaming_, _distributed_, and _fault-tolerant_\n+data source for Jet. It supports both the _at-least-once_ and\n+_exactly-once_ [processing\n+guarantees](../architecture/fault-tolerance.md#processing-guarantee-is-a-shared-concern).\n+\n+### Distribution\n+\n+Being a distributed source, it has multiple instances running in each\n+Jet cluster member. Each instance is responsible for reading from zero,\n+one or more KDS [shards](#shard). The assignment of shards to individual\n+source instances is based on partitioning the space of Kinesis [record\n+keys](#partition-key).\n+\n+Record keys, or partition keys as Kinesis calls them, are Unicode\n+strings, with a maximum length limit of 256 characters. The stream uses\n+the MD5 hash function to map these strings to 128-bit integer values.\n+The universe of these values is thus the [0 .. 2^128) range. Each\n+Kinesis shard has a continuous chunk of this range assigned to it,\n+called the shard's hash range. The stream assigns a record to a shard if\n+the record's partition key hashes into the shard's range.\n+\n+In the Jet Kinesis source, we use similar logic for assigning shards to\n+source instances. Each of our sources gets a part of the hash range\n+assigned to it. We say that a source owns a specific shard if and only\n+if the shard's hash range's starting point is inside the source's hash\n+range. Any similar range matching logic would work, as long as it's\n+non-ambiguous.\n+\n+### Discovery\n+\n+Reading records from shards assigned to them is only part of the\n+responsibility of sources. Sources also need a way to discover currently\n+active shards in the stream to take responsibility for them. Moreover,\n+this discovery process can't just happen once, on start-up, because\n+streams are dynamic, shards can be closed, and new shards can pop-up at\n+any time. For details, see the [resharding section](#resharding).\n+\n+Continuously monitoring the set of active shards in the stream is the\n+responsibility of **one** of the local source instances in each Jet\n+cluster member. This is an optimization. If all sources would run the\n+discovery, they would still obtain the same data, just with a multiplied\n+effort and cost. Monitoring means continuously polling the stream for\n+the list of all shards in it.\n+\n+Monitoring needs to take care not to cross the rate limit imposed by\n+Kinesis on this operation. For details, see the [quotas\n+section](#quotas).\n+\n+### Resharding\n+\n+Kinesis supports resharding, which lets you adjust the number of shards\n+in your stream to adapt to changes in data flow rate through the stream.\n+(Amazon charges on a per-shard basis, that's why it's desirable to have\n+the smallest amount of shards possible.)\n+\n+There are two types of resharding operations: shard **split** and shard\n+**merge**. In a shard split, you divide a single shard into two adjacent\n+shards. In a shard merge, you combine two adjacent shards into a single\n+shard. By \"adjacent\", we mean that one's hash range starts where the\n+other one's ends.\n+\n+Splitting increases the number of shards in your stream and therefore\n+increases the data capacity (and cost) of the stream. Similarly, merging\n+reduces the number of shards in your stream and therefore decreases the\n+data capacity (and cost).\n+\n+Resharding is always pairwise in the sense that you cannot split into\n+more than two shards in a single operation, and you cannot merge more\n+than two shards in a single operation. The shard or pair of shards that\n+the resharding operation acts on are called parent shards. The shard or\n+pair of shards that result from the resharding operation are called\n+child shards.\n+\n+When child shards, resulting from a split or merge, activate, their\n+parents get deactivated and will no longer get data inserted into them.\n+From that point onward, data goes into the children.\n+\n+### Read Order\n+\n+Resharding does not suspend the stream's dataflow, while it's going on.\n+Data continues to be ingested into the stream, and at some point, it\n+just stops being put into the parent shards and starts being put into\n+the child shards.\n+\n+The Kinesis Jet source would need to make sure that it finishes reading\n+from parents before reading from their children. However, this is not\n+possible since the children might end up being owned by an entirely\n+different instance of the source than their parents (for example, in a\n+split), possibly located in an entirely different Jet cluster member.\n+\n+Moreover, it's not enough to finish reading from the parent before\n+reading from the children. Even if that were achieved, data from parents\n+might overtake data from children further down the Jet pipeline, in its\n+various parallel components. A Kinesis source would need to make sure\n+that it has read all data from the parents and that data has fully\n+passed through the Jet pipeline before starting to read from the\n+children. Only then could it provide the same ordering as KDS while\n+resharding.\n+\n+This is currently not possible in Jet. Hopefully, future versions will\n+address the problem. Users of the Kinesis source need to be aware that\n+some data reordering might occur on resharding.\n+\n+### Fault Tolerance\n+\n+The Kinesis Jet source supports pipelines with both at-least-once and\n+exactly-once processing guarantees. It achieves this by saving KDS\n+offsets into its snapshots and starting data read from saved offsets\n+when restarted.\n+\n+The offsets are saved on a per-shard basis, and on restart, each source\n+instance receives all saved offsets for all shards, so it can function\n+properly regardless of how shards are assigned to sources after the\n+restart.\n+\n+### Watermarks\n+\n+The Kinesis source can provide native watermarks because the [record\n+data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Record.html)\n+has a field that can be turned towards this purpose\n+(`ApproximateArrivalTimestamp`). However, it should be pointed out that\n+these watermarks are \"native\" only from Jet's point of view. They are\n+KDS ingestion times, i.e., whenever a KDS producer managed to push said\n+record into the data stream. We have no way of knowing what's the real\n+event time of a record.\n+\n+Watermarks are also saved to and recovered from snapshots.\n+\n+### Metrics\n+\n+When receiving record batches, the [data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html#API_GetRecords_ResponseSyntax)\n+contains a field called `MillisBehindLatest` defined as following:\n+\n+> The number of milliseconds the GetRecords response is from the\n+> stream's tip, indicating how far behind the current time the consumer\n+> is. A value of zero indicates that record processing caught up, and\n+> there are no new records to process at this moment.\n+\n+This value can be useful for monitoring so the sources can publish it\n+as a per processor instance metric.\n+\n+### Code Example\n+\n+A typical example of setting up a Kinesis source in Jet would look like\n+this:\n+\n+```java\n+KinesisSources.kinesis(\"myStream\")\n+  .withRegion(\"us-east-1\")\n+  .withEndpoint(\"http://localhost:12345\")\n+  .withCredentials(\"accesskey\", \"secretkey\")\n+  .withRetryStrategy(RetryStrategies.indefinitely(250))\n+  .build();\n+```\n+\n+The only mandatory property is the Kinesis `stream name`. The others are\n+optional and can be specified via a fluent builder.\n+\n+If `region` is not specified, then _us-east-1_ will be used by default.\n+\n+If `endpoint` is not specified, then the region's default endpoint will\n+be used.\n+\n+If `credentials` aren't specified, then the [Default Credential Provider\n+Chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html#credentials-default)\n+will be followed.\n+\n+If `retry strategy` is not specified, then a default of our definition\n+will be used (maximum 10 retries, with exponential backoff limited to\n+a maximum of 3 seconds). A source's retry strategy applies to failures\n+of reading records from or listing shards of a stream.\n+\n+The actual source created will be of type\n+`StreamSource<Map.Entry<String, byte[]>>`, so basically a stream of\n+partition key - record data blob pairs.\n+\n+## Sink\n+\n+The Kinesis sink is a _distributed_, and _fault-tolerant_ data sink for\n+Jet. It supports both _streaming_ and _batching_ pipelines. The\n+fault-tolerance guarantee it can offer is only _at-least-once_ since\n+Kinesis does not offer transaction support.\n+\n+### Distribution\n+\n+Being a distributed sink, it has multiple instances running in each Jet\n+cluster member. When used in a pipeline, this sink forces its incoming\n+edges to be _distributed_ and _partitioned_. The partition keys used by\n+the edges are the same as the Kinesis [partition keys](#partition-key).\n+This ensures that all data with the same partition key will end up in\n+the same global sink instance and the same shard.\n+\n+### Flow Control\n+\n+Writing data into a Kinesis Data Stream is governed by multiple\n+limitations:\n+\n+* no more than 500 records can be written in one batch\n+* each record must contain no more than 1M of data\n+* each batch must contain no more than 5M of data\n+* each shard can ingest no more than 1,000 records per second\n+\n+While most of these limitations are simple to enforce, the shard\n+ingestion rate is not. Different partition keys get assigned to a shard\n+based on a hashing function, so partition keys going into the same shard\n+can be written by different sink instances. Currently, Jet has no\n+capability for computing and coordinating such a per shard rate among\n+all it's distributed sink instances.\n+\n+The sink takes a different approach to comply with this limitation. It\n+allows for the rate to be tripped (i.e., it doesn't attempt to prevent\n+it from happening), but once it gets tripped, sinks try to slow down the\n+amount of data they write to keep the rate violation as an occasional,\n+rare event and not a continuous storm.\n+\n+The source achieves this flow control in two ways:\n+\n+* by decreasing the send batch size; the default is the maximum of 500,\n+  which it will reduce, if necessary, as low as 10 records/batch\n+* by adding a delay between two subsequent send actions (which can be as\n+  little as 100ms, a reasonable value in case of Kinesis and as much as\n+  10 seconds, which is a lot, but would occur only in an unreasonably\n+  sized stream, as far as shard count is concerned - ultimately the\n+  owner of the stream is responsible for setting up enough shards to be\n+  able to handle his data rates)\n+\n+The flow control process is _adaptive_ in the sense that:\n+\n+* it kicks in only when batches start failing due to shard ingestion\n+  rates being tripped\n+* until failures repeat, it keeps quickly increasing the sleep delays\n+  to stop them from happening\n+* once failures stop, it slowly decreases the sleep delays until they\n+  are eliminated (i.e., the data volume spike was only temporary) or\n+  until failures start happening again\n+\n+Under normal circumstances, if there are enough shards in the stream and\n+their data ingestion rate covers the data flow, this whole flow control\n+process stays shut off. The sink publishes data with the lowest possible\n+latency.\n+\n+### Discovery\n+\n+As we've seen in the [flow control](#flow-control) section, one element\n+used to control the throughput is batch size. Under normal conditions,\n+the sink uses the default/maximum batch size of 500. When flow control\n+kicks a new batch size is picked as a function of the number of open\n+shards in the stream.\n+\n+For this to happen, the sinks need to have relatively up-to-date\n+information about the count of open shards. The sink achieves this by\n+using a mechanism very similar to the [discovery process employed by the\n+source](#discovery). The only real difference is that the sinks use the\n+[DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+operation instead of the\n+[ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+one.\n+\n+### Write Order\n+\n+Under normal circumstances, the Kinesis sink preserves the order of\n+items belonging to the same partition key. However, when the [flow\n+control](#flow-control) mechanism kicks in, the ordering might be lost\n+on occasion.\n+\n+This unfortunate fact originates in the way how KDS handles shard\n+ingestion rate violations. When KDS receives a batch to be ingested, it\n+processes each item in it one-by-one, and if some fail, it doesn't stop\n+processing the batch. The result is that some items from a batch get\n+rejected, some get ingested, but in a random manner. The sink does\n+resend the un-ingested item, they won't get lost, but there is nothing\n+it can do to preserve the initial ordering. This is very unfortunate", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 386}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI2MDg4MQ==", "bodyText": "This is not clear to me.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547260881", "createdAt": "2020-12-22T12:52:42Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.\n+\n+A **sequence number** is a unique identifier for each record. Sequence\n+numbers are assigned by KDS when a data producer publishes data into it.\n+Sequence numbers for the same partition key generally increase over\n+time.\n+\n+## APIs\n+\n+Amazon offers various choices of libraries that can be used to interact\n+with KDS:\n+\n+* **Kinesis Client Library (KCL)** and **Kinesis Producer Library\n+  (KPL)** are high-level libraries that are easy to use because they\n+  abstract away many concerns. They manage their threading policy, hide\n+  away the REST-based nature of Kinesis behind asynchronous constructs,\n+  balance load, handle failures, and react to resharding. However, all\n+  this convenience makes them unsuitable for building Jet connectors,\n+  where we need the most control possible to make choices that are\n+  suitable to Jet's architecture.\n+* **Amazon Kinesis Data Streams API** via **AWS SDK for Java** is the\n+  lower-level library that allows sufficient control when interacting\n+  with Kinesis. It consists of a simple set of [REST-based\n+  operations](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Operations.html).\n+  Every other concern mentioned above, when discussing the high-level\n+  libraries, has to be handled explicitly. This is the library used in\n+  the Jet source and sink implementations.\n+\n+## Quotas\n+\n+Amazon Kinesis Data Streams enforces quite a few quotas and limits,\n+which our sources and sinks need to comply with:\n+\n+* A single shard can ingest up to 1 MB of data per second (including\n+  partition keys) or 1,000 records per second for writes.\n+* The maximum size of the data payload of a record is 1 MB.\n+* The\n+  [GetRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html)\n+  operation can retrieve up to 10 MB of data per call from a single\n+  shard and up to 10,000 records per call.\n+* Each shard can support up to 5 GetRecords operations per second.\n+* Each shard can support up to a maximum total data read rate of 2 MB\n+  per second.\n+* The\n+  [ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+  operation has a limit of 100 transactions per second, per data stream.\n+  Each such transaction is able to return at most 100 shards, and if the\n+  stream has more, then multiple transactions need to be used for a full\n+  listing. (For details, see the [discovery](#discovery) section.)\n+* The\n+  [PutRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)\n+  operation can write at most 500 records into the stream. Each record\n+  in the request can be as large as 1MB, up to a limit of 5MB for the\n+  entire request, including partition keys. Each shard can support\n+  writes up to 1,000 records per second, up to a maximum data write\n+  total of 1 MiB per second.\n+* The\n+  [DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+  operation has a limit of 20 transactions per second per account. (For\n+  details, see the [adaptive throughput](#adaptive-throughput) section.)\n+\n+## Source\n+\n+The Kinesis source is a _streaming_, _distributed_, and _fault-tolerant_\n+data source for Jet. It supports both the _at-least-once_ and\n+_exactly-once_ [processing\n+guarantees](../architecture/fault-tolerance.md#processing-guarantee-is-a-shared-concern).\n+\n+### Distribution\n+\n+Being a distributed source, it has multiple instances running in each\n+Jet cluster member. Each instance is responsible for reading from zero,\n+one or more KDS [shards](#shard). The assignment of shards to individual\n+source instances is based on partitioning the space of Kinesis [record\n+keys](#partition-key).\n+\n+Record keys, or partition keys as Kinesis calls them, are Unicode\n+strings, with a maximum length limit of 256 characters. The stream uses\n+the MD5 hash function to map these strings to 128-bit integer values.\n+The universe of these values is thus the [0 .. 2^128) range. Each\n+Kinesis shard has a continuous chunk of this range assigned to it,\n+called the shard's hash range. The stream assigns a record to a shard if\n+the record's partition key hashes into the shard's range.\n+\n+In the Jet Kinesis source, we use similar logic for assigning shards to\n+source instances. Each of our sources gets a part of the hash range\n+assigned to it. We say that a source owns a specific shard if and only\n+if the shard's hash range's starting point is inside the source's hash\n+range. Any similar range matching logic would work, as long as it's\n+non-ambiguous.\n+\n+### Discovery\n+\n+Reading records from shards assigned to them is only part of the\n+responsibility of sources. Sources also need a way to discover currently\n+active shards in the stream to take responsibility for them. Moreover,\n+this discovery process can't just happen once, on start-up, because\n+streams are dynamic, shards can be closed, and new shards can pop-up at\n+any time. For details, see the [resharding section](#resharding).\n+\n+Continuously monitoring the set of active shards in the stream is the\n+responsibility of **one** of the local source instances in each Jet\n+cluster member. This is an optimization. If all sources would run the\n+discovery, they would still obtain the same data, just with a multiplied\n+effort and cost. Monitoring means continuously polling the stream for\n+the list of all shards in it.\n+\n+Monitoring needs to take care not to cross the rate limit imposed by\n+Kinesis on this operation. For details, see the [quotas\n+section](#quotas).\n+\n+### Resharding\n+\n+Kinesis supports resharding, which lets you adjust the number of shards\n+in your stream to adapt to changes in data flow rate through the stream.\n+(Amazon charges on a per-shard basis, that's why it's desirable to have\n+the smallest amount of shards possible.)\n+\n+There are two types of resharding operations: shard **split** and shard\n+**merge**. In a shard split, you divide a single shard into two adjacent\n+shards. In a shard merge, you combine two adjacent shards into a single\n+shard. By \"adjacent\", we mean that one's hash range starts where the\n+other one's ends.\n+\n+Splitting increases the number of shards in your stream and therefore\n+increases the data capacity (and cost) of the stream. Similarly, merging\n+reduces the number of shards in your stream and therefore decreases the\n+data capacity (and cost).\n+\n+Resharding is always pairwise in the sense that you cannot split into\n+more than two shards in a single operation, and you cannot merge more\n+than two shards in a single operation. The shard or pair of shards that\n+the resharding operation acts on are called parent shards. The shard or\n+pair of shards that result from the resharding operation are called\n+child shards.\n+\n+When child shards, resulting from a split or merge, activate, their\n+parents get deactivated and will no longer get data inserted into them.\n+From that point onward, data goes into the children.\n+\n+### Read Order\n+\n+Resharding does not suspend the stream's dataflow, while it's going on.\n+Data continues to be ingested into the stream, and at some point, it\n+just stops being put into the parent shards and starts being put into\n+the child shards.\n+\n+The Kinesis Jet source would need to make sure that it finishes reading\n+from parents before reading from their children. However, this is not\n+possible since the children might end up being owned by an entirely\n+different instance of the source than their parents (for example, in a\n+split), possibly located in an entirely different Jet cluster member.\n+\n+Moreover, it's not enough to finish reading from the parent before\n+reading from the children. Even if that were achieved, data from parents\n+might overtake data from children further down the Jet pipeline, in its\n+various parallel components. A Kinesis source would need to make sure\n+that it has read all data from the parents and that data has fully\n+passed through the Jet pipeline before starting to read from the\n+children. Only then could it provide the same ordering as KDS while\n+resharding.\n+\n+This is currently not possible in Jet. Hopefully, future versions will\n+address the problem. Users of the Kinesis source need to be aware that\n+some data reordering might occur on resharding.\n+\n+### Fault Tolerance\n+\n+The Kinesis Jet source supports pipelines with both at-least-once and\n+exactly-once processing guarantees. It achieves this by saving KDS\n+offsets into its snapshots and starting data read from saved offsets\n+when restarted.\n+\n+The offsets are saved on a per-shard basis, and on restart, each source\n+instance receives all saved offsets for all shards, so it can function\n+properly regardless of how shards are assigned to sources after the\n+restart.\n+\n+### Watermarks\n+\n+The Kinesis source can provide native watermarks because the [record\n+data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Record.html)\n+has a field that can be turned towards this purpose\n+(`ApproximateArrivalTimestamp`). However, it should be pointed out that\n+these watermarks are \"native\" only from Jet's point of view. They are\n+KDS ingestion times, i.e., whenever a KDS producer managed to push said\n+record into the data stream. We have no way of knowing what's the real\n+event time of a record.\n+\n+Watermarks are also saved to and recovered from snapshots.\n+\n+### Metrics\n+\n+When receiving record batches, the [data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html#API_GetRecords_ResponseSyntax)\n+contains a field called `MillisBehindLatest` defined as following:\n+\n+> The number of milliseconds the GetRecords response is from the\n+> stream's tip, indicating how far behind the current time the consumer\n+> is. A value of zero indicates that record processing caught up, and\n+> there are no new records to process at this moment.\n+\n+This value can be useful for monitoring so the sources can publish it\n+as a per processor instance metric.\n+\n+### Code Example\n+\n+A typical example of setting up a Kinesis source in Jet would look like\n+this:\n+\n+```java\n+KinesisSources.kinesis(\"myStream\")\n+  .withRegion(\"us-east-1\")\n+  .withEndpoint(\"http://localhost:12345\")\n+  .withCredentials(\"accesskey\", \"secretkey\")\n+  .withRetryStrategy(RetryStrategies.indefinitely(250))\n+  .build();\n+```\n+\n+The only mandatory property is the Kinesis `stream name`. The others are\n+optional and can be specified via a fluent builder.\n+\n+If `region` is not specified, then _us-east-1_ will be used by default.\n+\n+If `endpoint` is not specified, then the region's default endpoint will\n+be used.\n+\n+If `credentials` aren't specified, then the [Default Credential Provider\n+Chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html#credentials-default)\n+will be followed.\n+\n+If `retry strategy` is not specified, then a default of our definition\n+will be used (maximum 10 retries, with exponential backoff limited to\n+a maximum of 3 seconds). A source's retry strategy applies to failures\n+of reading records from or listing shards of a stream.\n+\n+The actual source created will be of type\n+`StreamSource<Map.Entry<String, byte[]>>`, so basically a stream of\n+partition key - record data blob pairs.\n+\n+## Sink\n+\n+The Kinesis sink is a _distributed_, and _fault-tolerant_ data sink for\n+Jet. It supports both _streaming_ and _batching_ pipelines. The\n+fault-tolerance guarantee it can offer is only _at-least-once_ since\n+Kinesis does not offer transaction support.\n+\n+### Distribution\n+\n+Being a distributed sink, it has multiple instances running in each Jet\n+cluster member. When used in a pipeline, this sink forces its incoming\n+edges to be _distributed_ and _partitioned_. The partition keys used by\n+the edges are the same as the Kinesis [partition keys](#partition-key).\n+This ensures that all data with the same partition key will end up in\n+the same global sink instance and the same shard.\n+\n+### Flow Control\n+\n+Writing data into a Kinesis Data Stream is governed by multiple\n+limitations:\n+\n+* no more than 500 records can be written in one batch\n+* each record must contain no more than 1M of data\n+* each batch must contain no more than 5M of data\n+* each shard can ingest no more than 1,000 records per second\n+\n+While most of these limitations are simple to enforce, the shard\n+ingestion rate is not. Different partition keys get assigned to a shard\n+based on a hashing function, so partition keys going into the same shard\n+can be written by different sink instances. Currently, Jet has no\n+capability for computing and coordinating such a per shard rate among\n+all it's distributed sink instances.\n+\n+The sink takes a different approach to comply with this limitation. It\n+allows for the rate to be tripped (i.e., it doesn't attempt to prevent\n+it from happening), but once it gets tripped, sinks try to slow down the\n+amount of data they write to keep the rate violation as an occasional,\n+rare event and not a continuous storm.\n+\n+The source achieves this flow control in two ways:\n+\n+* by decreasing the send batch size; the default is the maximum of 500,\n+  which it will reduce, if necessary, as low as 10 records/batch\n+* by adding a delay between two subsequent send actions (which can be as\n+  little as 100ms, a reasonable value in case of Kinesis and as much as\n+  10 seconds, which is a lot, but would occur only in an unreasonably\n+  sized stream, as far as shard count is concerned - ultimately the\n+  owner of the stream is responsible for setting up enough shards to be\n+  able to handle his data rates)\n+\n+The flow control process is _adaptive_ in the sense that:\n+\n+* it kicks in only when batches start failing due to shard ingestion\n+  rates being tripped\n+* until failures repeat, it keeps quickly increasing the sleep delays\n+  to stop them from happening\n+* once failures stop, it slowly decreases the sleep delays until they\n+  are eliminated (i.e., the data volume spike was only temporary) or\n+  until failures start happening again\n+\n+Under normal circumstances, if there are enough shards in the stream and\n+their data ingestion rate covers the data flow, this whole flow control\n+process stays shut off. The sink publishes data with the lowest possible\n+latency.\n+\n+### Discovery\n+\n+As we've seen in the [flow control](#flow-control) section, one element\n+used to control the throughput is batch size. Under normal conditions,\n+the sink uses the default/maximum batch size of 500. When flow control\n+kicks a new batch size is picked as a function of the number of open\n+shards in the stream.\n+\n+For this to happen, the sinks need to have relatively up-to-date\n+information about the count of open shards. The sink achieves this by\n+using a mechanism very similar to the [discovery process employed by the\n+source](#discovery). The only real difference is that the sinks use the\n+[DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+operation instead of the\n+[ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+one.\n+\n+### Write Order\n+\n+Under normal circumstances, the Kinesis sink preserves the order of\n+items belonging to the same partition key. However, when the [flow\n+control](#flow-control) mechanism kicks in, the ordering might be lost\n+on occasion.\n+\n+This unfortunate fact originates in the way how KDS handles shard\n+ingestion rate violations. When KDS receives a batch to be ingested, it\n+processes each item in it one-by-one, and if some fail, it doesn't stop\n+processing the batch. The result is that some items from a batch get\n+rejected, some get ingested, but in a random manner. The sink does\n+resend the un-ingested item, they won't get lost, but there is nothing\n+it can do to preserve the initial ordering. This is very unfortunate\n+since all that would be needed for avoiding this problem would be for\n+Kinesis to reject all remaining items from the batch once one of them\n+trips the ingestion rate limit. The designers probably made the choice\n+they did to save on data traffic.\n+\n+Nothing that we can do about it at this point needs to be documented,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 392}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI2MTA0MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Since there is no transaction support in Kinesis the sink can't support\n          \n          \n            \n            Since there is no transaction support in Kinesis, the sink can't support", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547261041", "createdAt": "2020-12-22T12:53:04Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.\n+\n+A **sequence number** is a unique identifier for each record. Sequence\n+numbers are assigned by KDS when a data producer publishes data into it.\n+Sequence numbers for the same partition key generally increase over\n+time.\n+\n+## APIs\n+\n+Amazon offers various choices of libraries that can be used to interact\n+with KDS:\n+\n+* **Kinesis Client Library (KCL)** and **Kinesis Producer Library\n+  (KPL)** are high-level libraries that are easy to use because they\n+  abstract away many concerns. They manage their threading policy, hide\n+  away the REST-based nature of Kinesis behind asynchronous constructs,\n+  balance load, handle failures, and react to resharding. However, all\n+  this convenience makes them unsuitable for building Jet connectors,\n+  where we need the most control possible to make choices that are\n+  suitable to Jet's architecture.\n+* **Amazon Kinesis Data Streams API** via **AWS SDK for Java** is the\n+  lower-level library that allows sufficient control when interacting\n+  with Kinesis. It consists of a simple set of [REST-based\n+  operations](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Operations.html).\n+  Every other concern mentioned above, when discussing the high-level\n+  libraries, has to be handled explicitly. This is the library used in\n+  the Jet source and sink implementations.\n+\n+## Quotas\n+\n+Amazon Kinesis Data Streams enforces quite a few quotas and limits,\n+which our sources and sinks need to comply with:\n+\n+* A single shard can ingest up to 1 MB of data per second (including\n+  partition keys) or 1,000 records per second for writes.\n+* The maximum size of the data payload of a record is 1 MB.\n+* The\n+  [GetRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html)\n+  operation can retrieve up to 10 MB of data per call from a single\n+  shard and up to 10,000 records per call.\n+* Each shard can support up to 5 GetRecords operations per second.\n+* Each shard can support up to a maximum total data read rate of 2 MB\n+  per second.\n+* The\n+  [ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+  operation has a limit of 100 transactions per second, per data stream.\n+  Each such transaction is able to return at most 100 shards, and if the\n+  stream has more, then multiple transactions need to be used for a full\n+  listing. (For details, see the [discovery](#discovery) section.)\n+* The\n+  [PutRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)\n+  operation can write at most 500 records into the stream. Each record\n+  in the request can be as large as 1MB, up to a limit of 5MB for the\n+  entire request, including partition keys. Each shard can support\n+  writes up to 1,000 records per second, up to a maximum data write\n+  total of 1 MiB per second.\n+* The\n+  [DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+  operation has a limit of 20 transactions per second per account. (For\n+  details, see the [adaptive throughput](#adaptive-throughput) section.)\n+\n+## Source\n+\n+The Kinesis source is a _streaming_, _distributed_, and _fault-tolerant_\n+data source for Jet. It supports both the _at-least-once_ and\n+_exactly-once_ [processing\n+guarantees](../architecture/fault-tolerance.md#processing-guarantee-is-a-shared-concern).\n+\n+### Distribution\n+\n+Being a distributed source, it has multiple instances running in each\n+Jet cluster member. Each instance is responsible for reading from zero,\n+one or more KDS [shards](#shard). The assignment of shards to individual\n+source instances is based on partitioning the space of Kinesis [record\n+keys](#partition-key).\n+\n+Record keys, or partition keys as Kinesis calls them, are Unicode\n+strings, with a maximum length limit of 256 characters. The stream uses\n+the MD5 hash function to map these strings to 128-bit integer values.\n+The universe of these values is thus the [0 .. 2^128) range. Each\n+Kinesis shard has a continuous chunk of this range assigned to it,\n+called the shard's hash range. The stream assigns a record to a shard if\n+the record's partition key hashes into the shard's range.\n+\n+In the Jet Kinesis source, we use similar logic for assigning shards to\n+source instances. Each of our sources gets a part of the hash range\n+assigned to it. We say that a source owns a specific shard if and only\n+if the shard's hash range's starting point is inside the source's hash\n+range. Any similar range matching logic would work, as long as it's\n+non-ambiguous.\n+\n+### Discovery\n+\n+Reading records from shards assigned to them is only part of the\n+responsibility of sources. Sources also need a way to discover currently\n+active shards in the stream to take responsibility for them. Moreover,\n+this discovery process can't just happen once, on start-up, because\n+streams are dynamic, shards can be closed, and new shards can pop-up at\n+any time. For details, see the [resharding section](#resharding).\n+\n+Continuously monitoring the set of active shards in the stream is the\n+responsibility of **one** of the local source instances in each Jet\n+cluster member. This is an optimization. If all sources would run the\n+discovery, they would still obtain the same data, just with a multiplied\n+effort and cost. Monitoring means continuously polling the stream for\n+the list of all shards in it.\n+\n+Monitoring needs to take care not to cross the rate limit imposed by\n+Kinesis on this operation. For details, see the [quotas\n+section](#quotas).\n+\n+### Resharding\n+\n+Kinesis supports resharding, which lets you adjust the number of shards\n+in your stream to adapt to changes in data flow rate through the stream.\n+(Amazon charges on a per-shard basis, that's why it's desirable to have\n+the smallest amount of shards possible.)\n+\n+There are two types of resharding operations: shard **split** and shard\n+**merge**. In a shard split, you divide a single shard into two adjacent\n+shards. In a shard merge, you combine two adjacent shards into a single\n+shard. By \"adjacent\", we mean that one's hash range starts where the\n+other one's ends.\n+\n+Splitting increases the number of shards in your stream and therefore\n+increases the data capacity (and cost) of the stream. Similarly, merging\n+reduces the number of shards in your stream and therefore decreases the\n+data capacity (and cost).\n+\n+Resharding is always pairwise in the sense that you cannot split into\n+more than two shards in a single operation, and you cannot merge more\n+than two shards in a single operation. The shard or pair of shards that\n+the resharding operation acts on are called parent shards. The shard or\n+pair of shards that result from the resharding operation are called\n+child shards.\n+\n+When child shards, resulting from a split or merge, activate, their\n+parents get deactivated and will no longer get data inserted into them.\n+From that point onward, data goes into the children.\n+\n+### Read Order\n+\n+Resharding does not suspend the stream's dataflow, while it's going on.\n+Data continues to be ingested into the stream, and at some point, it\n+just stops being put into the parent shards and starts being put into\n+the child shards.\n+\n+The Kinesis Jet source would need to make sure that it finishes reading\n+from parents before reading from their children. However, this is not\n+possible since the children might end up being owned by an entirely\n+different instance of the source than their parents (for example, in a\n+split), possibly located in an entirely different Jet cluster member.\n+\n+Moreover, it's not enough to finish reading from the parent before\n+reading from the children. Even if that were achieved, data from parents\n+might overtake data from children further down the Jet pipeline, in its\n+various parallel components. A Kinesis source would need to make sure\n+that it has read all data from the parents and that data has fully\n+passed through the Jet pipeline before starting to read from the\n+children. Only then could it provide the same ordering as KDS while\n+resharding.\n+\n+This is currently not possible in Jet. Hopefully, future versions will\n+address the problem. Users of the Kinesis source need to be aware that\n+some data reordering might occur on resharding.\n+\n+### Fault Tolerance\n+\n+The Kinesis Jet source supports pipelines with both at-least-once and\n+exactly-once processing guarantees. It achieves this by saving KDS\n+offsets into its snapshots and starting data read from saved offsets\n+when restarted.\n+\n+The offsets are saved on a per-shard basis, and on restart, each source\n+instance receives all saved offsets for all shards, so it can function\n+properly regardless of how shards are assigned to sources after the\n+restart.\n+\n+### Watermarks\n+\n+The Kinesis source can provide native watermarks because the [record\n+data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Record.html)\n+has a field that can be turned towards this purpose\n+(`ApproximateArrivalTimestamp`). However, it should be pointed out that\n+these watermarks are \"native\" only from Jet's point of view. They are\n+KDS ingestion times, i.e., whenever a KDS producer managed to push said\n+record into the data stream. We have no way of knowing what's the real\n+event time of a record.\n+\n+Watermarks are also saved to and recovered from snapshots.\n+\n+### Metrics\n+\n+When receiving record batches, the [data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html#API_GetRecords_ResponseSyntax)\n+contains a field called `MillisBehindLatest` defined as following:\n+\n+> The number of milliseconds the GetRecords response is from the\n+> stream's tip, indicating how far behind the current time the consumer\n+> is. A value of zero indicates that record processing caught up, and\n+> there are no new records to process at this moment.\n+\n+This value can be useful for monitoring so the sources can publish it\n+as a per processor instance metric.\n+\n+### Code Example\n+\n+A typical example of setting up a Kinesis source in Jet would look like\n+this:\n+\n+```java\n+KinesisSources.kinesis(\"myStream\")\n+  .withRegion(\"us-east-1\")\n+  .withEndpoint(\"http://localhost:12345\")\n+  .withCredentials(\"accesskey\", \"secretkey\")\n+  .withRetryStrategy(RetryStrategies.indefinitely(250))\n+  .build();\n+```\n+\n+The only mandatory property is the Kinesis `stream name`. The others are\n+optional and can be specified via a fluent builder.\n+\n+If `region` is not specified, then _us-east-1_ will be used by default.\n+\n+If `endpoint` is not specified, then the region's default endpoint will\n+be used.\n+\n+If `credentials` aren't specified, then the [Default Credential Provider\n+Chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html#credentials-default)\n+will be followed.\n+\n+If `retry strategy` is not specified, then a default of our definition\n+will be used (maximum 10 retries, with exponential backoff limited to\n+a maximum of 3 seconds). A source's retry strategy applies to failures\n+of reading records from or listing shards of a stream.\n+\n+The actual source created will be of type\n+`StreamSource<Map.Entry<String, byte[]>>`, so basically a stream of\n+partition key - record data blob pairs.\n+\n+## Sink\n+\n+The Kinesis sink is a _distributed_, and _fault-tolerant_ data sink for\n+Jet. It supports both _streaming_ and _batching_ pipelines. The\n+fault-tolerance guarantee it can offer is only _at-least-once_ since\n+Kinesis does not offer transaction support.\n+\n+### Distribution\n+\n+Being a distributed sink, it has multiple instances running in each Jet\n+cluster member. When used in a pipeline, this sink forces its incoming\n+edges to be _distributed_ and _partitioned_. The partition keys used by\n+the edges are the same as the Kinesis [partition keys](#partition-key).\n+This ensures that all data with the same partition key will end up in\n+the same global sink instance and the same shard.\n+\n+### Flow Control\n+\n+Writing data into a Kinesis Data Stream is governed by multiple\n+limitations:\n+\n+* no more than 500 records can be written in one batch\n+* each record must contain no more than 1M of data\n+* each batch must contain no more than 5M of data\n+* each shard can ingest no more than 1,000 records per second\n+\n+While most of these limitations are simple to enforce, the shard\n+ingestion rate is not. Different partition keys get assigned to a shard\n+based on a hashing function, so partition keys going into the same shard\n+can be written by different sink instances. Currently, Jet has no\n+capability for computing and coordinating such a per shard rate among\n+all it's distributed sink instances.\n+\n+The sink takes a different approach to comply with this limitation. It\n+allows for the rate to be tripped (i.e., it doesn't attempt to prevent\n+it from happening), but once it gets tripped, sinks try to slow down the\n+amount of data they write to keep the rate violation as an occasional,\n+rare event and not a continuous storm.\n+\n+The source achieves this flow control in two ways:\n+\n+* by decreasing the send batch size; the default is the maximum of 500,\n+  which it will reduce, if necessary, as low as 10 records/batch\n+* by adding a delay between two subsequent send actions (which can be as\n+  little as 100ms, a reasonable value in case of Kinesis and as much as\n+  10 seconds, which is a lot, but would occur only in an unreasonably\n+  sized stream, as far as shard count is concerned - ultimately the\n+  owner of the stream is responsible for setting up enough shards to be\n+  able to handle his data rates)\n+\n+The flow control process is _adaptive_ in the sense that:\n+\n+* it kicks in only when batches start failing due to shard ingestion\n+  rates being tripped\n+* until failures repeat, it keeps quickly increasing the sleep delays\n+  to stop them from happening\n+* once failures stop, it slowly decreases the sleep delays until they\n+  are eliminated (i.e., the data volume spike was only temporary) or\n+  until failures start happening again\n+\n+Under normal circumstances, if there are enough shards in the stream and\n+their data ingestion rate covers the data flow, this whole flow control\n+process stays shut off. The sink publishes data with the lowest possible\n+latency.\n+\n+### Discovery\n+\n+As we've seen in the [flow control](#flow-control) section, one element\n+used to control the throughput is batch size. Under normal conditions,\n+the sink uses the default/maximum batch size of 500. When flow control\n+kicks a new batch size is picked as a function of the number of open\n+shards in the stream.\n+\n+For this to happen, the sinks need to have relatively up-to-date\n+information about the count of open shards. The sink achieves this by\n+using a mechanism very similar to the [discovery process employed by the\n+source](#discovery). The only real difference is that the sinks use the\n+[DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+operation instead of the\n+[ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+one.\n+\n+### Write Order\n+\n+Under normal circumstances, the Kinesis sink preserves the order of\n+items belonging to the same partition key. However, when the [flow\n+control](#flow-control) mechanism kicks in, the ordering might be lost\n+on occasion.\n+\n+This unfortunate fact originates in the way how KDS handles shard\n+ingestion rate violations. When KDS receives a batch to be ingested, it\n+processes each item in it one-by-one, and if some fail, it doesn't stop\n+processing the batch. The result is that some items from a batch get\n+rejected, some get ingested, but in a random manner. The sink does\n+resend the un-ingested item, they won't get lost, but there is nothing\n+it can do to preserve the initial ordering. This is very unfortunate\n+since all that would be needed for avoiding this problem would be for\n+Kinesis to reject all remaining items from the batch once one of them\n+trips the ingestion rate limit. The designers probably made the choice\n+they did to save on data traffic.\n+\n+Nothing that we can do about it at this point needs to be documented,\n+and users of the sink need to be aware.\n+\n+### Fault Tolerance\n+\n+Since there is no transaction support in Kinesis the sink can't support", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 397}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzI2MTk4NQ==", "bodyText": "I don't think this is true, exactly-once processing means that a stateful transform won't account for one event twice, therefore destroying the correctness of its stateful computation. Exactly-once delivery is what's at stake here, and that's less severe: getting the same result twice in the output.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2656#discussion_r547261985", "createdAt": "2020-12-22T12:55:01Z", "author": {"login": "mtopolnik"}, "path": "site/docs/design-docs/018-kinesis-connectors.md", "diffHunk": "@@ -0,0 +1,476 @@\n+---\n+title: 018 - Kinesis Connectors\n+description: Sources and sinks for Amazon Kinesis Data Streams\n+---\n+\n+*Since*: 4.4\n+\n+## Summary\n+\n+[Amazon Kinesis Data\n+Streams](https://aws.amazon.com/kinesis/data-streams/) (KDS) is a\n+massively scalable and durable real-time data streaming service. As part\n+of the Amazon Web Services offering, KDS manages the infrastructure,\n+storage, networking, and configuration needed to stream your data at the\n+level of your data throughput. You do not have to worry about\n+provisioning, deployment, ongoing-maintenance of hardware, software, or\n+other services for your data streams. Also, Amazon Kinesis Data Streams\n+synchronously replicates data across three availability zones, providing\n+high availability and data durability.\n+\n+The purpose of this document is to describe the implementation of\n+distributed Jet sources and sinks, which make it possible to read data\n+from and write data into Kinesis via Jet.\n+\n+## Key Concepts\n+\n+A **shard** is the base throughput unit of KDS. One shard provides a\n+capacity of 1MB/sec data input and 2MB/sec data output. One shard can\n+support up to 1000 record publications per second. You will specify the\n+number of shards needed when you create a data stream. For example, you\n+can create a data stream with two shards. This data stream has a\n+throughput of 2MB/sec data input and 4MB/sec data output and allows up\n+to 2000 record publications per second. You can monitor shard-level\n+metrics in Kinesis and add or remove shards from your data stream\n+dynamically as your data throughput changes by resharding the data\n+stream.\n+\n+A **record** is the unit of data stored in Kinesis. A record is composed\n+of a sequence number, partition key, and data blob. Data blob is the\n+data of interest your data producer adds to a data stream. The maximum\n+size of a data blob (the data payload before Base64-encoding) is 1\n+megabyte (MB).\n+\n+A **partition key** is used to segregate and route records to different\n+shards of a data stream. A partition key is specified by your data\n+producer while adding data to KDS. For example, assuming you have a data\n+stream with two shards (shard 1 and shard 2). You can configure your\n+data producer to use two partition keys (key A and key B) so that all\n+records with key A are added to shard 1, and all records with key B are\n+added to shard 2.\n+\n+A **sequence number** is a unique identifier for each record. Sequence\n+numbers are assigned by KDS when a data producer publishes data into it.\n+Sequence numbers for the same partition key generally increase over\n+time.\n+\n+## APIs\n+\n+Amazon offers various choices of libraries that can be used to interact\n+with KDS:\n+\n+* **Kinesis Client Library (KCL)** and **Kinesis Producer Library\n+  (KPL)** are high-level libraries that are easy to use because they\n+  abstract away many concerns. They manage their threading policy, hide\n+  away the REST-based nature of Kinesis behind asynchronous constructs,\n+  balance load, handle failures, and react to resharding. However, all\n+  this convenience makes them unsuitable for building Jet connectors,\n+  where we need the most control possible to make choices that are\n+  suitable to Jet's architecture.\n+* **Amazon Kinesis Data Streams API** via **AWS SDK for Java** is the\n+  lower-level library that allows sufficient control when interacting\n+  with Kinesis. It consists of a simple set of [REST-based\n+  operations](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Operations.html).\n+  Every other concern mentioned above, when discussing the high-level\n+  libraries, has to be handled explicitly. This is the library used in\n+  the Jet source and sink implementations.\n+\n+## Quotas\n+\n+Amazon Kinesis Data Streams enforces quite a few quotas and limits,\n+which our sources and sinks need to comply with:\n+\n+* A single shard can ingest up to 1 MB of data per second (including\n+  partition keys) or 1,000 records per second for writes.\n+* The maximum size of the data payload of a record is 1 MB.\n+* The\n+  [GetRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html)\n+  operation can retrieve up to 10 MB of data per call from a single\n+  shard and up to 10,000 records per call.\n+* Each shard can support up to 5 GetRecords operations per second.\n+* Each shard can support up to a maximum total data read rate of 2 MB\n+  per second.\n+* The\n+  [ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+  operation has a limit of 100 transactions per second, per data stream.\n+  Each such transaction is able to return at most 100 shards, and if the\n+  stream has more, then multiple transactions need to be used for a full\n+  listing. (For details, see the [discovery](#discovery) section.)\n+* The\n+  [PutRecords](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)\n+  operation can write at most 500 records into the stream. Each record\n+  in the request can be as large as 1MB, up to a limit of 5MB for the\n+  entire request, including partition keys. Each shard can support\n+  writes up to 1,000 records per second, up to a maximum data write\n+  total of 1 MiB per second.\n+* The\n+  [DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+  operation has a limit of 20 transactions per second per account. (For\n+  details, see the [adaptive throughput](#adaptive-throughput) section.)\n+\n+## Source\n+\n+The Kinesis source is a _streaming_, _distributed_, and _fault-tolerant_\n+data source for Jet. It supports both the _at-least-once_ and\n+_exactly-once_ [processing\n+guarantees](../architecture/fault-tolerance.md#processing-guarantee-is-a-shared-concern).\n+\n+### Distribution\n+\n+Being a distributed source, it has multiple instances running in each\n+Jet cluster member. Each instance is responsible for reading from zero,\n+one or more KDS [shards](#shard). The assignment of shards to individual\n+source instances is based on partitioning the space of Kinesis [record\n+keys](#partition-key).\n+\n+Record keys, or partition keys as Kinesis calls them, are Unicode\n+strings, with a maximum length limit of 256 characters. The stream uses\n+the MD5 hash function to map these strings to 128-bit integer values.\n+The universe of these values is thus the [0 .. 2^128) range. Each\n+Kinesis shard has a continuous chunk of this range assigned to it,\n+called the shard's hash range. The stream assigns a record to a shard if\n+the record's partition key hashes into the shard's range.\n+\n+In the Jet Kinesis source, we use similar logic for assigning shards to\n+source instances. Each of our sources gets a part of the hash range\n+assigned to it. We say that a source owns a specific shard if and only\n+if the shard's hash range's starting point is inside the source's hash\n+range. Any similar range matching logic would work, as long as it's\n+non-ambiguous.\n+\n+### Discovery\n+\n+Reading records from shards assigned to them is only part of the\n+responsibility of sources. Sources also need a way to discover currently\n+active shards in the stream to take responsibility for them. Moreover,\n+this discovery process can't just happen once, on start-up, because\n+streams are dynamic, shards can be closed, and new shards can pop-up at\n+any time. For details, see the [resharding section](#resharding).\n+\n+Continuously monitoring the set of active shards in the stream is the\n+responsibility of **one** of the local source instances in each Jet\n+cluster member. This is an optimization. If all sources would run the\n+discovery, they would still obtain the same data, just with a multiplied\n+effort and cost. Monitoring means continuously polling the stream for\n+the list of all shards in it.\n+\n+Monitoring needs to take care not to cross the rate limit imposed by\n+Kinesis on this operation. For details, see the [quotas\n+section](#quotas).\n+\n+### Resharding\n+\n+Kinesis supports resharding, which lets you adjust the number of shards\n+in your stream to adapt to changes in data flow rate through the stream.\n+(Amazon charges on a per-shard basis, that's why it's desirable to have\n+the smallest amount of shards possible.)\n+\n+There are two types of resharding operations: shard **split** and shard\n+**merge**. In a shard split, you divide a single shard into two adjacent\n+shards. In a shard merge, you combine two adjacent shards into a single\n+shard. By \"adjacent\", we mean that one's hash range starts where the\n+other one's ends.\n+\n+Splitting increases the number of shards in your stream and therefore\n+increases the data capacity (and cost) of the stream. Similarly, merging\n+reduces the number of shards in your stream and therefore decreases the\n+data capacity (and cost).\n+\n+Resharding is always pairwise in the sense that you cannot split into\n+more than two shards in a single operation, and you cannot merge more\n+than two shards in a single operation. The shard or pair of shards that\n+the resharding operation acts on are called parent shards. The shard or\n+pair of shards that result from the resharding operation are called\n+child shards.\n+\n+When child shards, resulting from a split or merge, activate, their\n+parents get deactivated and will no longer get data inserted into them.\n+From that point onward, data goes into the children.\n+\n+### Read Order\n+\n+Resharding does not suspend the stream's dataflow, while it's going on.\n+Data continues to be ingested into the stream, and at some point, it\n+just stops being put into the parent shards and starts being put into\n+the child shards.\n+\n+The Kinesis Jet source would need to make sure that it finishes reading\n+from parents before reading from their children. However, this is not\n+possible since the children might end up being owned by an entirely\n+different instance of the source than their parents (for example, in a\n+split), possibly located in an entirely different Jet cluster member.\n+\n+Moreover, it's not enough to finish reading from the parent before\n+reading from the children. Even if that were achieved, data from parents\n+might overtake data from children further down the Jet pipeline, in its\n+various parallel components. A Kinesis source would need to make sure\n+that it has read all data from the parents and that data has fully\n+passed through the Jet pipeline before starting to read from the\n+children. Only then could it provide the same ordering as KDS while\n+resharding.\n+\n+This is currently not possible in Jet. Hopefully, future versions will\n+address the problem. Users of the Kinesis source need to be aware that\n+some data reordering might occur on resharding.\n+\n+### Fault Tolerance\n+\n+The Kinesis Jet source supports pipelines with both at-least-once and\n+exactly-once processing guarantees. It achieves this by saving KDS\n+offsets into its snapshots and starting data read from saved offsets\n+when restarted.\n+\n+The offsets are saved on a per-shard basis, and on restart, each source\n+instance receives all saved offsets for all shards, so it can function\n+properly regardless of how shards are assigned to sources after the\n+restart.\n+\n+### Watermarks\n+\n+The Kinesis source can provide native watermarks because the [record\n+data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_Record.html)\n+has a field that can be turned towards this purpose\n+(`ApproximateArrivalTimestamp`). However, it should be pointed out that\n+these watermarks are \"native\" only from Jet's point of view. They are\n+KDS ingestion times, i.e., whenever a KDS producer managed to push said\n+record into the data stream. We have no way of knowing what's the real\n+event time of a record.\n+\n+Watermarks are also saved to and recovered from snapshots.\n+\n+### Metrics\n+\n+When receiving record batches, the [data\n+structure](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html#API_GetRecords_ResponseSyntax)\n+contains a field called `MillisBehindLatest` defined as following:\n+\n+> The number of milliseconds the GetRecords response is from the\n+> stream's tip, indicating how far behind the current time the consumer\n+> is. A value of zero indicates that record processing caught up, and\n+> there are no new records to process at this moment.\n+\n+This value can be useful for monitoring so the sources can publish it\n+as a per processor instance metric.\n+\n+### Code Example\n+\n+A typical example of setting up a Kinesis source in Jet would look like\n+this:\n+\n+```java\n+KinesisSources.kinesis(\"myStream\")\n+  .withRegion(\"us-east-1\")\n+  .withEndpoint(\"http://localhost:12345\")\n+  .withCredentials(\"accesskey\", \"secretkey\")\n+  .withRetryStrategy(RetryStrategies.indefinitely(250))\n+  .build();\n+```\n+\n+The only mandatory property is the Kinesis `stream name`. The others are\n+optional and can be specified via a fluent builder.\n+\n+If `region` is not specified, then _us-east-1_ will be used by default.\n+\n+If `endpoint` is not specified, then the region's default endpoint will\n+be used.\n+\n+If `credentials` aren't specified, then the [Default Credential Provider\n+Chain](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html#credentials-default)\n+will be followed.\n+\n+If `retry strategy` is not specified, then a default of our definition\n+will be used (maximum 10 retries, with exponential backoff limited to\n+a maximum of 3 seconds). A source's retry strategy applies to failures\n+of reading records from or listing shards of a stream.\n+\n+The actual source created will be of type\n+`StreamSource<Map.Entry<String, byte[]>>`, so basically a stream of\n+partition key - record data blob pairs.\n+\n+## Sink\n+\n+The Kinesis sink is a _distributed_, and _fault-tolerant_ data sink for\n+Jet. It supports both _streaming_ and _batching_ pipelines. The\n+fault-tolerance guarantee it can offer is only _at-least-once_ since\n+Kinesis does not offer transaction support.\n+\n+### Distribution\n+\n+Being a distributed sink, it has multiple instances running in each Jet\n+cluster member. When used in a pipeline, this sink forces its incoming\n+edges to be _distributed_ and _partitioned_. The partition keys used by\n+the edges are the same as the Kinesis [partition keys](#partition-key).\n+This ensures that all data with the same partition key will end up in\n+the same global sink instance and the same shard.\n+\n+### Flow Control\n+\n+Writing data into a Kinesis Data Stream is governed by multiple\n+limitations:\n+\n+* no more than 500 records can be written in one batch\n+* each record must contain no more than 1M of data\n+* each batch must contain no more than 5M of data\n+* each shard can ingest no more than 1,000 records per second\n+\n+While most of these limitations are simple to enforce, the shard\n+ingestion rate is not. Different partition keys get assigned to a shard\n+based on a hashing function, so partition keys going into the same shard\n+can be written by different sink instances. Currently, Jet has no\n+capability for computing and coordinating such a per shard rate among\n+all it's distributed sink instances.\n+\n+The sink takes a different approach to comply with this limitation. It\n+allows for the rate to be tripped (i.e., it doesn't attempt to prevent\n+it from happening), but once it gets tripped, sinks try to slow down the\n+amount of data they write to keep the rate violation as an occasional,\n+rare event and not a continuous storm.\n+\n+The source achieves this flow control in two ways:\n+\n+* by decreasing the send batch size; the default is the maximum of 500,\n+  which it will reduce, if necessary, as low as 10 records/batch\n+* by adding a delay between two subsequent send actions (which can be as\n+  little as 100ms, a reasonable value in case of Kinesis and as much as\n+  10 seconds, which is a lot, but would occur only in an unreasonably\n+  sized stream, as far as shard count is concerned - ultimately the\n+  owner of the stream is responsible for setting up enough shards to be\n+  able to handle his data rates)\n+\n+The flow control process is _adaptive_ in the sense that:\n+\n+* it kicks in only when batches start failing due to shard ingestion\n+  rates being tripped\n+* until failures repeat, it keeps quickly increasing the sleep delays\n+  to stop them from happening\n+* once failures stop, it slowly decreases the sleep delays until they\n+  are eliminated (i.e., the data volume spike was only temporary) or\n+  until failures start happening again\n+\n+Under normal circumstances, if there are enough shards in the stream and\n+their data ingestion rate covers the data flow, this whole flow control\n+process stays shut off. The sink publishes data with the lowest possible\n+latency.\n+\n+### Discovery\n+\n+As we've seen in the [flow control](#flow-control) section, one element\n+used to control the throughput is batch size. Under normal conditions,\n+the sink uses the default/maximum batch size of 500. When flow control\n+kicks a new batch size is picked as a function of the number of open\n+shards in the stream.\n+\n+For this to happen, the sinks need to have relatively up-to-date\n+information about the count of open shards. The sink achieves this by\n+using a mechanism very similar to the [discovery process employed by the\n+source](#discovery). The only real difference is that the sinks use the\n+[DescribeStreamSummary](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_DescribeStreamSummary.html)\n+operation instead of the\n+[ListShards](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html)\n+one.\n+\n+### Write Order\n+\n+Under normal circumstances, the Kinesis sink preserves the order of\n+items belonging to the same partition key. However, when the [flow\n+control](#flow-control) mechanism kicks in, the ordering might be lost\n+on occasion.\n+\n+This unfortunate fact originates in the way how KDS handles shard\n+ingestion rate violations. When KDS receives a batch to be ingested, it\n+processes each item in it one-by-one, and if some fail, it doesn't stop\n+processing the batch. The result is that some items from a batch get\n+rejected, some get ingested, but in a random manner. The sink does\n+resend the un-ingested item, they won't get lost, but there is nothing\n+it can do to preserve the initial ordering. This is very unfortunate\n+since all that would be needed for avoiding this problem would be for\n+Kinesis to reject all remaining items from the batch once one of them\n+trips the ingestion rate limit. The designers probably made the choice\n+they did to save on data traffic.\n+\n+Nothing that we can do about it at this point needs to be documented,\n+and users of the sink need to be aware.\n+\n+### Fault Tolerance\n+\n+Since there is no transaction support in Kinesis the sink can't support\n+_exactly-once_ processing. It can, however, support _at-least-once_", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92c41d2d329b4c912024828b41a9676216106193"}, "originalPosition": 398}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a5baf1abf2f74b9833685767bde9201f69c47830", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/a5baf1abf2f74b9833685767bde9201f69c47830", "committedDate": "2020-12-22T13:33:53Z", "message": "Fix bug, improve tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "372f3e8ab648f821536cea718e4642f04074aa7d", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/372f3e8ab648f821536cea718e4642f04074aa7d", "committedDate": "2020-12-23T08:29:15Z", "message": "Improve tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8609e7b6f687a9982d8347cd6def0ef468083dbe", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/8609e7b6f687a9982d8347cd6def0ef468083dbe", "committedDate": "2020-12-23T08:29:24Z", "message": "Change expiration related implementation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "edbb8bb0904a41d091b0a85b9226a60fd9406b12", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/edbb8bb0904a41d091b0a85b9226a60fd9406b12", "committedDate": "2020-12-23T09:05:17Z", "message": "Remove unnecessary complications"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1d67987aafdd684d3f0ed25330ccee89a0d172ad", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/1d67987aafdd684d3f0ed25330ccee89a0d172ad", "committedDate": "2020-12-23T10:37:37Z", "message": "Address review concerns"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cb736640d67378785d6e72e9797a4d7f553196a9", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/cb736640d67378785d6e72e9797a4d7f553196a9", "committedDate": "2020-12-23T11:08:37Z", "message": "Remove local parallelism hardcoding"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "321f3a57fccc7a0b745f5c1fe50c49cee9a52390", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/321f3a57fccc7a0b745f5c1fe50c49cee9a52390", "committedDate": "2020-12-24T06:01:39Z", "message": "Address review concerns"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0df5525326fd92f192c121023d603512e23a663f", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/0df5525326fd92f192c121023d603512e23a663f", "committedDate": "2020-12-24T10:12:37Z", "message": "Implement metrics, improve tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bc138f18bef001b3ed039365d0561fd747a4ebcf", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/bc138f18bef001b3ed039365d0561fd747a4ebcf", "committedDate": "2020-12-28T12:57:50Z", "message": "Add Kinesis tutorial, make sure it works"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1b7f8560a717a8c169ead6af86fa04230b2b756c", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/1b7f8560a717a8c169ead6af86fa04230b2b756c", "committedDate": "2020-12-29T09:45:38Z", "message": "Handle producer retries in test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "898987a209e10b136a811550d0a7bda95b05a4d4", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/898987a209e10b136a811550d0a7bda95b05a4d4", "committedDate": "2020-12-29T10:06:51Z", "message": "Address ClientConfiguration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4c842ba12ea554c1ea944e24c8cc74b95f059f90", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/4c842ba12ea554c1ea944e24c8cc74b95f059f90", "committedDate": "2020-12-29T11:29:40Z", "message": "Remove ClientConfiguration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a3e66eed11f3f9aa761660da8b4aec8f7e4ebb8d", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/a3e66eed11f3f9aa761660da8b4aec8f7e4ebb8d", "committedDate": "2020-12-30T08:24:53Z", "message": "Finished sources & sinks documentation update"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "958abb850a7c507db8ee09e4a3345fc98048354f", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/958abb850a7c507db8ee09e4a3345fc98048354f", "committedDate": "2020-12-30T12:41:40Z", "message": "Write all javadoc (NEEDS CHECKING)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f97b566c186da9decfc80f4dc803af1e8f489692", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/f97b566c186da9decfc80f4dc803af1e8f489692", "committedDate": "2020-12-31T08:32:06Z", "message": "Spell-check recent javadocs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6433029c85d2d18542d640df370fad9b61889d51", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/6433029c85d2d18542d640df370fad9b61889d51", "committedDate": "2021-01-04T08:35:12Z", "message": "Merge branch 'master' into kinesis"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "942fae336e600bddd83a0d8d29fb46878b5f195f", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/942fae336e600bddd83a0d8d29fb46878b5f195f", "committedDate": "2021-01-04T10:29:33Z", "message": "Change shard state serialization"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f8a4c0da340a2d9dfd26633f46c0a4dc407c06d4", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/f8a4c0da340a2d9dfd26633f46c0a4dc407c06d4", "committedDate": "2021-01-04T11:27:49Z", "message": "Add clean-up section to tutorial"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7756af1bbd27c061ddd74d8f998326abdd171e15", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/7756af1bbd27c061ddd74d8f998326abdd171e15", "committedDate": "2021-01-04T11:33:18Z", "message": "Rename and document the asymmetrical tracker for sleep times"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b7fe0907044a3e74cb7f4cd5ccb03a7ab9909e47", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/b7fe0907044a3e74cb7f4cd5ccb03a7ab9909e47", "committedDate": "2021-01-04T12:16:46Z", "message": "Solve and remove minor TODOs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c2a9d3ec0acceb541585651d5fe349d44b56cac1", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/c2a9d3ec0acceb541585651d5fe349d44b56cac1", "committedDate": "2021-01-05T08:19:42Z", "message": "Solve final TODOs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "25afc92be6b738b3100650a2311e717658131453", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/25afc92be6b738b3100650a2311e717658131453", "committedDate": "2021-01-05T11:33:31Z", "message": "Merge branch 'master' into kinesis"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c0dfa6b217360339a2c18c688f8f9cca061ff5ef", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/c0dfa6b217360339a2c18c688f8f9cca061ff5ef", "committedDate": "2021-01-05T12:04:30Z", "message": "Make checkstyle happy"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03129217fa36d69ce61d8ee61ccb48cb6ea2695e", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/03129217fa36d69ce61d8ee61ccb48cb6ea2695e", "committedDate": "2021-01-07T07:37:03Z", "message": "Merge branch 'master' into kinesis"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "42c5ceeb17cd1a95869ed14ddfcddf96d58394c3", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/42c5ceeb17cd1a95869ed14ddfcddf96d58394c3", "committedDate": "2021-01-07T08:08:08Z", "message": "Address review concerns"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9dd61b0a43787374ffe95128cc815822403798cb", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/9dd61b0a43787374ffe95128cc815822403798cb", "committedDate": "2021-01-07T08:27:35Z", "message": "Address review concerns"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0ef28c3f311c74ca702cb77f69af345c1ffd8b2b", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/0ef28c3f311c74ca702cb77f69af345c1ffd8b2b", "committedDate": "2021-01-08T12:27:44Z", "message": "Improve tests a bit"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6528e1fbd8f42471cd7fd92549e8ac521b08041e", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/6528e1fbd8f42471cd7fd92549e8ac521b08041e", "committedDate": "2021-01-11T13:00:14Z", "message": "Make initial shard iterators configurable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3210b8420ed35ceb9bacd82995560f3bf97f76e5", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/3210b8420ed35ceb9bacd82995560f3bf97f76e5", "committedDate": "2021-01-12T11:19:00Z", "message": "Test initial shard iterator mechanism"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "71e88f662a3d4c2021925b9b8cea8c23271e1bcd", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/71e88f662a3d4c2021925b9b8cea8c23271e1bcd", "committedDate": "2021-01-12T14:04:41Z", "message": "Merge branch 'master' into kinesis"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "812bec6d345b083efca7ade5606565263803a944", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/812bec6d345b083efca7ade5606565263803a944", "committedDate": "2021-01-13T09:27:29Z", "message": "Fix version constants"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "79a7d0afa0901d33af02a2215570835661ff177e", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/79a7d0afa0901d33af02a2215570835661ff177e", "committedDate": "2021-01-13T21:58:19Z", "message": "use noop shard monitor for non leader kinesis sinks, also put shardCount into the monitor"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "78c741dfdbc6e466cd45855c767da80e246ffb12", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/78c741dfdbc6e466cd45855c767da80e246ffb12", "committedDate": "2021-01-13T21:13:53Z", "message": "last checkstyle"}, "afterCommit": {"oid": "79a7d0afa0901d33af02a2215570835661ff177e", "author": {"user": null}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/79a7d0afa0901d33af02a2215570835661ff177e", "committedDate": "2021-01-13T21:58:19Z", "message": "use noop shard monitor for non leader kinesis sinks, also put shardCount into the monitor"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3515, "cost": 1, "resetAt": "2021-11-01T14:51:55Z"}}}