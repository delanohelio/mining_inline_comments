{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc5NDM0ODg0", "number": 2012, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxNjozMjo0OVrODjJiLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxNjozMjo0OVrODjJiLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4MTgyOTU5OnYy", "diffSide": "RIGHT", "path": "site/docs/tutorials/kafka.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxNjozMjo0OVrOFuy6wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxNjozMjo0OVrOFuy6wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDYxMzA1OA==", "bodyText": "I think it's easier to use an auto-generated one, since you upload the schema file to registry normally. I will merge this for now and you can update later.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2012#discussion_r384613058", "createdAt": "2020-02-26T16:32:49Z", "author": {"login": "cangencer"}, "path": "site/docs/tutorials/kafka.md", "diffHunk": "@@ -3,10 +3,253 @@ title: Work with Apache Kafka\n id: kafka\n ---\n \n-TODO\n+Kafka is often used in real-time streaming data architectures to provide\n+real-time analytics. Since Kafka is a fast, scalable, durable and\n+fault-tolerant publish-subscribe messaging system; Kafka is used in\n+use-cases where other messaging systems may not even be considered due\n+to volume and responsiveness.\n+\n+Hazelcast Jet provides source and sink connectors for Kafka which are\n+suited for infinite stream processing jobs. The connectors also support\n+fault tolerance and snapshotting.\n+\n+## Enrich Data using IMap\n+\n+One of the most popular use-cases is enriching your data from an\n+external lookup table, in our case an `IMap`. Below code block shows\n+that we can easily read data from a topic, enrich the value by looking\n+up an IMap and write it to another topic.\n+\n+```java\n+Properties properties = new Properties();\n+properties.setProperty(\"bootstrap.servers\", \"localhost:9092\");\n+properties.setProperty(\"key.deserializer\", StringDeserializer.class.getCanonicalName());\n+properties.setProperty(\"value.deserializer\", StringDeserializer.class.getCanonicalName());\n+properties.setProperty(\"key.serializer\", StringSerializer.class.getCanonicalName());\n+properties.setProperty(\"value.serializer\", StringSerializer.class.getCanonicalName());\n+Pipeline p = Pipeline.create();\n+p.readFrom(KafkaSources.<String, String>kafka(properties, \"sourceTopic\"))\n+ .withoutTimestamps()\n+ .mapUsingIMap(\"lookupMap\", Map.Entry::getKey, (entry, lookupValue) -> entry(entry.getKey(), entry.getValue() + lookupValue))\n+ .writeTo(KafkaSinks.kafka(properties, \"sinkTopic\"));\n+JetInstance jet = Jet.bootstrappedInstance();\n+Job job = jet.newJob(p);\n+```\n \n ## Cache Data from Apache Kafka\n \n Jet offers a convenient way to read and cache data from Kafka. With a\n few lines of code you will be able to read a Kafka topic and materialize\n it inside an in-memory key-value based storage.\n+\n+```java\n+Properties properties = new Properties();\n+properties.setProperty(\"bootstrap.servers\", \"localhost:9092\");\n+properties.setProperty(\"key.deserializer\", StringDeserializer.class.getCanonicalName());\n+properties.setProperty(\"value.deserializer\", StringDeserializer.class.getCanonicalName());\n+\n+Pipeline p = Pipeline.create();\n+p.readFrom(KafkaSources.<String, String>kafka(properties, \"sourceTopic\"))\n+ .withoutTimestamps()\n+ .writeTo(Sinks.map(\"cacheMap\"));\n+\n+JetInstance jet = Jet.bootstrappedInstance();\n+Job job = jet.newJob(p);\n+```\n+\n+## Complex Record Types\n+\n+You can put your complex objects to Kafka using custom serializers and\n+de-serializers. Jet expects the consumed objects to be `Serializable`\n+since the objects will be moved between stages. You can also define a\n+projection function for source which will be applied to each record\n+before emitting to downstream. You can also define key and value\n+extractors for sink. Let's rewrite enrich data sample using a complex\n+object as the value of the record.\n+\n+```java\n+Properties properties = new Properties();\n+properties.setProperty(\"bootstrap.servers\", \"localhost:9092\");\n+properties.setProperty(\"key.deserializer\", StringDeserializer.class.getCanonicalName());\n+properties.setProperty(\"value.deserializer\", UserDeserializer.class.getCanonicalName());\n+properties.setProperty(\"key.serializer\", StringSerializer.class.getCanonicalName());\n+properties.setProperty(\"value.serializer\", UserSerializer.class.getCanonicalName());\n+\n+\n+Pipeline p = Pipeline.create();\n+p.readFrom(KafkaSources.kafka(properties, ConsumerRecord<String, User>::value, \"sourceTopic\"))\n+ .withoutTimestamps()\n+ .mapUsingIMap(\"passwordMap\", User::getName, (user, pass) -> user.setPassword((byte[])pass))\n+ .writeTo(KafkaSinks.kafka(properties, \"sinkTopic\", User::getName, user -> user));\n+\n+JetInstance jet = Jet.bootstrappedInstance();\n+Job job = jet.newJob(p);\n+```\n+\n+The **User** class:\n+\n+```java\n+public class User implements Serializable {\n+\n+    private String name;\n+    private byte[] password;\n+\n+    public User() {\n+    }\n+\n+    public User(String name, byte[] password) {\n+        this.name = name;\n+        this.password = password;\n+    }\n+\n+    public String getName() {\n+        return name;\n+    }\n+\n+    public User setName(String name) {\n+        this.name = name;\n+        return this;\n+    }\n+\n+    public byte[] getPassword() {\n+        return password;\n+    }\n+\n+    public User setPassword(byte[] password) {\n+        this.password = password;\n+        return this;\n+    }\n+}\n+```\n+\n+## Schema Registry\n+\n+While you can use custom serializer/de-serializer for your complex\n+objects, you can store your object as `json` or `avro` and use a **Schme\n+Registry** to handle the metadata.\n+\n+```java\n+Pipeline p = Pipeline.create();\n+\n+Properties properties = new Properties();\n+properties.setProperty(\"bootstrap.servers\", \"localhost:9092\");\n+properties.setProperty(\"key.deserializer\", StringDeserializer.class.getCanonicalName());\n+properties.setProperty(\"value.deserializer\", KafkaAvroDeserializer.class.getCanonicalName());\n+properties.setProperty(\"schema.registry.url\", \"http://localhost:8081\");\n+properties.setProperty(\"specific.avro.reader\", \"true\");\n+\n+p.readFrom(KafkaSources.<String, User>kafka(properties, \"sourceTopic\"))\n+ .withoutTimestamps()\n+ .writeTo(Sinks.logger());\n+```\n+\n+The **User** class with avro schema:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9692f2430e2fad0ed38fefa3519e62463ed88cda"}, "originalPosition": 145}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4880, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}