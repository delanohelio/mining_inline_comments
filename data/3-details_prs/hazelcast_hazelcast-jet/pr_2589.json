{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk4NDMzMzM3", "number": 2589, "title": "Publish CDC - Elastic blog post", "bodyText": "", "createdAt": "2020-10-06T10:27:30Z", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589", "merged": true, "mergeCommit": {"oid": "0fbe205b49c75a4f1df05b2f0d4ad29f81c43f04"}, "closed": true, "closedAt": "2020-10-08T14:30:49Z", "author": {"login": "frant-hartm"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdP1wP1AH2gAyNDk4NDMzMzM3OmEzZmFlNzRhMGQ2MmY3M2U0NGY0MjEzNGM3ODUzNzg4ZGFlNjQ3NDY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdQiAnUAFqTUwNDgxMTQyMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/a3fae74a0d62f73e44f42134c7853788dae64746", "committedDate": "2020-10-06T10:23:46Z", "message": "Publish CDC - Elastic blog post"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAyNzk5NDY5", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#pullrequestreview-502799469", "createdAt": "2020-10-06T10:31:24Z", "commit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxMDozMToyNFrOHc__6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxMDo0NzoyOFrOHdAgBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDE3MDcyOQ==", "bodyText": "react TO", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500170729", "createdAt": "2020-10-06T10:31:24Z", "author": {"login": "jbartok"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDE3MTI3Mg==", "bodyText": "rephrase this a bit, somehow this dashed section doesn't flow together with the rest, breaks the flow of thought", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500171272", "createdAt": "2020-10-06T10:32:34Z", "author": {"login": "jbartok"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDE3MjEzNA==", "bodyText": "Before writing to the search index, we will also enrich the data with a ...", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500172134", "createdAt": "2020-10-06T10:34:12Z", "author": {"login": "jbartok"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDE3MjYzMw==", "bodyText": "to provide", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500172633", "createdAt": "2020-10-06T10:35:09Z", "author": {"login": "jbartok"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDE3MjY5Nw==", "bodyText": "to allow", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500172697", "createdAt": "2020-10-06T10:35:17Z", "author": {"login": "jbartok"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDE3Mjc5OQ==", "bodyText": "to add", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500172799", "createdAt": "2020-10-06T10:35:26Z", "author": {"login": "jbartok"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDE3MzU1Nw==", "bodyText": "faster might be much more expensive in the long run... maybe drop one of them, say faster or cheaper, but not both", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500173557", "createdAt": "2020-10-06T10:36:49Z", "author": {"login": "jbartok"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDE3NDkyNA==", "bodyText": "its", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500174924", "createdAt": "2020-10-06T10:39:39Z", "author": {"login": "jbartok"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDE3NTgxNQ==", "bodyText": "maybe write out natural language processing, I already forgot what the acronym meant, by the time I got here", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500175815", "createdAt": "2020-10-06T10:41:24Z", "author": {"login": "jbartok"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDE3ODk1MA==", "bodyText": "Do these graph visualisations really add value? Maybe remove them?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500178950", "createdAt": "2020-10-06T10:47:28Z", "author": {"login": "jbartok"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,\n+otherwise the later stages might see it in inconsistent state, causing\n+issues which are hard to debug. We make defensive copies to avoid this.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/JoiningState.java#L11)\n+to see the full source code.\n+\n+### Deleting changes\n+\n+The `ChangeRecord#operation()` method provides information about what\n+kind of operation was performed on the record. If you need to handle\n+deletions you should use it. The Petclinic application doesn\u2019t allow\n+delete operations, so we don\u2019t handle those.\n+\n+### Elastic sink\n+\n+The fourth key part of the pipeline is the Elastic sink. It takes a\n+document and writes it to Elasticsearch. You as a developer need to\n+provide information where the Elastic instance is running, what index to\n+write to and how to convert the `Document` into one of `IndexRequest`,\n+`UpdateRequest` or `DeleteRequest`. The Sink then takes care of batching\n+the requests and parallelizing for higher performance, retries in case\n+of network issues.\n+\n+### The Pipeline\n+\n+The final pipeline composes all the individual steps in a\n+straightforward way:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .map(PetClinicIndexJob::mapChangeRecordToPOJO).setName(\"mapChangeRecordToPOJO\")\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords).setName(\"enrichWithKeywords\")\n+ .mapStateful(JoiningState::new, JoiningState::join).setName(\"JoiningState::join\")\n+ .writeTo(elasticSink);\n+```\n+\n+Or visualized as a graph:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 344}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAyODc3OTE3", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#pullrequestreview-502877917", "createdAt": "2020-10-06T12:19:32Z", "commit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "state": "COMMENTED", "comments": {"totalCount": 36, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxMjoxOTozMlrOHdDkKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQxMjozOTo1M1rOHdEVnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIyOTE2MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            certain settings and grant more permissions to the `petclinic` user\n          \n          \n            \n            certain settings and grant more permissions to the `petclinic` user.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500229161", "createdAt": "2020-10-06T12:19:32Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIyOTM2NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            password for the root user.\n          \n          \n            \n            password for the root user:", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500229365", "createdAt": "2020-10-06T12:19:54Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIyOTU3NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We need the following to grant privileges to the `petclinic` user to allow\n          \n          \n            \n            We need to grant the following privileges to the `petclinic` user to allow", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500229575", "createdAt": "2020-10-06T12:20:14Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIyOTcwMA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            listening to the database changes\n          \n          \n            \n            listening to the database changes:", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500229700", "createdAt": "2020-10-06T12:20:25Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzMDY2NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now that we have the Petclinic and Elastic set-up and working we can\n          \n          \n            \n            Now that we have the Petclinic and Elastic set up and working, we can", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500230665", "createdAt": "2020-10-06T12:21:58Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzMDk1Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            A Jet job is a pipeline of steps, that read, modify, aggregate or store\n          \n          \n            \n            A Jet job is a pipeline of steps that read, modify, aggregate or store", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500230952", "createdAt": "2020-10-06T12:22:30Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 188}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzMTY1Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n          \n          \n            \n            call a 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500231656", "createdAt": "2020-10-06T12:23:43Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 237}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzMTk5MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            information, Hazelcast Jet works with POJOs in the pipeline, so you need\n          \n          \n            \n            information. Hazelcast Jet works with POJOs in the pipeline, so you need", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500231991", "createdAt": "2020-10-06T12:24:17Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 254}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzMjIyNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            option is to use tuples. This works for a small number of fields. Other\n          \n          \n            \n            option is to use tuples. This works for a small number of fields. Another", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500232224", "createdAt": "2020-10-06T12:24:40Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 256}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzMjUzMA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            information - e.g. the `keywords` field. If you are on Java 14 or newer\n          \n          \n            \n            information - e.g. the `keywords` field. If you are on Java 14 or newer,", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500232530", "createdAt": "2020-10-06T12:25:04Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 258}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzMjg3OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            [ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n          \n          \n            \n            [ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html)).", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500232879", "createdAt": "2020-10-06T12:25:38Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 266}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzMzEyOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            row is all that you need then you are good to go. You can see such\n          \n          \n            \n            row is all that you need, you are good to go. You can see such an", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500233129", "createdAt": "2020-10-06T12:25:59Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 268}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzNDE4OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Our visit record contains the pet id but not the owner id, in such case,\n          \n          \n            \n            Our visit record contains the pet ID but not the owner ID,", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500234189", "createdAt": "2020-10-06T12:27:41Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 283}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzNDYzOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            of having a single state - the joining logic is simpler and the pipeline\n          \n          \n            \n            of having a single piece of state - the joining logic is simpler and the pipeline", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500234639", "createdAt": "2020-10-06T12:28:26Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzNDk1MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            is not scalable because the state may not fit onto a single member or it\n          \n          \n            \n            is not scalable because the state may not fit onto a single member and it", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500234950", "createdAt": "2020-10-06T12:28:53Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 288}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzNTA2NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            cannot be performed by a single thread;\n          \n          \n            \n            must be performed by a single thread;", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500235064", "createdAt": "2020-10-06T12:29:03Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 289}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzNTc4OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * create two separate mapping steps - one grouped on the owner id and\n          \n          \n            \n            * create two separate mapping steps - one grouped on the owner ID and", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500235789", "createdAt": "2020-10-06T12:30:17Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 291}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzNTkwNA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            the other on the pet id. This is a trade-off between simplicity and\n          \n          \n            \n            the other on the pet ID. This is a tradeoff between simplicity and", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500235904", "createdAt": "2020-10-06T12:30:26Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 292}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzNjk2NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * the mapping state must cover all data from the beginning, there might\n          \n          \n            \n            * the mapping state must cover all data from the beginning, unless there", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500236965", "createdAt": "2020-10-06T12:32:07Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 297}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzNzA0MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            be a domain-specific rule that would allow eviction of old items;\n          \n          \n            \n            is a domain-specific rule that would allow eviction of old items;", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500237041", "createdAt": "2020-10-06T12:32:14Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 298}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzNzE5MA==", "bodyText": "expand FK -> foreign key", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500237190", "createdAt": "2020-10-06T12:32:29Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 300}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzNzM2MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            * once the object is emitted from the join step it must not be modified,\n          \n          \n            \n            * once the object is emitted from the join step, it must not be modified,", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500237360", "createdAt": "2020-10-06T12:32:46Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 304}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzNzY4MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            deletions you should use it. The Petclinic application doesn\u2019t allow\n          \n          \n            \n            deletions, you should use it. The Petclinic application doesn\u2019t allow", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500237680", "createdAt": "2020-10-06T12:33:20Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,\n+otherwise the later stages might see it in inconsistent state, causing\n+issues which are hard to debug. We make defensive copies to avoid this.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/JoiningState.java#L11)\n+to see the full source code.\n+\n+### Deleting changes\n+\n+The `ChangeRecord#operation()` method provides information about what\n+kind of operation was performed on the record. If you need to handle\n+deletions you should use it. The Petclinic application doesn\u2019t allow", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 316}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzNzk1Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            document and writes it to Elasticsearch. You as a developer need to\n          \n          \n            \n            document and writes it to Elasticsearch. You as the developer need to", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500237952", "createdAt": "2020-10-06T12:33:47Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,\n+otherwise the later stages might see it in inconsistent state, causing\n+issues which are hard to debug. We make defensive copies to avoid this.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/JoiningState.java#L11)\n+to see the full source code.\n+\n+### Deleting changes\n+\n+The `ChangeRecord#operation()` method provides information about what\n+kind of operation was performed on the record. If you need to handle\n+deletions you should use it. The Petclinic application doesn\u2019t allow\n+delete operations, so we don\u2019t handle those.\n+\n+### Elastic sink\n+\n+The fourth key part of the pipeline is the Elastic sink. It takes a\n+document and writes it to Elasticsearch. You as a developer need to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 322}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzODQ5OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            For the pipeline with two step join see [the source\n          \n          \n            \n            For the pipeline with the two-step join, see [the source", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500238499", "createdAt": "2020-10-06T12:34:42Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,\n+otherwise the later stages might see it in inconsistent state, causing\n+issues which are hard to debug. We make defensive copies to avoid this.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/JoiningState.java#L11)\n+to see the full source code.\n+\n+### Deleting changes\n+\n+The `ChangeRecord#operation()` method provides information about what\n+kind of operation was performed on the record. If you need to handle\n+deletions you should use it. The Petclinic application doesn\u2019t allow\n+delete operations, so we don\u2019t handle those.\n+\n+### Elastic sink\n+\n+The fourth key part of the pipeline is the Elastic sink. It takes a\n+document and writes it to Elasticsearch. You as a developer need to\n+provide information where the Elastic instance is running, what index to\n+write to and how to convert the `Document` into one of `IndexRequest`,\n+`UpdateRequest` or `DeleteRequest`. The Sink then takes care of batching\n+the requests and parallelizing for higher performance, retries in case\n+of network issues.\n+\n+### The Pipeline\n+\n+The final pipeline composes all the individual steps in a\n+straightforward way:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .map(PetClinicIndexJob::mapChangeRecordToPOJO).setName(\"mapChangeRecordToPOJO\")\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords).setName(\"enrichWithKeywords\")\n+ .mapStateful(JoiningState::new, JoiningState::join).setName(\"JoiningState::join\")\n+ .writeTo(elasticSink);\n+```\n+\n+Or visualized as a graph:\n+\n+![Pipeline with global mapping state](assets/2020-10-06-global-join-step.png)\n+\n+For the pipeline with two step join see [the source", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 348}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzODcxNQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            We have the following options\n          \n          \n            \n            We have the following options:", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500238715", "createdAt": "2020-10-06T12:35:01Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,\n+otherwise the later stages might see it in inconsistent state, causing\n+issues which are hard to debug. We make defensive copies to avoid this.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/JoiningState.java#L11)\n+to see the full source code.\n+\n+### Deleting changes\n+\n+The `ChangeRecord#operation()` method provides information about what\n+kind of operation was performed on the record. If you need to handle\n+deletions you should use it. The Petclinic application doesn\u2019t allow\n+delete operations, so we don\u2019t handle those.\n+\n+### Elastic sink\n+\n+The fourth key part of the pipeline is the Elastic sink. It takes a\n+document and writes it to Elasticsearch. You as a developer need to\n+provide information where the Elastic instance is running, what index to\n+write to and how to convert the `Document` into one of `IndexRequest`,\n+`UpdateRequest` or `DeleteRequest`. The Sink then takes care of batching\n+the requests and parallelizing for higher performance, retries in case\n+of network issues.\n+\n+### The Pipeline\n+\n+The final pipeline composes all the individual steps in a\n+straightforward way:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .map(PetClinicIndexJob::mapChangeRecordToPOJO).setName(\"mapChangeRecordToPOJO\")\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords).setName(\"enrichWithKeywords\")\n+ .mapStateful(JoiningState::new, JoiningState::join).setName(\"JoiningState::join\")\n+ .writeTo(elasticSink);\n+```\n+\n+Or visualized as a graph:\n+\n+![Pipeline with global mapping state](assets/2020-10-06-global-join-step.png)\n+\n+For the pipeline with two step join see [the source\n+code](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/two-joins/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L87)\n+on Github. This is the graph visualisation of the pipeline:\n+\n+![Pipeline with 2 separate grouped join steps](assets/2020-10-06-two-join-step.png)\n+\n+## Testing the job\n+\n+In order to test the job, we need to create a test database identical to\n+the Petclinic database.\n+\n+We have the following options", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 359}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzOTM1Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In order to run the indexing job, first checkout the\n          \n          \n            \n            In order to run the indexing job, first check out the", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500239356", "createdAt": "2020-10-06T12:35:56Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,\n+otherwise the later stages might see it in inconsistent state, causing\n+issues which are hard to debug. We make defensive copies to avoid this.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/JoiningState.java#L11)\n+to see the full source code.\n+\n+### Deleting changes\n+\n+The `ChangeRecord#operation()` method provides information about what\n+kind of operation was performed on the record. If you need to handle\n+deletions you should use it. The Petclinic application doesn\u2019t allow\n+delete operations, so we don\u2019t handle those.\n+\n+### Elastic sink\n+\n+The fourth key part of the pipeline is the Elastic sink. It takes a\n+document and writes it to Elasticsearch. You as a developer need to\n+provide information where the Elastic instance is running, what index to\n+write to and how to convert the `Document` into one of `IndexRequest`,\n+`UpdateRequest` or `DeleteRequest`. The Sink then takes care of batching\n+the requests and parallelizing for higher performance, retries in case\n+of network issues.\n+\n+### The Pipeline\n+\n+The final pipeline composes all the individual steps in a\n+straightforward way:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .map(PetClinicIndexJob::mapChangeRecordToPOJO).setName(\"mapChangeRecordToPOJO\")\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords).setName(\"enrichWithKeywords\")\n+ .mapStateful(JoiningState::new, JoiningState::join).setName(\"JoiningState::join\")\n+ .writeTo(elasticSink);\n+```\n+\n+Or visualized as a graph:\n+\n+![Pipeline with global mapping state](assets/2020-10-06-global-join-step.png)\n+\n+For the pipeline with two step join see [the source\n+code](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/two-joins/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L87)\n+on Github. This is the graph visualisation of the pipeline:\n+\n+![Pipeline with 2 separate grouped join steps](assets/2020-10-06-two-join-step.png)\n+\n+## Testing the job\n+\n+In order to test the job, we need to create a test database identical to\n+the Petclinic database.\n+\n+We have the following options\n+\n+* Start the pet clinic application inside the same JVM - the Petclinic\n+is a Spring Boot application, running it in-process would be\n+relatively easy, but it represents a legacy application, which is\n+usually quite difficult to run in such way. Also, there is a high\n+probability of dependency clashes, which would make this difficult.\n+\n+* Start the pet clinic application in a Docker container - it is likely\n+that the application is not containerized, we would need to create\n+such an image only for the tests.\n+\n+* Create the database and data using SQL directly - reusing the SQL\n+scripts from the Petclinic application and running inserts via JDBC\n+seems the easiest way to do this so we will continue with this approach.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L121)\n+to see the full source code.\n+\n+## Running the Job\n+\n+In order to run the indexing job, first checkout the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 381}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDIzOTUyOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Now that we have the data in Elasticsearch index we can update the\n          \n          \n            \n            Now that we have the data in Elasticsearch index, we can update the", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500239529", "createdAt": "2020-10-06T12:36:15Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,\n+otherwise the later stages might see it in inconsistent state, causing\n+issues which are hard to debug. We make defensive copies to avoid this.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/JoiningState.java#L11)\n+to see the full source code.\n+\n+### Deleting changes\n+\n+The `ChangeRecord#operation()` method provides information about what\n+kind of operation was performed on the record. If you need to handle\n+deletions you should use it. The Petclinic application doesn\u2019t allow\n+delete operations, so we don\u2019t handle those.\n+\n+### Elastic sink\n+\n+The fourth key part of the pipeline is the Elastic sink. It takes a\n+document and writes it to Elasticsearch. You as a developer need to\n+provide information where the Elastic instance is running, what index to\n+write to and how to convert the `Document` into one of `IndexRequest`,\n+`UpdateRequest` or `DeleteRequest`. The Sink then takes care of batching\n+the requests and parallelizing for higher performance, retries in case\n+of network issues.\n+\n+### The Pipeline\n+\n+The final pipeline composes all the individual steps in a\n+straightforward way:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .map(PetClinicIndexJob::mapChangeRecordToPOJO).setName(\"mapChangeRecordToPOJO\")\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords).setName(\"enrichWithKeywords\")\n+ .mapStateful(JoiningState::new, JoiningState::join).setName(\"JoiningState::join\")\n+ .writeTo(elasticSink);\n+```\n+\n+Or visualized as a graph:\n+\n+![Pipeline with global mapping state](assets/2020-10-06-global-join-step.png)\n+\n+For the pipeline with two step join see [the source\n+code](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/two-joins/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L87)\n+on Github. This is the graph visualisation of the pipeline:\n+\n+![Pipeline with 2 separate grouped join steps](assets/2020-10-06-two-join-step.png)\n+\n+## Testing the job\n+\n+In order to test the job, we need to create a test database identical to\n+the Petclinic database.\n+\n+We have the following options\n+\n+* Start the pet clinic application inside the same JVM - the Petclinic\n+is a Spring Boot application, running it in-process would be\n+relatively easy, but it represents a legacy application, which is\n+usually quite difficult to run in such way. Also, there is a high\n+probability of dependency clashes, which would make this difficult.\n+\n+* Start the pet clinic application in a Docker container - it is likely\n+that the application is not containerized, we would need to create\n+such an image only for the tests.\n+\n+* Create the database and data using SQL directly - reusing the SQL\n+scripts from the Petclinic application and running inserts via JDBC\n+seems the easiest way to do this so we will continue with this approach.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L121)\n+to see the full source code.\n+\n+## Running the Job\n+\n+In order to run the indexing job, first checkout the\n+[pet-clinic-indexing-job](https://github.com/hazelcast-demos/pet-clinic-index-job/)\n+repository\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/pet-clinic-index-job.git\n+```\n+\n+Then build the JAR containing to job\n+\n+```bash\n+cd pet-clinic-indexing-job\n+mvn package\n+```\n+\n+And finally submit the job by running the following command:\n+\n+```bash\n+bin/jet submit \\\n+path/to/pet-clinic-index-job/target/pet-clinic-index-job-1.0-SNAPSHOT-jar-with-dependencies.jar\\\n+   --database-address localhost \\\n+   --database-port 3306 \\\n+   --database-user petclinic \\\n+   --database-password petclinic \\\n+   --elastic-host localhost:9200 \\\n+   --elastic-index petclinic-index\n+```\n+\n+## Petclinic application update\n+\n+Now that we have the data in Elasticsearch index we can update the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 411}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI0MDEzMA==", "bodyText": "There are several changes in the application, but the most important\nchanges are:\n\nCreate a search service to search for the data\nUpdate the search endpoint to use the new search service", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500240130", "createdAt": "2020-10-06T12:37:14Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,\n+otherwise the later stages might see it in inconsistent state, causing\n+issues which are hard to debug. We make defensive copies to avoid this.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/JoiningState.java#L11)\n+to see the full source code.\n+\n+### Deleting changes\n+\n+The `ChangeRecord#operation()` method provides information about what\n+kind of operation was performed on the record. If you need to handle\n+deletions you should use it. The Petclinic application doesn\u2019t allow\n+delete operations, so we don\u2019t handle those.\n+\n+### Elastic sink\n+\n+The fourth key part of the pipeline is the Elastic sink. It takes a\n+document and writes it to Elasticsearch. You as a developer need to\n+provide information where the Elastic instance is running, what index to\n+write to and how to convert the `Document` into one of `IndexRequest`,\n+`UpdateRequest` or `DeleteRequest`. The Sink then takes care of batching\n+the requests and parallelizing for higher performance, retries in case\n+of network issues.\n+\n+### The Pipeline\n+\n+The final pipeline composes all the individual steps in a\n+straightforward way:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .map(PetClinicIndexJob::mapChangeRecordToPOJO).setName(\"mapChangeRecordToPOJO\")\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords).setName(\"enrichWithKeywords\")\n+ .mapStateful(JoiningState::new, JoiningState::join).setName(\"JoiningState::join\")\n+ .writeTo(elasticSink);\n+```\n+\n+Or visualized as a graph:\n+\n+![Pipeline with global mapping state](assets/2020-10-06-global-join-step.png)\n+\n+For the pipeline with two step join see [the source\n+code](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/two-joins/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L87)\n+on Github. This is the graph visualisation of the pipeline:\n+\n+![Pipeline with 2 separate grouped join steps](assets/2020-10-06-two-join-step.png)\n+\n+## Testing the job\n+\n+In order to test the job, we need to create a test database identical to\n+the Petclinic database.\n+\n+We have the following options\n+\n+* Start the pet clinic application inside the same JVM - the Petclinic\n+is a Spring Boot application, running it in-process would be\n+relatively easy, but it represents a legacy application, which is\n+usually quite difficult to run in such way. Also, there is a high\n+probability of dependency clashes, which would make this difficult.\n+\n+* Start the pet clinic application in a Docker container - it is likely\n+that the application is not containerized, we would need to create\n+such an image only for the tests.\n+\n+* Create the database and data using SQL directly - reusing the SQL\n+scripts from the Petclinic application and running inserts via JDBC\n+seems the easiest way to do this so we will continue with this approach.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L121)\n+to see the full source code.\n+\n+## Running the Job\n+\n+In order to run the indexing job, first checkout the\n+[pet-clinic-indexing-job](https://github.com/hazelcast-demos/pet-clinic-index-job/)\n+repository\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/pet-clinic-index-job.git\n+```\n+\n+Then build the JAR containing to job\n+\n+```bash\n+cd pet-clinic-indexing-job\n+mvn package\n+```\n+\n+And finally submit the job by running the following command:\n+\n+```bash\n+bin/jet submit \\\n+path/to/pet-clinic-index-job/target/pet-clinic-index-job-1.0-SNAPSHOT-jar-with-dependencies.jar\\\n+   --database-address localhost \\\n+   --database-port 3306 \\\n+   --database-user petclinic \\\n+   --database-password petclinic \\\n+   --elastic-host localhost:9200 \\\n+   --elastic-index petclinic-index\n+```\n+\n+## Petclinic application update\n+\n+Now that we have the data in Elasticsearch index we can update the\n+Petclinic application to use it for search.\n+\n+There are several changes in the application, but the most important\n+changes are Create a search service to search for the data Update the\n+search endpoint to use the new search service", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 416}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI0MDU0Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            best-matching documents is returned from Elastic and loaded by id from\n          \n          \n            \n            best-matching documents is returned from Elastic and loaded by ID from", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500240543", "createdAt": "2020-10-06T12:37:56Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,\n+otherwise the later stages might see it in inconsistent state, causing\n+issues which are hard to debug. We make defensive copies to avoid this.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/JoiningState.java#L11)\n+to see the full source code.\n+\n+### Deleting changes\n+\n+The `ChangeRecord#operation()` method provides information about what\n+kind of operation was performed on the record. If you need to handle\n+deletions you should use it. The Petclinic application doesn\u2019t allow\n+delete operations, so we don\u2019t handle those.\n+\n+### Elastic sink\n+\n+The fourth key part of the pipeline is the Elastic sink. It takes a\n+document and writes it to Elasticsearch. You as a developer need to\n+provide information where the Elastic instance is running, what index to\n+write to and how to convert the `Document` into one of `IndexRequest`,\n+`UpdateRequest` or `DeleteRequest`. The Sink then takes care of batching\n+the requests and parallelizing for higher performance, retries in case\n+of network issues.\n+\n+### The Pipeline\n+\n+The final pipeline composes all the individual steps in a\n+straightforward way:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .map(PetClinicIndexJob::mapChangeRecordToPOJO).setName(\"mapChangeRecordToPOJO\")\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords).setName(\"enrichWithKeywords\")\n+ .mapStateful(JoiningState::new, JoiningState::join).setName(\"JoiningState::join\")\n+ .writeTo(elasticSink);\n+```\n+\n+Or visualized as a graph:\n+\n+![Pipeline with global mapping state](assets/2020-10-06-global-join-step.png)\n+\n+For the pipeline with two step join see [the source\n+code](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/two-joins/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L87)\n+on Github. This is the graph visualisation of the pipeline:\n+\n+![Pipeline with 2 separate grouped join steps](assets/2020-10-06-two-join-step.png)\n+\n+## Testing the job\n+\n+In order to test the job, we need to create a test database identical to\n+the Petclinic database.\n+\n+We have the following options\n+\n+* Start the pet clinic application inside the same JVM - the Petclinic\n+is a Spring Boot application, running it in-process would be\n+relatively easy, but it represents a legacy application, which is\n+usually quite difficult to run in such way. Also, there is a high\n+probability of dependency clashes, which would make this difficult.\n+\n+* Start the pet clinic application in a Docker container - it is likely\n+that the application is not containerized, we would need to create\n+such an image only for the tests.\n+\n+* Create the database and data using SQL directly - reusing the SQL\n+scripts from the Petclinic application and running inserts via JDBC\n+seems the easiest way to do this so we will continue with this approach.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L121)\n+to see the full source code.\n+\n+## Running the Job\n+\n+In order to run the indexing job, first checkout the\n+[pet-clinic-indexing-job](https://github.com/hazelcast-demos/pet-clinic-index-job/)\n+repository\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/pet-clinic-index-job.git\n+```\n+\n+Then build the JAR containing to job\n+\n+```bash\n+cd pet-clinic-indexing-job\n+mvn package\n+```\n+\n+And finally submit the job by running the following command:\n+\n+```bash\n+bin/jet submit \\\n+path/to/pet-clinic-index-job/target/pet-clinic-index-job-1.0-SNAPSHOT-jar-with-dependencies.jar\\\n+   --database-address localhost \\\n+   --database-port 3306 \\\n+   --database-user petclinic \\\n+   --database-password petclinic \\\n+   --elastic-host localhost:9200 \\\n+   --elastic-index petclinic-index\n+```\n+\n+## Petclinic application update\n+\n+Now that we have the data in Elasticsearch index we can update the\n+Petclinic application to use it for search.\n+\n+There are several changes in the application, but the most important\n+changes are Create a search service to search for the data Update the\n+search endpoint to use the new search service\n+\n+The following snippet shows the search method in the SearchService. It\n+uses the Elasticsearch client directly, but one could use Spring Data\n+Elasticsearch as well.\n+\n+```java\n+public Collection<Integer> search(String query) {\n+  SearchRequest searchRequest = new SearchRequest(index);\n+  searchRequest.source().fetchSource(true).query(QueryBuilders.wildcardQuery(searchField, query + \"*\"));\n+\n+  try {\n+     SearchResponse response = client.search(searchRequest, RequestOptions.DEFAULT);\n+\n+     SearchHits hits = response.getHits();\n+     return Arrays.stream(hits.getHits()).map(hit -> (Integer) hit.getSourceAsMap().get(\"ownerId\"))\n+           .collect(toList());\n+  }\n+  catch (IOException e) {\n+     throw new RuntimeException(e);\n+  }\n+}\n+```\n+\n+The search endpoint will use the SearchService to search for the data.\n+After retrieving the results the Owner entities need to be loaded from\n+the database. This is not an issue as only a small subset of\n+best-matching documents is returned from Elastic and loaded by id from", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 443}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI0MDY3MA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            There are some more minor changes to the view, and the controller, you\n          \n          \n            \n            There are some more minor changes to the view and the controller, you", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500240670", "createdAt": "2020-10-06T12:38:08Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,\n+otherwise the later stages might see it in inconsistent state, causing\n+issues which are hard to debug. We make defensive copies to avoid this.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/JoiningState.java#L11)\n+to see the full source code.\n+\n+### Deleting changes\n+\n+The `ChangeRecord#operation()` method provides information about what\n+kind of operation was performed on the record. If you need to handle\n+deletions you should use it. The Petclinic application doesn\u2019t allow\n+delete operations, so we don\u2019t handle those.\n+\n+### Elastic sink\n+\n+The fourth key part of the pipeline is the Elastic sink. It takes a\n+document and writes it to Elasticsearch. You as a developer need to\n+provide information where the Elastic instance is running, what index to\n+write to and how to convert the `Document` into one of `IndexRequest`,\n+`UpdateRequest` or `DeleteRequest`. The Sink then takes care of batching\n+the requests and parallelizing for higher performance, retries in case\n+of network issues.\n+\n+### The Pipeline\n+\n+The final pipeline composes all the individual steps in a\n+straightforward way:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .map(PetClinicIndexJob::mapChangeRecordToPOJO).setName(\"mapChangeRecordToPOJO\")\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords).setName(\"enrichWithKeywords\")\n+ .mapStateful(JoiningState::new, JoiningState::join).setName(\"JoiningState::join\")\n+ .writeTo(elasticSink);\n+```\n+\n+Or visualized as a graph:\n+\n+![Pipeline with global mapping state](assets/2020-10-06-global-join-step.png)\n+\n+For the pipeline with two step join see [the source\n+code](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/two-joins/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L87)\n+on Github. This is the graph visualisation of the pipeline:\n+\n+![Pipeline with 2 separate grouped join steps](assets/2020-10-06-two-join-step.png)\n+\n+## Testing the job\n+\n+In order to test the job, we need to create a test database identical to\n+the Petclinic database.\n+\n+We have the following options\n+\n+* Start the pet clinic application inside the same JVM - the Petclinic\n+is a Spring Boot application, running it in-process would be\n+relatively easy, but it represents a legacy application, which is\n+usually quite difficult to run in such way. Also, there is a high\n+probability of dependency clashes, which would make this difficult.\n+\n+* Start the pet clinic application in a Docker container - it is likely\n+that the application is not containerized, we would need to create\n+such an image only for the tests.\n+\n+* Create the database and data using SQL directly - reusing the SQL\n+scripts from the Petclinic application and running inserts via JDBC\n+seems the easiest way to do this so we will continue with this approach.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L121)\n+to see the full source code.\n+\n+## Running the Job\n+\n+In order to run the indexing job, first checkout the\n+[pet-clinic-indexing-job](https://github.com/hazelcast-demos/pet-clinic-index-job/)\n+repository\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/pet-clinic-index-job.git\n+```\n+\n+Then build the JAR containing to job\n+\n+```bash\n+cd pet-clinic-indexing-job\n+mvn package\n+```\n+\n+And finally submit the job by running the following command:\n+\n+```bash\n+bin/jet submit \\\n+path/to/pet-clinic-index-job/target/pet-clinic-index-job-1.0-SNAPSHOT-jar-with-dependencies.jar\\\n+   --database-address localhost \\\n+   --database-port 3306 \\\n+   --database-user petclinic \\\n+   --database-password petclinic \\\n+   --elastic-host localhost:9200 \\\n+   --elastic-index petclinic-index\n+```\n+\n+## Petclinic application update\n+\n+Now that we have the data in Elasticsearch index we can update the\n+Petclinic application to use it for search.\n+\n+There are several changes in the application, but the most important\n+changes are Create a search service to search for the data Update the\n+search endpoint to use the new search service\n+\n+The following snippet shows the search method in the SearchService. It\n+uses the Elasticsearch client directly, but one could use Spring Data\n+Elasticsearch as well.\n+\n+```java\n+public Collection<Integer> search(String query) {\n+  SearchRequest searchRequest = new SearchRequest(index);\n+  searchRequest.source().fetchSource(true).query(QueryBuilders.wildcardQuery(searchField, query + \"*\"));\n+\n+  try {\n+     SearchResponse response = client.search(searchRequest, RequestOptions.DEFAULT);\n+\n+     SearchHits hits = response.getHits();\n+     return Arrays.stream(hits.getHits()).map(hit -> (Integer) hit.getSourceAsMap().get(\"ownerId\"))\n+           .collect(toList());\n+  }\n+  catch (IOException e) {\n+     throw new RuntimeException(e);\n+  }\n+}\n+```\n+\n+The search endpoint will use the SearchService to search for the data.\n+After retrieving the results the Owner entities need to be loaded from\n+the database. This is not an issue as only a small subset of\n+best-matching documents is returned from Elastic and loaded by id from\n+the database.\n+\n+```java\n+Collection<Integer> ownerIds = searchService.search(searchForm.getQuery());\n+// find owners by last name\n+Collection<Owner> results = this.owners.findByIds(ownerIds);\n+```\n+\n+There are some more minor changes to the view, and the controller, you", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 452}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI0MDc2Mw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            Checkout the `elasticsearch` branch with the changes and restart the\n          \n          \n            \n            Check out the `elasticsearch` branch with the changes and restart the", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500240763", "createdAt": "2020-10-06T12:38:19Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,\n+otherwise the later stages might see it in inconsistent state, causing\n+issues which are hard to debug. We make defensive copies to avoid this.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/JoiningState.java#L11)\n+to see the full source code.\n+\n+### Deleting changes\n+\n+The `ChangeRecord#operation()` method provides information about what\n+kind of operation was performed on the record. If you need to handle\n+deletions you should use it. The Petclinic application doesn\u2019t allow\n+delete operations, so we don\u2019t handle those.\n+\n+### Elastic sink\n+\n+The fourth key part of the pipeline is the Elastic sink. It takes a\n+document and writes it to Elasticsearch. You as a developer need to\n+provide information where the Elastic instance is running, what index to\n+write to and how to convert the `Document` into one of `IndexRequest`,\n+`UpdateRequest` or `DeleteRequest`. The Sink then takes care of batching\n+the requests and parallelizing for higher performance, retries in case\n+of network issues.\n+\n+### The Pipeline\n+\n+The final pipeline composes all the individual steps in a\n+straightforward way:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .map(PetClinicIndexJob::mapChangeRecordToPOJO).setName(\"mapChangeRecordToPOJO\")\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords).setName(\"enrichWithKeywords\")\n+ .mapStateful(JoiningState::new, JoiningState::join).setName(\"JoiningState::join\")\n+ .writeTo(elasticSink);\n+```\n+\n+Or visualized as a graph:\n+\n+![Pipeline with global mapping state](assets/2020-10-06-global-join-step.png)\n+\n+For the pipeline with two step join see [the source\n+code](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/two-joins/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L87)\n+on Github. This is the graph visualisation of the pipeline:\n+\n+![Pipeline with 2 separate grouped join steps](assets/2020-10-06-two-join-step.png)\n+\n+## Testing the job\n+\n+In order to test the job, we need to create a test database identical to\n+the Petclinic database.\n+\n+We have the following options\n+\n+* Start the pet clinic application inside the same JVM - the Petclinic\n+is a Spring Boot application, running it in-process would be\n+relatively easy, but it represents a legacy application, which is\n+usually quite difficult to run in such way. Also, there is a high\n+probability of dependency clashes, which would make this difficult.\n+\n+* Start the pet clinic application in a Docker container - it is likely\n+that the application is not containerized, we would need to create\n+such an image only for the tests.\n+\n+* Create the database and data using SQL directly - reusing the SQL\n+scripts from the Petclinic application and running inserts via JDBC\n+seems the easiest way to do this so we will continue with this approach.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L121)\n+to see the full source code.\n+\n+## Running the Job\n+\n+In order to run the indexing job, first checkout the\n+[pet-clinic-indexing-job](https://github.com/hazelcast-demos/pet-clinic-index-job/)\n+repository\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/pet-clinic-index-job.git\n+```\n+\n+Then build the JAR containing to job\n+\n+```bash\n+cd pet-clinic-indexing-job\n+mvn package\n+```\n+\n+And finally submit the job by running the following command:\n+\n+```bash\n+bin/jet submit \\\n+path/to/pet-clinic-index-job/target/pet-clinic-index-job-1.0-SNAPSHOT-jar-with-dependencies.jar\\\n+   --database-address localhost \\\n+   --database-port 3306 \\\n+   --database-user petclinic \\\n+   --database-password petclinic \\\n+   --elastic-host localhost:9200 \\\n+   --elastic-index petclinic-index\n+```\n+\n+## Petclinic application update\n+\n+Now that we have the data in Elasticsearch index we can update the\n+Petclinic application to use it for search.\n+\n+There are several changes in the application, but the most important\n+changes are Create a search service to search for the data Update the\n+search endpoint to use the new search service\n+\n+The following snippet shows the search method in the SearchService. It\n+uses the Elasticsearch client directly, but one could use Spring Data\n+Elasticsearch as well.\n+\n+```java\n+public Collection<Integer> search(String query) {\n+  SearchRequest searchRequest = new SearchRequest(index);\n+  searchRequest.source().fetchSource(true).query(QueryBuilders.wildcardQuery(searchField, query + \"*\"));\n+\n+  try {\n+     SearchResponse response = client.search(searchRequest, RequestOptions.DEFAULT);\n+\n+     SearchHits hits = response.getHits();\n+     return Arrays.stream(hits.getHits()).map(hit -> (Integer) hit.getSourceAsMap().get(\"ownerId\"))\n+           .collect(toList());\n+  }\n+  catch (IOException e) {\n+     throw new RuntimeException(e);\n+  }\n+}\n+```\n+\n+The search endpoint will use the SearchService to search for the data.\n+After retrieving the results the Owner entities need to be loaded from\n+the database. This is not an issue as only a small subset of\n+best-matching documents is returned from Elastic and loaded by id from\n+the database.\n+\n+```java\n+Collection<Integer> ownerIds = searchService.search(searchForm.getQuery());\n+// find owners by last name\n+Collection<Owner> results = this.owners.findByIds(ownerIds);\n+```\n+\n+There are some more minor changes to the view, and the controller, you\n+can see all the changes in [this\n+commit](https://github.com/spring-projects/hazelcast-demos/compare/main...elasticsearch).\n+\n+Checkout the `elasticsearch` branch with the changes and restart the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 456}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI0MDg3NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            application by running the maven command again:\n          \n          \n            \n            application by running the Maven command again:", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500240874", "createdAt": "2020-10-06T12:38:29Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,\n+otherwise the later stages might see it in inconsistent state, causing\n+issues which are hard to debug. We make defensive copies to avoid this.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/JoiningState.java#L11)\n+to see the full source code.\n+\n+### Deleting changes\n+\n+The `ChangeRecord#operation()` method provides information about what\n+kind of operation was performed on the record. If you need to handle\n+deletions you should use it. The Petclinic application doesn\u2019t allow\n+delete operations, so we don\u2019t handle those.\n+\n+### Elastic sink\n+\n+The fourth key part of the pipeline is the Elastic sink. It takes a\n+document and writes it to Elasticsearch. You as a developer need to\n+provide information where the Elastic instance is running, what index to\n+write to and how to convert the `Document` into one of `IndexRequest`,\n+`UpdateRequest` or `DeleteRequest`. The Sink then takes care of batching\n+the requests and parallelizing for higher performance, retries in case\n+of network issues.\n+\n+### The Pipeline\n+\n+The final pipeline composes all the individual steps in a\n+straightforward way:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .map(PetClinicIndexJob::mapChangeRecordToPOJO).setName(\"mapChangeRecordToPOJO\")\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords).setName(\"enrichWithKeywords\")\n+ .mapStateful(JoiningState::new, JoiningState::join).setName(\"JoiningState::join\")\n+ .writeTo(elasticSink);\n+```\n+\n+Or visualized as a graph:\n+\n+![Pipeline with global mapping state](assets/2020-10-06-global-join-step.png)\n+\n+For the pipeline with two step join see [the source\n+code](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/two-joins/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L87)\n+on Github. This is the graph visualisation of the pipeline:\n+\n+![Pipeline with 2 separate grouped join steps](assets/2020-10-06-two-join-step.png)\n+\n+## Testing the job\n+\n+In order to test the job, we need to create a test database identical to\n+the Petclinic database.\n+\n+We have the following options\n+\n+* Start the pet clinic application inside the same JVM - the Petclinic\n+is a Spring Boot application, running it in-process would be\n+relatively easy, but it represents a legacy application, which is\n+usually quite difficult to run in such way. Also, there is a high\n+probability of dependency clashes, which would make this difficult.\n+\n+* Start the pet clinic application in a Docker container - it is likely\n+that the application is not containerized, we would need to create\n+such an image only for the tests.\n+\n+* Create the database and data using SQL directly - reusing the SQL\n+scripts from the Petclinic application and running inserts via JDBC\n+seems the easiest way to do this so we will continue with this approach.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L121)\n+to see the full source code.\n+\n+## Running the Job\n+\n+In order to run the indexing job, first checkout the\n+[pet-clinic-indexing-job](https://github.com/hazelcast-demos/pet-clinic-index-job/)\n+repository\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/pet-clinic-index-job.git\n+```\n+\n+Then build the JAR containing to job\n+\n+```bash\n+cd pet-clinic-indexing-job\n+mvn package\n+```\n+\n+And finally submit the job by running the following command:\n+\n+```bash\n+bin/jet submit \\\n+path/to/pet-clinic-index-job/target/pet-clinic-index-job-1.0-SNAPSHOT-jar-with-dependencies.jar\\\n+   --database-address localhost \\\n+   --database-port 3306 \\\n+   --database-user petclinic \\\n+   --database-password petclinic \\\n+   --elastic-host localhost:9200 \\\n+   --elastic-index petclinic-index\n+```\n+\n+## Petclinic application update\n+\n+Now that we have the data in Elasticsearch index we can update the\n+Petclinic application to use it for search.\n+\n+There are several changes in the application, but the most important\n+changes are Create a search service to search for the data Update the\n+search endpoint to use the new search service\n+\n+The following snippet shows the search method in the SearchService. It\n+uses the Elasticsearch client directly, but one could use Spring Data\n+Elasticsearch as well.\n+\n+```java\n+public Collection<Integer> search(String query) {\n+  SearchRequest searchRequest = new SearchRequest(index);\n+  searchRequest.source().fetchSource(true).query(QueryBuilders.wildcardQuery(searchField, query + \"*\"));\n+\n+  try {\n+     SearchResponse response = client.search(searchRequest, RequestOptions.DEFAULT);\n+\n+     SearchHits hits = response.getHits();\n+     return Arrays.stream(hits.getHits()).map(hit -> (Integer) hit.getSourceAsMap().get(\"ownerId\"))\n+           .collect(toList());\n+  }\n+  catch (IOException e) {\n+     throw new RuntimeException(e);\n+  }\n+}\n+```\n+\n+The search endpoint will use the SearchService to search for the data.\n+After retrieving the results the Owner entities need to be loaded from\n+the database. This is not an issue as only a small subset of\n+best-matching documents is returned from Elastic and loaded by id from\n+the database.\n+\n+```java\n+Collection<Integer> ownerIds = searchService.search(searchForm.getQuery());\n+// find owners by last name\n+Collection<Owner> results = this.owners.findByIds(ownerIds);\n+```\n+\n+There are some more minor changes to the view, and the controller, you\n+can see all the changes in [this\n+commit](https://github.com/spring-projects/hazelcast-demos/compare/main...elasticsearch).\n+\n+Checkout the `elasticsearch` branch with the changes and restart the\n+application by running the maven command again:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 457}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI0MTA3MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            in a first name, other in a name of a pet:\n          \n          \n            \n            in a first name, other in the name of a pet:", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500241071", "createdAt": "2020-10-06T12:38:46Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,\n+otherwise the later stages might see it in inconsistent state, causing\n+issues which are hard to debug. We make defensive copies to avoid this.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/JoiningState.java#L11)\n+to see the full source code.\n+\n+### Deleting changes\n+\n+The `ChangeRecord#operation()` method provides information about what\n+kind of operation was performed on the record. If you need to handle\n+deletions you should use it. The Petclinic application doesn\u2019t allow\n+delete operations, so we don\u2019t handle those.\n+\n+### Elastic sink\n+\n+The fourth key part of the pipeline is the Elastic sink. It takes a\n+document and writes it to Elasticsearch. You as a developer need to\n+provide information where the Elastic instance is running, what index to\n+write to and how to convert the `Document` into one of `IndexRequest`,\n+`UpdateRequest` or `DeleteRequest`. The Sink then takes care of batching\n+the requests and parallelizing for higher performance, retries in case\n+of network issues.\n+\n+### The Pipeline\n+\n+The final pipeline composes all the individual steps in a\n+straightforward way:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .map(PetClinicIndexJob::mapChangeRecordToPOJO).setName(\"mapChangeRecordToPOJO\")\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords).setName(\"enrichWithKeywords\")\n+ .mapStateful(JoiningState::new, JoiningState::join).setName(\"JoiningState::join\")\n+ .writeTo(elasticSink);\n+```\n+\n+Or visualized as a graph:\n+\n+![Pipeline with global mapping state](assets/2020-10-06-global-join-step.png)\n+\n+For the pipeline with two step join see [the source\n+code](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/two-joins/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L87)\n+on Github. This is the graph visualisation of the pipeline:\n+\n+![Pipeline with 2 separate grouped join steps](assets/2020-10-06-two-join-step.png)\n+\n+## Testing the job\n+\n+In order to test the job, we need to create a test database identical to\n+the Petclinic database.\n+\n+We have the following options\n+\n+* Start the pet clinic application inside the same JVM - the Petclinic\n+is a Spring Boot application, running it in-process would be\n+relatively easy, but it represents a legacy application, which is\n+usually quite difficult to run in such way. Also, there is a high\n+probability of dependency clashes, which would make this difficult.\n+\n+* Start the pet clinic application in a Docker container - it is likely\n+that the application is not containerized, we would need to create\n+such an image only for the tests.\n+\n+* Create the database and data using SQL directly - reusing the SQL\n+scripts from the Petclinic application and running inserts via JDBC\n+seems the easiest way to do this so we will continue with this approach.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L121)\n+to see the full source code.\n+\n+## Running the Job\n+\n+In order to run the indexing job, first checkout the\n+[pet-clinic-indexing-job](https://github.com/hazelcast-demos/pet-clinic-index-job/)\n+repository\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/pet-clinic-index-job.git\n+```\n+\n+Then build the JAR containing to job\n+\n+```bash\n+cd pet-clinic-indexing-job\n+mvn package\n+```\n+\n+And finally submit the job by running the following command:\n+\n+```bash\n+bin/jet submit \\\n+path/to/pet-clinic-index-job/target/pet-clinic-index-job-1.0-SNAPSHOT-jar-with-dependencies.jar\\\n+   --database-address localhost \\\n+   --database-port 3306 \\\n+   --database-user petclinic \\\n+   --database-password petclinic \\\n+   --elastic-host localhost:9200 \\\n+   --elastic-index petclinic-index\n+```\n+\n+## Petclinic application update\n+\n+Now that we have the data in Elasticsearch index we can update the\n+Petclinic application to use it for search.\n+\n+There are several changes in the application, but the most important\n+changes are Create a search service to search for the data Update the\n+search endpoint to use the new search service\n+\n+The following snippet shows the search method in the SearchService. It\n+uses the Elasticsearch client directly, but one could use Spring Data\n+Elasticsearch as well.\n+\n+```java\n+public Collection<Integer> search(String query) {\n+  SearchRequest searchRequest = new SearchRequest(index);\n+  searchRequest.source().fetchSource(true).query(QueryBuilders.wildcardQuery(searchField, query + \"*\"));\n+\n+  try {\n+     SearchResponse response = client.search(searchRequest, RequestOptions.DEFAULT);\n+\n+     SearchHits hits = response.getHits();\n+     return Arrays.stream(hits.getHits()).map(hit -> (Integer) hit.getSourceAsMap().get(\"ownerId\"))\n+           .collect(toList());\n+  }\n+  catch (IOException e) {\n+     throw new RuntimeException(e);\n+  }\n+}\n+```\n+\n+The search endpoint will use the SearchService to search for the data.\n+After retrieving the results the Owner entities need to be loaded from\n+the database. This is not an issue as only a small subset of\n+best-matching documents is returned from Elastic and loaded by id from\n+the database.\n+\n+```java\n+Collection<Integer> ownerIds = searchService.search(searchForm.getQuery());\n+// find owners by last name\n+Collection<Owner> results = this.owners.findByIds(ownerIds);\n+```\n+\n+There are some more minor changes to the view, and the controller, you\n+can see all the changes in [this\n+commit](https://github.com/spring-projects/hazelcast-demos/compare/main...elasticsearch).\n+\n+Checkout the `elasticsearch` branch with the changes and restart the\n+application by running the maven command again:\n+\n+```bash\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+We can verify that searching for \u201cGeorge\u201d returns 2 results, one found\n+in a first name, other in a name of a pet:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 464}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI0MTQ5OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            functionality to the user.\n          \n          \n            \n            search functionality to the user.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500241498", "createdAt": "2020-10-06T12:39:24Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,\n+otherwise the later stages might see it in inconsistent state, causing\n+issues which are hard to debug. We make defensive copies to avoid this.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/JoiningState.java#L11)\n+to see the full source code.\n+\n+### Deleting changes\n+\n+The `ChangeRecord#operation()` method provides information about what\n+kind of operation was performed on the record. If you need to handle\n+deletions you should use it. The Petclinic application doesn\u2019t allow\n+delete operations, so we don\u2019t handle those.\n+\n+### Elastic sink\n+\n+The fourth key part of the pipeline is the Elastic sink. It takes a\n+document and writes it to Elasticsearch. You as a developer need to\n+provide information where the Elastic instance is running, what index to\n+write to and how to convert the `Document` into one of `IndexRequest`,\n+`UpdateRequest` or `DeleteRequest`. The Sink then takes care of batching\n+the requests and parallelizing for higher performance, retries in case\n+of network issues.\n+\n+### The Pipeline\n+\n+The final pipeline composes all the individual steps in a\n+straightforward way:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .map(PetClinicIndexJob::mapChangeRecordToPOJO).setName(\"mapChangeRecordToPOJO\")\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords).setName(\"enrichWithKeywords\")\n+ .mapStateful(JoiningState::new, JoiningState::join).setName(\"JoiningState::join\")\n+ .writeTo(elasticSink);\n+```\n+\n+Or visualized as a graph:\n+\n+![Pipeline with global mapping state](assets/2020-10-06-global-join-step.png)\n+\n+For the pipeline with two step join see [the source\n+code](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/two-joins/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L87)\n+on Github. This is the graph visualisation of the pipeline:\n+\n+![Pipeline with 2 separate grouped join steps](assets/2020-10-06-two-join-step.png)\n+\n+## Testing the job\n+\n+In order to test the job, we need to create a test database identical to\n+the Petclinic database.\n+\n+We have the following options\n+\n+* Start the pet clinic application inside the same JVM - the Petclinic\n+is a Spring Boot application, running it in-process would be\n+relatively easy, but it represents a legacy application, which is\n+usually quite difficult to run in such way. Also, there is a high\n+probability of dependency clashes, which would make this difficult.\n+\n+* Start the pet clinic application in a Docker container - it is likely\n+that the application is not containerized, we would need to create\n+such an image only for the tests.\n+\n+* Create the database and data using SQL directly - reusing the SQL\n+scripts from the Petclinic application and running inserts via JDBC\n+seems the easiest way to do this so we will continue with this approach.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L121)\n+to see the full source code.\n+\n+## Running the Job\n+\n+In order to run the indexing job, first checkout the\n+[pet-clinic-indexing-job](https://github.com/hazelcast-demos/pet-clinic-index-job/)\n+repository\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/pet-clinic-index-job.git\n+```\n+\n+Then build the JAR containing to job\n+\n+```bash\n+cd pet-clinic-indexing-job\n+mvn package\n+```\n+\n+And finally submit the job by running the following command:\n+\n+```bash\n+bin/jet submit \\\n+path/to/pet-clinic-index-job/target/pet-clinic-index-job-1.0-SNAPSHOT-jar-with-dependencies.jar\\\n+   --database-address localhost \\\n+   --database-port 3306 \\\n+   --database-user petclinic \\\n+   --database-password petclinic \\\n+   --elastic-host localhost:9200 \\\n+   --elastic-index petclinic-index\n+```\n+\n+## Petclinic application update\n+\n+Now that we have the data in Elasticsearch index we can update the\n+Petclinic application to use it for search.\n+\n+There are several changes in the application, but the most important\n+changes are Create a search service to search for the data Update the\n+search endpoint to use the new search service\n+\n+The following snippet shows the search method in the SearchService. It\n+uses the Elasticsearch client directly, but one could use Spring Data\n+Elasticsearch as well.\n+\n+```java\n+public Collection<Integer> search(String query) {\n+  SearchRequest searchRequest = new SearchRequest(index);\n+  searchRequest.source().fetchSource(true).query(QueryBuilders.wildcardQuery(searchField, query + \"*\"));\n+\n+  try {\n+     SearchResponse response = client.search(searchRequest, RequestOptions.DEFAULT);\n+\n+     SearchHits hits = response.getHits();\n+     return Arrays.stream(hits.getHits()).map(hit -> (Integer) hit.getSourceAsMap().get(\"ownerId\"))\n+           .collect(toList());\n+  }\n+  catch (IOException e) {\n+     throw new RuntimeException(e);\n+  }\n+}\n+```\n+\n+The search endpoint will use the SearchService to search for the data.\n+After retrieving the results the Owner entities need to be loaded from\n+the database. This is not an issue as only a small subset of\n+best-matching documents is returned from Elastic and loaded by id from\n+the database.\n+\n+```java\n+Collection<Integer> ownerIds = searchService.search(searchForm.getQuery());\n+// find owners by last name\n+Collection<Owner> results = this.owners.findByIds(ownerIds);\n+```\n+\n+There are some more minor changes to the view, and the controller, you\n+can see all the changes in [this\n+commit](https://github.com/spring-projects/hazelcast-demos/compare/main...elasticsearch).\n+\n+Checkout the `elasticsearch` branch with the changes and restart the\n+application by running the maven command again:\n+\n+```bash\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+We can verify that searching for \u201cGeorge\u201d returns 2 results, one found\n+in a first name, other in a name of a pet:\n+\n+![Search \"George\"](assets/2020-10-06-search-george.png)\n+\n+Entering \u201crabies\u201d should bring up a single result (the detail screen is\n+shown in that case) matching a keyword extracted from the description:\n+\n+![Search \"rabbies\"](assets/2020-10-06-search-rabbies.png)\n+\n+## Summary\n+\n+We have shown how to stream changes using CDC, enrich the data,\n+correlate (join) the records with other records and finally store the\n+data into an Elasticsearch index so an application can provide better\n+functionality to the user.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 478}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDI0MTgyMQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            If you would like to discuss this topic with use, drop by at [our\n          \n          \n            \n            If you would like to discuss this topic with us, drop by at [our", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#discussion_r500241821", "createdAt": "2020-10-06T12:39:53Z", "author": {"login": "mtopolnik"}, "path": "site/website/blog/2020-10-06-enabling-full-text-search.md", "diffHunk": "@@ -0,0 +1,492 @@\n+---\n+title: Enabling Full-text Search with Change Data Capture in a Legacy Application\n+description: Debezium based Change Data Capture sources for Hazelcast Jet\n+author: Franti\u0161ek Hartman\n+authorURL: https://www.linkedin.com/in/frantisek-hartman/\n+authorImageURL: https://i.stack.imgur.com/3X7wE.png \n+---\n+\n+In this post, we will take a legacy application and improve its search\n+functionality. We will do this with only a few changes to the\n+application itself - the technique called change data capture  (CDC)\n+allows us to listen for changes in a database and react on these\n+changes&nbsp;- write to a search index in our case. We will also enrich\n+the data before we write it to the search index with a natural language\n+processing (NLP) step that extracts keywords from a text description.\n+\n+We might want to use an external search index for various reasons,\n+especially:\n+\n+* Providing more feature-rich functionality than full-text search in a\n+relational database.\n+\n+* It allows us to scale the search independently of the database.\n+\n+* Add more functionality (keyword extraction) to the application without\n+modifying it - making changes to a legacy application is sometimes\n+risky, or time-consuming, so taking this approach might be faster,\n+therefore cheaper.  \n+\n+In this tutorial we will do the following:\n+\n+* Use Hazelcast Jet - an open-source stream processing system - and it\u2019s\n+CDC module to read changes made to the application database for\n+further processing in a Jet pipeline\n+\n+* In the Jet pipeline, we will enrich the data with an NLP mapping step\n+\n+* Write the results to an Elasticsearch index, using an Elasticsearch\n+connector, [released in Jet 4.2](/blog/2020/07/14/jet-42-is-released).\n+\n+![Architecture](assets/2020-10-06-architecture.png)\n+\n+## The Application\n+\n+We chose the [Spring\n+PetClinic](https://github.com/spring-projects/spring-petclinic)\n+application. It is relatively well-known to many Java developers and is\n+rather similar to a typical enterprise application.\n+\n+The application provides a management tool for managing pets, their\n+owners, and visits to vets. It allows searching for owners, but only\n+using the owner\u2019s last name. We will make the search more feature-rich,\n+allowing us to search on first name, last name, pet names and keywords\n+extracted from descriptions of the visits.\n+\n+First, let\u2019s start a vanilla version of the application. Run the\n+following command to start MySQL database inside a Docker container, we\n+use the official MySQL image, which allows us to create a database for\n+the pet clinic application easily:\n+\n+```bash\n+docker run --name petclinic-mysql -it \\\n+  -e MYSQL_DATABASE=petclinic \\\n+  -e MYSQL_USER=petclinic \\\n+  -e MYSQL_PASSWORD=petclinic \\\n+  -e MYSQL_ROOT_PASSWORD=mysql \\\n+  -p 3306:3306 mysql\n+```\n+\n+Clone the pet clinic application source code from\n+[Github](https://github.com/hazelcast-demos/spring-petclinic) (this is a\n+fork of the official [Spring PetClinic\n+repository](https://github.com/spring-projects/spring-petclinic) with a\n+branch containing the changes we will make later for your convenience).\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/spring-petclinic.git\n+```\n+\n+Start the pet clinic application from the command line by running the\n+following command:\n+\n+```bash\n+cd spring-petclinic\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+And wait for the following log message:\n+\n+```text\n+2020-09-30 16:17:04.113  INFO 24847 --- [  restartedMain] o.s.s.petclinic.PetClinicApplication     : Started PetClinicApplication in 3.177 seconds (JVM running for 3.5)\n+```\n+\n+Navigate to [http://localhost:8080/](http://localhost:8080/)\n+\n+-> Click Find Owners in top-level menu\n+\n+-> Click Find Owner button to list all data\n+\n+and you should see a screen similar to the following:\n+\n+![Spring PetClinic Sample Application](assets/2020-10-06-spring-petclinic-list.png)\n+\n+## Prepare for Change Data Capture\n+\n+In order to stream the changes from the MySQL database we need to modify\n+certain settings and grant more permissions to the `petclinic` user\n+\n+Run the following command to start MySQL client, enter the `mysql`\n+password for the root user.\n+\n+```bash\n+docker run -it --rm --link petclinic-mysql:petclinic-mysql mysql mysql -hpetclinic-mysql -uroot -p\n+```\n+\n+We need the following to grant privileges to the `petclinic` user to allow\n+listening to the database changes\n+\n+```sql\n+ALTER USER petclinic IDENTIFIED WITH mysql_native_password BY 'petclinic';\n+GRANT RELOAD ON *.* TO 'petclinic';\n+GRANT REPLICATION CLIENT ON *.* TO 'petclinic';\n+GRANT REPLICATION SLAVE ON *.* TO 'petclinic';\n+```\n+\n+The `ALTER USER` command changes the default authentication method to\n+one supported by the Jet CDC connector. The `GRANT` commands allow the\n+petclinic user to stream changes from the database in the same way as\n+during master-slave replication.\n+\n+## Start Elasticsearch\n+\n+We want to enable full-text search across multiple fields, but not all\n+fields. With Elasticseach we need to create an index mapping that\n+copies fields into a single field, the field is then used for searching.\n+\n+Run the following command to start Elasticsearch inside a docker\n+container:\n+\n+```bash\n+docker run --name petclinic-elastic \\\n+  -e discovery.type=single-node \\\n+  -e cluster.routing.allocation.disk.threshold_enabled=false \\\n+  -p9200:9200 elasticsearch:7.9.2\n+```\n+\n+Create an Elasticsearch index mapping by running the following command:\n+\n+```bash\n+curl -XPUT -H \"Content-type: application/json\" -d '\n+{\n+  \"mappings\": {\n+    \"properties\": {\n+      \"first_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"last_name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.name\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"pets.visits.keywords\": {\n+        \"type\": \"text\",\n+        \"copy_to\": \"search\"\n+      },\n+      \"search\": {\n+        \"type\": \"text\"\n+      }\n+    }\n+  }\n+}' http://localhost:9200/petclinic-index\n+```\n+\n+The default setting of Elasticsearch analyzer is suitable for us. For\n+more elaborate configuration of the text analysis, you should look into\n+the Elasticsearch documentation or some other source.\n+\n+## Hazelcast Jet Job\n+\n+Now that we have the Petclinic and Elastic set-up and working we can\n+start the data pump - a Jet job reading change events from MySQL\n+database and writing into the Elastic index.\n+\n+A Jet job is a pipeline of steps, that read, modify, aggregate or store\n+data items. The job definition is written in Java and packaged as a jar\n+file. The jar file is deployed to a Jet cluster, which takes care of the\n+execution, scaling, fail-over and other operational aspects.\n+\n+The job consists of 4 main parts:\n+\n+* CDC Source connecting to MySQL database\n+\n+* A mapping step running the keyword extraction\n+\n+* A joining step reconstructing the document from change records from\n+* different tables\n+\n+* Elasticsearch sink\n+\n+### CDC Source\n+\n+The configuration of the source is straightforward, just set the required\n+parameters:\n+\n+```java\n+StreamSource<ChangeRecord> source = MySqlCdcSources\n+    .mysql(\"mysql\")\n+    .setDatabaseAddress(databaseAddress)\n+    .setDatabasePort(databasePort)\n+    .setDatabaseUser(databaseUser)\n+    .setDatabasePassword(databasePassword)\n+    .setClusterName(clusterName)\n+    .setDatabaseWhitelist(\"petclinic\")\n+    .setTableWhitelist(\"petclinic.owners\", \"petclinic.pets\", \"petclinic.visits\")\n+    .build();\n+```\n+\n+Use the source to read change events from MySQL into the pipeline:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+StreamStage<ChangeRecord> allRecords = p.readFrom(mysqlSource)\n+                                        .withoutTimestamps();\n+```\n+\n+### Keyword extraction\n+\n+The next step in the pipeline is to extract the keywords from the visit\n+description. We will use Rapid Automatic Keyword Extraction (RAKE)\n+algorithm, the implementation we use was originally published on\n+[Github](https://github.com/Linguistic/rake). The implementation is not\n+important for the demonstration, you could use any other Java library,\n+call 3rd party service, e.g. via [grpc](/docs/how-tos/grpc) or use our\n+[python integration](/docs/tutorials/python).\n+\n+```java\n+// Create factory for keyword service\n+ServiceFactory<?, Rake> keywordService = ServiceFactories.sharedService((context) -> new Rake(\"en\"));\n+\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords)\n+```\n+\n+The method `enrichWithKeywords` simply passes the description field from\n+the change record to the keyword extraction service and sets the results\n+into the keywords field of the Visit instance.\n+\n+It is a common pattern to enrich an item in a pipeline with more\n+information, Hazelcast Jet works with POJOs in the pipeline, so you need\n+to somehow compose the original item and the enriching information. One\n+option is to use tuples. This works for a small number of fields. Other\n+option would be to use a domain-specific object holding all the\n+information - e.g. the `keywords` field. If you are on Java 14 or newer\n+you might want to give the [Records preview\n+feature](https://openjdk.java.net/jeps/359) a try.\n+\n+### Joining\n+\n+Change data capture sends changes to the database as individual records\n+(represented by\n+[ChangeRecord](https://jet-start.sh/javadoc/4.2/com/hazelcast/jet/cdc/ChangeRecord.html).\n+Each record represents a changed row in a database table. If a single\n+row is all that you need then you are good to go. You can see such\n+example in our [Evergreen Cache blog\n+post](/blog/2020/07/16/designing-evergreen-cache-cdc). But having a\n+single row to work with is not always the case - often you need to\n+aggregate either multiple records from the same table or join records\n+from different tables into a single aggregate record.\n+\n+In our case, we need to join 3 tables into a single document, which we\n+then index into Elasticsearch. To perform the joining we will use\n+`mapStateful` step. This can be performed either globally or partitioned\n+on a grouping key. If all your records share a common key it is always a\n+good idea to do the grouping by the key, because the mapping state is\n+then evenly distributed across all nodes and the mapping operation\n+parallelized.\n+\n+Our visit record contains the pet id but not the owner id, in such case,\n+so we don\u2019t have a single grouping key for all records. There are two options: \n+* perform the stateful mapping globally, which has the advantage\n+of having a single state - the joining logic is simpler and the pipeline\n+more straightforward. The obvious disadvantage is that such a solution\n+is not scalable because the state may not fit onto a single member or it\n+cannot be performed by a single thread;\n+\n+* create two separate mapping steps - one grouped on the owner id and\n+the other on the pet id. This is a trade-off between simplicity and\n+scalability.\n+\n+There are several observations which hold for both implementations:\n+\n+* the mapping state must cover all data from the beginning, there might\n+be a domain-specific rule that would allow eviction of old items;\n+\n+* the records might arrive in any order, even if there is an FK between\n+the records in the database, the order is not guaranteed, this is a\n+property of CDC.\n+\n+* once the object is emitted from the join step it must not be modified,\n+otherwise the later stages might see it in inconsistent state, causing\n+issues which are hard to debug. We make defensive copies to avoid this.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/JoiningState.java#L11)\n+to see the full source code.\n+\n+### Deleting changes\n+\n+The `ChangeRecord#operation()` method provides information about what\n+kind of operation was performed on the record. If you need to handle\n+deletions you should use it. The Petclinic application doesn\u2019t allow\n+delete operations, so we don\u2019t handle those.\n+\n+### Elastic sink\n+\n+The fourth key part of the pipeline is the Elastic sink. It takes a\n+document and writes it to Elasticsearch. You as a developer need to\n+provide information where the Elastic instance is running, what index to\n+write to and how to convert the `Document` into one of `IndexRequest`,\n+`UpdateRequest` or `DeleteRequest`. The Sink then takes care of batching\n+the requests and parallelizing for higher performance, retries in case\n+of network issues.\n+\n+### The Pipeline\n+\n+The final pipeline composes all the individual steps in a\n+straightforward way:\n+\n+```java\n+Pipeline p = Pipeline.create();\n+p.readFrom(mysqlSource)\n+ .withoutTimestamps()\n+ .map(PetClinicIndexJob::mapChangeRecordToPOJO).setName(\"mapChangeRecordToPOJO\")\n+ .mapUsingService(keywordService, PetClinicIndexJob::enrichWithKeywords).setName(\"enrichWithKeywords\")\n+ .mapStateful(JoiningState::new, JoiningState::join).setName(\"JoiningState::join\")\n+ .writeTo(elasticSink);\n+```\n+\n+Or visualized as a graph:\n+\n+![Pipeline with global mapping state](assets/2020-10-06-global-join-step.png)\n+\n+For the pipeline with two step join see [the source\n+code](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/two-joins/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L87)\n+on Github. This is the graph visualisation of the pipeline:\n+\n+![Pipeline with 2 separate grouped join steps](assets/2020-10-06-two-join-step.png)\n+\n+## Testing the job\n+\n+In order to test the job, we need to create a test database identical to\n+the Petclinic database.\n+\n+We have the following options\n+\n+* Start the pet clinic application inside the same JVM - the Petclinic\n+is a Spring Boot application, running it in-process would be\n+relatively easy, but it represents a legacy application, which is\n+usually quite difficult to run in such way. Also, there is a high\n+probability of dependency clashes, which would make this difficult.\n+\n+* Start the pet clinic application in a Docker container - it is likely\n+that the application is not containerized, we would need to create\n+such an image only for the tests.\n+\n+* Create the database and data using SQL directly - reusing the SQL\n+scripts from the Petclinic application and running inserts via JDBC\n+seems the easiest way to do this so we will continue with this approach.\n+\n+Go to\n+[Github](https://github.com/hazelcast-demos/pet-clinic-index-job/blob/master/src/main/java/org/example/jet/petclinic/PetClinicIndexJob.java#L121)\n+to see the full source code.\n+\n+## Running the Job\n+\n+In order to run the indexing job, first checkout the\n+[pet-clinic-indexing-job](https://github.com/hazelcast-demos/pet-clinic-index-job/)\n+repository\n+\n+```bash\n+git clone https://github.com/hazelcast-demos/pet-clinic-index-job.git\n+```\n+\n+Then build the JAR containing to job\n+\n+```bash\n+cd pet-clinic-indexing-job\n+mvn package\n+```\n+\n+And finally submit the job by running the following command:\n+\n+```bash\n+bin/jet submit \\\n+path/to/pet-clinic-index-job/target/pet-clinic-index-job-1.0-SNAPSHOT-jar-with-dependencies.jar\\\n+   --database-address localhost \\\n+   --database-port 3306 \\\n+   --database-user petclinic \\\n+   --database-password petclinic \\\n+   --elastic-host localhost:9200 \\\n+   --elastic-index petclinic-index\n+```\n+\n+## Petclinic application update\n+\n+Now that we have the data in Elasticsearch index we can update the\n+Petclinic application to use it for search.\n+\n+There are several changes in the application, but the most important\n+changes are Create a search service to search for the data Update the\n+search endpoint to use the new search service\n+\n+The following snippet shows the search method in the SearchService. It\n+uses the Elasticsearch client directly, but one could use Spring Data\n+Elasticsearch as well.\n+\n+```java\n+public Collection<Integer> search(String query) {\n+  SearchRequest searchRequest = new SearchRequest(index);\n+  searchRequest.source().fetchSource(true).query(QueryBuilders.wildcardQuery(searchField, query + \"*\"));\n+\n+  try {\n+     SearchResponse response = client.search(searchRequest, RequestOptions.DEFAULT);\n+\n+     SearchHits hits = response.getHits();\n+     return Arrays.stream(hits.getHits()).map(hit -> (Integer) hit.getSourceAsMap().get(\"ownerId\"))\n+           .collect(toList());\n+  }\n+  catch (IOException e) {\n+     throw new RuntimeException(e);\n+  }\n+}\n+```\n+\n+The search endpoint will use the SearchService to search for the data.\n+After retrieving the results the Owner entities need to be loaded from\n+the database. This is not an issue as only a small subset of\n+best-matching documents is returned from Elastic and loaded by id from\n+the database.\n+\n+```java\n+Collection<Integer> ownerIds = searchService.search(searchForm.getQuery());\n+// find owners by last name\n+Collection<Owner> results = this.owners.findByIds(ownerIds);\n+```\n+\n+There are some more minor changes to the view, and the controller, you\n+can see all the changes in [this\n+commit](https://github.com/spring-projects/hazelcast-demos/compare/main...elasticsearch).\n+\n+Checkout the `elasticsearch` branch with the changes and restart the\n+application by running the maven command again:\n+\n+```bash\n+./mvnw spring-boot:run -Dspring-boot.run.profiles=mysql\n+```\n+\n+We can verify that searching for \u201cGeorge\u201d returns 2 results, one found\n+in a first name, other in a name of a pet:\n+\n+![Search \"George\"](assets/2020-10-06-search-george.png)\n+\n+Entering \u201crabies\u201d should bring up a single result (the detail screen is\n+shown in that case) matching a keyword extracted from the description:\n+\n+![Search \"rabbies\"](assets/2020-10-06-search-rabbies.png)\n+\n+## Summary\n+\n+We have shown how to stream changes using CDC, enrich the data,\n+correlate (join) the records with other records and finally store the\n+data into an Elasticsearch index so an application can provide better\n+functionality to the user.\n+\n+This is mostly done independently of the original application and its\n+database, reducing the impact it has on the original legacy system.\n+\n+If you would like to discuss this topic with use, drop by at [our", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "originalPosition": 483}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAyOTAxMDY4", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#pullrequestreview-502901068", "createdAt": "2020-10-06T12:47:37Z", "commit": {"oid": "a3fae74a0d62f73e44f42134c7853788dae64746"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5f8369e0ddb80bcd875d60de75155ca43bd04495", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/5f8369e0ddb80bcd875d60de75155ca43bd04495", "committedDate": "2020-10-06T20:36:40Z", "message": "Address feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c8ac60940787195566056857bb6f64c21db72c85", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/c8ac60940787195566056857bb6f64c21db72c85", "committedDate": "2020-10-07T11:49:26Z", "message": "Add a demo card for CDC/Elastic"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0ODExNDIw", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2589#pullrequestreview-504811420", "createdAt": "2020-10-08T13:57:28Z", "commit": {"oid": "c8ac60940787195566056857bb6f64c21db72c85"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3471, "cost": 1, "resetAt": "2021-11-01T14:51:55Z"}}}