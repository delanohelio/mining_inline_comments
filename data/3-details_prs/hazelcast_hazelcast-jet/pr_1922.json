{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY5ODAzNTYy", "number": 1922, "title": "Add refman for transactional sources", "bodyText": "", "createdAt": "2020-01-31T22:50:57Z", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922", "merged": true, "mergeCommit": {"oid": "b82664fab26ba70e09f4f0f6b4d6703da85ed6c0"}, "closed": true, "closedAt": "2020-02-07T16:22:24Z", "author": {"login": "viliam-durina"}, "timelineItems": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb_3MKIgH2gAyMzY5ODAzNTYyOjNmMzcxNDJjODZiODhiMjZjZTdlM2M2MDRkYmY5MWU1ZjIxY2JjOTQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcCBAFuAFqTM1NTIzMjI4OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "3f37142c86b88b26ce7e3c604dbf91e5f21cbc94", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/3f37142c86b88b26ce7e3c604dbf91e5f21cbc94", "committedDate": "2020-01-31T22:50:13Z", "message": "Add refman for transactional sources"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b8724ae7d11b4da758083053c72c48f7357e99d2", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/b8724ae7d11b4da758083053c72c48f7357e99d2", "committedDate": "2020-02-04T10:41:46Z", "message": "Document individual connectors"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "756dc6c22380ddd2decf704688bc715c91f3f56a", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/756dc6c22380ddd2decf704688bc715c91f3f56a", "committedDate": "2020-02-04T10:42:19Z", "message": "Merge branch 'master' into 2pc-refman"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUzNTgyOTQ0", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#pullrequestreview-353582944", "createdAt": "2020-02-05T09:48:03Z", "commit": {"oid": "756dc6c22380ddd2decf704688bc715c91f3f56a"}, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQwOTo0ODowM1rOFlxhCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQxMDowMDo1MlrOFlx8Dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTE1MjkwNg==", "bodyText": "Could you rewrite this to use the active voice?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375152906", "createdAt": "2020-02-05T09:48:03Z", "author": {"login": "mtopolnik"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/Sinks.java", "diffHunk": "@@ -798,12 +798,12 @@ private Sinks() {\n      * <h3>Fault tolerance</h3>\n      * If the job is running in <i>exactly-once</i> mode, items will be written\n      * to temporary files (ending with a {@value\n-     * FileSinkBuilder#TEMP_FILE_SUFFIX} suffix). When the snapshot is", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "756dc6c22380ddd2decf704688bc715c91f3f56a"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTE1MzUyNg==", "bodyText": "Same here (active voice)", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375153526", "createdAt": "2020-02-05T09:49:13Z", "author": {"login": "mtopolnik"}, "path": "reference-manual/src/main/asciidoc/source-sink-connectors/file-socket.adoc", "diffHunk": "@@ -56,6 +63,15 @@ member will write to different file names. You can achieve the effect of\n a distributed sink if you manually collect all the output files on all\n members and combine their contents.\n \n+If the job is running in exactly-once mode, items will be written to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "756dc6c22380ddd2decf704688bc715c91f3f56a"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTE1MzcyNw==", "bodyText": "a -> are", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375153727", "createdAt": "2020-02-05T09:49:37Z", "author": {"login": "mtopolnik"}, "path": "reference-manual/src/main/asciidoc/source-sink-connectors/kafka.adoc", "diffHunk": "@@ -42,17 +36,22 @@ The Kafka source supports snapshots. Upon each snapshot it saves the\n current offset for each partition. When the job is restarted from a\n snapshot, the source can continue reading from the saved offset.\n \n-If snapshots are disabled, the source will commit the offset of the\n+If snapshots are disabled, the source will commit the offsets of the\n last record it read to the Kafka cluster. Since the fact that the source\n read an item doesn't mean that the whole Jet pipeline processed it, this\n-doesn't guarantee against data loss.\n+prevents neither data loss nor duplicate processing.\n \n == Using Kafka as a Sink\n \n-The Kafka sink creates one `KafkaProducer` per cluster member and shares\n-it among all the sink processors on that member. You can provide a\n-mapping function that transforms the items the sink receives into\n-`ProducerRecord`s.\n+The sink provides exactly-once guarantee at the cost of using Kafka\n+transactions: Jet commits the produced records after each snapshot is\n+completed. This greatly increases the latency because consumers see the\n+records only after they are committed.\n+\n+If you use _at-least-once_ guarantee, records a visible immediately, but", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "756dc6c22380ddd2decf704688bc715c91f3f56a"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTE1Mzg4Nw==", "bodyText": "the exactly-once guarantee", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375153887", "createdAt": "2020-02-05T09:49:52Z", "author": {"login": "mtopolnik"}, "path": "reference-manual/src/main/asciidoc/source-sink-connectors/kafka.adoc", "diffHunk": "@@ -42,17 +36,22 @@ The Kafka source supports snapshots. Upon each snapshot it saves the\n current offset for each partition. When the job is restarted from a\n snapshot, the source can continue reading from the saved offset.\n \n-If snapshots are disabled, the source will commit the offset of the\n+If snapshots are disabled, the source will commit the offsets of the\n last record it read to the Kafka cluster. Since the fact that the source\n read an item doesn't mean that the whole Jet pipeline processed it, this\n-doesn't guarantee against data loss.\n+prevents neither data loss nor duplicate processing.\n \n == Using Kafka as a Sink\n \n-The Kafka sink creates one `KafkaProducer` per cluster member and shares\n-it among all the sink processors on that member. You can provide a\n-mapping function that transforms the items the sink receives into\n-`ProducerRecord`s.\n+The sink provides exactly-once guarantee at the cost of using Kafka", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "756dc6c22380ddd2decf704688bc715c91f3f56a"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTE1NDc4Mw==", "bodyText": "the Kafka sink", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375154783", "createdAt": "2020-02-05T09:51:33Z", "author": {"login": "mtopolnik"}, "path": "reference-manual/src/main/asciidoc/source-sink-connectors/overview.adoc", "diffHunk": "@@ -24,45 +24,66 @@ Journal feature). You can also set up an `IMap`/`ICache` as a sink for\n an infinite amount of data, either by ensuring that the size of the\n keyset will be finite or by allowing the eviction of old entries.\n \n-== Is it Replayable?\n+== Is it Fault-Tolerant?\n \n-Most finite data sources are replayable because they come from\n-persistent storage. You can easily replay the whole dataset. However, an\n-infinite data source may be of such nature that it can be consumed only\n-once. An example is the TCP socket connector. Such sources are bad at\n-fault tolerance: if anything goes wrong during the computation, it\n-cannot be retried.\n+Can the connector work in a way that even in case of a job failure and\n+restart it doesn't miss or duplicate any items it reads or creates? There are\n+multiple ways this can be achieved:\n \n-== Does it Support Checkpointing?\n+=== Replayable Source\n \n-You cannot retry to process an infinite data stream from the very\n-beginning. You must save the complete state at regular intervals, then\n-replay the input stream from the last saved position (_checkpoint_).\n-Jet can create snapshots of its internal processing state, but for this\n-to be useful the data source must have the ability to replay its data\n-from the chosen point, discarding everything before it. Both Kafka and\n-the Hazelcast Event Journal support this.\n+If a source has a notion of item offset and a reader can seek to an\n+offset and read the exactly same data from it multiple times, we say\n+it's replayable. For finite sources it's feasible to restart from offset\n+0, such as re-read a file. However, an infinite data source may be of\n+such nature that it can be consumed only once. An example is the TCP\n+socket connector. Such sources are not fault-tolerant: if anything goes\n+wrong during the computation, it cannot be retried. Examples of\n+unbounded replayable sources are Hazelcast Event Journal or Kafka.\n+\n+=== Consumption Acknowledgements\n+\n+If a consumer has to acknowledge each processed message and\n+non-acknowledged messages are redelivered, it's possible to create a\n+fault-tolerant source. An example of this is the JMS source.\n+\n+=== XA Transactions\n+\n+Two-phase transactions are another way to achieve fault-tolerance. Jet\n+can use the standard X/Open XA Transactions or equivalent API. An\n+example is Kafka sink. JMS and JDBC are planned for the next release.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "756dc6c22380ddd2decf704688bc715c91f3f56a"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTE1NTkzNQ==", "bodyText": "one Jet member", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375155935", "createdAt": "2020-02-05T09:53:38Z", "author": {"login": "mtopolnik"}, "path": "reference-manual/src/main/asciidoc/source-sink-connectors/overview.adoc", "diffHunk": "@@ -24,45 +24,66 @@ Journal feature). You can also set up an `IMap`/`ICache` as a sink for\n an infinite amount of data, either by ensuring that the size of the\n keyset will be finite or by allowing the eviction of old entries.\n \n-== Is it Replayable?\n+== Is it Fault-Tolerant?\n \n-Most finite data sources are replayable because they come from\n-persistent storage. You can easily replay the whole dataset. However, an\n-infinite data source may be of such nature that it can be consumed only\n-once. An example is the TCP socket connector. Such sources are bad at\n-fault tolerance: if anything goes wrong during the computation, it\n-cannot be retried.\n+Can the connector work in a way that even in case of a job failure and\n+restart it doesn't miss or duplicate any items it reads or creates? There are\n+multiple ways this can be achieved:\n \n-== Does it Support Checkpointing?\n+=== Replayable Source\n \n-You cannot retry to process an infinite data stream from the very\n-beginning. You must save the complete state at regular intervals, then\n-replay the input stream from the last saved position (_checkpoint_).\n-Jet can create snapshots of its internal processing state, but for this\n-to be useful the data source must have the ability to replay its data\n-from the chosen point, discarding everything before it. Both Kafka and\n-the Hazelcast Event Journal support this.\n+If a source has a notion of item offset and a reader can seek to an\n+offset and read the exactly same data from it multiple times, we say\n+it's replayable. For finite sources it's feasible to restart from offset\n+0, such as re-read a file. However, an infinite data source may be of\n+such nature that it can be consumed only once. An example is the TCP\n+socket connector. Such sources are not fault-tolerant: if anything goes\n+wrong during the computation, it cannot be retried. Examples of\n+unbounded replayable sources are Hazelcast Event Journal or Kafka.\n+\n+=== Consumption Acknowledgements\n+\n+If a consumer has to acknowledge each processed message and\n+non-acknowledged messages are redelivered, it's possible to create a\n+fault-tolerant source. An example of this is the JMS source.\n+\n+=== XA Transactions\n+\n+Two-phase transactions are another way to achieve fault-tolerance. Jet\n+can use the standard X/Open XA Transactions or equivalent API. An\n+example is Kafka sink. JMS and JDBC are planned for the next release.\n+\n+=== Idempotent writes\n+\n+An operation is idempotent if, even if executed multiple times, the end\n+result is the same as if executed just once. If a sink is idempotent and\n+the job restarts and repeat some writes, the duplicate writes will have\n+no effect. An example is an IMap sink, but only in case the keys are\n+unique and deterministic.\n \n == Is it Distributed?\n \n+A source is distributed if it can be read by multiple readers and each\n+will get a part of the data. We don't discuss sinks here as practically\n+all targets can be written to by multiple writers. It also doesn't\n+matter whether the resource actually stores data in multiple nodes.\n+\n A distributed computation engine prefers to work with distributed data\n-resources. If the resource is not distributed, all Jet members will have\n-to contend for access to a single endpoint. Kafka, HDFS, `IMap` and\n-`ICache` are all distributed. On the other hand, an `IList` is not: it\n-resides on a single member. When used as a source, only one Jet member\n-pulls its data. When used as a sink, all Jet members send their data\n-to the one that holds it.\n+resources. If the resource is not distributed, one Jet members has to do", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "756dc6c22380ddd2decf704688bc715c91f3f56a"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTE1NzQ3MA==", "bodyText": "StackOverflowError also has unrestricted scope because the exception can happen in the middle of updating a shared structure. It's basically the same damage potential as OutOfMemoryError.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375157470", "createdAt": "2020-02-05T09:56:28Z", "author": {"login": "mtopolnik"}, "path": "reference-manual/src/main/asciidoc/source-sink-connectors/overview.adoc", "diffHunk": "@@ -78,10 +99,9 @@ choices: have Jet connect to a Hazelcast IMDG cluster, or use Jet itself\n to host the data (since a Jet cluster is at the same time a Hazelcast\n IMDG cluster). In the second case Jet will automatically ensure a\n data-local access pattern, but there's a caveat: if the Jet job causes\n-an error of unrestricted scope, such as `OutOfMemoryError` or\n-`StackOverflowError`, it will have unpredictable consequences for the\n-state of the whole Jet member, jeopardizing the integrity of the data\n-stored on it.\n+an error of unrestricted scope, such as `OutOfMemoryError`, it will have", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "756dc6c22380ddd2decf704688bc715c91f3f56a"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTE1OTIwMw==", "bodyText": "with the exactly-once guarantee", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375159203", "createdAt": "2020-02-05T09:59:39Z", "author": {"login": "mtopolnik"}, "path": "reference-manual/src/main/asciidoc/work-with-jet/configure-fault-tolerance.adoc", "diffHunk": "@@ -6,29 +6,31 @@ correctly in the face of Jet members failing and leaving the cluster.\n \n Jet takes snapshots of the entire state of the computation at regular\n intervals. It coordinates the snapshot across the cluster and\n-synchronizes it with a checkpoint on the data source. The source must\n-ensure that, in the case of a restart, it will be able to replay all the\n-data it emitted after the last checkpoint. Each of the other components\n-in the job will restore its processing state to exactly what it was at\n-the time of the last snapshot. If a cluster member goes away, Jet will\n-restart the job on the remaining members, rewind the sources to the\n-last checkpoint, restore the state of processing from the last\n-snapshot, and then seamlessly continue from that point.\n+synchronizes it with connectors with fault-tolerance support. The source\n+must ensure that, in the case of a restart, it will be able to replay\n+all the data it emitted after the last checkpoint. Each of the other\n+components in the job will restore their processing state to exactly\n+what it was at the time of the last snapshot. If a cluster member goes\n+away, Jet will restart the job on the remaining members, restore the\n+state of processing from the last snapshot, and then seamlessly continue\n+from that point.\n \n == Exactly-Once\n \n \"Exactly-once processing\" means the output is consistent with processing\n each stream item exactly once. This is the ultimate guarantee you can\n ask for.\n \n-As of version 0.6, Hazelcast Jet supports exactly-once processing with\n-the source being either a Hazelcast `IMap` journal or a Kafka topic,\n-and the sink being a Hazelcast `IMap`.\n+Hazelcast Jet supports exactly-once processing with multiple sources and\n+sinks. Each of them specifies the guarantees it is able to provide, for\n+details see the summary in <<overview-of-sources-and-sinks>>.\n \n-If you configure Jet for exactly-once but use Kafka as the sink, after a\n-job restart you may get duplicates in the output. As opposed to doubly\n-processing an input item, this is more benign because it just means\n-getting the exact same result twice.\n+If you configure Jet job for exactly-once but use a sink with", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "756dc6c22380ddd2decf704688bc715c91f3f56a"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTE1OTM2OQ==", "bodyText": "same", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375159369", "createdAt": "2020-02-05T09:59:59Z", "author": {"login": "mtopolnik"}, "path": "reference-manual/src/main/asciidoc/work-with-jet/configure-fault-tolerance.adoc", "diffHunk": "@@ -6,29 +6,31 @@ correctly in the face of Jet members failing and leaving the cluster.\n \n Jet takes snapshots of the entire state of the computation at regular\n intervals. It coordinates the snapshot across the cluster and\n-synchronizes it with a checkpoint on the data source. The source must\n-ensure that, in the case of a restart, it will be able to replay all the\n-data it emitted after the last checkpoint. Each of the other components\n-in the job will restore its processing state to exactly what it was at\n-the time of the last snapshot. If a cluster member goes away, Jet will\n-restart the job on the remaining members, rewind the sources to the\n-last checkpoint, restore the state of processing from the last\n-snapshot, and then seamlessly continue from that point.\n+synchronizes it with connectors with fault-tolerance support. The source\n+must ensure that, in the case of a restart, it will be able to replay\n+all the data it emitted after the last checkpoint. Each of the other\n+components in the job will restore their processing state to exactly\n+what it was at the time of the last snapshot. If a cluster member goes\n+away, Jet will restart the job on the remaining members, restore the\n+state of processing from the last snapshot, and then seamlessly continue\n+from that point.\n \n == Exactly-Once\n \n \"Exactly-once processing\" means the output is consistent with processing\n each stream item exactly once. This is the ultimate guarantee you can\n ask for.\n \n-As of version 0.6, Hazelcast Jet supports exactly-once processing with\n-the source being either a Hazelcast `IMap` journal or a Kafka topic,\n-and the sink being a Hazelcast `IMap`.\n+Hazelcast Jet supports exactly-once processing with multiple sources and\n+sinks. Each of them specifies the guarantees it is able to provide, for\n+details see the summary in <<overview-of-sources-and-sinks>>.\n \n-If you configure Jet for exactly-once but use Kafka as the sink, after a\n-job restart you may get duplicates in the output. As opposed to doubly\n-processing an input item, this is more benign because it just means\n-getting the exact same result twice.\n+If you configure Jet job for exactly-once but use a sink with\n+at-least-once guarantee, after a job restart you may get duplicates in\n+the output. As opposed to doubly processing an input item, this is more\n+benign because it just means getting the exact same result twice. If you\n+use a source with at-least-once guarantee, some events might be", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "756dc6c22380ddd2decf704688bc715c91f3f56a"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTE1OTU1MA==", "bodyText": "in the case", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375159550", "createdAt": "2020-02-05T10:00:19Z", "author": {"login": "mtopolnik"}, "path": "reference-manual/src/main/asciidoc/work-with-jet/configure-fault-tolerance.adoc", "diffHunk": "@@ -42,6 +44,11 @@ In some other cases, however, duplicate processing of data items can\n have quite surprising consequences. There is more information about this\n in our <<pitfalls-alo, Jet Concepts>> chapter.\n \n+Also note that duplicate event processing can occur only in case of a", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "756dc6c22380ddd2decf704688bc715c91f3f56a"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTE1OTc2OA==", "bodyText": "in the case", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375159768", "createdAt": "2020-02-05T10:00:45Z", "author": {"login": "mtopolnik"}, "path": "reference-manual/src/main/asciidoc/source-sink-connectors/kafka.adoc", "diffHunk": "@@ -42,17 +36,22 @@ The Kafka source supports snapshots. Upon each snapshot it saves the\n current offset for each partition. When the job is restarted from a\n snapshot, the source can continue reading from the saved offset.\n \n-If snapshots are disabled, the source will commit the offset of the\n+If snapshots are disabled, the source will commit the offsets of the\n last record it read to the Kafka cluster. Since the fact that the source\n read an item doesn't mean that the whole Jet pipeline processed it, this\n-doesn't guarantee against data loss.\n+prevents neither data loss nor duplicate processing.\n \n == Using Kafka as a Sink\n \n-The Kafka sink creates one `KafkaProducer` per cluster member and shares\n-it among all the sink processors on that member. You can provide a\n-mapping function that transforms the items the sink receives into\n-`ProducerRecord`s.\n+The sink provides exactly-once guarantee at the cost of using Kafka\n+transactions: Jet commits the produced records after each snapshot is\n+completed. This greatly increases the latency because consumers see the\n+records only after they are committed.\n+\n+If you use _at-least-once_ guarantee, records a visible immediately, but\n+in case of a failure some records could be produced duplicately. You can", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "756dc6c22380ddd2decf704688bc715c91f3f56a"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTE1OTgyMg==", "bodyText": "in the case", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375159822", "createdAt": "2020-02-05T10:00:52Z", "author": {"login": "mtopolnik"}, "path": "reference-manual/src/main/asciidoc/source-sink-connectors/overview.adoc", "diffHunk": "@@ -24,45 +24,66 @@ Journal feature). You can also set up an `IMap`/`ICache` as a sink for\n an infinite amount of data, either by ensuring that the size of the\n keyset will be finite or by allowing the eviction of old entries.\n \n-== Is it Replayable?\n+== Is it Fault-Tolerant?\n \n-Most finite data sources are replayable because they come from\n-persistent storage. You can easily replay the whole dataset. However, an\n-infinite data source may be of such nature that it can be consumed only\n-once. An example is the TCP socket connector. Such sources are bad at\n-fault tolerance: if anything goes wrong during the computation, it\n-cannot be retried.\n+Can the connector work in a way that even in case of a job failure and", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "756dc6c22380ddd2decf704688bc715c91f3f56a"}, "originalPosition": 19}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "41cc04e7c483f1cc09531bf39c6bf29d3386696e", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/41cc04e7c483f1cc09531bf39c6bf29d3386696e", "committedDate": "2020-02-05T14:29:49Z", "message": "Address review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f6a83c304b7ee4a6fb4457793569fd96f40ac94f", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/f6a83c304b7ee4a6fb4457793569fd96f40ac94f", "committedDate": "2020-02-05T14:32:25Z", "message": "One more change"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUzODAwMTI0", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#pullrequestreview-353800124", "createdAt": "2020-02-05T15:20:48Z", "commit": {"oid": "41cc04e7c483f1cc09531bf39c6bf29d3386696e"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQxNToyMDo0OFrOFl7tnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQxNToyMTo0MVrOFl7vxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTMxOTk2Nw==", "bodyText": "the exactly-once guarantee", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375319967", "createdAt": "2020-02-05T15:20:48Z", "author": {"login": "mtopolnik"}, "path": "reference-manual/src/main/asciidoc/source-sink-connectors/file-socket.adoc", "diffHunk": "@@ -63,11 +63,11 @@ member will write to different file names. You can achieve the effect of\n a distributed sink if you manually collect all the output files on all\n members and combine their contents.\n \n-If the job is running in exactly-once mode, items will be written to\n-temporary files (ending with a \".tmp\" suffix). When a snapshot is\n-completed, the file will be atomically renamed to remove this suffix.\n-Thanks to the two-phase commit of the snapshot, exactly-once guarantee\n-is provided for the sink.\n+If the job is running in exactly-once mode, items are written to\n+temporary files (ending with a \".tmp\" suffix). When Jet commits a\n+snapshot, it atomically renames the file to remove this suffix. Thanks\n+to the two-phase commit of the snapshot the sink provides exactly-once", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41cc04e7c483f1cc09531bf39c6bf29d3386696e"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTMyMDUxOA==", "bodyText": "with the at-least once", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375320518", "createdAt": "2020-02-05T15:21:41Z", "author": {"login": "mtopolnik"}, "path": "reference-manual/src/main/asciidoc/work-with-jet/configure-fault-tolerance.adoc", "diffHunk": "@@ -25,12 +25,12 @@ Hazelcast Jet supports exactly-once processing with multiple sources and\n sinks. Each of them specifies the guarantees it is able to provide, for\n details see the summary in <<overview-of-sources-and-sinks>>.\n \n-If you configure Jet job for exactly-once but use a sink with\n-at-least-once guarantee, after a job restart you may get duplicates in\n-the output. As opposed to doubly processing an input item, this is more\n-benign because it just means getting the exact same result twice. If you\n-use a source with at-least-once guarantee, some events might be\n-processed multiple times.\n+If you configure Jet job with the exactly-once guarantee but use a sink\n+with at-least-once guarantee, after a job restart you may get duplicates", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41cc04e7c483f1cc09531bf39c6bf29d3386696e"}, "originalPosition": 11}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzUzODE0NDU2", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#pullrequestreview-353814456", "createdAt": "2020-02-05T15:37:17Z", "commit": {"oid": "f6a83c304b7ee4a6fb4457793569fd96f40ac94f"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQxNTozNzoxN1rOFl8Zuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wNVQxNTo1NToyM1rOFl9JFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTMzMTI1OQ==", "bodyText": "It won't work on Windows. It seems obvious that it is bash command but it can be confusing for not experienced users.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375331259", "createdAt": "2020-02-05T15:37:17Z", "author": {"login": "olukas"}, "path": "reference-manual/src/main/asciidoc/source-sink-connectors/file-socket.adoc", "diffHunk": "@@ -46,6 +46,13 @@ after startup: both new files and new content appended to existing ones. Files\n must be updated in an append-only fashion; if the existing content changes, the\n behavior is undefined.\n \n+Often, GUI text editors, even if you just add lines at the end, delete\n+the whole file and recreate it. This gives undefined results. For\n+testing it's best to use:\n+```\n+echo \"foo\" >> your_file", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f6a83c304b7ee4a6fb4457793569fd96f40ac94f"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTMzMTMyNg==", "bodyText": "JMS and JDBC are planned for the next release. - is it something what we want to have in reference manual now? I mean during implementation and testing of JMS source it was reimplemented from XA to Consumption Acknowledgements - something similar can happen for these as well. I think we should not advertise those sources here for version 4.0.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375331326", "createdAt": "2020-02-05T15:37:24Z", "author": {"login": "olukas"}, "path": "reference-manual/src/main/asciidoc/source-sink-connectors/overview.adoc", "diffHunk": "@@ -24,45 +24,67 @@ Journal feature). You can also set up an `IMap`/`ICache` as a sink for\n an infinite amount of data, either by ensuring that the size of the\n keyset will be finite or by allowing the eviction of old entries.\n \n-== Is it Replayable?\n+== Is it Fault-Tolerant?\n \n-Most finite data sources are replayable because they come from\n-persistent storage. You can easily replay the whole dataset. However, an\n-infinite data source may be of such nature that it can be consumed only\n-once. An example is the TCP socket connector. Such sources are bad at\n-fault tolerance: if anything goes wrong during the computation, it\n-cannot be retried.\n+Can the connector work in a way that even in the case of a job failure\n+and restart it doesn't miss or duplicate any items it reads or creates?\n+There are multiple ways this can be achieved:\n \n-== Does it Support Checkpointing?\n+=== Replayable Source\n \n-You cannot retry to process an infinite data stream from the very\n-beginning. You must save the complete state at regular intervals, then\n-replay the input stream from the last saved position (_checkpoint_).\n-Jet can create snapshots of its internal processing state, but for this\n-to be useful the data source must have the ability to replay its data\n-from the chosen point, discarding everything before it. Both Kafka and\n-the Hazelcast Event Journal support this.\n+If a source has a notion of item offset and a reader can seek to an\n+offset and read the exactly same data from it multiple times, we say\n+it's replayable. For finite sources it's feasible to restart from offset\n+0, such as re-read a file. However, an infinite data source may be of\n+such nature that it can be consumed only once. An example is the TCP\n+socket connector. Such sources are not fault-tolerant: if anything goes\n+wrong during the computation, it cannot be retried. Examples of\n+unbounded replayable sources are Hazelcast Event Journal or Kafka.\n+\n+=== Consumption Acknowledgements\n+\n+If a consumer has to acknowledge each processed message and\n+non-acknowledged messages are redelivered, it's possible to create a\n+fault-tolerant source. An example of this is the JMS source.\n+\n+=== XA Transactions\n+\n+Two-phase transactions are another way to achieve fault-tolerance. Jet\n+can use the standard X/Open XA Transactions or equivalent API. An\n+example is the Kafka sink. JMS and JDBC are planned for the next", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f6a83c304b7ee4a6fb4457793569fd96f40ac94f"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NTM0MzM4MQ==", "bodyText": "This Guarantee seems to me like \"default\" guarantee and not \"the most precise guarantee which can be achieved\". For example if I take a look into the table I'll start to think that JMS source is exactly-once by default.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#discussion_r375343381", "createdAt": "2020-02-05T15:55:23Z", "author": {"login": "olukas"}, "path": "reference-manual/src/main/asciidoc/source-sink-connectors/overview.adoc", "diffHunk": "@@ -96,16 +117,17 @@ aren't packaged with Jet.\n \n .Sources and Sinks\n |===\n-|Resource|Javadoc|Sample|Unbounded?|Replayable?|Checkpointing?|Distributed?|Data Locality\n+|Resource|Javadoc|Sample|Unbounded?|Guarantee|Distributed?|Data Locality", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f6a83c304b7ee4a6fb4457793569fd96f40ac94f"}, "originalPosition": 118}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a84030dcddda65b1a211c2c2445a97b334fca841", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/a84030dcddda65b1a211c2c2445a97b334fca841", "committedDate": "2020-02-06T08:03:37Z", "message": "Address Ondrej's review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9e30dd4419aaa12e60969ef0b0221eca5766f52c", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/9e30dd4419aaa12e60969ef0b0221eca5766f52c", "committedDate": "2020-02-06T10:11:10Z", "message": "Touchups"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d3df6f3d1c0599220bd90775dd5e79ee89f49d73", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/d3df6f3d1c0599220bd90775dd5e79ee89f49d73", "committedDate": "2020-02-06T10:20:16Z", "message": "Merge branch 'master' into 2pc-refman"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU1MDY4MTI1", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#pullrequestreview-355068125", "createdAt": "2020-02-07T10:36:44Z", "commit": {"oid": "d3df6f3d1c0599220bd90775dd5e79ee89f49d73"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4bebd7835452f4ebe8baf5a5086d62e4cbc2e4ab", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/4bebd7835452f4ebe8baf5a5086d62e4cbc2e4ab", "committedDate": "2020-02-07T10:49:56Z", "message": "Merge branch 'master' into 2pc-refman"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ee4dc591a6c850c78af2fae82fc2fb7981ce3d3d", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/ee4dc591a6c850c78af2fae82fc2fb7981ce3d3d", "committedDate": "2020-02-07T14:19:45Z", "message": "Update JMS refman"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aedfd6b9beea0a5531b1327bf18ebfde861e7356", "author": {"user": {"login": "viliam-durina", "name": "Viliam Durina"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/aedfd6b9beea0a5531b1327bf18ebfde861e7356", "committedDate": "2020-02-07T14:34:57Z", "message": "Merge branch 'master' into 2pc-refman\n\n# Conflicts:\n#\treference-manual/src/main/asciidoc/source-sink-connectors/kafka.adoc"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU1MjAyODEx", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#pullrequestreview-355202811", "createdAt": "2020-02-07T14:43:46Z", "commit": {"oid": "aedfd6b9beea0a5531b1327bf18ebfde861e7356"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU1MjMyMjg5", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1922#pullrequestreview-355232289", "createdAt": "2020-02-07T15:23:56Z", "commit": {"oid": "aedfd6b9beea0a5531b1327bf18ebfde861e7356"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2966, "cost": 1, "resetAt": "2021-11-01T14:51:55Z"}}}