{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDg3MTk5OTA3", "number": 2518, "title": "File data ingestion", "bodyText": "Unified API for reading files across sources and formats.\nFixes #1205.\nChecklist\n\n Labels and Milestone set\n Breaking changes documented\n New public APIs have @Nonnull/@Nullable annotations\n New public APIs have @since tags in Javadoc\n For code samples, code sample main readme is updated", "createdAt": "2020-09-15T10:32:09Z", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518", "merged": true, "mergeCommit": {"oid": "c1fc292d5226a41347ab86bf7b9b63f09dcde25f"}, "closed": true, "closedAt": "2020-11-26T09:26:52Z", "author": {"login": "frant-hartm"}, "timelineItems": {"totalCount": 51, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdJPZQbgFqTQ4OTE0NjMzMQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdgPGDcAH2gAyNDg3MTk5OTA3OmFhOGQ4NTY5ZmFmNjZlYzE2M2Y0MzFhODExMWIyMTBhNDU0NjUwYzQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg5MTQ2MzMx", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#pullrequestreview-489146331", "createdAt": "2020-09-15T22:17:46Z", "commit": {"oid": "250662eb1edc7691ce12577016b295234a976157"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMjoxNzo0N1rOHSXiog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMjoxNzo0N1rOHSXiog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTAyMjExNA==", "bodyText": "we shouldn't use any class(NullWritable) from hadoop, it adds a dependency to hadoop even if user wants to read files without hadoop module.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r489022114", "createdAt": "2020-09-15T22:17:47Z", "author": {"login": "gurbuzali"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/AvroFileFormat.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file;\n+\n+import com.hazelcast.function.BiFunctionEx;\n+import com.hazelcast.function.FunctionEx;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileReader;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.mapred.AvroKey;\n+import org.apache.avro.mapreduce.AvroKeyInputFormat;\n+import org.apache.avro.reflect.ReflectData;\n+import org.apache.avro.reflect.ReflectDatumReader;\n+import org.apache.avro.specific.SpecificDatumReader;\n+import org.apache.hadoop.io.NullWritable;\n+\n+import java.nio.file.Path;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+\n+/**\n+ * FileFormat for avro files\n+ *\n+ *\n+ * @param <T>\n+ */\n+public class AvroFileFormat<T> extends AbstractFileFormat<AvroKey<T>, NullWritable, T>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "250662eb1edc7691ce12577016b295234a976157"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg5MjkwNjQ4", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#pullrequestreview-489290648", "createdAt": "2020-09-16T05:44:41Z", "commit": {"oid": "250662eb1edc7691ce12577016b295234a976157"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNTo0NDo0MVrOHSg-Fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNTo0NDo0MVrOHSg-Fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTE3NjU5OQ==", "bodyText": "we use jackson-dataformat here which adds a dependency to the core", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r489176599", "createdAt": "2020-09-16T05:44:41Z", "author": {"login": "gurbuzali"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/CsvFileFormat.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file;\n+\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.hazelcast.function.BiFunctionEx;\n+import com.hazelcast.function.FunctionEx;\n+import org.apache.hadoop.io.NullWritable;\n+\n+import java.io.InputStream;\n+import java.util.Spliterator;\n+import java.util.Spliterators;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+public class CsvFileFormat<T> extends AbstractFileFormat<NullWritable, T, T> implements FileFormat<NullWritable, T, T> {\n+\n+    public static final String CSV_INPUT_FORMAT_BEAN_CLASS = \"csv.bean.class\";\n+\n+    private final Class<T> clazz;\n+\n+    public CsvFileFormat(Class<T> clazz) {\n+        this.clazz = clazz;\n+\n+        withOption(INPUT_FORMAT_CLASS, \"com.hazelcast.jet.hadoop.impl.CsvInputFormat\");\n+        withOption(CSV_INPUT_FORMAT_BEAN_CLASS, clazz.getCanonicalName());\n+    }\n+\n+    @Override\n+    public FunctionEx<InputStream, Stream<T>> mapInputStreamFn() {\n+        CsvSchema schema = CsvSchema.emptySchema().withHeader();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "250662eb1edc7691ce12577016b295234a976157"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk0NjE4Nzcz", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#pullrequestreview-494618773", "createdAt": "2020-09-23T12:44:35Z", "commit": {"oid": "5da2197c944aae3b23bde72c463d1ec289bf6fe8"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QxMjo0NDozNVrOHWrm2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QxMzoxNzowNlrOHWtgFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU0NTE3Ng==", "bodyText": "Couple the returned format to the format of handled FileFormat? Otherwise it can easily get out of sync.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r493545176", "createdAt": "2020-09-23T12:44:35Z", "author": {"login": "gierlachg"}, "path": "extensions/avro/src/main/java/com/hazelcast/jet/avro/AvroMapFnProvider.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.avro;\n+\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.pipeline.file.AvroFileFormat;\n+import com.hazelcast.jet.pipeline.file.impl.MapFnProvider;\n+import org.apache.avro.file.DataFileReader;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.reflect.ReflectDatumReader;\n+import org.apache.avro.specific.SpecificDatumReader;\n+\n+import java.nio.file.Path;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+\n+/**\n+ * MapFnProvider for Avro files, reading given path and deserializing using\n+ * avro DatumReader\n+ *\n+ * @param <T> type of the item emitted from the source\n+ */\n+public class AvroMapFnProvider<T> implements MapFnProvider<AvroFileFormat<T>, T> {\n+\n+    @Override\n+    public FunctionEx<Path, Stream<T>> create(AvroFileFormat<T> format) {\n+        Class<T> reflectClass = format.reflectClass();\n+        return (path) -> {\n+            DatumReader<T> datumReader = datumReader(reflectClass);\n+            DataFileReader<T> reader = new DataFileReader<>(path.toFile(), datumReader);\n+            return StreamSupport.stream(reader.spliterator(), false)\n+                                .onClose(() -> uncheckRun(reader::close));\n+        };\n+    }\n+\n+    private static <T> DatumReader<T> datumReader(Class<T> reflectClass) {\n+        return reflectClass == null ? new SpecificDatumReader<>() : new ReflectDatumReader<>(reflectClass);\n+    }\n+\n+    @Override\n+    public String format() {\n+        return \"avro\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5da2197c944aae3b23bde72c463d1ec289bf6fe8"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU3MjA3Nw==", "bodyText": "Get the key from the provider? To keep them in sync?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r493572077", "createdAt": "2020-09-23T13:12:36Z", "author": {"login": "gierlachg"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file.impl;\n+\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.datamodel.Tuple2;\n+import com.hazelcast.jet.impl.util.IOUtil;\n+import com.hazelcast.jet.json.JsonUtil;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.Sources;\n+import com.hazelcast.jet.pipeline.file.CsvFileFormat;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSourceFactory;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+import java.util.Spliterator;\n+import java.util.Spliterators;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import static com.hazelcast.jet.datamodel.Tuple2.tuple2;\n+import static com.hazelcast.jet.impl.util.Util.uncheckCall;\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+\n+/**\n+ * Implementation of FileSourceFactory for local filesystem\n+ *\n+ * @param <T> type of the item emitted from the source\n+ */\n+public class LocalFileSourceFactory<T> implements FileSourceFactory<T> {\n+\n+    private Map<String, MapFnProvider<? extends FileFormat<?>, ?>> mapFns;\n+\n+    /**\n+     * Default constructor\n+     */\n+    public LocalFileSourceFactory() {\n+        mapFns = new HashMap<>();\n+\n+        mapFns.put(\"csv\", new CsvMapFnProvider());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5da2197c944aae3b23bde72c463d1ec289bf6fe8"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU3NTg3Mg==", "bodyText": "Why the '1' suffix?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r493575872", "createdAt": "2020-09-23T13:16:40Z", "author": {"login": "gierlachg"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file.impl;\n+\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.datamodel.Tuple2;\n+import com.hazelcast.jet.impl.util.IOUtil;\n+import com.hazelcast.jet.json.JsonUtil;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.Sources;\n+import com.hazelcast.jet.pipeline.file.CsvFileFormat;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSourceFactory;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+import java.util.Spliterator;\n+import java.util.Spliterators;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import static com.hazelcast.jet.datamodel.Tuple2.tuple2;\n+import static com.hazelcast.jet.impl.util.Util.uncheckCall;\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+\n+/**\n+ * Implementation of FileSourceFactory for local filesystem\n+ *\n+ * @param <T> type of the item emitted from the source\n+ */\n+public class LocalFileSourceFactory<T> implements FileSourceFactory<T> {\n+\n+    private Map<String, MapFnProvider<? extends FileFormat<?>, ?>> mapFns;\n+\n+    /**\n+     * Default constructor\n+     */\n+    public LocalFileSourceFactory() {\n+        mapFns = new HashMap<>();\n+\n+        mapFns.put(\"csv\", new CsvMapFnProvider());\n+        mapFns.put(\"jsonl\", new JsonMapFnProvider<>());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5da2197c944aae3b23bde72c463d1ec289bf6fe8"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU3NjIxNA==", "bodyText": "Just 'lines' instead of 'txt1'?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r493576214", "createdAt": "2020-09-23T13:17:06Z", "author": {"login": "gierlachg"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file.impl;\n+\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.datamodel.Tuple2;\n+import com.hazelcast.jet.impl.util.IOUtil;\n+import com.hazelcast.jet.json.JsonUtil;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.Sources;\n+import com.hazelcast.jet.pipeline.file.CsvFileFormat;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSourceFactory;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+import java.util.Spliterator;\n+import java.util.Spliterators;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import static com.hazelcast.jet.datamodel.Tuple2.tuple2;\n+import static com.hazelcast.jet.impl.util.Util.uncheckCall;\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+\n+/**\n+ * Implementation of FileSourceFactory for local filesystem\n+ *\n+ * @param <T> type of the item emitted from the source\n+ */\n+public class LocalFileSourceFactory<T> implements FileSourceFactory<T> {\n+\n+    private Map<String, MapFnProvider<? extends FileFormat<?>, ?>> mapFns;\n+\n+    /**\n+     * Default constructor\n+     */\n+    public LocalFileSourceFactory() {\n+        mapFns = new HashMap<>();\n+\n+        mapFns.put(\"csv\", new CsvMapFnProvider());\n+        mapFns.put(\"jsonl\", new JsonMapFnProvider<>());\n+        mapFns.put(\"txtl\", new LinesMapFnProvider());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5da2197c944aae3b23bde72c463d1ec289bf6fe8"}, "originalPosition": 76}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk2MjU1NzMz", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#pullrequestreview-496255733", "createdAt": "2020-09-25T09:24:26Z", "commit": {"oid": "ad181730f72e87b1054c64207f6432048d0be93a"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwOToyNDoyNlrOHX8AHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwOTo0OToxMFrOHX84ZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg2MjM2NQ==", "bodyText": "This class is public API, shouldn't be in impl.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r494862365", "createdAt": "2020-09-25T09:24:26Z", "author": {"login": "mtopolnik"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/MapFnProvider.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file.impl;\n+\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+\n+import java.nio.file.Path;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Provides a mapping function from a Path to a Stream of items emitted from local filesystem source\n+ *\n+ * @param <F> FileFormat type\n+ * @param <T> type of the items emitted from the file source\n+ */\n+public interface MapFnProvider<F extends FileFormat<?>, T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad181730f72e87b1054c64207f6432048d0be93a"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg3Njc3Mg==", "bodyText": "A factory method would be a nicer choice, I think. It removes the diamond operator and new, looks cleaner.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r494876772", "createdAt": "2020-09-25T09:49:10Z", "author": {"login": "mtopolnik"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/FileSourceBuilder.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file;\n+\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.file.impl.LocalFileSourceFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+/**\n+ * Builder for file sources\n+ * <p>\n+ * The builder works with local filesystem and with hadoop supported\n+ * filesystems.\n+ * <p>\n+ * The builder requires 'path' and 'format' parameters and creates a\n+ * {@link BatchSource}. The path specifies the location of the file(s)\n+ * and possibly the data source - s3a://, hdfs://, etc..\n+ * <p>\n+ * The format determines how the contents of the file is parsed and\n+ * also determines the type of the source items. E.g. the\n+ * {@link LinesTextFileFormat} returns each line as a String,\n+ * {@link JsonFileFormat} returns each line of a JSON Lines file\n+ * deserialized into an instance of a specified class.\n+ * <p>\n+ * You may also use Hadoop to read local files by specifying the\n+ * {@link #useHadoopForLocalFiles()} flag.\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ * BatchSource<User> source = new FileSourceBuilder(\"data/users.jsonl\")\n+ *   .withFormat(new JsonFileFormat<>(User.class))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad181730f72e87b1054c64207f6432048d0be93a"}, "originalPosition": 54}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk2Mzc1MDc0", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#pullrequestreview-496375074", "createdAt": "2020-09-25T12:30:50Z", "commit": {"oid": "aea745df108100358b6d094c25f6576d807a0820"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxMjozMDo1MFrOHYBn9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxMjozMDo1MFrOHYBn9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk1NDQ4NQ==", "bodyText": "We have fileSourceBuilder.build() and then we have HadoopSourceFactory.create(fileSourceBuilder). This is inconsistent API. It also creates the problem of nullability, you can pass in a builder in any state.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r494954485", "createdAt": "2020-09-25T12:30:50Z", "author": {"login": "mtopolnik"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/HadoopSourceFactory.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import com.hazelcast.function.BiFunctionEx;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.hadoop.HadoopSources;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.file.AvroFileFormat;\n+import com.hazelcast.jet.pipeline.file.CsvFileFormat;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSourceFactory;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import org.apache.avro.Schema;\n+import org.apache.avro.mapred.AvroKey;\n+import org.apache.avro.mapreduce.AvroJob;\n+import org.apache.avro.mapreduce.AvroKeyInputFormat;\n+import org.apache.avro.reflect.ReflectData;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.BytesWritable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n+import org.apache.parquet.avro.AvroParquetInputFormat;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static com.hazelcast.jet.hadoop.impl.CsvInputFormat.CSV_INPUT_FORMAT_BEAN_CLASS;\n+import static com.hazelcast.jet.hadoop.impl.JsonInputFormat.JSON_INPUT_FORMAT_BEAN_CLASS;\n+\n+/**\n+ * Hadoop based implementation for FileSourceFactory\n+ *\n+ * @param <T> type of the items emitted from the source\n+ */\n+public class HadoopSourceFactory<T> implements FileSourceFactory<T> {\n+\n+    private final Map<Class<? extends FileFormat>, JobConfigurer<?, ?>> configs;\n+\n+    /**\n+     * Creates HadoopSourceFactory\n+     */\n+    public HadoopSourceFactory() {\n+        configs = new HashMap<>();\n+\n+        configs.put(AvroFileFormat.class, new AvroFormatJobConfigurer());\n+        configs.put(CsvFileFormat.class, new CsvFormatJobConfigurer());\n+        configs.put(JsonFileFormat.class, new JsonFormatJobConfigurer());\n+        configs.put(LinesTextFileFormat.class, new LineTextJobConfigurer());\n+        configs.put(ParquetFileFormat.class, new ParquetFormatJobConfigurer());\n+        configs.put(RawBytesFileFormat.class, new RawBytesFormatJobConfigurer());\n+        configs.put(TextFileFormat.class, new TextJobConfigurer());\n+    }\n+\n+    @Override\n+    public BatchSource<T> create(FileSourceBuilder<T> builder) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aea745df108100358b6d094c25f6576d807a0820"}, "originalPosition": 81}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "508a079652e8cbe6419f65b6938fdc5528caff23", "author": {"user": {"login": "mtopolnik", "name": "Marko Topolnik"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/508a079652e8cbe6419f65b6938fdc5528caff23", "committedDate": "2020-11-04T14:00:29Z", "message": "More Javadoc"}, "afterCommit": {"oid": "ed6e8e3462cadc066e386d3bdb1ae57777cd3a99", "author": {"user": {"login": "mtopolnik", "name": "Marko Topolnik"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/ed6e8e3462cadc066e386d3bdb1ae57777cd3a99", "committedDate": "2020-11-04T14:06:37Z", "message": "More Javadoc"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMzk0MjA0", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#pullrequestreview-523394204", "createdAt": "2020-11-04T14:09:21Z", "commit": {"oid": "ed6e8e3462cadc066e386d3bdb1ae57777cd3a99"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d59d83d81f8079302fab7a7b2641166f8df692f8", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/d59d83d81f8079302fab7a7b2641166f8df692f8", "committedDate": "2020-11-11T13:34:28Z", "message": "Do not list licenses without a dependency\n\nSome dependencies are present only either in main distribution or in\nseparate download. This way we don't list a license without at least one\ndependency present."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1611c55fd7a4283fff1a2eab2fcaf720543f0803", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/1611c55fd7a4283fff1a2eab2fcaf720543f0803", "committedDate": "2020-11-11T13:34:28Z", "message": "Add unified file connector\n\nThis commit adds unified file connector API and its implementation for\nlocal filesystem."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c19183e7dadb1973643cd6bbda5c485b8552be33", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/c19183e7dadb1973643cd6bbda5c485b8552be33", "committedDate": "2020-11-11T13:34:28Z", "message": "Implement unified file connector for hadoop"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1900a3e96f27177c93213abdd51197b5498eddc4", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/1900a3e96f27177c93213abdd51197b5498eddc4", "committedDate": "2020-11-11T13:34:28Z", "message": "Add modules for s3, gcs, azure and hadoop-all"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3f3f0d44de035aaf8a17f2cccf36bc5c269b05bc", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/3f3f0d44de035aaf8a17f2cccf36bc5c269b05bc", "committedDate": "2020-11-11T13:34:28Z", "message": "Add TDD for File Data Ingestion"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db3f8bb00606ca9ae5825f51a5e4e56f1967adc7", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/db3f8bb00606ca9ae5825f51a5e4e56f1967adc7", "committedDate": "2020-11-12T08:17:52Z", "message": "Add documentation for Unified File Connector"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5800b6cf0c95bcebc40afe5a4e2a829f8aa49c94", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/5800b6cf0c95bcebc40afe5a4e2a829f8aa49c94", "committedDate": "2020-11-12T08:17:52Z", "message": "Override guava version in transitive dependencies"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "57c4f7c0bcf2251b6ca6042c7856e8b93cf5b923", "author": {"user": {"login": "mtopolnik", "name": "Marko Topolnik"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/57c4f7c0bcf2251b6ca6042c7856e8b93cf5b923", "committedDate": "2020-11-12T08:17:52Z", "message": "Improve Javadoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa0c1adab037aeebf77b72b6bb653d288f0ae9d8", "author": {"user": {"login": "mtopolnik", "name": "Marko Topolnik"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/aa0c1adab037aeebf77b72b6bb653d288f0ae9d8", "committedDate": "2020-11-12T08:17:52Z", "message": "Nullability, Javadoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a878d25dcbdae932513f91031d6ea430fb449ed9", "author": {"user": {"login": "mtopolnik", "name": "Marko Topolnik"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/a878d25dcbdae932513f91031d6ea430fb449ed9", "committedDate": "2020-11-12T08:17:52Z", "message": "More Javadoc"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c7a5c7da7971eaf8827f09ea85702bb514162a5c", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/c7a5c7da7971eaf8827f09ea85702bb514162a5c", "committedDate": "2020-11-12T08:17:52Z", "message": "Add support for sharedFileSystem flag"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6216722ab769777773720f1f00178cde84224269", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/6216722ab769777773720f1f00178cde84224269", "committedDate": "2020-11-12T08:17:52Z", "message": "Add test with glob in the middle of filename"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "92fb5360c7e80808d589c9c21bae0f4d872d449a", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/92fb5360c7e80808d589c9c21bae0f4d872d449a", "committedDate": "2020-11-12T08:17:52Z", "message": "Add test for glob in path (supported by hadoop, not by local file\nconnector)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5fd6815598dc26d79a8b6836dce6d5f4c48678b7", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/5fd6815598dc26d79a8b6836dce6d5f4c48678b7", "committedDate": "2020-11-12T08:28:47Z", "message": "Make FileFormat constructors package private"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ed6e8e3462cadc066e386d3bdb1ae57777cd3a99", "author": {"user": {"login": "mtopolnik", "name": "Marko Topolnik"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/ed6e8e3462cadc066e386d3bdb1ae57777cd3a99", "committedDate": "2020-11-04T14:06:37Z", "message": "More Javadoc"}, "afterCommit": {"oid": "e082892d8063ca9f4683156d4af5333286b28daf", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/e082892d8063ca9f4683156d4af5333286b28daf", "committedDate": "2020-10-30T12:40:08Z", "message": "Add support for Azure Data Lake Gen 2"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e082892d8063ca9f4683156d4af5333286b28daf", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/e082892d8063ca9f4683156d4af5333286b28daf", "committedDate": "2020-10-30T12:40:08Z", "message": "Add support for Azure Data Lake Gen 2"}, "afterCommit": {"oid": "5fd6815598dc26d79a8b6836dce6d5f4c48678b7", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/5fd6815598dc26d79a8b6836dce6d5f4c48678b7", "committedDate": "2020-11-12T08:28:47Z", "message": "Make FileFormat constructors package private"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI4OTA5MTcy", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#pullrequestreview-528909172", "createdAt": "2020-11-12T09:41:11Z", "commit": {"oid": "c7a5c7da7971eaf8827f09ea85702bb514162a5c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwOTo0MToxMVrOHxybWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwOTo0MToxMVrOHxybWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTk2ODQ3Mg==", "bodyText": "Boolean getters tend to be confusing:\nFileSourceBuilder<String> source = FileSources.files(\"src/test/resources/directory/\")\n                                              .withFormat(FileFormat.lines());\nsource.sharedFileSystem();\nUnless there's a smart IDE warning that warns you're ignoring the boolean return value (you're ignoring it all the time anyway, with return this methods), this will be a very surprising thing.\nA better name isisSharedFileSystem.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r521968472", "createdAt": "2020-11-12T09:41:11Z", "author": {"login": "mtopolnik"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/FileSourceBuilder.java", "diffHunk": "@@ -163,6 +189,13 @@ public String path() {\n         return format;\n     }\n \n+    /**\n+     * Returns if the filesystem is shared. Only valid for local filesystem, distributed filesystems are always shared.\n+     */\n+    public boolean sharedFileSystem() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7a5c7da7971eaf8827f09ea85702bb514162a5c"}, "originalPosition": 47}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fee408de9eac16c424fd2e95bf70ebf87f0d5aa2", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/fee408de9eac16c424fd2e95bf70ebf87f0d5aa2", "committedDate": "2020-11-12T11:35:30Z", "message": "Address Marko's comments on naming of builder methods"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0734140c5b02fe396708c38509aa306e02fabb85", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/0734140c5b02fe396708c38509aa306e02fabb85", "committedDate": "2020-11-12T14:34:58Z", "message": "Move CSV to separate module"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/dcef04e03e01bb7f2a3ee111156d663fdbe7662f", "committedDate": "2020-11-12T14:34:58Z", "message": "Add @since to javadoc"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5ODE0NzM3", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#pullrequestreview-529814737", "createdAt": "2020-11-13T07:02:20Z", "commit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwNzowMjoyMFrOHyfVFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwNzo0OTo1MlrOHyh-Lg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjcwNDE1MQ==", "bodyText": "why don't we use Nonnull and force user to use the no-argument variant. We use this convention for LinesTextFileFormat and TextFileFormat for example.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r522704151", "createdAt": "2020-11-13T07:02:20Z", "author": {"login": "gurbuzali"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/FileFormat.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.charset.Charset;\n+\n+/**\n+ * Identifies the data format of a file to be used as a Jet data source.\n+ * This is a data object that holds the configuration; actual implementation\n+ * code is looked up elsewhere, by using this object as a key.\n+ *\n+ * @param <T> type of items a source using this file format will emit\n+ * @since 4.4\n+ */\n+public interface FileFormat<T> {\n+\n+    /**\n+     * Returns the unique identifier of the file format. The convention is to\n+     * use the well-known filename suffix or, if there is none, a short-form\n+     * name of the format.\n+     */\n+    @Nonnull\n+    String format();\n+\n+\n+    // Factory methods for supported file formats are here for easy discoverability.\n+\n+    /**\n+     * Returns a file format for Avro files.\n+     */\n+    @Nonnull\n+    static <T> AvroFileFormat<T> avro() {\n+        return avro(null);\n+    }\n+\n+    /**\n+     * Returns a file format for Avro files that specifies to use reflection\n+     * to deserialize the data into instances of the provided Java class.\n+     * Jet will use the {@code ReflectDatumReader} to read Avro data. The\n+     * parameter may be {@code null}, disabling the option to deserialize\n+     * using reflection, but for that case you should prefer the no-argument\n+     * {@link #avro()} call.\n+     */\n+    @Nonnull\n+    static <T> AvroFileFormat<T> avro(@Nullable Class<T> clazz) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjcwODAxNQ==", "bodyText": "I would make this Nonnull too. By default it is null, if user explicitly calls it then the user wants to configure the class.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r522708015", "createdAt": "2020-11-13T07:07:00Z", "author": {"login": "gurbuzali"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/AvroFileFormat.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * {@link FileFormat} for avro files. See {@link FileFormat#avro} for more\n+ * details.\n+ *\n+ * @param <T> type of items a source using this file format will emit\n+ * @since 4.4\n+ */\n+public class AvroFileFormat<T> implements FileFormat<T> {\n+\n+    /**\n+     * Format id for Avro.\n+     */\n+    public static final String FORMAT_AVRO = \"avro\";\n+\n+    private Class<T> reflectClass;\n+\n+    /**\n+     * Creates {@link AvroFileFormat}. See {@link FileFormat#avro} for more\n+     * details.\n+     */\n+    AvroFileFormat() {\n+    }\n+\n+    /**\n+     * Specifies to use reflection to deserialize data into the given class.\n+     * Jet will use the {@code ReflectDatumReader} to read Avro data. The\n+     * parameter may be {@code null}, this disables the option to deserialize\n+     * using reflection.\n+     *\n+     * @param reflectClass class to deserialize data into\n+     */\n+    @Nonnull\n+    public AvroFileFormat<T> withReflect(@Nullable Class<T> reflectClass) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjc0NTM5NQ==", "bodyText": "I think the logic to find if the path is a directory or a file is wrong.  the comment says:\n        // We can't ask the filesystem because this code runs on the client, which\n        // is likely a different machine than the cluster members. So this is a\n        // best guess, we assume that directories end with '/'.\n\nLet's say I want to read the files in the directory /user/home/tmp and I didn't put a / to the end.\nThe source tries to read the file tmp in the parent directory /usr/home. Since there is no such file in the directory (uses file-name as glob), the source just completes.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r522745395", "createdAt": "2020-11-13T07:47:44Z", "author": {"login": "gurbuzali"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file.impl;\n+\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.datamodel.Tuple2;\n+import com.hazelcast.jet.impl.util.IOUtil;\n+import com.hazelcast.jet.json.JsonUtil;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.Sources;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import javax.annotation.Nonnull;\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+import java.util.stream.Stream;\n+\n+import static com.hazelcast.jet.datamodel.Tuple2.tuple2;\n+import static com.hazelcast.jet.impl.util.Util.uncheckCall;\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.util.Objects.requireNonNull;\n+\n+/**\n+ * Implementation of FileSourceFactory for the local filesystem.\n+ */\n+public class LocalFileSourceFactory implements FileSourceFactory {\n+\n+    private static Map<String, ReadFileFnProvider> readFileFnProviders;\n+\n+    static {\n+        Map<String, ReadFileFnProvider> mapFns = new HashMap<>();\n+\n+        addMapFnProvider(mapFns, new JsonReadFileFnProvider());\n+        addMapFnProvider(mapFns, new LinesReadFileFnProvider());\n+        addMapFnProvider(mapFns, new ParquetReadFileFnProvider());\n+        addMapFnProvider(mapFns, new RawBytesReadFileFnProvider());\n+        addMapFnProvider(mapFns, new TextReadFileFnProvider());\n+\n+        ServiceLoader<ReadFileFnProvider> loader = ServiceLoader.load(ReadFileFnProvider.class);\n+        for (ReadFileFnProvider readFileFnProvider : loader) {\n+            addMapFnProvider(mapFns, readFileFnProvider);\n+        }\n+\n+        LocalFileSourceFactory.readFileFnProviders = Collections.unmodifiableMap(mapFns);\n+    }\n+\n+    private static void addMapFnProvider(Map<String, ReadFileFnProvider> mapFns, ReadFileFnProvider provider) {\n+        mapFns.put(provider.format(), provider);\n+    }\n+\n+    @Nonnull @Override\n+    public <T> BatchSource<T> create(@Nonnull FileSourceBuilder<T> builder) {\n+        Tuple2<String, String> dirAndGlob = deriveDirectoryAndGlobFromPath(builder.path());\n+        assert dirAndGlob.f0() != null && dirAndGlob.f1() != null;\n+\n+        FileFormat<T> format = requireNonNull(builder.format());\n+        ReadFileFnProvider readFileFnProvider = readFileFnProviders.get(format.format());\n+        FunctionEx<Path, Stream<T>> mapFn = readFileFnProvider.createReadFileFn(format);\n+        return Sources.filesBuilder(dirAndGlob.f0())\n+                      .glob(dirAndGlob.f1())\n+                      .sharedFileSystem(builder.isSharedFileSystem())\n+                      .build(mapFn);\n+    }\n+\n+    private Tuple2<String, String> deriveDirectoryAndGlobFromPath(String path) {\n+        Path p = Paths.get(path);\n+\n+        String directory;\n+        String glob = \"*\";\n+        if (isDirectory(path)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjc0NzQzOA==", "bodyText": "readFileFnProvider can be null if user provides a custom file format ?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r522747438", "createdAt": "2020-11-13T07:49:52Z", "author": {"login": "gurbuzali"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file.impl;\n+\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.datamodel.Tuple2;\n+import com.hazelcast.jet.impl.util.IOUtil;\n+import com.hazelcast.jet.json.JsonUtil;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.Sources;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import javax.annotation.Nonnull;\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+import java.util.stream.Stream;\n+\n+import static com.hazelcast.jet.datamodel.Tuple2.tuple2;\n+import static com.hazelcast.jet.impl.util.Util.uncheckCall;\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.util.Objects.requireNonNull;\n+\n+/**\n+ * Implementation of FileSourceFactory for the local filesystem.\n+ */\n+public class LocalFileSourceFactory implements FileSourceFactory {\n+\n+    private static Map<String, ReadFileFnProvider> readFileFnProviders;\n+\n+    static {\n+        Map<String, ReadFileFnProvider> mapFns = new HashMap<>();\n+\n+        addMapFnProvider(mapFns, new JsonReadFileFnProvider());\n+        addMapFnProvider(mapFns, new LinesReadFileFnProvider());\n+        addMapFnProvider(mapFns, new ParquetReadFileFnProvider());\n+        addMapFnProvider(mapFns, new RawBytesReadFileFnProvider());\n+        addMapFnProvider(mapFns, new TextReadFileFnProvider());\n+\n+        ServiceLoader<ReadFileFnProvider> loader = ServiceLoader.load(ReadFileFnProvider.class);\n+        for (ReadFileFnProvider readFileFnProvider : loader) {\n+            addMapFnProvider(mapFns, readFileFnProvider);\n+        }\n+\n+        LocalFileSourceFactory.readFileFnProviders = Collections.unmodifiableMap(mapFns);\n+    }\n+\n+    private static void addMapFnProvider(Map<String, ReadFileFnProvider> mapFns, ReadFileFnProvider provider) {\n+        mapFns.put(provider.format(), provider);\n+    }\n+\n+    @Nonnull @Override\n+    public <T> BatchSource<T> create(@Nonnull FileSourceBuilder<T> builder) {\n+        Tuple2<String, String> dirAndGlob = deriveDirectoryAndGlobFromPath(builder.path());\n+        assert dirAndGlob.f0() != null && dirAndGlob.f1() != null;\n+\n+        FileFormat<T> format = requireNonNull(builder.format());\n+        ReadFileFnProvider readFileFnProvider = readFileFnProviders.get(format.format());\n+        FunctionEx<Path, Stream<T>> mapFn = readFileFnProvider.createReadFileFn(format);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 89}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMDc1Mzgx", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#pullrequestreview-530075381", "createdAt": "2020-11-13T13:49:51Z", "commit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwODAwMzk2", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#pullrequestreview-530800396", "createdAt": "2020-11-15T13:49:13Z", "commit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxMzo0OToxM1rOHzf2sA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxNDoxMjo0MVrOHzgCEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2MTMyOA==", "bodyText": "why do we call initialize here explicitly while for other FileInputFormats we don't?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523761328", "createdAt": "2020-11-15T13:49:13Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/WholeFileInputFormat.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.BytesWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+\n+\n+/**\n+ * Adapted from code example from book\n+ * Hadoop: The Definitive Guide, Fourth Edition by Tom White (O'Reilly, 2014)\n+ * https://github.com/tomwhite/hadoop-book/blob/master/ch08-mr-types/src/main/java/WholeFileInputFormat.java\n+ */\n+public class WholeFileInputFormat extends FileInputFormat<NullWritable, BytesWritable> {\n+\n+    @Override\n+    protected boolean isSplitable(JobContext context, Path file) {\n+        return false;\n+    }\n+\n+    @Override\n+    public RecordReader<NullWritable, BytesWritable> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+        WholeFileRecordReader reader = new WholeFileRecordReader();\n+        reader.initialize(split, context);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2MTgxOA==", "bodyText": "we can use ReflectionUtils.loadClass() here. it removes the necessity of catching ClassNotFoundException", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523761818", "createdAt": "2020-11-15T13:52:42Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/CsvInputFormat.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import com.fasterxml.jackson.databind.MappingIterator;\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+public class CsvInputFormat extends FileInputFormat<NullWritable, Object> {\n+\n+    public static final String CSV_INPUT_FORMAT_BEAN_CLASS = \"csv.bean.class\";\n+\n+    @Override\n+    public RecordReader<NullWritable, Object> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+\n+        return new RecordReader<NullWritable, Object>() {\n+\n+            private Object current;\n+            private MappingIterator<Object> iterator;\n+\n+            @Override\n+            public void initialize(InputSplit split, TaskAttemptContext context) throws IOException {\n+\n+                FileSplit fileSplit = (FileSplit) split;\n+                Configuration conf = context.getConfiguration();\n+\n+                try {\n+\n+                    Configuration configuration = context.getConfiguration();\n+                    String className = configuration.get(CSV_INPUT_FORMAT_BEAN_CLASS);\n+                    Class<?> clazz = Thread.currentThread().getContextClassLoader().loadClass(className);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2MjYzMw==", "bodyText": "we can use ReflectionUtils.loadClass() here", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523762633", "createdAt": "2020-11-15T13:59:28Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/JsonInputFormat.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import com.hazelcast.jet.json.JsonUtil;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.hadoop.io.compress.CompressionCodecFactory;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.LineRecordReader;\n+\n+import java.io.IOException;\n+\n+public class JsonInputFormat extends FileInputFormat<LongWritable, Object> {\n+\n+    public static final String JSON_INPUT_FORMAT_BEAN_CLASS = \"json.bean.class\";\n+\n+    @Override\n+    public RecordReader<LongWritable, Object> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+\n+        try {\n+            Configuration configuration = context.getConfiguration();\n+            String className = configuration.get(JSON_INPUT_FORMAT_BEAN_CLASS);\n+            Class<?> clazz = Thread.currentThread().getContextClassLoader().loadClass(className);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2MzAwNA==", "bodyText": "can be simplified as\nreturn codec == null || codec instanceof SplittableCompressionCodec;", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523763004", "createdAt": "2020-11-15T14:02:21Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/JsonInputFormat.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import com.hazelcast.jet.json.JsonUtil;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.hadoop.io.compress.CompressionCodecFactory;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.LineRecordReader;\n+\n+import java.io.IOException;\n+\n+public class JsonInputFormat extends FileInputFormat<LongWritable, Object> {\n+\n+    public static final String JSON_INPUT_FORMAT_BEAN_CLASS = \"json.bean.class\";\n+\n+    @Override\n+    public RecordReader<LongWritable, Object> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+\n+        try {\n+            Configuration configuration = context.getConfiguration();\n+            String className = configuration.get(JSON_INPUT_FORMAT_BEAN_CLASS);\n+            Class<?> clazz = Thread.currentThread().getContextClassLoader().loadClass(className);\n+\n+            return new RecordReader<LongWritable, Object>() {\n+\n+                final LineRecordReader reader = new LineRecordReader();\n+\n+                @Override\n+                public void initialize(InputSplit split, TaskAttemptContext context) throws IOException {\n+                    reader.initialize(split, context);\n+                }\n+\n+                @Override\n+                public boolean nextKeyValue() throws IOException {\n+                    return reader.nextKeyValue();\n+                }\n+\n+                @Override\n+                public LongWritable getCurrentKey() {\n+                    return reader.getCurrentKey();\n+                }\n+\n+                @Override\n+                public Object getCurrentValue() throws IOException {\n+                    return JsonUtil.beanFrom(reader.getCurrentValue().toString(), clazz);\n+                }\n+\n+                @Override\n+                public float getProgress() throws IOException {\n+                    return reader.getProgress();\n+                }\n+\n+                @Override\n+                public void close() throws IOException {\n+                    reader.close();\n+                }\n+            };\n+\n+        } catch (ClassNotFoundException e) {\n+            throw new RuntimeException(e);\n+        }\n+\n+    }\n+\n+    @Override\n+    protected boolean isSplitable(JobContext context, Path file) {\n+        final CompressionCodec codec = new CompressionCodecFactory(context.getConfiguration()).getCodec(file);\n+        if (null == codec) {\n+            return true;\n+        }\n+        return codec instanceof SplittableCompressionCodec;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2MzU5Mg==", "bodyText": "like WholeFileInputFormat why do we call initialize explicitly?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523763592", "createdAt": "2020-11-15T14:07:19Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/WholeTextInputFormat.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+\n+\n+/**\n+ * Adapted from code example from book\n+ * Hadoop: The Definitive Guide, Fourth Edition by Tom White (O'Reilly, 2014)\n+ * https://github.com/tomwhite/hadoop-book/blob/master/ch08-mr-types/src/main/java/WholeFileInputFormat.java\n+ */\n+public class WholeTextInputFormat extends FileInputFormat<NullWritable, Text> {\n+\n+    @Override\n+    protected boolean isSplitable(JobContext context, Path file) {\n+        return false;\n+    }\n+\n+    @Override\n+    public RecordReader<NullWritable, Text> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+        WholeTextRecordReader reader = new WholeTextRecordReader();\n+        reader.initialize(split, context);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2Mzk3NQ==", "bodyText": "WholeTextRecordReader and WholeFileRecordReader looks exactly the same except the generic type of the value. can we merge them somehow?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523763975", "createdAt": "2020-11-15T14:10:28Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/WholeTextRecordReader.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Adapted from code example from book\n+ * Hadoop: The Definitive Guide, Fourth Edition by Tom White (O'Reilly, 2014)\n+ * https://github.com/tomwhite/hadoop-book/blob/master/ch08-mr-types/src/main/java/WholeFileRecordReader.java\n+ */\n+class WholeTextRecordReader extends RecordReader<NullWritable, Text> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2NDI0MA==", "bodyText": "we should do a null check here for custom file format", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523764240", "createdAt": "2020-11-15T14:12:41Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/HadoopFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import com.hazelcast.function.BiFunctionEx;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.hadoop.HadoopSources;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.file.AvroFileFormat;\n+import com.hazelcast.jet.pipeline.file.CsvFileFormat;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import com.hazelcast.jet.pipeline.file.impl.FileSourceFactory;\n+import org.apache.avro.Schema;\n+import org.apache.avro.mapred.AvroKey;\n+import org.apache.avro.mapreduce.AvroJob;\n+import org.apache.avro.mapreduce.AvroKeyInputFormat;\n+import org.apache.avro.reflect.ReflectData;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.BytesWritable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n+import org.apache.parquet.avro.AvroParquetInputFormat;\n+\n+import javax.annotation.Nonnull;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+import static com.hazelcast.jet.hadoop.impl.CsvInputFormat.CSV_INPUT_FORMAT_BEAN_CLASS;\n+import static com.hazelcast.jet.hadoop.impl.JsonInputFormat.JSON_INPUT_FORMAT_BEAN_CLASS;\n+import static java.util.Objects.requireNonNull;\n+\n+/**\n+ * Hadoop-based implementation of {@link FileSourceFactory}.\n+ */\n+public class HadoopFileSourceFactory implements FileSourceFactory {\n+\n+    private final Map<String, JobConfigurer> configs;\n+\n+    /**\n+     * Creates the HadoopSourceFactory.\n+     */\n+    public HadoopFileSourceFactory() {\n+        configs = new HashMap<>();\n+\n+        configs.put(AvroFileFormat.FORMAT_AVRO, new AvroFormatJobConfigurer());\n+        configs.put(CsvFileFormat.FORMAT_CSV, new CsvFormatJobConfigurer());\n+        configs.put(JsonFileFormat.FORMAT_JSONL, new JsonFormatJobConfigurer());\n+        configs.put(LinesTextFileFormat.FORMAT_LINES, new LineTextJobConfigurer());\n+        configs.put(ParquetFileFormat.FORMAT_PARQUET, new ParquetFormatJobConfigurer());\n+        configs.put(RawBytesFileFormat.FORMAT_BIN, new RawBytesFormatJobConfigurer());\n+        configs.put(TextFileFormat.FORMAT_TXT, new TextJobConfigurer());\n+    }\n+\n+    @Nonnull @Override\n+    @SuppressWarnings(\"unchecked\")\n+    public <T> BatchSource<T> create(@Nonnull FileSourceBuilder<T> builder) {\n+\n+        try {\n+            Job job = Job.getInstance();\n+\n+            Configuration configuration = job.getConfiguration();\n+            for (Entry<String, String> option : builder.options().entrySet()) {\n+                configuration.set(option.getKey(), option.getValue());\n+            }\n+\n+            FileInputFormat.addInputPath(job, new Path(builder.path()));\n+\n+            FileFormat<T> fileFormat = requireNonNull(builder.format());\n+            JobConfigurer configurer = configs.get(fileFormat.format());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 96}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwODAzMTY2", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#pullrequestreview-530803166", "createdAt": "2020-11-15T14:22:36Z", "commit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxNDoyMjozNlrOHzgGpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxNDoyNzoxM1rOHzgI8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2NTQxNQ==", "bodyText": "this is not necessary, the instances are terminated at the end of the test already.\nwe should also move the instance creation to a setup method so that new instances are not created for each call of this method.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523765415", "createdAt": "2020-11-15T14:22:36Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/test/java/com/hazelcast/jet/hadoop/file/BaseFileFormatTest.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.file;\n+\n+import com.hazelcast.function.ConsumerEx;\n+import com.hazelcast.jet.JetInstance;\n+import com.hazelcast.jet.core.JetTestSupport;\n+import com.hazelcast.jet.pipeline.Pipeline;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.test.Assertions;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Parameterized.Parameter;\n+import org.junit.runners.Parameterized.Parameters;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+\n+@RunWith(Parameterized.class)\n+public abstract class BaseFileFormatTest extends JetTestSupport {\n+\n+    @Parameter\n+    public boolean useHadoop;\n+\n+    @Parameters(name = \"{index}: useHadoop={0}\")\n+    public static Iterable<?> parameters() {\n+        return Arrays.asList(true, false);\n+    }\n+\n+    @SafeVarargs\n+    protected final <T> void assertItemsInSource(FileSourceBuilder<T> source, T... items) {\n+        assertItemsInSource(source, collected -> assertThat(collected).containsOnly(items));\n+    }\n+\n+    protected <T> void assertItemsInSource(\n+            FileSourceBuilder<T> source, ConsumerEx<List<T>> assertion\n+    ) {\n+        assertItemsInSource(1, source, assertion);\n+    }\n+\n+    protected <T> void assertItemsInSource(\n+            int memberCount, FileSourceBuilder<T> source, ConsumerEx<List<T>> assertion\n+    ) {\n+        if (useHadoop) {\n+            source.useHadoopForLocalFiles(true);\n+        }\n+\n+        Pipeline p = Pipeline.create();\n+\n+        p.readFrom(source.build())\n+         .apply(Assertions.assertCollected(assertion));\n+\n+        JetInstance[] jets = createJetMembers(memberCount);\n+        jets[0].newJob(p).join();\n+\n+        for (JetInstance jet : jets) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2NTc2NQ==", "bodyText": "why don't we pre-create this file in resources like other file types?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523765765", "createdAt": "2020-11-15T14:25:26Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/test/java/com/hazelcast/jet/hadoop/file/AvroFileFormatTest.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.file;\n+\n+import com.hazelcast.jet.hadoop.file.generated.SpecificUser;\n+import com.hazelcast.jet.hadoop.file.model.User;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSources;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.specific.SpecificDatumWriter;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+\n+public class AvroFileFormatTest extends BaseFileFormatTest {\n+\n+    @Test\n+    public void shouldReadAvroWithSchema() throws Exception {\n+        createAvroFile();\n+\n+        FileSourceBuilder<SpecificUser> source = FileSources.files(\"target/avro/file.avro\")\n+                                                            .format(FileFormat.avro());\n+        assertItemsInSource(source,\n+                new SpecificUser(\"Frantisek\", 7),\n+                new SpecificUser(\"Ali\", 42)\n+        );\n+\n+    }\n+\n+    @Test\n+    public void shouldReadAvroWithReflection() throws Exception {\n+        createAvroFile();\n+\n+        FileSourceBuilder<User> source = FileSources.files(\"target/avro/file.avro\")\n+                                                    .format(FileFormat.avro(User.class));\n+\n+        assertItemsInSource(source,\n+                new User(\"Frantisek\", 7),\n+                new User(\"Ali\", 42)\n+        );\n+    }\n+\n+    private static void createAvroFile() throws IOException {\n+        Path inputPath = new Path(\"target/avro\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2NjAwMg==", "bodyText": "like avro file, can't we keep this created file in resources rather than creating it on the fly?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523766002", "createdAt": "2020-11-15T14:27:13Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/test/java/com/hazelcast/jet/hadoop/file/ParquetFileFormatTest.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.file;\n+\n+import com.hazelcast.jet.hadoop.file.generated.SpecificUser;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSources;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.avro.AvroParquetWriter;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.junit.Test;\n+import org.junit.runners.Parameterized.Parameters;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+public class ParquetFileFormatTest extends BaseFileFormatTest {\n+\n+    // Parquet has a dependency on Hadoop so it does not make sense to run it without it\n+    @Parameters(name = \"{index}: useHadoop={0}\")\n+    public static Iterable<?> parameters() {\n+        return Arrays.asList(true);\n+    }\n+\n+    @Test\n+    public void shouldReadParquetFile() throws Exception {\n+        createParquetFile();\n+\n+        FileSourceBuilder<SpecificUser> source = FileSources.files(\"target/parquet/file.parquet\")\n+                                                            .format(FileFormat.parquet());\n+        assertItemsInSource(source,\n+                new SpecificUser(\"Frantisek\", 7),\n+                new SpecificUser(\"Ali\", 42)\n+        );\n+    }\n+\n+    private void createParquetFile() throws IOException {\n+        Path inputPath = new Path(\"target/parquet\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 56}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ea26ce7d61b75e437cf4f245bcbbad99ccfd6aa2", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/ea26ce7d61b75e437cf4f245bcbbad99ccfd6aa2", "committedDate": "2020-11-23T07:39:36Z", "message": "Ignore unknown columns in CSV files"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7c9b383b51377c3614b90613f8c906ff93a38514", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/7c9b383b51377c3614b90613f8c906ff93a38514", "committedDate": "2020-11-23T07:56:50Z", "message": "Add examples for new unified file API"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "752f01f8ed399099e0f15cd7799a81fd43b5eb99", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/752f01f8ed399099e0f15cd7799a81fd43b5eb99", "committedDate": "2020-11-23T12:22:24Z", "message": "Add examples for cloud"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2882d426aa5e77b7a52df6eff408dc4575b06710", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/2882d426aa5e77b7a52df6eff408dc4575b06710", "committedDate": "2020-11-23T12:22:58Z", "message": "Merge branch 'master' into file-data-ingestion-api"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1594f2636b4c931f5528ebe856214a9f8a9dcb1d", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/1594f2636b4c931f5528ebe856214a9f8a9dcb1d", "committedDate": "2020-11-23T15:49:21Z", "message": "Add tests for custom format, check ReadFileFnProvider/JobConfigurer for null"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "74762648f6997ffd19a2904f4e774beb79b2a955", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/74762648f6997ffd19a2904f4e774beb79b2a955", "committedDate": "2020-11-23T20:55:48Z", "message": "Address review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "228786ac89441b81e426b1a1fdf523407a83f8e4", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/228786ac89441b81e426b1a1fdf523407a83f8e4", "committedDate": "2020-11-23T21:15:15Z", "message": "Avoid code duplication in WholeFileRecordReader and WholeTextRecordReader"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ffd0f83b9e22558bcb3c0f4de88a24fb76bb7f65", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/ffd0f83b9e22558bcb3c0f4de88a24fb76bb7f65", "committedDate": "2020-11-23T21:50:16Z", "message": "Move deriveDirectoryAndGlobFromPath from client to processor supplier running on Jet nodes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "43c5e55c50df0e58e8043a5e3fc0ff9100d2fe76", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/43c5e55c50df0e58e8043a5e3fc0ff9100d2fe76", "committedDate": "2020-11-24T22:23:30Z", "message": "Fix some glob corner cases"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1455c4ef7e83d815e5ee38a67db7358c4402366b", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/1455c4ef7e83d815e5ee38a67db7358c4402366b", "committedDate": "2020-11-24T22:57:43Z", "message": "Fix issues with windows\n\nIgnore hadoop and fix some path issues."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1035ecff69b1c1698ec13310a30fd554098b9a10", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/1035ecff69b1c1698ec13310a30fd554098b9a10", "committedDate": "2020-11-25T07:48:30Z", "message": "Add test for file with * in name (non-windows only)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "46663f49d7a4e0eb8db6bae56bfe0f0aa1ccd385", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/46663f49d7a4e0eb8db6bae56bfe0f0aa1ccd385", "committedDate": "2020-11-25T09:04:29Z", "message": "Clarify where to obtain the artifacts"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7efed9f765a6aa6ba8049bb3002659c711cfede5", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/7efed9f765a6aa6ba8049bb3002659c711cfede5", "committedDate": "2020-11-25T13:02:35Z", "message": "Change path parameter to path to directory and a glob"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b46df75977345e9bbef489536b1cd7b7f7d858f1", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/b46df75977345e9bbef489536b1cd7b7f7d858f1", "committedDate": "2020-11-25T13:28:43Z", "message": "Remove getters from the FileSourceBuilder"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4e07620930fb408a77f33cacf39e51dd441807a2", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/4e07620930fb408a77f33cacf39e51dd441807a2", "committedDate": "2020-11-25T13:50:24Z", "message": "Handle non existing directory and non matching glob"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3c2f04851b35a3b9cf285f7c248bfa11987f361a", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/3c2f04851b35a3b9cf285f7c248bfa11987f361a", "committedDate": "2020-11-25T15:13:36Z", "message": "Fix check for absolute path for hadoop paths"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "306159b5f1c7244c57349cd2209541c105cdd12f", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/306159b5f1c7244c57349cd2209541c105cdd12f", "committedDate": "2020-11-25T18:39:33Z", "message": "Add logs for nonmatching paths"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM4ODM1MjA5", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#pullrequestreview-538835209", "createdAt": "2020-11-25T20:44:59Z", "commit": {"oid": "306159b5f1c7244c57349cd2209541c105cdd12f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQyMDo0NDo1OVrOH6DVRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQyMDo0NDo1OVrOH6DVRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDYzNDA1Mw==", "bodyText": "this part needs an update following recent changes of the path parameter", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r530634053", "createdAt": "2020-11-25T20:44:59Z", "author": {"login": "gurbuzali"}, "path": "site/docs/api/sources-sinks.md", "diffHunk": "@@ -6,8 +6,368 @@ description: Birds-eye view of all pre-defined sources available in Jet.\n Hazelcast Jet comes out of the box with many different sources and sinks\n that you can work with, that are also referred to as _connectors_.\n \n+## Unified File Connector API\n+\n+> This section describes the Unified File Connector API introduced in\n+> Hazelcast Jet 4.4.\n+>\n+> As of version 4.4, the API provides source capability only.\n+> For sinks, see the [Files](#files) section.\n+\n+The Unified File Connector API provides a simple way to read files,\n+unified across different sources of the data. Using API this you can\n+read files from the local filesystem, HDFS and cloud storage systems\n+such as Amazon S3, Google Cloud Storage or Azure Blob Storage. At the\n+same time the connector supports various formats of the data - text\n+files, CSV, Json, Avro, etc., regardless of the source.\n+\n+### The Source\n+\n+Hazelcast Jet supports the following sources:\n+\n+* Local Filesystem (both shared and local to the member)\n+* Hadoop Distributed File System (HDFS)\n+* Amazon S3\n+* Google Cloud Storage\n+* Azure Cloud Storage\n+* Azure Data Lake (both generation 1 and generation 2)\n+\n+These are the officially supported sources. However, you can read from\n+any Hadoop compatible file system.\n+\n+Support for reading from the local filesystem is included in the base\n+distribution of Hazelcast Jet. You don't need any additional\n+dependencies. To access Hadoop or any of the cloud based stores use the\n+separately downloadable module. See the details in the\n+[Supported Storage Systems](#supported-storage-systems) section.\n+\n+The main entrypoint to the file connector is `FileSources.files`, which\n+takes a `path` as a String parameter and returns a `FileSourceBuilder`.\n+The following shows the simplest use of the file source, which reads a\n+text file line by line:\n+\n+```java\n+BatchSource<String> source = FileSources.files(\"path/to/my/file\")\n+                                        .build();\n+```\n+\n+The `path` parameter takes both relative and absolute paths. It can", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306159b5f1c7244c57349cd2209541c105cdd12f"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM4ODM2ODky", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#pullrequestreview-538836892", "createdAt": "2020-11-25T20:48:29Z", "commit": {"oid": "306159b5f1c7244c57349cd2209541c105cdd12f"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa8d8569faf66ec163f431a8111b210a454650c4", "author": {"user": {"login": "frant-hartm", "name": "Franti\u0161ek Hartman"}}, "url": "https://github.com/hazelcast/hazelcast-jet/commit/aa8d8569faf66ec163f431a8111b210a454650c4", "committedDate": "2020-11-26T08:58:00Z", "message": "Update documentation and example after glob API change"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3601, "cost": 1, "resetAt": "2021-11-01T14:51:55Z"}}}