{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ2MzAwODUy", "number": 2393, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOTo1MTozNFrOEOnKaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOTo1MTozNFrOEOnKaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNzU3MTYzOnYy", "diffSide": "LEFT", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/impl/execution/init/PartitionArrangement.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOTo1MTozNFrOGx2zxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNzo0Njo1N1rOG1DISA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkzMTM5Nw==", "bodyText": "are we removing this condition? if so, why?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2393#discussion_r454931397", "createdAt": "2020-07-15T09:51:34Z", "author": {"login": "cangencer"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/impl/execution/init/PartitionArrangement.java", "diffHunk": "@@ -35,62 +33,58 @@\n class PartitionArrangement {\n     /**\n      * Mapping from each remote member address to the partition IDs it owns.\n+     * Members without partitions are missing.\n      */\n-    final Supplier<Map<Address, int[]>> remotePartitionAssignment;\n-    private final Supplier<int[]> localPartitions;\n-    private final Supplier<int[]> allPartitions;\n+    private final Map<Address, int[]> remotePartitionAssignment;\n+\n+    /** An array of [0, 1, 2, ... partitionCount-1] */\n+    private final int[] allPartitions;\n+\n+    /** Array of local partitions */\n+    private final int[] localPartitions;\n \n     PartitionArrangement(Address[] partitionOwners, Address thisAddress) {\n-        localPartitions = memoize(() -> arrangeLocalPartitions(partitionOwners, thisAddress));\n-        allPartitions = memoize(() -> arrangeAllPartitions(partitionOwners, localPartitions.get()));\n-        remotePartitionAssignment = memoize(() -> remotePartitionAssignment(partitionOwners, thisAddress));\n+        localPartitions = IntStream.range(0, partitionOwners.length)\n+                .filter(partitionId -> thisAddress.equals(partitionOwners[partitionId]))\n+                .toArray();\n+\n+        allPartitions = IntStream.range(0, partitionOwners.length).toArray();\n+\n+        remotePartitionAssignment = IntStream.range(0, partitionOwners.length)\n+                .filter(partitionId -> !thisAddress.equals(partitionOwners[partitionId]))\n+                .boxed()\n+                .collect(groupingBy(partitionId -> partitionOwners[partitionId],\n+                        collectingAndThen(Collectors.toList(), l -> l.stream().mapToInt(i -> i).toArray())));\n+    }\n+\n+    Map<Address, int[]> getRemotePartitionAssignment() {\n+        return remotePartitionAssignment;\n     }\n \n     /**\n-     * Determines for each processor instance the partition IDs it will be in charge of\n-     * (processors are identified by their index). The method is called separately for\n-     * each edge. For a distributed edge, only partitions owned by the local member need\n-     * to be assigned; for a non-distributed edge, every partition ID must be assigned.\n-     * Local partitions will get the same assignments in both cases, and repeating the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e9e145e6e28fefd2c785e60bbbd487b34ebc6475"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzQwNTU4Mg==", "bodyText": "Yes, this PR is removing it? I'd reverse the question: why would we keep it? It's not used, it's not documented and it's very rarely beneficial. So the reason is to simplify the code.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2393#discussion_r457405582", "createdAt": "2020-07-20T13:53:42Z", "author": {"login": "viliam-durina"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/impl/execution/init/PartitionArrangement.java", "diffHunk": "@@ -35,62 +33,58 @@\n class PartitionArrangement {\n     /**\n      * Mapping from each remote member address to the partition IDs it owns.\n+     * Members without partitions are missing.\n      */\n-    final Supplier<Map<Address, int[]>> remotePartitionAssignment;\n-    private final Supplier<int[]> localPartitions;\n-    private final Supplier<int[]> allPartitions;\n+    private final Map<Address, int[]> remotePartitionAssignment;\n+\n+    /** An array of [0, 1, 2, ... partitionCount-1] */\n+    private final int[] allPartitions;\n+\n+    /** Array of local partitions */\n+    private final int[] localPartitions;\n \n     PartitionArrangement(Address[] partitionOwners, Address thisAddress) {\n-        localPartitions = memoize(() -> arrangeLocalPartitions(partitionOwners, thisAddress));\n-        allPartitions = memoize(() -> arrangeAllPartitions(partitionOwners, localPartitions.get()));\n-        remotePartitionAssignment = memoize(() -> remotePartitionAssignment(partitionOwners, thisAddress));\n+        localPartitions = IntStream.range(0, partitionOwners.length)\n+                .filter(partitionId -> thisAddress.equals(partitionOwners[partitionId]))\n+                .toArray();\n+\n+        allPartitions = IntStream.range(0, partitionOwners.length).toArray();\n+\n+        remotePartitionAssignment = IntStream.range(0, partitionOwners.length)\n+                .filter(partitionId -> !thisAddress.equals(partitionOwners[partitionId]))\n+                .boxed()\n+                .collect(groupingBy(partitionId -> partitionOwners[partitionId],\n+                        collectingAndThen(Collectors.toList(), l -> l.stream().mapToInt(i -> i).toArray())));\n+    }\n+\n+    Map<Address, int[]> getRemotePartitionAssignment() {\n+        return remotePartitionAssignment;\n     }\n \n     /**\n-     * Determines for each processor instance the partition IDs it will be in charge of\n-     * (processors are identified by their index). The method is called separately for\n-     * each edge. For a distributed edge, only partitions owned by the local member need\n-     * to be assigned; for a non-distributed edge, every partition ID must be assigned.\n-     * Local partitions will get the same assignments in both cases, and repeating the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkzMTM5Nw=="}, "originalCommit": {"oid": "e9e145e6e28fefd2c785e60bbbd487b34ebc6475"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI3ODk4NA==", "bodyText": "As discussed, currently we don't make use of this feature anywhere and it makes the code unnecessarily complex.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2393#discussion_r458278984", "createdAt": "2020-07-21T17:46:57Z", "author": {"login": "cangencer"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/impl/execution/init/PartitionArrangement.java", "diffHunk": "@@ -35,62 +33,58 @@\n class PartitionArrangement {\n     /**\n      * Mapping from each remote member address to the partition IDs it owns.\n+     * Members without partitions are missing.\n      */\n-    final Supplier<Map<Address, int[]>> remotePartitionAssignment;\n-    private final Supplier<int[]> localPartitions;\n-    private final Supplier<int[]> allPartitions;\n+    private final Map<Address, int[]> remotePartitionAssignment;\n+\n+    /** An array of [0, 1, 2, ... partitionCount-1] */\n+    private final int[] allPartitions;\n+\n+    /** Array of local partitions */\n+    private final int[] localPartitions;\n \n     PartitionArrangement(Address[] partitionOwners, Address thisAddress) {\n-        localPartitions = memoize(() -> arrangeLocalPartitions(partitionOwners, thisAddress));\n-        allPartitions = memoize(() -> arrangeAllPartitions(partitionOwners, localPartitions.get()));\n-        remotePartitionAssignment = memoize(() -> remotePartitionAssignment(partitionOwners, thisAddress));\n+        localPartitions = IntStream.range(0, partitionOwners.length)\n+                .filter(partitionId -> thisAddress.equals(partitionOwners[partitionId]))\n+                .toArray();\n+\n+        allPartitions = IntStream.range(0, partitionOwners.length).toArray();\n+\n+        remotePartitionAssignment = IntStream.range(0, partitionOwners.length)\n+                .filter(partitionId -> !thisAddress.equals(partitionOwners[partitionId]))\n+                .boxed()\n+                .collect(groupingBy(partitionId -> partitionOwners[partitionId],\n+                        collectingAndThen(Collectors.toList(), l -> l.stream().mapToInt(i -> i).toArray())));\n+    }\n+\n+    Map<Address, int[]> getRemotePartitionAssignment() {\n+        return remotePartitionAssignment;\n     }\n \n     /**\n-     * Determines for each processor instance the partition IDs it will be in charge of\n-     * (processors are identified by their index). The method is called separately for\n-     * each edge. For a distributed edge, only partitions owned by the local member need\n-     * to be assigned; for a non-distributed edge, every partition ID must be assigned.\n-     * Local partitions will get the same assignments in both cases, and repeating the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkzMTM5Nw=="}, "originalCommit": {"oid": "e9e145e6e28fefd2c785e60bbbd487b34ebc6475"}, "originalPosition": 60}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4669, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}