{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDg3MTk5OTA3", "number": 2518, "reviewThreads": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMjoxNzo0N1rOEjzXAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQyMDo0NDo1OVrOE9YvTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1OTc3MDg5OnYy", "diffSide": "RIGHT", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/AvroFileFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMjoxNzo0N1rOHSXiog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQyMjoxNzo0N1rOHSXiog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTAyMjExNA==", "bodyText": "we shouldn't use any class(NullWritable) from hadoop, it adds a dependency to hadoop even if user wants to read files without hadoop module.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r489022114", "createdAt": "2020-09-15T22:17:47Z", "author": {"login": "gurbuzali"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/AvroFileFormat.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file;\n+\n+import com.hazelcast.function.BiFunctionEx;\n+import com.hazelcast.function.FunctionEx;\n+import org.apache.avro.Schema;\n+import org.apache.avro.file.DataFileReader;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.mapred.AvroKey;\n+import org.apache.avro.mapreduce.AvroKeyInputFormat;\n+import org.apache.avro.reflect.ReflectData;\n+import org.apache.avro.reflect.ReflectDatumReader;\n+import org.apache.avro.specific.SpecificDatumReader;\n+import org.apache.hadoop.io.NullWritable;\n+\n+import java.nio.file.Path;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+\n+/**\n+ * FileFormat for avro files\n+ *\n+ *\n+ * @param <T>\n+ */\n+public class AvroFileFormat<T> extends AbstractFileFormat<AvroKey<T>, NullWritable, T>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "250662eb1edc7691ce12577016b295234a976157"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2MDc1MjgxOnYy", "diffSide": "RIGHT", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/CsvFileFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNTo0NDo0MVrOHSg-Fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQwNTo0NDo0MVrOHSg-Fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTE3NjU5OQ==", "bodyText": "we use jackson-dataformat here which adds a dependency to the core", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r489176599", "createdAt": "2020-09-16T05:44:41Z", "author": {"login": "gurbuzali"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/CsvFileFormat.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file;\n+\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.hazelcast.function.BiFunctionEx;\n+import com.hazelcast.function.FunctionEx;\n+import org.apache.hadoop.io.NullWritable;\n+\n+import java.io.InputStream;\n+import java.util.Spliterator;\n+import java.util.Spliterators;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+public class CsvFileFormat<T> extends AbstractFileFormat<NullWritable, T, T> implements FileFormat<NullWritable, T, T> {\n+\n+    public static final String CSV_INPUT_FORMAT_BEAN_CLASS = \"csv.bean.class\";\n+\n+    private final Class<T> clazz;\n+\n+    public CsvFileFormat(Class<T> clazz) {\n+        this.clazz = clazz;\n+\n+        withOption(INPUT_FORMAT_CLASS, \"com.hazelcast.jet.hadoop.impl.CsvInputFormat\");\n+        withOption(CSV_INPUT_FORMAT_BEAN_CLASS, clazz.getCanonicalName());\n+    }\n+\n+    @Override\n+    public FunctionEx<InputStream, Stream<T>> mapInputStreamFn() {\n+        CsvSchema schema = CsvSchema.emptySchema().withHeader();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "250662eb1edc7691ce12577016b295234a976157"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4ODgzNjgwOnYy", "diffSide": "RIGHT", "path": "extensions/avro/src/main/java/com/hazelcast/jet/avro/AvroMapFnProvider.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QxMjo0NDozNVrOHWrm2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwODowNDo1N1rOHXOkUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU0NTE3Ng==", "bodyText": "Couple the returned format to the format of handled FileFormat? Otherwise it can easily get out of sync.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r493545176", "createdAt": "2020-09-23T12:44:35Z", "author": {"login": "gierlachg"}, "path": "extensions/avro/src/main/java/com/hazelcast/jet/avro/AvroMapFnProvider.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.avro;\n+\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.pipeline.file.AvroFileFormat;\n+import com.hazelcast.jet.pipeline.file.impl.MapFnProvider;\n+import org.apache.avro.file.DataFileReader;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.reflect.ReflectDatumReader;\n+import org.apache.avro.specific.SpecificDatumReader;\n+\n+import java.nio.file.Path;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+\n+/**\n+ * MapFnProvider for Avro files, reading given path and deserializing using\n+ * avro DatumReader\n+ *\n+ * @param <T> type of the item emitted from the source\n+ */\n+public class AvroMapFnProvider<T> implements MapFnProvider<AvroFileFormat<T>, T> {\n+\n+    @Override\n+    public FunctionEx<Path, Stream<T>> create(AvroFileFormat<T> format) {\n+        Class<T> reflectClass = format.reflectClass();\n+        return (path) -> {\n+            DatumReader<T> datumReader = datumReader(reflectClass);\n+            DataFileReader<T> reader = new DataFileReader<>(path.toFile(), datumReader);\n+            return StreamSupport.stream(reader.spliterator(), false)\n+                                .onClose(() -> uncheckRun(reader::close));\n+        };\n+    }\n+\n+    private static <T> DatumReader<T> datumReader(Class<T> reflectClass) {\n+        return reflectClass == null ? new SpecificDatumReader<>() : new ReflectDatumReader<>(reflectClass);\n+    }\n+\n+    @Override\n+    public String format() {\n+        return \"avro\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5da2197c944aae3b23bde72c463d1ec289bf6fe8"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDExNzk3MA==", "bodyText": "Done.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r494117970", "createdAt": "2020-09-24T08:04:57Z", "author": {"login": "frant-hartm"}, "path": "extensions/avro/src/main/java/com/hazelcast/jet/avro/AvroMapFnProvider.java", "diffHunk": "@@ -0,0 +1,60 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.avro;\n+\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.pipeline.file.AvroFileFormat;\n+import com.hazelcast.jet.pipeline.file.impl.MapFnProvider;\n+import org.apache.avro.file.DataFileReader;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.reflect.ReflectDatumReader;\n+import org.apache.avro.specific.SpecificDatumReader;\n+\n+import java.nio.file.Path;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+\n+/**\n+ * MapFnProvider for Avro files, reading given path and deserializing using\n+ * avro DatumReader\n+ *\n+ * @param <T> type of the item emitted from the source\n+ */\n+public class AvroMapFnProvider<T> implements MapFnProvider<AvroFileFormat<T>, T> {\n+\n+    @Override\n+    public FunctionEx<Path, Stream<T>> create(AvroFileFormat<T> format) {\n+        Class<T> reflectClass = format.reflectClass();\n+        return (path) -> {\n+            DatumReader<T> datumReader = datumReader(reflectClass);\n+            DataFileReader<T> reader = new DataFileReader<>(path.toFile(), datumReader);\n+            return StreamSupport.stream(reader.spliterator(), false)\n+                                .onClose(() -> uncheckRun(reader::close));\n+        };\n+    }\n+\n+    private static <T> DatumReader<T> datumReader(Class<T> reflectClass) {\n+        return reflectClass == null ? new SpecificDatumReader<>() : new ReflectDatumReader<>(reflectClass);\n+    }\n+\n+    @Override\n+    public String format() {\n+        return \"avro\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU0NTE3Ng=="}, "originalCommit": {"oid": "5da2197c944aae3b23bde72c463d1ec289bf6fe8"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4OTAwMTM0OnYy", "diffSide": "RIGHT", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QxMzoxMjozNlrOHWtP7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwODowNToxNlrOHXOlFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU3MjA3Nw==", "bodyText": "Get the key from the provider? To keep them in sync?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r493572077", "createdAt": "2020-09-23T13:12:36Z", "author": {"login": "gierlachg"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file.impl;\n+\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.datamodel.Tuple2;\n+import com.hazelcast.jet.impl.util.IOUtil;\n+import com.hazelcast.jet.json.JsonUtil;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.Sources;\n+import com.hazelcast.jet.pipeline.file.CsvFileFormat;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSourceFactory;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+import java.util.Spliterator;\n+import java.util.Spliterators;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import static com.hazelcast.jet.datamodel.Tuple2.tuple2;\n+import static com.hazelcast.jet.impl.util.Util.uncheckCall;\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+\n+/**\n+ * Implementation of FileSourceFactory for local filesystem\n+ *\n+ * @param <T> type of the item emitted from the source\n+ */\n+public class LocalFileSourceFactory<T> implements FileSourceFactory<T> {\n+\n+    private Map<String, MapFnProvider<? extends FileFormat<?>, ?>> mapFns;\n+\n+    /**\n+     * Default constructor\n+     */\n+    public LocalFileSourceFactory() {\n+        mapFns = new HashMap<>();\n+\n+        mapFns.put(\"csv\", new CsvMapFnProvider());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5da2197c944aae3b23bde72c463d1ec289bf6fe8"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDExODE2NA==", "bodyText": "Done.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r494118164", "createdAt": "2020-09-24T08:05:16Z", "author": {"login": "frant-hartm"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file.impl;\n+\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.datamodel.Tuple2;\n+import com.hazelcast.jet.impl.util.IOUtil;\n+import com.hazelcast.jet.json.JsonUtil;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.Sources;\n+import com.hazelcast.jet.pipeline.file.CsvFileFormat;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSourceFactory;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+import java.util.Spliterator;\n+import java.util.Spliterators;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import static com.hazelcast.jet.datamodel.Tuple2.tuple2;\n+import static com.hazelcast.jet.impl.util.Util.uncheckCall;\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+\n+/**\n+ * Implementation of FileSourceFactory for local filesystem\n+ *\n+ * @param <T> type of the item emitted from the source\n+ */\n+public class LocalFileSourceFactory<T> implements FileSourceFactory<T> {\n+\n+    private Map<String, MapFnProvider<? extends FileFormat<?>, ?>> mapFns;\n+\n+    /**\n+     * Default constructor\n+     */\n+    public LocalFileSourceFactory() {\n+        mapFns = new HashMap<>();\n+\n+        mapFns.put(\"csv\", new CsvMapFnProvider());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU3MjA3Nw=="}, "originalCommit": {"oid": "5da2197c944aae3b23bde72c463d1ec289bf6fe8"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4OTAyNTMyOnYy", "diffSide": "RIGHT", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QxMzoxNjo0MFrOHWtewA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QxODowMjoyMVrOHW6VSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU3NTg3Mg==", "bodyText": "Why the '1' suffix?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r493575872", "createdAt": "2020-09-23T13:16:40Z", "author": {"login": "gierlachg"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file.impl;\n+\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.datamodel.Tuple2;\n+import com.hazelcast.jet.impl.util.IOUtil;\n+import com.hazelcast.jet.json.JsonUtil;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.Sources;\n+import com.hazelcast.jet.pipeline.file.CsvFileFormat;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSourceFactory;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+import java.util.Spliterator;\n+import java.util.Spliterators;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import static com.hazelcast.jet.datamodel.Tuple2.tuple2;\n+import static com.hazelcast.jet.impl.util.Util.uncheckCall;\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+\n+/**\n+ * Implementation of FileSourceFactory for local filesystem\n+ *\n+ * @param <T> type of the item emitted from the source\n+ */\n+public class LocalFileSourceFactory<T> implements FileSourceFactory<T> {\n+\n+    private Map<String, MapFnProvider<? extends FileFormat<?>, ?>> mapFns;\n+\n+    /**\n+     * Default constructor\n+     */\n+    public LocalFileSourceFactory() {\n+        mapFns = new HashMap<>();\n+\n+        mapFns.put(\"csv\", new CsvMapFnProvider());\n+        mapFns.put(\"jsonl\", new JsonMapFnProvider<>());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5da2197c944aae3b23bde72c463d1ec289bf6fe8"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzc4NjQ0MA==", "bodyText": "That's lowercase L. See https://jsonlines.org/, which is the actual supported format, not regular json.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r493786440", "createdAt": "2020-09-23T18:02:21Z", "author": {"login": "frant-hartm"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file.impl;\n+\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.datamodel.Tuple2;\n+import com.hazelcast.jet.impl.util.IOUtil;\n+import com.hazelcast.jet.json.JsonUtil;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.Sources;\n+import com.hazelcast.jet.pipeline.file.CsvFileFormat;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSourceFactory;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+import java.util.Spliterator;\n+import java.util.Spliterators;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import static com.hazelcast.jet.datamodel.Tuple2.tuple2;\n+import static com.hazelcast.jet.impl.util.Util.uncheckCall;\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+\n+/**\n+ * Implementation of FileSourceFactory for local filesystem\n+ *\n+ * @param <T> type of the item emitted from the source\n+ */\n+public class LocalFileSourceFactory<T> implements FileSourceFactory<T> {\n+\n+    private Map<String, MapFnProvider<? extends FileFormat<?>, ?>> mapFns;\n+\n+    /**\n+     * Default constructor\n+     */\n+    public LocalFileSourceFactory() {\n+        mapFns = new HashMap<>();\n+\n+        mapFns.put(\"csv\", new CsvMapFnProvider());\n+        mapFns.put(\"jsonl\", new JsonMapFnProvider<>());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU3NTg3Mg=="}, "originalCommit": {"oid": "5da2197c944aae3b23bde72c463d1ec289bf6fe8"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA4OTAyNzQ1OnYy", "diffSide": "RIGHT", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QxMzoxNzowNlrOHWtgFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QxMzoxNzowNlrOHWtgFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzU3NjIxNA==", "bodyText": "Just 'lines' instead of 'txt1'?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r493576214", "createdAt": "2020-09-23T13:17:06Z", "author": {"login": "gierlachg"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,241 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file.impl;\n+\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.datamodel.Tuple2;\n+import com.hazelcast.jet.impl.util.IOUtil;\n+import com.hazelcast.jet.json.JsonUtil;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.Sources;\n+import com.hazelcast.jet.pipeline.file.CsvFileFormat;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSourceFactory;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+import java.util.Spliterator;\n+import java.util.Spliterators;\n+import java.util.stream.Stream;\n+import java.util.stream.StreamSupport;\n+\n+import static com.hazelcast.jet.datamodel.Tuple2.tuple2;\n+import static com.hazelcast.jet.impl.util.Util.uncheckCall;\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+\n+/**\n+ * Implementation of FileSourceFactory for local filesystem\n+ *\n+ * @param <T> type of the item emitted from the source\n+ */\n+public class LocalFileSourceFactory<T> implements FileSourceFactory<T> {\n+\n+    private Map<String, MapFnProvider<? extends FileFormat<?>, ?>> mapFns;\n+\n+    /**\n+     * Default constructor\n+     */\n+    public LocalFileSourceFactory() {\n+        mapFns = new HashMap<>();\n+\n+        mapFns.put(\"csv\", new CsvMapFnProvider());\n+        mapFns.put(\"jsonl\", new JsonMapFnProvider<>());\n+        mapFns.put(\"txtl\", new LinesMapFnProvider());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5da2197c944aae3b23bde72c463d1ec289bf6fe8"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NzI3MDI5OnYy", "diffSide": "RIGHT", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/MapFnProvider.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwOToyNDoyNlrOHX8AHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwOToyNDoyNlrOHX8AHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg2MjM2NQ==", "bodyText": "This class is public API, shouldn't be in impl.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r494862365", "createdAt": "2020-09-25T09:24:26Z", "author": {"login": "mtopolnik"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/MapFnProvider.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file.impl;\n+\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+\n+import java.nio.file.Path;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Provides a mapping function from a Path to a Stream of items emitted from local filesystem source\n+ *\n+ * @param <F> FileFormat type\n+ * @param <T> type of the items emitted from the file source\n+ */\n+public interface MapFnProvider<F extends FileFormat<?>, T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad181730f72e87b1054c64207f6432048d0be93a"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NzM2NTA4OnYy", "diffSide": "RIGHT", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/FileSourceBuilder.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwOTo0OToxMFrOHX84ZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMTozNTozMVrOHhOsNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg3Njc3Mg==", "bodyText": "A factory method would be a nicer choice, I think. It removes the diamond operator and new, looks cleaner.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r494876772", "createdAt": "2020-09-25T09:49:10Z", "author": {"login": "mtopolnik"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/FileSourceBuilder.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file;\n+\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.file.impl.LocalFileSourceFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+/**\n+ * Builder for file sources\n+ * <p>\n+ * The builder works with local filesystem and with hadoop supported\n+ * filesystems.\n+ * <p>\n+ * The builder requires 'path' and 'format' parameters and creates a\n+ * {@link BatchSource}. The path specifies the location of the file(s)\n+ * and possibly the data source - s3a://, hdfs://, etc..\n+ * <p>\n+ * The format determines how the contents of the file is parsed and\n+ * also determines the type of the source items. E.g. the\n+ * {@link LinesTextFileFormat} returns each line as a String,\n+ * {@link JsonFileFormat} returns each line of a JSON Lines file\n+ * deserialized into an instance of a specified class.\n+ * <p>\n+ * You may also use Hadoop to read local files by specifying the\n+ * {@link #useHadoopForLocalFiles()} flag.\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ * BatchSource<User> source = new FileSourceBuilder(\"data/users.jsonl\")\n+ *   .withFormat(new JsonFileFormat<>(User.class))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad181730f72e87b1054c64207f6432048d0be93a"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYwNTc1MQ==", "bodyText": "Added factory methods for all formats.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r504605751", "createdAt": "2020-10-14T11:35:31Z", "author": {"login": "frant-hartm"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/FileSourceBuilder.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file;\n+\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.file.impl.LocalFileSourceFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+\n+import static java.util.Objects.requireNonNull;\n+\n+/**\n+ * Builder for file sources\n+ * <p>\n+ * The builder works with local filesystem and with hadoop supported\n+ * filesystems.\n+ * <p>\n+ * The builder requires 'path' and 'format' parameters and creates a\n+ * {@link BatchSource}. The path specifies the location of the file(s)\n+ * and possibly the data source - s3a://, hdfs://, etc..\n+ * <p>\n+ * The format determines how the contents of the file is parsed and\n+ * also determines the type of the source items. E.g. the\n+ * {@link LinesTextFileFormat} returns each line as a String,\n+ * {@link JsonFileFormat} returns each line of a JSON Lines file\n+ * deserialized into an instance of a specified class.\n+ * <p>\n+ * You may also use Hadoop to read local files by specifying the\n+ * {@link #useHadoopForLocalFiles()} flag.\n+ * <p>\n+ * Usage:\n+ * <pre>{@code\n+ * BatchSource<User> source = new FileSourceBuilder(\"data/users.jsonl\")\n+ *   .withFormat(new JsonFileFormat<>(User.class))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDg3Njc3Mg=="}, "originalCommit": {"oid": "ad181730f72e87b1054c64207f6432048d0be93a"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5Nzg2ODgwOnYy", "diffSide": "RIGHT", "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/HadoopSourceFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxMjozMDo1MFrOHYBn9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxMzowMzoxOFrOHYCsjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk1NDQ4NQ==", "bodyText": "We have fileSourceBuilder.build() and then we have HadoopSourceFactory.create(fileSourceBuilder). This is inconsistent API. It also creates the problem of nullability, you can pass in a builder in any state.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r494954485", "createdAt": "2020-09-25T12:30:50Z", "author": {"login": "mtopolnik"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/HadoopSourceFactory.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import com.hazelcast.function.BiFunctionEx;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.hadoop.HadoopSources;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.file.AvroFileFormat;\n+import com.hazelcast.jet.pipeline.file.CsvFileFormat;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSourceFactory;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import org.apache.avro.Schema;\n+import org.apache.avro.mapred.AvroKey;\n+import org.apache.avro.mapreduce.AvroJob;\n+import org.apache.avro.mapreduce.AvroKeyInputFormat;\n+import org.apache.avro.reflect.ReflectData;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.BytesWritable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n+import org.apache.parquet.avro.AvroParquetInputFormat;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static com.hazelcast.jet.hadoop.impl.CsvInputFormat.CSV_INPUT_FORMAT_BEAN_CLASS;\n+import static com.hazelcast.jet.hadoop.impl.JsonInputFormat.JSON_INPUT_FORMAT_BEAN_CLASS;\n+\n+/**\n+ * Hadoop based implementation for FileSourceFactory\n+ *\n+ * @param <T> type of the items emitted from the source\n+ */\n+public class HadoopSourceFactory<T> implements FileSourceFactory<T> {\n+\n+    private final Map<Class<? extends FileFormat>, JobConfigurer<?, ?>> configs;\n+\n+    /**\n+     * Creates HadoopSourceFactory\n+     */\n+    public HadoopSourceFactory() {\n+        configs = new HashMap<>();\n+\n+        configs.put(AvroFileFormat.class, new AvroFormatJobConfigurer());\n+        configs.put(CsvFileFormat.class, new CsvFormatJobConfigurer());\n+        configs.put(JsonFileFormat.class, new JsonFormatJobConfigurer());\n+        configs.put(LinesTextFileFormat.class, new LineTextJobConfigurer());\n+        configs.put(ParquetFileFormat.class, new ParquetFormatJobConfigurer());\n+        configs.put(RawBytesFileFormat.class, new RawBytesFormatJobConfigurer());\n+        configs.put(TextFileFormat.class, new TextJobConfigurer());\n+    }\n+\n+    @Override\n+    public BatchSource<T> create(FileSourceBuilder<T> builder) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aea745df108100358b6d094c25f6576d807a0820"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk3MjA0Ng==", "bodyText": "HadoopSourceFactory is internal class (hence in impl package).", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r494972046", "createdAt": "2020-09-25T13:03:18Z", "author": {"login": "frant-hartm"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/HadoopSourceFactory.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import com.hazelcast.function.BiFunctionEx;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.hadoop.HadoopSources;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.file.AvroFileFormat;\n+import com.hazelcast.jet.pipeline.file.CsvFileFormat;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSourceFactory;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import org.apache.avro.Schema;\n+import org.apache.avro.mapred.AvroKey;\n+import org.apache.avro.mapreduce.AvroJob;\n+import org.apache.avro.mapreduce.AvroKeyInputFormat;\n+import org.apache.avro.reflect.ReflectData;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.BytesWritable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n+import org.apache.parquet.avro.AvroParquetInputFormat;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import static com.hazelcast.jet.hadoop.impl.CsvInputFormat.CSV_INPUT_FORMAT_BEAN_CLASS;\n+import static com.hazelcast.jet.hadoop.impl.JsonInputFormat.JSON_INPUT_FORMAT_BEAN_CLASS;\n+\n+/**\n+ * Hadoop based implementation for FileSourceFactory\n+ *\n+ * @param <T> type of the items emitted from the source\n+ */\n+public class HadoopSourceFactory<T> implements FileSourceFactory<T> {\n+\n+    private final Map<Class<? extends FileFormat>, JobConfigurer<?, ?>> configs;\n+\n+    /**\n+     * Creates HadoopSourceFactory\n+     */\n+    public HadoopSourceFactory() {\n+        configs = new HashMap<>();\n+\n+        configs.put(AvroFileFormat.class, new AvroFormatJobConfigurer());\n+        configs.put(CsvFileFormat.class, new CsvFormatJobConfigurer());\n+        configs.put(JsonFileFormat.class, new JsonFormatJobConfigurer());\n+        configs.put(LinesTextFileFormat.class, new LineTextJobConfigurer());\n+        configs.put(ParquetFileFormat.class, new ParquetFormatJobConfigurer());\n+        configs.put(RawBytesFileFormat.class, new RawBytesFormatJobConfigurer());\n+        configs.put(TextFileFormat.class, new TextJobConfigurer());\n+    }\n+\n+    @Override\n+    public BatchSource<T> create(FileSourceBuilder<T> builder) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk1NDQ4NQ=="}, "originalCommit": {"oid": "aea745df108100358b6d094c25f6576d807a0820"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MjM3MzM0OnYy", "diffSide": "RIGHT", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/FileSourceBuilder.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwOTo0MToxMVrOHxybWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQwOTo0MToxMVrOHxybWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTk2ODQ3Mg==", "bodyText": "Boolean getters tend to be confusing:\nFileSourceBuilder<String> source = FileSources.files(\"src/test/resources/directory/\")\n                                              .withFormat(FileFormat.lines());\nsource.sharedFileSystem();\nUnless there's a smart IDE warning that warns you're ignoring the boolean return value (you're ignoring it all the time anyway, with return this methods), this will be a very surprising thing.\nA better name isisSharedFileSystem.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r521968472", "createdAt": "2020-11-12T09:41:11Z", "author": {"login": "mtopolnik"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/FileSourceBuilder.java", "diffHunk": "@@ -163,6 +189,13 @@ public String path() {\n         return format;\n     }\n \n+    /**\n+     * Returns if the filesystem is shared. Only valid for local filesystem, distributed filesystems are always shared.\n+     */\n+    public boolean sharedFileSystem() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c7a5c7da7971eaf8827f09ea85702bb514162a5c"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NzAwMTg5OnYy", "diffSide": "RIGHT", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/FileFormat.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwNzowMjoyMFrOHyfVFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QyMDozODowOVrOH4edHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjcwNDE1MQ==", "bodyText": "why don't we use Nonnull and force user to use the no-argument variant. We use this convention for LinesTextFileFormat and TextFileFormat for example.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r522704151", "createdAt": "2020-11-13T07:02:20Z", "author": {"login": "gurbuzali"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/FileFormat.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.charset.Charset;\n+\n+/**\n+ * Identifies the data format of a file to be used as a Jet data source.\n+ * This is a data object that holds the configuration; actual implementation\n+ * code is looked up elsewhere, by using this object as a key.\n+ *\n+ * @param <T> type of items a source using this file format will emit\n+ * @since 4.4\n+ */\n+public interface FileFormat<T> {\n+\n+    /**\n+     * Returns the unique identifier of the file format. The convention is to\n+     * use the well-known filename suffix or, if there is none, a short-form\n+     * name of the format.\n+     */\n+    @Nonnull\n+    String format();\n+\n+\n+    // Factory methods for supported file formats are here for easy discoverability.\n+\n+    /**\n+     * Returns a file format for Avro files.\n+     */\n+    @Nonnull\n+    static <T> AvroFileFormat<T> avro() {\n+        return avro(null);\n+    }\n+\n+    /**\n+     * Returns a file format for Avro files that specifies to use reflection\n+     * to deserialize the data into instances of the provided Java class.\n+     * Jet will use the {@code ReflectDatumReader} to read Avro data. The\n+     * parameter may be {@code null}, disabling the option to deserialize\n+     * using reflection, but for that case you should prefer the no-argument\n+     * {@link #avro()} call.\n+     */\n+    @Nonnull\n+    static <T> AvroFileFormat<T> avro(@Nullable Class<T> clazz) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODk4MTI3OA==", "bodyText": "See comment on the related method.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r528981278", "createdAt": "2020-11-23T20:38:09Z", "author": {"login": "frant-hartm"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/FileFormat.java", "diffHunk": "@@ -0,0 +1,151 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+import java.nio.charset.Charset;\n+\n+/**\n+ * Identifies the data format of a file to be used as a Jet data source.\n+ * This is a data object that holds the configuration; actual implementation\n+ * code is looked up elsewhere, by using this object as a key.\n+ *\n+ * @param <T> type of items a source using this file format will emit\n+ * @since 4.4\n+ */\n+public interface FileFormat<T> {\n+\n+    /**\n+     * Returns the unique identifier of the file format. The convention is to\n+     * use the well-known filename suffix or, if there is none, a short-form\n+     * name of the format.\n+     */\n+    @Nonnull\n+    String format();\n+\n+\n+    // Factory methods for supported file formats are here for easy discoverability.\n+\n+    /**\n+     * Returns a file format for Avro files.\n+     */\n+    @Nonnull\n+    static <T> AvroFileFormat<T> avro() {\n+        return avro(null);\n+    }\n+\n+    /**\n+     * Returns a file format for Avro files that specifies to use reflection\n+     * to deserialize the data into instances of the provided Java class.\n+     * Jet will use the {@code ReflectDatumReader} to read Avro data. The\n+     * parameter may be {@code null}, disabling the option to deserialize\n+     * using reflection, but for that case you should prefer the no-argument\n+     * {@link #avro()} call.\n+     */\n+    @Nonnull\n+    static <T> AvroFileFormat<T> avro(@Nullable Class<T> clazz) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjcwNDE1MQ=="}, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NzAyNDA5OnYy", "diffSide": "RIGHT", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/AvroFileFormat.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwNzowNzowMFrOHyfkLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QyMDo0MToyOFrOH4ejvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjcwODAxNQ==", "bodyText": "I would make this Nonnull too. By default it is null, if user explicitly calls it then the user wants to configure the class.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r522708015", "createdAt": "2020-11-13T07:07:00Z", "author": {"login": "gurbuzali"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/AvroFileFormat.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * {@link FileFormat} for avro files. See {@link FileFormat#avro} for more\n+ * details.\n+ *\n+ * @param <T> type of items a source using this file format will emit\n+ * @since 4.4\n+ */\n+public class AvroFileFormat<T> implements FileFormat<T> {\n+\n+    /**\n+     * Format id for Avro.\n+     */\n+    public static final String FORMAT_AVRO = \"avro\";\n+\n+    private Class<T> reflectClass;\n+\n+    /**\n+     * Creates {@link AvroFileFormat}. See {@link FileFormat#avro} for more\n+     * details.\n+     */\n+    AvroFileFormat() {\n+    }\n+\n+    /**\n+     * Specifies to use reflection to deserialize data into the given class.\n+     * Jet will use the {@code ReflectDatumReader} to read Avro data. The\n+     * parameter may be {@code null}, this disables the option to deserialize\n+     * using reflection.\n+     *\n+     * @param reflectClass class to deserialize data into\n+     */\n+    @Nonnull\n+    public AvroFileFormat<T> withReflect(@Nullable Class<T> reflectClass) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjkwODc3NA==", "bodyText": "The counterargument is that the user may have some general code that has a nullable class. With our @Nullable, it can just pass it in, otherwise it must do a boilerplate null-check to decide which method to call.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r522908774", "createdAt": "2020-11-13T12:04:51Z", "author": {"login": "mtopolnik"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/AvroFileFormat.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * {@link FileFormat} for avro files. See {@link FileFormat#avro} for more\n+ * details.\n+ *\n+ * @param <T> type of items a source using this file format will emit\n+ * @since 4.4\n+ */\n+public class AvroFileFormat<T> implements FileFormat<T> {\n+\n+    /**\n+     * Format id for Avro.\n+     */\n+    public static final String FORMAT_AVRO = \"avro\";\n+\n+    private Class<T> reflectClass;\n+\n+    /**\n+     * Creates {@link AvroFileFormat}. See {@link FileFormat#avro} for more\n+     * details.\n+     */\n+    AvroFileFormat() {\n+    }\n+\n+    /**\n+     * Specifies to use reflection to deserialize data into the given class.\n+     * Jet will use the {@code ReflectDatumReader} to read Avro data. The\n+     * parameter may be {@code null}, this disables the option to deserialize\n+     * using reflection.\n+     *\n+     * @param reflectClass class to deserialize data into\n+     */\n+    @Nonnull\n+    public AvroFileFormat<T> withReflect(@Nullable Class<T> reflectClass) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjcwODAxNQ=="}, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzEyNjQ2MA==", "bodyText": "well, we keep the Nonnull convention for other file-formats already, this one is the exception", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523126460", "createdAt": "2020-11-13T17:55:23Z", "author": {"login": "gurbuzali"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/AvroFileFormat.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * {@link FileFormat} for avro files. See {@link FileFormat#avro} for more\n+ * details.\n+ *\n+ * @param <T> type of items a source using this file format will emit\n+ * @since 4.4\n+ */\n+public class AvroFileFormat<T> implements FileFormat<T> {\n+\n+    /**\n+     * Format id for Avro.\n+     */\n+    public static final String FORMAT_AVRO = \"avro\";\n+\n+    private Class<T> reflectClass;\n+\n+    /**\n+     * Creates {@link AvroFileFormat}. See {@link FileFormat#avro} for more\n+     * details.\n+     */\n+    AvroFileFormat() {\n+    }\n+\n+    /**\n+     * Specifies to use reflection to deserialize data into the given class.\n+     * Jet will use the {@code ReflectDatumReader} to read Avro data. The\n+     * parameter may be {@code null}, this disables the option to deserialize\n+     * using reflection.\n+     *\n+     * @param reflectClass class to deserialize data into\n+     */\n+    @Nonnull\n+    public AvroFileFormat<T> withReflect(@Nullable Class<T> reflectClass) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjcwODAxNQ=="}, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODk4Mjk3NQ==", "bodyText": "It's an exception, because this is optional. All the Nonnull parameters elsewhere are compulsory or have a reasonable default value.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r528982975", "createdAt": "2020-11-23T20:41:28Z", "author": {"login": "frant-hartm"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/AvroFileFormat.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file;\n+\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+\n+/**\n+ * {@link FileFormat} for avro files. See {@link FileFormat#avro} for more\n+ * details.\n+ *\n+ * @param <T> type of items a source using this file format will emit\n+ * @since 4.4\n+ */\n+public class AvroFileFormat<T> implements FileFormat<T> {\n+\n+    /**\n+     * Format id for Avro.\n+     */\n+    public static final String FORMAT_AVRO = \"avro\";\n+\n+    private Class<T> reflectClass;\n+\n+    /**\n+     * Creates {@link AvroFileFormat}. See {@link FileFormat#avro} for more\n+     * details.\n+     */\n+    AvroFileFormat() {\n+    }\n+\n+    /**\n+     * Specifies to use reflection to deserialize data into the given class.\n+     * Jet will use the {@code ReflectDatumReader} to read Avro data. The\n+     * parameter may be {@code null}, this disables the option to deserialize\n+     * using reflection.\n+     *\n+     * @param reflectClass class to deserialize data into\n+     */\n+    @Nonnull\n+    public AvroFileFormat<T> withReflect(@Nullable Class<T> reflectClass) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjcwODAxNQ=="}, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NzI0MTcwOnYy", "diffSide": "RIGHT", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwNzo0Nzo0NFrOHyh2Mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QyMjoyMzowOVrOH4htDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjc0NTM5NQ==", "bodyText": "I think the logic to find if the path is a directory or a file is wrong.  the comment says:\n        // We can't ask the filesystem because this code runs on the client, which\n        // is likely a different machine than the cluster members. So this is a\n        // best guess, we assume that directories end with '/'.\n\nLet's say I want to read the files in the directory /user/home/tmp and I didn't put a / to the end.\nThe source tries to read the file tmp in the parent directory /usr/home. Since there is no such file in the directory (uses file-name as glob), the source just completes.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r522745395", "createdAt": "2020-11-13T07:47:44Z", "author": {"login": "gurbuzali"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file.impl;\n+\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.datamodel.Tuple2;\n+import com.hazelcast.jet.impl.util.IOUtil;\n+import com.hazelcast.jet.json.JsonUtil;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.Sources;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import javax.annotation.Nonnull;\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+import java.util.stream.Stream;\n+\n+import static com.hazelcast.jet.datamodel.Tuple2.tuple2;\n+import static com.hazelcast.jet.impl.util.Util.uncheckCall;\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.util.Objects.requireNonNull;\n+\n+/**\n+ * Implementation of FileSourceFactory for the local filesystem.\n+ */\n+public class LocalFileSourceFactory implements FileSourceFactory {\n+\n+    private static Map<String, ReadFileFnProvider> readFileFnProviders;\n+\n+    static {\n+        Map<String, ReadFileFnProvider> mapFns = new HashMap<>();\n+\n+        addMapFnProvider(mapFns, new JsonReadFileFnProvider());\n+        addMapFnProvider(mapFns, new LinesReadFileFnProvider());\n+        addMapFnProvider(mapFns, new ParquetReadFileFnProvider());\n+        addMapFnProvider(mapFns, new RawBytesReadFileFnProvider());\n+        addMapFnProvider(mapFns, new TextReadFileFnProvider());\n+\n+        ServiceLoader<ReadFileFnProvider> loader = ServiceLoader.load(ReadFileFnProvider.class);\n+        for (ReadFileFnProvider readFileFnProvider : loader) {\n+            addMapFnProvider(mapFns, readFileFnProvider);\n+        }\n+\n+        LocalFileSourceFactory.readFileFnProviders = Collections.unmodifiableMap(mapFns);\n+    }\n+\n+    private static void addMapFnProvider(Map<String, ReadFileFnProvider> mapFns, ReadFileFnProvider provider) {\n+        mapFns.put(provider.format(), provider);\n+    }\n+\n+    @Nonnull @Override\n+    public <T> BatchSource<T> create(@Nonnull FileSourceBuilder<T> builder) {\n+        Tuple2<String, String> dirAndGlob = deriveDirectoryAndGlobFromPath(builder.path());\n+        assert dirAndGlob.f0() != null && dirAndGlob.f1() != null;\n+\n+        FileFormat<T> format = requireNonNull(builder.format());\n+        ReadFileFnProvider readFileFnProvider = readFileFnProviders.get(format.format());\n+        FunctionEx<Path, Stream<T>> mapFn = readFileFnProvider.createReadFileFn(format);\n+        return Sources.filesBuilder(dirAndGlob.f0())\n+                      .glob(dirAndGlob.f1())\n+                      .sharedFileSystem(builder.isSharedFileSystem())\n+                      .build(mapFn);\n+    }\n+\n+    private Tuple2<String, String> deriveDirectoryAndGlobFromPath(String path) {\n+        Path p = Paths.get(path);\n+\n+        String directory;\n+        String glob = \"*\";\n+        if (isDirectory(path)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTAzNDUwOA==", "bodyText": "I see 2 options:\n\nwe either keep the directory and glob parameters instead of single path parameters, for hadoop we would just concatenate these\nwe move this piece of code to processor supplier, I have tried to have a go at it, see the latest commit.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r529034508", "createdAt": "2020-11-23T22:23:09Z", "author": {"login": "frant-hartm"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file.impl;\n+\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.datamodel.Tuple2;\n+import com.hazelcast.jet.impl.util.IOUtil;\n+import com.hazelcast.jet.json.JsonUtil;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.Sources;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import javax.annotation.Nonnull;\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+import java.util.stream.Stream;\n+\n+import static com.hazelcast.jet.datamodel.Tuple2.tuple2;\n+import static com.hazelcast.jet.impl.util.Util.uncheckCall;\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.util.Objects.requireNonNull;\n+\n+/**\n+ * Implementation of FileSourceFactory for the local filesystem.\n+ */\n+public class LocalFileSourceFactory implements FileSourceFactory {\n+\n+    private static Map<String, ReadFileFnProvider> readFileFnProviders;\n+\n+    static {\n+        Map<String, ReadFileFnProvider> mapFns = new HashMap<>();\n+\n+        addMapFnProvider(mapFns, new JsonReadFileFnProvider());\n+        addMapFnProvider(mapFns, new LinesReadFileFnProvider());\n+        addMapFnProvider(mapFns, new ParquetReadFileFnProvider());\n+        addMapFnProvider(mapFns, new RawBytesReadFileFnProvider());\n+        addMapFnProvider(mapFns, new TextReadFileFnProvider());\n+\n+        ServiceLoader<ReadFileFnProvider> loader = ServiceLoader.load(ReadFileFnProvider.class);\n+        for (ReadFileFnProvider readFileFnProvider : loader) {\n+            addMapFnProvider(mapFns, readFileFnProvider);\n+        }\n+\n+        LocalFileSourceFactory.readFileFnProviders = Collections.unmodifiableMap(mapFns);\n+    }\n+\n+    private static void addMapFnProvider(Map<String, ReadFileFnProvider> mapFns, ReadFileFnProvider provider) {\n+        mapFns.put(provider.format(), provider);\n+    }\n+\n+    @Nonnull @Override\n+    public <T> BatchSource<T> create(@Nonnull FileSourceBuilder<T> builder) {\n+        Tuple2<String, String> dirAndGlob = deriveDirectoryAndGlobFromPath(builder.path());\n+        assert dirAndGlob.f0() != null && dirAndGlob.f1() != null;\n+\n+        FileFormat<T> format = requireNonNull(builder.format());\n+        ReadFileFnProvider readFileFnProvider = readFileFnProviders.get(format.format());\n+        FunctionEx<Path, Stream<T>> mapFn = readFileFnProvider.createReadFileFn(format);\n+        return Sources.filesBuilder(dirAndGlob.f0())\n+                      .glob(dirAndGlob.f1())\n+                      .sharedFileSystem(builder.isSharedFileSystem())\n+                      .build(mapFn);\n+    }\n+\n+    private Tuple2<String, String> deriveDirectoryAndGlobFromPath(String path) {\n+        Path p = Paths.get(path);\n+\n+        String directory;\n+        String glob = \"*\";\n+        if (isDirectory(path)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjc0NTM5NQ=="}, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3NzI1MzMwOnYy", "diffSide": "RIGHT", "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwNzo0OTo1MlrOHyh-Lg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QwNzo0OTo1MlrOHyh-Lg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjc0NzQzOA==", "bodyText": "readFileFnProvider can be null if user provides a custom file format ?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r522747438", "createdAt": "2020-11-13T07:49:52Z", "author": {"login": "gurbuzali"}, "path": "hazelcast-jet-core/src/main/java/com/hazelcast/jet/pipeline/file/impl/LocalFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.pipeline.file.impl;\n+\n+import com.hazelcast.function.FunctionEx;\n+import com.hazelcast.jet.datamodel.Tuple2;\n+import com.hazelcast.jet.impl.util.IOUtil;\n+import com.hazelcast.jet.json.JsonUtil;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.Sources;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+import javax.annotation.Nonnull;\n+import java.io.BufferedReader;\n+import java.io.FileInputStream;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.nio.charset.Charset;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.ServiceLoader;\n+import java.util.stream.Stream;\n+\n+import static com.hazelcast.jet.datamodel.Tuple2.tuple2;\n+import static com.hazelcast.jet.impl.util.Util.uncheckCall;\n+import static com.hazelcast.jet.impl.util.Util.uncheckRun;\n+import static java.nio.charset.StandardCharsets.UTF_8;\n+import static java.util.Objects.requireNonNull;\n+\n+/**\n+ * Implementation of FileSourceFactory for the local filesystem.\n+ */\n+public class LocalFileSourceFactory implements FileSourceFactory {\n+\n+    private static Map<String, ReadFileFnProvider> readFileFnProviders;\n+\n+    static {\n+        Map<String, ReadFileFnProvider> mapFns = new HashMap<>();\n+\n+        addMapFnProvider(mapFns, new JsonReadFileFnProvider());\n+        addMapFnProvider(mapFns, new LinesReadFileFnProvider());\n+        addMapFnProvider(mapFns, new ParquetReadFileFnProvider());\n+        addMapFnProvider(mapFns, new RawBytesReadFileFnProvider());\n+        addMapFnProvider(mapFns, new TextReadFileFnProvider());\n+\n+        ServiceLoader<ReadFileFnProvider> loader = ServiceLoader.load(ReadFileFnProvider.class);\n+        for (ReadFileFnProvider readFileFnProvider : loader) {\n+            addMapFnProvider(mapFns, readFileFnProvider);\n+        }\n+\n+        LocalFileSourceFactory.readFileFnProviders = Collections.unmodifiableMap(mapFns);\n+    }\n+\n+    private static void addMapFnProvider(Map<String, ReadFileFnProvider> mapFns, ReadFileFnProvider provider) {\n+        mapFns.put(provider.format(), provider);\n+    }\n+\n+    @Nonnull @Override\n+    public <T> BatchSource<T> create(@Nonnull FileSourceBuilder<T> builder) {\n+        Tuple2<String, String> dirAndGlob = deriveDirectoryAndGlobFromPath(builder.path());\n+        assert dirAndGlob.f0() != null && dirAndGlob.f1() != null;\n+\n+        FileFormat<T> format = requireNonNull(builder.format());\n+        ReadFileFnProvider readFileFnProvider = readFileFnProviders.get(format.format());\n+        FunctionEx<Path, Stream<T>> mapFn = readFileFnProvider.createReadFileFn(format);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4MzgyMjMyOnYy", "diffSide": "RIGHT", "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/WholeFileInputFormat.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxMzo0OToxM1rOHzf2sA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QyMDo1MDo1MFrOH4e20g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2MTMyOA==", "bodyText": "why do we call initialize here explicitly while for other FileInputFormats we don't?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523761328", "createdAt": "2020-11-15T13:49:13Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/WholeFileInputFormat.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.BytesWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+\n+\n+/**\n+ * Adapted from code example from book\n+ * Hadoop: The Definitive Guide, Fourth Edition by Tom White (O'Reilly, 2014)\n+ * https://github.com/tomwhite/hadoop-book/blob/master/ch08-mr-types/src/main/java/WholeFileInputFormat.java\n+ */\n+public class WholeFileInputFormat extends FileInputFormat<NullWritable, BytesWritable> {\n+\n+    @Override\n+    protected boolean isSplitable(JobContext context, Path file) {\n+        return false;\n+    }\n+\n+    @Override\n+    public RecordReader<NullWritable, BytesWritable> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+        WholeFileRecordReader reader = new WholeFileRecordReader();\n+        reader.initialize(split, context);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODk4Nzg1OA==", "bodyText": "It was part of the source I took from the Hadoop book Seems it is not needed in the new API, the javadoc says the hadoop framework will call initialize and it the tests pass if I remove it.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r528987858", "createdAt": "2020-11-23T20:50:50Z", "author": {"login": "frant-hartm"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/WholeFileInputFormat.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.BytesWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+\n+\n+/**\n+ * Adapted from code example from book\n+ * Hadoop: The Definitive Guide, Fourth Edition by Tom White (O'Reilly, 2014)\n+ * https://github.com/tomwhite/hadoop-book/blob/master/ch08-mr-types/src/main/java/WholeFileInputFormat.java\n+ */\n+public class WholeFileInputFormat extends FileInputFormat<NullWritable, BytesWritable> {\n+\n+    @Override\n+    protected boolean isSplitable(JobContext context, Path file) {\n+        return false;\n+    }\n+\n+    @Override\n+    public RecordReader<NullWritable, BytesWritable> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+        WholeFileRecordReader reader = new WholeFileRecordReader();\n+        reader.initialize(split, context);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2MTMyOA=="}, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4MzgyNjI2OnYy", "diffSide": "RIGHT", "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/CsvInputFormat.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxMzo1Mjo0MlrOHzf4mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QyMDo1MToyN1rOH4e3-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2MTgxOA==", "bodyText": "we can use ReflectionUtils.loadClass() here. it removes the necessity of catching ClassNotFoundException", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523761818", "createdAt": "2020-11-15T13:52:42Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/CsvInputFormat.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import com.fasterxml.jackson.databind.MappingIterator;\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+public class CsvInputFormat extends FileInputFormat<NullWritable, Object> {\n+\n+    public static final String CSV_INPUT_FORMAT_BEAN_CLASS = \"csv.bean.class\";\n+\n+    @Override\n+    public RecordReader<NullWritable, Object> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+\n+        return new RecordReader<NullWritable, Object>() {\n+\n+            private Object current;\n+            private MappingIterator<Object> iterator;\n+\n+            @Override\n+            public void initialize(InputSplit split, TaskAttemptContext context) throws IOException {\n+\n+                FileSplit fileSplit = (FileSplit) split;\n+                Configuration conf = context.getConfiguration();\n+\n+                try {\n+\n+                    Configuration configuration = context.getConfiguration();\n+                    String className = configuration.get(CSV_INPUT_FORMAT_BEAN_CLASS);\n+                    Class<?> clazz = Thread.currentThread().getContextClassLoader().loadClass(className);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODk4ODE1NQ==", "bodyText": "Done.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r528988155", "createdAt": "2020-11-23T20:51:27Z", "author": {"login": "frant-hartm"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/CsvInputFormat.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import com.fasterxml.jackson.databind.MappingIterator;\n+import com.fasterxml.jackson.databind.ObjectReader;\n+import com.fasterxml.jackson.dataformat.csv.CsvMapper;\n+import com.fasterxml.jackson.dataformat.csv.CsvSchema;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+public class CsvInputFormat extends FileInputFormat<NullWritable, Object> {\n+\n+    public static final String CSV_INPUT_FORMAT_BEAN_CLASS = \"csv.bean.class\";\n+\n+    @Override\n+    public RecordReader<NullWritable, Object> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+\n+        return new RecordReader<NullWritable, Object>() {\n+\n+            private Object current;\n+            private MappingIterator<Object> iterator;\n+\n+            @Override\n+            public void initialize(InputSplit split, TaskAttemptContext context) throws IOException {\n+\n+                FileSplit fileSplit = (FileSplit) split;\n+                Configuration conf = context.getConfiguration();\n+\n+                try {\n+\n+                    Configuration configuration = context.getConfiguration();\n+                    String className = configuration.get(CSV_INPUT_FORMAT_BEAN_CLASS);\n+                    Class<?> clazz = Thread.currentThread().getContextClassLoader().loadClass(className);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2MTgxOA=="}, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4MzgzMjk4OnYy", "diffSide": "RIGHT", "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/JsonInputFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxMzo1OToyOFrOHzf7yQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxMzo1OToyOFrOHzf7yQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2MjYzMw==", "bodyText": "we can use ReflectionUtils.loadClass() here", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523762633", "createdAt": "2020-11-15T13:59:28Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/JsonInputFormat.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import com.hazelcast.jet.json.JsonUtil;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.hadoop.io.compress.CompressionCodecFactory;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.LineRecordReader;\n+\n+import java.io.IOException;\n+\n+public class JsonInputFormat extends FileInputFormat<LongWritable, Object> {\n+\n+    public static final String JSON_INPUT_FORMAT_BEAN_CLASS = \"json.bean.class\";\n+\n+    @Override\n+    public RecordReader<LongWritable, Object> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+\n+        try {\n+            Configuration configuration = context.getConfiguration();\n+            String className = configuration.get(JSON_INPUT_FORMAT_BEAN_CLASS);\n+            Class<?> clazz = Thread.currentThread().getContextClassLoader().loadClass(className);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4MzgzNjE3OnYy", "diffSide": "RIGHT", "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/JsonInputFormat.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxNDowMjoyMVrOHzf9PA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QyMDo1MTozOFrOH4e4Wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2MzAwNA==", "bodyText": "can be simplified as\nreturn codec == null || codec instanceof SplittableCompressionCodec;", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523763004", "createdAt": "2020-11-15T14:02:21Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/JsonInputFormat.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import com.hazelcast.jet.json.JsonUtil;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.hadoop.io.compress.CompressionCodecFactory;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.LineRecordReader;\n+\n+import java.io.IOException;\n+\n+public class JsonInputFormat extends FileInputFormat<LongWritable, Object> {\n+\n+    public static final String JSON_INPUT_FORMAT_BEAN_CLASS = \"json.bean.class\";\n+\n+    @Override\n+    public RecordReader<LongWritable, Object> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+\n+        try {\n+            Configuration configuration = context.getConfiguration();\n+            String className = configuration.get(JSON_INPUT_FORMAT_BEAN_CLASS);\n+            Class<?> clazz = Thread.currentThread().getContextClassLoader().loadClass(className);\n+\n+            return new RecordReader<LongWritable, Object>() {\n+\n+                final LineRecordReader reader = new LineRecordReader();\n+\n+                @Override\n+                public void initialize(InputSplit split, TaskAttemptContext context) throws IOException {\n+                    reader.initialize(split, context);\n+                }\n+\n+                @Override\n+                public boolean nextKeyValue() throws IOException {\n+                    return reader.nextKeyValue();\n+                }\n+\n+                @Override\n+                public LongWritable getCurrentKey() {\n+                    return reader.getCurrentKey();\n+                }\n+\n+                @Override\n+                public Object getCurrentValue() throws IOException {\n+                    return JsonUtil.beanFrom(reader.getCurrentValue().toString(), clazz);\n+                }\n+\n+                @Override\n+                public float getProgress() throws IOException {\n+                    return reader.getProgress();\n+                }\n+\n+                @Override\n+                public void close() throws IOException {\n+                    reader.close();\n+                }\n+            };\n+\n+        } catch (ClassNotFoundException e) {\n+            throw new RuntimeException(e);\n+        }\n+\n+    }\n+\n+    @Override\n+    protected boolean isSplitable(JobContext context, Path file) {\n+        final CompressionCodec codec = new CompressionCodecFactory(context.getConfiguration()).getCodec(file);\n+        if (null == codec) {\n+            return true;\n+        }\n+        return codec instanceof SplittableCompressionCodec;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODk4ODI1MA==", "bodyText": "Done.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r528988250", "createdAt": "2020-11-23T20:51:38Z", "author": {"login": "frant-hartm"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/JsonInputFormat.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import com.hazelcast.jet.json.JsonUtil;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.compress.CompressionCodec;\n+import org.apache.hadoop.io.compress.CompressionCodecFactory;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.LineRecordReader;\n+\n+import java.io.IOException;\n+\n+public class JsonInputFormat extends FileInputFormat<LongWritable, Object> {\n+\n+    public static final String JSON_INPUT_FORMAT_BEAN_CLASS = \"json.bean.class\";\n+\n+    @Override\n+    public RecordReader<LongWritable, Object> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+\n+        try {\n+            Configuration configuration = context.getConfiguration();\n+            String className = configuration.get(JSON_INPUT_FORMAT_BEAN_CLASS);\n+            Class<?> clazz = Thread.currentThread().getContextClassLoader().loadClass(className);\n+\n+            return new RecordReader<LongWritable, Object>() {\n+\n+                final LineRecordReader reader = new LineRecordReader();\n+\n+                @Override\n+                public void initialize(InputSplit split, TaskAttemptContext context) throws IOException {\n+                    reader.initialize(split, context);\n+                }\n+\n+                @Override\n+                public boolean nextKeyValue() throws IOException {\n+                    return reader.nextKeyValue();\n+                }\n+\n+                @Override\n+                public LongWritable getCurrentKey() {\n+                    return reader.getCurrentKey();\n+                }\n+\n+                @Override\n+                public Object getCurrentValue() throws IOException {\n+                    return JsonUtil.beanFrom(reader.getCurrentValue().toString(), clazz);\n+                }\n+\n+                @Override\n+                public float getProgress() throws IOException {\n+                    return reader.getProgress();\n+                }\n+\n+                @Override\n+                public void close() throws IOException {\n+                    reader.close();\n+                }\n+            };\n+\n+        } catch (ClassNotFoundException e) {\n+            throw new RuntimeException(e);\n+        }\n+\n+    }\n+\n+    @Override\n+    protected boolean isSplitable(JobContext context, Path file) {\n+        final CompressionCodec codec = new CompressionCodecFactory(context.getConfiguration()).getCodec(file);\n+        if (null == codec) {\n+            return true;\n+        }\n+        return codec instanceof SplittableCompressionCodec;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2MzAwNA=="}, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4Mzg0MDk1OnYy", "diffSide": "RIGHT", "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/WholeTextInputFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxNDowNzoxOVrOHzf_iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxNDowNzoxOVrOHzf_iA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2MzU5Mg==", "bodyText": "like WholeFileInputFormat why do we call initialize explicitly?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523763592", "createdAt": "2020-11-15T14:07:19Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/WholeTextInputFormat.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+\n+\n+/**\n+ * Adapted from code example from book\n+ * Hadoop: The Definitive Guide, Fourth Edition by Tom White (O'Reilly, 2014)\n+ * https://github.com/tomwhite/hadoop-book/blob/master/ch08-mr-types/src/main/java/WholeFileInputFormat.java\n+ */\n+public class WholeTextInputFormat extends FileInputFormat<NullWritable, Text> {\n+\n+    @Override\n+    protected boolean isSplitable(JobContext context, Path file) {\n+        return false;\n+    }\n+\n+    @Override\n+    public RecordReader<NullWritable, Text> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+        WholeTextRecordReader reader = new WholeTextRecordReader();\n+        reader.initialize(split, context);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4Mzg0NDA3OnYy", "diffSide": "RIGHT", "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/WholeTextRecordReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxNDoxMDoyOFrOHzgBBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QyMTowMTozNVrOH4fLmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2Mzk3NQ==", "bodyText": "WholeTextRecordReader and WholeFileRecordReader looks exactly the same except the generic type of the value. can we merge them somehow?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523763975", "createdAt": "2020-11-15T14:10:28Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/WholeTextRecordReader.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Adapted from code example from book\n+ * Hadoop: The Definitive Guide, Fourth Edition by Tom White (O'Reilly, 2014)\n+ * https://github.com/tomwhite/hadoop-book/blob/master/ch08-mr-types/src/main/java/WholeFileRecordReader.java\n+ */\n+class WholeTextRecordReader extends RecordReader<NullWritable, Text> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODk5MzE3OA==", "bodyText": "See the update if you like it. If not I will revert it.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r528993178", "createdAt": "2020-11-23T21:01:35Z", "author": {"login": "frant-hartm"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/WholeTextRecordReader.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Adapted from code example from book\n+ * Hadoop: The Definitive Guide, Fourth Edition by Tom White (O'Reilly, 2014)\n+ * https://github.com/tomwhite/hadoop-book/blob/master/ch08-mr-types/src/main/java/WholeFileRecordReader.java\n+ */\n+class WholeTextRecordReader extends RecordReader<NullWritable, Text> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2Mzk3NQ=="}, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4Mzg0NjA2OnYy", "diffSide": "RIGHT", "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/HadoopFileSourceFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxNDoxMjo0MVrOHzgCEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxNTo0OTozNFrOH4TjTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2NDI0MA==", "bodyText": "we should do a null check here for custom file format", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523764240", "createdAt": "2020-11-15T14:12:41Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/HadoopFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import com.hazelcast.function.BiFunctionEx;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.hadoop.HadoopSources;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.file.AvroFileFormat;\n+import com.hazelcast.jet.pipeline.file.CsvFileFormat;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import com.hazelcast.jet.pipeline.file.impl.FileSourceFactory;\n+import org.apache.avro.Schema;\n+import org.apache.avro.mapred.AvroKey;\n+import org.apache.avro.mapreduce.AvroJob;\n+import org.apache.avro.mapreduce.AvroKeyInputFormat;\n+import org.apache.avro.reflect.ReflectData;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.BytesWritable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n+import org.apache.parquet.avro.AvroParquetInputFormat;\n+\n+import javax.annotation.Nonnull;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+import static com.hazelcast.jet.hadoop.impl.CsvInputFormat.CSV_INPUT_FORMAT_BEAN_CLASS;\n+import static com.hazelcast.jet.hadoop.impl.JsonInputFormat.JSON_INPUT_FORMAT_BEAN_CLASS;\n+import static java.util.Objects.requireNonNull;\n+\n+/**\n+ * Hadoop-based implementation of {@link FileSourceFactory}.\n+ */\n+public class HadoopFileSourceFactory implements FileSourceFactory {\n+\n+    private final Map<String, JobConfigurer> configs;\n+\n+    /**\n+     * Creates the HadoopSourceFactory.\n+     */\n+    public HadoopFileSourceFactory() {\n+        configs = new HashMap<>();\n+\n+        configs.put(AvroFileFormat.FORMAT_AVRO, new AvroFormatJobConfigurer());\n+        configs.put(CsvFileFormat.FORMAT_CSV, new CsvFormatJobConfigurer());\n+        configs.put(JsonFileFormat.FORMAT_JSONL, new JsonFormatJobConfigurer());\n+        configs.put(LinesTextFileFormat.FORMAT_LINES, new LineTextJobConfigurer());\n+        configs.put(ParquetFileFormat.FORMAT_PARQUET, new ParquetFormatJobConfigurer());\n+        configs.put(RawBytesFileFormat.FORMAT_BIN, new RawBytesFormatJobConfigurer());\n+        configs.put(TextFileFormat.FORMAT_TXT, new TextJobConfigurer());\n+    }\n+\n+    @Nonnull @Override\n+    @SuppressWarnings(\"unchecked\")\n+    public <T> BatchSource<T> create(@Nonnull FileSourceBuilder<T> builder) {\n+\n+        try {\n+            Job job = Job.getInstance();\n+\n+            Configuration configuration = job.getConfiguration();\n+            for (Entry<String, String> option : builder.options().entrySet()) {\n+                configuration.set(option.getKey(), option.getValue());\n+            }\n+\n+            FileInputFormat.addInputPath(job, new Path(builder.path()));\n+\n+            FileFormat<T> fileFormat = requireNonNull(builder.format());\n+            JobConfigurer configurer = configs.get(fileFormat.format());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODgwMjYzOQ==", "bodyText": "Added.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r528802639", "createdAt": "2020-11-23T15:49:34Z", "author": {"login": "frant-hartm"}, "path": "extensions/hadoop/src/main/java/com/hazelcast/jet/hadoop/impl/HadoopFileSourceFactory.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.impl;\n+\n+import com.hazelcast.function.BiFunctionEx;\n+import com.hazelcast.jet.JetException;\n+import com.hazelcast.jet.hadoop.HadoopSources;\n+import com.hazelcast.jet.pipeline.BatchSource;\n+import com.hazelcast.jet.pipeline.file.AvroFileFormat;\n+import com.hazelcast.jet.pipeline.file.CsvFileFormat;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.JsonFileFormat;\n+import com.hazelcast.jet.pipeline.file.LinesTextFileFormat;\n+import com.hazelcast.jet.pipeline.file.ParquetFileFormat;\n+import com.hazelcast.jet.pipeline.file.RawBytesFileFormat;\n+import com.hazelcast.jet.pipeline.file.TextFileFormat;\n+import com.hazelcast.jet.pipeline.file.impl.FileSourceFactory;\n+import org.apache.avro.Schema;\n+import org.apache.avro.mapred.AvroKey;\n+import org.apache.avro.mapreduce.AvroJob;\n+import org.apache.avro.mapreduce.AvroKeyInputFormat;\n+import org.apache.avro.reflect.ReflectData;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.BytesWritable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n+import org.apache.parquet.avro.AvroParquetInputFormat;\n+\n+import javax.annotation.Nonnull;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+import static com.hazelcast.jet.hadoop.impl.CsvInputFormat.CSV_INPUT_FORMAT_BEAN_CLASS;\n+import static com.hazelcast.jet.hadoop.impl.JsonInputFormat.JSON_INPUT_FORMAT_BEAN_CLASS;\n+import static java.util.Objects.requireNonNull;\n+\n+/**\n+ * Hadoop-based implementation of {@link FileSourceFactory}.\n+ */\n+public class HadoopFileSourceFactory implements FileSourceFactory {\n+\n+    private final Map<String, JobConfigurer> configs;\n+\n+    /**\n+     * Creates the HadoopSourceFactory.\n+     */\n+    public HadoopFileSourceFactory() {\n+        configs = new HashMap<>();\n+\n+        configs.put(AvroFileFormat.FORMAT_AVRO, new AvroFormatJobConfigurer());\n+        configs.put(CsvFileFormat.FORMAT_CSV, new CsvFormatJobConfigurer());\n+        configs.put(JsonFileFormat.FORMAT_JSONL, new JsonFormatJobConfigurer());\n+        configs.put(LinesTextFileFormat.FORMAT_LINES, new LineTextJobConfigurer());\n+        configs.put(ParquetFileFormat.FORMAT_PARQUET, new ParquetFormatJobConfigurer());\n+        configs.put(RawBytesFileFormat.FORMAT_BIN, new RawBytesFormatJobConfigurer());\n+        configs.put(TextFileFormat.FORMAT_TXT, new TextJobConfigurer());\n+    }\n+\n+    @Nonnull @Override\n+    @SuppressWarnings(\"unchecked\")\n+    public <T> BatchSource<T> create(@Nonnull FileSourceBuilder<T> builder) {\n+\n+        try {\n+            Job job = Job.getInstance();\n+\n+            Configuration configuration = job.getConfiguration();\n+            for (Entry<String, String> option : builder.options().entrySet()) {\n+                configuration.set(option.getKey(), option.getValue());\n+            }\n+\n+            FileInputFormat.addInputPath(job, new Path(builder.path()));\n+\n+            FileFormat<T> fileFormat = requireNonNull(builder.format());\n+            JobConfigurer configurer = configs.get(fileFormat.format());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2NDI0MA=="}, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4Mzg1NTYxOnYy", "diffSide": "RIGHT", "path": "extensions/hadoop/src/test/java/com/hazelcast/jet/hadoop/file/BaseFileFormatTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxNDoyMjozNlrOHzgGpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxNDozOTozOFrOH4QT9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2NTQxNQ==", "bodyText": "this is not necessary, the instances are terminated at the end of the test already.\nwe should also move the instance creation to a setup method so that new instances are not created for each call of this method.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523765415", "createdAt": "2020-11-15T14:22:36Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/test/java/com/hazelcast/jet/hadoop/file/BaseFileFormatTest.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.file;\n+\n+import com.hazelcast.function.ConsumerEx;\n+import com.hazelcast.jet.JetInstance;\n+import com.hazelcast.jet.core.JetTestSupport;\n+import com.hazelcast.jet.pipeline.Pipeline;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.test.Assertions;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Parameterized.Parameter;\n+import org.junit.runners.Parameterized.Parameters;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+\n+@RunWith(Parameterized.class)\n+public abstract class BaseFileFormatTest extends JetTestSupport {\n+\n+    @Parameter\n+    public boolean useHadoop;\n+\n+    @Parameters(name = \"{index}: useHadoop={0}\")\n+    public static Iterable<?> parameters() {\n+        return Arrays.asList(true, false);\n+    }\n+\n+    @SafeVarargs\n+    protected final <T> void assertItemsInSource(FileSourceBuilder<T> source, T... items) {\n+        assertItemsInSource(source, collected -> assertThat(collected).containsOnly(items));\n+    }\n+\n+    protected <T> void assertItemsInSource(\n+            FileSourceBuilder<T> source, ConsumerEx<List<T>> assertion\n+    ) {\n+        assertItemsInSource(1, source, assertion);\n+    }\n+\n+    protected <T> void assertItemsInSource(\n+            int memberCount, FileSourceBuilder<T> source, ConsumerEx<List<T>> assertion\n+    ) {\n+        if (useHadoop) {\n+            source.useHadoopForLocalFiles(true);\n+        }\n+\n+        Pipeline p = Pipeline.create();\n+\n+        p.readFrom(source.build())\n+         .apply(Assertions.assertCollected(assertion));\n+\n+        JetInstance[] jets = createJetMembers(memberCount);\n+        jets[0].newJob(p).join();\n+\n+        for (JetInstance jet : jets) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc0OTU1OA==", "bodyText": "The com.hazelcast.jet.pipeline.test.Assertions#assertCollected checks against all collected items, so you actually need a new instance everytime. Then if the instances are not shut down they connect with instances from previous calls.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r528749558", "createdAt": "2020-11-23T14:39:38Z", "author": {"login": "frant-hartm"}, "path": "extensions/hadoop/src/test/java/com/hazelcast/jet/hadoop/file/BaseFileFormatTest.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.file;\n+\n+import com.hazelcast.function.ConsumerEx;\n+import com.hazelcast.jet.JetInstance;\n+import com.hazelcast.jet.core.JetTestSupport;\n+import com.hazelcast.jet.pipeline.Pipeline;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.test.Assertions;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Parameterized.Parameter;\n+import org.junit.runners.Parameterized.Parameters;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+\n+@RunWith(Parameterized.class)\n+public abstract class BaseFileFormatTest extends JetTestSupport {\n+\n+    @Parameter\n+    public boolean useHadoop;\n+\n+    @Parameters(name = \"{index}: useHadoop={0}\")\n+    public static Iterable<?> parameters() {\n+        return Arrays.asList(true, false);\n+    }\n+\n+    @SafeVarargs\n+    protected final <T> void assertItemsInSource(FileSourceBuilder<T> source, T... items) {\n+        assertItemsInSource(source, collected -> assertThat(collected).containsOnly(items));\n+    }\n+\n+    protected <T> void assertItemsInSource(\n+            FileSourceBuilder<T> source, ConsumerEx<List<T>> assertion\n+    ) {\n+        assertItemsInSource(1, source, assertion);\n+    }\n+\n+    protected <T> void assertItemsInSource(\n+            int memberCount, FileSourceBuilder<T> source, ConsumerEx<List<T>> assertion\n+    ) {\n+        if (useHadoop) {\n+            source.useHadoopForLocalFiles(true);\n+        }\n+\n+        Pipeline p = Pipeline.create();\n+\n+        p.readFrom(source.build())\n+         .apply(Assertions.assertCollected(assertion));\n+\n+        JetInstance[] jets = createJetMembers(memberCount);\n+        jets[0].newJob(p).join();\n+\n+        for (JetInstance jet : jets) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2NTQxNQ=="}, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4Mzg1ODQxOnYy", "diffSide": "RIGHT", "path": "extensions/hadoop/src/test/java/com/hazelcast/jet/hadoop/file/AvroFileFormatTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxNDoyNToyNlrOHzgIBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxNDowODoxNlrOH4O9qQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2NTc2NQ==", "bodyText": "why don't we pre-create this file in resources like other file types?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523765765", "createdAt": "2020-11-15T14:25:26Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/test/java/com/hazelcast/jet/hadoop/file/AvroFileFormatTest.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.file;\n+\n+import com.hazelcast.jet.hadoop.file.generated.SpecificUser;\n+import com.hazelcast.jet.hadoop.file.model.User;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSources;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.specific.SpecificDatumWriter;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+\n+public class AvroFileFormatTest extends BaseFileFormatTest {\n+\n+    @Test\n+    public void shouldReadAvroWithSchema() throws Exception {\n+        createAvroFile();\n+\n+        FileSourceBuilder<SpecificUser> source = FileSources.files(\"target/avro/file.avro\")\n+                                                            .format(FileFormat.avro());\n+        assertItemsInSource(source,\n+                new SpecificUser(\"Frantisek\", 7),\n+                new SpecificUser(\"Ali\", 42)\n+        );\n+\n+    }\n+\n+    @Test\n+    public void shouldReadAvroWithReflection() throws Exception {\n+        createAvroFile();\n+\n+        FileSourceBuilder<User> source = FileSources.files(\"target/avro/file.avro\")\n+                                                    .format(FileFormat.avro(User.class));\n+\n+        assertItemsInSource(source,\n+                new User(\"Frantisek\", 7),\n+                new User(\"Ali\", 42)\n+        );\n+    }\n+\n+    private static void createAvroFile() throws IOException {\n+        Path inputPath = new Path(\"target/avro\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODcyNzQ2NQ==", "bodyText": "I think creating it on the fly is easier to maintain. It is a binary file so imbossible to edit in case we need to modify it for a test case or similar.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r528727465", "createdAt": "2020-11-23T14:08:16Z", "author": {"login": "frant-hartm"}, "path": "extensions/hadoop/src/test/java/com/hazelcast/jet/hadoop/file/AvroFileFormatTest.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.file;\n+\n+import com.hazelcast.jet.hadoop.file.generated.SpecificUser;\n+import com.hazelcast.jet.hadoop.file.model.User;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSources;\n+import org.apache.avro.file.DataFileWriter;\n+import org.apache.avro.specific.SpecificDatumWriter;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+\n+public class AvroFileFormatTest extends BaseFileFormatTest {\n+\n+    @Test\n+    public void shouldReadAvroWithSchema() throws Exception {\n+        createAvroFile();\n+\n+        FileSourceBuilder<SpecificUser> source = FileSources.files(\"target/avro/file.avro\")\n+                                                            .format(FileFormat.avro());\n+        assertItemsInSource(source,\n+                new SpecificUser(\"Frantisek\", 7),\n+                new SpecificUser(\"Ali\", 42)\n+        );\n+\n+    }\n+\n+    @Test\n+    public void shouldReadAvroWithReflection() throws Exception {\n+        createAvroFile();\n+\n+        FileSourceBuilder<User> source = FileSources.files(\"target/avro/file.avro\")\n+                                                    .format(FileFormat.avro(User.class));\n+\n+        assertItemsInSource(source,\n+                new User(\"Frantisek\", 7),\n+                new User(\"Ali\", 42)\n+        );\n+    }\n+\n+    private static void createAvroFile() throws IOException {\n+        Path inputPath = new Path(\"target/avro\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2NTc2NQ=="}, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4Mzg2MDI5OnYy", "diffSide": "RIGHT", "path": "extensions/hadoop/src/test/java/com/hazelcast/jet/hadoop/file/ParquetFileFormatTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxNDoyNzoxM1rOHzgI8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNVQxNDoyNzoxM1rOHzgI8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc2NjAwMg==", "bodyText": "like avro file, can't we keep this created file in resources rather than creating it on the fly?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r523766002", "createdAt": "2020-11-15T14:27:13Z", "author": {"login": "gurbuzali"}, "path": "extensions/hadoop/src/test/java/com/hazelcast/jet/hadoop/file/ParquetFileFormatTest.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Copyright 2020 Hazelcast Inc.\n+ *\n+ * Licensed under the Hazelcast Community License (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://hazelcast.com/hazelcast-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.jet.hadoop.file;\n+\n+import com.hazelcast.jet.hadoop.file.generated.SpecificUser;\n+import com.hazelcast.jet.pipeline.file.FileFormat;\n+import com.hazelcast.jet.pipeline.file.FileSourceBuilder;\n+import com.hazelcast.jet.pipeline.file.FileSources;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.avro.AvroParquetWriter;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.junit.Test;\n+import org.junit.runners.Parameterized.Parameters;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+public class ParquetFileFormatTest extends BaseFileFormatTest {\n+\n+    // Parquet has a dependency on Hadoop so it does not make sense to run it without it\n+    @Parameters(name = \"{index}: useHadoop={0}\")\n+    public static Iterable<?> parameters() {\n+        return Arrays.asList(true);\n+    }\n+\n+    @Test\n+    public void shouldReadParquetFile() throws Exception {\n+        createParquetFile();\n+\n+        FileSourceBuilder<SpecificUser> source = FileSources.files(\"target/parquet/file.parquet\")\n+                                                            .format(FileFormat.parquet());\n+        assertItemsInSource(source,\n+                new SpecificUser(\"Frantisek\", 7),\n+                new SpecificUser(\"Ali\", 42)\n+        );\n+    }\n+\n+    private void createParquetFile() throws IOException {\n+        Path inputPath = new Path(\"target/parquet\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dcef04e03e01bb7f2a3ee111156d663fdbe7662f"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMyODAzOTE4OnYy", "diffSide": "RIGHT", "path": "site/docs/api/sources-sinks.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQyMDo0NDo1OVrOH6DVRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQyMDo0NDo1OVrOH6DVRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDYzNDA1Mw==", "bodyText": "this part needs an update following recent changes of the path parameter", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2518#discussion_r530634053", "createdAt": "2020-11-25T20:44:59Z", "author": {"login": "gurbuzali"}, "path": "site/docs/api/sources-sinks.md", "diffHunk": "@@ -6,8 +6,368 @@ description: Birds-eye view of all pre-defined sources available in Jet.\n Hazelcast Jet comes out of the box with many different sources and sinks\n that you can work with, that are also referred to as _connectors_.\n \n+## Unified File Connector API\n+\n+> This section describes the Unified File Connector API introduced in\n+> Hazelcast Jet 4.4.\n+>\n+> As of version 4.4, the API provides source capability only.\n+> For sinks, see the [Files](#files) section.\n+\n+The Unified File Connector API provides a simple way to read files,\n+unified across different sources of the data. Using API this you can\n+read files from the local filesystem, HDFS and cloud storage systems\n+such as Amazon S3, Google Cloud Storage or Azure Blob Storage. At the\n+same time the connector supports various formats of the data - text\n+files, CSV, Json, Avro, etc., regardless of the source.\n+\n+### The Source\n+\n+Hazelcast Jet supports the following sources:\n+\n+* Local Filesystem (both shared and local to the member)\n+* Hadoop Distributed File System (HDFS)\n+* Amazon S3\n+* Google Cloud Storage\n+* Azure Cloud Storage\n+* Azure Data Lake (both generation 1 and generation 2)\n+\n+These are the officially supported sources. However, you can read from\n+any Hadoop compatible file system.\n+\n+Support for reading from the local filesystem is included in the base\n+distribution of Hazelcast Jet. You don't need any additional\n+dependencies. To access Hadoop or any of the cloud based stores use the\n+separately downloadable module. See the details in the\n+[Supported Storage Systems](#supported-storage-systems) section.\n+\n+The main entrypoint to the file connector is `FileSources.files`, which\n+takes a `path` as a String parameter and returns a `FileSourceBuilder`.\n+The following shows the simplest use of the file source, which reads a\n+text file line by line:\n+\n+```java\n+BatchSource<String> source = FileSources.files(\"path/to/my/file\")\n+                                        .build();\n+```\n+\n+The `path` parameter takes both relative and absolute paths. It can", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306159b5f1c7244c57349cd2209541c105cdd12f"}, "originalPosition": 49}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4560, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}