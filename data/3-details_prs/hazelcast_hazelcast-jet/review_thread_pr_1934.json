{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcxODQxNzYy", "number": 1934, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxMDoxMzo1OVrODd-T5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxMDo1Nzo1N1rODd_GKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNzU2MTk2OnYy", "diffSide": "RIGHT", "path": "extensions/kafka/src/main/java/com/hazelcast/jet/kafka/KafkaSources.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxMDoxMzo1OVrOFm4VAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxMDo0NzozNlrOFm5Qbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjMxMzA4OA==", "bodyText": "this is normally on by default, do we explicitly disable it? if so this should be mentioned.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1934#discussion_r376313088", "createdAt": "2020-02-07T10:13:59Z", "author": {"login": "cangencer"}, "path": "extensions/kafka/src/main/java/com/hazelcast/jet/kafka/KafkaSources.java", "diffHunk": "@@ -91,12 +90,13 @@ private KafkaSources() {\n      *     restart or failure, the reading continues from the saved offset. You\n      *     can achieve exactly-once or at-least-once behavior.\n      *\n-     *     <li>if processing guarantee is disabled, the source commits the\n-     *     offsets to Kafka using {@link KafkaConsumer#commitSync()}. But the\n-     *     offsets are committed before or after the event is fully processed.\n-     *     Therefore some events can be processed twice or not at all. You can\n-     *     configure {@code group.id} in this case. If not configured a random\n-     *     UUID will be set.\n+     *     <li>if processing guarantee is disabled, the source will start\n+     *     reading from default offsets (based on the {@code auto.offset.reset}\n+     *     property). You can enable offset committing by assigning a {@code\n+     *     group.id}, enabling auto offset committing using {@code\n+     *     enable.auto.commit} and configuring {@code auto.commit.interval.ms}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d9618e579ec5c2bca3db52aeae9224c43f697cc"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjMyODMwMg==", "bodyText": "We don't disable it, now we do nothing to the properties. It's actually good for us: just by assigning the group.id we rely on kafka committing the offsets.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1934#discussion_r376328302", "createdAt": "2020-02-07T10:47:36Z", "author": {"login": "viliam-durina"}, "path": "extensions/kafka/src/main/java/com/hazelcast/jet/kafka/KafkaSources.java", "diffHunk": "@@ -91,12 +90,13 @@ private KafkaSources() {\n      *     restart or failure, the reading continues from the saved offset. You\n      *     can achieve exactly-once or at-least-once behavior.\n      *\n-     *     <li>if processing guarantee is disabled, the source commits the\n-     *     offsets to Kafka using {@link KafkaConsumer#commitSync()}. But the\n-     *     offsets are committed before or after the event is fully processed.\n-     *     Therefore some events can be processed twice or not at all. You can\n-     *     configure {@code group.id} in this case. If not configured a random\n-     *     UUID will be set.\n+     *     <li>if processing guarantee is disabled, the source will start\n+     *     reading from default offsets (based on the {@code auto.offset.reset}\n+     *     property). You can enable offset committing by assigning a {@code\n+     *     group.id}, enabling auto offset committing using {@code\n+     *     enable.auto.commit} and configuring {@code auto.commit.interval.ms}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjMxMzA4OA=="}, "originalCommit": {"oid": "2d9618e579ec5c2bca3db52aeae9224c43f697cc"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMyNzY5MDY0OnYy", "diffSide": "RIGHT", "path": "extensions/kafka/src/main/java/com/hazelcast/jet/kafka/KafkaSources.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxMDo1Nzo1N1rOFm5iZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxMDo1Nzo1N1rOFm5iZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjMzMjkwMg==", "bodyText": "shouldn't we remove this? since it's been fixed?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/1934#discussion_r376332902", "createdAt": "2020-02-07T10:57:57Z", "author": {"login": "cangencer"}, "path": "extensions/kafka/src/main/java/com/hazelcast/jet/kafka/KafkaSources.java", "diffHunk": "@@ -110,7 +110,7 @@ private KafkaSources() {\n      * {@code poll(timeout)} if the cluster is down. If {@link\n      * JobConfig#setSnapshotIntervalMillis(long) snapshotting is enabled},\n      * entire job might be blocked. This is a known issue of Kafka\n-     * (KAFKA-1894). Refer to Kafka documentation for details.\n+     * (KAFKA-1894, now fixed). Refer to Kafka documentation for details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d9618e579ec5c2bca3db52aeae9224c43f697cc"}, "originalPosition": 33}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4969, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}