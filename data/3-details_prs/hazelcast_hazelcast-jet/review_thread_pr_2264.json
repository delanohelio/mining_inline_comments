{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIwMjc1OTE4", "number": 2264, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNjo0MTozOVrOD-FAUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwNzo0MDo0OVrOD_jq9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NDIwMzA1OnYy", "diffSide": "RIGHT", "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNjo0MTozOVrOGX9SXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwNjo0OToyOFrOGX9fTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc3NDU1OA==", "bodyText": "can you run the markdown linter? you need to wrap your lines among other things yarn lint:markdow", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2264#discussion_r427774558", "createdAt": "2020-05-20T06:41:39Z", "author": {"login": "cangencer"}, "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "diffHunk": "@@ -0,0 +1,260 @@\n+---\n+title: Designing an evergreen cache with Change Data Capture\n+author: Nicolas Frankel\n+authorURL: https://twitter.com/nicolas_frankel\n+authorImageURL: https://3l0wd94f0qdd10om8642z9se-wpengine.netdna-ssl.com/wp-content/uploads/2019/12/nicolas-frankel-170x170.jpg\n+---\n+\n+It's been said that there are two things hard in software development, naming things and cache invalidation (while some add off-by-one errors to the mix).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d69b87e25a98c9f4b57db18e4d3304c8ce824b8f"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc3Nzg3MQ==", "bodyText": "Yes, I can", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2264#discussion_r427777871", "createdAt": "2020-05-20T06:49:28Z", "author": {"login": "nfrankel"}, "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "diffHunk": "@@ -0,0 +1,260 @@\n+---\n+title: Designing an evergreen cache with Change Data Capture\n+author: Nicolas Frankel\n+authorURL: https://twitter.com/nicolas_frankel\n+authorImageURL: https://3l0wd94f0qdd10om8642z9se-wpengine.netdna-ssl.com/wp-content/uploads/2019/12/nicolas-frankel-170x170.jpg\n+---\n+\n+It's been said that there are two things hard in software development, naming things and cache invalidation (while some add off-by-one errors to the mix).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzc3NDU1OA=="}, "originalCommit": {"oid": "d69b87e25a98c9f4b57db18e4d3304c8ce824b8f"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjU5MTM2OnYy", "diffSide": "RIGHT", "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNjo0OTo1NFrOGYU8KA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNjo0OTo1NFrOGYU8KA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE2MjA4OA==", "bodyText": "this should be replaced my remoteMap sink, to simplify the logic and also to prevent the memory leak(since client will not be shutdown automatically with this approach after job finishes)", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2264#discussion_r428162088", "createdAt": "2020-05-20T16:49:54Z", "author": {"login": "cangencer"}, "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "diffHunk": "@@ -0,0 +1,361 @@\n+---\n+title: Designing an evergreen cache with Change Data Capture\n+author: Nicolas Frankel\n+authorURL: https://twitter.com/nicolas_frankel\n+authorImageURL: https://3l0wd94f0qdd10om8642z9se-wpengine.netdna-ssl.com/wp-content/uploads/2019/12/nicolas-frankel-170x170.jpg\n+---\n+\n+It's been said that there are two things hard in software development,\n+naming things and cache invalidation (while some add off-by-one errors\n+to the mix).\n+I believe that keeping the cache in sync with the source of truth might\n+count as a third one.\n+In this post, I'd like to tackle this issue, describe the ideal\n+situation -\n+1 cache, 1 datastore, describe the problem of having multiple components\n+that can write to the datastore, list all possible solutions, and\n+describe one elegant solution based on Change Data Capture and Jet.\n+\n+## The ideal design\n+\n+In a system, to improve performance, one of the first short-term\n+measures is to set up a cache.\n+It's a tradeoff between getting the data faster at the cost of the data\n+being not that fresh: one loads the data in-memory close to the\n+consumer, and presto, one gets an instant performance boost. In regard\n+to a database, this is akin to the following:\n+\n+![Starting architecture](assets/2020-05-20-starting-architecture.svg)\n+\n+In this read-through design, when the app requires an item, the cache\n+first checks whether it has it.\n+If yes, it returns it.\n+If not, it fetches it from the underlying Relational Database Management\n+System, stores it, and returns it.\n+For a write, it stores it, and also calls the RDBMS to store it.\n+\n+Note that using a cache-aside design instead of read-through would have\n+the same issue.\n+The only difference would be the fact that the app would be responsible\n+for the fetching/storing logic instead of the cache.\n+\n+The RDBMS is the sole source of truth - as it should be. Since the cache\n+intercepts write statements to the RDBMS, it's a mirror of the data.\n+\n+## Handling third-party database updates\n+\n+This design works as expected until the database receives updates from\n+another source:\n+\n+![Updating the database while bypassing the\n+cache](assets/2020-05-20-updating-database-bypassing-cache.svg)\n+\n+Now, the RDBMS is still the source of truth, but the cache is not aware\n+of changes made by other components.\n+Hence, it might (will) return data that it has stored, but that is stale\n+compared to what is the source of truth in the RDBMS.\n+\n+There are multiple ways to cope with this issue.\n+\n+### Cache invalidation\n+\n+Since the cache only queries the RDBMS if it doesn't store the requested\n+item, let's remove items after a specific time.\n+This is a built-in feature in enterprise-grade caches such as Hazelcast\n+IMDG, and it is known as the Time-To-Live.\n+When an item is stored, a TTL can be attached to it.\n+After that time has elapsed, the item is removed from the cache, and it\n+will be fetched from the RDBMS again if needed.\n+\n+This approach has a couple of downsides:\n+\n+1. If an item is not updated in the RDBMS, but is evicted from the\n+cache, then there's an extra query from the cache to the RDBMS when it's\n+needed by the app.\n+This is a net loss of resources.\n+2. If an item is updated in the RDBMS, but its TTL has not been reached\n+yet, then the cache will return the state data.\n+This defeats the purpose.\n+\n+With longer TTL, we avoid unnecessary round trips but return more stale\n+data.\n+With shorter TTL, we waste resources with lesser chances of stale data.\n+\n+### Polling the RDBMS\n+\n+Because the TTL doesn't seem to be the right approach, we could devise a\n+dedicated component that watches the RDBMS by regularly sending queries\n+to it and updating the cache accordingly.\n+\n+Unfortunately, this strategy incurs the same issues as cache\n+invalidation:\n+the more frequent the queries, the more chances to catch changes, but\n+the more resources are wasted.\n+Worse, this also will put extra load on the RDBMS.\n+\n+### RDBMS triggers\n+\n+A common downside of the above approaches is the way they both poll the\n+database.\n+Polling happens with a specific frequency, while writes don't follow any\n+regular periodicity.\n+Thus, it's not possible to make the two match.\n+\n+Instead of polling, it would make much more sense to be event-driven:\n+\n+1. if no writes happen, there's no need to update the cache\n+2. if a write happens, then the relevant cache item should be updated\n+accordingly\n+\n+In RDBMS, this event-driven approach is implemented via _triggers_.\n+Triggers are dedicated stored procedures that are launched in response\n+to specific events, such as an `INSERT` or an `UPDATE`.\n+\n+That works pretty well when the acted-upon object is inside the database\n+_e.g._ \"when a record of table A is updated, then add a record to table\n+B\".\n+For our use-case, where the acted-upon object is the cache, which sits\n+outside the database, it's not as simple.\n+For example, MySQL allows you to [make an external system call from a\n+trigger](https://dev.mysql.com/doc/refman/8.0/en/faqs-triggers.html#faq-mysql-can-triggers-udf).\n+However, this approach is very implementation-dependent and makes the\n+overall design of the system much more fragile.\n+Also, only some RDBMS implement triggers. Even if they do, there's no\n+standard implementation.\n+\n+## Change Data Capture\n+\n+Wikipedia defines Change Data Capture (or CDC) as:\n+\n+> [...] a set of software design patterns used to determine and track\n+> the data that has changed so that action can be taken using the\n+> changed data.\n+> >\n+> CDC is an approach to data integration that is based on the\n+> identification, capture and delivery of the changes made to enterprise\n+> data sources.\n+>\n+In practice, CDC is a tool that allows to transform standard write\n+queries into events.\n+It implements it by \"turning the database inside-out\" (quote from Martin\n+Kleppmann).\n+This definition is because a database keeps a record of all changes in\n+an implementation-dependent append-only log.\n+Regularly, it uses it to manage its state. Some RDBMS also have other\n+usage _e.g._ MySQL uses the log for replication across nodes.\n+\n+For example, here's a sample for MySQL binlog:\n+\n+```text\n+### UPDATE `test`.`t`\n+### WHERE\n+###   @1=1 /* INT meta=0 nullable=0 is_null=0 */\n+###   @2='apple' /* VARSTRING(20) meta=20 nullable=0 is_null=0 */\n+###   @3=NULL /* VARSTRING(20) meta=0 nullable=1 is_null=1 */\n+### SET\n+###   @1=1 /* INT meta=0 nullable=0 is_null=0 */\n+###   @2='pear' /* VARSTRING(20) meta=20 nullable=0 is_null=0 */\n+###   @3='2009:01:01' /* DATE meta=0 nullable=1 is_null=0 */\n+# at 569\n+#150112 21:40:14 server id 1  end_log_pos 617 CRC32 0xf134ad89\n+#Table_map: `test`.`t` mapped to number 251\n+# at 617\n+#150112 21:40:14 server id 1  end_log_pos 665 CRC32 0x87047106\n+#Delete_rows: table id 251 flags: STMT_END_F\n+```\n+\n+A CDC component connects to this immutable log to extract change events.\n+\n+One can view CDC as the opposite of Event Sourcing:\n+the latter captures state by aggregating events, while the former\n+extracts events \"from the state\".\n+\n+## Debezium\n+\n+CDC is quite recent, and hadn't time to mature.\n+As such, there's no universal standard, but specific tools.\n+In this section, we are going to have a look at\n+[Debezium](https://debezium.io/).\n+Debezium is an Open Source set of services for CDC provided by Red Hat.\n+\n+Debezium is an umbrella term covering several components:\n+\n+1. Debezium Connectors are specific bridges that read the append-only\n+proprietary log for each supported database.\n+For example, there\u2019s a connector for MySQL, one for MongoDB, one for\n+PostgreSQL, etc.\n+2. Each connector is also a Kafka Connect Source Connector:\n+this allows to easily output CDC events to one\u2019s Kafka cluster\n+3. Finally, the Debezium Engine is a JAR that allows Debezium to be\n+embedded in one\u2019s applications.\n+Note that even in that case, Debezium produces Kafka Connect-specific\n+content, which then needs to be handled and transformed in one\u2019s\n+application.\n+\n+While Kafka is a great technology, and probably also quite widespread\n+nowadays, data in Kafka needs to be persisted to disk.\n+The benefit of persistence is that data survive even in the event of the\n+cluster going down.\n+The tradeoff, however, is that the access time of disk-persisted data is\n+one (or 2) orders of magnitude slower than the access time of in-memory\n+data, depending on the underlying disk technology.\n+\n+## Hazelcast Jet\n+\n+[Hazelcast Jet](https://jet-start.sh/) is a distributed stream\n+processing framework built on Hazelcast and combines a cache with\n+fault-tolerant data processing.\n+It has sources and sinks to integrate with several file, messaging and\n+database systems (such as Amazon S3, Kafka, message brokers and\n+relational databases).\n+\n+Jet also provides a Debezium module where it can process change events\n+directly from the database, and write them to its distributed key-value\n+store.\n+This avoids having to write the intermediate messages to Kafka and then\n+read again to be written to a separate cache.\n+\n+## Putting it all together\n+\n+It\u2019s (finally!) time to assemble all the previous bits together.\n+Here are the components and their responsibilities:\n+\n+1. A MySQL database instance is where the data is stored.\n+It\u2019s accessed in read-only mode by the cache, and in write-only mode by\n+some external component\n+2. A Jet instance reads events to MySQL through the Debezium connector,\n+transforms them into cache-compatible key-value pairs, and updates the\n+cache accordingly.\n+Note that while Jet pipelines provide filtering capabilities, it\u2019s also\n+possible to filter items in the CDC connector to optimize load of the\n+pipeline\n+3. The app uses the cache, which is always up-to-date with the database,\n+ give or take the time it takes for the above to execute\n+\n+![Final architecture with\n+CDC](assets/2020-05-20-architecture-with-cdc.svg)\n+\n+Note that this architecture assumes one starts from a legacy state, with\n+an existing app that uses caching, where later on a new component was\n+set up that could update the database.\n+\n+If one starts from scratch, it\u2019s possible to simplify the above diagram\n+(and associated code) as Jet embeds its own Hazelcast instance.\n+In that case, instead of Jet being a client of a third-party Hazelcast\n+instance, Jet is the one to configure and start the instance.\n+Obviously, it also can then get/put data.\n+\n+## Talk is cheap, show me the code\n+\n+Sources for this post are available [on\n+GitHub](https://github.com/hazelcast/evergreen-cache).\n+\n+The repository is made of the following modules:\n+\n+- `app` is a Spring Boot application using Spring Data JDBC to access a\n+MySQL database.\n+It abstracts away Hazelcast by using a Spring Cache layer\n+- `update` is a Spring Shell application.\n+It allows to update the data inside the database, with the cache none\n+the wiser\n+- `pipeline` is the Jet pipeline that listens to CDC events and update\n+the cache when data is updated\n+\n+The pipeline definition is quite straightforward:\n+\n+```java\n+pipeline.readFrom(DebeziumSources.cdc(configuration))           // 1\n+  .withoutTimestamps()\n+  .map(r -> Values.convertToString(r.valueSchema(), r.value())) // 2\n+  .apply(new JsonToPerson())                                    // 3\n+  .writeTo(\n+    SinkBuilder.sinkBuilder(                                    // 4", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0c122f766d77b65fd4a33d9cc6919398e928331"}, "originalPosition": 272}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjU5Mjg4OnYy", "diffSide": "RIGHT", "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNjo1MDoxN1rOGYU9Kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNjo1MDoxN1rOGYU9Kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE2MjM0Nw==", "bodyText": "this double > shows up as a blank line", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2264#discussion_r428162347", "createdAt": "2020-05-20T16:50:17Z", "author": {"login": "cangencer"}, "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "diffHunk": "@@ -0,0 +1,361 @@\n+---\n+title: Designing an evergreen cache with Change Data Capture\n+author: Nicolas Frankel\n+authorURL: https://twitter.com/nicolas_frankel\n+authorImageURL: https://3l0wd94f0qdd10om8642z9se-wpengine.netdna-ssl.com/wp-content/uploads/2019/12/nicolas-frankel-170x170.jpg\n+---\n+\n+It's been said that there are two things hard in software development,\n+naming things and cache invalidation (while some add off-by-one errors\n+to the mix).\n+I believe that keeping the cache in sync with the source of truth might\n+count as a third one.\n+In this post, I'd like to tackle this issue, describe the ideal\n+situation -\n+1 cache, 1 datastore, describe the problem of having multiple components\n+that can write to the datastore, list all possible solutions, and\n+describe one elegant solution based on Change Data Capture and Jet.\n+\n+## The ideal design\n+\n+In a system, to improve performance, one of the first short-term\n+measures is to set up a cache.\n+It's a tradeoff between getting the data faster at the cost of the data\n+being not that fresh: one loads the data in-memory close to the\n+consumer, and presto, one gets an instant performance boost. In regard\n+to a database, this is akin to the following:\n+\n+![Starting architecture](assets/2020-05-20-starting-architecture.svg)\n+\n+In this read-through design, when the app requires an item, the cache\n+first checks whether it has it.\n+If yes, it returns it.\n+If not, it fetches it from the underlying Relational Database Management\n+System, stores it, and returns it.\n+For a write, it stores it, and also calls the RDBMS to store it.\n+\n+Note that using a cache-aside design instead of read-through would have\n+the same issue.\n+The only difference would be the fact that the app would be responsible\n+for the fetching/storing logic instead of the cache.\n+\n+The RDBMS is the sole source of truth - as it should be. Since the cache\n+intercepts write statements to the RDBMS, it's a mirror of the data.\n+\n+## Handling third-party database updates\n+\n+This design works as expected until the database receives updates from\n+another source:\n+\n+![Updating the database while bypassing the\n+cache](assets/2020-05-20-updating-database-bypassing-cache.svg)\n+\n+Now, the RDBMS is still the source of truth, but the cache is not aware\n+of changes made by other components.\n+Hence, it might (will) return data that it has stored, but that is stale\n+compared to what is the source of truth in the RDBMS.\n+\n+There are multiple ways to cope with this issue.\n+\n+### Cache invalidation\n+\n+Since the cache only queries the RDBMS if it doesn't store the requested\n+item, let's remove items after a specific time.\n+This is a built-in feature in enterprise-grade caches such as Hazelcast\n+IMDG, and it is known as the Time-To-Live.\n+When an item is stored, a TTL can be attached to it.\n+After that time has elapsed, the item is removed from the cache, and it\n+will be fetched from the RDBMS again if needed.\n+\n+This approach has a couple of downsides:\n+\n+1. If an item is not updated in the RDBMS, but is evicted from the\n+cache, then there's an extra query from the cache to the RDBMS when it's\n+needed by the app.\n+This is a net loss of resources.\n+2. If an item is updated in the RDBMS, but its TTL has not been reached\n+yet, then the cache will return the state data.\n+This defeats the purpose.\n+\n+With longer TTL, we avoid unnecessary round trips but return more stale\n+data.\n+With shorter TTL, we waste resources with lesser chances of stale data.\n+\n+### Polling the RDBMS\n+\n+Because the TTL doesn't seem to be the right approach, we could devise a\n+dedicated component that watches the RDBMS by regularly sending queries\n+to it and updating the cache accordingly.\n+\n+Unfortunately, this strategy incurs the same issues as cache\n+invalidation:\n+the more frequent the queries, the more chances to catch changes, but\n+the more resources are wasted.\n+Worse, this also will put extra load on the RDBMS.\n+\n+### RDBMS triggers\n+\n+A common downside of the above approaches is the way they both poll the\n+database.\n+Polling happens with a specific frequency, while writes don't follow any\n+regular periodicity.\n+Thus, it's not possible to make the two match.\n+\n+Instead of polling, it would make much more sense to be event-driven:\n+\n+1. if no writes happen, there's no need to update the cache\n+2. if a write happens, then the relevant cache item should be updated\n+accordingly\n+\n+In RDBMS, this event-driven approach is implemented via _triggers_.\n+Triggers are dedicated stored procedures that are launched in response\n+to specific events, such as an `INSERT` or an `UPDATE`.\n+\n+That works pretty well when the acted-upon object is inside the database\n+_e.g._ \"when a record of table A is updated, then add a record to table\n+B\".\n+For our use-case, where the acted-upon object is the cache, which sits\n+outside the database, it's not as simple.\n+For example, MySQL allows you to [make an external system call from a\n+trigger](https://dev.mysql.com/doc/refman/8.0/en/faqs-triggers.html#faq-mysql-can-triggers-udf).\n+However, this approach is very implementation-dependent and makes the\n+overall design of the system much more fragile.\n+Also, only some RDBMS implement triggers. Even if they do, there's no\n+standard implementation.\n+\n+## Change Data Capture\n+\n+Wikipedia defines Change Data Capture (or CDC) as:\n+\n+> [...] a set of software design patterns used to determine and track\n+> the data that has changed so that action can be taken using the\n+> changed data.\n+> >", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0c122f766d77b65fd4a33d9cc6919398e928331"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY2NjU5NDA1OnYy", "diffSide": "RIGHT", "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNjo1MDozN1rOGYU96g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQxNjo1MDozN1rOGYU96g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE2MjUzOA==", "bodyText": "i think should be newline after this.", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2264#discussion_r428162538", "createdAt": "2020-05-20T16:50:37Z", "author": {"login": "cangencer"}, "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "diffHunk": "@@ -0,0 +1,361 @@\n+---\n+title: Designing an evergreen cache with Change Data Capture\n+author: Nicolas Frankel\n+authorURL: https://twitter.com/nicolas_frankel\n+authorImageURL: https://3l0wd94f0qdd10om8642z9se-wpengine.netdna-ssl.com/wp-content/uploads/2019/12/nicolas-frankel-170x170.jpg\n+---\n+\n+It's been said that there are two things hard in software development,\n+naming things and cache invalidation (while some add off-by-one errors\n+to the mix).\n+I believe that keeping the cache in sync with the source of truth might\n+count as a third one.\n+In this post, I'd like to tackle this issue, describe the ideal\n+situation -\n+1 cache, 1 datastore, describe the problem of having multiple components\n+that can write to the datastore, list all possible solutions, and\n+describe one elegant solution based on Change Data Capture and Jet.\n+\n+## The ideal design\n+\n+In a system, to improve performance, one of the first short-term\n+measures is to set up a cache.\n+It's a tradeoff between getting the data faster at the cost of the data\n+being not that fresh: one loads the data in-memory close to the\n+consumer, and presto, one gets an instant performance boost. In regard\n+to a database, this is akin to the following:\n+\n+![Starting architecture](assets/2020-05-20-starting-architecture.svg)\n+\n+In this read-through design, when the app requires an item, the cache\n+first checks whether it has it.\n+If yes, it returns it.\n+If not, it fetches it from the underlying Relational Database Management\n+System, stores it, and returns it.\n+For a write, it stores it, and also calls the RDBMS to store it.\n+\n+Note that using a cache-aside design instead of read-through would have\n+the same issue.\n+The only difference would be the fact that the app would be responsible\n+for the fetching/storing logic instead of the cache.\n+\n+The RDBMS is the sole source of truth - as it should be. Since the cache\n+intercepts write statements to the RDBMS, it's a mirror of the data.\n+\n+## Handling third-party database updates\n+\n+This design works as expected until the database receives updates from\n+another source:\n+\n+![Updating the database while bypassing the\n+cache](assets/2020-05-20-updating-database-bypassing-cache.svg)\n+\n+Now, the RDBMS is still the source of truth, but the cache is not aware\n+of changes made by other components.\n+Hence, it might (will) return data that it has stored, but that is stale\n+compared to what is the source of truth in the RDBMS.\n+\n+There are multiple ways to cope with this issue.\n+\n+### Cache invalidation\n+\n+Since the cache only queries the RDBMS if it doesn't store the requested\n+item, let's remove items after a specific time.\n+This is a built-in feature in enterprise-grade caches such as Hazelcast\n+IMDG, and it is known as the Time-To-Live.\n+When an item is stored, a TTL can be attached to it.\n+After that time has elapsed, the item is removed from the cache, and it\n+will be fetched from the RDBMS again if needed.\n+\n+This approach has a couple of downsides:\n+\n+1. If an item is not updated in the RDBMS, but is evicted from the\n+cache, then there's an extra query from the cache to the RDBMS when it's\n+needed by the app.\n+This is a net loss of resources.\n+2. If an item is updated in the RDBMS, but its TTL has not been reached\n+yet, then the cache will return the state data.\n+This defeats the purpose.\n+\n+With longer TTL, we avoid unnecessary round trips but return more stale\n+data.\n+With shorter TTL, we waste resources with lesser chances of stale data.\n+\n+### Polling the RDBMS\n+\n+Because the TTL doesn't seem to be the right approach, we could devise a\n+dedicated component that watches the RDBMS by regularly sending queries\n+to it and updating the cache accordingly.\n+\n+Unfortunately, this strategy incurs the same issues as cache\n+invalidation:\n+the more frequent the queries, the more chances to catch changes, but\n+the more resources are wasted.\n+Worse, this also will put extra load on the RDBMS.\n+\n+### RDBMS triggers\n+\n+A common downside of the above approaches is the way they both poll the\n+database.\n+Polling happens with a specific frequency, while writes don't follow any\n+regular periodicity.\n+Thus, it's not possible to make the two match.\n+\n+Instead of polling, it would make much more sense to be event-driven:\n+\n+1. if no writes happen, there's no need to update the cache\n+2. if a write happens, then the relevant cache item should be updated\n+accordingly\n+\n+In RDBMS, this event-driven approach is implemented via _triggers_.\n+Triggers are dedicated stored procedures that are launched in response\n+to specific events, such as an `INSERT` or an `UPDATE`.\n+\n+That works pretty well when the acted-upon object is inside the database\n+_e.g._ \"when a record of table A is updated, then add a record to table\n+B\".\n+For our use-case, where the acted-upon object is the cache, which sits\n+outside the database, it's not as simple.\n+For example, MySQL allows you to [make an external system call from a\n+trigger](https://dev.mysql.com/doc/refman/8.0/en/faqs-triggers.html#faq-mysql-can-triggers-udf).\n+However, this approach is very implementation-dependent and makes the\n+overall design of the system much more fragile.\n+Also, only some RDBMS implement triggers. Even if they do, there's no\n+standard implementation.\n+\n+## Change Data Capture\n+\n+Wikipedia defines Change Data Capture (or CDC) as:\n+\n+> [...] a set of software design patterns used to determine and track\n+> the data that has changed so that action can be taken using the\n+> changed data.\n+> >\n+> CDC is an approach to data integration that is based on the\n+> identification, capture and delivery of the changes made to enterprise\n+> data sources.\n+>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0c122f766d77b65fd4a33d9cc6919398e928331"}, "originalPosition": 137}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MjE0NDE0OnYy", "diffSide": "RIGHT", "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwNTo1MTozMVrOGZLb4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwNzozNzoxOVrOGaSIKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA1NDk0NA==", "bodyText": "what's \"CustomClientConfig\"?", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2264#discussion_r429054944", "createdAt": "2020-05-22T05:51:31Z", "author": {"login": "cangencer"}, "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "diffHunk": "@@ -0,0 +1,346 @@\n+---\n+title: Designing an evergreen cache with Change Data Capture\n+author: Nicolas Frankel\n+authorURL: https://twitter.com/nicolas_frankel\n+authorImageURL: https://3l0wd94f0qdd10om8642z9se-wpengine.netdna-ssl.com/wp-content/uploads/2019/12/nicolas-frankel-170x170.jpg\n+---\n+\n+It's been said that there are two things hard in software development,\n+naming things and cache invalidation (while some add off-by-one errors\n+to the mix).\n+I believe that keeping the cache in sync with the source of truth might\n+count as a third one.\n+In this post, I'd like to tackle this issue, describe the ideal\n+situation -\n+1 cache, 1 datastore, describe the problem of having multiple components\n+that can write to the datastore, list all possible solutions, and\n+describe one elegant solution based on Change Data Capture and Jet.\n+\n+## The ideal design\n+\n+In a system, to improve performance, one of the first short-term\n+measures is to set up a cache.\n+It's a tradeoff between getting the data faster at the cost of the data\n+being not that fresh: one loads the data in-memory close to the\n+consumer, and presto, one gets an instant performance boost. In regard\n+to a database, this is akin to the following:\n+\n+![Starting architecture](assets/2020-05-20-starting-architecture.svg)\n+\n+In this read-through design, when the app requires an item, the cache\n+first checks whether it has it.\n+If yes, it returns it.\n+If not, it fetches it from the underlying Relational Database Management\n+System, stores it, and returns it.\n+For a write, it stores it, and also calls the RDBMS to store it.\n+\n+Note that using a cache-aside design instead of read-through would have\n+the same issue.\n+The only difference would be the fact that the app would be responsible\n+for the fetching/storing logic instead of the cache.\n+\n+The RDBMS is the sole source of truth - as it should be. Since the cache\n+intercepts write statements to the RDBMS, it's a mirror of the data.\n+\n+## Handling third-party database updates\n+\n+This design works as expected until the database receives updates from\n+another source:\n+\n+![Updating the database while bypassing the\n+cache](assets/2020-05-20-updating-database-bypassing-cache.svg)\n+\n+Now, the RDBMS is still the source of truth, but the cache is not aware\n+of changes made by other components.\n+Hence, it might (will) return data that it has stored, but that is stale\n+compared to what is the source of truth in the RDBMS.\n+\n+There are multiple ways to cope with this issue.\n+\n+### Cache invalidation\n+\n+Since the cache only queries the RDBMS if it doesn't store the requested\n+item, let's remove items after a specific time.\n+This is a built-in feature in enterprise-grade caches such as Hazelcast\n+IMDG, and it is known as the Time-To-Live.\n+When an item is stored, a TTL can be attached to it.\n+After that time has elapsed, the item is removed from the cache, and it\n+will be fetched from the RDBMS again if needed.\n+\n+This approach has a couple of downsides:\n+\n+1. If an item is not updated in the RDBMS, but is evicted from the\n+cache, then there's an extra query from the cache to the RDBMS when it's\n+needed by the app.\n+This is a net loss of resources.\n+2. If an item is updated in the RDBMS, but its TTL has not been reached\n+yet, then the cache will return the state data.\n+This defeats the purpose.\n+\n+With longer TTL, we avoid unnecessary round trips but return more stale\n+data.\n+With shorter TTL, we waste resources with lesser chances of stale data.\n+\n+### Polling the RDBMS\n+\n+Because the TTL doesn't seem to be the right approach, we could devise a\n+dedicated component that watches the RDBMS by regularly sending queries\n+to it and updating the cache accordingly.\n+\n+Unfortunately, this strategy incurs the same issues as cache\n+invalidation:\n+the more frequent the queries, the more chances to catch changes, but\n+the more resources are wasted.\n+Worse, this also will put extra load on the RDBMS.\n+\n+### RDBMS triggers\n+\n+A common downside of the above approaches is the way they both poll the\n+database.\n+Polling happens with a specific frequency, while writes don't follow any\n+regular periodicity.\n+Thus, it's not possible to make the two match.\n+\n+Instead of polling, it would make much more sense to be event-driven:\n+\n+1. if no writes happen, there's no need to update the cache\n+2. if a write happens, then the relevant cache item should be updated\n+accordingly\n+\n+In RDBMS, this event-driven approach is implemented via _triggers_.\n+Triggers are dedicated stored procedures that are launched in response\n+to specific events, such as an `INSERT` or an `UPDATE`.\n+\n+That works pretty well when the acted-upon object is inside the database\n+_e.g._ \"when a record of table A is updated, then add a record to table\n+B\".\n+For our use-case, where the acted-upon object is the cache, which sits\n+outside the database, it's not as simple.\n+For example, MySQL allows you to [make an external system call from a\n+trigger](https://dev.mysql.com/doc/refman/8.0/en/faqs-triggers.html#faq-mysql-can-triggers-udf).\n+However, this approach is very implementation-dependent and makes the\n+overall design of the system much more fragile.\n+Also, only some RDBMS implement triggers. Even if they do, there's no\n+standard implementation.\n+\n+## Change Data Capture\n+\n+Wikipedia defines Change Data Capture (or CDC) as:\n+\n+> [...] a set of software design patterns used to determine and track\n+> the data that has changed so that action can be taken using the\n+> changed data.\n+>\n+> CDC is an approach to data integration that is based on the\n+> identification, capture and delivery of the changes made to enterprise\n+> data sources.\n+\n+In practice, CDC is a tool that allows to transform standard write\n+queries into events.\n+It implements it by \"turning the database inside-out\" (quote from Martin\n+Kleppmann).\n+This definition is because a database keeps a record of all changes in\n+an implementation-dependent append-only log.\n+Regularly, it uses it to manage its state. Some RDBMS also have other\n+usage _e.g._ MySQL uses the log for replication across nodes.\n+\n+For example, here's a sample for MySQL binlog:\n+\n+```text\n+### UPDATE `test`.`t`\n+### WHERE\n+###   @1=1 /* INT meta=0 nullable=0 is_null=0 */\n+###   @2='apple' /* VARSTRING(20) meta=20 nullable=0 is_null=0 */\n+###   @3=NULL /* VARSTRING(20) meta=0 nullable=1 is_null=1 */\n+### SET\n+###   @1=1 /* INT meta=0 nullable=0 is_null=0 */\n+###   @2='pear' /* VARSTRING(20) meta=20 nullable=0 is_null=0 */\n+###   @3='2009:01:01' /* DATE meta=0 nullable=1 is_null=0 */\n+# at 569\n+#150112 21:40:14 server id 1  end_log_pos 617 CRC32 0xf134ad89\n+#Table_map: `test`.`t` mapped to number 251\n+# at 617\n+#150112 21:40:14 server id 1  end_log_pos 665 CRC32 0x87047106\n+#Delete_rows: table id 251 flags: STMT_END_F\n+```\n+\n+A CDC component connects to this immutable log to extract change events.\n+\n+One can view CDC as the opposite of Event Sourcing:\n+the latter captures state by aggregating events, while the former\n+extracts events \"from the state\".\n+\n+## Debezium\n+\n+CDC is quite recent, and hadn't time to mature.\n+As such, there's no universal standard, but specific tools.\n+In this section, we are going to have a look at\n+[Debezium](https://debezium.io/).\n+Debezium is an Open Source set of services for CDC provided by Red Hat.\n+\n+Debezium is an umbrella term covering several components:\n+\n+1. Debezium Connectors are specific bridges that read the append-only\n+proprietary log for each supported database.\n+For example, there\u2019s a connector for MySQL, one for MongoDB, one for\n+PostgreSQL, etc.\n+2. Each connector is also a Kafka Connect Source Connector:\n+this allows to easily output CDC events to one\u2019s Kafka cluster\n+3. Finally, the Debezium Engine is a JAR that allows Debezium to be\n+embedded in one\u2019s applications.\n+Note that even in that case, Debezium produces Kafka Connect-specific\n+content, which then needs to be handled and transformed in one\u2019s\n+application.\n+\n+While Kafka is a great technology, and probably also quite widespread\n+nowadays, data in Kafka needs to be persisted to disk.\n+The benefit of persistence is that data survive even in the event of the\n+cluster going down.\n+The tradeoff, however, is that the access time of disk-persisted data is\n+one (or 2) orders of magnitude slower than the access time of in-memory\n+data, depending on the underlying disk technology.\n+\n+## Hazelcast Jet\n+\n+[Hazelcast Jet](https://jet-start.sh/) is a distributed stream\n+processing framework built on Hazelcast and combines a cache with\n+fault-tolerant data processing.\n+It has sources and sinks to integrate with several file, messaging and\n+database systems (such as Amazon S3, Kafka, message brokers and\n+relational databases).\n+\n+Jet also provides a Debezium module where it can process change events\n+directly from the database, and write them to its distributed key-value\n+store.\n+This avoids having to write the intermediate messages to Kafka and then\n+read again to be written to a separate cache.\n+\n+## Putting it all together\n+\n+It\u2019s (finally!) time to assemble all the previous bits together.\n+Here are the components and their responsibilities:\n+\n+1. A MySQL database instance is where the data is stored.\n+It\u2019s accessed in read-only mode by the cache, and in write-only mode by\n+some external component\n+2. A Jet instance reads events to MySQL through the Debezium connector,\n+transforms them into cache-compatible key-value pairs, and updates the\n+cache accordingly.\n+Note that while Jet pipelines provide filtering capabilities, it\u2019s also\n+possible to filter items in the CDC connector to optimize load of the\n+pipeline\n+3. The app uses the cache, which is always up-to-date with the database,\n+ give or take the time it takes for the above to execute\n+\n+![Final architecture with\n+CDC](assets/2020-05-20-architecture-with-cdc.svg)\n+\n+Note that this architecture assumes one starts from a legacy state, with\n+an existing app that uses caching, where later on a new component was\n+set up that could update the database.\n+\n+If one starts from scratch, it\u2019s possible to simplify the above diagram\n+(and associated code) as Jet embeds its own Hazelcast instance.\n+In that case, instead of Jet being a client of a third-party Hazelcast\n+instance, Jet is the one to configure and start the instance.\n+Obviously, it also can then get/put data.\n+\n+## Talk is cheap, show me the code\n+\n+Sources for this post are available [on\n+GitHub](https://github.com/hazelcast/evergreen-cache).\n+\n+The repository is made of the following modules:\n+\n+- `app` is a Spring Boot application using Spring Data JDBC to access a\n+MySQL database.\n+It abstracts away Hazelcast by using a Spring Cache layer\n+- `update` is a Spring Shell application.\n+It allows to update the data inside the database, with the cache none\n+the wiser\n+- `pipeline` is the Jet pipeline that listens to CDC events and update\n+the cache when data is updated\n+\n+The pipeline definition is quite straightforward:\n+\n+```java\n+pipeline.readFrom(DebeziumSources.cdc(configuration))            // 1\n+  .withoutTimestamps()\n+  .map(r -> Values.convertToString(r.valueSchema(), r.value()))  // 2\n+  .apply(new JsonToPerson())                                     // 3\n+  .writeTo(Sinks.remoteMap(                                      // 4\n+    \"entities\",                                                  // 5\n+    new CustomClientConfig(env.get(\"CACHE_HOST\"))                // 6", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b7185b9e1e5ccd74f0c316a8fbc9d2771e056e8"}, "originalPosition": 273}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDIxMzE2MQ==", "bodyText": "It's described in the list below. I've added the code to be more explicit", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2264#discussion_r430213161", "createdAt": "2020-05-26T07:37:19Z", "author": {"login": "nfrankel"}, "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "diffHunk": "@@ -0,0 +1,346 @@\n+---\n+title: Designing an evergreen cache with Change Data Capture\n+author: Nicolas Frankel\n+authorURL: https://twitter.com/nicolas_frankel\n+authorImageURL: https://3l0wd94f0qdd10om8642z9se-wpengine.netdna-ssl.com/wp-content/uploads/2019/12/nicolas-frankel-170x170.jpg\n+---\n+\n+It's been said that there are two things hard in software development,\n+naming things and cache invalidation (while some add off-by-one errors\n+to the mix).\n+I believe that keeping the cache in sync with the source of truth might\n+count as a third one.\n+In this post, I'd like to tackle this issue, describe the ideal\n+situation -\n+1 cache, 1 datastore, describe the problem of having multiple components\n+that can write to the datastore, list all possible solutions, and\n+describe one elegant solution based on Change Data Capture and Jet.\n+\n+## The ideal design\n+\n+In a system, to improve performance, one of the first short-term\n+measures is to set up a cache.\n+It's a tradeoff between getting the data faster at the cost of the data\n+being not that fresh: one loads the data in-memory close to the\n+consumer, and presto, one gets an instant performance boost. In regard\n+to a database, this is akin to the following:\n+\n+![Starting architecture](assets/2020-05-20-starting-architecture.svg)\n+\n+In this read-through design, when the app requires an item, the cache\n+first checks whether it has it.\n+If yes, it returns it.\n+If not, it fetches it from the underlying Relational Database Management\n+System, stores it, and returns it.\n+For a write, it stores it, and also calls the RDBMS to store it.\n+\n+Note that using a cache-aside design instead of read-through would have\n+the same issue.\n+The only difference would be the fact that the app would be responsible\n+for the fetching/storing logic instead of the cache.\n+\n+The RDBMS is the sole source of truth - as it should be. Since the cache\n+intercepts write statements to the RDBMS, it's a mirror of the data.\n+\n+## Handling third-party database updates\n+\n+This design works as expected until the database receives updates from\n+another source:\n+\n+![Updating the database while bypassing the\n+cache](assets/2020-05-20-updating-database-bypassing-cache.svg)\n+\n+Now, the RDBMS is still the source of truth, but the cache is not aware\n+of changes made by other components.\n+Hence, it might (will) return data that it has stored, but that is stale\n+compared to what is the source of truth in the RDBMS.\n+\n+There are multiple ways to cope with this issue.\n+\n+### Cache invalidation\n+\n+Since the cache only queries the RDBMS if it doesn't store the requested\n+item, let's remove items after a specific time.\n+This is a built-in feature in enterprise-grade caches such as Hazelcast\n+IMDG, and it is known as the Time-To-Live.\n+When an item is stored, a TTL can be attached to it.\n+After that time has elapsed, the item is removed from the cache, and it\n+will be fetched from the RDBMS again if needed.\n+\n+This approach has a couple of downsides:\n+\n+1. If an item is not updated in the RDBMS, but is evicted from the\n+cache, then there's an extra query from the cache to the RDBMS when it's\n+needed by the app.\n+This is a net loss of resources.\n+2. If an item is updated in the RDBMS, but its TTL has not been reached\n+yet, then the cache will return the state data.\n+This defeats the purpose.\n+\n+With longer TTL, we avoid unnecessary round trips but return more stale\n+data.\n+With shorter TTL, we waste resources with lesser chances of stale data.\n+\n+### Polling the RDBMS\n+\n+Because the TTL doesn't seem to be the right approach, we could devise a\n+dedicated component that watches the RDBMS by regularly sending queries\n+to it and updating the cache accordingly.\n+\n+Unfortunately, this strategy incurs the same issues as cache\n+invalidation:\n+the more frequent the queries, the more chances to catch changes, but\n+the more resources are wasted.\n+Worse, this also will put extra load on the RDBMS.\n+\n+### RDBMS triggers\n+\n+A common downside of the above approaches is the way they both poll the\n+database.\n+Polling happens with a specific frequency, while writes don't follow any\n+regular periodicity.\n+Thus, it's not possible to make the two match.\n+\n+Instead of polling, it would make much more sense to be event-driven:\n+\n+1. if no writes happen, there's no need to update the cache\n+2. if a write happens, then the relevant cache item should be updated\n+accordingly\n+\n+In RDBMS, this event-driven approach is implemented via _triggers_.\n+Triggers are dedicated stored procedures that are launched in response\n+to specific events, such as an `INSERT` or an `UPDATE`.\n+\n+That works pretty well when the acted-upon object is inside the database\n+_e.g._ \"when a record of table A is updated, then add a record to table\n+B\".\n+For our use-case, where the acted-upon object is the cache, which sits\n+outside the database, it's not as simple.\n+For example, MySQL allows you to [make an external system call from a\n+trigger](https://dev.mysql.com/doc/refman/8.0/en/faqs-triggers.html#faq-mysql-can-triggers-udf).\n+However, this approach is very implementation-dependent and makes the\n+overall design of the system much more fragile.\n+Also, only some RDBMS implement triggers. Even if they do, there's no\n+standard implementation.\n+\n+## Change Data Capture\n+\n+Wikipedia defines Change Data Capture (or CDC) as:\n+\n+> [...] a set of software design patterns used to determine and track\n+> the data that has changed so that action can be taken using the\n+> changed data.\n+>\n+> CDC is an approach to data integration that is based on the\n+> identification, capture and delivery of the changes made to enterprise\n+> data sources.\n+\n+In practice, CDC is a tool that allows to transform standard write\n+queries into events.\n+It implements it by \"turning the database inside-out\" (quote from Martin\n+Kleppmann).\n+This definition is because a database keeps a record of all changes in\n+an implementation-dependent append-only log.\n+Regularly, it uses it to manage its state. Some RDBMS also have other\n+usage _e.g._ MySQL uses the log for replication across nodes.\n+\n+For example, here's a sample for MySQL binlog:\n+\n+```text\n+### UPDATE `test`.`t`\n+### WHERE\n+###   @1=1 /* INT meta=0 nullable=0 is_null=0 */\n+###   @2='apple' /* VARSTRING(20) meta=20 nullable=0 is_null=0 */\n+###   @3=NULL /* VARSTRING(20) meta=0 nullable=1 is_null=1 */\n+### SET\n+###   @1=1 /* INT meta=0 nullable=0 is_null=0 */\n+###   @2='pear' /* VARSTRING(20) meta=20 nullable=0 is_null=0 */\n+###   @3='2009:01:01' /* DATE meta=0 nullable=1 is_null=0 */\n+# at 569\n+#150112 21:40:14 server id 1  end_log_pos 617 CRC32 0xf134ad89\n+#Table_map: `test`.`t` mapped to number 251\n+# at 617\n+#150112 21:40:14 server id 1  end_log_pos 665 CRC32 0x87047106\n+#Delete_rows: table id 251 flags: STMT_END_F\n+```\n+\n+A CDC component connects to this immutable log to extract change events.\n+\n+One can view CDC as the opposite of Event Sourcing:\n+the latter captures state by aggregating events, while the former\n+extracts events \"from the state\".\n+\n+## Debezium\n+\n+CDC is quite recent, and hadn't time to mature.\n+As such, there's no universal standard, but specific tools.\n+In this section, we are going to have a look at\n+[Debezium](https://debezium.io/).\n+Debezium is an Open Source set of services for CDC provided by Red Hat.\n+\n+Debezium is an umbrella term covering several components:\n+\n+1. Debezium Connectors are specific bridges that read the append-only\n+proprietary log for each supported database.\n+For example, there\u2019s a connector for MySQL, one for MongoDB, one for\n+PostgreSQL, etc.\n+2. Each connector is also a Kafka Connect Source Connector:\n+this allows to easily output CDC events to one\u2019s Kafka cluster\n+3. Finally, the Debezium Engine is a JAR that allows Debezium to be\n+embedded in one\u2019s applications.\n+Note that even in that case, Debezium produces Kafka Connect-specific\n+content, which then needs to be handled and transformed in one\u2019s\n+application.\n+\n+While Kafka is a great technology, and probably also quite widespread\n+nowadays, data in Kafka needs to be persisted to disk.\n+The benefit of persistence is that data survive even in the event of the\n+cluster going down.\n+The tradeoff, however, is that the access time of disk-persisted data is\n+one (or 2) orders of magnitude slower than the access time of in-memory\n+data, depending on the underlying disk technology.\n+\n+## Hazelcast Jet\n+\n+[Hazelcast Jet](https://jet-start.sh/) is a distributed stream\n+processing framework built on Hazelcast and combines a cache with\n+fault-tolerant data processing.\n+It has sources and sinks to integrate with several file, messaging and\n+database systems (such as Amazon S3, Kafka, message brokers and\n+relational databases).\n+\n+Jet also provides a Debezium module where it can process change events\n+directly from the database, and write them to its distributed key-value\n+store.\n+This avoids having to write the intermediate messages to Kafka and then\n+read again to be written to a separate cache.\n+\n+## Putting it all together\n+\n+It\u2019s (finally!) time to assemble all the previous bits together.\n+Here are the components and their responsibilities:\n+\n+1. A MySQL database instance is where the data is stored.\n+It\u2019s accessed in read-only mode by the cache, and in write-only mode by\n+some external component\n+2. A Jet instance reads events to MySQL through the Debezium connector,\n+transforms them into cache-compatible key-value pairs, and updates the\n+cache accordingly.\n+Note that while Jet pipelines provide filtering capabilities, it\u2019s also\n+possible to filter items in the CDC connector to optimize load of the\n+pipeline\n+3. The app uses the cache, which is always up-to-date with the database,\n+ give or take the time it takes for the above to execute\n+\n+![Final architecture with\n+CDC](assets/2020-05-20-architecture-with-cdc.svg)\n+\n+Note that this architecture assumes one starts from a legacy state, with\n+an existing app that uses caching, where later on a new component was\n+set up that could update the database.\n+\n+If one starts from scratch, it\u2019s possible to simplify the above diagram\n+(and associated code) as Jet embeds its own Hazelcast instance.\n+In that case, instead of Jet being a client of a third-party Hazelcast\n+instance, Jet is the one to configure and start the instance.\n+Obviously, it also can then get/put data.\n+\n+## Talk is cheap, show me the code\n+\n+Sources for this post are available [on\n+GitHub](https://github.com/hazelcast/evergreen-cache).\n+\n+The repository is made of the following modules:\n+\n+- `app` is a Spring Boot application using Spring Data JDBC to access a\n+MySQL database.\n+It abstracts away Hazelcast by using a Spring Cache layer\n+- `update` is a Spring Shell application.\n+It allows to update the data inside the database, with the cache none\n+the wiser\n+- `pipeline` is the Jet pipeline that listens to CDC events and update\n+the cache when data is updated\n+\n+The pipeline definition is quite straightforward:\n+\n+```java\n+pipeline.readFrom(DebeziumSources.cdc(configuration))            // 1\n+  .withoutTimestamps()\n+  .map(r -> Values.convertToString(r.valueSchema(), r.value()))  // 2\n+  .apply(new JsonToPerson())                                     // 3\n+  .writeTo(Sinks.remoteMap(                                      // 4\n+    \"entities\",                                                  // 5\n+    new CustomClientConfig(env.get(\"CACHE_HOST\"))                // 6", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA1NDk0NA=="}, "originalCommit": {"oid": "0b7185b9e1e5ccd74f0c316a8fbc9d2771e056e8"}, "originalPosition": 273}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3OTcxMTIxOnYy", "diffSide": "RIGHT", "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwNzo0MDoxOFrOGaSOLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwNzo0MDoxOFrOGaSOLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDIxNDcwMw==", "bodyText": "this is already default", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2264#discussion_r430214703", "createdAt": "2020-05-26T07:40:18Z", "author": {"login": "cangencer"}, "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "diffHunk": "@@ -0,0 +1,361 @@\n+---\n+title: Designing an evergreen cache with Change Data Capture\n+author: Nicolas Frankel\n+authorURL: https://twitter.com/nicolas_frankel\n+authorImageURL: https://3l0wd94f0qdd10om8642z9se-wpengine.netdna-ssl.com/wp-content/uploads/2019/12/nicolas-frankel-170x170.jpg\n+---\n+\n+It's been said that there are two things hard in software development,\n+naming things and cache invalidation (while some add off-by-one errors\n+to the mix).\n+I believe that keeping the cache in sync with the source of truth might\n+count as a third one.\n+In this post, I'd like to tackle this issue, describe the ideal\n+situation -\n+1 cache, 1 datastore, describe the problem of having multiple components\n+that can write to the datastore, list all possible solutions, and\n+describe one elegant solution based on Change Data Capture and Jet.\n+\n+## The ideal design\n+\n+In a system, to improve performance, one of the first short-term\n+measures is to set up a cache.\n+It's a tradeoff between getting the data faster at the cost of the data\n+being not that fresh: one loads the data in-memory close to the\n+consumer, and presto, one gets an instant performance boost. In regard\n+to a database, this is akin to the following:\n+\n+![Starting architecture](assets/2020-05-20-starting-architecture.svg)\n+\n+In this read-through design, when the app requires an item, the cache\n+first checks whether it has it.\n+If yes, it returns it.\n+If not, it fetches it from the underlying Relational Database Management\n+System, stores it, and returns it.\n+For a write, it stores it, and also calls the RDBMS to store it.\n+\n+Note that using a cache-aside design instead of read-through would have\n+the same issue.\n+The only difference would be the fact that the app would be responsible\n+for the fetching/storing logic instead of the cache.\n+\n+The RDBMS is the sole source of truth - as it should be. Since the cache\n+intercepts write statements to the RDBMS, it's a mirror of the data.\n+\n+## Handling third-party database updates\n+\n+This design works as expected until the database receives updates from\n+another source:\n+\n+![Updating the database while bypassing the\n+cache](assets/2020-05-20-updating-database-bypassing-cache.svg)\n+\n+Now, the RDBMS is still the source of truth, but the cache is not aware\n+of changes made by other components.\n+Hence, it might (will) return data that it has stored, but that is stale\n+compared to what is the source of truth in the RDBMS.\n+\n+There are multiple ways to cope with this issue.\n+\n+### Cache invalidation\n+\n+Since the cache only queries the RDBMS if it doesn't store the requested\n+item, let's remove items after a specific time.\n+This is a built-in feature in enterprise-grade caches such as Hazelcast\n+IMDG, and it is known as the Time-To-Live.\n+When an item is stored, a TTL can be attached to it.\n+After that time has elapsed, the item is removed from the cache, and it\n+will be fetched from the RDBMS again if needed.\n+\n+This approach has a couple of downsides:\n+\n+1. If an item is not updated in the RDBMS, but is evicted from the\n+cache, then there's an extra query from the cache to the RDBMS when it's\n+needed by the app.\n+This is a net loss of resources.\n+2. If an item is updated in the RDBMS, but its TTL has not been reached\n+yet, then the cache will return the state data.\n+This defeats the purpose.\n+\n+With longer TTL, we avoid unnecessary round trips but return more stale\n+data.\n+With shorter TTL, we waste resources with lesser chances of stale data.\n+\n+### Polling the RDBMS\n+\n+Because the TTL doesn't seem to be the right approach, we could devise a\n+dedicated component that watches the RDBMS by regularly sending queries\n+to it and updating the cache accordingly.\n+\n+Unfortunately, this strategy incurs the same issues as cache\n+invalidation:\n+the more frequent the queries, the more chances to catch changes, but\n+the more resources are wasted.\n+Worse, this also will put extra load on the RDBMS.\n+\n+### RDBMS triggers\n+\n+A common downside of the above approaches is the way they both poll the\n+database.\n+Polling happens with a specific frequency, while writes don't follow any\n+regular periodicity.\n+Thus, it's not possible to make the two match.\n+\n+Instead of polling, it would make much more sense to be event-driven:\n+\n+1. if no writes happen, there's no need to update the cache\n+2. if a write happens, then the relevant cache item should be updated\n+accordingly\n+\n+In RDBMS, this event-driven approach is implemented via _triggers_.\n+Triggers are dedicated stored procedures that are launched in response\n+to specific events, such as an `INSERT` or an `UPDATE`.\n+\n+That works pretty well when the acted-upon object is inside the database\n+_e.g._ \"when a record of table A is updated, then add a record to table\n+B\".\n+For our use-case, where the acted-upon object is the cache, which sits\n+outside the database, it's not as simple.\n+For example, MySQL allows you to [make an external system call from a\n+trigger](https://dev.mysql.com/doc/refman/8.0/en/faqs-triggers.html#faq-mysql-can-triggers-udf).\n+However, this approach is very implementation-dependent and makes the\n+overall design of the system much more fragile.\n+Also, only some RDBMS implement triggers. Even if they do, there's no\n+standard implementation.\n+\n+## Change Data Capture\n+\n+Wikipedia defines Change Data Capture (or CDC) as:\n+\n+> [...] a set of software design patterns used to determine and track\n+> the data that has changed so that action can be taken using the\n+> changed data.\n+>\n+> CDC is an approach to data integration that is based on the\n+> identification, capture and delivery of the changes made to enterprise\n+> data sources.\n+\n+In practice, CDC is a tool that allows to transform standard write\n+queries into events.\n+It implements it by \"turning the database inside-out\" (quote from Martin\n+Kleppmann).\n+This definition is because a database keeps a record of all changes in\n+an implementation-dependent append-only log.\n+Regularly, it uses it to manage its state. Some RDBMS also have other\n+usage _e.g._ MySQL uses the log for replication across nodes.\n+\n+For example, here's a sample for MySQL binlog:\n+\n+```text\n+### UPDATE `test`.`t`\n+### WHERE\n+###   @1=1 /* INT meta=0 nullable=0 is_null=0 */\n+###   @2='apple' /* VARSTRING(20) meta=20 nullable=0 is_null=0 */\n+###   @3=NULL /* VARSTRING(20) meta=0 nullable=1 is_null=1 */\n+### SET\n+###   @1=1 /* INT meta=0 nullable=0 is_null=0 */\n+###   @2='pear' /* VARSTRING(20) meta=20 nullable=0 is_null=0 */\n+###   @3='2009:01:01' /* DATE meta=0 nullable=1 is_null=0 */\n+# at 569\n+#150112 21:40:14 server id 1  end_log_pos 617 CRC32 0xf134ad89\n+#Table_map: `test`.`t` mapped to number 251\n+# at 617\n+#150112 21:40:14 server id 1  end_log_pos 665 CRC32 0x87047106\n+#Delete_rows: table id 251 flags: STMT_END_F\n+```\n+\n+A CDC component connects to this immutable log to extract change events.\n+\n+One can view CDC as the opposite of Event Sourcing:\n+the latter captures state by aggregating events, while the former\n+extracts events \"from the state\".\n+\n+## Debezium\n+\n+CDC is quite recent, and hadn't time to mature.\n+As such, there's no universal standard, but specific tools.\n+In this section, we are going to have a look at\n+[Debezium](https://debezium.io/).\n+Debezium is an Open Source set of services for CDC provided by Red Hat.\n+\n+Debezium is an umbrella term covering several components:\n+\n+1. Debezium Connectors are specific bridges that read the append-only\n+proprietary log for each supported database.\n+For example, there\u2019s a connector for MySQL, one for MongoDB, one for\n+PostgreSQL, etc.\n+2. Each connector is also a Kafka Connect Source Connector:\n+this allows to easily output CDC events to one\u2019s Kafka cluster\n+3. Finally, the Debezium Engine is a JAR that allows Debezium to be\n+embedded in one\u2019s applications.\n+Note that even in that case, Debezium produces Kafka Connect-specific\n+content, which then needs to be handled and transformed in one\u2019s\n+application.\n+\n+While Kafka is a great technology, and probably also quite widespread\n+nowadays, data in Kafka needs to be persisted to disk.\n+The benefit of persistence is that data survive even in the event of the\n+cluster going down.\n+The tradeoff, however, is that the access time of disk-persisted data is\n+one (or 2) orders of magnitude slower than the access time of in-memory\n+data, depending on the underlying disk technology.\n+\n+## Hazelcast Jet\n+\n+[Hazelcast Jet](https://jet-start.sh/) is a distributed stream\n+processing framework built on Hazelcast and combines a cache with\n+fault-tolerant data processing.\n+It has sources and sinks to integrate with several file, messaging and\n+database systems (such as Amazon S3, Kafka, message brokers and\n+relational databases).\n+\n+Jet also provides a Debezium module where it can process change events\n+directly from the database, and write them to its distributed key-value\n+store.\n+This avoids having to write the intermediate messages to Kafka and then\n+read again to be written to a separate cache.\n+\n+## Putting it all together\n+\n+It\u2019s (finally!) time to assemble all the previous bits together.\n+Here are the components and their responsibilities:\n+\n+1. A MySQL database instance is where the data is stored.\n+It\u2019s accessed in read-only mode by the cache, and in write-only mode by\n+some external component\n+2. A Jet instance reads events to MySQL through the Debezium connector,\n+transforms them into cache-compatible key-value pairs, and updates the\n+cache accordingly.\n+Note that while Jet pipelines provide filtering capabilities, it\u2019s also\n+possible to filter items in the CDC connector to optimize load of the\n+pipeline\n+3. The app uses the cache, which is always up-to-date with the database,\n+ give or take the time it takes for the above to execute\n+\n+![Final architecture with\n+CDC](assets/2020-05-20-architecture-with-cdc.svg)\n+\n+Note that this architecture assumes one starts from a legacy state, with\n+an existing app that uses caching, where later on a new component was\n+set up that could update the database.\n+\n+If one starts from scratch, it\u2019s possible to simplify the above diagram\n+(and associated code) as Jet embeds its own Hazelcast instance.\n+In that case, instead of Jet being a client of a third-party Hazelcast\n+instance, Jet is the one to configure and start the instance.\n+Obviously, it also can then get/put data.\n+\n+## Talk is cheap, show me the code\n+\n+Sources for this post are available [on\n+GitHub](https://github.com/hazelcast/evergreen-cache).\n+\n+The repository is made of the following modules:\n+\n+- `app` is a Spring Boot application using Spring Data JDBC to access a\n+MySQL database.\n+It abstracts away Hazelcast by using a Spring Cache layer\n+- `update` is a Spring Shell application.\n+It allows to update the data inside the database, with the cache none\n+the wiser\n+- `pipeline` is the Jet pipeline that listens to CDC events and update\n+the cache when data is updated\n+\n+The pipeline definition is quite straightforward:\n+\n+```java\n+pipeline.readFrom(DebeziumSources.cdc(configuration))            // 1\n+  .withoutTimestamps()\n+  .map(r -> Values.convertToString(r.valueSchema(), r.value()))  // 2\n+  .apply(new JsonToPerson())                                     // 3\n+  .writeTo(Sinks.remoteMap(                                      // 4\n+    \"entities\",                                                  // 5\n+    new CustomClientConfig(env.get(\"CACHE_HOST\"))                // 6\n+  ));\n+```\n+\n+1. Get a stream of Kafka `StageRecord`\n+2. Convert `StageRecord` to plain `String` that is formatted as JSON\n+3. Custom code converts the `String` to a regular `Person` POJO\n+4. Create the sink to write to, a remote map\n+5. Name of the remote map\n+6. Client configuration so it can connect to the right host, cluster\n+and instance\n+\n+```java\n+public class CustomClientConfig extends ClientConfig {\n+\n+  public CustomClientConfig(String cacheHost) {\n+    setClusterName(\"dev\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0205d0ebea5a33edc83a44831d63ee7cf1accf28"}, "originalPosition": 289}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3OTcxMjA1OnYy", "diffSide": "RIGHT", "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwNzo0MDozMlrOGaSOtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwNzo0MDozMlrOGaSOtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDIxNDgzOA==", "bodyText": "why you need to set the name ? it doesn't bring anything", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2264#discussion_r430214838", "createdAt": "2020-05-26T07:40:32Z", "author": {"login": "cangencer"}, "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "diffHunk": "@@ -0,0 +1,361 @@\n+---\n+title: Designing an evergreen cache with Change Data Capture\n+author: Nicolas Frankel\n+authorURL: https://twitter.com/nicolas_frankel\n+authorImageURL: https://3l0wd94f0qdd10om8642z9se-wpengine.netdna-ssl.com/wp-content/uploads/2019/12/nicolas-frankel-170x170.jpg\n+---\n+\n+It's been said that there are two things hard in software development,\n+naming things and cache invalidation (while some add off-by-one errors\n+to the mix).\n+I believe that keeping the cache in sync with the source of truth might\n+count as a third one.\n+In this post, I'd like to tackle this issue, describe the ideal\n+situation -\n+1 cache, 1 datastore, describe the problem of having multiple components\n+that can write to the datastore, list all possible solutions, and\n+describe one elegant solution based on Change Data Capture and Jet.\n+\n+## The ideal design\n+\n+In a system, to improve performance, one of the first short-term\n+measures is to set up a cache.\n+It's a tradeoff between getting the data faster at the cost of the data\n+being not that fresh: one loads the data in-memory close to the\n+consumer, and presto, one gets an instant performance boost. In regard\n+to a database, this is akin to the following:\n+\n+![Starting architecture](assets/2020-05-20-starting-architecture.svg)\n+\n+In this read-through design, when the app requires an item, the cache\n+first checks whether it has it.\n+If yes, it returns it.\n+If not, it fetches it from the underlying Relational Database Management\n+System, stores it, and returns it.\n+For a write, it stores it, and also calls the RDBMS to store it.\n+\n+Note that using a cache-aside design instead of read-through would have\n+the same issue.\n+The only difference would be the fact that the app would be responsible\n+for the fetching/storing logic instead of the cache.\n+\n+The RDBMS is the sole source of truth - as it should be. Since the cache\n+intercepts write statements to the RDBMS, it's a mirror of the data.\n+\n+## Handling third-party database updates\n+\n+This design works as expected until the database receives updates from\n+another source:\n+\n+![Updating the database while bypassing the\n+cache](assets/2020-05-20-updating-database-bypassing-cache.svg)\n+\n+Now, the RDBMS is still the source of truth, but the cache is not aware\n+of changes made by other components.\n+Hence, it might (will) return data that it has stored, but that is stale\n+compared to what is the source of truth in the RDBMS.\n+\n+There are multiple ways to cope with this issue.\n+\n+### Cache invalidation\n+\n+Since the cache only queries the RDBMS if it doesn't store the requested\n+item, let's remove items after a specific time.\n+This is a built-in feature in enterprise-grade caches such as Hazelcast\n+IMDG, and it is known as the Time-To-Live.\n+When an item is stored, a TTL can be attached to it.\n+After that time has elapsed, the item is removed from the cache, and it\n+will be fetched from the RDBMS again if needed.\n+\n+This approach has a couple of downsides:\n+\n+1. If an item is not updated in the RDBMS, but is evicted from the\n+cache, then there's an extra query from the cache to the RDBMS when it's\n+needed by the app.\n+This is a net loss of resources.\n+2. If an item is updated in the RDBMS, but its TTL has not been reached\n+yet, then the cache will return the state data.\n+This defeats the purpose.\n+\n+With longer TTL, we avoid unnecessary round trips but return more stale\n+data.\n+With shorter TTL, we waste resources with lesser chances of stale data.\n+\n+### Polling the RDBMS\n+\n+Because the TTL doesn't seem to be the right approach, we could devise a\n+dedicated component that watches the RDBMS by regularly sending queries\n+to it and updating the cache accordingly.\n+\n+Unfortunately, this strategy incurs the same issues as cache\n+invalidation:\n+the more frequent the queries, the more chances to catch changes, but\n+the more resources are wasted.\n+Worse, this also will put extra load on the RDBMS.\n+\n+### RDBMS triggers\n+\n+A common downside of the above approaches is the way they both poll the\n+database.\n+Polling happens with a specific frequency, while writes don't follow any\n+regular periodicity.\n+Thus, it's not possible to make the two match.\n+\n+Instead of polling, it would make much more sense to be event-driven:\n+\n+1. if no writes happen, there's no need to update the cache\n+2. if a write happens, then the relevant cache item should be updated\n+accordingly\n+\n+In RDBMS, this event-driven approach is implemented via _triggers_.\n+Triggers are dedicated stored procedures that are launched in response\n+to specific events, such as an `INSERT` or an `UPDATE`.\n+\n+That works pretty well when the acted-upon object is inside the database\n+_e.g._ \"when a record of table A is updated, then add a record to table\n+B\".\n+For our use-case, where the acted-upon object is the cache, which sits\n+outside the database, it's not as simple.\n+For example, MySQL allows you to [make an external system call from a\n+trigger](https://dev.mysql.com/doc/refman/8.0/en/faqs-triggers.html#faq-mysql-can-triggers-udf).\n+However, this approach is very implementation-dependent and makes the\n+overall design of the system much more fragile.\n+Also, only some RDBMS implement triggers. Even if they do, there's no\n+standard implementation.\n+\n+## Change Data Capture\n+\n+Wikipedia defines Change Data Capture (or CDC) as:\n+\n+> [...] a set of software design patterns used to determine and track\n+> the data that has changed so that action can be taken using the\n+> changed data.\n+>\n+> CDC is an approach to data integration that is based on the\n+> identification, capture and delivery of the changes made to enterprise\n+> data sources.\n+\n+In practice, CDC is a tool that allows to transform standard write\n+queries into events.\n+It implements it by \"turning the database inside-out\" (quote from Martin\n+Kleppmann).\n+This definition is because a database keeps a record of all changes in\n+an implementation-dependent append-only log.\n+Regularly, it uses it to manage its state. Some RDBMS also have other\n+usage _e.g._ MySQL uses the log for replication across nodes.\n+\n+For example, here's a sample for MySQL binlog:\n+\n+```text\n+### UPDATE `test`.`t`\n+### WHERE\n+###   @1=1 /* INT meta=0 nullable=0 is_null=0 */\n+###   @2='apple' /* VARSTRING(20) meta=20 nullable=0 is_null=0 */\n+###   @3=NULL /* VARSTRING(20) meta=0 nullable=1 is_null=1 */\n+### SET\n+###   @1=1 /* INT meta=0 nullable=0 is_null=0 */\n+###   @2='pear' /* VARSTRING(20) meta=20 nullable=0 is_null=0 */\n+###   @3='2009:01:01' /* DATE meta=0 nullable=1 is_null=0 */\n+# at 569\n+#150112 21:40:14 server id 1  end_log_pos 617 CRC32 0xf134ad89\n+#Table_map: `test`.`t` mapped to number 251\n+# at 617\n+#150112 21:40:14 server id 1  end_log_pos 665 CRC32 0x87047106\n+#Delete_rows: table id 251 flags: STMT_END_F\n+```\n+\n+A CDC component connects to this immutable log to extract change events.\n+\n+One can view CDC as the opposite of Event Sourcing:\n+the latter captures state by aggregating events, while the former\n+extracts events \"from the state\".\n+\n+## Debezium\n+\n+CDC is quite recent, and hadn't time to mature.\n+As such, there's no universal standard, but specific tools.\n+In this section, we are going to have a look at\n+[Debezium](https://debezium.io/).\n+Debezium is an Open Source set of services for CDC provided by Red Hat.\n+\n+Debezium is an umbrella term covering several components:\n+\n+1. Debezium Connectors are specific bridges that read the append-only\n+proprietary log for each supported database.\n+For example, there\u2019s a connector for MySQL, one for MongoDB, one for\n+PostgreSQL, etc.\n+2. Each connector is also a Kafka Connect Source Connector:\n+this allows to easily output CDC events to one\u2019s Kafka cluster\n+3. Finally, the Debezium Engine is a JAR that allows Debezium to be\n+embedded in one\u2019s applications.\n+Note that even in that case, Debezium produces Kafka Connect-specific\n+content, which then needs to be handled and transformed in one\u2019s\n+application.\n+\n+While Kafka is a great technology, and probably also quite widespread\n+nowadays, data in Kafka needs to be persisted to disk.\n+The benefit of persistence is that data survive even in the event of the\n+cluster going down.\n+The tradeoff, however, is that the access time of disk-persisted data is\n+one (or 2) orders of magnitude slower than the access time of in-memory\n+data, depending on the underlying disk technology.\n+\n+## Hazelcast Jet\n+\n+[Hazelcast Jet](https://jet-start.sh/) is a distributed stream\n+processing framework built on Hazelcast and combines a cache with\n+fault-tolerant data processing.\n+It has sources and sinks to integrate with several file, messaging and\n+database systems (such as Amazon S3, Kafka, message brokers and\n+relational databases).\n+\n+Jet also provides a Debezium module where it can process change events\n+directly from the database, and write them to its distributed key-value\n+store.\n+This avoids having to write the intermediate messages to Kafka and then\n+read again to be written to a separate cache.\n+\n+## Putting it all together\n+\n+It\u2019s (finally!) time to assemble all the previous bits together.\n+Here are the components and their responsibilities:\n+\n+1. A MySQL database instance is where the data is stored.\n+It\u2019s accessed in read-only mode by the cache, and in write-only mode by\n+some external component\n+2. A Jet instance reads events to MySQL through the Debezium connector,\n+transforms them into cache-compatible key-value pairs, and updates the\n+cache accordingly.\n+Note that while Jet pipelines provide filtering capabilities, it\u2019s also\n+possible to filter items in the CDC connector to optimize load of the\n+pipeline\n+3. The app uses the cache, which is always up-to-date with the database,\n+ give or take the time it takes for the above to execute\n+\n+![Final architecture with\n+CDC](assets/2020-05-20-architecture-with-cdc.svg)\n+\n+Note that this architecture assumes one starts from a legacy state, with\n+an existing app that uses caching, where later on a new component was\n+set up that could update the database.\n+\n+If one starts from scratch, it\u2019s possible to simplify the above diagram\n+(and associated code) as Jet embeds its own Hazelcast instance.\n+In that case, instead of Jet being a client of a third-party Hazelcast\n+instance, Jet is the one to configure and start the instance.\n+Obviously, it also can then get/put data.\n+\n+## Talk is cheap, show me the code\n+\n+Sources for this post are available [on\n+GitHub](https://github.com/hazelcast/evergreen-cache).\n+\n+The repository is made of the following modules:\n+\n+- `app` is a Spring Boot application using Spring Data JDBC to access a\n+MySQL database.\n+It abstracts away Hazelcast by using a Spring Cache layer\n+- `update` is a Spring Shell application.\n+It allows to update the data inside the database, with the cache none\n+the wiser\n+- `pipeline` is the Jet pipeline that listens to CDC events and update\n+the cache when data is updated\n+\n+The pipeline definition is quite straightforward:\n+\n+```java\n+pipeline.readFrom(DebeziumSources.cdc(configuration))            // 1\n+  .withoutTimestamps()\n+  .map(r -> Values.convertToString(r.valueSchema(), r.value()))  // 2\n+  .apply(new JsonToPerson())                                     // 3\n+  .writeTo(Sinks.remoteMap(                                      // 4\n+    \"entities\",                                                  // 5\n+    new CustomClientConfig(env.get(\"CACHE_HOST\"))                // 6\n+  ));\n+```\n+\n+1. Get a stream of Kafka `StageRecord`\n+2. Convert `StageRecord` to plain `String` that is formatted as JSON\n+3. Custom code converts the `String` to a regular `Person` POJO\n+4. Create the sink to write to, a remote map\n+5. Name of the remote map\n+6. Client configuration so it can connect to the right host, cluster\n+and instance\n+\n+```java\n+public class CustomClientConfig extends ClientConfig {\n+\n+  public CustomClientConfig(String cacheHost) {\n+    setClusterName(\"dev\");\n+    setInstanceName(\"hazelcastInstance\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0205d0ebea5a33edc83a44831d63ee7cf1accf28"}, "originalPosition": 290}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3OTcxMzE3OnYy", "diffSide": "RIGHT", "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwNzo0MDo0OVrOGaSPYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yNlQwNzo0MDo0OVrOGaSPYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDIxNTAwOA==", "bodyText": "addAddresses() is simpler", "url": "https://github.com/hazelcast/hazelcast-jet/pull/2264#discussion_r430215008", "createdAt": "2020-05-26T07:40:49Z", "author": {"login": "cangencer"}, "path": "site/website/blog/2020-05-20-designing-evergreen-cache-cdc.md", "diffHunk": "@@ -0,0 +1,361 @@\n+---\n+title: Designing an evergreen cache with Change Data Capture\n+author: Nicolas Frankel\n+authorURL: https://twitter.com/nicolas_frankel\n+authorImageURL: https://3l0wd94f0qdd10om8642z9se-wpengine.netdna-ssl.com/wp-content/uploads/2019/12/nicolas-frankel-170x170.jpg\n+---\n+\n+It's been said that there are two things hard in software development,\n+naming things and cache invalidation (while some add off-by-one errors\n+to the mix).\n+I believe that keeping the cache in sync with the source of truth might\n+count as a third one.\n+In this post, I'd like to tackle this issue, describe the ideal\n+situation -\n+1 cache, 1 datastore, describe the problem of having multiple components\n+that can write to the datastore, list all possible solutions, and\n+describe one elegant solution based on Change Data Capture and Jet.\n+\n+## The ideal design\n+\n+In a system, to improve performance, one of the first short-term\n+measures is to set up a cache.\n+It's a tradeoff between getting the data faster at the cost of the data\n+being not that fresh: one loads the data in-memory close to the\n+consumer, and presto, one gets an instant performance boost. In regard\n+to a database, this is akin to the following:\n+\n+![Starting architecture](assets/2020-05-20-starting-architecture.svg)\n+\n+In this read-through design, when the app requires an item, the cache\n+first checks whether it has it.\n+If yes, it returns it.\n+If not, it fetches it from the underlying Relational Database Management\n+System, stores it, and returns it.\n+For a write, it stores it, and also calls the RDBMS to store it.\n+\n+Note that using a cache-aside design instead of read-through would have\n+the same issue.\n+The only difference would be the fact that the app would be responsible\n+for the fetching/storing logic instead of the cache.\n+\n+The RDBMS is the sole source of truth - as it should be. Since the cache\n+intercepts write statements to the RDBMS, it's a mirror of the data.\n+\n+## Handling third-party database updates\n+\n+This design works as expected until the database receives updates from\n+another source:\n+\n+![Updating the database while bypassing the\n+cache](assets/2020-05-20-updating-database-bypassing-cache.svg)\n+\n+Now, the RDBMS is still the source of truth, but the cache is not aware\n+of changes made by other components.\n+Hence, it might (will) return data that it has stored, but that is stale\n+compared to what is the source of truth in the RDBMS.\n+\n+There are multiple ways to cope with this issue.\n+\n+### Cache invalidation\n+\n+Since the cache only queries the RDBMS if it doesn't store the requested\n+item, let's remove items after a specific time.\n+This is a built-in feature in enterprise-grade caches such as Hazelcast\n+IMDG, and it is known as the Time-To-Live.\n+When an item is stored, a TTL can be attached to it.\n+After that time has elapsed, the item is removed from the cache, and it\n+will be fetched from the RDBMS again if needed.\n+\n+This approach has a couple of downsides:\n+\n+1. If an item is not updated in the RDBMS, but is evicted from the\n+cache, then there's an extra query from the cache to the RDBMS when it's\n+needed by the app.\n+This is a net loss of resources.\n+2. If an item is updated in the RDBMS, but its TTL has not been reached\n+yet, then the cache will return the state data.\n+This defeats the purpose.\n+\n+With longer TTL, we avoid unnecessary round trips but return more stale\n+data.\n+With shorter TTL, we waste resources with lesser chances of stale data.\n+\n+### Polling the RDBMS\n+\n+Because the TTL doesn't seem to be the right approach, we could devise a\n+dedicated component that watches the RDBMS by regularly sending queries\n+to it and updating the cache accordingly.\n+\n+Unfortunately, this strategy incurs the same issues as cache\n+invalidation:\n+the more frequent the queries, the more chances to catch changes, but\n+the more resources are wasted.\n+Worse, this also will put extra load on the RDBMS.\n+\n+### RDBMS triggers\n+\n+A common downside of the above approaches is the way they both poll the\n+database.\n+Polling happens with a specific frequency, while writes don't follow any\n+regular periodicity.\n+Thus, it's not possible to make the two match.\n+\n+Instead of polling, it would make much more sense to be event-driven:\n+\n+1. if no writes happen, there's no need to update the cache\n+2. if a write happens, then the relevant cache item should be updated\n+accordingly\n+\n+In RDBMS, this event-driven approach is implemented via _triggers_.\n+Triggers are dedicated stored procedures that are launched in response\n+to specific events, such as an `INSERT` or an `UPDATE`.\n+\n+That works pretty well when the acted-upon object is inside the database\n+_e.g._ \"when a record of table A is updated, then add a record to table\n+B\".\n+For our use-case, where the acted-upon object is the cache, which sits\n+outside the database, it's not as simple.\n+For example, MySQL allows you to [make an external system call from a\n+trigger](https://dev.mysql.com/doc/refman/8.0/en/faqs-triggers.html#faq-mysql-can-triggers-udf).\n+However, this approach is very implementation-dependent and makes the\n+overall design of the system much more fragile.\n+Also, only some RDBMS implement triggers. Even if they do, there's no\n+standard implementation.\n+\n+## Change Data Capture\n+\n+Wikipedia defines Change Data Capture (or CDC) as:\n+\n+> [...] a set of software design patterns used to determine and track\n+> the data that has changed so that action can be taken using the\n+> changed data.\n+>\n+> CDC is an approach to data integration that is based on the\n+> identification, capture and delivery of the changes made to enterprise\n+> data sources.\n+\n+In practice, CDC is a tool that allows to transform standard write\n+queries into events.\n+It implements it by \"turning the database inside-out\" (quote from Martin\n+Kleppmann).\n+This definition is because a database keeps a record of all changes in\n+an implementation-dependent append-only log.\n+Regularly, it uses it to manage its state. Some RDBMS also have other\n+usage _e.g._ MySQL uses the log for replication across nodes.\n+\n+For example, here's a sample for MySQL binlog:\n+\n+```text\n+### UPDATE `test`.`t`\n+### WHERE\n+###   @1=1 /* INT meta=0 nullable=0 is_null=0 */\n+###   @2='apple' /* VARSTRING(20) meta=20 nullable=0 is_null=0 */\n+###   @3=NULL /* VARSTRING(20) meta=0 nullable=1 is_null=1 */\n+### SET\n+###   @1=1 /* INT meta=0 nullable=0 is_null=0 */\n+###   @2='pear' /* VARSTRING(20) meta=20 nullable=0 is_null=0 */\n+###   @3='2009:01:01' /* DATE meta=0 nullable=1 is_null=0 */\n+# at 569\n+#150112 21:40:14 server id 1  end_log_pos 617 CRC32 0xf134ad89\n+#Table_map: `test`.`t` mapped to number 251\n+# at 617\n+#150112 21:40:14 server id 1  end_log_pos 665 CRC32 0x87047106\n+#Delete_rows: table id 251 flags: STMT_END_F\n+```\n+\n+A CDC component connects to this immutable log to extract change events.\n+\n+One can view CDC as the opposite of Event Sourcing:\n+the latter captures state by aggregating events, while the former\n+extracts events \"from the state\".\n+\n+## Debezium\n+\n+CDC is quite recent, and hadn't time to mature.\n+As such, there's no universal standard, but specific tools.\n+In this section, we are going to have a look at\n+[Debezium](https://debezium.io/).\n+Debezium is an Open Source set of services for CDC provided by Red Hat.\n+\n+Debezium is an umbrella term covering several components:\n+\n+1. Debezium Connectors are specific bridges that read the append-only\n+proprietary log for each supported database.\n+For example, there\u2019s a connector for MySQL, one for MongoDB, one for\n+PostgreSQL, etc.\n+2. Each connector is also a Kafka Connect Source Connector:\n+this allows to easily output CDC events to one\u2019s Kafka cluster\n+3. Finally, the Debezium Engine is a JAR that allows Debezium to be\n+embedded in one\u2019s applications.\n+Note that even in that case, Debezium produces Kafka Connect-specific\n+content, which then needs to be handled and transformed in one\u2019s\n+application.\n+\n+While Kafka is a great technology, and probably also quite widespread\n+nowadays, data in Kafka needs to be persisted to disk.\n+The benefit of persistence is that data survive even in the event of the\n+cluster going down.\n+The tradeoff, however, is that the access time of disk-persisted data is\n+one (or 2) orders of magnitude slower than the access time of in-memory\n+data, depending on the underlying disk technology.\n+\n+## Hazelcast Jet\n+\n+[Hazelcast Jet](https://jet-start.sh/) is a distributed stream\n+processing framework built on Hazelcast and combines a cache with\n+fault-tolerant data processing.\n+It has sources and sinks to integrate with several file, messaging and\n+database systems (such as Amazon S3, Kafka, message brokers and\n+relational databases).\n+\n+Jet also provides a Debezium module where it can process change events\n+directly from the database, and write them to its distributed key-value\n+store.\n+This avoids having to write the intermediate messages to Kafka and then\n+read again to be written to a separate cache.\n+\n+## Putting it all together\n+\n+It\u2019s (finally!) time to assemble all the previous bits together.\n+Here are the components and their responsibilities:\n+\n+1. A MySQL database instance is where the data is stored.\n+It\u2019s accessed in read-only mode by the cache, and in write-only mode by\n+some external component\n+2. A Jet instance reads events to MySQL through the Debezium connector,\n+transforms them into cache-compatible key-value pairs, and updates the\n+cache accordingly.\n+Note that while Jet pipelines provide filtering capabilities, it\u2019s also\n+possible to filter items in the CDC connector to optimize load of the\n+pipeline\n+3. The app uses the cache, which is always up-to-date with the database,\n+ give or take the time it takes for the above to execute\n+\n+![Final architecture with\n+CDC](assets/2020-05-20-architecture-with-cdc.svg)\n+\n+Note that this architecture assumes one starts from a legacy state, with\n+an existing app that uses caching, where later on a new component was\n+set up that could update the database.\n+\n+If one starts from scratch, it\u2019s possible to simplify the above diagram\n+(and associated code) as Jet embeds its own Hazelcast instance.\n+In that case, instead of Jet being a client of a third-party Hazelcast\n+instance, Jet is the one to configure and start the instance.\n+Obviously, it also can then get/put data.\n+\n+## Talk is cheap, show me the code\n+\n+Sources for this post are available [on\n+GitHub](https://github.com/hazelcast/evergreen-cache).\n+\n+The repository is made of the following modules:\n+\n+- `app` is a Spring Boot application using Spring Data JDBC to access a\n+MySQL database.\n+It abstracts away Hazelcast by using a Spring Cache layer\n+- `update` is a Spring Shell application.\n+It allows to update the data inside the database, with the cache none\n+the wiser\n+- `pipeline` is the Jet pipeline that listens to CDC events and update\n+the cache when data is updated\n+\n+The pipeline definition is quite straightforward:\n+\n+```java\n+pipeline.readFrom(DebeziumSources.cdc(configuration))            // 1\n+  .withoutTimestamps()\n+  .map(r -> Values.convertToString(r.valueSchema(), r.value()))  // 2\n+  .apply(new JsonToPerson())                                     // 3\n+  .writeTo(Sinks.remoteMap(                                      // 4\n+    \"entities\",                                                  // 5\n+    new CustomClientConfig(env.get(\"CACHE_HOST\"))                // 6\n+  ));\n+```\n+\n+1. Get a stream of Kafka `StageRecord`\n+2. Convert `StageRecord` to plain `String` that is formatted as JSON\n+3. Custom code converts the `String` to a regular `Person` POJO\n+4. Create the sink to write to, a remote map\n+5. Name of the remote map\n+6. Client configuration so it can connect to the right host, cluster\n+and instance\n+\n+```java\n+public class CustomClientConfig extends ClientConfig {\n+\n+  public CustomClientConfig(String cacheHost) {\n+    setClusterName(\"dev\");\n+    setInstanceName(\"hazelcastInstance\");\n+    getNetworkConfig().setAddresses(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0205d0ebea5a33edc83a44831d63ee7cf1accf28"}, "originalPosition": 291}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4726, "cost": 1, "resetAt": "2021-11-12T20:44:06Z"}}}