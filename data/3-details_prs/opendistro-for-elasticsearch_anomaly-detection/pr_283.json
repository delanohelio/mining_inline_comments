{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA2OTY0MDk5", "number": 283, "title": "Add checkpoint index retention for multi entity detector", "bodyText": "Issue #, if available:\nDescription of changes:\nAdd checkpoint index retention logic for multi entity detectors.\nTesting:\n\nUT - Done\nFunctional testing in test domain - Done\n\nBy submitting this pull request, I confirm that you can use, modify, copy, and redistribute this contribution, under the terms of your choice.", "createdAt": "2020-10-20T17:06:05Z", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283", "merged": true, "mergeCommit": {"oid": "f0105ccfe6526bf045edb7f0b5ead2c13c166f4a"}, "closed": true, "closedAt": "2020-11-06T22:46:40Z", "author": {"login": "weicongs-amazon"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdUb3I3gH2gAyNTA2OTY0MDk5OjJmN2YzMzk3MDE3NzM5NGNhMjYxMTdkZTU0ZmZiY2Q4YTU0YTRhNDk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdZ-76nAFqTUyNTU0MDExMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "2f7f33970177394ca26117de54ffbcd8a54a4a49", "author": {"user": {"login": "weicongs-amazon", "name": "Weicong Sun"}}, "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/2f7f33970177394ca26117de54ffbcd8a54a4a49", "committedDate": "2020-10-20T17:03:23Z", "message": "Add checkpoint index retention for multi entity detector"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2773a92369f2f7cdb703efd5bd1d81afd0645438", "author": {"user": {"login": "weicongs-amazon", "name": "Weicong Sun"}}, "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/2773a92369f2f7cdb703efd5bd1d81afd0645438", "committedDate": "2020-10-20T17:31:05Z", "message": "run splotlessJavaApply to fix style issues"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a819d8d062a87fa897fcc819cf7360c8feb20760", "author": {"user": {"login": "weicongs-amazon", "name": "Weicong Sun"}}, "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/a819d8d062a87fa897fcc819cf7360c8feb20760", "committedDate": "2020-10-21T02:45:53Z", "message": "set refresh as true"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE0MjUxNTU4", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#pullrequestreview-514251558", "createdAt": "2020-10-21T23:07:49Z", "commit": {"oid": "a819d8d062a87fa897fcc819cf7360c8feb20760"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1MzcxNTI2", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#pullrequestreview-525371526", "createdAt": "2020-11-06T17:53:05Z", "commit": {"oid": "a819d8d062a87fa897fcc819cf7360c8feb20760"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNzo1MzowNVrOHu30dQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNzo1MzowNVrOHu30dQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkxMTA5Mw==", "bodyText": "Will this work with FGAC? We have moved AD indices to system indices list. May not be able to delete directly.\nRefer to AnomalyResultTransportAction.java#L218", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#discussion_r518911093", "createdAt": "2020-11-06T17:53:05Z", "author": {"login": "ylwu-amzn"}, "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/IndexCleanup.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.cluster.diskcleanup;\n+\n+import java.util.Arrays;\n+import java.util.Objects;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.action.admin.indices.stats.CommonStats;\n+import org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest;\n+import org.elasticsearch.action.admin.indices.stats.IndicesStatsResponse;\n+import org.elasticsearch.action.admin.indices.stats.ShardStats;\n+import org.elasticsearch.action.support.IndicesOptions;\n+import org.elasticsearch.client.Client;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.index.query.QueryBuilder;\n+import org.elasticsearch.index.reindex.DeleteByQueryAction;\n+import org.elasticsearch.index.reindex.DeleteByQueryRequest;\n+import org.elasticsearch.index.store.StoreStats;\n+\n+import com.amazon.opendistroforelasticsearch.ad.util.ClientUtil;\n+\n+/**\n+ * Clean up the old docs for indices.\n+ */\n+public class IndexCleanup {\n+    private static final Logger LOG = LogManager.getLogger(IndexCleanup.class);\n+\n+    private final Client client;\n+    private final ClientUtil clientUtil;\n+    private final ClusterService clusterService;\n+\n+    public IndexCleanup(Client client, ClientUtil clientUtil, ClusterService clusterService) {\n+        this.client = client;\n+        this.clientUtil = clientUtil;\n+        this.clusterService = clusterService;\n+    }\n+\n+    /**\n+     * delete docs when shard size is bigger than max limitation.\n+     * @param indexName index name\n+     * @param maxShardSize max shard size\n+     * @param queryForDeleteByQueryRequest query request\n+     * @param listener action listener\n+     */\n+    public void deleteDocsBasedOnShardSize(\n+        String indexName,\n+        long maxShardSize,\n+        QueryBuilder queryForDeleteByQueryRequest,\n+        ActionListener<Boolean> listener\n+    ) {\n+\n+        if (!clusterService.state().getRoutingTable().hasIndex(indexName)) {\n+            LOG.debug(\"skip as the index:{} doesn't exist\", indexName);\n+            return;\n+        }\n+\n+        ActionListener<IndicesStatsResponse> indicesStatsResponseListener = ActionListener.wrap(indicesStatsResponse -> {\n+            // Check if any shard size is bigger than maxShardSize\n+            boolean cleanupNeeded = Arrays\n+                .stream(indicesStatsResponse.getShards())\n+                .map(ShardStats::getStats)\n+                .filter(Objects::nonNull)\n+                .map(CommonStats::getStore)\n+                .filter(Objects::nonNull)\n+                .map(StoreStats::getSizeInBytes)\n+                .anyMatch(size -> size > maxShardSize);\n+\n+            if (cleanupNeeded) {\n+                deleteDocsByQuery(\n+                    indexName,\n+                    queryForDeleteByQueryRequest,\n+                    ActionListener.wrap(r -> listener.onResponse(true), listener::onFailure)\n+                );\n+            } else {\n+                listener.onResponse(false);\n+            }\n+        }, listener::onFailure);\n+\n+        getCheckpointShardStoreStats(indexName, indicesStatsResponseListener);\n+    }\n+\n+    private void getCheckpointShardStoreStats(String indexName, ActionListener<IndicesStatsResponse> listener) {\n+        IndicesStatsRequest indicesStatsRequest = new IndicesStatsRequest();\n+        indicesStatsRequest.store();\n+        indicesStatsRequest.indices(indexName);\n+        client.admin().indices().stats(indicesStatsRequest, listener);\n+    }\n+\n+    /**\n+     * Delete docs based on query request\n+     * @param indexName index name\n+     * @param queryForDeleteByQueryRequest query request\n+     * @param listener action listener\n+     */\n+    public void deleteDocsByQuery(String indexName, QueryBuilder queryForDeleteByQueryRequest, ActionListener<Long> listener) {\n+        DeleteByQueryRequest deleteRequest = new DeleteByQueryRequest(indexName)\n+            .setQuery(queryForDeleteByQueryRequest)\n+            .setIndicesOptions(IndicesOptions.LENIENT_EXPAND_OPEN)\n+            .setRefresh(true);\n+        clientUtil.execute(DeleteByQueryAction.INSTANCE, deleteRequest, ActionListener.wrap(response -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a819d8d062a87fa897fcc819cf7360c8feb20760"}, "originalPosition": 116}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1MzkxODQw", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#pullrequestreview-525391840", "createdAt": "2020-11-06T18:23:07Z", "commit": {"oid": "a819d8d062a87fa897fcc819cf7360c8feb20760"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxODoyMzowOFrOHu4xwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxODoyMzowOFrOHu4xwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkyNjc4NA==", "bodyText": "Why need to minus 1 day for next cleanup TTL? If the defaultCheckpointTtl is 5 days at first, the TTL will become shorter and shorter until reach 1 day ?", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#discussion_r518926784", "createdAt": "2020-11-06T18:23:08Z", "author": {"login": "ylwu-amzn"}, "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/ModelCheckpointIndexRetention.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.cluster.diskcleanup;\n+\n+import java.time.Clock;\n+import java.time.Duration;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.index.query.QueryBuilders;\n+\n+import com.amazon.opendistroforelasticsearch.ad.constant.CommonName;\n+import com.amazon.opendistroforelasticsearch.ad.ml.CheckpointDao;\n+\n+/**\n+ * Model checkpoints cleanup of multi-entity detectors.\n+ * <p> <b>Problem:</b>\n+ *     In multi-entity detectors, we can have thousands, even millions of entities, of which the model checkpoints will consume\n+ *     lots of disk resources. To protect the our disk usage, the checkpoint index size will be limited with specified threshold.\n+ *     Once its size exceeds the threshold, the model checkpoints cleanup process will be activated.\n+ * </p>\n+ * <p> <b>Solution:</b>\n+ *     Before multi-entity detectors, there is daily cron job to clean up the inactive checkpoints longer than some configurable days.\n+ *     We will keep the this logic, and add new clean up way based on shard size.\n+ * </p>\n+ */\n+public class ModelCheckpointIndexRetention implements Runnable {\n+    private static final Logger LOG = LogManager.getLogger(ModelCheckpointIndexRetention.class);\n+\n+    // The recommended max shard size is 50G, we don't wanna our index exceeds this number\n+    private static final long MAX_SHARD_SIZE_IN_BYTE = 50 * 1024 * 1024 * 1024L;\n+    // We can't clean up all of the checkpoints. At least keep models for 1 day\n+    private static final Duration MINIMUM_CHECKPOINT_TTL = Duration.ofDays(1);\n+\n+    private final Duration defaultCheckpointTtl;\n+    private final Clock clock;\n+    private final IndexCleanup indexCleanup;\n+\n+    public ModelCheckpointIndexRetention(Duration defaultCheckpointTtl, Clock clock, IndexCleanup indexCleanup) {\n+        this.defaultCheckpointTtl = defaultCheckpointTtl;\n+        this.clock = clock;\n+        this.indexCleanup = indexCleanup;\n+    }\n+\n+    @Override\n+    public void run() {\n+        indexCleanup\n+            .deleteDocsByQuery(\n+                CommonName.CHECKPOINT_INDEX_NAME,\n+                QueryBuilders\n+                    .boolQuery()\n+                    .filter(\n+                        QueryBuilders\n+                            .rangeQuery(CheckpointDao.TIMESTAMP)\n+                            .lte(clock.millis() - defaultCheckpointTtl.toMillis())\n+                            .format(CommonName.EPOCH_MILLIS_FORMAT)\n+                    ),\n+                ActionListener\n+                    .wrap(\n+                        response -> { cleanupBasedOnShardSize(defaultCheckpointTtl.minusDays(1)); },\n+                        exception -> LOG.error(\"delete docs by query fails for checkpoint index\", exception)\n+                    )\n+            );\n+\n+    }\n+\n+    private void cleanupBasedOnShardSize(Duration cleanUpTtl) {\n+        indexCleanup\n+            .deleteDocsBasedOnShardSize(\n+                CommonName.CHECKPOINT_INDEX_NAME,\n+                MAX_SHARD_SIZE_IN_BYTE,\n+                QueryBuilders\n+                    .boolQuery()\n+                    .filter(\n+                        QueryBuilders\n+                            .rangeQuery(CheckpointDao.TIMESTAMP)\n+                            .lte(clock.millis() - cleanUpTtl.toMillis())\n+                            .format(CommonName.EPOCH_MILLIS_FORMAT)\n+                    ),\n+                ActionListener.wrap(cleanupNeeded -> {\n+                    if (cleanupNeeded) {\n+                        if (cleanUpTtl.equals(MINIMUM_CHECKPOINT_TTL)) {\n+                            return;\n+                        }\n+\n+                        Duration nextCleanupTtl = cleanUpTtl.minusDays(1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a819d8d062a87fa897fcc819cf7360c8feb20760"}, "originalPosition": 100}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1MzkzNTY0", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#pullrequestreview-525393564", "createdAt": "2020-11-06T18:25:52Z", "commit": {"oid": "a819d8d062a87fa897fcc819cf7360c8feb20760"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxODoyNTo1MlrOHu43VA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxODoyNTo1MlrOHu43VA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkyODIxMg==", "bodyText": "If any exception happens, will stop clean up cron job? It's possible that high load causes failure and next run may succeed.\nSame question for line 108", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#discussion_r518928212", "createdAt": "2020-11-06T18:25:52Z", "author": {"login": "ylwu-amzn"}, "path": "src/main/java/com/amazon/opendistroforelasticsearch/ad/cluster/diskcleanup/ModelCheckpointIndexRetention.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\").\n+ * You may not use this file except in compliance with the License.\n+ * A copy of the License is located at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * or in the \"license\" file accompanying this file. This file is distributed\n+ * on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n+ * express or implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ */\n+\n+package com.amazon.opendistroforelasticsearch.ad.cluster.diskcleanup;\n+\n+import java.time.Clock;\n+import java.time.Duration;\n+\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.elasticsearch.action.ActionListener;\n+import org.elasticsearch.index.query.QueryBuilders;\n+\n+import com.amazon.opendistroforelasticsearch.ad.constant.CommonName;\n+import com.amazon.opendistroforelasticsearch.ad.ml.CheckpointDao;\n+\n+/**\n+ * Model checkpoints cleanup of multi-entity detectors.\n+ * <p> <b>Problem:</b>\n+ *     In multi-entity detectors, we can have thousands, even millions of entities, of which the model checkpoints will consume\n+ *     lots of disk resources. To protect the our disk usage, the checkpoint index size will be limited with specified threshold.\n+ *     Once its size exceeds the threshold, the model checkpoints cleanup process will be activated.\n+ * </p>\n+ * <p> <b>Solution:</b>\n+ *     Before multi-entity detectors, there is daily cron job to clean up the inactive checkpoints longer than some configurable days.\n+ *     We will keep the this logic, and add new clean up way based on shard size.\n+ * </p>\n+ */\n+public class ModelCheckpointIndexRetention implements Runnable {\n+    private static final Logger LOG = LogManager.getLogger(ModelCheckpointIndexRetention.class);\n+\n+    // The recommended max shard size is 50G, we don't wanna our index exceeds this number\n+    private static final long MAX_SHARD_SIZE_IN_BYTE = 50 * 1024 * 1024 * 1024L;\n+    // We can't clean up all of the checkpoints. At least keep models for 1 day\n+    private static final Duration MINIMUM_CHECKPOINT_TTL = Duration.ofDays(1);\n+\n+    private final Duration defaultCheckpointTtl;\n+    private final Clock clock;\n+    private final IndexCleanup indexCleanup;\n+\n+    public ModelCheckpointIndexRetention(Duration defaultCheckpointTtl, Clock clock, IndexCleanup indexCleanup) {\n+        this.defaultCheckpointTtl = defaultCheckpointTtl;\n+        this.clock = clock;\n+        this.indexCleanup = indexCleanup;\n+    }\n+\n+    @Override\n+    public void run() {\n+        indexCleanup\n+            .deleteDocsByQuery(\n+                CommonName.CHECKPOINT_INDEX_NAME,\n+                QueryBuilders\n+                    .boolQuery()\n+                    .filter(\n+                        QueryBuilders\n+                            .rangeQuery(CheckpointDao.TIMESTAMP)\n+                            .lte(clock.millis() - defaultCheckpointTtl.toMillis())\n+                            .format(CommonName.EPOCH_MILLIS_FORMAT)\n+                    ),\n+                ActionListener\n+                    .wrap(\n+                        response -> { cleanupBasedOnShardSize(defaultCheckpointTtl.minusDays(1)); },\n+                        exception -> LOG.error(\"delete docs by query fails for checkpoint index\", exception)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a819d8d062a87fa897fcc819cf7360c8feb20760"}, "originalPosition": 75}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c9e13f26f1473181d1c2a85c14e2eef0b5af40bc", "author": {"user": {"login": "weicongs-amazon", "name": "Weicong Sun"}}, "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/c9e13f26f1473181d1c2a85c14e2eef0b5af40bc", "committedDate": "2020-11-06T21:22:54Z", "message": "stash context for index clean up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "facd5c5593e3ff3bfb86ef9bb5c66ae98560d77e", "author": {"user": {"login": "weicongs-amazon", "name": "Weicong Sun"}}, "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/facd5c5593e3ff3bfb86ef9bb5c66ae98560d77e", "committedDate": "2020-11-06T21:28:30Z", "message": "remove useless import to fix checkstyle error"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4dbb6523b1cbad6511267c98cb8eaa5473ca00f4", "author": {"user": {"login": "weicongs-amazon", "name": "Weicong Sun"}}, "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/4dbb6523b1cbad6511267c98cb8eaa5473ca00f4", "committedDate": "2020-11-06T22:22:25Z", "message": "Add comments for error handling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7c24c8bcedb4654e97617a05f3bba448f793b362", "author": {"user": {"login": "weicongs-amazon", "name": "Weicong Sun"}}, "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/commit/7c24c8bcedb4654e97617a05f3bba448f793b362", "committedDate": "2020-11-06T22:29:20Z", "message": "Fix format error"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI1NTQwMTEx", "url": "https://github.com/opendistro-for-elasticsearch/anomaly-detection/pull/283#pullrequestreview-525540111", "createdAt": "2020-11-06T22:44:54Z", "commit": {"oid": "7c24c8bcedb4654e97617a05f3bba448f793b362"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1392, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}