{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMyMTg5ODk1", "number": 1255, "title": "[CDAP-17248] Added support for decoding files encoded in fixed-length charsets when reading from a file source.", "bodyText": "As it stands, UTF-32, ISO-8859-1 and Windows-1252 formats are supported.\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.", "createdAt": "2020-12-03T23:44:57Z", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255", "merged": true, "mergeCommit": {"oid": "e8330cf9e3b68587aeca244298fbf48fcc1ef67e"}, "closed": true, "closedAt": "2020-12-28T22:42:22Z", "author": {"login": "fernst"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdir_LRgFqTU0NDU1Mzc3NQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdpK9ioABqjQxNDYyMzYwNzI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NTUzNzc1", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#pullrequestreview-544553775", "createdAt": "2020-12-03T23:45:34Z", "commit": {"oid": "e11babb0b1c4c9cff529037debb1fae3169d6c3b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMzo0NTozNFrOH-6T2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMzo0NTozNFrOH-6T2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTcyOTExNQ==", "bodyText": "Ignore these log statements for now. I've been using them for debugging.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535729115", "createdAt": "2020-12-03T23:45:34Z", "author": {"login": "fernst"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingCodec.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.DefaultCodec;\n+import org.apache.hadoop.io.compress.DirectDecompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * Codec implementation that returns a decompressor for Fixed Length character encodings.\n+ */\n+public class FixedLengthCharsetTransformingCodec extends DefaultCodec\n+  implements Configurable, SplittableCompressionCodec {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingCodec.class);\n+\n+  FixedLengthCharset fixedLengthCharset;\n+\n+  public FixedLengthCharsetTransformingCodec(FixedLengthCharset fixedLengthCharset) {\n+    this.fixedLengthCharset = fixedLengthCharset;\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out) throws IOException {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor) throws IOException {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public Class<? extends Compressor> getCompressorType() {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public Compressor createCompressor() {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public CompressionInputStream createInputStream(InputStream in) throws IOException {\n+    return super.createInputStream(in, new FixedLengthCharsetTransformingDecompressor(this.fixedLengthCharset));\n+  }\n+\n+  @Override\n+  public CompressionInputStream createInputStream(InputStream in, Decompressor decompressor) throws IOException {\n+    return super.createInputStream(in, decompressor);\n+  }\n+\n+  @Override\n+  public Class<? extends Decompressor> getDecompressorType() {\n+    return FixedLengthCharsetTransformingDecompressor.class;\n+  }\n+\n+  @Override\n+  public DirectDecompressor createDirectDecompressor() {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public String getDefaultExtension() {\n+    return \".*\";\n+  }\n+\n+  @Override\n+  public Decompressor createDecompressor() {\n+    return new FixedLengthCharsetTransformingDecompressor(fixedLengthCharset);\n+  }\n+\n+  @Override\n+  public SplitCompressionInputStream createInputStream(InputStream seekableIn,\n+                                                       Decompressor decompressor,\n+                                                       long start,\n+                                                       long end,\n+                                                       READ_MODE readMode) throws IOException {\n+    if (!(seekableIn instanceof Seekable)) {\n+      throw new IOException(\"seekableIn must be an instance of \" +\n+                              Seekable.class.getName());\n+    }\n+\n+    //Adjust start to align to the next character boundary.\n+    if (start % fixedLengthCharset.getCharLength() != 0) {\n+      LOG.info(\"Adjusted Start from {} to {} by {} bytes\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e11babb0b1c4c9cff529037debb1fae3169d6c3b"}, "originalPosition": 112}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0Njc4NTIx", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#pullrequestreview-544678521", "createdAt": "2020-12-04T06:05:08Z", "commit": {"oid": "e11babb0b1c4c9cff529037debb1fae3169d6c3b"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwNjowNTowOFrOH_CLFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwNjoyMjowNFrOH_CiiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg1Nzk0Mw==", "bodyText": "Can you add comments about which lines are different than the copy", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535857943", "createdAt": "2020-12-04T06:05:08Z", "author": {"login": "albertshau"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/CharsetTransformingLineRecordReader.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset;\n+\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharset;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingCodec;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.hadoop.mapreduce.lib.input.LineRecordReader;\n+import org.apache.hadoop.mapreduce.lib.input.SplitLineReader;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Copy of Hadoop's Line Record Reader (Hadoop Version 2.3.0)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e11babb0b1c4c9cff529037debb1fae3169d6c3b"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg1OTQ5Mg==", "bodyText": "nit: try to avoid throwing Exception or RuntimeException. This method is similar to Enum's valueOf() method, which will throw IllegalArgumentException in a similar situation.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535859492", "createdAt": "2020-12-04T06:09:31Z", "author": {"login": "albertshau"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharset.java", "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Enumeration containing all supported Fixed Length charsets.\n+ */\n+public enum FixedLengthCharset {\n+  UTF_32(\"UTF-32\", Charset.forName(\"UTF-32\"), 4),\n+  ISO_8859_1(\"ISO-8859-1\", StandardCharsets.ISO_8859_1, 1),\n+  WINDOWS_1252(\"Windows-1252\", Charset.forName(\"windows-1252\"), 1);\n+\n+  private final String name;\n+  private final Charset charset;\n+  private final int charLength;\n+\n+  FixedLengthCharset(String name, Charset charset, int charLength) {\n+    this.name = name;\n+    this.charset = charset;\n+    this.charLength = charLength;\n+  }\n+\n+  public String getName() {\n+    return name;\n+  }\n+\n+  public Charset getCharset() {\n+    return charset;\n+  }\n+\n+  public int getCharLength() {\n+    return charLength;\n+  }\n+\n+  /**\n+   * Find a FixedLengthCharset for a given encoding name. Throws a runtime exception if not found.\n+   * @param name Charset name\n+   * @return FixedLengthCharset for the desired charset.\n+   */\n+  public static FixedLengthCharset forName(String name) {\n+    for (FixedLengthCharset c : FixedLengthCharset.values()) {\n+      if (name.equalsIgnoreCase(c.getName())) {\n+        return c;\n+      }\n+    }\n+\n+    throw new RuntimeException(\"Cannot find FixedLengthCharset with name: \" + name);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e11babb0b1c4c9cff529037debb1fae3169d6c3b"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg2MDEyOA==", "bodyText": "privatre final", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535860128", "createdAt": "2020-12-04T06:11:21Z", "author": {"login": "albertshau"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingCodec.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.DefaultCodec;\n+import org.apache.hadoop.io.compress.DirectDecompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * Codec implementation that returns a decompressor for Fixed Length character encodings.\n+ */\n+public class FixedLengthCharsetTransformingCodec extends DefaultCodec\n+  implements Configurable, SplittableCompressionCodec {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingCodec.class);\n+\n+  FixedLengthCharset fixedLengthCharset;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e11babb0b1c4c9cff529037debb1fae3169d6c3b"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg2MDI5Mg==", "bodyText": "UnsupportedOperationException (same with others)", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535860292", "createdAt": "2020-12-04T06:11:45Z", "author": {"login": "albertshau"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingCodec.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.DefaultCodec;\n+import org.apache.hadoop.io.compress.DirectDecompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * Codec implementation that returns a decompressor for Fixed Length character encodings.\n+ */\n+public class FixedLengthCharsetTransformingCodec extends DefaultCodec\n+  implements Configurable, SplittableCompressionCodec {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingCodec.class);\n+\n+  FixedLengthCharset fixedLengthCharset;\n+\n+  public FixedLengthCharsetTransformingCodec(FixedLengthCharset fixedLengthCharset) {\n+    this.fixedLengthCharset = fixedLengthCharset;\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out) throws IOException {\n+    throw new RuntimeException(\"Not supported\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e11babb0b1c4c9cff529037debb1fae3169d6c3b"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg2MTExOQ==", "bodyText": "private final", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535861119", "createdAt": "2020-12-04T06:14:16Z", "author": {"login": "albertshau"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressor.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.io.compress.Decompressor;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+\n+/**\n+ * Decompressor that can be used to convert byte streams in fixed-length character encodings to a stream of UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressor implements Decompressor {\n+\n+  final FixedLengthCharset origin;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e11babb0b1c4c9cff529037debb1fae3169d6c3b"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg2MzIwNw==", "bodyText": "unused?", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535863207", "createdAt": "2020-12-04T06:20:15Z", "author": {"login": "albertshau"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressorStream.class);\n+\n+  //Starting and ending position in the file.\n+  long start;\n+  long end;\n+\n+  protected FixedLengthCharsetTransformingDecompressorStream(InputStream in,\n+                                                             FixedLengthCharset fixedLengthCharset,\n+                                                             long start,\n+                                                             long end)\n+    throws IOException {\n+    super(in, new FixedLengthCharsetTransformingDecompressor(fixedLengthCharset));\n+    long skippedBytes = in.skip(start);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e11babb0b1c4c9cff529037debb1fae3169d6c3b"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg2Mzk0NA==", "bodyText": "seems like this would never happen?", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r535863944", "createdAt": "2020-12-04T06:22:04Z", "author": {"login": "albertshau"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressorStream.class);\n+\n+  //Starting and ending position in the file.\n+  long start;\n+  long end;\n+\n+  protected FixedLengthCharsetTransformingDecompressorStream(InputStream in,\n+                                                             FixedLengthCharset fixedLengthCharset,\n+                                                             long start,\n+                                                             long end)\n+    throws IOException {\n+    super(in, new FixedLengthCharsetTransformingDecompressor(fixedLengthCharset));\n+    long skippedBytes = in.skip(start);\n+    this.start = start;\n+    this.end = end;\n+  }\n+\n+  @Override\n+  protected int decompress(byte[] b, int off, int len) throws IOException {\n+    //Set input for decompression if it's needed for execution.\n+    if (this.decompressor.needsInput()) {\n+      int l = getCompressedData();\n+      if (l > 0) {\n+        this.decompressor.setInput(buffer, 0, l);\n+      }\n+    }\n+\n+    //Proceed with super method.\n+    return super.decompress(b, off, len);\n+  }\n+\n+  @Override\n+  public long getPos() throws IOException {\n+    //If we're working with a Charset Transforming decompressor, we can calculate the current position on the input file\n+    // By adding the starting position in the file with the number of bytes we have read so far.\n+    if (this.decompressor instanceof FixedLengthCharsetTransformingDecompressor) {\n+      FixedLengthCharsetTransformingDecompressor flcDecompressor =\n+        (FixedLengthCharsetTransformingDecompressor) this.decompressor;\n+\n+      //Actual position is starting possition + the number of bytes we have consumed - the remaining bytes\n+      return start + flcDecompressor.getNumConsumedBytes();\n+    } else {\n+      return super.getPos();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e11babb0b1c4c9cff529037debb1fae3169d6c3b"}, "originalPosition": 74}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ1Mjk5NjY1", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#pullrequestreview-545299665", "createdAt": "2020-12-04T21:13:04Z", "commit": {"oid": "e11babb0b1c4c9cff529037debb1fae3169d6c3b"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQyMToyOTozN1rOH_irZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQyMTo1NDowOVrOH_jiHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjM5MDUwMQ==", "bodyText": "I don't think you need this override", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r536390501", "createdAt": "2020-12-04T21:29:37Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingCodec.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.DefaultCodec;\n+import org.apache.hadoop.io.compress.DirectDecompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * Codec implementation that returns a decompressor for Fixed Length character encodings.\n+ */\n+public class FixedLengthCharsetTransformingCodec extends DefaultCodec\n+  implements Configurable, SplittableCompressionCodec {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingCodec.class);\n+\n+  FixedLengthCharset fixedLengthCharset;\n+\n+  public FixedLengthCharsetTransformingCodec(FixedLengthCharset fixedLengthCharset) {\n+    this.fixedLengthCharset = fixedLengthCharset;\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out) throws IOException {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor) throws IOException {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public Class<? extends Compressor> getCompressorType() {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public Compressor createCompressor() {\n+    throw new RuntimeException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public CompressionInputStream createInputStream(InputStream in) throws IOException {\n+    return super.createInputStream(in, new FixedLengthCharsetTransformingDecompressor(this.fixedLengthCharset));\n+  }\n+\n+  @Override\n+  public CompressionInputStream createInputStream(InputStream in, Decompressor decompressor) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e11babb0b1c4c9cff529037debb1fae3169d6c3b"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjM5NjkzMA==", "bodyText": "You are doing a lot of copying here. Please consider using java.nio.charset.Charset along with ByteBuffer/CharBuffer.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r536396930", "createdAt": "2020-12-04T21:43:51Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressor.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.io.compress.Decompressor;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+\n+/**\n+ * Decompressor that can be used to convert byte streams in fixed-length character encodings to a stream of UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressor implements Decompressor {\n+\n+  final FixedLengthCharset origin;\n+  final Charset destination = StandardCharsets.UTF_8;\n+  long numConsumedBytes = 0;\n+\n+  ByteArrayOutputStream incomingBuffer = new ByteArrayOutputStream();\n+  ByteArrayInputStream outgoingBuffer = new ByteArrayInputStream(new byte[]{});\n+\n+  public FixedLengthCharsetTransformingDecompressor(FixedLengthCharset origin) {\n+    this.origin = origin;\n+  }\n+\n+  @Override\n+  public void setInput(byte[] b, int off, int len) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e11babb0b1c4c9cff529037debb1fae3169d6c3b"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQwMTc3NQ==", "bodyText": "Can we have EBCDIC here. Also I am not sure if it would make sense to have some kind of override to be able to use it for arbitrary charset.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r536401775", "createdAt": "2020-12-04T21:51:30Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharset.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Enumeration containing all currently supported Fixed Length charsets.\n+ *\n+ * This currently includes:\n+ * - UTF-32,\n+ * - ISO-8859 variants supported by Java\n+ * - Windows single-byte code pages supported by Java.\n+ */\n+public enum FixedLengthCharset {\n+  UTF_32(\"UTF-32\", Charset.forName(\"UTF-32\"), 4),\n+  ISO_8859_1(\"ISO-8859-1\", StandardCharsets.ISO_8859_1, 1),\n+  ISO_8859_2(\"ISO-8859-2\", Charset.forName(\"ISO-8859-2\"), 1),\n+  ISO_8859_3(\"ISO-8859-3\", Charset.forName(\"ISO-8859-3\"), 1),\n+  ISO_8859_4(\"ISO-8859-4\", Charset.forName(\"ISO-8859-4\"), 1),\n+  ISO_8859_5(\"ISO-8859-5\", Charset.forName(\"ISO-8859-5\"), 1),\n+  ISO_8859_6(\"ISO-8859-6\", Charset.forName(\"ISO-8859-6\"), 1),\n+  ISO_8859_7(\"ISO-8859-7\", Charset.forName(\"ISO-8859-7\"), 1),\n+  ISO_8859_8(\"ISO-8859-8\", Charset.forName(\"ISO-8859-8\"), 1),\n+  ISO_8859_9(\"ISO-8859-9\", Charset.forName(\"ISO-8859-9\"), 1),\n+  ISO_8859_11(\"ISO-8859-11\", Charset.forName(\"ISO-8859-11\"), 1),\n+  ISO_8859_13(\"ISO-8859-13\", Charset.forName(\"ISO-8859-13\"), 1),\n+  ISO_8859_15(\"ISO-8859-15\", Charset.forName(\"ISO-8859-15\"), 1),\n+  WINDOWS_1250(\"Windows-1250\", Charset.forName(\"windows-1250\"), 1),\n+  WINDOWS_1251(\"Windows-1251\", Charset.forName(\"windows-1251\"), 1),\n+  WINDOWS_1252(\"Windows-1252\", Charset.forName(\"windows-1252\"), 1),\n+  WINDOWS_1253(\"Windows-1253\", Charset.forName(\"windows-1253\"), 1),\n+  WINDOWS_1254(\"Windows-1254\", Charset.forName(\"windows-1254\"), 1),\n+  WINDOWS_1255(\"Windows-1255\", Charset.forName(\"windows-1255\"), 1),\n+  WINDOWS_1256(\"Windows-1256\", Charset.forName(\"windows-1256\"), 1),\n+  WINDOWS_1257(\"Windows-1257\", Charset.forName(\"windows-1257\"), 1),\n+  WINDOWS_1258(\"Windows-1258\", Charset.forName(\"windows-1258\"), 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "83427f7002b016044f5f4606d998b92511c92ceb"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQwNDUxMA==", "bodyText": "You can also try to use java.nio.charset.CharsetDecoder maxCharsPerByte/averageCharsPerByte and skip this table altogether", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r536404510", "createdAt": "2020-12-04T21:54:09Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharset.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Enumeration containing all currently supported Fixed Length charsets.\n+ *\n+ * This currently includes:\n+ * - UTF-32,\n+ * - ISO-8859 variants supported by Java\n+ * - Windows single-byte code pages supported by Java.\n+ */\n+public enum FixedLengthCharset {\n+  UTF_32(\"UTF-32\", Charset.forName(\"UTF-32\"), 4),\n+  ISO_8859_1(\"ISO-8859-1\", StandardCharsets.ISO_8859_1, 1),\n+  ISO_8859_2(\"ISO-8859-2\", Charset.forName(\"ISO-8859-2\"), 1),\n+  ISO_8859_3(\"ISO-8859-3\", Charset.forName(\"ISO-8859-3\"), 1),\n+  ISO_8859_4(\"ISO-8859-4\", Charset.forName(\"ISO-8859-4\"), 1),\n+  ISO_8859_5(\"ISO-8859-5\", Charset.forName(\"ISO-8859-5\"), 1),\n+  ISO_8859_6(\"ISO-8859-6\", Charset.forName(\"ISO-8859-6\"), 1),\n+  ISO_8859_7(\"ISO-8859-7\", Charset.forName(\"ISO-8859-7\"), 1),\n+  ISO_8859_8(\"ISO-8859-8\", Charset.forName(\"ISO-8859-8\"), 1),\n+  ISO_8859_9(\"ISO-8859-9\", Charset.forName(\"ISO-8859-9\"), 1),\n+  ISO_8859_11(\"ISO-8859-11\", Charset.forName(\"ISO-8859-11\"), 1),\n+  ISO_8859_13(\"ISO-8859-13\", Charset.forName(\"ISO-8859-13\"), 1),\n+  ISO_8859_15(\"ISO-8859-15\", Charset.forName(\"ISO-8859-15\"), 1),\n+  WINDOWS_1250(\"Windows-1250\", Charset.forName(\"windows-1250\"), 1),\n+  WINDOWS_1251(\"Windows-1251\", Charset.forName(\"windows-1251\"), 1),\n+  WINDOWS_1252(\"Windows-1252\", Charset.forName(\"windows-1252\"), 1),\n+  WINDOWS_1253(\"Windows-1253\", Charset.forName(\"windows-1253\"), 1),\n+  WINDOWS_1254(\"Windows-1254\", Charset.forName(\"windows-1254\"), 1),\n+  WINDOWS_1255(\"Windows-1255\", Charset.forName(\"windows-1255\"), 1),\n+  WINDOWS_1256(\"Windows-1256\", Charset.forName(\"windows-1256\"), 1),\n+  WINDOWS_1257(\"Windows-1257\", Charset.forName(\"windows-1257\"), 1),\n+  WINDOWS_1258(\"Windows-1258\", Charset.forName(\"windows-1258\"), 1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQwMTc3NQ=="}, "originalCommit": {"oid": "83427f7002b016044f5f4606d998b92511c92ceb"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ1MzgwNTM3", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#pullrequestreview-545380537", "createdAt": "2020-12-05T00:44:32Z", "commit": {"oid": "8aa2eead624ea2e0b5e6dc4fd61859f844554160"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNVQwMDo0NDozMlrOH_nKRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNVQwMDo0NDozMlrOH_nKRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2Mzk0Mw==", "bodyText": "What is modified and why is needed?", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r536463943", "createdAt": "2020-12-05T00:44:32Z", "author": {"login": "chtyim"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/CharsetTransformingLineRecordReader.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset;\n+\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharset;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingCodec;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingDecompressorStream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CodecPool;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.hadoop.mapreduce.lib.input.LineRecordReader;\n+import org.apache.hadoop.mapreduce.lib.input.SplitLineReader;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Copy of Hadoop's LineRecordReader (Hadoop Version 2.3.0). The reason we copy this class is to modify some behaviors\n+ * related to the creation of the decompressor. This also allows us to implement", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8aa2eead624ea2e0b5e6dc4fd61859f844554160"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ2NTQ5NjI1", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#pullrequestreview-546549625", "createdAt": "2020-12-07T21:26:18Z", "commit": {"oid": "9040a4823a7aac5b96f42cb3f759b543c2074309"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMToyNjoxOVrOIA7bCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMToyNjoxOVrOIA7bCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0NDQ4OA==", "bodyText": "@chtyim @albertshau @tivv I want to bring your attention to this line function:\nThe problem I'm facing at this time is that, in order to ensure that we respect the partition boundaries, and process each record exactly once, I need to make sure that I only read the last line that begins in a partition. This means that if a line is \"split\" by a partition, the worker that was reading the previous partition is the one that reads this line.\nThe way Hadoop does this for normal text files is to check against the file position in order to make sure the last line has not been read. However, this poses a problem for me, because the compressor reads part of the file in chunks, and the file position does not necessarily align with the position of the last line that was read and decoded.\nThe block of code I'm describing can be found here: https://github.com/cdapio/hydrator-plugins/blob/834s7f7002b016044f5f4606d998b92511c92ceb/format-common/src/main/java/io/cdap/plugin/format/charset/CharsetTransformingLineRecordReader.java#L165-L171\nThis poses a challenge for me. I see 2 ways to fix this:\n\n\nAs you can see in my implementation above, once I approach the partition boundary, I make sure to only read one character at a time, until the function that reads a line is able to complete successfully, and break out of the loop. This means that, as I approach the partition boundary, the execution speed slows down.\n\n\nThe other solution I saw is to calculate the file position based on reading character-by-character from the input in the decompressor. The problem I see with that is that, for every character I read, I would need to create a String instance, convert to bytes, and fill the output buffer in this way. This would give me the ability to accurately determine my position in the underlying file, but doing the Bytes -> 1 Character long String -> Bytes in UTF-8 charset conversion for every character in my input file is definitely not a performant solution, and this will impact performance everywhere in my partition, not just the end of the partition.\n\n\nLet me know if you can think of a better way to approach this partition boundary issue. The hybrid approach would be to add this \"character by character\" logic on the decompressor, but this does not strike me as the place where this logic should live.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r537844488", "createdAt": "2020-12-07T21:26:19Z", "author": {"login": "fernst"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import io.cdap.plugin.format.charset.CharsetTransformingLineRecordReader;\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressorStream.class);\n+\n+  //Starting and ending position in the file.\n+  protected final long start;\n+  protected final long end;\n+  protected final FixedLengthCharset fixedLengthCharset;\n+\n+  protected FixedLengthCharsetTransformingDecompressorStream(InputStream in,\n+                                                             FixedLengthCharset fixedLengthCharset,\n+                                                             long start,\n+                                                             long end)\n+    throws IOException {\n+    super(in, new FixedLengthCharsetTransformingDecompressor(fixedLengthCharset));\n+    long skippedBytes = in.skip(start);\n+    this.fixedLengthCharset = fixedLengthCharset;\n+    this.start = start;\n+    this.end = end;\n+  }\n+\n+  @Override\n+  protected int decompress(byte[] b, int off, int len) throws IOException {\n+    //Set input for decompression if it's needed for execution.\n+    if (this.decompressor.needsInput()) {\n+      int l = getCompressedData();\n+      if (l > 0) {\n+        this.decompressor.setInput(buffer, 0, l);\n+      }\n+    }\n+\n+    //Proceed with super method.\n+    return super.decompress(b, off, len);\n+  }\n+\n+  @Override\n+  public long getPos() throws IOException {\n+    // Since we're working with a Charset Transforming decompressor, we can calculate the current position on the\n+    // input file.\n+    // By adding the starting position in the file with the number of bytes we have read so far.\n+    FixedLengthCharsetTransformingDecompressor flcDecompressor =\n+      (FixedLengthCharsetTransformingDecompressor) this.decompressor;\n+\n+    //Actual position is starting possition + the number of bytes we have consumed.\n+    return start + flcDecompressor.getNumConsumedBytes();\n+  }\n+\n+  /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9040a4823a7aac5b96f42cb3f759b543c2074309"}, "originalPosition": 78}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4NTY5OTk4", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#pullrequestreview-548569998", "createdAt": "2020-12-09T20:14:08Z", "commit": {"oid": "758257c745606bbe61d48d819215a189a4c7c286"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQyMDoxNDowOFrOICnclA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQyMToyNjowOVrOICqNxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYxNDM1Ng==", "bodyText": "How does UTF-8 become null here? I am missing it.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539614356", "createdAt": "2020-12-09T20:14:08Z", "author": {"login": "tivv"}, "path": "core-plugins/src/main/java/io/cdap/plugin/batch/source/FileBatchSource.java", "diffHunk": "@@ -48,6 +48,9 @@ public FileBatchSource(FileSourceConfig config) {\n     if (config.shouldCopyHeader()) {\n       properties.put(PathTrackingInputFormat.COPY_HEADER, \"true\");\n     }\n+    if (config.getFileEncoding() != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "758257c745606bbe61d48d819215a189a4c7c286"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYxNzM3Ng==", "bodyText": "Well, I believe if we have a single codec specified in io.compression.codecs configuration and it returns an empty   string as a suffix it would be picked up, is not it?", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539617376", "createdAt": "2020-12-09T20:18:58Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/CharsetTransformingLineRecordReader.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset;\n+\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharset;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingCodec;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingDecompressorStream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.CodecPool;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.hadoop.mapreduce.lib.input.LineRecordReader;\n+import org.apache.hadoop.mapreduce.lib.input.SplitLineReader;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Copy of Hadoop's LineRecordReader (Hadoop Version 2.3.0). The reason we copy this class is to modify some behaviors\n+ * related to the creation of the decompressor. This also allows us to implement", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2Mzk0Mw=="}, "originalCommit": {"oid": "8aa2eead624ea2e0b5e6dc4fd61859f844554160"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYyMTMxOA==", "bodyText": "So, can we automatically allow any single-byte encoding here?", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539621318", "createdAt": "2020-12-09T20:25:23Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharset.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * Enumeration containing all currently supported Fixed Length charsets.\n+ *\n+ * This currently includes:\n+ * - UTF-32,\n+ * - ISO-8859 variants supported by Java\n+ * - Windows single-byte code pages supported by Java.\n+ */\n+public enum FixedLengthCharset {\n+  UTF_32(\"UTF-32\", Charset.forName(\"UTF-32\"), 4),\n+  ISO_8859_1(\"ISO-8859-1\", StandardCharsets.ISO_8859_1, 1),\n+  ISO_8859_2(\"ISO-8859-2\", Charset.forName(\"ISO-8859-2\"), 1),\n+  ISO_8859_3(\"ISO-8859-3\", Charset.forName(\"ISO-8859-3\"), 1),\n+  ISO_8859_4(\"ISO-8859-4\", Charset.forName(\"ISO-8859-4\"), 1),\n+  ISO_8859_5(\"ISO-8859-5\", Charset.forName(\"ISO-8859-5\"), 1),\n+  ISO_8859_6(\"ISO-8859-6\", Charset.forName(\"ISO-8859-6\"), 1),\n+  ISO_8859_7(\"ISO-8859-7\", Charset.forName(\"ISO-8859-7\"), 1),\n+  ISO_8859_8(\"ISO-8859-8\", Charset.forName(\"ISO-8859-8\"), 1),\n+  ISO_8859_9(\"ISO-8859-9\", Charset.forName(\"ISO-8859-9\"), 1),\n+  ISO_8859_11(\"ISO-8859-11\", Charset.forName(\"ISO-8859-11\"), 1),\n+  ISO_8859_13(\"ISO-8859-13\", Charset.forName(\"ISO-8859-13\"), 1),\n+  ISO_8859_15(\"ISO-8859-15\", Charset.forName(\"ISO-8859-15\"), 1),\n+  WINDOWS_1250(\"Windows-1250\", Charset.forName(\"windows-1250\"), 1),\n+  WINDOWS_1251(\"Windows-1251\", Charset.forName(\"windows-1251\"), 1),\n+  WINDOWS_1252(\"Windows-1252\", Charset.forName(\"windows-1252\"), 1),\n+  WINDOWS_1253(\"Windows-1253\", Charset.forName(\"windows-1253\"), 1),\n+  WINDOWS_1254(\"Windows-1254\", Charset.forName(\"windows-1254\"), 1),\n+  WINDOWS_1255(\"Windows-1255\", Charset.forName(\"windows-1255\"), 1),\n+  WINDOWS_1256(\"Windows-1256\", Charset.forName(\"windows-1256\"), 1),\n+  WINDOWS_1257(\"Windows-1257\", Charset.forName(\"windows-1257\"), 1),\n+  WINDOWS_1258(\"Windows-1258\", Charset.forName(\"windows-1258\"), 1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQwMTc3NQ=="}, "originalCommit": {"oid": "83427f7002b016044f5f4606d998b92511c92ceb"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYzNTE0Mg==", "bodyText": "Why do we need this? I see that super already handles similar case", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539635142", "createdAt": "2020-12-09T20:46:18Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,78 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import io.cdap.plugin.format.charset.CharsetTransformingLineRecordReader;\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressorStream.class);\n+\n+  //Starting and ending position in the file.\n+  protected final long start;\n+  protected final long end;\n+  protected final FixedLengthCharset fixedLengthCharset;\n+\n+  protected FixedLengthCharsetTransformingDecompressorStream(InputStream in,\n+                                                             FixedLengthCharset fixedLengthCharset,\n+                                                             long start,\n+                                                             long end)\n+    throws IOException {\n+    super(in, new FixedLengthCharsetTransformingDecompressor(fixedLengthCharset));\n+    in.skip(start);\n+    this.fixedLengthCharset = fixedLengthCharset;\n+    this.start = start;\n+    this.end = end;\n+  }\n+\n+  @Override\n+  protected int decompress(byte[] b, int off, int len) throws IOException {\n+    //Set input for decompression if it's needed for execution.\n+    if (this.decompressor.needsInput()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "758257c745606bbe61d48d819215a189a4c7c286"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYzOTI4NA==", "bodyText": "I believe record delimiting is done after we recoded everything to UTF-8, so  this value should be in UTF-8. Could you please add a test with record delimiting?", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539639284", "createdAt": "2020-12-09T20:52:56Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/input/CharsetTransformingPathTrackingInputFormat.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.input;\n+\n+import io.cdap.plugin.format.charset.CharsetTransformingLineRecordReader;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharset;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\n+\n+/**\n+ * An input format that tracks which the file path each record was read from. This InputFormat is a wrapper around\n+ * underlying input formats. The responsibility of this class is to keep track of which file each record is reading\n+ * from, and to add the file URI to each record. In addition, for text files, it can be configured to keep track\n+ * of the header for the file, which underlying record readers can use.\n+ */\n+public class CharsetTransformingPathTrackingInputFormat extends TextInputFormat {\n+\n+  protected final FixedLengthCharset fixedLengthCharset;\n+\n+  public CharsetTransformingPathTrackingInputFormat(String charsetName) {\n+    this.fixedLengthCharset = FixedLengthCharset.forName(charsetName);\n+  }\n+\n+  @Override\n+  public RecordReader<LongWritable, Text> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    String delimiter = context.getConfiguration().get(\"textinputformat.record.delimiter\");\n+    byte[] recordDelimiterBytes = null;\n+    if (null != delimiter) {\n+      recordDelimiterBytes = delimiter.getBytes(fixedLengthCharset.getCharset());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "758257c745606bbe61d48d819215a189a4c7c286"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY1NzAyMQ==", "bodyText": "Can we just do ByteBuffer.wrap(b, off, len) and save one more copy?", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539657021", "createdAt": "2020-12-09T21:22:05Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressor.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.CharBuffer;\n+import java.nio.charset.Charset;\n+import java.nio.charset.CharsetDecoder;\n+import java.nio.charset.CharsetEncoder;\n+import java.nio.charset.CoderResult;\n+import java.nio.charset.StandardCharsets;\n+\n+/**\n+ * Decompressor that can be used to convert byte streams in fixed-length character encodings to a stream of UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressor implements Decompressor {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressor.class);\n+\n+  protected final FixedLengthCharset sourceEncoding;\n+  protected final CharsetDecoder decoder;\n+  protected final CharsetEncoder encoder;\n+  protected final Charset targetCharset = StandardCharsets.UTF_8;\n+  protected long numDecodedCharacters = 0;\n+  protected long numEncodedCharacters = 0;\n+\n+  //Initializing all buffers.\n+  protected ByteBuffer inputByteBuffer = ByteBuffer.allocate(0);\n+  protected CharBuffer decodedCharBuffer = CharBuffer.allocate(0);\n+  protected ByteBuffer partialOutputByteBuffer = ByteBuffer.allocate(0);\n+\n+  public FixedLengthCharsetTransformingDecompressor(FixedLengthCharset sourceEncoding) {\n+    this.sourceEncoding = sourceEncoding;\n+    this.decoder = sourceEncoding.getCharset().newDecoder();\n+    this.encoder = targetCharset.newEncoder();\n+  }\n+\n+  @Override\n+  public void setInput(byte[] b, int off, int len) {\n+    //Expand incoming buffer if needed.\n+    if (inputByteBuffer.remaining() < len) {\n+      //Allocate new buffer that can fill the existing input + newly received bytes\n+      ByteBuffer newIncomingBuffer = ByteBuffer.allocate(len + inputByteBuffer.capacity());\n+\n+      //Set up incoming buffer for reads and copy contents into new buffer.\n+      inputByteBuffer.flip();\n+      newIncomingBuffer.put(inputByteBuffer);\n+\n+      inputByteBuffer = newIncomingBuffer;\n+    }\n+\n+    //Copy incoming payload into Input Byte Buffer\n+    inputByteBuffer.put(b, off, len);\n+    inputByteBuffer.flip();\n+\n+    //Set up char buffer for writes\n+    decodedCharBuffer.compact();\n+\n+    //Expand the char buffer if needed.\n+    if (decodedCharBuffer.capacity() < inputByteBuffer.limit() / sourceEncoding.getNumBytesPerCharacter()) {\n+      decodedCharBuffer = CharBuffer.allocate(inputByteBuffer.limit() / sourceEncoding.getNumBytesPerCharacter());\n+    }\n+\n+    //Decode bytes from the input buffer into the Decoded Char Buffer\n+    decodeByteBufferIntoCharBuffer(inputByteBuffer);\n+\n+    //Set up decoded char buffer for reads.\n+    decodedCharBuffer.flip();\n+\n+    //Set up incoming buffer for writes.\n+    inputByteBuffer.compact();\n+\n+  }\n+\n+  /**\n+   * Note that we only ask for additional input once we have completely depleted out outgoing buffer.\n+   */\n+  @Override\n+  public boolean needsInput() {\n+    return decodedCharBuffer.remaining() == 0;\n+  }\n+\n+  @Override\n+  public void setDictionary(byte[] b, int off, int len) {\n+    //no-op\n+  }\n+\n+  @Override\n+  public boolean needsDictionary() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean finished() {\n+    return decodedCharBuffer.remaining() == 0 && partialOutputByteBuffer.remaining() == 0;\n+  }\n+\n+  @Override\n+  public int decompress(byte[] b, int off, int len) throws IOException {\n+\n+    //Allocate new outgoing buffer\n+    ByteBuffer encodedBuffer = ByteBuffer.allocate(len - off);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "758257c745606bbe61d48d819215a189a4c7c286"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTY1OTcxNg==", "bodyText": "This is a pretty heavy operation. I think we need it only when we could not encode anything. It's totally fine to fil less than len as soon as we fill at least something. We are not required to fill each block exactly to the length.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r539659716", "createdAt": "2020-12-09T21:26:09Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressor.java", "diffHunk": "@@ -0,0 +1,266 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.CharBuffer;\n+import java.nio.charset.Charset;\n+import java.nio.charset.CharsetDecoder;\n+import java.nio.charset.CharsetEncoder;\n+import java.nio.charset.CoderResult;\n+import java.nio.charset.StandardCharsets;\n+\n+/**\n+ * Decompressor that can be used to convert byte streams in fixed-length character encodings to a stream of UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressor implements Decompressor {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressor.class);\n+\n+  protected final FixedLengthCharset sourceEncoding;\n+  protected final CharsetDecoder decoder;\n+  protected final CharsetEncoder encoder;\n+  protected final Charset targetCharset = StandardCharsets.UTF_8;\n+  protected long numDecodedCharacters = 0;\n+  protected long numEncodedCharacters = 0;\n+\n+  //Initializing all buffers.\n+  protected ByteBuffer inputByteBuffer = ByteBuffer.allocate(0);\n+  protected CharBuffer decodedCharBuffer = CharBuffer.allocate(0);\n+  protected ByteBuffer partialOutputByteBuffer = ByteBuffer.allocate(0);\n+\n+  public FixedLengthCharsetTransformingDecompressor(FixedLengthCharset sourceEncoding) {\n+    this.sourceEncoding = sourceEncoding;\n+    this.decoder = sourceEncoding.getCharset().newDecoder();\n+    this.encoder = targetCharset.newEncoder();\n+  }\n+\n+  @Override\n+  public void setInput(byte[] b, int off, int len) {\n+    //Expand incoming buffer if needed.\n+    if (inputByteBuffer.remaining() < len) {\n+      //Allocate new buffer that can fill the existing input + newly received bytes\n+      ByteBuffer newIncomingBuffer = ByteBuffer.allocate(len + inputByteBuffer.capacity());\n+\n+      //Set up incoming buffer for reads and copy contents into new buffer.\n+      inputByteBuffer.flip();\n+      newIncomingBuffer.put(inputByteBuffer);\n+\n+      inputByteBuffer = newIncomingBuffer;\n+    }\n+\n+    //Copy incoming payload into Input Byte Buffer\n+    inputByteBuffer.put(b, off, len);\n+    inputByteBuffer.flip();\n+\n+    //Set up char buffer for writes\n+    decodedCharBuffer.compact();\n+\n+    //Expand the char buffer if needed.\n+    if (decodedCharBuffer.capacity() < inputByteBuffer.limit() / sourceEncoding.getNumBytesPerCharacter()) {\n+      decodedCharBuffer = CharBuffer.allocate(inputByteBuffer.limit() / sourceEncoding.getNumBytesPerCharacter());\n+    }\n+\n+    //Decode bytes from the input buffer into the Decoded Char Buffer\n+    decodeByteBufferIntoCharBuffer(inputByteBuffer);\n+\n+    //Set up decoded char buffer for reads.\n+    decodedCharBuffer.flip();\n+\n+    //Set up incoming buffer for writes.\n+    inputByteBuffer.compact();\n+\n+  }\n+\n+  /**\n+   * Note that we only ask for additional input once we have completely depleted out outgoing buffer.\n+   */\n+  @Override\n+  public boolean needsInput() {\n+    return decodedCharBuffer.remaining() == 0;\n+  }\n+\n+  @Override\n+  public void setDictionary(byte[] b, int off, int len) {\n+    //no-op\n+  }\n+\n+  @Override\n+  public boolean needsDictionary() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean finished() {\n+    return decodedCharBuffer.remaining() == 0 && partialOutputByteBuffer.remaining() == 0;\n+  }\n+\n+  @Override\n+  public int decompress(byte[] b, int off, int len) throws IOException {\n+\n+    //Allocate new outgoing buffer\n+    ByteBuffer encodedBuffer = ByteBuffer.allocate(len - off);\n+\n+    //Consume any remaining bytes from a previous decompress invocation.\n+    while (partialOutputByteBuffer != null && partialOutputByteBuffer.hasRemaining() && encodedBuffer.hasRemaining()) {\n+      encodedBuffer.put(partialOutputByteBuffer.get());\n+    }\n+\n+    //Encode as many characters as possible into the Encoded Buffer.\n+    encodeCharBufferIntoByteBuffer(encodedBuffer);\n+\n+    // Handle the case where the outgoing buffer can still fit additional space.\n+    // This means we need to encode one extra character and add as many bytes as possible into the output buffer.\n+    if (decodedCharBuffer.remaining() > 0 && encodedBuffer.remaining() > 0) {\n+      encodePartialCharacter(encodedBuffer);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "758257c745606bbe61d48d819215a189a4c7c286"}, "originalPosition": 134}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzMTUzNTAx", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#pullrequestreview-553153501", "createdAt": "2020-12-16T00:14:48Z", "commit": {"oid": "125413aeb91761443be156c954426dc91869793a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwMDoxNDo0OFrOIGlx1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwMDoxNDo0OFrOIGlx1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc4MTMzMw==", "bodyText": "I see you committed this for test support, but I can't seem to find the test that uses it.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r543781333", "createdAt": "2020-12-16T00:14:48Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -104,4 +119,17 @@ protected int getCompressedData() throws IOException {\n \n     return numReadBytes;\n   }\n+\n+  /**\n+   * Method that invokes the parent method to get decompressed data, without partition boundary awareness.\n+   * <p>\n+   * This method is visible for the purposes of testing the partition boundaries.\n+   *\n+   * @return number of bytes read from source input stream\n+   * @throws IOException If there was a problem reading from the underlying input stream.\n+   */\n+  @VisibleForTesting\n+  protected int getCompressedDataSuper() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "125413aeb91761443be156c954426dc91869793a"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzMTg4MjIw", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#pullrequestreview-553188220", "createdAt": "2020-12-16T00:39:43Z", "commit": {"oid": "125413aeb91761443be156c954426dc91869793a"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwMDozOTo0NFrOIGmXzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwMDo0MTozMVrOIGmaTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTA1Mw==", "bodyText": "What does this inheritance give you? You are only using it to wrap into TransformingCompressionInputStream. Would it make sense to move logic from here to TransformingCompressionInputStream and use DecompressorStream from TransformingCompressionInputStream. Having one less layer can make position handling easier.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r543791053", "createdAt": "2020-12-16T00:39:44Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "125413aeb91761443be156c954426dc91869793a"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTQxNA==", "bodyText": "I believe this is not needed. Same logic would be done in DefaultCodec", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r543791414", "createdAt": "2020-12-16T00:40:37Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingCodec.java", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.DefaultCodec;\n+import org.apache.hadoop.io.compress.DirectDecompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * Codec implementation that returns a decompressor for Fixed Length character encodings.\n+ */\n+public class FixedLengthCharsetTransformingCodec extends DefaultCodec\n+  implements Configurable, SplittableCompressionCodec {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingCodec.class);\n+\n+  private final FixedLengthCharset sourceEncoding;\n+\n+  public FixedLengthCharsetTransformingCodec(FixedLengthCharset sourceEncoding) {\n+    this.sourceEncoding = sourceEncoding;\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "125413aeb91761443be156c954426dc91869793a"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTU0Mg==", "bodyText": "I believe this is not needed. Given compressor is passed it's fine to allow it.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r543791542", "createdAt": "2020-12-16T00:41:03Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingCodec.java", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.DefaultCodec;\n+import org.apache.hadoop.io.compress.DirectDecompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * Codec implementation that returns a decompressor for Fixed Length character encodings.\n+ */\n+public class FixedLengthCharsetTransformingCodec extends DefaultCodec\n+  implements Configurable, SplittableCompressionCodec {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingCodec.class);\n+\n+  private final FixedLengthCharset sourceEncoding;\n+\n+  public FixedLengthCharsetTransformingCodec(FixedLengthCharset sourceEncoding) {\n+    this.sourceEncoding = sourceEncoding;\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out) throws IOException {\n+    throw new UnsupportedOperationException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "125413aeb91761443be156c954426dc91869793a"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzc5MTY5NQ==", "bodyText": "I believe this is not needed. Same logic would be done in DefaultCodec", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r543791695", "createdAt": "2020-12-16T00:41:31Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingCodec.java", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.CompressionOutputStream;\n+import org.apache.hadoop.io.compress.Compressor;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.DefaultCodec;\n+import org.apache.hadoop.io.compress.DirectDecompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+/**\n+ * Codec implementation that returns a decompressor for Fixed Length character encodings.\n+ */\n+public class FixedLengthCharsetTransformingCodec extends DefaultCodec\n+  implements Configurable, SplittableCompressionCodec {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingCodec.class);\n+\n+  private final FixedLengthCharset sourceEncoding;\n+\n+  public FixedLengthCharsetTransformingCodec(FixedLengthCharset sourceEncoding) {\n+    this.sourceEncoding = sourceEncoding;\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out) throws IOException {\n+    throw new UnsupportedOperationException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public CompressionOutputStream createOutputStream(OutputStream out, Compressor compressor) throws IOException {\n+    throw new UnsupportedOperationException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public Class<? extends Compressor> getCompressorType() {\n+    throw new UnsupportedOperationException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public Compressor createCompressor() {\n+    throw new UnsupportedOperationException(\"Not supported\");\n+  }\n+\n+  @Override\n+  public CompressionInputStream createInputStream(InputStream in) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "125413aeb91761443be156c954426dc91869793a"}, "originalPosition": 70}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0ODI1NTE5", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#pullrequestreview-554825519", "createdAt": "2020-12-17T17:27:43Z", "commit": {"oid": "40bd8dc118818fd5afcbfae8edcec9841ee001cb"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzoyNzo0M1rOIIAhmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzozNDozMVrOIIA08A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI2ODEyMg==", "bodyText": "Some imports are not used. Please optimize imports across files.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545268122", "createdAt": "2020-12-17T17:27:43Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharset.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import java.nio.charset.Charset;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Collections;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "40bd8dc118818fd5afcbfae8edcec9841ee001cb"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3MTQ4Nw==", "bodyText": "I don't think this one is used", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545271487", "createdAt": "2020-12-17T17:32:32Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressorStream.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "40bd8dc118818fd5afcbfae8edcec9841ee001cb"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3MTcwMQ==", "bodyText": "I don't think you use this value outside of constructor, no need for a field", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545271701", "createdAt": "2020-12-17T17:32:53Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {\n+  private static final Logger LOG = LoggerFactory.getLogger(FixedLengthCharsetTransformingDecompressorStream.class);\n+\n+  //Starting and ending position in the file.\n+  private final long start;\n+  private final long end;\n+  private final FixedLengthCharset fixedLengthCharset;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "40bd8dc118818fd5afcbfae8edcec9841ee001cb"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3MzA3Mg==", "bodyText": "nit: pos is not used. It may make sense even to remove the method", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545273072", "createdAt": "2020-12-17T17:34:31Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/CharsetTransformingLineRecordReader.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharset;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingCodec;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingDecompressorStream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.Seekable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.compress.Decompressor;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.io.compress.SplittableCompressionCodec;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader;\n+import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n+import org.apache.hadoop.mapreduce.lib.input.SplitLineReader;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Copy of Hadoop's LineRecordReader (Hadoop Version 2.3.0). The reason we copy this class is to modify some behaviors\n+ * related to the creation of the decompressor. This also allows us to implement\n+ * <p>\n+ * This class uses a fixed Codec and Decompressor to parse records.\n+ */\n+public class CharsetTransformingLineRecordReader extends RecordReader<LongWritable, Text> {\n+  private static final Logger LOG = LoggerFactory.getLogger(CharsetTransformingLineRecordReader.class);\n+  public static final String MAX_LINE_LENGTH =\n+    \"mapreduce.input.linerecordreader.line.maxlength\";\n+\n+  private final FixedLengthCharset fixedLengthCharset;\n+  private final byte[] recordDelimiterBytes;\n+  private long start;\n+  private long pos;\n+  private long end;\n+  private SplitLineReader in;\n+  private Seekable filePosition;\n+  private int maxLineLength;\n+  private LongWritable key;\n+  private Text value;\n+  private Decompressor decompressor;\n+\n+  public CharsetTransformingLineRecordReader(FixedLengthCharset fixedLengthCharset, byte[] recordDelimiter) {\n+    this.fixedLengthCharset = fixedLengthCharset;\n+    this.recordDelimiterBytes = recordDelimiter;\n+  }\n+\n+  @VisibleForTesting\n+  protected CharsetTransformingLineRecordReader(FixedLengthCharset fixedLengthCharset,\n+                                                byte[] recordDelimiter,\n+                                                SplitLineReader in,\n+                                                long start,\n+                                                long pos,\n+                                                long end,\n+                                                int maxLineLength) {\n+    this(fixedLengthCharset, recordDelimiter);\n+    this.in = in;\n+    this.start = start;\n+    this.pos = pos;\n+    this.end = end;\n+    this.maxLineLength = maxLineLength;\n+  }\n+\n+  /**\n+   * Initialize method from parent class, simplified for this our use case from the base class.\n+   *\n+   * @param genericSplit File Split\n+   * @param context      Execution context\n+   * @throws IOException if the underlying file or decompression operations fail.\n+   */\n+  public void initialize(InputSplit genericSplit,\n+                         TaskAttemptContext context) throws IOException {\n+    FileSplit split = (FileSplit) genericSplit;\n+    Configuration job = context.getConfiguration();\n+    this.maxLineLength = job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n+    start = split.getStart();\n+    end = start + split.getLength();\n+    final Path file = split.getPath();\n+\n+    // open the file and seek to the start of the split\n+    final FileSystem fs = file.getFileSystem(job);\n+    FSDataInputStream fileIn = fs.open(file);\n+\n+    SplittableCompressionCodec codec = new FixedLengthCharsetTransformingCodec(fixedLengthCharset);\n+    decompressor = codec.createDecompressor();\n+\n+    final SplitCompressionInputStream cIn =\n+      codec.createInputStream(\n+        fileIn, decompressor, start, end,\n+        SplittableCompressionCodec.READ_MODE.CONTINUOUS);\n+    in = new CompressedSplitLineReader(cIn, job,\n+                                       this.recordDelimiterBytes);\n+    start = cIn.getAdjustedStart();\n+    end = cIn.getAdjustedEnd();\n+    filePosition = cIn;\n+\n+    // If this is not the first split, we always throw away first record\n+    // because we always (except the last split) read one extra line in\n+    // next() method.\n+    if (start != 0) {\n+      Text t = new Text();\n+      start += in.readLine(t, 4096, maxBytesToConsume(start));\n+      LOG.info(\"Discarded line: \" + t.toString());\n+    }\n+    this.pos = start;\n+  }\n+\n+  /**\n+   * Returns the maximum of bytes to consume from the input stream\n+   * Since the input is compressed, there is no way to accurately determine how many bytes we need to consume.\n+   *\n+   * @param pos Current file position\n+   * @return Number of bytes to consume.\n+   */\n+  private int maxBytesToConsume(long pos) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "40bd8dc118818fd5afcbfae8edcec9841ee001cb"}, "originalPosition": 139}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cbd73e1a617376a2b7b5ed07da1b0220aa07d43c", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/cbd73e1a617376a2b7b5ed07da1b0220aa07d43c", "committedDate": "2020-12-17T20:46:39Z", "message": "Further cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings.."}, "afterCommit": {"oid": "7bf14e395cb5ecd3c3ac71a4fb097469d23ddad9", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/7bf14e395cb5ecd3c3ac71a4fb097469d23ddad9", "committedDate": "2020-12-17T20:52:11Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings.."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7bf14e395cb5ecd3c3ac71a4fb097469d23ddad9", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/7bf14e395cb5ecd3c3ac71a4fb097469d23ddad9", "committedDate": "2020-12-17T20:52:11Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings.."}, "afterCommit": {"oid": "8ac4d372de73284b8c29febc16f041675dc3b287", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/8ac4d372de73284b8c29febc16f041675dc3b287", "committedDate": "2020-12-17T21:08:35Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings.."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8ac4d372de73284b8c29febc16f041675dc3b287", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/8ac4d372de73284b8c29febc16f041675dc3b287", "committedDate": "2020-12-17T21:08:35Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings.."}, "afterCommit": {"oid": "309086086aadd7941a7339d1abbfe9917dba8b48", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/309086086aadd7941a7339d1abbfe9917dba8b48", "committedDate": "2020-12-17T22:28:00Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings.."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MDc2NzM4", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#pullrequestreview-555076738", "createdAt": "2020-12-17T23:46:44Z", "commit": {"oid": "309086086aadd7941a7339d1abbfe9917dba8b48"}, "state": "APPROVED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMzo0Njo0NVrOIINbUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMDowNzowN1rOIIN4fA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ3OTUwNw==", "bodyText": "Should just name this \"UTF-8\", as the default is already controlled by the \"default\" widget-attribute.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545479507", "createdAt": "2020-12-17T23:46:45Z", "author": {"login": "albertshau"}, "path": "core-plugins/widgets/File-batchsource.json", "diffHunk": "@@ -154,6 +154,82 @@\n           \"label\": \"File System Properties\",\n           \"name\": \"fileSystemProperties\"\n         },\n+        {\n+          \"widget-type\": \"select\",\n+          \"label\": \"File encoding\",\n+          \"name\": \"fileEncoding\",\n+          \"widget-attributes\": {\n+            \"values\": [\n+              \"UTF-8 (default)\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "309086086aadd7941a7339d1abbfe9917dba8b48"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ4MTA4Nw==", "bodyText": "there was another change that was recently merged adding a shouldGetSchema() method (see #1288). That new method also needs to be updated to also check that the fileEncoding is not macro enabled.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545481087", "createdAt": "2020-12-17T23:51:08Z", "author": {"login": "albertshau"}, "path": "format-common/src/main/java/io/cdap/plugin/format/plugin/AbstractFileSourceConfig.java", "diffHunk": "@@ -133,6 +140,17 @@ public void validate(FailureCollector collector) {\n       collector.addFailure(e.getMessage(), null).withConfigProperty(NAME_SCHEMA).withStacktrace(e.getStackTrace());\n     }\n \n+    if (fileEncoding != null && !fileEncoding.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "309086086aadd7941a7339d1abbfe9917dba8b48"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ4NDE3Mg==", "bodyText": "is this needed? Seems simpler to just have getFileEncoding() with return fileEncoding == null || fileEncoding.isEmpty() ? DEFAULT_FILE_ENCODING : fileEncoding", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545484172", "createdAt": "2020-12-17T23:58:53Z", "author": {"login": "albertshau"}, "path": "format-common/src/main/java/io/cdap/plugin/format/plugin/AbstractFileSourceConfig.java", "diffHunk": "@@ -198,6 +216,15 @@ public boolean skipHeader() {\n     return skipHeader == null ? false : skipHeader;\n   }\n \n+  @Nullable\n+  public String getFileEncoding() {\n+    return fileEncoding;\n+  }\n+\n+  public String getDefaultFileEncoding() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "309086086aadd7941a7339d1abbfe9917dba8b48"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ4NTU4MQ==", "bodyText": "style: we only import the class, not the static method.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545485581", "createdAt": "2020-12-18T00:03:06Z", "author": {"login": "albertshau"}, "path": "format-common/src/test/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTest.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static io.cdap.plugin.format.plugin.AbstractFileSourceConfig.cleanFileEncodingName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "309086086aadd7941a7339d1abbfe9917dba8b48"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ4NjIxNg==", "bodyText": "it's better not to mutate any fields, as it could lead to weird bugs if getFileEncoding() is called before validate().\nInstead, it seems like we should change the getter to clean the encoding name before returning it. This incurs the extra cost of cleaning per get call, but it shouldn't be a big deal.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545486216", "createdAt": "2020-12-18T00:04:59Z", "author": {"login": "albertshau"}, "path": "format-common/src/main/java/io/cdap/plugin/format/plugin/AbstractFileSourceConfig.java", "diffHunk": "@@ -133,6 +140,17 @@ public void validate(FailureCollector collector) {\n       collector.addFailure(e.getMessage(), null).withConfigProperty(NAME_SCHEMA).withStacktrace(e.getStackTrace());\n     }\n \n+    if (fileEncoding != null && !fileEncoding.isEmpty()) {\n+      fileEncoding = cleanFileEncodingName(fileEncoding);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "309086086aadd7941a7339d1abbfe9917dba8b48"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ4Njk3Mg==", "bodyText": "ideally we would just configure the UI to use a different display name than the actual value instead of performing some cleanup in the backend. However, I don't think there's a way to do this for the select widget.  @elfenheart can you confirm?", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545486972", "createdAt": "2020-12-18T00:07:07Z", "author": {"login": "albertshau"}, "path": "format-common/src/main/java/io/cdap/plugin/format/plugin/AbstractFileSourceConfig.java", "diffHunk": "@@ -210,4 +237,16 @@ public Schema getSchema() {\n   public boolean shouldCopyHeader() {\n     return copyHeader;\n   }\n+\n+  /**\n+   * Takes the first word of the file encoding string, as any further words are just charset descriptions.\n+   *\n+   * @param fileEncoding The file encoding parameter supplied by the user\n+   * @return the cleaned up file encoding name\n+   */\n+  public static String cleanFileEncodingName(String fileEncoding) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "309086086aadd7941a7339d1abbfe9917dba8b48"}, "originalPosition": 86}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MTIwODg4", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#pullrequestreview-555120888", "createdAt": "2020-12-18T01:54:33Z", "commit": {"oid": "309086086aadd7941a7339d1abbfe9917dba8b48"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMTo1NDozM1rOIIQB3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMTo1NDozM1rOIIQB3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUyMjE0MA==", "bodyText": "Please update the javadoc", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r545522140", "createdAt": "2020-12-18T01:54:33Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharset.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import com.google.common.collect.ImmutableMap;\n+\n+import java.nio.charset.Charset;\n+import java.nio.charset.CharsetEncoder;\n+import java.util.Map;\n+\n+/**\n+ * Enumeration containing all currently supported Fixed Length charsets.\n+ * <p>\n+ * This currently includes:\n+ * - UTF-32,\n+ * - ISO-8859 variants supported by Java\n+ * - Windows single-byte code pages supported by Java.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "309086086aadd7941a7339d1abbfe9917dba8b48"}, "originalPosition": 31}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "309086086aadd7941a7339d1abbfe9917dba8b48", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/309086086aadd7941a7339d1abbfe9917dba8b48", "committedDate": "2020-12-17T22:28:00Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings.."}, "afterCommit": {"oid": "7b6eee761cf030794bbafa222a42ecc221a2bfd5", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/7b6eee761cf030794bbafa222a42ecc221a2bfd5", "committedDate": "2020-12-21T22:12:42Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings.."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU2NzQ4NDc0", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#pullrequestreview-556748474", "createdAt": "2020-12-22T00:40:37Z", "commit": {"oid": "7b6eee761cf030794bbafa222a42ecc221a2bfd5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwMDo0MDozN1rOIJqaxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwMDo0MDozN1rOIJqaxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzAwMzA3OQ==", "bodyText": "Almost forgot. In this version of JUnit you do assertEquals(expected, actual). Please fix, otherwise errors are looking weird.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r547003079", "createdAt": "2020-12-22T00:40:37Z", "author": {"login": "tivv"}, "path": "format-common/src/test/java/io/cdap/plugin/format/charset/CharsetTransformingLineRecordReaderTest.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset;\n+\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharset;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingCodec;\n+import io.cdap.plugin.format.charset.fixedlength.FixedLengthCharsetTransformingDecompressorStream;\n+import io.cdap.plugin.format.charset.fixedlength.TransformingCompressionInputStream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.compress.CompressionInputStream;\n+import org.apache.hadoop.io.compress.SplitCompressionInputStream;\n+import org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.junit.MockitoJUnitRunner;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+\n+import static org.powermock.api.mockito.PowerMockito.doAnswer;\n+import static org.powermock.api.mockito.PowerMockito.spy;\n+\n+@RunWith(MockitoJUnitRunner.class)\n+public class CharsetTransformingLineRecordReaderTest {\n+\n+  Configuration conf;\n+  FixedLengthCharset fixedLengthCharset;\n+  CharsetTransformingLineRecordReader recordReader;\n+  FixedLengthCharsetTransformingCodec codec;\n+  ByteArrayInputStream inputStream;\n+  int availableBytes;\n+  FixedLengthCharsetTransformingDecompressorStream decompressorStream;\n+  SplitCompressionInputStream compressionInputStream;\n+\n+  //We set up the input so each line is 5 characters long, which is 20 bytes in UTF-32.\n+  final String input = \"abcd\\nedfg\\nijkl\\n\";\n+\n+  @Before\n+  public void before() throws IOException {\n+    //Set up the Compressed Split Line Reader with a buffer size of 4096 bytes.\n+    //This ensures the buffer will consume all characters in the input stream if we allow it to.\n+    conf = new Configuration();\n+    conf.setInt(\"io.file.buffer.size\", 4096);\n+\n+    fixedLengthCharset = FixedLengthCharset.UTF_32;\n+\n+    codec = new FixedLengthCharsetTransformingCodec(fixedLengthCharset);\n+    codec.setConf(conf);\n+\n+    inputStream = new ByteArrayInputStream(input.getBytes(fixedLengthCharset.getCharset()));\n+    availableBytes = inputStream.available();\n+  }\n+\n+  public void setUpRecordReaderForTest(SplitCompressionInputStream splitCompressionInputStream) throws IOException {\n+\n+    // Set up record reader to assume we'll read the file from the beggining, and the partition size is 32 bytes\n+    // which is 8 characters in UTF-32, meaning we expect to read the first 2 lines for this partition.\n+    recordReader = spy(new CharsetTransformingLineRecordReader(\n+      fixedLengthCharset,\n+      null,\n+      new CompressedSplitLineReader(splitCompressionInputStream, conf, null),\n+      0,\n+      0,\n+      32,\n+      4096\n+    ));\n+\n+    //We will calculate position based on the number of bytes consumed from the input stream.\n+    doAnswer(a -> (long) availableBytes - inputStream.available()).when(recordReader).getFilePosition();\n+  }\n+\n+  @Test\n+  public void testGetNextLine() throws IOException {\n+    decompressorStream =\n+      new FixedLengthCharsetTransformingDecompressorStream(inputStream, FixedLengthCharset.UTF_32, 0, 32);\n+    compressionInputStream = new TransformingCompressionInputStream(decompressorStream, 0, 32);\n+\n+    //Set up test\n+    setUpRecordReaderForTest(compressionInputStream);\n+\n+    //Ensure the first line is read\n+    Assert.assertTrue(recordReader.nextKeyValue());\n+    Assert.assertEquals(recordReader.getCurrentKey().get(), 0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7b6eee761cf030794bbafa222a42ecc221a2bfd5"}, "originalPosition": 99}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7b6eee761cf030794bbafa222a42ecc221a2bfd5", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/7b6eee761cf030794bbafa222a42ecc221a2bfd5", "committedDate": "2020-12-21T22:12:42Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings.."}, "afterCommit": {"oid": "95b37bf97bdfad5e7d43ff4af2cfd16e49da7cc8", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/95b37bf97bdfad5e7d43ff4af2cfd16e49da7cc8", "committedDate": "2020-12-22T02:16:57Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings.."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "95b37bf97bdfad5e7d43ff4af2cfd16e49da7cc8", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/95b37bf97bdfad5e7d43ff4af2cfd16e49da7cc8", "committedDate": "2020-12-22T02:16:57Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings.."}, "afterCommit": {"oid": "5d36de5ca33a0e277fec9e7e9362ed24934451e6", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/5d36de5ca33a0e277fec9e7e9362ed24934451e6", "committedDate": "2020-12-22T15:54:16Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings.."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5d36de5ca33a0e277fec9e7e9362ed24934451e6", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/5d36de5ca33a0e277fec9e7e9362ed24934451e6", "committedDate": "2020-12-22T15:54:16Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings.."}, "afterCommit": {"oid": "6200875de859e968d129712bddb6feb816ff8f05", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/6200875de859e968d129712bddb6feb816ff8f05", "committedDate": "2020-12-23T16:58:16Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings..\nMerge pull request #462 from data-integrations/feature/PLUGIN-464-flatten-fix\n\nPLUGIN-464 fix flatten to be a no-op on empty lists"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4MjYzNTI2", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#pullrequestreview-558263526", "createdAt": "2020-12-23T22:59:40Z", "commit": {"oid": "6200875de859e968d129712bddb6feb816ff8f05"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QyMjo1OTo0MFrOIK6Idg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QyMjo1OTo0MFrOIK6Idg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODMwOTExMA==", "bodyText": "Please cast to seekable and use seek instead. E.g. on S3 filesystem FSDataInputStream is constructed on top of S3InputStream that do not override default skip, so skip is done in old way - by reading and skipping all the data.", "url": "https://github.com/cdapio/hydrator-plugins/pull/1255#discussion_r548309110", "createdAt": "2020-12-23T22:59:40Z", "author": {"login": "tivv"}, "path": "format-common/src/main/java/io/cdap/plugin/format/charset/fixedlength/FixedLengthCharsetTransformingDecompressorStream.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Copyright \u00a9 2020 Cask Data, Inc.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not\n+ * use this file except in compliance with the License. You may obtain a copy of\n+ * the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n+ * License for the specific language governing permissions and limitations under\n+ * the License.\n+ */\n+\n+package io.cdap.plugin.format.charset.fixedlength;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.io.compress.DecompressorStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * DecompressorStream implementation for the FixedLengthCharsetTransformingDecompressor.\n+ * <p>\n+ * This can be used to convert input streams containing bytes for fixed length charsets into UTF-8 bytes.\n+ */\n+public class FixedLengthCharsetTransformingDecompressorStream extends DecompressorStream {\n+\n+  private final long start;\n+  private final long end;\n+  private long totalReadBytes = 0;\n+\n+  public FixedLengthCharsetTransformingDecompressorStream(InputStream in,\n+                                                          FixedLengthCharset fixedLengthCharset,\n+                                                          long start,\n+                                                          long end)\n+    throws IOException {\n+    super(in, new FixedLengthCharsetTransformingDecompressor(fixedLengthCharset), 4096);\n+    in.skip(start);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6200875de859e968d129712bddb6feb816ff8f05"}, "originalPosition": 44}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9ba9dc7558b3f545c97e68fad49998ab434a5b0e", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/9ba9dc7558b3f545c97e68fad49998ab434a5b0e", "committedDate": "2020-12-24T03:13:38Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nUpdated Decompressor Stream to use the Seek method from Seekable implementations."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6200875de859e968d129712bddb6feb816ff8f05", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/6200875de859e968d129712bddb6feb816ff8f05", "committedDate": "2020-12-23T16:58:16Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nI've identified an edge case when some lines get skipped in the partition edges. I'm close to isolating the cause of the issue.\n\nFixed issue with the final line (or lines) or a partition being skipped because of boundary calculation issues.\n\nAdded additional fixed-length encodings and tests.\n\nWill continue to add additonal tests and tie all configurations together.\n\nAdded support for configuration parameter supplied by the UI.\n\nAdditional cleanup.\n\nAddressed some of the PR comments.\n\nAdded additional comments.\n\nRewrote decompressor to use ByteBuffer and CharBuffer.\n\nAdded UI elements for the new File Encoding option for the File Batch Source.\n\nSmall tweaks based on comments on the PR.\n\nAdded logic to deal with partition boundaries. This ensure records are read exactly once.\n\nTested with UTF-32 and ISO-8859-1.\n\nSmall tweak based on comments.\n\nImproved Decompressor Stream partition boundary handling.\n\nFixed issue with Abstract File Source configuration. When the default encoding (UTF-8) is selected, the standard Hadoop Line Record Reader is used.\n\nAdded additional tests to validate partition boundary behavior.\n\nAdded missing test class.\n\nRemoved some redundant functions.\n\nAddressed some performance concerns on the decompressor.\n\nSimplified edge case handling for the decompressor.\n\nFurther cleanup based on review.\n\nSet up test which highlights the partition boundary issues we have to solve in the DecompressorStream.\n\nCleaning up.\n\nAdditional cleaning up for DecompressorStream class.\n\nUpdated tests to use base instances to highlight implementation details in our solution\n\nFurther cleanup.\n\nAdded support for all IBM code pages supported by Java, including the EBCIDC encodings..\nMerge pull request #462 from data-integrations/feature/PLUGIN-464-flatten-fix\n\nPLUGIN-464 fix flatten to be a no-op on empty lists"}, "afterCommit": {"oid": "9ba9dc7558b3f545c97e68fad49998ab434a5b0e", "author": {"user": {"login": "fernst", "name": "Fernando Velasquez"}}, "url": "https://github.com/cdapio/hydrator-plugins/commit/9ba9dc7558b3f545c97e68fad49998ab434a5b0e", "committedDate": "2020-12-24T03:13:38Z", "message": "Added support for decoding files encoded in fixed-length charsets when reading from a file source.\n\nAs it stands, UTF-32, ISO-8859, Windows-1252 and IBM formats are supported.\n\nUpdated Decompressor Stream to use the Seek method from Seekable implementations."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1514, "cost": 1, "resetAt": "2021-11-01T13:07:16Z"}}}