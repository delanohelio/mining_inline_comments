{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIxNTc1MDMw", "number": 10774, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNzo1Mjo0OFrOE543dA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwODowMDowOVrOE55A1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MTM1OTg4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/elasticsearch/gateway/Gateway.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwNzo1Mjo0OFrOH0oDog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwODoxODowNlrOH0pGzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk0NDI5MA==", "bodyText": "Why do we diverge here from elastic/elasticsearch@61de092#diff-4b3484e2c7a2e90b18ee27be05b638f9de273fa7976d1af077cfde3f353019caR54?\nAfaik it will set nodeIds only to the request to avoid streaming each bigger DiscoveryNode. Any reason to change this?", "url": "https://github.com/crate/crate/pull/10774#discussion_r524944290", "createdAt": "2020-11-17T07:52:48Z", "author": {"login": "seut"}, "path": "server/src/main/java/org/elasticsearch/gateway/Gateway.java", "diffHunk": "@@ -41,22 +44,27 @@\n \n     private final ClusterService clusterService;\n \n-    private final TransportNodesListGatewayMetaState listGatewayMetaState;\n+    private final NodeClient client;\n \n     private final IndicesService indicesService;\n \n     public Gateway(final Settings settings, final ClusterService clusterService,\n-                   final TransportNodesListGatewayMetaState listGatewayMetaState,\n+                   final NodeClient client,\n                    final IndicesService indicesService) {\n         this.indicesService = indicesService;\n         this.clusterService = clusterService;\n-        this.listGatewayMetaState = listGatewayMetaState;\n+        this.client = client;\n     }\n \n     public void performStateRecovery(final GatewayStateRecoveredListener listener) throws GatewayException {\n-        DiscoveryNode[] discoveryNodes = clusterService.state().nodes().getMasterNodes().values().toArray(DiscoveryNode.class);\n-        LOGGER.trace(\"performing state recovery from {}\", discoveryNodes);\n-        final TransportNodesListGatewayMetaState.NodesGatewayMetaState nodesState = listGatewayMetaState.list(discoveryNodes, null).actionGet();\n+        final DiscoveryNode[] nodes = clusterService.state().nodes().getMasterNodes().values().toArray(DiscoveryNode.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1148e0740afdc9f7cb8e5dabcb520cbd1d6044d6"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk1ODk3OQ==", "bodyText": "We changed this in  ed18312\nMaybe we could have gone the other way to avoid streaming the bigger DiscoveryNode. Maybe something to revisit later.", "url": "https://github.com/crate/crate/pull/10774#discussion_r524958979", "createdAt": "2020-11-17T08:13:43Z", "author": {"login": "mfussenegger"}, "path": "server/src/main/java/org/elasticsearch/gateway/Gateway.java", "diffHunk": "@@ -41,22 +44,27 @@\n \n     private final ClusterService clusterService;\n \n-    private final TransportNodesListGatewayMetaState listGatewayMetaState;\n+    private final NodeClient client;\n \n     private final IndicesService indicesService;\n \n     public Gateway(final Settings settings, final ClusterService clusterService,\n-                   final TransportNodesListGatewayMetaState listGatewayMetaState,\n+                   final NodeClient client,\n                    final IndicesService indicesService) {\n         this.indicesService = indicesService;\n         this.clusterService = clusterService;\n-        this.listGatewayMetaState = listGatewayMetaState;\n+        this.client = client;\n     }\n \n     public void performStateRecovery(final GatewayStateRecoveredListener listener) throws GatewayException {\n-        DiscoveryNode[] discoveryNodes = clusterService.state().nodes().getMasterNodes().values().toArray(DiscoveryNode.class);\n-        LOGGER.trace(\"performing state recovery from {}\", discoveryNodes);\n-        final TransportNodesListGatewayMetaState.NodesGatewayMetaState nodesState = listGatewayMetaState.list(discoveryNodes, null).actionGet();\n+        final DiscoveryNode[] nodes = clusterService.state().nodes().getMasterNodes().values().toArray(DiscoveryNode.class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk0NDI5MA=="}, "originalCommit": {"oid": "1148e0740afdc9f7cb8e5dabcb520cbd1d6044d6"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk2MDA3Mw==", "bodyText": "I see, thanks for the info, wasn't aware of this.", "url": "https://github.com/crate/crate/pull/10774#discussion_r524960073", "createdAt": "2020-11-17T08:15:31Z", "author": {"login": "seut"}, "path": "server/src/main/java/org/elasticsearch/gateway/Gateway.java", "diffHunk": "@@ -41,22 +44,27 @@\n \n     private final ClusterService clusterService;\n \n-    private final TransportNodesListGatewayMetaState listGatewayMetaState;\n+    private final NodeClient client;\n \n     private final IndicesService indicesService;\n \n     public Gateway(final Settings settings, final ClusterService clusterService,\n-                   final TransportNodesListGatewayMetaState listGatewayMetaState,\n+                   final NodeClient client,\n                    final IndicesService indicesService) {\n         this.indicesService = indicesService;\n         this.clusterService = clusterService;\n-        this.listGatewayMetaState = listGatewayMetaState;\n+        this.client = client;\n     }\n \n     public void performStateRecovery(final GatewayStateRecoveredListener listener) throws GatewayException {\n-        DiscoveryNode[] discoveryNodes = clusterService.state().nodes().getMasterNodes().values().toArray(DiscoveryNode.class);\n-        LOGGER.trace(\"performing state recovery from {}\", discoveryNodes);\n-        final TransportNodesListGatewayMetaState.NodesGatewayMetaState nodesState = listGatewayMetaState.list(discoveryNodes, null).actionGet();\n+        final DiscoveryNode[] nodes = clusterService.state().nodes().getMasterNodes().values().toArray(DiscoveryNode.class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk0NDI5MA=="}, "originalCommit": {"oid": "1148e0740afdc9f7cb8e5dabcb520cbd1d6044d6"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk2MTQ4Nw==", "bodyText": "I also forgot about it until I had to look why the cherry-pick conflicted :)", "url": "https://github.com/crate/crate/pull/10774#discussion_r524961487", "createdAt": "2020-11-17T08:18:06Z", "author": {"login": "mfussenegger"}, "path": "server/src/main/java/org/elasticsearch/gateway/Gateway.java", "diffHunk": "@@ -41,22 +44,27 @@\n \n     private final ClusterService clusterService;\n \n-    private final TransportNodesListGatewayMetaState listGatewayMetaState;\n+    private final NodeClient client;\n \n     private final IndicesService indicesService;\n \n     public Gateway(final Settings settings, final ClusterService clusterService,\n-                   final TransportNodesListGatewayMetaState listGatewayMetaState,\n+                   final NodeClient client,\n                    final IndicesService indicesService) {\n         this.indicesService = indicesService;\n         this.clusterService = clusterService;\n-        this.listGatewayMetaState = listGatewayMetaState;\n+        this.client = client;\n     }\n \n     public void performStateRecovery(final GatewayStateRecoveredListener listener) throws GatewayException {\n-        DiscoveryNode[] discoveryNodes = clusterService.state().nodes().getMasterNodes().values().toArray(DiscoveryNode.class);\n-        LOGGER.trace(\"performing state recovery from {}\", discoveryNodes);\n-        final TransportNodesListGatewayMetaState.NodesGatewayMetaState nodesState = listGatewayMetaState.list(discoveryNodes, null).actionGet();\n+        final DiscoveryNode[] nodes = clusterService.state().nodes().getMasterNodes().values().toArray(DiscoveryNode.class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk0NDI5MA=="}, "originalCommit": {"oid": "1148e0740afdc9f7cb8e5dabcb520cbd1d6044d6"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MTM4Mzg4OnYy", "diffSide": "RIGHT", "path": "server/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorIT.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwODowMDowOVrOH0oRrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwODozOTo0MFrOH0p4fQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk0Nzg4NQ==", "bodyText": "debug traces? at least the original test does not contain it...", "url": "https://github.com/crate/crate/pull/10774#discussion_r524947885", "createdAt": "2020-11-17T08:00:09Z", "author": {"login": "seut"}, "path": "server/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorIT.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.gateway;\n+\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasItem;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.elasticsearch.action.admin.indices.flush.SyncedFlushAction;\n+import org.elasticsearch.action.admin.indices.flush.SyncedFlushRequest;\n+import org.elasticsearch.action.admin.indices.flush.SyncedFlushResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.seqno.ReplicationTracker;\n+import org.elasticsearch.indices.recovery.PeerRecoveryTargetService;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalSettingsPlugin;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.transport.MockTransportService;\n+import org.elasticsearch.transport.TransportService;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ReplicaShardAllocatorIT extends SQLTransportIntegrationTest {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        var nodePlugins = new ArrayList<>(super.nodePlugins());\n+        nodePlugins.add(MockTransportService.TestPlugin.class);\n+        nodePlugins.add(InternalSettingsPlugin.class);\n+        return nodePlugins;\n+    }\n+\n+    /**\n+     * Ensure that we fetch the latest shard store from the primary when a new node joins so we won't cancel the current recovery\n+     * for the copy on the newly joined node unless we can perform a noop recovery with that node.\n+     */\n+    @Test\n+    @Ignore(\"Fails currently, maybe missing backports\")\n+    public void testRecentPrimaryInformation() throws Exception {\n+        String indexName = \"test\";\n+        String nodeWithPrimary = internalCluster().startNode();\n+\n+        execute(\"\"\"\n+            create table doc.test (x int)\n+            clustered into 1 shards with (\n+                number_of_replicas = 1,\n+                \"recovery.file_based_threshold\" = 1.0,\n+                \"global_checkpoint_sync.interval\" = '100ms',\n+                \"unassigned.node_left.delayed_timeout\" = '1ms'\n+            )\n+        \"\"\");\n+        String nodeWithReplica = internalCluster().startDataOnlyNode();\n+        DiscoveryNode discoNodeWithReplica = internalCluster().getInstance(ClusterService.class, nodeWithReplica).localNode();\n+        Settings nodeWithReplicaSettings = internalCluster().dataPathSettings(nodeWithReplica);\n+        ensureGreen(indexName);\n+        execute(\"insert into doc.test (x) values (?)\", new Object[][] {\n+            new Object[] { randomIntBetween(10, 100) },\n+            new Object[] { randomIntBetween(10, 100) },\n+        });\n+        assertBusy(() -> {\n+            SyncedFlushResponse syncedFlushResponse = client()\n+                .execute(SyncedFlushAction.INSTANCE, new SyncedFlushRequest(indexName))\n+                .actionGet(5, TimeUnit.SECONDS);\n+            assertThat(syncedFlushResponse.successfulShards(), equalTo(2));\n+        });\n+        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(nodeWithReplica));\n+        if (randomBoolean()) {\n+            execute(\"insert into doc.test (x) values (?)\", new Object[][] {\n+                new Object[] { randomIntBetween(10, 100) },\n+                new Object[] { randomIntBetween(10, 100) },\n+            });\n+        }\n+        CountDownLatch blockRecovery = new CountDownLatch(1);\n+        CountDownLatch recoveryStarted = new CountDownLatch(1);\n+        MockTransportService transportServiceOnPrimary\n+            = (MockTransportService) internalCluster().getInstance(TransportService.class, nodeWithPrimary);\n+        transportServiceOnPrimary.addSendBehavior((connection, requestId, action, request, options) -> {\n+            if (PeerRecoveryTargetService.Actions.FILES_INFO.equals(action)) {\n+                recoveryStarted.countDown();\n+                try {\n+                    blockRecovery.await(5, TimeUnit.SECONDS);\n+                } catch (InterruptedException e) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+            connection.sendRequest(requestId, action, request, options);\n+        });\n+        try {\n+            String newNode = internalCluster().startDataOnlyNode();\n+            recoveryStarted.await(5, TimeUnit.SECONDS);\n+            // Index more documents and flush to destroy sync_id and remove the retention lease (as file_based_recovery_threshold reached).\n+            execute(\"insert into doc.test (x) values (?)\", new Object[][] {\n+                new Object[] { randomIntBetween(10, 100) },\n+                new Object[] { randomIntBetween(10, 100) },\n+            });\n+            execute(\"optimize table doc.test with (flush = true, max_num_segments = 1)\");\n+            assertBusy(() -> {\n+                execute(\"select unnest(retention_leases['leases']['id']) from sys.shards where table_name = 'test'\");\n+                for (var row : response.rows()) {\n+                    assertThat(row[0], not(equalTo((ReplicationTracker.getPeerRecoveryRetentionLeaseId(discoNodeWithReplica.getId())))));\n+                }\n+            });\n+            // AllocationService only calls GatewayAllocator if there are unassigned shards\n+            execute(\"\"\"\n+                create table doc.dummy (x int)\n+                with (\"routing.allocation.require.attr\" = 'not-found')\n+            \"\"\");\n+            internalCluster().startDataOnlyNode(nodeWithReplicaSettings);\n+            // need to wait for events to ensure the reroute has happened since we perform it async when a new node joins.\n+            client().admin().cluster()\n+                .prepareHealth(indexName)\n+                .setWaitForYellowStatus()\n+                .setWaitForEvents(Priority.LANGUID)\n+                .execute()\n+                .get(5, TimeUnit.SECONDS);\n+            blockRecovery.countDown();\n+            ensureGreen(indexName);\n+            assertThat(internalCluster().nodesInclude(indexName), hasItem(newNode));\n+            //for (RecoveryState recovery : client().admin().indices().prepareRecoveries(indexName).get().shardRecoveryStates().get(indexName)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f139aabfaf154149b3c0d220d48789c476fb4454"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk1OTg1Ng==", "bodyText": "Right - it came in later in a patch that we already applied - I commented it out because prepareRecoveries doesn't exist in CrateDB and the test currently fails as is before it gets to this line.\nAny suggestion how to handle it?  I suspected the failure my be related to other missing patches in the gateway area which is why I didn't spend too much time debugging.", "url": "https://github.com/crate/crate/pull/10774#discussion_r524959856", "createdAt": "2020-11-17T08:15:10Z", "author": {"login": "mfussenegger"}, "path": "server/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorIT.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.gateway;\n+\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasItem;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.elasticsearch.action.admin.indices.flush.SyncedFlushAction;\n+import org.elasticsearch.action.admin.indices.flush.SyncedFlushRequest;\n+import org.elasticsearch.action.admin.indices.flush.SyncedFlushResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.seqno.ReplicationTracker;\n+import org.elasticsearch.indices.recovery.PeerRecoveryTargetService;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalSettingsPlugin;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.transport.MockTransportService;\n+import org.elasticsearch.transport.TransportService;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ReplicaShardAllocatorIT extends SQLTransportIntegrationTest {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        var nodePlugins = new ArrayList<>(super.nodePlugins());\n+        nodePlugins.add(MockTransportService.TestPlugin.class);\n+        nodePlugins.add(InternalSettingsPlugin.class);\n+        return nodePlugins;\n+    }\n+\n+    /**\n+     * Ensure that we fetch the latest shard store from the primary when a new node joins so we won't cancel the current recovery\n+     * for the copy on the newly joined node unless we can perform a noop recovery with that node.\n+     */\n+    @Test\n+    @Ignore(\"Fails currently, maybe missing backports\")\n+    public void testRecentPrimaryInformation() throws Exception {\n+        String indexName = \"test\";\n+        String nodeWithPrimary = internalCluster().startNode();\n+\n+        execute(\"\"\"\n+            create table doc.test (x int)\n+            clustered into 1 shards with (\n+                number_of_replicas = 1,\n+                \"recovery.file_based_threshold\" = 1.0,\n+                \"global_checkpoint_sync.interval\" = '100ms',\n+                \"unassigned.node_left.delayed_timeout\" = '1ms'\n+            )\n+        \"\"\");\n+        String nodeWithReplica = internalCluster().startDataOnlyNode();\n+        DiscoveryNode discoNodeWithReplica = internalCluster().getInstance(ClusterService.class, nodeWithReplica).localNode();\n+        Settings nodeWithReplicaSettings = internalCluster().dataPathSettings(nodeWithReplica);\n+        ensureGreen(indexName);\n+        execute(\"insert into doc.test (x) values (?)\", new Object[][] {\n+            new Object[] { randomIntBetween(10, 100) },\n+            new Object[] { randomIntBetween(10, 100) },\n+        });\n+        assertBusy(() -> {\n+            SyncedFlushResponse syncedFlushResponse = client()\n+                .execute(SyncedFlushAction.INSTANCE, new SyncedFlushRequest(indexName))\n+                .actionGet(5, TimeUnit.SECONDS);\n+            assertThat(syncedFlushResponse.successfulShards(), equalTo(2));\n+        });\n+        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(nodeWithReplica));\n+        if (randomBoolean()) {\n+            execute(\"insert into doc.test (x) values (?)\", new Object[][] {\n+                new Object[] { randomIntBetween(10, 100) },\n+                new Object[] { randomIntBetween(10, 100) },\n+            });\n+        }\n+        CountDownLatch blockRecovery = new CountDownLatch(1);\n+        CountDownLatch recoveryStarted = new CountDownLatch(1);\n+        MockTransportService transportServiceOnPrimary\n+            = (MockTransportService) internalCluster().getInstance(TransportService.class, nodeWithPrimary);\n+        transportServiceOnPrimary.addSendBehavior((connection, requestId, action, request, options) -> {\n+            if (PeerRecoveryTargetService.Actions.FILES_INFO.equals(action)) {\n+                recoveryStarted.countDown();\n+                try {\n+                    blockRecovery.await(5, TimeUnit.SECONDS);\n+                } catch (InterruptedException e) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+            connection.sendRequest(requestId, action, request, options);\n+        });\n+        try {\n+            String newNode = internalCluster().startDataOnlyNode();\n+            recoveryStarted.await(5, TimeUnit.SECONDS);\n+            // Index more documents and flush to destroy sync_id and remove the retention lease (as file_based_recovery_threshold reached).\n+            execute(\"insert into doc.test (x) values (?)\", new Object[][] {\n+                new Object[] { randomIntBetween(10, 100) },\n+                new Object[] { randomIntBetween(10, 100) },\n+            });\n+            execute(\"optimize table doc.test with (flush = true, max_num_segments = 1)\");\n+            assertBusy(() -> {\n+                execute(\"select unnest(retention_leases['leases']['id']) from sys.shards where table_name = 'test'\");\n+                for (var row : response.rows()) {\n+                    assertThat(row[0], not(equalTo((ReplicationTracker.getPeerRecoveryRetentionLeaseId(discoNodeWithReplica.getId())))));\n+                }\n+            });\n+            // AllocationService only calls GatewayAllocator if there are unassigned shards\n+            execute(\"\"\"\n+                create table doc.dummy (x int)\n+                with (\"routing.allocation.require.attr\" = 'not-found')\n+            \"\"\");\n+            internalCluster().startDataOnlyNode(nodeWithReplicaSettings);\n+            // need to wait for events to ensure the reroute has happened since we perform it async when a new node joins.\n+            client().admin().cluster()\n+                .prepareHealth(indexName)\n+                .setWaitForYellowStatus()\n+                .setWaitForEvents(Priority.LANGUID)\n+                .execute()\n+                .get(5, TimeUnit.SECONDS);\n+            blockRecovery.countDown();\n+            ensureGreen(indexName);\n+            assertThat(internalCluster().nodesInclude(indexName), hasItem(newNode));\n+            //for (RecoveryState recovery : client().admin().indices().prepareRecoveries(indexName).get().shardRecoveryStates().get(indexName)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk0Nzg4NQ=="}, "originalCommit": {"oid": "f139aabfaf154149b3c0d220d48789c476fb4454"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk2MTg2Nw==", "bodyText": "I see. Shall we maybe create an issue for this test (which we could also use at the ignore annotation) to track it?", "url": "https://github.com/crate/crate/pull/10774#discussion_r524961867", "createdAt": "2020-11-17T08:18:48Z", "author": {"login": "seut"}, "path": "server/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorIT.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.gateway;\n+\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasItem;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.elasticsearch.action.admin.indices.flush.SyncedFlushAction;\n+import org.elasticsearch.action.admin.indices.flush.SyncedFlushRequest;\n+import org.elasticsearch.action.admin.indices.flush.SyncedFlushResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.seqno.ReplicationTracker;\n+import org.elasticsearch.indices.recovery.PeerRecoveryTargetService;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalSettingsPlugin;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.transport.MockTransportService;\n+import org.elasticsearch.transport.TransportService;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ReplicaShardAllocatorIT extends SQLTransportIntegrationTest {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        var nodePlugins = new ArrayList<>(super.nodePlugins());\n+        nodePlugins.add(MockTransportService.TestPlugin.class);\n+        nodePlugins.add(InternalSettingsPlugin.class);\n+        return nodePlugins;\n+    }\n+\n+    /**\n+     * Ensure that we fetch the latest shard store from the primary when a new node joins so we won't cancel the current recovery\n+     * for the copy on the newly joined node unless we can perform a noop recovery with that node.\n+     */\n+    @Test\n+    @Ignore(\"Fails currently, maybe missing backports\")\n+    public void testRecentPrimaryInformation() throws Exception {\n+        String indexName = \"test\";\n+        String nodeWithPrimary = internalCluster().startNode();\n+\n+        execute(\"\"\"\n+            create table doc.test (x int)\n+            clustered into 1 shards with (\n+                number_of_replicas = 1,\n+                \"recovery.file_based_threshold\" = 1.0,\n+                \"global_checkpoint_sync.interval\" = '100ms',\n+                \"unassigned.node_left.delayed_timeout\" = '1ms'\n+            )\n+        \"\"\");\n+        String nodeWithReplica = internalCluster().startDataOnlyNode();\n+        DiscoveryNode discoNodeWithReplica = internalCluster().getInstance(ClusterService.class, nodeWithReplica).localNode();\n+        Settings nodeWithReplicaSettings = internalCluster().dataPathSettings(nodeWithReplica);\n+        ensureGreen(indexName);\n+        execute(\"insert into doc.test (x) values (?)\", new Object[][] {\n+            new Object[] { randomIntBetween(10, 100) },\n+            new Object[] { randomIntBetween(10, 100) },\n+        });\n+        assertBusy(() -> {\n+            SyncedFlushResponse syncedFlushResponse = client()\n+                .execute(SyncedFlushAction.INSTANCE, new SyncedFlushRequest(indexName))\n+                .actionGet(5, TimeUnit.SECONDS);\n+            assertThat(syncedFlushResponse.successfulShards(), equalTo(2));\n+        });\n+        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(nodeWithReplica));\n+        if (randomBoolean()) {\n+            execute(\"insert into doc.test (x) values (?)\", new Object[][] {\n+                new Object[] { randomIntBetween(10, 100) },\n+                new Object[] { randomIntBetween(10, 100) },\n+            });\n+        }\n+        CountDownLatch blockRecovery = new CountDownLatch(1);\n+        CountDownLatch recoveryStarted = new CountDownLatch(1);\n+        MockTransportService transportServiceOnPrimary\n+            = (MockTransportService) internalCluster().getInstance(TransportService.class, nodeWithPrimary);\n+        transportServiceOnPrimary.addSendBehavior((connection, requestId, action, request, options) -> {\n+            if (PeerRecoveryTargetService.Actions.FILES_INFO.equals(action)) {\n+                recoveryStarted.countDown();\n+                try {\n+                    blockRecovery.await(5, TimeUnit.SECONDS);\n+                } catch (InterruptedException e) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+            connection.sendRequest(requestId, action, request, options);\n+        });\n+        try {\n+            String newNode = internalCluster().startDataOnlyNode();\n+            recoveryStarted.await(5, TimeUnit.SECONDS);\n+            // Index more documents and flush to destroy sync_id and remove the retention lease (as file_based_recovery_threshold reached).\n+            execute(\"insert into doc.test (x) values (?)\", new Object[][] {\n+                new Object[] { randomIntBetween(10, 100) },\n+                new Object[] { randomIntBetween(10, 100) },\n+            });\n+            execute(\"optimize table doc.test with (flush = true, max_num_segments = 1)\");\n+            assertBusy(() -> {\n+                execute(\"select unnest(retention_leases['leases']['id']) from sys.shards where table_name = 'test'\");\n+                for (var row : response.rows()) {\n+                    assertThat(row[0], not(equalTo((ReplicationTracker.getPeerRecoveryRetentionLeaseId(discoNodeWithReplica.getId())))));\n+                }\n+            });\n+            // AllocationService only calls GatewayAllocator if there are unassigned shards\n+            execute(\"\"\"\n+                create table doc.dummy (x int)\n+                with (\"routing.allocation.require.attr\" = 'not-found')\n+            \"\"\");\n+            internalCluster().startDataOnlyNode(nodeWithReplicaSettings);\n+            // need to wait for events to ensure the reroute has happened since we perform it async when a new node joins.\n+            client().admin().cluster()\n+                .prepareHealth(indexName)\n+                .setWaitForYellowStatus()\n+                .setWaitForEvents(Priority.LANGUID)\n+                .execute()\n+                .get(5, TimeUnit.SECONDS);\n+            blockRecovery.countDown();\n+            ensureGreen(indexName);\n+            assertThat(internalCluster().nodesInclude(indexName), hasItem(newNode));\n+            //for (RecoveryState recovery : client().admin().indices().prepareRecoveries(indexName).get().shardRecoveryStates().get(indexName)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk0Nzg4NQ=="}, "originalCommit": {"oid": "f139aabfaf154149b3c0d220d48789c476fb4454"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk3NDIwNQ==", "bodyText": "As discussed, we'll track that by a project card.", "url": "https://github.com/crate/crate/pull/10774#discussion_r524974205", "createdAt": "2020-11-17T08:39:40Z", "author": {"login": "seut"}, "path": "server/src/test/java/org/elasticsearch/gateway/ReplicaShardAllocatorIT.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.gateway;\n+\n+import static org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked;\n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.Matchers.equalTo;\n+import static org.hamcrest.Matchers.hasItem;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.elasticsearch.action.admin.indices.flush.SyncedFlushAction;\n+import org.elasticsearch.action.admin.indices.flush.SyncedFlushRequest;\n+import org.elasticsearch.action.admin.indices.flush.SyncedFlushResponse;\n+import org.elasticsearch.cluster.node.DiscoveryNode;\n+import org.elasticsearch.cluster.service.ClusterService;\n+import org.elasticsearch.common.Priority;\n+import org.elasticsearch.common.settings.Settings;\n+import org.elasticsearch.index.seqno.ReplicationTracker;\n+import org.elasticsearch.indices.recovery.PeerRecoveryTargetService;\n+import org.elasticsearch.plugins.Plugin;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalSettingsPlugin;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.transport.MockTransportService;\n+import org.elasticsearch.transport.TransportService;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+public class ReplicaShardAllocatorIT extends SQLTransportIntegrationTest {\n+\n+    @Override\n+    protected Collection<Class<? extends Plugin>> nodePlugins() {\n+        var nodePlugins = new ArrayList<>(super.nodePlugins());\n+        nodePlugins.add(MockTransportService.TestPlugin.class);\n+        nodePlugins.add(InternalSettingsPlugin.class);\n+        return nodePlugins;\n+    }\n+\n+    /**\n+     * Ensure that we fetch the latest shard store from the primary when a new node joins so we won't cancel the current recovery\n+     * for the copy on the newly joined node unless we can perform a noop recovery with that node.\n+     */\n+    @Test\n+    @Ignore(\"Fails currently, maybe missing backports\")\n+    public void testRecentPrimaryInformation() throws Exception {\n+        String indexName = \"test\";\n+        String nodeWithPrimary = internalCluster().startNode();\n+\n+        execute(\"\"\"\n+            create table doc.test (x int)\n+            clustered into 1 shards with (\n+                number_of_replicas = 1,\n+                \"recovery.file_based_threshold\" = 1.0,\n+                \"global_checkpoint_sync.interval\" = '100ms',\n+                \"unassigned.node_left.delayed_timeout\" = '1ms'\n+            )\n+        \"\"\");\n+        String nodeWithReplica = internalCluster().startDataOnlyNode();\n+        DiscoveryNode discoNodeWithReplica = internalCluster().getInstance(ClusterService.class, nodeWithReplica).localNode();\n+        Settings nodeWithReplicaSettings = internalCluster().dataPathSettings(nodeWithReplica);\n+        ensureGreen(indexName);\n+        execute(\"insert into doc.test (x) values (?)\", new Object[][] {\n+            new Object[] { randomIntBetween(10, 100) },\n+            new Object[] { randomIntBetween(10, 100) },\n+        });\n+        assertBusy(() -> {\n+            SyncedFlushResponse syncedFlushResponse = client()\n+                .execute(SyncedFlushAction.INSTANCE, new SyncedFlushRequest(indexName))\n+                .actionGet(5, TimeUnit.SECONDS);\n+            assertThat(syncedFlushResponse.successfulShards(), equalTo(2));\n+        });\n+        internalCluster().stopRandomNode(InternalTestCluster.nameFilter(nodeWithReplica));\n+        if (randomBoolean()) {\n+            execute(\"insert into doc.test (x) values (?)\", new Object[][] {\n+                new Object[] { randomIntBetween(10, 100) },\n+                new Object[] { randomIntBetween(10, 100) },\n+            });\n+        }\n+        CountDownLatch blockRecovery = new CountDownLatch(1);\n+        CountDownLatch recoveryStarted = new CountDownLatch(1);\n+        MockTransportService transportServiceOnPrimary\n+            = (MockTransportService) internalCluster().getInstance(TransportService.class, nodeWithPrimary);\n+        transportServiceOnPrimary.addSendBehavior((connection, requestId, action, request, options) -> {\n+            if (PeerRecoveryTargetService.Actions.FILES_INFO.equals(action)) {\n+                recoveryStarted.countDown();\n+                try {\n+                    blockRecovery.await(5, TimeUnit.SECONDS);\n+                } catch (InterruptedException e) {\n+                    Thread.currentThread().interrupt();\n+                }\n+            }\n+            connection.sendRequest(requestId, action, request, options);\n+        });\n+        try {\n+            String newNode = internalCluster().startDataOnlyNode();\n+            recoveryStarted.await(5, TimeUnit.SECONDS);\n+            // Index more documents and flush to destroy sync_id and remove the retention lease (as file_based_recovery_threshold reached).\n+            execute(\"insert into doc.test (x) values (?)\", new Object[][] {\n+                new Object[] { randomIntBetween(10, 100) },\n+                new Object[] { randomIntBetween(10, 100) },\n+            });\n+            execute(\"optimize table doc.test with (flush = true, max_num_segments = 1)\");\n+            assertBusy(() -> {\n+                execute(\"select unnest(retention_leases['leases']['id']) from sys.shards where table_name = 'test'\");\n+                for (var row : response.rows()) {\n+                    assertThat(row[0], not(equalTo((ReplicationTracker.getPeerRecoveryRetentionLeaseId(discoNodeWithReplica.getId())))));\n+                }\n+            });\n+            // AllocationService only calls GatewayAllocator if there are unassigned shards\n+            execute(\"\"\"\n+                create table doc.dummy (x int)\n+                with (\"routing.allocation.require.attr\" = 'not-found')\n+            \"\"\");\n+            internalCluster().startDataOnlyNode(nodeWithReplicaSettings);\n+            // need to wait for events to ensure the reroute has happened since we perform it async when a new node joins.\n+            client().admin().cluster()\n+                .prepareHealth(indexName)\n+                .setWaitForYellowStatus()\n+                .setWaitForEvents(Priority.LANGUID)\n+                .execute()\n+                .get(5, TimeUnit.SECONDS);\n+            blockRecovery.countDown();\n+            ensureGreen(indexName);\n+            assertThat(internalCluster().nodesInclude(indexName), hasItem(newNode));\n+            //for (RecoveryState recovery : client().admin().indices().prepareRecoveries(indexName).get().shardRecoveryStates().get(indexName)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk0Nzg4NQ=="}, "originalCommit": {"oid": "f139aabfaf154149b3c0d220d48789c476fb4454"}, "originalPosition": 149}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 912, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}