{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk0MTI4NTc4", "number": 10605, "title": "Add DiskDisruptionIT to integration tests", "bodyText": "Summary of the changes / Why this improves CrateDB\nBackport of DiskDisruptionIT introduced in elastic/elasticsearch@f27e808\nChecklist\n\n Added an entry in CHANGES.txt for user facing changes\n Updated documentation & sql_features table for user facing changes\n Touched code is covered by tests\n CLA is signed\n This does not contain breaking changes, or if it does:\n\nIt is released within a major release\nIt is recorded in CHANGES.txt\nIt was marked as deprecated in an earlier release if possible\nYou've thought about the consequences and other components are adapted\n(E.g. AdminUI)", "createdAt": "2020-09-28T13:06:04Z", "url": "https://github.com/crate/crate/pull/10605", "merged": true, "mergeCommit": {"oid": "37eb6518473595c1f22aa799f96f13d17533cd65"}, "closed": true, "closedAt": "2020-10-05T13:02:07Z", "author": {"login": "mkleen"}, "timelineItems": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdNrIrjgBqjM4MjA2ODcxNzU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdPi6tiABqjM4NDAxNTgxNDE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "52fc94c54a55608a8f83872c04480fb640298583", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/52fc94c54a55608a8f83872c04480fb640298583", "committedDate": "2020-09-29T16:44:08Z", "message": "wip"}, "afterCommit": {"oid": "6d6a38a84b89fcf6048a038554adb12b1aa677c1", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/6d6a38a84b89fcf6048a038554adb12b1aa677c1", "committedDate": "2020-09-29T16:53:22Z", "message": "Add DiskDisruptionIT to integration tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8f1003f619046a0d0c495f986b2e5840231b816a", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/8f1003f619046a0d0c495f986b2e5840231b816a", "committedDate": "2020-09-29T21:12:31Z", "message": "fix test"}, "afterCommit": {"oid": "3b20ad83c8ff0bbe1b63ac2acc3cfb2d810780bf", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/3b20ad83c8ff0bbe1b63ac2acc3cfb2d810780bf", "committedDate": "2020-09-30T11:50:12Z", "message": "cleanup"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bc24cb13fc7f312c7470ae62acbda2ded8373876", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/bc24cb13fc7f312c7470ae62acbda2ded8373876", "committedDate": "2020-09-30T11:52:06Z", "message": "cleanup"}, "afterCommit": {"oid": "44072398444cabca810730fcb4d0bc8be21c2b34", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/44072398444cabca810730fcb4d0bc8be21c2b34", "committedDate": "2020-10-02T09:07:59Z", "message": "Add DiskDisruptionIT to integration tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2f1c89293454ba7d60acb622a5044ea4e89eb33d", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/2f1c89293454ba7d60acb622a5044ea4e89eb33d", "committedDate": "2020-10-02T09:41:47Z", "message": "cleanup"}, "afterCommit": {"oid": "5fcfd9cb1b859469228fb05c2f081b08e5d93cd1", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/5fcfd9cb1b859469228fb05c2f081b08e5d93cd1", "committedDate": "2020-10-02T13:25:32Z", "message": "Add DiskDisruptionIT to integration tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5fcfd9cb1b859469228fb05c2f081b08e5d93cd1", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/5fcfd9cb1b859469228fb05c2f081b08e5d93cd1", "committedDate": "2020-10-02T13:25:32Z", "message": "Add DiskDisruptionIT to integration tests"}, "afterCommit": {"oid": "83965e3e129b6ad89036b0b115fa6919c8375390", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/83965e3e129b6ad89036b0b115fa6919c8375390", "committedDate": "2020-10-02T13:28:52Z", "message": "Add DiskDisruptionIT to integration tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxMTM4Njc2", "url": "https://github.com/crate/crate/pull/10605#pullrequestreview-501138676", "createdAt": "2020-10-02T13:33:41Z", "commit": {"oid": "83965e3e129b6ad89036b0b115fa6919c8375390"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxMzozMzo0MlrOHbtz0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxMzozMzo0MlrOHbtz0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODgyNDE0Nw==", "bodyText": "I removed useAutoGeneratedIDs because afaik primary keys can not be autogenerated using sql.", "url": "https://github.com/crate/crate/pull/10605#discussion_r498824147", "createdAt": "2020-10-02T13:33:42Z", "author": {"login": "mkleen"}, "path": "server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.test;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import com.carrotsearch.randomizedtesting.generators.RandomNumbers;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.testing.SQLTransportExecutor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.logging.log4j.util.Supplier;\n+import org.elasticsearch.common.util.concurrent.ConcurrentCollections;\n+import org.junit.Assert;\n+\n+import javax.annotation.Nullable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Locale;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+\n+import static org.hamcrest.Matchers.emptyIterable;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class BackgroundIndexer implements AutoCloseable {\n+\n+    private final Logger logger = LogManager.getLogger(getClass());\n+\n+    final Thread[] writers;\n+    final SQLTransportExecutor sqlExecutor;\n+    final CountDownLatch stopLatch;\n+    final Collection<Exception> failures = new ArrayList<>();\n+    final AtomicBoolean stop = new AtomicBoolean(false);\n+    final AtomicLong idGenerator = new AtomicLong();\n+    final CountDownLatch startLatch = new CountDownLatch(1);\n+    final AtomicBoolean hasBudget = new AtomicBoolean(false); // when set to true, writers will acquire writes from a semaphore\n+    final Semaphore availableBudget = new Semaphore(0);\n+    private final Set<String> ids = ConcurrentCollections.newConcurrentSet();\n+    private volatile Consumer<Exception> failureAssertion = null;\n+\n+    volatile int minFieldSize = 10;\n+    volatile int maxFieldSize = 140;\n+\n+    /**\n+     * Start indexing in the background using a random number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table     table name to write data\n+     * @param column    column name to write data\n+     * @param client    client to use\n+     * @param numOfDocs number of document to index before pausing. Set to -1 to have no limit.\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs) {\n+        this(table, column, client, numOfDocs, RandomizedTest.scaledRandomIntBetween(2, 5));\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param client      client to use\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs, final int writerCount) {\n+        this(table, column, client, numOfDocs, writerCount, true, null);\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     * @param autoStart   set to true to start indexing as soon as all threads have been created.\n+     * @param random      random instance to use\n+     */\n+    public BackgroundIndexer(final String table, final String column, final SQLTransportExecutor sqlExecutor, final int numOfDocs, final int writerCount,\n+                             boolean autoStart, Random random) {\n+        if (random == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "83965e3e129b6ad89036b0b115fa6919c8375390"}, "originalPosition": 110}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "83965e3e129b6ad89036b0b115fa6919c8375390", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/83965e3e129b6ad89036b0b115fa6919c8375390", "committedDate": "2020-10-02T13:28:52Z", "message": "Add DiskDisruptionIT to integration tests"}, "afterCommit": {"oid": "803846af4c06be3f5b09c4f5c7a3b54877999888", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/803846af4c06be3f5b09c4f5c7a3b54877999888", "committedDate": "2020-10-02T13:41:40Z", "message": "Add DiskDisruptionIT to integration tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "803846af4c06be3f5b09c4f5c7a3b54877999888", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/803846af4c06be3f5b09c4f5c7a3b54877999888", "committedDate": "2020-10-02T13:41:40Z", "message": "Add DiskDisruptionIT to integration tests"}, "afterCommit": {"oid": "92b78b3fe2a2bc96c2b18d4fbd8c42a4b9fa9d76", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/92b78b3fe2a2bc96c2b18d4fbd8c42a4b9fa9d76", "committedDate": "2020-10-02T13:48:28Z", "message": "Add DiskDisruptionIT to integration tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNzMyOTc3", "url": "https://github.com/crate/crate/pull/10605#pullrequestreview-501732977", "createdAt": "2020-10-05T05:38:20Z", "commit": {"oid": "92b78b3fe2a2bc96c2b18d4fbd8c42a4b9fa9d76"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNTozODoyMFrOHcOLbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNTozODoyMFrOHcOLbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM1NDQ3OA==", "bodyText": "This makes sure file disruption ends after all nodes are stopped.", "url": "https://github.com/crate/crate/pull/10605#discussion_r499354478", "createdAt": "2020-10-05T05:38:20Z", "author": {"login": "mkleen"}, "path": "server/src/test/java/org/elasticsearch/test/InternalTestCluster.java", "diffHunk": "@@ -1693,6 +1693,7 @@ public synchronized void fullRestart(RestartCallback callback) throws Exception\n         }\n \n         assert nodesByRoles.values().stream().mapToInt(List::size).sum() == nodes.size();\n+        callback.onAllNodesStopped();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92b78b3fe2a2bc96c2b18d4fbd8c42a4b9fa9d76"}, "originalPosition": 4}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "92b78b3fe2a2bc96c2b18d4fbd8c42a4b9fa9d76", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/92b78b3fe2a2bc96c2b18d4fbd8c42a4b9fa9d76", "committedDate": "2020-10-02T13:48:28Z", "message": "Add DiskDisruptionIT to integration tests"}, "afterCommit": {"oid": "c5b6b7597901d9f0b4b07b3a2faa0c6c5b8f3614", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/c5b6b7597901d9f0b4b07b3a2faa0c6c5b8f3614", "committedDate": "2020-10-05T05:40:08Z", "message": "Add DiskDisruptionIT to integration tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c5b6b7597901d9f0b4b07b3a2faa0c6c5b8f3614", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/c5b6b7597901d9f0b4b07b3a2faa0c6c5b8f3614", "committedDate": "2020-10-05T05:40:08Z", "message": "Add DiskDisruptionIT to integration tests"}, "afterCommit": {"oid": "00b983c496ae824e23e9a4a9b9c67733289123c5", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/00b983c496ae824e23e9a4a9b9c67733289123c5", "committedDate": "2020-10-05T05:47:41Z", "message": "Add DiskDisruptionIT to integration tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxODMwODg1", "url": "https://github.com/crate/crate/pull/10605#pullrequestreview-501830885", "createdAt": "2020-10-05T08:33:03Z", "commit": {"oid": "00b983c496ae824e23e9a4a9b9c67733289123c5"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwODozMzowM1rOHcSuHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwODo1NDozOFrOHcTgAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQyODg5NA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                var response = sqlExecutor.execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n          \n          \n            \n                                                                   \"from sys.shards where table_name='test'\", null).actionGet();\n          \n          \n            \n                                var response = execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n          \n          \n            \n                                                                   \"from sys.shards where table_name='test'\");", "url": "https://github.com/crate/crate/pull/10605#discussion_r499428894", "createdAt": "2020-10-05T08:33:03Z", "author": {"login": "seut"}, "path": "server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.crate.integrationtests.disruption.discovery;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+import io.crate.integrationtests.Setup;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.index.seqno.SequenceNumbers;\n+import org.elasticsearch.test.BackgroundIndexer;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.FileSystem;\n+import java.nio.file.FileSystems;\n+import java.nio.file.OpenOption;\n+import java.nio.file.Path;\n+import java.nio.file.attribute.FileAttribute;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+@SQLTransportIntegrationTest.Slow\n+public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n+\n+    private static DisruptTranslogFileSystemProvider disruptTranslogFileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installDisruptTranslogFS() {\n+        FileSystem current = FileSystems.getDefault();\n+        disruptTranslogFileSystemProvider = new DisruptTranslogFileSystemProvider(current);\n+        PathUtilsForTesting.installMock(disruptTranslogFileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeDisruptTranslogFS() {\n+        PathUtilsForTesting.teardown();\n+    }\n+\n+    void injectTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(true);\n+    }\n+\n+    @After\n+    void stopTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(false);\n+    }\n+\n+    static class DisruptTranslogFileSystemProvider extends FilterFileSystemProvider {\n+\n+        AtomicBoolean injectFailures = new AtomicBoolean();\n+\n+        DisruptTranslogFileSystemProvider(FileSystem inner) {\n+            super(\"disrupttranslog://\", inner);\n+        }\n+\n+        @Override\n+        public FileChannel newFileChannel(Path path, Set<? extends OpenOption> options, FileAttribute<?>... attrs) throws IOException {\n+            if (injectFailures.get() && path.toString().endsWith(\".ckp\")) {\n+                // prevents checkpoint file to be updated\n+                throw new IOException(\"fake IOException\");\n+            }\n+            return super.newFileChannel(path, options, attrs);\n+        }\n+    }\n+\n+    /**\n+     * This test checks that all operations below the global checkpoint are properly persisted.\n+     * It simulates a full power outage by preventing translog checkpoint files to be written and restart the cluster. This means that\n+     * all un-fsynced data will be lost.\n+     */\n+    public void testGlobalCheckpointIsSafe() throws Exception {\n+        startCluster(rarely() ? 5 : 3);\n+\n+        var numberOfShards = 1 + randomInt(2);\n+        var numberOfReplicas = randomInt(2);\n+\n+        execute(\"create table test (id int primary key, data string) clustered into \" + numberOfShards + \" shards \" +\n+                            \"with (number_of_replicas = ?)\", new Object[] {numberOfReplicas});\n+\n+        ensureGreen();\n+\n+        AtomicBoolean stopGlobalCheckpointFetcher = new AtomicBoolean();\n+\n+        Map<Integer, Long> shardToGcp = new ConcurrentHashMap<>();\n+        for (int i = 0; i < numberOfShards; i++) {\n+            shardToGcp.put(i, SequenceNumbers.NO_OPS_PERFORMED);\n+        }\n+        final Thread globalCheckpointSampler = new Thread(() -> {\n+            while (stopGlobalCheckpointFetcher.get() == false) {\n+                try {\n+                    var response = sqlExecutor.execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n+                                                       \"from sys.shards where table_name='test'\", null).actionGet();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00b983c496ae824e23e9a4a9b9c67733289123c5"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQzMDI5Nw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                   execute(\"select distinct id, seq_no_stats['max_seq_no'] from sys.shards where table_name='test' and \" +\n          \n          \n            \n                    execute(\"select distinct id, seq_no_stats['max_seq_no'] from sys.shards where table_name='test' and \" +", "url": "https://github.com/crate/crate/pull/10605#discussion_r499430297", "createdAt": "2020-10-05T08:35:31Z", "author": {"login": "seut"}, "path": "server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.crate.integrationtests.disruption.discovery;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+import io.crate.integrationtests.Setup;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.index.seqno.SequenceNumbers;\n+import org.elasticsearch.test.BackgroundIndexer;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.FileSystem;\n+import java.nio.file.FileSystems;\n+import java.nio.file.OpenOption;\n+import java.nio.file.Path;\n+import java.nio.file.attribute.FileAttribute;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+@SQLTransportIntegrationTest.Slow\n+public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n+\n+    private static DisruptTranslogFileSystemProvider disruptTranslogFileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installDisruptTranslogFS() {\n+        FileSystem current = FileSystems.getDefault();\n+        disruptTranslogFileSystemProvider = new DisruptTranslogFileSystemProvider(current);\n+        PathUtilsForTesting.installMock(disruptTranslogFileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeDisruptTranslogFS() {\n+        PathUtilsForTesting.teardown();\n+    }\n+\n+    void injectTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(true);\n+    }\n+\n+    @After\n+    void stopTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(false);\n+    }\n+\n+    static class DisruptTranslogFileSystemProvider extends FilterFileSystemProvider {\n+\n+        AtomicBoolean injectFailures = new AtomicBoolean();\n+\n+        DisruptTranslogFileSystemProvider(FileSystem inner) {\n+            super(\"disrupttranslog://\", inner);\n+        }\n+\n+        @Override\n+        public FileChannel newFileChannel(Path path, Set<? extends OpenOption> options, FileAttribute<?>... attrs) throws IOException {\n+            if (injectFailures.get() && path.toString().endsWith(\".ckp\")) {\n+                // prevents checkpoint file to be updated\n+                throw new IOException(\"fake IOException\");\n+            }\n+            return super.newFileChannel(path, options, attrs);\n+        }\n+    }\n+\n+    /**\n+     * This test checks that all operations below the global checkpoint are properly persisted.\n+     * It simulates a full power outage by preventing translog checkpoint files to be written and restart the cluster. This means that\n+     * all un-fsynced data will be lost.\n+     */\n+    public void testGlobalCheckpointIsSafe() throws Exception {\n+        startCluster(rarely() ? 5 : 3);\n+\n+        var numberOfShards = 1 + randomInt(2);\n+        var numberOfReplicas = randomInt(2);\n+\n+        execute(\"create table test (id int primary key, data string) clustered into \" + numberOfShards + \" shards \" +\n+                            \"with (number_of_replicas = ?)\", new Object[] {numberOfReplicas});\n+\n+        ensureGreen();\n+\n+        AtomicBoolean stopGlobalCheckpointFetcher = new AtomicBoolean();\n+\n+        Map<Integer, Long> shardToGcp = new ConcurrentHashMap<>();\n+        for (int i = 0; i < numberOfShards; i++) {\n+            shardToGcp.put(i, SequenceNumbers.NO_OPS_PERFORMED);\n+        }\n+        final Thread globalCheckpointSampler = new Thread(() -> {\n+            while (stopGlobalCheckpointFetcher.get() == false) {\n+                try {\n+                    var response = sqlExecutor.execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n+                                                       \"from sys.shards where table_name='test'\", null).actionGet();\n+                    for (var row : response.rows()) {\n+                        final int shardId = (int) row[0];\n+                        final long globalCheckpoint = (long) row[1];\n+                        shardToGcp.compute(shardId, (i, v) -> Math.max(v, globalCheckpoint));\n+                    }\n+                } catch (Exception e) {\n+                    // ignore\n+                    logger.debug(\"failed to fetch shard stats\", e);\n+                }\n+            }\n+        });\n+\n+        globalCheckpointSampler.start();\n+\n+        try (BackgroundIndexer indexer = new BackgroundIndexer(\"test\", \"data\", sqlExecutor, -1, RandomizedTest.scaledRandomIntBetween(2, 5),\n+                                                               false, random())) {\n+            indexer.setRequestTimeout(TimeValue.ZERO);\n+            indexer.setIgnoreIndexingFailures(true);\n+            indexer.setFailureAssertion(e -> {});\n+            indexer.start(-1);\n+\n+            waitForDocs(randomIntBetween(1, 100), indexer, \"test\");\n+\n+            logger.info(\"injecting failures\");\n+            injectTranslogFailures();\n+            logger.info(\"stopping indexing\");\n+        }\n+\n+        logger.info(\"full cluster restart\");\n+        internalCluster().fullRestart(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public void onAllNodesStopped() {\n+                logger.info(\"stopping failures\");\n+                stopTranslogFailures();\n+            }\n+\n+        });\n+\n+        stopGlobalCheckpointFetcher.set(true);\n+\n+        logger.info(\"waiting for global checkpoint sampler\");\n+        globalCheckpointSampler.join();\n+\n+        logger.info(\"waiting for green\");\n+        ensureGreen();\n+\n+       execute(\"select distinct id, seq_no_stats['max_seq_no'] from sys.shards where table_name='test' and \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00b983c496ae824e23e9a4a9b9c67733289123c5"}, "originalPosition": 172}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQzMjg5NQ==", "bodyText": "Any reason why this method is not added to the ESIntegTestCase like in the ES upstream?", "url": "https://github.com/crate/crate/pull/10605#discussion_r499432895", "createdAt": "2020-10-05T08:40:20Z", "author": {"login": "seut"}, "path": "server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java", "diffHunk": "@@ -0,0 +1,228 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.crate.integrationtests.disruption.discovery;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+import io.crate.integrationtests.Setup;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.index.seqno.SequenceNumbers;\n+import org.elasticsearch.test.BackgroundIndexer;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.elasticsearch.test.junit.annotations.TestLogging;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.FileSystem;\n+import java.nio.file.FileSystems;\n+import java.nio.file.OpenOption;\n+import java.nio.file.Path;\n+import java.nio.file.attribute.FileAttribute;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+@SQLTransportIntegrationTest.Slow\n+public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n+\n+    private static DisruptTranslogFileSystemProvider disruptTranslogFileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installDisruptTranslogFS() {\n+        FileSystem current = FileSystems.getDefault();\n+        disruptTranslogFileSystemProvider = new DisruptTranslogFileSystemProvider(current);\n+        PathUtilsForTesting.installMock(disruptTranslogFileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeDisruptTranslogFS() {\n+        PathUtilsForTesting.teardown();\n+    }\n+\n+    void injectTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(true);\n+    }\n+\n+    @After\n+    void stopTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(false);\n+    }\n+\n+    static class DisruptTranslogFileSystemProvider extends FilterFileSystemProvider {\n+\n+        AtomicBoolean injectFailures = new AtomicBoolean();\n+\n+        DisruptTranslogFileSystemProvider(FileSystem inner) {\n+            super(\"disrupttranslog://\", inner);\n+        }\n+\n+        @Override\n+        public FileChannel newFileChannel(Path path, Set<? extends OpenOption> options, FileAttribute<?>... attrs) throws IOException {\n+            if (injectFailures.get() && path.toString().endsWith(\".ckp\")) {\n+                // prevents checkpoint file to be updated\n+                throw new IOException(\"fake IOException\");\n+            }\n+            return super.newFileChannel(path, options, attrs);\n+        }\n+    }\n+\n+    /**\n+     * This test checks that all operations below the global checkpoint are properly persisted.\n+     * It simulates a full power outage by preventing translog checkpoint files to be written and restart the cluster. This means that\n+     * all un-fsynced data will be lost.\n+     */\n+    public void testGlobalCheckpointIsSafe() throws Exception {\n+        startCluster(rarely() ? 5 : 3);\n+\n+        var numberOfShards = 1 + randomInt(2);\n+        var numberOfReplicas = randomInt(2);\n+\n+        execute(\"create table test (id int primary key, data string) clustered into \" + numberOfShards + \" shards \" +\n+                            \"with (number_of_replicas = ?)\", new Object[] {numberOfReplicas});\n+\n+        ensureGreen();\n+\n+        AtomicBoolean stopGlobalCheckpointFetcher = new AtomicBoolean();\n+\n+        Map<Integer, Long> shardToGcp = new ConcurrentHashMap<>();\n+        for (int i = 0; i < numberOfShards; i++) {\n+            shardToGcp.put(i, SequenceNumbers.NO_OPS_PERFORMED);\n+        }\n+        final Thread globalCheckpointSampler = new Thread(() -> {\n+            while (stopGlobalCheckpointFetcher.get() == false) {\n+                try {\n+                    var response = sqlExecutor.execute(\"select id, seq_no_stats['global_checkpoint'] \" +\n+                                                       \"from sys.shards where table_name='test'\", null).actionGet();\n+                    for (var row : response.rows()) {\n+                        final int shardId = (int) row[0];\n+                        final long globalCheckpoint = (long) row[1];\n+                        shardToGcp.compute(shardId, (i, v) -> Math.max(v, globalCheckpoint));\n+                    }\n+                } catch (Exception e) {\n+                    // ignore\n+                    logger.debug(\"failed to fetch shard stats\", e);\n+                }\n+            }\n+        });\n+\n+        globalCheckpointSampler.start();\n+\n+        try (BackgroundIndexer indexer = new BackgroundIndexer(\"test\", \"data\", sqlExecutor, -1, RandomizedTest.scaledRandomIntBetween(2, 5),\n+                                                               false, random())) {\n+            indexer.setRequestTimeout(TimeValue.ZERO);\n+            indexer.setIgnoreIndexingFailures(true);\n+            indexer.setFailureAssertion(e -> {});\n+            indexer.start(-1);\n+\n+            waitForDocs(randomIntBetween(1, 100), indexer, \"test\");\n+\n+            logger.info(\"injecting failures\");\n+            injectTranslogFailures();\n+            logger.info(\"stopping indexing\");\n+        }\n+\n+        logger.info(\"full cluster restart\");\n+        internalCluster().fullRestart(new InternalTestCluster.RestartCallback() {\n+\n+            @Override\n+            public void onAllNodesStopped() {\n+                logger.info(\"stopping failures\");\n+                stopTranslogFailures();\n+            }\n+\n+        });\n+\n+        stopGlobalCheckpointFetcher.set(true);\n+\n+        logger.info(\"waiting for global checkpoint sampler\");\n+        globalCheckpointSampler.join();\n+\n+        logger.info(\"waiting for green\");\n+        ensureGreen();\n+\n+       execute(\"select distinct id, seq_no_stats['max_seq_no'] from sys.shards where table_name='test' and \" +\n+               \"routing_state in ('STARTED', 'RELOCATING')\");\n+\n+        assertThat(response.rowCount(), is((long) numberOfShards));\n+\n+        for (var row : response.rows()) {\n+            final int shardId = (int) row[0];\n+            final long maxSeqNo = (long) row[1];\n+            assertThat(maxSeqNo, greaterThanOrEqualTo(shardToGcp.get(shardId)));\n+        }\n+    }\n+\n+    /**\n+     * Waits until at least a give number of document is visible for searchers\n+     *\n+     * @param numDocs number of documents to wait for\n+     * @param indexer a {@link org.elasticsearch.test.BackgroundIndexer}. It will be first checked for documents indexed.\n+     *                This saves on unneeded searches.\n+     */\n+    public void waitForDocs(final long numDocs, final BackgroundIndexer indexer, String table) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00b983c496ae824e23e9a4a9b9c67733289123c5"}, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQzODA3MQ==", "bodyText": "If no primary key is defined on the table, the internal id _id will be generated, otherwise the values of the PK cols are used to compute a _id value.\nAs the _id column can be selected, I see no issue on keeping the useAutogeneratedIDs feature. Do I miss anything?", "url": "https://github.com/crate/crate/pull/10605#discussion_r499438071", "createdAt": "2020-10-05T08:48:46Z", "author": {"login": "seut"}, "path": "server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java", "diffHunk": "@@ -0,0 +1,322 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.test;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import com.carrotsearch.randomizedtesting.generators.RandomNumbers;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.testing.SQLTransportExecutor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.logging.log4j.util.Supplier;\n+import org.elasticsearch.common.util.concurrent.ConcurrentCollections;\n+import org.junit.Assert;\n+\n+import javax.annotation.Nullable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Locale;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+\n+import static org.hamcrest.Matchers.emptyIterable;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class BackgroundIndexer implements AutoCloseable {\n+\n+    private final Logger logger = LogManager.getLogger(getClass());\n+\n+    final Thread[] writers;\n+    final SQLTransportExecutor sqlExecutor;\n+    final CountDownLatch stopLatch;\n+    final Collection<Exception> failures = new ArrayList<>();\n+    final AtomicBoolean stop = new AtomicBoolean(false);\n+    final AtomicLong idGenerator = new AtomicLong();\n+    final CountDownLatch startLatch = new CountDownLatch(1);\n+    final AtomicBoolean hasBudget = new AtomicBoolean(false); // when set to true, writers will acquire writes from a semaphore\n+    final Semaphore availableBudget = new Semaphore(0);\n+    private final Set<String> ids = ConcurrentCollections.newConcurrentSet();\n+    private volatile Consumer<Exception> failureAssertion = null;\n+\n+    volatile int minFieldSize = 10;\n+    volatile int maxFieldSize = 140;\n+\n+    /**\n+     * Start indexing in the background using a random number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table     table name to write data\n+     * @param column    column name to write data\n+     * @param client    client to use\n+     * @param numOfDocs number of document to index before pausing. Set to -1 to have no limit.\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs) {\n+        this(table, column, client, numOfDocs, RandomizedTest.scaledRandomIntBetween(2, 5));\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param client      client to use\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs, final int writerCount) {\n+        this(table, column, client, numOfDocs, writerCount, true, null);\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     * @param autoStart   set to true to start indexing as soon as all threads have been created.\n+     * @param random      random instance to use\n+     */\n+    public BackgroundIndexer(final String table, final String column, final SQLTransportExecutor sqlExecutor, final int numOfDocs, final int writerCount,\n+                             boolean autoStart, Random random) {\n+        if (random == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODgyNDE0Nw=="}, "originalCommit": {"oid": "83965e3e129b6ad89036b0b115fa6919c8375390"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MDAzOA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                                var response = sqlExecutor.execute(insert, insertData).actionGet();\n          \n          \n            \n                                                var response = sqlExecutor.exec(insert, insertData);\n          \n      \n    \n    \n  \n\nOtherwise, please also keep in mind to always add proper timeouts to any blocking calls in general to let if fail early.", "url": "https://github.com/crate/crate/pull/10605#discussion_r499440038", "createdAt": "2020-10-05T08:51:57Z", "author": {"login": "seut"}, "path": "server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.test;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import com.carrotsearch.randomizedtesting.generators.RandomNumbers;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.testing.SQLTransportExecutor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.logging.log4j.util.Supplier;\n+import org.elasticsearch.common.util.concurrent.ConcurrentCollections;\n+import org.junit.Assert;\n+\n+import javax.annotation.Nullable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Locale;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+\n+import static org.hamcrest.Matchers.emptyIterable;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class BackgroundIndexer implements AutoCloseable {\n+\n+    private final Logger logger = LogManager.getLogger(getClass());\n+\n+    final Thread[] writers;\n+    final SQLTransportExecutor sqlExecutor;\n+    final CountDownLatch stopLatch;\n+    final Collection<Exception> failures = new ArrayList<>();\n+    final AtomicBoolean stop = new AtomicBoolean(false);\n+    final AtomicLong idGenerator = new AtomicLong();\n+    final CountDownLatch startLatch = new CountDownLatch(1);\n+    final AtomicBoolean hasBudget = new AtomicBoolean(false); // when set to true, writers will acquire writes from a semaphore\n+    final Semaphore availableBudget = new Semaphore(0);\n+    private final Set<String> ids = ConcurrentCollections.newConcurrentSet();\n+    private volatile Consumer<Exception> failureAssertion = null;\n+\n+    volatile int minFieldSize = 10;\n+    volatile int maxFieldSize = 140;\n+\n+    /**\n+     * Start indexing in the background using a random number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table     table name to write data\n+     * @param column    column name to write data\n+     * @param client    client to use\n+     * @param numOfDocs number of document to index before pausing. Set to -1 to have no limit.\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs) {\n+        this(table, column, client, numOfDocs, RandomizedTest.scaledRandomIntBetween(2, 5));\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param client      client to use\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs, final int writerCount) {\n+        this(table, column, client, numOfDocs, writerCount, true, null);\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     * @param autoStart   set to true to start indexing as soon as all threads have been created.\n+     * @param random      random instance to use\n+     */\n+    public BackgroundIndexer(final String table, final String column, final SQLTransportExecutor sqlExecutor, final int numOfDocs, final int writerCount,\n+                             boolean autoStart, Random random) {\n+        if (random == null) {\n+            random = RandomizedTest.getRandom();\n+        }\n+        this.sqlExecutor = sqlExecutor;\n+        writers = new Thread[writerCount];\n+        stopLatch = new CountDownLatch(writers.length);\n+        logger.info(\"--> creating {} indexing threads (auto start: [{}], numOfDocs: [{}])\", writerCount, autoStart, numOfDocs);\n+        for (int i = 0; i < writers.length; i++) {\n+            final int indexerId = i;\n+            final boolean batch = random.nextBoolean();\n+            final Random threadRandom = new Random(random.nextLong());\n+            writers[i] = new Thread() {\n+                @Override\n+                public void run() {\n+                    long id = -1;\n+                    try {\n+                        startLatch.await();\n+                        logger.info(\"**** starting indexing thread {}\", indexerId);\n+                        while (!stop.get()) {\n+                            if (batch) {\n+                                int batchSize = threadRandom.nextInt(20) + 1;\n+                                if (hasBudget.get()) {\n+                                    // always try to get at least one\n+                                    batchSize = Math.max(Math.min(batchSize, availableBudget.availablePermits()), 1);\n+                                    if (!availableBudget.tryAcquire(batchSize, 250, TimeUnit.MILLISECONDS)) {\n+                                        // time out -> check if we have to stop.\n+                                        continue;\n+                                    }\n+                                }\n+                                var insertData = new Object[batchSize];\n+                                for (int i = 0; i < batchSize; i++) {\n+                                    insertData[i] = generateValues(idGenerator.incrementAndGet(), threadRandom);\n+                                }\n+                                try {\n+                                    var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);\n+                                    var response = sqlExecutor.execute(insert, insertData).actionGet();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00b983c496ae824e23e9a4a9b9c67733289123c5"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MDI1OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                                var response = sqlExecutor.execute(insert, generateValues(idGenerator.incrementAndGet(), threadRandom)).actionGet();\n          \n          \n            \n                                                var response = sqlExecutor.exec(insert, generateValues(idGenerator.incrementAndGet(), threadRandom));", "url": "https://github.com/crate/crate/pull/10605#discussion_r499440259", "createdAt": "2020-10-05T08:52:18Z", "author": {"login": "seut"}, "path": "server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.test;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import com.carrotsearch.randomizedtesting.generators.RandomNumbers;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.testing.SQLTransportExecutor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.logging.log4j.util.Supplier;\n+import org.elasticsearch.common.util.concurrent.ConcurrentCollections;\n+import org.junit.Assert;\n+\n+import javax.annotation.Nullable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Locale;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+\n+import static org.hamcrest.Matchers.emptyIterable;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class BackgroundIndexer implements AutoCloseable {\n+\n+    private final Logger logger = LogManager.getLogger(getClass());\n+\n+    final Thread[] writers;\n+    final SQLTransportExecutor sqlExecutor;\n+    final CountDownLatch stopLatch;\n+    final Collection<Exception> failures = new ArrayList<>();\n+    final AtomicBoolean stop = new AtomicBoolean(false);\n+    final AtomicLong idGenerator = new AtomicLong();\n+    final CountDownLatch startLatch = new CountDownLatch(1);\n+    final AtomicBoolean hasBudget = new AtomicBoolean(false); // when set to true, writers will acquire writes from a semaphore\n+    final Semaphore availableBudget = new Semaphore(0);\n+    private final Set<String> ids = ConcurrentCollections.newConcurrentSet();\n+    private volatile Consumer<Exception> failureAssertion = null;\n+\n+    volatile int minFieldSize = 10;\n+    volatile int maxFieldSize = 140;\n+\n+    /**\n+     * Start indexing in the background using a random number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table     table name to write data\n+     * @param column    column name to write data\n+     * @param client    client to use\n+     * @param numOfDocs number of document to index before pausing. Set to -1 to have no limit.\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs) {\n+        this(table, column, client, numOfDocs, RandomizedTest.scaledRandomIntBetween(2, 5));\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param client      client to use\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs, final int writerCount) {\n+        this(table, column, client, numOfDocs, writerCount, true, null);\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     * @param autoStart   set to true to start indexing as soon as all threads have been created.\n+     * @param random      random instance to use\n+     */\n+    public BackgroundIndexer(final String table, final String column, final SQLTransportExecutor sqlExecutor, final int numOfDocs, final int writerCount,\n+                             boolean autoStart, Random random) {\n+        if (random == null) {\n+            random = RandomizedTest.getRandom();\n+        }\n+        this.sqlExecutor = sqlExecutor;\n+        writers = new Thread[writerCount];\n+        stopLatch = new CountDownLatch(writers.length);\n+        logger.info(\"--> creating {} indexing threads (auto start: [{}], numOfDocs: [{}])\", writerCount, autoStart, numOfDocs);\n+        for (int i = 0; i < writers.length; i++) {\n+            final int indexerId = i;\n+            final boolean batch = random.nextBoolean();\n+            final Random threadRandom = new Random(random.nextLong());\n+            writers[i] = new Thread() {\n+                @Override\n+                public void run() {\n+                    long id = -1;\n+                    try {\n+                        startLatch.await();\n+                        logger.info(\"**** starting indexing thread {}\", indexerId);\n+                        while (!stop.get()) {\n+                            if (batch) {\n+                                int batchSize = threadRandom.nextInt(20) + 1;\n+                                if (hasBudget.get()) {\n+                                    // always try to get at least one\n+                                    batchSize = Math.max(Math.min(batchSize, availableBudget.availablePermits()), 1);\n+                                    if (!availableBudget.tryAcquire(batchSize, 250, TimeUnit.MILLISECONDS)) {\n+                                        // time out -> check if we have to stop.\n+                                        continue;\n+                                    }\n+                                }\n+                                var insertData = new Object[batchSize];\n+                                for (int i = 0; i < batchSize; i++) {\n+                                    insertData[i] = generateValues(idGenerator.incrementAndGet(), threadRandom);\n+                                }\n+                                try {\n+                                    var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);\n+                                    var response = sqlExecutor.execute(insert, insertData).actionGet();\n+                                    for (var generatedId : response.rows()[0]) {\n+                                        ids.add((String) generatedId);\n+                                    }\n+                                } catch (Exception e) {\n+                                    if (ignoreIndexingFailures == false) {\n+                                        throw e;\n+                                    }\n+                                }\n+                            } else {\n+                                if (hasBudget.get() && !availableBudget.tryAcquire(250, TimeUnit.MILLISECONDS)) {\n+                                    // time out -> check if we have to stop.\n+                                    continue;\n+                                }\n+                                try {\n+                                    var insert = String.format(Locale.ENGLISH, \"insert into %s (id, %s) values(?, ?) returning _id\", table, column);\n+                                    var response = sqlExecutor.execute(insert, generateValues(idGenerator.incrementAndGet(), threadRandom)).actionGet();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00b983c496ae824e23e9a4a9b9c67733289123c5"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0MTY2Ng==", "bodyText": "In the upstream code, this line is above the assertion line. Any reason for this change?", "url": "https://github.com/crate/crate/pull/10605#discussion_r499441666", "createdAt": "2020-10-05T08:54:38Z", "author": {"login": "seut"}, "path": "server/src/test/java/org/elasticsearch/test/InternalTestCluster.java", "diffHunk": "@@ -1693,6 +1693,7 @@ public synchronized void fullRestart(RestartCallback callback) throws Exception\n         }\n \n         assert nodesByRoles.values().stream().mapToInt(List::size).sum() == nodes.size();\n+        callback.onAllNodesStopped();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00b983c496ae824e23e9a4a9b9c67733289123c5"}, "originalPosition": 4}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "237c32a16fa21db66806dd4894388894cd0214dd", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/237c32a16fa21db66806dd4894388894cd0214dd", "committedDate": "2020-10-05T11:07:03Z", "message": "Fix SqlExecutor execution"}, "afterCommit": {"oid": "bd4045719c3e8fc23f35a833ace4263e92fc212c", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/bd4045719c3e8fc23f35a833ace4263e92fc212c", "committedDate": "2020-10-05T11:14:00Z", "message": "Fix SqlExecutor execution"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxOTkxNjIz", "url": "https://github.com/crate/crate/pull/10605#pullrequestreview-501991623", "createdAt": "2020-10-05T12:12:33Z", "commit": {"oid": "8febaf2933a2b9f4b8cfe7a86d24a324b62680cd"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMjoxMjozM1rOHcaOLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMjoxNDo0MFrOHcaS7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU1MTc5MQ==", "bodyText": "could use execute(..) instead. sorry my fault, wrong suggestion by me ;)", "url": "https://github.com/crate/crate/pull/10605#discussion_r499551791", "createdAt": "2020-10-05T12:12:33Z", "author": {"login": "seut"}, "path": "server/src/test/java/io/crate/integrationtests/disruption/discovery/DiskDisruptionIT.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package io.crate.integrationtests.disruption.discovery;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.integrationtests.SQLTransportIntegrationTest;\n+import org.apache.lucene.mockfile.FilterFileSystemProvider;\n+import org.elasticsearch.common.io.PathUtilsForTesting;\n+import org.elasticsearch.index.seqno.SequenceNumbers;\n+import org.elasticsearch.test.BackgroundIndexer;\n+import org.elasticsearch.test.ESIntegTestCase;\n+import org.elasticsearch.test.InternalTestCluster;\n+import org.junit.After;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.FileSystem;\n+import java.nio.file.FileSystems;\n+import java.nio.file.OpenOption;\n+import java.nio.file.Path;\n+import java.nio.file.attribute.FileAttribute;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static org.hamcrest.Matchers.greaterThanOrEqualTo;\n+import static org.hamcrest.Matchers.is;\n+\n+@ESIntegTestCase.ClusterScope(scope = ESIntegTestCase.Scope.TEST, numDataNodes = 0)\n+@SQLTransportIntegrationTest.Slow\n+public class DiskDisruptionIT extends AbstractDisruptionTestCase {\n+\n+    private static DisruptTranslogFileSystemProvider disruptTranslogFileSystemProvider;\n+\n+    @BeforeClass\n+    public static void installDisruptTranslogFS() {\n+        FileSystem current = FileSystems.getDefault();\n+        disruptTranslogFileSystemProvider = new DisruptTranslogFileSystemProvider(current);\n+        PathUtilsForTesting.installMock(disruptTranslogFileSystemProvider.getFileSystem(null));\n+    }\n+\n+    @AfterClass\n+    public static void removeDisruptTranslogFS() {\n+        PathUtilsForTesting.teardown();\n+    }\n+\n+    void injectTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(true);\n+    }\n+\n+    @After\n+    void stopTranslogFailures() {\n+        disruptTranslogFileSystemProvider.injectFailures.set(false);\n+    }\n+\n+    static class DisruptTranslogFileSystemProvider extends FilterFileSystemProvider {\n+\n+        AtomicBoolean injectFailures = new AtomicBoolean();\n+\n+        DisruptTranslogFileSystemProvider(FileSystem inner) {\n+            super(\"disrupttranslog://\", inner);\n+        }\n+\n+        @Override\n+        public FileChannel newFileChannel(Path path, Set<? extends OpenOption> options, FileAttribute<?>... attrs) throws IOException {\n+            if (injectFailures.get() && path.toString().endsWith(\".ckp\")) {\n+                // prevents checkpoint file to be updated\n+                throw new IOException(\"fake IOException\");\n+            }\n+            return super.newFileChannel(path, options, attrs);\n+        }\n+    }\n+\n+    /**\n+     * This test checks that all operations below the global checkpoint are properly persisted.\n+     * It simulates a full power outage by preventing translog checkpoint files to be written and restart the cluster. This means that\n+     * all un-fsynced data will be lost.\n+     */\n+    public void testGlobalCheckpointIsSafe() throws Exception {\n+        startCluster(rarely() ? 5 : 3);\n+\n+        var numberOfShards = 1 + randomInt(2);\n+        var numberOfReplicas = randomInt(2);\n+\n+        execute(\"create table test (id int primary key, data string) clustered into \" + numberOfShards + \" shards \" +\n+                            \"with (number_of_replicas = ?)\", new Object[] {numberOfReplicas});\n+\n+        ensureGreen();\n+\n+        AtomicBoolean stopGlobalCheckpointFetcher = new AtomicBoolean();\n+\n+        Map<Integer, Long> shardToGcp = new ConcurrentHashMap<>();\n+        for (int i = 0; i < numberOfShards; i++) {\n+            shardToGcp.put(i, SequenceNumbers.NO_OPS_PERFORMED);\n+        }\n+        final Thread globalCheckpointSampler = new Thread(() -> {\n+            while (stopGlobalCheckpointFetcher.get() == false) {\n+                try {\n+                    var response = sqlExecutor.exec(\"select id, seq_no_stats['global_checkpoint'] \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8febaf2933a2b9f4b8cfe7a86d24a324b62680cd"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU1MzAwNQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                                var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);\n          \n          \n            \n                                                var insert = String.format(Locale.ENGLISH, \"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);", "url": "https://github.com/crate/crate/pull/10605#discussion_r499553005", "createdAt": "2020-10-05T12:14:40Z", "author": {"login": "seut"}, "path": "server/src/test/java/org/elasticsearch/test/BackgroundIndexer.java", "diffHunk": "@@ -0,0 +1,323 @@\n+/*\n+ * Licensed to Elasticsearch under one or more contributor\n+ * license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright\n+ * ownership. Elasticsearch licenses this file to you under\n+ * the Apache License, Version 2.0 (the \"License\"); you may\n+ * not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.elasticsearch.test;\n+\n+import com.carrotsearch.randomizedtesting.RandomizedTest;\n+import com.carrotsearch.randomizedtesting.generators.RandomNumbers;\n+import com.carrotsearch.randomizedtesting.generators.RandomStrings;\n+import io.crate.common.unit.TimeValue;\n+import io.crate.testing.SQLTransportExecutor;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+import org.apache.logging.log4j.message.ParameterizedMessage;\n+import org.apache.logging.log4j.util.Supplier;\n+import org.elasticsearch.common.util.concurrent.ConcurrentCollections;\n+import org.junit.Assert;\n+\n+import javax.annotation.Nullable;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Locale;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.Semaphore;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+\n+import static org.hamcrest.Matchers.emptyIterable;\n+import static org.hamcrest.Matchers.equalTo;\n+\n+public class BackgroundIndexer implements AutoCloseable {\n+\n+    private final Logger logger = LogManager.getLogger(getClass());\n+\n+    final Thread[] writers;\n+    final SQLTransportExecutor sqlExecutor;\n+    final CountDownLatch stopLatch;\n+    final Collection<Exception> failures = new ArrayList<>();\n+    final AtomicBoolean stop = new AtomicBoolean(false);\n+    final AtomicLong idGenerator = new AtomicLong();\n+    final CountDownLatch startLatch = new CountDownLatch(1);\n+    final AtomicBoolean hasBudget = new AtomicBoolean(false); // when set to true, writers will acquire writes from a semaphore\n+    final Semaphore availableBudget = new Semaphore(0);\n+    private final Set<String> ids = ConcurrentCollections.newConcurrentSet();\n+    private volatile Consumer<Exception> failureAssertion = null;\n+    final String table;\n+\n+    volatile int minFieldSize = 10;\n+    volatile int maxFieldSize = 140;\n+\n+    /**\n+     * Start indexing in the background using a random number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table     table name to write data\n+     * @param column    column name to write data\n+     * @param client    client to use\n+     * @param numOfDocs number of document to index before pausing. Set to -1 to have no limit.\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs) {\n+        this(table, column, client, numOfDocs, RandomizedTest.scaledRandomIntBetween(2, 5));\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param client      client to use\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     */\n+    public BackgroundIndexer(String table, String column, SQLTransportExecutor client, int numOfDocs, final int writerCount) {\n+        this(table, column, client, numOfDocs, writerCount, true, null);\n+    }\n+\n+    /**\n+     * Start indexing in the background using a given number of threads. Indexing will be paused after numOfDocs docs has\n+     * been indexed.\n+     *\n+     * @param table       table name to write data\n+     * @param column      column name to write data\n+     * @param numOfDocs   number of document to index before pausing. Set to -1 to have no limit.\n+     * @param writerCount number of indexing threads to use\n+     * @param autoStart   set to true to start indexing as soon as all threads have been created.\n+     * @param random      random instance to use\n+     */\n+    public BackgroundIndexer(final String table, final String column, final SQLTransportExecutor sqlExecutor, final int numOfDocs, final int writerCount,\n+                             boolean autoStart, Random random) {\n+        if (random == null) {\n+            random = RandomizedTest.getRandom();\n+        }\n+        this.table = table;\n+        this.sqlExecutor = sqlExecutor;\n+        writers = new Thread[writerCount];\n+        stopLatch = new CountDownLatch(writers.length);\n+        logger.info(\"--> creating {} indexing threads (auto start: [{}], numOfDocs: [{}])\", writerCount, autoStart, numOfDocs);\n+        for (int i = 0; i < writers.length; i++) {\n+            final int indexerId = i;\n+            final boolean batch = random.nextBoolean();\n+            final Random threadRandom = new Random(random.nextLong());\n+            writers[i] = new Thread() {\n+                @Override\n+                public void run() {\n+                    long id = -1;\n+                    try {\n+                        startLatch.await();\n+                        logger.info(\"**** starting indexing thread {}\", indexerId);\n+                        while (!stop.get()) {\n+                            if (batch) {\n+                                int batchSize = threadRandom.nextInt(20) + 1;\n+                                if (hasBudget.get()) {\n+                                    // always try to get at least one\n+                                    batchSize = Math.max(Math.min(batchSize, availableBudget.availablePermits()), 1);\n+                                    if (!availableBudget.tryAcquire(batchSize, 250, TimeUnit.MILLISECONDS)) {\n+                                        // time out -> check if we have to stop.\n+                                        continue;\n+                                    }\n+                                }\n+                                var insertData = new Object[batchSize];\n+                                for (int i = 0; i < batchSize; i++) {\n+                                    insertData[i] = generateValues(idGenerator.incrementAndGet(), threadRandom);\n+                                }\n+                                try {\n+                                    var insert = String.format(Locale.ENGLISH,\"insert into %s (id, %s) (select * from unnest(?)) returning _id\", table, column);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8febaf2933a2b9f4b8cfe7a86d24a324b62680cd"}, "originalPosition": 146}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8cebc7d642107e5dd70383b127a3b276785e4e8e", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/8cebc7d642107e5dd70383b127a3b276785e4e8e", "committedDate": "2020-10-05T12:26:46Z", "message": "Add DiskDisruptionIT to integration tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8febaf2933a2b9f4b8cfe7a86d24a324b62680cd", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/8febaf2933a2b9f4b8cfe7a86d24a324b62680cd", "committedDate": "2020-10-05T11:26:03Z", "message": "Move waitForDocs to ESIntegTestCase"}, "afterCommit": {"oid": "8cebc7d642107e5dd70383b127a3b276785e4e8e", "author": {"user": {"login": "mkleen", "name": "Michael Kleen"}}, "url": "https://github.com/crate/crate/commit/8cebc7d642107e5dd70383b127a3b276785e4e8e", "committedDate": "2020-10-05T12:26:46Z", "message": "Add DiskDisruptionIT to integration tests"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3794, "cost": 1, "resetAt": "2021-11-01T13:51:04Z"}}}