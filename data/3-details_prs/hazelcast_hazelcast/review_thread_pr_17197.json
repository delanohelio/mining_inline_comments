{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ2ODUwNzgz", "number": 17197, "reviewThreads": {"totalCount": 29, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwNDoyMTowOVrOETFpcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxNToyODoyOVrOEcoSIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NDUwOTMwOnYy", "diffSide": "RIGHT", "path": "docs/design/partitioning/07-parallel-migrations.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwNDoyMTowOVrOG4oBJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwNDoyMTowOVrOG4oBJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjAyOTA5Mw==", "bodyText": "minor: \"...pick the group with the greatest...\"", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r462029093", "createdAt": "2020-07-29T04:21:09Z", "author": {"login": "vbekiaris"}, "path": "docs/design/partitioning/07-parallel-migrations.md", "diffHunk": "@@ -0,0 +1,536 @@\n+# Parallel Independent Migrations\n+\n+|\u2139\ufe0f Since: 4.1| \n+|-------------|\n+\n+## Background\n+\n+The partition table is an eventually consistent and monotonic data structure. It\u2019s only updated by the master member \n+and published to the cluster. Some members may lag behind the master\u2019s version at any time but for those members to be \n+able to participate in a migration, they have to get the latest partition table from master. To provide the monotonic \n+order, the partition table has a global version. And because of this global order, we have to migrate partitions \n+one-by-one. Once the master member changes, it fetches the partition tables and the list of completed & active migrations \n+from the cluster and determines the most recent version of the partition table and then publishes it back. \n+\n+Right now this is a very strict guarantee, because there were data loss issues during shutdown and migrations \n+a few years back and we wanted all partition updates and migrations to be more consistent.\n+\n+The good thing is, in practice, we don\u2019t need this global partition table update order. Because, partitions are not \n+dependent on each other. Instead of a global partition table version, we can have a version per partition. And we can \n+update partitions independently. So, after each partition migration, we can just publish that partition info only, \n+not the whole partition table. Also we can migrate different partitions concurrently. For example, assume there are 10 \n+members, we can execute 5 migrations between 5 member pairs in parallel. With these two optimizations \n+(removal of global update order and parallel migrations), repartitioning can become significantly faster than current.\n+\n+### Previous Work\n+\n+Previous works related to partitioning system and migration mechanism: \n+\n+| Version | Design |\n+| :-----: | :----- |\n+| 3.7 | [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md) |\n+| 3.7 | [Graceful Shutdown Improvements](02-graceful-shutdown-improvements.md) |\n+| 3.9 | [Fine-Grained Anti Entropy Mechanism](03-fine-grained-anti-entropy-mechanism.md) |\n+| 3.9 | [Gigantic Cache Migration Enhancements](04-gigantic-cache-migration-enhancements.md) |\n+| 3.12 | [Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) |\n+| 4.0 | [Refactor MigrationListener API](06-refactor-migrationlistener-api.md) |\n+\n+_Being familiar with these will help to understand the new changes._\n+ \n+## Design\n+\n+### 1 - Global Version to Partition Level\n+\n+First of the major changes is the removal of the global partition table update ordering. Instead we will version each \n+partition individually. \n+\n+#### 1.1 - Current Versioning Mechanism\n+\n+Currently, master member increments global version on each partition update. When the partition table update is received \n+from the master, members decide whether to apply or to ignore the update comparing their local version to received \n+version. If local version is less than received version then partition table is applied, otherwise it's ignored. \n+\n+Similarly, `MigrationInfo` contains the partition table versions before and after \n+the migration. When a migration operation is received, members execute the operation only and only if local version is \n+equal to the version before migration. Otherwise, migration is rejected. A migration operation also contains the most \n+recently completed `MigrationInfo` list. This was  introduced in \n+[Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) \n+work. Before executing the migration operation, members apply these completed migrations in case they haven't seen them yet.\n+\n+While committing promotions, promoted member rejects the commit if its local version is greater than the version in operation.\n+This is similar to applying partition table updates.  \n+\n+#### 1.2 - New Versioning Mechanism\n+\n+With the proposed change, partition table version will be removed and master will update specific partition's version \n+on partition updates. On partition table broadcast, receiving members will do version comparisons for individual \n+partitions. So, they can partially apply, partially ignore the received partition table.\n+\n+During migrations, migration operations will still carry the most recent completed migrations, but only for the partition\n+that migration is belongs to. After applying completed migrations of that partition, migration operation will check \n+the local version against the version in `MigrationInfo`. If they are equal operation will be executed, otherwise it will\n+be rejected.\n+\n+For some cases, we need to check if the whole partition table has changed or not, with a simple comparison. Comparing each \n+partition version one-by-one is not simple and requires keeping all partition versions locally. For those cases, instead \n+of global partition table version, _hash/stamp_ of the partition table will be used. This stamp will be cached locally\n+and updated on each partition modification and also will be easily calculated with a pure hash function. We will use\n+a `64-bit MurmurHash3` to calculate the stamp.\n+\n+```java\n+public static long calculateStamp(InternalPartition[] partitions) {\n+    byte[] bb = new byte[Integer.BYTES * partitions.length];\n+    for (InternalPartition partition : partitions) {\n+        Bits.writeIntB(bb, partition.getPartitionId() * Integer.BYTES, partition.version());\n+    }\n+    return HashUtil.MurmurHash3_x64_64(bb, 0, bb.length);\n+}\n+```  \n+\n+During promotion commit, partition table stamp will be used the check whether local partition table is the same with \n+the one received with promotion. If yes, promotion commit will be ignored. Otherwise, we'll compare versions of individual\n+partitions and skip promotions of partitions which have greater version locally.    \n+   \n+_Other usages of partition table stamp will be explained in later sections._   \n+  \n+\n+### 2 - Parallel Migrations\n+\n+#### 2.1 - Current Migration Mechanism\n+\n+When a new member joins or an existing member leaves the cluster, eventually a `RepartitioningTask` is scheduled on \n+migration thread. `RepartitioningTask` prepares the new partition table and schedules the individual `MigrationTask`s\n+to reach the target partition table. These `MigrationTask`s are executed serially by migration thread and they submit \n+migration operations to the migration participants. After each migration completion, migration is committed to the \n+destination, and partition table is updated. All of these steps are executed serially.\n+\n+ \n+#### 2.2 - Parallel Migrations Mechanism\n+\n+With parallel migrations, most the mechanism remains the same. `RepartitioningTask` prepares the migration plan using \n+the target partition table. Instead of individual `MigrationTask`s, a single `MigrationPlanTask` will be scheduled. \n+`MigrationPlanTask` will submit parallel migration tasks and keep track of started/completed tasks. \n+\n+Migrations belonging to the same partition must be executed in order serially. This is required to provide consistency \n+and safety guarantees while migrating/replicating partition data. For instance, a migration can depend on an earlier one\n+in the plan. Or replication to a missing backup replica is more important than a migration that is just to rebalance the load.\n+_For more info please see [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md)._   \n+\n+To achieve this, `RepartitioningTask` creates migration plan with a consistent order. And `MigrationPlanTask` will \n+parallelize only migrations belonging to different partitions. Migrations belonging to the same partition will still be \n+executed serially. That's the upper limit of migration parallelization. But still we may want to limit parallel \n+migrations to keep heap memory usage under control and to avoid overload of network resources.\n+\n+A new `ClusterProperty`, `hazelcast.partition.max.parallel.migrations`, will be introduced, with a default value of `10`. \n+This property defines the maximum number of partition migrations to be executed on a member. That member can be either source \n+or destination of the migration. _Value of `10` is selected as a heuristic after benchmarking on EC2. You can find the \n+benchmark results below._ There may be consequences of using too much or too less parallelization:\n+\n+* Having too much parallelization can increase the heap memory usage and overload the network during partition rebalance.\n+* Having less parallelization can increase the total migration completion time.\n+       \n+\n+### 3 - Other Changes\n+\n+#### 3.1 - Cluster State Management Changes\n+\n+Cluster state & version changes are performed via transactional process and one of the prerequisites was partition table\n+version of the transaction originator must be equal to the version of the each transaction participant. Since partition\n+table version is now removed in favor of per-partition versions, we will use _partition table stamp_ instead.     \n+\n+#### 3.2 - Hot Restart Changes\n+\n+There will be a few small changes. First one is partition table persistence format. Format will be decided due to cluster \n+version while restoring/persisting partition table. When cluster version is upgraded, partition table will be persisted \n+with the new format once. \n+\n+Current format is:\n+\n+```\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n+| Partition Table Version     |\n++-----------------------------+\n+| Partition Table Length      |\n++-----------------------------+\n+| Replicas (Address + UUID)   |\n++-----------------------------+\n+| Partition-0 Replica Indexes |\n++-----------------------------+\n+| Partition-1 Replica Indexes |\n++-----------------------------+\n+| ........................... |\n++-----------------------------+\n+| Partition-N Replica Indexes |\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+     \n+```                                 \n+\n+New format will be:\n+\n+```\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n+| Partition Table Length      |\n++-----------------------------+\n+| Replicas (Address + UUID)   |\n++-----------------------------+\n+| Partition-0 Version         |\n++-----------------------------+\n+| Partition-0 Replica Indexes |\n++-----------------------------+\n+| Partition-1 Version         |\n++-----------------------------+\n+| Partition-1 Replica Indexes |\n++-----------------------------+\n+| ........................... |\n++-----------------------------+\n+| Partition-N Version         |\n++-----------------------------+\n+| Partition-N Replica Indexes |\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+     \n+```          \n+\n+Second change is in the partition table restore process. While restoring partition table, if a replica address change is\n+detected, we replace the old address of the replica with the new one in the partition table. In current mechanism, \n+global partition version is incremented by the number of updates, as if partition table is updated in the runtime.\n+With the new versioning, only updated partitions' versions will be incremented.\n+\n+Third change is in the partial restart decision process. During partial restart, we decide which members to perform the \n+restart, which members to be excluded and delete their data. To select member groups having the same partition table,\n+we use the partition table version. Members with the same partition table are grouped together. Also to find the most \n+recent partition table, we pick the partition table with the greatest version. If the policy is `PARTIAL_RECOVERY_MOST_RECENT`,\n+we pick the group the greatest partition table version. If it's `PARTIAL_RECOVERY_MOST_COMPLETE`, then we pick ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "624690d24ebe3be55010eb44849b7bbee3c92a98"}, "originalPosition": 200}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwODk5NDQyOnYy", "diffSide": "RIGHT", "path": "docs/design/partitioning/07-parallel-migrations.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQxMzo1MzozNlrOG8Ksuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwODoxMDozMFrOHIFNow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc0MzAzNQ==", "bodyText": "So MigrationPlanTask is executed in MigrationThread? Which executor is used to execute the individual MigrationTasks?\nupdate: found out after having reviewed the complete PR :-)", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r465743035", "createdAt": "2020-08-05T13:53:36Z", "author": {"login": "vbekiaris"}, "path": "docs/design/partitioning/07-parallel-migrations.md", "diffHunk": "@@ -0,0 +1,536 @@\n+# Parallel Independent Migrations\n+\n+|\u2139\ufe0f Since: 4.1| \n+|-------------|\n+\n+## Background\n+\n+The partition table is an eventually consistent and monotonic data structure. It\u2019s only updated by the master member \n+and published to the cluster. Some members may lag behind the master\u2019s version at any time but for those members to be \n+able to participate in a migration, they have to get the latest partition table from master. To provide the monotonic \n+order, the partition table has a global version. And because of this global order, we have to migrate partitions \n+one-by-one. Once the master member changes, it fetches the partition tables and the list of completed & active migrations \n+from the cluster and determines the most recent version of the partition table and then publishes it back. \n+\n+Right now this is a very strict guarantee, because there were data loss issues during shutdown and migrations \n+a few years back and we wanted all partition updates and migrations to be more consistent.\n+\n+The good thing is, in practice, we don\u2019t need this global partition table update order. Because, partitions are not \n+dependent on each other. Instead of a global partition table version, we can have a version per partition. And we can \n+update partitions independently. So, after each partition migration, we can just publish that partition info only, \n+not the whole partition table. Also we can migrate different partitions concurrently. For example, assume there are 10 \n+members, we can execute 5 migrations between 5 member pairs in parallel. With these two optimizations \n+(removal of global update order and parallel migrations), repartitioning can become significantly faster than current.\n+\n+### Previous Work\n+\n+Previous works related to partitioning system and migration mechanism: \n+\n+| Version | Design |\n+| :-----: | :----- |\n+| 3.7 | [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md) |\n+| 3.7 | [Graceful Shutdown Improvements](02-graceful-shutdown-improvements.md) |\n+| 3.9 | [Fine-Grained Anti Entropy Mechanism](03-fine-grained-anti-entropy-mechanism.md) |\n+| 3.9 | [Gigantic Cache Migration Enhancements](04-gigantic-cache-migration-enhancements.md) |\n+| 3.12 | [Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) |\n+| 4.0 | [Refactor MigrationListener API](06-refactor-migrationlistener-api.md) |\n+\n+_Being familiar with these will help to understand the new changes._\n+ \n+## Design\n+\n+### 1 - Global Version to Partition Level\n+\n+First of the major changes is the removal of the global partition table update ordering. Instead we will version each \n+partition individually. \n+\n+#### 1.1 - Current Versioning Mechanism\n+\n+Currently, master member increments global version on each partition update. When the partition table update is received \n+from the master, members decide whether to apply or to ignore the update comparing their local version to received \n+version. If local version is less than received version then partition table is applied, otherwise it's ignored. \n+\n+Similarly, `MigrationInfo` contains the partition table versions before and after \n+the migration. When a migration operation is received, members execute the operation only and only if local version is \n+equal to the version before migration. Otherwise, migration is rejected. A migration operation also contains the most \n+recently completed `MigrationInfo` list. This was  introduced in \n+[Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) \n+work. Before executing the migration operation, members apply these completed migrations in case they haven't seen them yet.\n+\n+While committing promotions, promoted member rejects the commit if its local version is greater than the version in operation.\n+This is similar to applying partition table updates.  \n+\n+#### 1.2 - New Versioning Mechanism\n+\n+With the proposed change, partition table version will be removed and master will update specific partition's version \n+on partition updates. On partition table broadcast, receiving members will do version comparisons for individual \n+partitions. So, they can partially apply, partially ignore the received partition table.\n+\n+During migrations, migration operations will still carry the most recent completed migrations, but only for the partition\n+that migration is belongs to. After applying completed migrations of that partition, migration operation will check \n+the local version against the version in `MigrationInfo`. If they are equal operation will be executed, otherwise it will\n+be rejected.\n+\n+For some cases, we need to check if the whole partition table has changed or not, with a simple comparison. Comparing each \n+partition version one-by-one is not simple and requires keeping all partition versions locally. For those cases, instead \n+of global partition table version, _hash/stamp_ of the partition table will be used. This stamp will be cached locally\n+and updated on each partition modification and also will be easily calculated with a pure hash function. We will use\n+a `64-bit MurmurHash3` to calculate the stamp.\n+\n+```java\n+public static long calculateStamp(InternalPartition[] partitions) {\n+    byte[] bb = new byte[Integer.BYTES * partitions.length];\n+    for (InternalPartition partition : partitions) {\n+        Bits.writeIntB(bb, partition.getPartitionId() * Integer.BYTES, partition.version());\n+    }\n+    return HashUtil.MurmurHash3_x64_64(bb, 0, bb.length);\n+}\n+```  \n+\n+During promotion commit, partition table stamp will be used the check whether local partition table is the same with \n+the one received with promotion. If yes, promotion commit will be ignored. Otherwise, we'll compare versions of individual\n+partitions and skip promotions of partitions which have greater version locally.    \n+   \n+_Other usages of partition table stamp will be explained in later sections._   \n+  \n+\n+### 2 - Parallel Migrations\n+\n+#### 2.1 - Current Migration Mechanism\n+\n+When a new member joins or an existing member leaves the cluster, eventually a `RepartitioningTask` is scheduled on \n+migration thread. `RepartitioningTask` prepares the new partition table and schedules the individual `MigrationTask`s\n+to reach the target partition table. These `MigrationTask`s are executed serially by migration thread and they submit \n+migration operations to the migration participants. After each migration completion, migration is committed to the \n+destination, and partition table is updated. All of these steps are executed serially.\n+\n+ \n+#### 2.2 - Parallel Migrations Mechanism\n+\n+With parallel migrations, most the mechanism remains the same. `RepartitioningTask` prepares the migration plan using \n+the target partition table. Instead of individual `MigrationTask`s, a single `MigrationPlanTask` will be scheduled. \n+`MigrationPlanTask` will submit parallel migration tasks and keep track of started/completed tasks. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "624690d24ebe3be55010eb44849b7bbee3c92a98"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODIzNjA2Nw==", "bodyText": "For the record; AsyncMigrationTasks are also executed in MigrationThread. They invoke MigrationRequestOperation asynchronously, don't wait for the completion. Once migration is committed/finalized, a new AsyncMigrationTask is executed.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478236067", "createdAt": "2020-08-27T08:10:30Z", "author": {"login": "mdogan"}, "path": "docs/design/partitioning/07-parallel-migrations.md", "diffHunk": "@@ -0,0 +1,536 @@\n+# Parallel Independent Migrations\n+\n+|\u2139\ufe0f Since: 4.1| \n+|-------------|\n+\n+## Background\n+\n+The partition table is an eventually consistent and monotonic data structure. It\u2019s only updated by the master member \n+and published to the cluster. Some members may lag behind the master\u2019s version at any time but for those members to be \n+able to participate in a migration, they have to get the latest partition table from master. To provide the monotonic \n+order, the partition table has a global version. And because of this global order, we have to migrate partitions \n+one-by-one. Once the master member changes, it fetches the partition tables and the list of completed & active migrations \n+from the cluster and determines the most recent version of the partition table and then publishes it back. \n+\n+Right now this is a very strict guarantee, because there were data loss issues during shutdown and migrations \n+a few years back and we wanted all partition updates and migrations to be more consistent.\n+\n+The good thing is, in practice, we don\u2019t need this global partition table update order. Because, partitions are not \n+dependent on each other. Instead of a global partition table version, we can have a version per partition. And we can \n+update partitions independently. So, after each partition migration, we can just publish that partition info only, \n+not the whole partition table. Also we can migrate different partitions concurrently. For example, assume there are 10 \n+members, we can execute 5 migrations between 5 member pairs in parallel. With these two optimizations \n+(removal of global update order and parallel migrations), repartitioning can become significantly faster than current.\n+\n+### Previous Work\n+\n+Previous works related to partitioning system and migration mechanism: \n+\n+| Version | Design |\n+| :-----: | :----- |\n+| 3.7 | [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md) |\n+| 3.7 | [Graceful Shutdown Improvements](02-graceful-shutdown-improvements.md) |\n+| 3.9 | [Fine-Grained Anti Entropy Mechanism](03-fine-grained-anti-entropy-mechanism.md) |\n+| 3.9 | [Gigantic Cache Migration Enhancements](04-gigantic-cache-migration-enhancements.md) |\n+| 3.12 | [Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) |\n+| 4.0 | [Refactor MigrationListener API](06-refactor-migrationlistener-api.md) |\n+\n+_Being familiar with these will help to understand the new changes._\n+ \n+## Design\n+\n+### 1 - Global Version to Partition Level\n+\n+First of the major changes is the removal of the global partition table update ordering. Instead we will version each \n+partition individually. \n+\n+#### 1.1 - Current Versioning Mechanism\n+\n+Currently, master member increments global version on each partition update. When the partition table update is received \n+from the master, members decide whether to apply or to ignore the update comparing their local version to received \n+version. If local version is less than received version then partition table is applied, otherwise it's ignored. \n+\n+Similarly, `MigrationInfo` contains the partition table versions before and after \n+the migration. When a migration operation is received, members execute the operation only and only if local version is \n+equal to the version before migration. Otherwise, migration is rejected. A migration operation also contains the most \n+recently completed `MigrationInfo` list. This was  introduced in \n+[Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) \n+work. Before executing the migration operation, members apply these completed migrations in case they haven't seen them yet.\n+\n+While committing promotions, promoted member rejects the commit if its local version is greater than the version in operation.\n+This is similar to applying partition table updates.  \n+\n+#### 1.2 - New Versioning Mechanism\n+\n+With the proposed change, partition table version will be removed and master will update specific partition's version \n+on partition updates. On partition table broadcast, receiving members will do version comparisons for individual \n+partitions. So, they can partially apply, partially ignore the received partition table.\n+\n+During migrations, migration operations will still carry the most recent completed migrations, but only for the partition\n+that migration is belongs to. After applying completed migrations of that partition, migration operation will check \n+the local version against the version in `MigrationInfo`. If they are equal operation will be executed, otherwise it will\n+be rejected.\n+\n+For some cases, we need to check if the whole partition table has changed or not, with a simple comparison. Comparing each \n+partition version one-by-one is not simple and requires keeping all partition versions locally. For those cases, instead \n+of global partition table version, _hash/stamp_ of the partition table will be used. This stamp will be cached locally\n+and updated on each partition modification and also will be easily calculated with a pure hash function. We will use\n+a `64-bit MurmurHash3` to calculate the stamp.\n+\n+```java\n+public static long calculateStamp(InternalPartition[] partitions) {\n+    byte[] bb = new byte[Integer.BYTES * partitions.length];\n+    for (InternalPartition partition : partitions) {\n+        Bits.writeIntB(bb, partition.getPartitionId() * Integer.BYTES, partition.version());\n+    }\n+    return HashUtil.MurmurHash3_x64_64(bb, 0, bb.length);\n+}\n+```  \n+\n+During promotion commit, partition table stamp will be used the check whether local partition table is the same with \n+the one received with promotion. If yes, promotion commit will be ignored. Otherwise, we'll compare versions of individual\n+partitions and skip promotions of partitions which have greater version locally.    \n+   \n+_Other usages of partition table stamp will be explained in later sections._   \n+  \n+\n+### 2 - Parallel Migrations\n+\n+#### 2.1 - Current Migration Mechanism\n+\n+When a new member joins or an existing member leaves the cluster, eventually a `RepartitioningTask` is scheduled on \n+migration thread. `RepartitioningTask` prepares the new partition table and schedules the individual `MigrationTask`s\n+to reach the target partition table. These `MigrationTask`s are executed serially by migration thread and they submit \n+migration operations to the migration participants. After each migration completion, migration is committed to the \n+destination, and partition table is updated. All of these steps are executed serially.\n+\n+ \n+#### 2.2 - Parallel Migrations Mechanism\n+\n+With parallel migrations, most the mechanism remains the same. `RepartitioningTask` prepares the migration plan using \n+the target partition table. Instead of individual `MigrationTask`s, a single `MigrationPlanTask` will be scheduled. \n+`MigrationPlanTask` will submit parallel migration tasks and keep track of started/completed tasks. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc0MzAzNQ=="}, "originalCommit": {"oid": "624690d24ebe3be55010eb44849b7bbee3c92a98"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0Njg0ODAxOnYy", "diffSide": "RIGHT", "path": "docs/design/partitioning/07-parallel-migrations.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxMjo1NTowM1rOHBniZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxMjo1NTowM1rOHBniZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ1ODQwNQ==", "bodyText": "New heading for partition table stamp would make reading easy.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r471458405", "createdAt": "2020-08-17T12:55:03Z", "author": {"login": "ahmetmircik"}, "path": "docs/design/partitioning/07-parallel-migrations.md", "diffHunk": "@@ -0,0 +1,536 @@\n+# Parallel Independent Migrations\n+\n+|\u2139\ufe0f Since: 4.1| \n+|-------------|\n+\n+## Background\n+\n+The partition table is an eventually consistent and monotonic data structure. It\u2019s only updated by the master member \n+and published to the cluster. Some members may lag behind the master\u2019s version at any time but for those members to be \n+able to participate in a migration, they have to get the latest partition table from master. To provide the monotonic \n+order, the partition table has a global version. And because of this global order, we have to migrate partitions \n+one-by-one. Once the master member changes, it fetches the partition tables and the list of completed & active migrations \n+from the cluster and determines the most recent version of the partition table and then publishes it back. \n+\n+Right now this is a very strict guarantee, because there were data loss issues during shutdown and migrations \n+a few years back and we wanted all partition updates and migrations to be more consistent.\n+\n+The good thing is, in practice, we don\u2019t need this global partition table update order. Because, partitions are not \n+dependent on each other. Instead of a global partition table version, we can have a version per partition. And we can \n+update partitions independently. So, after each partition migration, we can just publish that partition info only, \n+not the whole partition table. Also we can migrate different partitions concurrently. For example, assume there are 10 \n+members, we can execute 5 migrations between 5 member pairs in parallel. With these two optimizations \n+(removal of global update order and parallel migrations), repartitioning can become significantly faster than current.\n+\n+### Previous Work\n+\n+Previous works related to partitioning system and migration mechanism: \n+\n+| Version | Design |\n+| :-----: | :----- |\n+| 3.7 | [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md) |\n+| 3.7 | [Graceful Shutdown Improvements](02-graceful-shutdown-improvements.md) |\n+| 3.9 | [Fine-Grained Anti Entropy Mechanism](03-fine-grained-anti-entropy-mechanism.md) |\n+| 3.9 | [Gigantic Cache Migration Enhancements](04-gigantic-cache-migration-enhancements.md) |\n+| 3.12 | [Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) |\n+| 4.0 | [Refactor MigrationListener API](06-refactor-migrationlistener-api.md) |\n+\n+_Being familiar with these will help to understand the new changes._\n+ \n+## Design\n+\n+### 1 - Global Version to Partition Level\n+\n+First of the major changes is the removal of the global partition table update ordering. Instead we will version each \n+partition individually. \n+\n+#### 1.1 - Current Versioning Mechanism\n+\n+Currently, master member increments global version on each partition update. When the partition table update is received \n+from the master, members decide whether to apply or to ignore the update comparing their local version to received \n+version. If local version is less than received version then partition table is applied, otherwise it's ignored. \n+\n+Similarly, `MigrationInfo` contains the partition table versions before and after \n+the migration. When a migration operation is received, members execute the operation only and only if local version is \n+equal to the version before migration. Otherwise, migration is rejected. A migration operation also contains the most \n+recently completed `MigrationInfo` list. This was  introduced in \n+[Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) \n+work. Before executing the migration operation, members apply these completed migrations in case they haven't seen them yet.\n+\n+While committing promotions, promoted member rejects the commit if its local version is greater than the version in operation.\n+This is similar to applying partition table updates.  \n+\n+#### 1.2 - New Versioning Mechanism\n+\n+With the proposed change, partition table version will be removed and master will update specific partition's version \n+on partition updates. On partition table broadcast, receiving members will do version comparisons for individual \n+partitions. So, they can partially apply, partially ignore the received partition table.\n+\n+During migrations, migration operations will still carry the most recent completed migrations, but only for the partition\n+that migration is belongs to. After applying completed migrations of that partition, migration operation will check \n+the local version against the version in `MigrationInfo`. If they are equal operation will be executed, otherwise it will\n+be rejected.\n+\n+For some cases, we need to check if the whole partition table has changed or not, with a simple comparison. Comparing each ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fa223ac0d6c487e16940496f7173d5c35546ee9a"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0Njg3NzM5OnYy", "diffSide": "RIGHT", "path": "docs/design/partitioning/07-parallel-migrations.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxMzowMjo1MlrOHBn0OA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDoyNTozMlrOHDBEiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ2Mjk2OA==", "bodyText": "maybe we can reference all-checks-to-start-a-migration-operation documentation, if there is any.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r471462968", "createdAt": "2020-08-17T13:02:52Z", "author": {"login": "ahmetmircik"}, "path": "docs/design/partitioning/07-parallel-migrations.md", "diffHunk": "@@ -0,0 +1,536 @@\n+# Parallel Independent Migrations\n+\n+|\u2139\ufe0f Since: 4.1| \n+|-------------|\n+\n+## Background\n+\n+The partition table is an eventually consistent and monotonic data structure. It\u2019s only updated by the master member \n+and published to the cluster. Some members may lag behind the master\u2019s version at any time but for those members to be \n+able to participate in a migration, they have to get the latest partition table from master. To provide the monotonic \n+order, the partition table has a global version. And because of this global order, we have to migrate partitions \n+one-by-one. Once the master member changes, it fetches the partition tables and the list of completed & active migrations \n+from the cluster and determines the most recent version of the partition table and then publishes it back. \n+\n+Right now this is a very strict guarantee, because there were data loss issues during shutdown and migrations \n+a few years back and we wanted all partition updates and migrations to be more consistent.\n+\n+The good thing is, in practice, we don\u2019t need this global partition table update order. Because, partitions are not \n+dependent on each other. Instead of a global partition table version, we can have a version per partition. And we can \n+update partitions independently. So, after each partition migration, we can just publish that partition info only, \n+not the whole partition table. Also we can migrate different partitions concurrently. For example, assume there are 10 \n+members, we can execute 5 migrations between 5 member pairs in parallel. With these two optimizations \n+(removal of global update order and parallel migrations), repartitioning can become significantly faster than current.\n+\n+### Previous Work\n+\n+Previous works related to partitioning system and migration mechanism: \n+\n+| Version | Design |\n+| :-----: | :----- |\n+| 3.7 | [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md) |\n+| 3.7 | [Graceful Shutdown Improvements](02-graceful-shutdown-improvements.md) |\n+| 3.9 | [Fine-Grained Anti Entropy Mechanism](03-fine-grained-anti-entropy-mechanism.md) |\n+| 3.9 | [Gigantic Cache Migration Enhancements](04-gigantic-cache-migration-enhancements.md) |\n+| 3.12 | [Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) |\n+| 4.0 | [Refactor MigrationListener API](06-refactor-migrationlistener-api.md) |\n+\n+_Being familiar with these will help to understand the new changes._\n+ \n+## Design\n+\n+### 1 - Global Version to Partition Level\n+\n+First of the major changes is the removal of the global partition table update ordering. Instead we will version each \n+partition individually. \n+\n+#### 1.1 - Current Versioning Mechanism\n+\n+Currently, master member increments global version on each partition update. When the partition table update is received \n+from the master, members decide whether to apply or to ignore the update comparing their local version to received \n+version. If local version is less than received version then partition table is applied, otherwise it's ignored. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fa223ac0d6c487e16940496f7173d5c35546ee9a"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyNTMyMA==", "bodyText": "Afaik, we haven't documented all of these verifications before. Even some of them are introduced eventually with bug-fixing.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472925320", "createdAt": "2020-08-19T10:25:32Z", "author": {"login": "mdogan"}, "path": "docs/design/partitioning/07-parallel-migrations.md", "diffHunk": "@@ -0,0 +1,536 @@\n+# Parallel Independent Migrations\n+\n+|\u2139\ufe0f Since: 4.1| \n+|-------------|\n+\n+## Background\n+\n+The partition table is an eventually consistent and monotonic data structure. It\u2019s only updated by the master member \n+and published to the cluster. Some members may lag behind the master\u2019s version at any time but for those members to be \n+able to participate in a migration, they have to get the latest partition table from master. To provide the monotonic \n+order, the partition table has a global version. And because of this global order, we have to migrate partitions \n+one-by-one. Once the master member changes, it fetches the partition tables and the list of completed & active migrations \n+from the cluster and determines the most recent version of the partition table and then publishes it back. \n+\n+Right now this is a very strict guarantee, because there were data loss issues during shutdown and migrations \n+a few years back and we wanted all partition updates and migrations to be more consistent.\n+\n+The good thing is, in practice, we don\u2019t need this global partition table update order. Because, partitions are not \n+dependent on each other. Instead of a global partition table version, we can have a version per partition. And we can \n+update partitions independently. So, after each partition migration, we can just publish that partition info only, \n+not the whole partition table. Also we can migrate different partitions concurrently. For example, assume there are 10 \n+members, we can execute 5 migrations between 5 member pairs in parallel. With these two optimizations \n+(removal of global update order and parallel migrations), repartitioning can become significantly faster than current.\n+\n+### Previous Work\n+\n+Previous works related to partitioning system and migration mechanism: \n+\n+| Version | Design |\n+| :-----: | :----- |\n+| 3.7 | [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md) |\n+| 3.7 | [Graceful Shutdown Improvements](02-graceful-shutdown-improvements.md) |\n+| 3.9 | [Fine-Grained Anti Entropy Mechanism](03-fine-grained-anti-entropy-mechanism.md) |\n+| 3.9 | [Gigantic Cache Migration Enhancements](04-gigantic-cache-migration-enhancements.md) |\n+| 3.12 | [Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) |\n+| 4.0 | [Refactor MigrationListener API](06-refactor-migrationlistener-api.md) |\n+\n+_Being familiar with these will help to understand the new changes._\n+ \n+## Design\n+\n+### 1 - Global Version to Partition Level\n+\n+First of the major changes is the removal of the global partition table update ordering. Instead we will version each \n+partition individually. \n+\n+#### 1.1 - Current Versioning Mechanism\n+\n+Currently, master member increments global version on each partition update. When the partition table update is received \n+from the master, members decide whether to apply or to ignore the update comparing their local version to received \n+version. If local version is less than received version then partition table is applied, otherwise it's ignored. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ2Mjk2OA=="}, "originalCommit": {"oid": "fa223ac0d6c487e16940496f7173d5c35546ee9a"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0Njg4MTYyOnYy", "diffSide": "RIGHT", "path": "docs/design/partitioning/07-parallel-migrations.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxMzowNDowMVrOHBn20w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QxMzowNDowMVrOHBn20w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTQ2MzYzNQ==", "bodyText": "to see all usages in one place, some referencing can help here.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r471463635", "createdAt": "2020-08-17T13:04:01Z", "author": {"login": "ahmetmircik"}, "path": "docs/design/partitioning/07-parallel-migrations.md", "diffHunk": "@@ -0,0 +1,536 @@\n+# Parallel Independent Migrations\n+\n+|\u2139\ufe0f Since: 4.1| \n+|-------------|\n+\n+## Background\n+\n+The partition table is an eventually consistent and monotonic data structure. It\u2019s only updated by the master member \n+and published to the cluster. Some members may lag behind the master\u2019s version at any time but for those members to be \n+able to participate in a migration, they have to get the latest partition table from master. To provide the monotonic \n+order, the partition table has a global version. And because of this global order, we have to migrate partitions \n+one-by-one. Once the master member changes, it fetches the partition tables and the list of completed & active migrations \n+from the cluster and determines the most recent version of the partition table and then publishes it back. \n+\n+Right now this is a very strict guarantee, because there were data loss issues during shutdown and migrations \n+a few years back and we wanted all partition updates and migrations to be more consistent.\n+\n+The good thing is, in practice, we don\u2019t need this global partition table update order. Because, partitions are not \n+dependent on each other. Instead of a global partition table version, we can have a version per partition. And we can \n+update partitions independently. So, after each partition migration, we can just publish that partition info only, \n+not the whole partition table. Also we can migrate different partitions concurrently. For example, assume there are 10 \n+members, we can execute 5 migrations between 5 member pairs in parallel. With these two optimizations \n+(removal of global update order and parallel migrations), repartitioning can become significantly faster than current.\n+\n+### Previous Work\n+\n+Previous works related to partitioning system and migration mechanism: \n+\n+| Version | Design |\n+| :-----: | :----- |\n+| 3.7 | [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md) |\n+| 3.7 | [Graceful Shutdown Improvements](02-graceful-shutdown-improvements.md) |\n+| 3.9 | [Fine-Grained Anti Entropy Mechanism](03-fine-grained-anti-entropy-mechanism.md) |\n+| 3.9 | [Gigantic Cache Migration Enhancements](04-gigantic-cache-migration-enhancements.md) |\n+| 3.12 | [Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) |\n+| 4.0 | [Refactor MigrationListener API](06-refactor-migrationlistener-api.md) |\n+\n+_Being familiar with these will help to understand the new changes._\n+ \n+## Design\n+\n+### 1 - Global Version to Partition Level\n+\n+First of the major changes is the removal of the global partition table update ordering. Instead we will version each \n+partition individually. \n+\n+#### 1.1 - Current Versioning Mechanism\n+\n+Currently, master member increments global version on each partition update. When the partition table update is received \n+from the master, members decide whether to apply or to ignore the update comparing their local version to received \n+version. If local version is less than received version then partition table is applied, otherwise it's ignored. \n+\n+Similarly, `MigrationInfo` contains the partition table versions before and after \n+the migration. When a migration operation is received, members execute the operation only and only if local version is \n+equal to the version before migration. Otherwise, migration is rejected. A migration operation also contains the most \n+recently completed `MigrationInfo` list. This was  introduced in \n+[Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) \n+work. Before executing the migration operation, members apply these completed migrations in case they haven't seen them yet.\n+\n+While committing promotions, promoted member rejects the commit if its local version is greater than the version in operation.\n+This is similar to applying partition table updates.  \n+\n+#### 1.2 - New Versioning Mechanism\n+\n+With the proposed change, partition table version will be removed and master will update specific partition's version \n+on partition updates. On partition table broadcast, receiving members will do version comparisons for individual \n+partitions. So, they can partially apply, partially ignore the received partition table.\n+\n+During migrations, migration operations will still carry the most recent completed migrations, but only for the partition\n+that migration is belongs to. After applying completed migrations of that partition, migration operation will check \n+the local version against the version in `MigrationInfo`. If they are equal operation will be executed, otherwise it will\n+be rejected.\n+\n+For some cases, we need to check if the whole partition table has changed or not, with a simple comparison. Comparing each \n+partition version one-by-one is not simple and requires keeping all partition versions locally. For those cases, instead \n+of global partition table version, _hash/stamp_ of the partition table will be used. This stamp will be cached locally\n+and updated on each partition modification and also will be easily calculated with a pure hash function. We will use\n+a `64-bit MurmurHash3` to calculate the stamp.\n+\n+```java\n+public static long calculateStamp(InternalPartition[] partitions) {\n+    byte[] bb = new byte[Integer.BYTES * partitions.length];\n+    for (InternalPartition partition : partitions) {\n+        Bits.writeIntB(bb, partition.getPartitionId() * Integer.BYTES, partition.version());\n+    }\n+    return HashUtil.MurmurHash3_x64_64(bb, 0, bb.length);\n+}\n+```  \n+\n+During promotion commit, partition table stamp will be used the check whether local partition table is the same with \n+the one received with promotion. If yes, promotion commit will be ignored. Otherwise, we'll compare versions of individual\n+partitions and skip promotions of partitions which have greater version locally.    \n+   \n+_Other usages of partition table stamp will be explained in later sections._   ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fa223ac0d6c487e16940496f7173d5c35546ee9a"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NTk3MDUxOnYy", "diffSide": "RIGHT", "path": "docs/design/partitioning/07-parallel-migrations.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQwOTo1Mzo1NVrOHC__iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQwOTo1Mzo1NVrOHC__iA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkwNzY1Ng==", "bodyText": "instead of according to the version, rewording it as according to the cluster version seemed clearer to understand.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472907656", "createdAt": "2020-08-19T09:53:55Z", "author": {"login": "ahmetmircik"}, "path": "docs/design/partitioning/07-parallel-migrations.md", "diffHunk": "@@ -0,0 +1,536 @@\n+# Parallel Independent Migrations\n+\n+|\u2139\ufe0f Since: 4.1| \n+|-------------|\n+\n+## Background\n+\n+The partition table is an eventually consistent and monotonic data structure. It\u2019s only updated by the master member \n+and published to the cluster. Some members may lag behind the master\u2019s version at any time but for those members to be \n+able to participate in a migration, they have to get the latest partition table from master. To provide the monotonic \n+order, the partition table has a global version. And because of this global order, we have to migrate partitions \n+one-by-one. Once the master member changes, it fetches the partition tables and the list of completed & active migrations \n+from the cluster and determines the most recent version of the partition table and then publishes it back. \n+\n+Right now this is a very strict guarantee, because there were data loss issues during shutdown and migrations \n+a few years back and we wanted all partition updates and migrations to be more consistent.\n+\n+The good thing is, in practice, we don\u2019t need this global partition table update order. Because, partitions are not \n+dependent on each other. Instead of a global partition table version, we can have a version per partition. And we can \n+update partitions independently. So, after each partition migration, we can just publish that partition info only, \n+not the whole partition table. Also we can migrate different partitions concurrently. For example, assume there are 10 \n+members, we can execute 5 migrations between 5 member pairs in parallel. With these two optimizations \n+(removal of global update order and parallel migrations), repartitioning can become significantly faster than current.\n+\n+### Previous Work\n+\n+Previous works related to partitioning system and migration mechanism: \n+\n+| Version | Design |\n+| :-----: | :----- |\n+| 3.7 | [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md) |\n+| 3.7 | [Graceful Shutdown Improvements](02-graceful-shutdown-improvements.md) |\n+| 3.9 | [Fine-Grained Anti Entropy Mechanism](03-fine-grained-anti-entropy-mechanism.md) |\n+| 3.9 | [Gigantic Cache Migration Enhancements](04-gigantic-cache-migration-enhancements.md) |\n+| 3.12 | [Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) |\n+| 4.0 | [Refactor MigrationListener API](06-refactor-migrationlistener-api.md) |\n+\n+_Being familiar with these will help to understand the new changes._\n+ \n+## Design\n+\n+### 1 - Global Version to Partition Level\n+\n+First of the major changes is the removal of the global partition table update ordering. Instead we will version each \n+partition individually. \n+\n+#### 1.1 - Current Versioning Mechanism\n+\n+Currently, master member increments global version on each partition update. When the partition table update is received \n+from the master, members decide whether to apply or to ignore the update comparing their local version to received \n+version. If local version is less than received version then partition table is applied, otherwise it's ignored. \n+\n+Similarly, `MigrationInfo` contains the partition table versions before and after \n+the migration. When a migration operation is received, members execute the operation only and only if local version is \n+equal to the version before migration. Otherwise, migration is rejected. A migration operation also contains the most \n+recently completed `MigrationInfo` list. This was  introduced in \n+[Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) \n+work. Before executing the migration operation, members apply these completed migrations in case they haven't seen them yet.\n+\n+While committing promotions, promoted member rejects the commit if its local version is greater than the version in operation.\n+This is similar to applying partition table updates.  \n+\n+#### 1.2 - New Versioning Mechanism\n+\n+With the proposed change, partition table version will be removed and master will update specific partition's version \n+on partition updates. On partition table broadcast, receiving members will do version comparisons for individual \n+partitions. So, they can partially apply, partially ignore the received partition table.\n+\n+During migrations, migration operations will still carry the most recent completed migrations, but only for the partition\n+that migration is belongs to. After applying completed migrations of that partition, migration operation will check \n+the local version against the version in `MigrationInfo`. If they are equal operation will be executed, otherwise it will\n+be rejected.\n+\n+For some cases, we need to check if the whole partition table has changed or not, with a simple comparison. Comparing each \n+partition version one-by-one is not simple and requires keeping all partition versions locally. For those cases, instead \n+of global partition table version, _hash/stamp_ of the partition table will be used. This stamp will be cached locally\n+and updated on each partition modification and also will be easily calculated with a pure hash function. We will use\n+a `64-bit MurmurHash3` to calculate the stamp.\n+\n+```java\n+public static long calculateStamp(InternalPartition[] partitions) {\n+    byte[] bb = new byte[Integer.BYTES * partitions.length];\n+    for (InternalPartition partition : partitions) {\n+        Bits.writeIntB(bb, partition.getPartitionId() * Integer.BYTES, partition.version());\n+    }\n+    return HashUtil.MurmurHash3_x64_64(bb, 0, bb.length);\n+}\n+```  \n+\n+During promotion commit, partition table stamp will be used the check whether local partition table is the same with \n+the one received with promotion. If yes, promotion commit will be ignored. Otherwise, we'll compare versions of individual\n+partitions and skip promotions of partitions which have greater version locally.    \n+   \n+_Other usages of partition table stamp will be explained in later sections._   \n+  \n+\n+### 2 - Parallel Migrations\n+\n+#### 2.1 - Current Migration Mechanism\n+\n+When a new member joins or an existing member leaves the cluster, eventually a `RepartitioningTask` is scheduled on \n+migration thread. `RepartitioningTask` prepares the new partition table and schedules the individual `MigrationTask`s\n+to reach the target partition table. These `MigrationTask`s are executed serially by migration thread and they submit \n+migration operations to the migration participants. After each migration completion, migration is committed to the \n+destination, and partition table is updated. All of these steps are executed serially.\n+\n+ \n+#### 2.2 - Parallel Migrations Mechanism\n+\n+With parallel migrations, most the mechanism remains the same. `RepartitioningTask` prepares the migration plan using \n+the target partition table. Instead of individual `MigrationTask`s, a single `MigrationPlanTask` will be scheduled. \n+`MigrationPlanTask` will submit parallel migration tasks and keep track of started/completed tasks. \n+\n+Migrations belonging to the same partition must be executed in order serially. This is required to provide consistency \n+and safety guarantees while migrating/replicating partition data. For instance, a migration can depend on an earlier one\n+in the plan. Or replication to a missing backup replica is more important than a migration that is just to rebalance the load.\n+_For more info please see [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md)._   \n+\n+To achieve this, `RepartitioningTask` creates migration plan with a consistent order. And `MigrationPlanTask` will \n+parallelize only migrations belonging to different partitions. Migrations belonging to the same partition will still be \n+executed serially. That's the upper limit of migration parallelization. But still we may want to limit parallel \n+migrations to keep heap memory usage under control and to avoid overload of network resources.\n+\n+A new `ClusterProperty`, `hazelcast.partition.max.parallel.migrations`, will be introduced, with a default value of `10`. \n+This property defines the maximum number of partition migrations to be executed on a member. That member can be either source \n+or destination of the migration. _Value of `10` is selected as a heuristic after benchmarking on EC2. You can find the \n+benchmark results below._ There may be consequences of using too much or too less parallelization:\n+\n+* Having too much parallelization can increase the heap memory usage and overload the network during partition rebalance.\n+* Having less parallelization can increase the total migration completion time.\n+       \n+\n+### 3 - Other Changes\n+\n+#### 3.1 - Cluster State Management Changes\n+\n+Cluster state & version changes are performed via transactional process and one of the prerequisites was partition table\n+version of the transaction originator must be equal to the version of the each transaction participant. Since partition\n+table version is now removed in favor of per-partition versions, we will use _partition table stamp_ instead.     \n+\n+#### 3.2 - Hot Restart Changes\n+\n+There will be a few small changes. First one is partition table persistence format. Format will be decided due to cluster \n+version while restoring/persisting partition table. When cluster version is upgraded, partition table will be persisted \n+with the new format once. \n+\n+Current format is:\n+\n+```\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n+| Partition Table Version     |\n++-----------------------------+\n+| Partition Table Length      |\n++-----------------------------+\n+| Replicas (Address + UUID)   |\n++-----------------------------+\n+| Partition-0 Replica Indexes |\n++-----------------------------+\n+| Partition-1 Replica Indexes |\n++-----------------------------+\n+| ........................... |\n++-----------------------------+\n+| Partition-N Replica Indexes |\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+     \n+```                                 \n+\n+New format will be:\n+\n+```\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n+| Partition Table Length      |\n++-----------------------------+\n+| Replicas (Address + UUID)   |\n++-----------------------------+\n+| Partition-0 Version         |\n++-----------------------------+\n+| Partition-0 Replica Indexes |\n++-----------------------------+\n+| Partition-1 Version         |\n++-----------------------------+\n+| Partition-1 Replica Indexes |\n++-----------------------------+\n+| ........................... |\n++-----------------------------+\n+| Partition-N Version         |\n++-----------------------------+\n+| Partition-N Replica Indexes |\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+     \n+```          \n+\n+Second change is in the partition table restore process. While restoring partition table, if a replica address change is\n+detected, we replace the old address of the replica with the new one in the partition table. In current mechanism, \n+global partition version is incremented by the number of updates, as if partition table is updated in the runtime.\n+With the new versioning, only updated partitions' versions will be incremented.\n+\n+Third change is in the partial restart decision process. During partial restart, we decide which members to perform the \n+restart, which members to be excluded and delete their data. To select member groups having the same partition table,\n+we use the partition table version. Members with the same partition table are grouped together. Also to find the most \n+recent partition table, we pick the partition table with the greatest version. If the policy is `PARTIAL_RECOVERY_MOST_RECENT`,\n+we pick the group the greatest partition table version. If it's `PARTIAL_RECOVERY_MOST_COMPLETE`, then we pick \n+the largest group. In other places we will use _partition table stamp_ instead of removed partition table version. But in\n+this case, we cannot use _partition table stamp_. Because stamp is only valid for equality comparison, it cannot be used\n+to compare order of the partition tables. Instead we will use the sum of per-partition versions to compare the order. \n+_This works because Hot Restart does not support arbitrary server crashes. Only proper server shutdowns are supported._\n+Otherwise, resolving conflicts on partition level, determining the most recent versions of each partition, composing\n+a new partition table with these partitions and adapting Hot Restart to restore data based on this new partition table\n+requires a large rework and time. And supporting rolling upgrade scenarios makes this more complicated.            \n+\n+\n+#### 3.3 - Rolling Upgrade\n+\n+- To support rolling upgrade, we will have two distinct paths for most the migration scheduling and execution logic. \n+\n+- While updating partition table and applying completed migrations, there'll be separate paths too.\n+\n+- Once the cluster is upgraded to `4.1`, all partition versions will be set to the latest partition table version.\n+\n+- Cluster state change transaction mechanism will use either partition table version or stamp according to the version.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 218}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjA1NTI5OnYy", "diffSide": "RIGHT", "path": "docs/design/partitioning/07-parallel-migrations.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDoxODoxNFrOHDAz0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDozNTo0OVrOHDBadA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyMTA0Mw==", "bodyText": "This works because Hot Restart does not support arbitrary server crashes according to this note, no regression exists compared to old versioning mechanism, but i wonder if there is any regression for the effective behavior (i mean for undocumented working of the system) of old mechanism with global-partition-table-version? For example, can we say, in some arbitrary server crash scenarios, older mechanism is better?", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472921043", "createdAt": "2020-08-19T10:18:14Z", "author": {"login": "ahmetmircik"}, "path": "docs/design/partitioning/07-parallel-migrations.md", "diffHunk": "@@ -0,0 +1,536 @@\n+# Parallel Independent Migrations\n+\n+|\u2139\ufe0f Since: 4.1| \n+|-------------|\n+\n+## Background\n+\n+The partition table is an eventually consistent and monotonic data structure. It\u2019s only updated by the master member \n+and published to the cluster. Some members may lag behind the master\u2019s version at any time but for those members to be \n+able to participate in a migration, they have to get the latest partition table from master. To provide the monotonic \n+order, the partition table has a global version. And because of this global order, we have to migrate partitions \n+one-by-one. Once the master member changes, it fetches the partition tables and the list of completed & active migrations \n+from the cluster and determines the most recent version of the partition table and then publishes it back. \n+\n+Right now this is a very strict guarantee, because there were data loss issues during shutdown and migrations \n+a few years back and we wanted all partition updates and migrations to be more consistent.\n+\n+The good thing is, in practice, we don\u2019t need this global partition table update order. Because, partitions are not \n+dependent on each other. Instead of a global partition table version, we can have a version per partition. And we can \n+update partitions independently. So, after each partition migration, we can just publish that partition info only, \n+not the whole partition table. Also we can migrate different partitions concurrently. For example, assume there are 10 \n+members, we can execute 5 migrations between 5 member pairs in parallel. With these two optimizations \n+(removal of global update order and parallel migrations), repartitioning can become significantly faster than current.\n+\n+### Previous Work\n+\n+Previous works related to partitioning system and migration mechanism: \n+\n+| Version | Design |\n+| :-----: | :----- |\n+| 3.7 | [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md) |\n+| 3.7 | [Graceful Shutdown Improvements](02-graceful-shutdown-improvements.md) |\n+| 3.9 | [Fine-Grained Anti Entropy Mechanism](03-fine-grained-anti-entropy-mechanism.md) |\n+| 3.9 | [Gigantic Cache Migration Enhancements](04-gigantic-cache-migration-enhancements.md) |\n+| 3.12 | [Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) |\n+| 4.0 | [Refactor MigrationListener API](06-refactor-migrationlistener-api.md) |\n+\n+_Being familiar with these will help to understand the new changes._\n+ \n+## Design\n+\n+### 1 - Global Version to Partition Level\n+\n+First of the major changes is the removal of the global partition table update ordering. Instead we will version each \n+partition individually. \n+\n+#### 1.1 - Current Versioning Mechanism\n+\n+Currently, master member increments global version on each partition update. When the partition table update is received \n+from the master, members decide whether to apply or to ignore the update comparing their local version to received \n+version. If local version is less than received version then partition table is applied, otherwise it's ignored. \n+\n+Similarly, `MigrationInfo` contains the partition table versions before and after \n+the migration. When a migration operation is received, members execute the operation only and only if local version is \n+equal to the version before migration. Otherwise, migration is rejected. A migration operation also contains the most \n+recently completed `MigrationInfo` list. This was  introduced in \n+[Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) \n+work. Before executing the migration operation, members apply these completed migrations in case they haven't seen them yet.\n+\n+While committing promotions, promoted member rejects the commit if its local version is greater than the version in operation.\n+This is similar to applying partition table updates.  \n+\n+#### 1.2 - New Versioning Mechanism\n+\n+With the proposed change, partition table version will be removed and master will update specific partition's version \n+on partition updates. On partition table broadcast, receiving members will do version comparisons for individual \n+partitions. So, they can partially apply, partially ignore the received partition table.\n+\n+During migrations, migration operations will still carry the most recent completed migrations, but only for the partition\n+that migration is belongs to. After applying completed migrations of that partition, migration operation will check \n+the local version against the version in `MigrationInfo`. If they are equal operation will be executed, otherwise it will\n+be rejected.\n+\n+For some cases, we need to check if the whole partition table has changed or not, with a simple comparison. Comparing each \n+partition version one-by-one is not simple and requires keeping all partition versions locally. For those cases, instead \n+of global partition table version, _hash/stamp_ of the partition table will be used. This stamp will be cached locally\n+and updated on each partition modification and also will be easily calculated with a pure hash function. We will use\n+a `64-bit MurmurHash3` to calculate the stamp.\n+\n+```java\n+public static long calculateStamp(InternalPartition[] partitions) {\n+    byte[] bb = new byte[Integer.BYTES * partitions.length];\n+    for (InternalPartition partition : partitions) {\n+        Bits.writeIntB(bb, partition.getPartitionId() * Integer.BYTES, partition.version());\n+    }\n+    return HashUtil.MurmurHash3_x64_64(bb, 0, bb.length);\n+}\n+```  \n+\n+During promotion commit, partition table stamp will be used the check whether local partition table is the same with \n+the one received with promotion. If yes, promotion commit will be ignored. Otherwise, we'll compare versions of individual\n+partitions and skip promotions of partitions which have greater version locally.    \n+   \n+_Other usages of partition table stamp will be explained in later sections._   \n+  \n+\n+### 2 - Parallel Migrations\n+\n+#### 2.1 - Current Migration Mechanism\n+\n+When a new member joins or an existing member leaves the cluster, eventually a `RepartitioningTask` is scheduled on \n+migration thread. `RepartitioningTask` prepares the new partition table and schedules the individual `MigrationTask`s\n+to reach the target partition table. These `MigrationTask`s are executed serially by migration thread and they submit \n+migration operations to the migration participants. After each migration completion, migration is committed to the \n+destination, and partition table is updated. All of these steps are executed serially.\n+\n+ \n+#### 2.2 - Parallel Migrations Mechanism\n+\n+With parallel migrations, most the mechanism remains the same. `RepartitioningTask` prepares the migration plan using \n+the target partition table. Instead of individual `MigrationTask`s, a single `MigrationPlanTask` will be scheduled. \n+`MigrationPlanTask` will submit parallel migration tasks and keep track of started/completed tasks. \n+\n+Migrations belonging to the same partition must be executed in order serially. This is required to provide consistency \n+and safety guarantees while migrating/replicating partition data. For instance, a migration can depend on an earlier one\n+in the plan. Or replication to a missing backup replica is more important than a migration that is just to rebalance the load.\n+_For more info please see [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md)._   \n+\n+To achieve this, `RepartitioningTask` creates migration plan with a consistent order. And `MigrationPlanTask` will \n+parallelize only migrations belonging to different partitions. Migrations belonging to the same partition will still be \n+executed serially. That's the upper limit of migration parallelization. But still we may want to limit parallel \n+migrations to keep heap memory usage under control and to avoid overload of network resources.\n+\n+A new `ClusterProperty`, `hazelcast.partition.max.parallel.migrations`, will be introduced, with a default value of `10`. \n+This property defines the maximum number of partition migrations to be executed on a member. That member can be either source \n+or destination of the migration. _Value of `10` is selected as a heuristic after benchmarking on EC2. You can find the \n+benchmark results below._ There may be consequences of using too much or too less parallelization:\n+\n+* Having too much parallelization can increase the heap memory usage and overload the network during partition rebalance.\n+* Having less parallelization can increase the total migration completion time.\n+       \n+\n+### 3 - Other Changes\n+\n+#### 3.1 - Cluster State Management Changes\n+\n+Cluster state & version changes are performed via transactional process and one of the prerequisites was partition table\n+version of the transaction originator must be equal to the version of the each transaction participant. Since partition\n+table version is now removed in favor of per-partition versions, we will use _partition table stamp_ instead.     \n+\n+#### 3.2 - Hot Restart Changes\n+\n+There will be a few small changes. First one is partition table persistence format. Format will be decided due to cluster \n+version while restoring/persisting partition table. When cluster version is upgraded, partition table will be persisted \n+with the new format once. \n+\n+Current format is:\n+\n+```\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n+| Partition Table Version     |\n++-----------------------------+\n+| Partition Table Length      |\n++-----------------------------+\n+| Replicas (Address + UUID)   |\n++-----------------------------+\n+| Partition-0 Replica Indexes |\n++-----------------------------+\n+| Partition-1 Replica Indexes |\n++-----------------------------+\n+| ........................... |\n++-----------------------------+\n+| Partition-N Replica Indexes |\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+     \n+```                                 \n+\n+New format will be:\n+\n+```\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n+| Partition Table Length      |\n++-----------------------------+\n+| Replicas (Address + UUID)   |\n++-----------------------------+\n+| Partition-0 Version         |\n++-----------------------------+\n+| Partition-0 Replica Indexes |\n++-----------------------------+\n+| Partition-1 Version         |\n++-----------------------------+\n+| Partition-1 Replica Indexes |\n++-----------------------------+\n+| ........................... |\n++-----------------------------+\n+| Partition-N Version         |\n++-----------------------------+\n+| Partition-N Replica Indexes |\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+     \n+```          \n+\n+Second change is in the partition table restore process. While restoring partition table, if a replica address change is\n+detected, we replace the old address of the replica with the new one in the partition table. In current mechanism, \n+global partition version is incremented by the number of updates, as if partition table is updated in the runtime.\n+With the new versioning, only updated partitions' versions will be incremented.\n+\n+Third change is in the partial restart decision process. During partial restart, we decide which members to perform the \n+restart, which members to be excluded and delete their data. To select member groups having the same partition table,\n+we use the partition table version. Members with the same partition table are grouped together. Also to find the most \n+recent partition table, we pick the partition table with the greatest version. If the policy is `PARTIAL_RECOVERY_MOST_RECENT`,\n+we pick the group the greatest partition table version. If it's `PARTIAL_RECOVERY_MOST_COMPLETE`, then we pick \n+the largest group. In other places we will use _partition table stamp_ instead of removed partition table version. But in\n+this case, we cannot use _partition table stamp_. Because stamp is only valid for equality comparison, it cannot be used\n+to compare order of the partition tables. Instead we will use the sum of per-partition versions to compare the order. \n+_This works because Hot Restart does not support arbitrary server crashes. Only proper server shutdowns are supported._", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 204}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzMDkzMg==", "bodyText": "I think no, there won't be any observable behavior change.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472930932", "createdAt": "2020-08-19T10:35:49Z", "author": {"login": "mdogan"}, "path": "docs/design/partitioning/07-parallel-migrations.md", "diffHunk": "@@ -0,0 +1,536 @@\n+# Parallel Independent Migrations\n+\n+|\u2139\ufe0f Since: 4.1| \n+|-------------|\n+\n+## Background\n+\n+The partition table is an eventually consistent and monotonic data structure. It\u2019s only updated by the master member \n+and published to the cluster. Some members may lag behind the master\u2019s version at any time but for those members to be \n+able to participate in a migration, they have to get the latest partition table from master. To provide the monotonic \n+order, the partition table has a global version. And because of this global order, we have to migrate partitions \n+one-by-one. Once the master member changes, it fetches the partition tables and the list of completed & active migrations \n+from the cluster and determines the most recent version of the partition table and then publishes it back. \n+\n+Right now this is a very strict guarantee, because there were data loss issues during shutdown and migrations \n+a few years back and we wanted all partition updates and migrations to be more consistent.\n+\n+The good thing is, in practice, we don\u2019t need this global partition table update order. Because, partitions are not \n+dependent on each other. Instead of a global partition table version, we can have a version per partition. And we can \n+update partitions independently. So, after each partition migration, we can just publish that partition info only, \n+not the whole partition table. Also we can migrate different partitions concurrently. For example, assume there are 10 \n+members, we can execute 5 migrations between 5 member pairs in parallel. With these two optimizations \n+(removal of global update order and parallel migrations), repartitioning can become significantly faster than current.\n+\n+### Previous Work\n+\n+Previous works related to partitioning system and migration mechanism: \n+\n+| Version | Design |\n+| :-----: | :----- |\n+| 3.7 | [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md) |\n+| 3.7 | [Graceful Shutdown Improvements](02-graceful-shutdown-improvements.md) |\n+| 3.9 | [Fine-Grained Anti Entropy Mechanism](03-fine-grained-anti-entropy-mechanism.md) |\n+| 3.9 | [Gigantic Cache Migration Enhancements](04-gigantic-cache-migration-enhancements.md) |\n+| 3.12 | [Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) |\n+| 4.0 | [Refactor MigrationListener API](06-refactor-migrationlistener-api.md) |\n+\n+_Being familiar with these will help to understand the new changes._\n+ \n+## Design\n+\n+### 1 - Global Version to Partition Level\n+\n+First of the major changes is the removal of the global partition table update ordering. Instead we will version each \n+partition individually. \n+\n+#### 1.1 - Current Versioning Mechanism\n+\n+Currently, master member increments global version on each partition update. When the partition table update is received \n+from the master, members decide whether to apply or to ignore the update comparing their local version to received \n+version. If local version is less than received version then partition table is applied, otherwise it's ignored. \n+\n+Similarly, `MigrationInfo` contains the partition table versions before and after \n+the migration. When a migration operation is received, members execute the operation only and only if local version is \n+equal to the version before migration. Otherwise, migration is rejected. A migration operation also contains the most \n+recently completed `MigrationInfo` list. This was  introduced in \n+[Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) \n+work. Before executing the migration operation, members apply these completed migrations in case they haven't seen them yet.\n+\n+While committing promotions, promoted member rejects the commit if its local version is greater than the version in operation.\n+This is similar to applying partition table updates.  \n+\n+#### 1.2 - New Versioning Mechanism\n+\n+With the proposed change, partition table version will be removed and master will update specific partition's version \n+on partition updates. On partition table broadcast, receiving members will do version comparisons for individual \n+partitions. So, they can partially apply, partially ignore the received partition table.\n+\n+During migrations, migration operations will still carry the most recent completed migrations, but only for the partition\n+that migration is belongs to. After applying completed migrations of that partition, migration operation will check \n+the local version against the version in `MigrationInfo`. If they are equal operation will be executed, otherwise it will\n+be rejected.\n+\n+For some cases, we need to check if the whole partition table has changed or not, with a simple comparison. Comparing each \n+partition version one-by-one is not simple and requires keeping all partition versions locally. For those cases, instead \n+of global partition table version, _hash/stamp_ of the partition table will be used. This stamp will be cached locally\n+and updated on each partition modification and also will be easily calculated with a pure hash function. We will use\n+a `64-bit MurmurHash3` to calculate the stamp.\n+\n+```java\n+public static long calculateStamp(InternalPartition[] partitions) {\n+    byte[] bb = new byte[Integer.BYTES * partitions.length];\n+    for (InternalPartition partition : partitions) {\n+        Bits.writeIntB(bb, partition.getPartitionId() * Integer.BYTES, partition.version());\n+    }\n+    return HashUtil.MurmurHash3_x64_64(bb, 0, bb.length);\n+}\n+```  \n+\n+During promotion commit, partition table stamp will be used the check whether local partition table is the same with \n+the one received with promotion. If yes, promotion commit will be ignored. Otherwise, we'll compare versions of individual\n+partitions and skip promotions of partitions which have greater version locally.    \n+   \n+_Other usages of partition table stamp will be explained in later sections._   \n+  \n+\n+### 2 - Parallel Migrations\n+\n+#### 2.1 - Current Migration Mechanism\n+\n+When a new member joins or an existing member leaves the cluster, eventually a `RepartitioningTask` is scheduled on \n+migration thread. `RepartitioningTask` prepares the new partition table and schedules the individual `MigrationTask`s\n+to reach the target partition table. These `MigrationTask`s are executed serially by migration thread and they submit \n+migration operations to the migration participants. After each migration completion, migration is committed to the \n+destination, and partition table is updated. All of these steps are executed serially.\n+\n+ \n+#### 2.2 - Parallel Migrations Mechanism\n+\n+With parallel migrations, most the mechanism remains the same. `RepartitioningTask` prepares the migration plan using \n+the target partition table. Instead of individual `MigrationTask`s, a single `MigrationPlanTask` will be scheduled. \n+`MigrationPlanTask` will submit parallel migration tasks and keep track of started/completed tasks. \n+\n+Migrations belonging to the same partition must be executed in order serially. This is required to provide consistency \n+and safety guarantees while migrating/replicating partition data. For instance, a migration can depend on an earlier one\n+in the plan. Or replication to a missing backup replica is more important than a migration that is just to rebalance the load.\n+_For more info please see [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md)._   \n+\n+To achieve this, `RepartitioningTask` creates migration plan with a consistent order. And `MigrationPlanTask` will \n+parallelize only migrations belonging to different partitions. Migrations belonging to the same partition will still be \n+executed serially. That's the upper limit of migration parallelization. But still we may want to limit parallel \n+migrations to keep heap memory usage under control and to avoid overload of network resources.\n+\n+A new `ClusterProperty`, `hazelcast.partition.max.parallel.migrations`, will be introduced, with a default value of `10`. \n+This property defines the maximum number of partition migrations to be executed on a member. That member can be either source \n+or destination of the migration. _Value of `10` is selected as a heuristic after benchmarking on EC2. You can find the \n+benchmark results below._ There may be consequences of using too much or too less parallelization:\n+\n+* Having too much parallelization can increase the heap memory usage and overload the network during partition rebalance.\n+* Having less parallelization can increase the total migration completion time.\n+       \n+\n+### 3 - Other Changes\n+\n+#### 3.1 - Cluster State Management Changes\n+\n+Cluster state & version changes are performed via transactional process and one of the prerequisites was partition table\n+version of the transaction originator must be equal to the version of the each transaction participant. Since partition\n+table version is now removed in favor of per-partition versions, we will use _partition table stamp_ instead.     \n+\n+#### 3.2 - Hot Restart Changes\n+\n+There will be a few small changes. First one is partition table persistence format. Format will be decided due to cluster \n+version while restoring/persisting partition table. When cluster version is upgraded, partition table will be persisted \n+with the new format once. \n+\n+Current format is:\n+\n+```\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n+| Partition Table Version     |\n++-----------------------------+\n+| Partition Table Length      |\n++-----------------------------+\n+| Replicas (Address + UUID)   |\n++-----------------------------+\n+| Partition-0 Replica Indexes |\n++-----------------------------+\n+| Partition-1 Replica Indexes |\n++-----------------------------+\n+| ........................... |\n++-----------------------------+\n+| Partition-N Replica Indexes |\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+     \n+```                                 \n+\n+New format will be:\n+\n+```\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n+| Partition Table Length      |\n++-----------------------------+\n+| Replicas (Address + UUID)   |\n++-----------------------------+\n+| Partition-0 Version         |\n++-----------------------------+\n+| Partition-0 Replica Indexes |\n++-----------------------------+\n+| Partition-1 Version         |\n++-----------------------------+\n+| Partition-1 Replica Indexes |\n++-----------------------------+\n+| ........................... |\n++-----------------------------+\n+| Partition-N Version         |\n++-----------------------------+\n+| Partition-N Replica Indexes |\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+     \n+```          \n+\n+Second change is in the partition table restore process. While restoring partition table, if a replica address change is\n+detected, we replace the old address of the replica with the new one in the partition table. In current mechanism, \n+global partition version is incremented by the number of updates, as if partition table is updated in the runtime.\n+With the new versioning, only updated partitions' versions will be incremented.\n+\n+Third change is in the partial restart decision process. During partial restart, we decide which members to perform the \n+restart, which members to be excluded and delete their data. To select member groups having the same partition table,\n+we use the partition table version. Members with the same partition table are grouped together. Also to find the most \n+recent partition table, we pick the partition table with the greatest version. If the policy is `PARTIAL_RECOVERY_MOST_RECENT`,\n+we pick the group the greatest partition table version. If it's `PARTIAL_RECOVERY_MOST_COMPLETE`, then we pick \n+the largest group. In other places we will use _partition table stamp_ instead of removed partition table version. But in\n+this case, we cannot use _partition table stamp_. Because stamp is only valid for equality comparison, it cannot be used\n+to compare order of the partition tables. Instead we will use the sum of per-partition versions to compare the order. \n+_This works because Hot Restart does not support arbitrary server crashes. Only proper server shutdowns are supported._", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyMTA0Mw=="}, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 204}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjA2Nzc1OnYy", "diffSide": "RIGHT", "path": "docs/design/partitioning/07-parallel-migrations.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDoyMToyOVrOHDA7Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo0NDoxMVrOHDBr0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyMjkxNA==", "bodyText": "I can see no tests with default value 10.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472922914", "createdAt": "2020-08-19T10:21:29Z", "author": {"login": "ahmetmircik"}, "path": "docs/design/partitioning/07-parallel-migrations.md", "diffHunk": "@@ -0,0 +1,536 @@\n+# Parallel Independent Migrations\n+\n+|\u2139\ufe0f Since: 4.1| \n+|-------------|\n+\n+## Background\n+\n+The partition table is an eventually consistent and monotonic data structure. It\u2019s only updated by the master member \n+and published to the cluster. Some members may lag behind the master\u2019s version at any time but for those members to be \n+able to participate in a migration, they have to get the latest partition table from master. To provide the monotonic \n+order, the partition table has a global version. And because of this global order, we have to migrate partitions \n+one-by-one. Once the master member changes, it fetches the partition tables and the list of completed & active migrations \n+from the cluster and determines the most recent version of the partition table and then publishes it back. \n+\n+Right now this is a very strict guarantee, because there were data loss issues during shutdown and migrations \n+a few years back and we wanted all partition updates and migrations to be more consistent.\n+\n+The good thing is, in practice, we don\u2019t need this global partition table update order. Because, partitions are not \n+dependent on each other. Instead of a global partition table version, we can have a version per partition. And we can \n+update partitions independently. So, after each partition migration, we can just publish that partition info only, \n+not the whole partition table. Also we can migrate different partitions concurrently. For example, assume there are 10 \n+members, we can execute 5 migrations between 5 member pairs in parallel. With these two optimizations \n+(removal of global update order and parallel migrations), repartitioning can become significantly faster than current.\n+\n+### Previous Work\n+\n+Previous works related to partitioning system and migration mechanism: \n+\n+| Version | Design |\n+| :-----: | :----- |\n+| 3.7 | [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md) |\n+| 3.7 | [Graceful Shutdown Improvements](02-graceful-shutdown-improvements.md) |\n+| 3.9 | [Fine-Grained Anti Entropy Mechanism](03-fine-grained-anti-entropy-mechanism.md) |\n+| 3.9 | [Gigantic Cache Migration Enhancements](04-gigantic-cache-migration-enhancements.md) |\n+| 3.12 | [Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) |\n+| 4.0 | [Refactor MigrationListener API](06-refactor-migrationlistener-api.md) |\n+\n+_Being familiar with these will help to understand the new changes._\n+ \n+## Design\n+\n+### 1 - Global Version to Partition Level\n+\n+First of the major changes is the removal of the global partition table update ordering. Instead we will version each \n+partition individually. \n+\n+#### 1.1 - Current Versioning Mechanism\n+\n+Currently, master member increments global version on each partition update. When the partition table update is received \n+from the master, members decide whether to apply or to ignore the update comparing their local version to received \n+version. If local version is less than received version then partition table is applied, otherwise it's ignored. \n+\n+Similarly, `MigrationInfo` contains the partition table versions before and after \n+the migration. When a migration operation is received, members execute the operation only and only if local version is \n+equal to the version before migration. Otherwise, migration is rejected. A migration operation also contains the most \n+recently completed `MigrationInfo` list. This was  introduced in \n+[Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) \n+work. Before executing the migration operation, members apply these completed migrations in case they haven't seen them yet.\n+\n+While committing promotions, promoted member rejects the commit if its local version is greater than the version in operation.\n+This is similar to applying partition table updates.  \n+\n+#### 1.2 - New Versioning Mechanism\n+\n+With the proposed change, partition table version will be removed and master will update specific partition's version \n+on partition updates. On partition table broadcast, receiving members will do version comparisons for individual \n+partitions. So, they can partially apply, partially ignore the received partition table.\n+\n+During migrations, migration operations will still carry the most recent completed migrations, but only for the partition\n+that migration is belongs to. After applying completed migrations of that partition, migration operation will check \n+the local version against the version in `MigrationInfo`. If they are equal operation will be executed, otherwise it will\n+be rejected.\n+\n+For some cases, we need to check if the whole partition table has changed or not, with a simple comparison. Comparing each \n+partition version one-by-one is not simple and requires keeping all partition versions locally. For those cases, instead \n+of global partition table version, _hash/stamp_ of the partition table will be used. This stamp will be cached locally\n+and updated on each partition modification and also will be easily calculated with a pure hash function. We will use\n+a `64-bit MurmurHash3` to calculate the stamp.\n+\n+```java\n+public static long calculateStamp(InternalPartition[] partitions) {\n+    byte[] bb = new byte[Integer.BYTES * partitions.length];\n+    for (InternalPartition partition : partitions) {\n+        Bits.writeIntB(bb, partition.getPartitionId() * Integer.BYTES, partition.version());\n+    }\n+    return HashUtil.MurmurHash3_x64_64(bb, 0, bb.length);\n+}\n+```  \n+\n+During promotion commit, partition table stamp will be used the check whether local partition table is the same with \n+the one received with promotion. If yes, promotion commit will be ignored. Otherwise, we'll compare versions of individual\n+partitions and skip promotions of partitions which have greater version locally.    \n+   \n+_Other usages of partition table stamp will be explained in later sections._   \n+  \n+\n+### 2 - Parallel Migrations\n+\n+#### 2.1 - Current Migration Mechanism\n+\n+When a new member joins or an existing member leaves the cluster, eventually a `RepartitioningTask` is scheduled on \n+migration thread. `RepartitioningTask` prepares the new partition table and schedules the individual `MigrationTask`s\n+to reach the target partition table. These `MigrationTask`s are executed serially by migration thread and they submit \n+migration operations to the migration participants. After each migration completion, migration is committed to the \n+destination, and partition table is updated. All of these steps are executed serially.\n+\n+ \n+#### 2.2 - Parallel Migrations Mechanism\n+\n+With parallel migrations, most the mechanism remains the same. `RepartitioningTask` prepares the migration plan using \n+the target partition table. Instead of individual `MigrationTask`s, a single `MigrationPlanTask` will be scheduled. \n+`MigrationPlanTask` will submit parallel migration tasks and keep track of started/completed tasks. \n+\n+Migrations belonging to the same partition must be executed in order serially. This is required to provide consistency \n+and safety guarantees while migrating/replicating partition data. For instance, a migration can depend on an earlier one\n+in the plan. Or replication to a missing backup replica is more important than a migration that is just to rebalance the load.\n+_For more info please see [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md)._   \n+\n+To achieve this, `RepartitioningTask` creates migration plan with a consistent order. And `MigrationPlanTask` will \n+parallelize only migrations belonging to different partitions. Migrations belonging to the same partition will still be \n+executed serially. That's the upper limit of migration parallelization. But still we may want to limit parallel \n+migrations to keep heap memory usage under control and to avoid overload of network resources.\n+\n+A new `ClusterProperty`, `hazelcast.partition.max.parallel.migrations`, will be introduced, with a default value of `10`. \n+This property defines the maximum number of partition migrations to be executed on a member. That member can be either source \n+or destination of the migration. _Value of `10` is selected as a heuristic after benchmarking on EC2. You can find the \n+benchmark results below._ There may be consequences of using too much or too less parallelization:\n+\n+* Having too much parallelization can increase the heap memory usage and overload the network during partition rebalance.\n+* Having less parallelization can increase the total migration completion time.\n+       \n+\n+### 3 - Other Changes\n+\n+#### 3.1 - Cluster State Management Changes\n+\n+Cluster state & version changes are performed via transactional process and one of the prerequisites was partition table\n+version of the transaction originator must be equal to the version of the each transaction participant. Since partition\n+table version is now removed in favor of per-partition versions, we will use _partition table stamp_ instead.     \n+\n+#### 3.2 - Hot Restart Changes\n+\n+There will be a few small changes. First one is partition table persistence format. Format will be decided due to cluster \n+version while restoring/persisting partition table. When cluster version is upgraded, partition table will be persisted \n+with the new format once. \n+\n+Current format is:\n+\n+```\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n+| Partition Table Version     |\n++-----------------------------+\n+| Partition Table Length      |\n++-----------------------------+\n+| Replicas (Address + UUID)   |\n++-----------------------------+\n+| Partition-0 Replica Indexes |\n++-----------------------------+\n+| Partition-1 Replica Indexes |\n++-----------------------------+\n+| ........................... |\n++-----------------------------+\n+| Partition-N Replica Indexes |\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+     \n+```                                 \n+\n+New format will be:\n+\n+```\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n+| Partition Table Length      |\n++-----------------------------+\n+| Replicas (Address + UUID)   |\n++-----------------------------+\n+| Partition-0 Version         |\n++-----------------------------+\n+| Partition-0 Replica Indexes |\n++-----------------------------+\n+| Partition-1 Version         |\n++-----------------------------+\n+| Partition-1 Replica Indexes |\n++-----------------------------+\n+| ........................... |\n++-----------------------------+\n+| Partition-N Version         |\n++-----------------------------+\n+| Partition-N Replica Indexes |\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+     \n+```          \n+\n+Second change is in the partition table restore process. While restoring partition table, if a replica address change is\n+detected, we replace the old address of the replica with the new one in the partition table. In current mechanism, \n+global partition version is incremented by the number of updates, as if partition table is updated in the runtime.\n+With the new versioning, only updated partitions' versions will be incremented.\n+\n+Third change is in the partial restart decision process. During partial restart, we decide which members to perform the \n+restart, which members to be excluded and delete their data. To select member groups having the same partition table,\n+we use the partition table version. Members with the same partition table are grouped together. Also to find the most \n+recent partition table, we pick the partition table with the greatest version. If the policy is `PARTIAL_RECOVERY_MOST_RECENT`,\n+we pick the group the greatest partition table version. If it's `PARTIAL_RECOVERY_MOST_COMPLETE`, then we pick \n+the largest group. In other places we will use _partition table stamp_ instead of removed partition table version. But in\n+this case, we cannot use _partition table stamp_. Because stamp is only valid for equality comparison, it cannot be used\n+to compare order of the partition tables. Instead we will use the sum of per-partition versions to compare the order. \n+_This works because Hot Restart does not support arbitrary server crashes. Only proper server shutdowns are supported._\n+Otherwise, resolving conflicts on partition level, determining the most recent versions of each partition, composing\n+a new partition table with these partitions and adapting Hot Restart to restore data based on this new partition table\n+requires a large rework and time. And supporting rolling upgrade scenarios makes this more complicated.            \n+\n+\n+#### 3.3 - Rolling Upgrade\n+\n+- To support rolling upgrade, we will have two distinct paths for most the migration scheduling and execution logic. \n+\n+- While updating partition table and applying completed migrations, there'll be separate paths too.\n+\n+- Once the cluster is upgraded to `4.1`, all partition versions will be set to the latest partition table version.\n+\n+- Cluster state change transaction mechanism will use either partition table version or stamp according to the version.\n+\n+- Similarly Hot Restart partition restoration and persistence mechanism will use different logic according to \n+the version. There will be `LegacyPartitionTableReader/Writer` classes to support old format.  \n+\n+\n+## Benchmarks\n+\n+We compared `parallel-migrations` branch with `master` branch and measured stabilization times of shutting down single\n+member, adding a new member and crash of a single member scenarios. \n+\n+### Setup\n+\n+- Tested on EC2 with `hzCmd` tool.\n+- Used `r5.16xlarge` (64 vCPU, 512 GiB memory, 20 Gbps network bandwidth) instance type.\n+- Member JVM heap memory settings was `-Xms6G -Xmx6G`.\n+- Partition count was `20k`.\n+- Configured native memory size was 500GB. \n+- Inserted total of `135 million` entries to either a single map or `10` different maps (`13.5 million` per-map) with single backup.\n+- Value size was `16K` bytes.\n+- There were continuous `IMap.get/set` operations by 20 threads per-member.\n+\n+#### Server configuration:\n+```xml\n+<hazelcast>\n+    <properties>\n+        <property name=\"hazelcast.phone.home.enabled\">false</property>\n+        <property name=\"hazelcast.hidensity.check.freememory\">false</property>\n+        <property name=\"hazelcast.health.monitoring.level\">NOISY</property>\n+        <property name=\"hazelcast.health.monitoring.delay.seconds\">15</property>\n+        <property name=\"hazelcast.diagnostics.enabled\">true</property>\n+        <property name=\"hazelcast.diagnostics.metrics.period.seconds\">10</property>   \n+        <property name=\"hazelcast.graceful.shutdown.max.wait\">7200</property>   \n+        <property name=\"hazelcast.partition.count\">20000</property>\n+        <property name=\"hazelcast.partition.max.parallel.migrations\">20</property>\n+    </properties>\n+\n+    ...\n+\n+    <native-memory allocator-type=\"POOLED\" enabled=\"true\">\n+        <size unit=\"GIGABYTES\" value=\"500\"/>\n+        <!-- disabled schema validation via hazelcast.internal.override.version system property. -->\n+        <metadata-space-percentage>1</metadata-space-percentage>\n+    </native-memory>\n+\n+    <map name=\"default\">\n+        <in-memory-format>NATIVE</in-memory-format>\n+    </map>\n+</hazelcast>\n+```\n+\n+#### Memory usage:\n+\n+**Native memory usage with 10 members:**\n+```\n+native.memory.used=414.3G, \n+native.memory.free=80.7G, \n+native.memory.total=414.4G, \n+native.memory.max=495.0G, \n+native.meta.memory.used=1011.8M, \n+native.meta.memory.free=4.0G\n+```\n+\n+**Native memory usage with 9 members:**\n+```\n+native.memory.used=460.4G, \n+native.memory.free=34.6G, \n+native.memory.total=460.7G, \n+native.memory.max=495.0G, \n+native.meta.memory.used=1.1G, \n+native.meta.memory.free=3.9G\n+```\n+\n+This was total of `~4.1 TB` data (including the backup) in the whole cluster.\n+\n+### Test Flow\n+\n+```shell script\n+# Initially started 10 members\n+hz cluster -size M10 -v 4.1-SNAPSHOT -ee -boxes a.box -user ec2-user\n+\n+# Load initial data into map\n+hz run ops/load     \n+# Validate map size\n+hz run ops/mapSize\n+               \n+# Start get/set ops at the background\n+hz run ops/get &\n+hz run ops/set &\n+       \n+# Scale-down \n+# Shutdown Member10\n+hz driver Member10\n+hz run ops/shutDown            \n+# Verify cluster-size=9 and wait until migrations complete\n+hz run ops/untilClusterSize9 ops/untilClusterSafe        \n+\n+# Scale-up\n+# Start Member10 back\n+hz boot Member10\n+# Verify cluster-size=10 and wait until migrations complete\n+hz run ops/untilClusterSize10 ops/untilClusterSafe\n+\n+# Member failure\n+# Kill Member10\n+hz kill Member10\n+# Verify cluster-size=9 and wait until migrations complete\n+hz run ops/untilClusterSize9 ops/untilClusterSafe9\n+```\n+\n+### Single Map Benchmark\n+\n+Used a single HD `IMap` and inserted `135 million` entries with `~16k` values. \n+\n+#### Master Branch\n+\n+- *Scale down from 10 to 9 members*: `14000` migrations were completed in **35 minutes and 34 seconds**. \n+Member shutdown in **2145 seconds**.\n+\n+- *Scale up from 9 to 10 members*: `14000` migrations were completed in **34 minutes and 16 seconds**.\n+\n+- *Crash single member*: Around `~26150` migrations were completed in **33 minutes and 38 seconds**.\n+\n+| GET p99 Latency  | GET p99.9 Latency |\n+|:-------------:|:-------------:|\n+| ![Plot](resources/07/single/master/get/p99.png) | ![Plot](resources/07/single/master/get/p999.png) |\n+| GET p99 Min-Max  | GET p99.9 Min-Max |\n+| ![Plot](resources/07/single/master/get/p99-min-mean-max.png) | ![Plot](resources/07/single/master/get/p999-min-mean-max.png) |\n+\n+\n+| SET p99 Latency  | SET p99.9 Latency |\n+|:-------------:|:-------------:|\n+| ![Plot](resources/07/single/master/set/p99.png) | ![Plot](resources/07/single/master/set/p999.png) |\n+| SET p99 Min-Max  | SET p99.9 Min-Max |\n+| ![Plot](resources/07/single/master/set/p99-min-mean-max.png) | ![Plot](resources/07/single/master/set/p999-min-mean-max.png) |\n+\n+\n+**Heap Usage and GC Pauses**\n+\n+![Plot](resources/07/single/master/heap.png)\n+\n+| GC Summary  | |\n+|:-------------|-------------:|\n+| Full collections |\t36 |\n+| Mean garbage collection pause (ms) |\t89.4 |\n+| Mean interval between collections (ms) |\t180394 |\n+| Proportion of time spent in garbage collection pauses (%) |\t3.39 |\n+| Proportion of time spent unpaused (%) |\t96.61 |\n+\n+\n+---\n+\n+#### Parallel Migrations - 5x\n+\n+5x parallelization was used: `hazelcast.partition.max.parallel.migrations=5`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 372}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzNTM3OA==", "bodyText": "Initially I was considering 5 as the default value and during PoC (which you can see the results on wiki) I did tests with 5, 10 and 20. But for final tests I just used 5 and 20 to show there's no very significant difference between them. Then I picked 10 as the default value, so a middle-ground.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472935378", "createdAt": "2020-08-19T10:44:11Z", "author": {"login": "mdogan"}, "path": "docs/design/partitioning/07-parallel-migrations.md", "diffHunk": "@@ -0,0 +1,536 @@\n+# Parallel Independent Migrations\n+\n+|\u2139\ufe0f Since: 4.1| \n+|-------------|\n+\n+## Background\n+\n+The partition table is an eventually consistent and monotonic data structure. It\u2019s only updated by the master member \n+and published to the cluster. Some members may lag behind the master\u2019s version at any time but for those members to be \n+able to participate in a migration, they have to get the latest partition table from master. To provide the monotonic \n+order, the partition table has a global version. And because of this global order, we have to migrate partitions \n+one-by-one. Once the master member changes, it fetches the partition tables and the list of completed & active migrations \n+from the cluster and determines the most recent version of the partition table and then publishes it back. \n+\n+Right now this is a very strict guarantee, because there were data loss issues during shutdown and migrations \n+a few years back and we wanted all partition updates and migrations to be more consistent.\n+\n+The good thing is, in practice, we don\u2019t need this global partition table update order. Because, partitions are not \n+dependent on each other. Instead of a global partition table version, we can have a version per partition. And we can \n+update partitions independently. So, after each partition migration, we can just publish that partition info only, \n+not the whole partition table. Also we can migrate different partitions concurrently. For example, assume there are 10 \n+members, we can execute 5 migrations between 5 member pairs in parallel. With these two optimizations \n+(removal of global update order and parallel migrations), repartitioning can become significantly faster than current.\n+\n+### Previous Work\n+\n+Previous works related to partitioning system and migration mechanism: \n+\n+| Version | Design |\n+| :-----: | :----- |\n+| 3.7 | [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md) |\n+| 3.7 | [Graceful Shutdown Improvements](02-graceful-shutdown-improvements.md) |\n+| 3.9 | [Fine-Grained Anti Entropy Mechanism](03-fine-grained-anti-entropy-mechanism.md) |\n+| 3.9 | [Gigantic Cache Migration Enhancements](04-gigantic-cache-migration-enhancements.md) |\n+| 3.12 | [Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) |\n+| 4.0 | [Refactor MigrationListener API](06-refactor-migrationlistener-api.md) |\n+\n+_Being familiar with these will help to understand the new changes._\n+ \n+## Design\n+\n+### 1 - Global Version to Partition Level\n+\n+First of the major changes is the removal of the global partition table update ordering. Instead we will version each \n+partition individually. \n+\n+#### 1.1 - Current Versioning Mechanism\n+\n+Currently, master member increments global version on each partition update. When the partition table update is received \n+from the master, members decide whether to apply or to ignore the update comparing their local version to received \n+version. If local version is less than received version then partition table is applied, otherwise it's ignored. \n+\n+Similarly, `MigrationInfo` contains the partition table versions before and after \n+the migration. When a migration operation is received, members execute the operation only and only if local version is \n+equal to the version before migration. Otherwise, migration is rejected. A migration operation also contains the most \n+recently completed `MigrationInfo` list. This was  introduced in \n+[Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) \n+work. Before executing the migration operation, members apply these completed migrations in case they haven't seen them yet.\n+\n+While committing promotions, promoted member rejects the commit if its local version is greater than the version in operation.\n+This is similar to applying partition table updates.  \n+\n+#### 1.2 - New Versioning Mechanism\n+\n+With the proposed change, partition table version will be removed and master will update specific partition's version \n+on partition updates. On partition table broadcast, receiving members will do version comparisons for individual \n+partitions. So, they can partially apply, partially ignore the received partition table.\n+\n+During migrations, migration operations will still carry the most recent completed migrations, but only for the partition\n+that migration is belongs to. After applying completed migrations of that partition, migration operation will check \n+the local version against the version in `MigrationInfo`. If they are equal operation will be executed, otherwise it will\n+be rejected.\n+\n+For some cases, we need to check if the whole partition table has changed or not, with a simple comparison. Comparing each \n+partition version one-by-one is not simple and requires keeping all partition versions locally. For those cases, instead \n+of global partition table version, _hash/stamp_ of the partition table will be used. This stamp will be cached locally\n+and updated on each partition modification and also will be easily calculated with a pure hash function. We will use\n+a `64-bit MurmurHash3` to calculate the stamp.\n+\n+```java\n+public static long calculateStamp(InternalPartition[] partitions) {\n+    byte[] bb = new byte[Integer.BYTES * partitions.length];\n+    for (InternalPartition partition : partitions) {\n+        Bits.writeIntB(bb, partition.getPartitionId() * Integer.BYTES, partition.version());\n+    }\n+    return HashUtil.MurmurHash3_x64_64(bb, 0, bb.length);\n+}\n+```  \n+\n+During promotion commit, partition table stamp will be used the check whether local partition table is the same with \n+the one received with promotion. If yes, promotion commit will be ignored. Otherwise, we'll compare versions of individual\n+partitions and skip promotions of partitions which have greater version locally.    \n+   \n+_Other usages of partition table stamp will be explained in later sections._   \n+  \n+\n+### 2 - Parallel Migrations\n+\n+#### 2.1 - Current Migration Mechanism\n+\n+When a new member joins or an existing member leaves the cluster, eventually a `RepartitioningTask` is scheduled on \n+migration thread. `RepartitioningTask` prepares the new partition table and schedules the individual `MigrationTask`s\n+to reach the target partition table. These `MigrationTask`s are executed serially by migration thread and they submit \n+migration operations to the migration participants. After each migration completion, migration is committed to the \n+destination, and partition table is updated. All of these steps are executed serially.\n+\n+ \n+#### 2.2 - Parallel Migrations Mechanism\n+\n+With parallel migrations, most the mechanism remains the same. `RepartitioningTask` prepares the migration plan using \n+the target partition table. Instead of individual `MigrationTask`s, a single `MigrationPlanTask` will be scheduled. \n+`MigrationPlanTask` will submit parallel migration tasks and keep track of started/completed tasks. \n+\n+Migrations belonging to the same partition must be executed in order serially. This is required to provide consistency \n+and safety guarantees while migrating/replicating partition data. For instance, a migration can depend on an earlier one\n+in the plan. Or replication to a missing backup replica is more important than a migration that is just to rebalance the load.\n+_For more info please see [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md)._   \n+\n+To achieve this, `RepartitioningTask` creates migration plan with a consistent order. And `MigrationPlanTask` will \n+parallelize only migrations belonging to different partitions. Migrations belonging to the same partition will still be \n+executed serially. That's the upper limit of migration parallelization. But still we may want to limit parallel \n+migrations to keep heap memory usage under control and to avoid overload of network resources.\n+\n+A new `ClusterProperty`, `hazelcast.partition.max.parallel.migrations`, will be introduced, with a default value of `10`. \n+This property defines the maximum number of partition migrations to be executed on a member. That member can be either source \n+or destination of the migration. _Value of `10` is selected as a heuristic after benchmarking on EC2. You can find the \n+benchmark results below._ There may be consequences of using too much or too less parallelization:\n+\n+* Having too much parallelization can increase the heap memory usage and overload the network during partition rebalance.\n+* Having less parallelization can increase the total migration completion time.\n+       \n+\n+### 3 - Other Changes\n+\n+#### 3.1 - Cluster State Management Changes\n+\n+Cluster state & version changes are performed via transactional process and one of the prerequisites was partition table\n+version of the transaction originator must be equal to the version of the each transaction participant. Since partition\n+table version is now removed in favor of per-partition versions, we will use _partition table stamp_ instead.     \n+\n+#### 3.2 - Hot Restart Changes\n+\n+There will be a few small changes. First one is partition table persistence format. Format will be decided due to cluster \n+version while restoring/persisting partition table. When cluster version is upgraded, partition table will be persisted \n+with the new format once. \n+\n+Current format is:\n+\n+```\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n+| Partition Table Version     |\n++-----------------------------+\n+| Partition Table Length      |\n++-----------------------------+\n+| Replicas (Address + UUID)   |\n++-----------------------------+\n+| Partition-0 Replica Indexes |\n++-----------------------------+\n+| Partition-1 Replica Indexes |\n++-----------------------------+\n+| ........................... |\n++-----------------------------+\n+| Partition-N Replica Indexes |\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+     \n+```                                 \n+\n+New format will be:\n+\n+```\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n+| Partition Table Length      |\n++-----------------------------+\n+| Replicas (Address + UUID)   |\n++-----------------------------+\n+| Partition-0 Version         |\n++-----------------------------+\n+| Partition-0 Replica Indexes |\n++-----------------------------+\n+| Partition-1 Version         |\n++-----------------------------+\n+| Partition-1 Replica Indexes |\n++-----------------------------+\n+| ........................... |\n++-----------------------------+\n+| Partition-N Version         |\n++-----------------------------+\n+| Partition-N Replica Indexes |\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+     \n+```          \n+\n+Second change is in the partition table restore process. While restoring partition table, if a replica address change is\n+detected, we replace the old address of the replica with the new one in the partition table. In current mechanism, \n+global partition version is incremented by the number of updates, as if partition table is updated in the runtime.\n+With the new versioning, only updated partitions' versions will be incremented.\n+\n+Third change is in the partial restart decision process. During partial restart, we decide which members to perform the \n+restart, which members to be excluded and delete their data. To select member groups having the same partition table,\n+we use the partition table version. Members with the same partition table are grouped together. Also to find the most \n+recent partition table, we pick the partition table with the greatest version. If the policy is `PARTIAL_RECOVERY_MOST_RECENT`,\n+we pick the group the greatest partition table version. If it's `PARTIAL_RECOVERY_MOST_COMPLETE`, then we pick \n+the largest group. In other places we will use _partition table stamp_ instead of removed partition table version. But in\n+this case, we cannot use _partition table stamp_. Because stamp is only valid for equality comparison, it cannot be used\n+to compare order of the partition tables. Instead we will use the sum of per-partition versions to compare the order. \n+_This works because Hot Restart does not support arbitrary server crashes. Only proper server shutdowns are supported._\n+Otherwise, resolving conflicts on partition level, determining the most recent versions of each partition, composing\n+a new partition table with these partitions and adapting Hot Restart to restore data based on this new partition table\n+requires a large rework and time. And supporting rolling upgrade scenarios makes this more complicated.            \n+\n+\n+#### 3.3 - Rolling Upgrade\n+\n+- To support rolling upgrade, we will have two distinct paths for most the migration scheduling and execution logic. \n+\n+- While updating partition table and applying completed migrations, there'll be separate paths too.\n+\n+- Once the cluster is upgraded to `4.1`, all partition versions will be set to the latest partition table version.\n+\n+- Cluster state change transaction mechanism will use either partition table version or stamp according to the version.\n+\n+- Similarly Hot Restart partition restoration and persistence mechanism will use different logic according to \n+the version. There will be `LegacyPartitionTableReader/Writer` classes to support old format.  \n+\n+\n+## Benchmarks\n+\n+We compared `parallel-migrations` branch with `master` branch and measured stabilization times of shutting down single\n+member, adding a new member and crash of a single member scenarios. \n+\n+### Setup\n+\n+- Tested on EC2 with `hzCmd` tool.\n+- Used `r5.16xlarge` (64 vCPU, 512 GiB memory, 20 Gbps network bandwidth) instance type.\n+- Member JVM heap memory settings was `-Xms6G -Xmx6G`.\n+- Partition count was `20k`.\n+- Configured native memory size was 500GB. \n+- Inserted total of `135 million` entries to either a single map or `10` different maps (`13.5 million` per-map) with single backup.\n+- Value size was `16K` bytes.\n+- There were continuous `IMap.get/set` operations by 20 threads per-member.\n+\n+#### Server configuration:\n+```xml\n+<hazelcast>\n+    <properties>\n+        <property name=\"hazelcast.phone.home.enabled\">false</property>\n+        <property name=\"hazelcast.hidensity.check.freememory\">false</property>\n+        <property name=\"hazelcast.health.monitoring.level\">NOISY</property>\n+        <property name=\"hazelcast.health.monitoring.delay.seconds\">15</property>\n+        <property name=\"hazelcast.diagnostics.enabled\">true</property>\n+        <property name=\"hazelcast.diagnostics.metrics.period.seconds\">10</property>   \n+        <property name=\"hazelcast.graceful.shutdown.max.wait\">7200</property>   \n+        <property name=\"hazelcast.partition.count\">20000</property>\n+        <property name=\"hazelcast.partition.max.parallel.migrations\">20</property>\n+    </properties>\n+\n+    ...\n+\n+    <native-memory allocator-type=\"POOLED\" enabled=\"true\">\n+        <size unit=\"GIGABYTES\" value=\"500\"/>\n+        <!-- disabled schema validation via hazelcast.internal.override.version system property. -->\n+        <metadata-space-percentage>1</metadata-space-percentage>\n+    </native-memory>\n+\n+    <map name=\"default\">\n+        <in-memory-format>NATIVE</in-memory-format>\n+    </map>\n+</hazelcast>\n+```\n+\n+#### Memory usage:\n+\n+**Native memory usage with 10 members:**\n+```\n+native.memory.used=414.3G, \n+native.memory.free=80.7G, \n+native.memory.total=414.4G, \n+native.memory.max=495.0G, \n+native.meta.memory.used=1011.8M, \n+native.meta.memory.free=4.0G\n+```\n+\n+**Native memory usage with 9 members:**\n+```\n+native.memory.used=460.4G, \n+native.memory.free=34.6G, \n+native.memory.total=460.7G, \n+native.memory.max=495.0G, \n+native.meta.memory.used=1.1G, \n+native.meta.memory.free=3.9G\n+```\n+\n+This was total of `~4.1 TB` data (including the backup) in the whole cluster.\n+\n+### Test Flow\n+\n+```shell script\n+# Initially started 10 members\n+hz cluster -size M10 -v 4.1-SNAPSHOT -ee -boxes a.box -user ec2-user\n+\n+# Load initial data into map\n+hz run ops/load     \n+# Validate map size\n+hz run ops/mapSize\n+               \n+# Start get/set ops at the background\n+hz run ops/get &\n+hz run ops/set &\n+       \n+# Scale-down \n+# Shutdown Member10\n+hz driver Member10\n+hz run ops/shutDown            \n+# Verify cluster-size=9 and wait until migrations complete\n+hz run ops/untilClusterSize9 ops/untilClusterSafe        \n+\n+# Scale-up\n+# Start Member10 back\n+hz boot Member10\n+# Verify cluster-size=10 and wait until migrations complete\n+hz run ops/untilClusterSize10 ops/untilClusterSafe\n+\n+# Member failure\n+# Kill Member10\n+hz kill Member10\n+# Verify cluster-size=9 and wait until migrations complete\n+hz run ops/untilClusterSize9 ops/untilClusterSafe9\n+```\n+\n+### Single Map Benchmark\n+\n+Used a single HD `IMap` and inserted `135 million` entries with `~16k` values. \n+\n+#### Master Branch\n+\n+- *Scale down from 10 to 9 members*: `14000` migrations were completed in **35 minutes and 34 seconds**. \n+Member shutdown in **2145 seconds**.\n+\n+- *Scale up from 9 to 10 members*: `14000` migrations were completed in **34 minutes and 16 seconds**.\n+\n+- *Crash single member*: Around `~26150` migrations were completed in **33 minutes and 38 seconds**.\n+\n+| GET p99 Latency  | GET p99.9 Latency |\n+|:-------------:|:-------------:|\n+| ![Plot](resources/07/single/master/get/p99.png) | ![Plot](resources/07/single/master/get/p999.png) |\n+| GET p99 Min-Max  | GET p99.9 Min-Max |\n+| ![Plot](resources/07/single/master/get/p99-min-mean-max.png) | ![Plot](resources/07/single/master/get/p999-min-mean-max.png) |\n+\n+\n+| SET p99 Latency  | SET p99.9 Latency |\n+|:-------------:|:-------------:|\n+| ![Plot](resources/07/single/master/set/p99.png) | ![Plot](resources/07/single/master/set/p999.png) |\n+| SET p99 Min-Max  | SET p99.9 Min-Max |\n+| ![Plot](resources/07/single/master/set/p99-min-mean-max.png) | ![Plot](resources/07/single/master/set/p999-min-mean-max.png) |\n+\n+\n+**Heap Usage and GC Pauses**\n+\n+![Plot](resources/07/single/master/heap.png)\n+\n+| GC Summary  | |\n+|:-------------|-------------:|\n+| Full collections |\t36 |\n+| Mean garbage collection pause (ms) |\t89.4 |\n+| Mean interval between collections (ms) |\t180394 |\n+| Proportion of time spent in garbage collection pauses (%) |\t3.39 |\n+| Proportion of time spent unpaused (%) |\t96.61 |\n+\n+\n+---\n+\n+#### Parallel Migrations - 5x\n+\n+5x parallelization was used: `hazelcast.partition.max.parallel.migrations=5`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyMjkxNA=="}, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 372}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjA3OTQzOnYy", "diffSide": "RIGHT", "path": "docs/design/partitioning/07-parallel-migrations.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDoyNDozOFrOHDBCsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDoyNDozOFrOHDBCsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyNDg1MA==", "bodyText": "i can't calculate the improved percentage compared to current master. Congrats for amazing results. From half an hour to 3 minutes. \ud83d\udc4f", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472924850", "createdAt": "2020-08-19T10:24:38Z", "author": {"login": "ahmetmircik"}, "path": "docs/design/partitioning/07-parallel-migrations.md", "diffHunk": "@@ -0,0 +1,536 @@\n+# Parallel Independent Migrations\n+\n+|\u2139\ufe0f Since: 4.1| \n+|-------------|\n+\n+## Background\n+\n+The partition table is an eventually consistent and monotonic data structure. It\u2019s only updated by the master member \n+and published to the cluster. Some members may lag behind the master\u2019s version at any time but for those members to be \n+able to participate in a migration, they have to get the latest partition table from master. To provide the monotonic \n+order, the partition table has a global version. And because of this global order, we have to migrate partitions \n+one-by-one. Once the master member changes, it fetches the partition tables and the list of completed & active migrations \n+from the cluster and determines the most recent version of the partition table and then publishes it back. \n+\n+Right now this is a very strict guarantee, because there were data loss issues during shutdown and migrations \n+a few years back and we wanted all partition updates and migrations to be more consistent.\n+\n+The good thing is, in practice, we don\u2019t need this global partition table update order. Because, partitions are not \n+dependent on each other. Instead of a global partition table version, we can have a version per partition. And we can \n+update partitions independently. So, after each partition migration, we can just publish that partition info only, \n+not the whole partition table. Also we can migrate different partitions concurrently. For example, assume there are 10 \n+members, we can execute 5 migrations between 5 member pairs in parallel. With these two optimizations \n+(removal of global update order and parallel migrations), repartitioning can become significantly faster than current.\n+\n+### Previous Work\n+\n+Previous works related to partitioning system and migration mechanism: \n+\n+| Version | Design |\n+| :-----: | :----- |\n+| 3.7 | [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md) |\n+| 3.7 | [Graceful Shutdown Improvements](02-graceful-shutdown-improvements.md) |\n+| 3.9 | [Fine-Grained Anti Entropy Mechanism](03-fine-grained-anti-entropy-mechanism.md) |\n+| 3.9 | [Gigantic Cache Migration Enhancements](04-gigantic-cache-migration-enhancements.md) |\n+| 3.12 | [Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) |\n+| 4.0 | [Refactor MigrationListener API](06-refactor-migrationlistener-api.md) |\n+\n+_Being familiar with these will help to understand the new changes._\n+ \n+## Design\n+\n+### 1 - Global Version to Partition Level\n+\n+First of the major changes is the removal of the global partition table update ordering. Instead we will version each \n+partition individually. \n+\n+#### 1.1 - Current Versioning Mechanism\n+\n+Currently, master member increments global version on each partition update. When the partition table update is received \n+from the master, members decide whether to apply or to ignore the update comparing their local version to received \n+version. If local version is less than received version then partition table is applied, otherwise it's ignored. \n+\n+Similarly, `MigrationInfo` contains the partition table versions before and after \n+the migration. When a migration operation is received, members execute the operation only and only if local version is \n+equal to the version before migration. Otherwise, migration is rejected. A migration operation also contains the most \n+recently completed `MigrationInfo` list. This was  introduced in \n+[Speed-Up Partition Migrations](05-speed-up-partition-migrations.md) \n+work. Before executing the migration operation, members apply these completed migrations in case they haven't seen them yet.\n+\n+While committing promotions, promoted member rejects the commit if its local version is greater than the version in operation.\n+This is similar to applying partition table updates.  \n+\n+#### 1.2 - New Versioning Mechanism\n+\n+With the proposed change, partition table version will be removed and master will update specific partition's version \n+on partition updates. On partition table broadcast, receiving members will do version comparisons for individual \n+partitions. So, they can partially apply, partially ignore the received partition table.\n+\n+During migrations, migration operations will still carry the most recent completed migrations, but only for the partition\n+that migration is belongs to. After applying completed migrations of that partition, migration operation will check \n+the local version against the version in `MigrationInfo`. If they are equal operation will be executed, otherwise it will\n+be rejected.\n+\n+For some cases, we need to check if the whole partition table has changed or not, with a simple comparison. Comparing each \n+partition version one-by-one is not simple and requires keeping all partition versions locally. For those cases, instead \n+of global partition table version, _hash/stamp_ of the partition table will be used. This stamp will be cached locally\n+and updated on each partition modification and also will be easily calculated with a pure hash function. We will use\n+a `64-bit MurmurHash3` to calculate the stamp.\n+\n+```java\n+public static long calculateStamp(InternalPartition[] partitions) {\n+    byte[] bb = new byte[Integer.BYTES * partitions.length];\n+    for (InternalPartition partition : partitions) {\n+        Bits.writeIntB(bb, partition.getPartitionId() * Integer.BYTES, partition.version());\n+    }\n+    return HashUtil.MurmurHash3_x64_64(bb, 0, bb.length);\n+}\n+```  \n+\n+During promotion commit, partition table stamp will be used the check whether local partition table is the same with \n+the one received with promotion. If yes, promotion commit will be ignored. Otherwise, we'll compare versions of individual\n+partitions and skip promotions of partitions which have greater version locally.    \n+   \n+_Other usages of partition table stamp will be explained in later sections._   \n+  \n+\n+### 2 - Parallel Migrations\n+\n+#### 2.1 - Current Migration Mechanism\n+\n+When a new member joins or an existing member leaves the cluster, eventually a `RepartitioningTask` is scheduled on \n+migration thread. `RepartitioningTask` prepares the new partition table and schedules the individual `MigrationTask`s\n+to reach the target partition table. These `MigrationTask`s are executed serially by migration thread and they submit \n+migration operations to the migration participants. After each migration completion, migration is committed to the \n+destination, and partition table is updated. All of these steps are executed serially.\n+\n+ \n+#### 2.2 - Parallel Migrations Mechanism\n+\n+With parallel migrations, most the mechanism remains the same. `RepartitioningTask` prepares the migration plan using \n+the target partition table. Instead of individual `MigrationTask`s, a single `MigrationPlanTask` will be scheduled. \n+`MigrationPlanTask` will submit parallel migration tasks and keep track of started/completed tasks. \n+\n+Migrations belonging to the same partition must be executed in order serially. This is required to provide consistency \n+and safety guarantees while migrating/replicating partition data. For instance, a migration can depend on an earlier one\n+in the plan. Or replication to a missing backup replica is more important than a migration that is just to rebalance the load.\n+_For more info please see [Avoid Data Loss on Migration](01-avoid-data-loss-on-migration.md)._   \n+\n+To achieve this, `RepartitioningTask` creates migration plan with a consistent order. And `MigrationPlanTask` will \n+parallelize only migrations belonging to different partitions. Migrations belonging to the same partition will still be \n+executed serially. That's the upper limit of migration parallelization. But still we may want to limit parallel \n+migrations to keep heap memory usage under control and to avoid overload of network resources.\n+\n+A new `ClusterProperty`, `hazelcast.partition.max.parallel.migrations`, will be introduced, with a default value of `10`. \n+This property defines the maximum number of partition migrations to be executed on a member. That member can be either source \n+or destination of the migration. _Value of `10` is selected as a heuristic after benchmarking on EC2. You can find the \n+benchmark results below._ There may be consequences of using too much or too less parallelization:\n+\n+* Having too much parallelization can increase the heap memory usage and overload the network during partition rebalance.\n+* Having less parallelization can increase the total migration completion time.\n+       \n+\n+### 3 - Other Changes\n+\n+#### 3.1 - Cluster State Management Changes\n+\n+Cluster state & version changes are performed via transactional process and one of the prerequisites was partition table\n+version of the transaction originator must be equal to the version of the each transaction participant. Since partition\n+table version is now removed in favor of per-partition versions, we will use _partition table stamp_ instead.     \n+\n+#### 3.2 - Hot Restart Changes\n+\n+There will be a few small changes. First one is partition table persistence format. Format will be decided due to cluster \n+version while restoring/persisting partition table. When cluster version is upgraded, partition table will be persisted \n+with the new format once. \n+\n+Current format is:\n+\n+```\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n+| Partition Table Version     |\n++-----------------------------+\n+| Partition Table Length      |\n++-----------------------------+\n+| Replicas (Address + UUID)   |\n++-----------------------------+\n+| Partition-0 Replica Indexes |\n++-----------------------------+\n+| Partition-1 Replica Indexes |\n++-----------------------------+\n+| ........................... |\n++-----------------------------+\n+| Partition-N Replica Indexes |\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+     \n+```                                 \n+\n+New format will be:\n+\n+```\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n+| Partition Table Length      |\n++-----------------------------+\n+| Replicas (Address + UUID)   |\n++-----------------------------+\n+| Partition-0 Version         |\n++-----------------------------+\n+| Partition-0 Replica Indexes |\n++-----------------------------+\n+| Partition-1 Version         |\n++-----------------------------+\n+| Partition-1 Replica Indexes |\n++-----------------------------+\n+| ........................... |\n++-----------------------------+\n+| Partition-N Version         |\n++-----------------------------+\n+| Partition-N Replica Indexes |\n++-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+     \n+```          \n+\n+Second change is in the partition table restore process. While restoring partition table, if a replica address change is\n+detected, we replace the old address of the replica with the new one in the partition table. In current mechanism, \n+global partition version is incremented by the number of updates, as if partition table is updated in the runtime.\n+With the new versioning, only updated partitions' versions will be incremented.\n+\n+Third change is in the partial restart decision process. During partial restart, we decide which members to perform the \n+restart, which members to be excluded and delete their data. To select member groups having the same partition table,\n+we use the partition table version. Members with the same partition table are grouped together. Also to find the most \n+recent partition table, we pick the partition table with the greatest version. If the policy is `PARTIAL_RECOVERY_MOST_RECENT`,\n+we pick the group the greatest partition table version. If it's `PARTIAL_RECOVERY_MOST_COMPLETE`, then we pick \n+the largest group. In other places we will use _partition table stamp_ instead of removed partition table version. But in\n+this case, we cannot use _partition table stamp_. Because stamp is only valid for equality comparison, it cannot be used\n+to compare order of the partition tables. Instead we will use the sum of per-partition versions to compare the order. \n+_This works because Hot Restart does not support arbitrary server crashes. Only proper server shutdowns are supported._\n+Otherwise, resolving conflicts on partition level, determining the most recent versions of each partition, composing\n+a new partition table with these partitions and adapting Hot Restart to restore data based on this new partition table\n+requires a large rework and time. And supporting rolling upgrade scenarios makes this more complicated.            \n+\n+\n+#### 3.3 - Rolling Upgrade\n+\n+- To support rolling upgrade, we will have two distinct paths for most the migration scheduling and execution logic. \n+\n+- While updating partition table and applying completed migrations, there'll be separate paths too.\n+\n+- Once the cluster is upgraded to `4.1`, all partition versions will be set to the latest partition table version.\n+\n+- Cluster state change transaction mechanism will use either partition table version or stamp according to the version.\n+\n+- Similarly Hot Restart partition restoration and persistence mechanism will use different logic according to \n+the version. There will be `LegacyPartitionTableReader/Writer` classes to support old format.  \n+\n+\n+## Benchmarks\n+\n+We compared `parallel-migrations` branch with `master` branch and measured stabilization times of shutting down single\n+member, adding a new member and crash of a single member scenarios. \n+\n+### Setup\n+\n+- Tested on EC2 with `hzCmd` tool.\n+- Used `r5.16xlarge` (64 vCPU, 512 GiB memory, 20 Gbps network bandwidth) instance type.\n+- Member JVM heap memory settings was `-Xms6G -Xmx6G`.\n+- Partition count was `20k`.\n+- Configured native memory size was 500GB. \n+- Inserted total of `135 million` entries to either a single map or `10` different maps (`13.5 million` per-map) with single backup.\n+- Value size was `16K` bytes.\n+- There were continuous `IMap.get/set` operations by 20 threads per-member.\n+\n+#### Server configuration:\n+```xml\n+<hazelcast>\n+    <properties>\n+        <property name=\"hazelcast.phone.home.enabled\">false</property>\n+        <property name=\"hazelcast.hidensity.check.freememory\">false</property>\n+        <property name=\"hazelcast.health.monitoring.level\">NOISY</property>\n+        <property name=\"hazelcast.health.monitoring.delay.seconds\">15</property>\n+        <property name=\"hazelcast.diagnostics.enabled\">true</property>\n+        <property name=\"hazelcast.diagnostics.metrics.period.seconds\">10</property>   \n+        <property name=\"hazelcast.graceful.shutdown.max.wait\">7200</property>   \n+        <property name=\"hazelcast.partition.count\">20000</property>\n+        <property name=\"hazelcast.partition.max.parallel.migrations\">20</property>\n+    </properties>\n+\n+    ...\n+\n+    <native-memory allocator-type=\"POOLED\" enabled=\"true\">\n+        <size unit=\"GIGABYTES\" value=\"500\"/>\n+        <!-- disabled schema validation via hazelcast.internal.override.version system property. -->\n+        <metadata-space-percentage>1</metadata-space-percentage>\n+    </native-memory>\n+\n+    <map name=\"default\">\n+        <in-memory-format>NATIVE</in-memory-format>\n+    </map>\n+</hazelcast>\n+```\n+\n+#### Memory usage:\n+\n+**Native memory usage with 10 members:**\n+```\n+native.memory.used=414.3G, \n+native.memory.free=80.7G, \n+native.memory.total=414.4G, \n+native.memory.max=495.0G, \n+native.meta.memory.used=1011.8M, \n+native.meta.memory.free=4.0G\n+```\n+\n+**Native memory usage with 9 members:**\n+```\n+native.memory.used=460.4G, \n+native.memory.free=34.6G, \n+native.memory.total=460.7G, \n+native.memory.max=495.0G, \n+native.meta.memory.used=1.1G, \n+native.meta.memory.free=3.9G\n+```\n+\n+This was total of `~4.1 TB` data (including the backup) in the whole cluster.\n+\n+### Test Flow\n+\n+```shell script\n+# Initially started 10 members\n+hz cluster -size M10 -v 4.1-SNAPSHOT -ee -boxes a.box -user ec2-user\n+\n+# Load initial data into map\n+hz run ops/load     \n+# Validate map size\n+hz run ops/mapSize\n+               \n+# Start get/set ops at the background\n+hz run ops/get &\n+hz run ops/set &\n+       \n+# Scale-down \n+# Shutdown Member10\n+hz driver Member10\n+hz run ops/shutDown            \n+# Verify cluster-size=9 and wait until migrations complete\n+hz run ops/untilClusterSize9 ops/untilClusterSafe        \n+\n+# Scale-up\n+# Start Member10 back\n+hz boot Member10\n+# Verify cluster-size=10 and wait until migrations complete\n+hz run ops/untilClusterSize10 ops/untilClusterSafe\n+\n+# Member failure\n+# Kill Member10\n+hz kill Member10\n+# Verify cluster-size=9 and wait until migrations complete\n+hz run ops/untilClusterSize9 ops/untilClusterSafe9\n+```\n+\n+### Single Map Benchmark\n+\n+Used a single HD `IMap` and inserted `135 million` entries with `~16k` values. \n+\n+#### Master Branch\n+\n+- *Scale down from 10 to 9 members*: `14000` migrations were completed in **35 minutes and 34 seconds**. \n+Member shutdown in **2145 seconds**.\n+\n+- *Scale up from 9 to 10 members*: `14000` migrations were completed in **34 minutes and 16 seconds**.\n+\n+- *Crash single member*: Around `~26150` migrations were completed in **33 minutes and 38 seconds**.\n+\n+| GET p99 Latency  | GET p99.9 Latency |\n+|:-------------:|:-------------:|\n+| ![Plot](resources/07/single/master/get/p99.png) | ![Plot](resources/07/single/master/get/p999.png) |\n+| GET p99 Min-Max  | GET p99.9 Min-Max |\n+| ![Plot](resources/07/single/master/get/p99-min-mean-max.png) | ![Plot](resources/07/single/master/get/p999-min-mean-max.png) |\n+\n+\n+| SET p99 Latency  | SET p99.9 Latency |\n+|:-------------:|:-------------:|\n+| ![Plot](resources/07/single/master/set/p99.png) | ![Plot](resources/07/single/master/set/p999.png) |\n+| SET p99 Min-Max  | SET p99.9 Min-Max |\n+| ![Plot](resources/07/single/master/set/p99-min-mean-max.png) | ![Plot](resources/07/single/master/set/p999-min-mean-max.png) |\n+\n+\n+**Heap Usage and GC Pauses**\n+\n+![Plot](resources/07/single/master/heap.png)\n+\n+| GC Summary  | |\n+|:-------------|-------------:|\n+| Full collections |\t36 |\n+| Mean garbage collection pause (ms) |\t89.4 |\n+| Mean interval between collections (ms) |\t180394 |\n+| Proportion of time spent in garbage collection pauses (%) |\t3.39 |\n+| Proportion of time spent unpaused (%) |\t96.61 |\n+\n+\n+---\n+\n+#### Parallel Migrations - 5x\n+\n+5x parallelization was used: `hazelcast.partition.max.parallel.migrations=5`.\n+\n+- *Scale down from 10 to 9 members*: `14000` migrations were completed in **4 minutes and 45 seconds**. \n+Member shutdown in **299 seconds**. \n+\n+- *Scale up from 9 to 10 members*: `14000` migrations were completed in **7 minutes and 19 seconds**.\n+\n+- *Crash single member*: Around `~26150` migrations were completed in **2 minutes and 17 seconds**.\n+\n+| GET p99 Latency  | GET p99.9 Latency |\n+|:-------------:|:-------------:|\n+| ![Plot](resources/07/single/parallel-5x/get/p99.png) | ![Plot](resources/07/single/parallel-5x/get/p999.png) |\n+| GET p99 Min-Max  | GET p99.9 Min-Max |\n+| ![Plot](resources/07/single/parallel-5x/get/p99-min-mean-max.png) | ![Plot](resources/07/single/parallel-5x/get/p999-min-mean-max.png) |\n+\n+\n+| SET p99 Latency  | SET p99.9 Latency |\n+|:-------------:|:-------------:|\n+| ![Plot](resources/07/single/parallel-5x/set/p99.png) | ![Plot](resources/07/single/parallel-5x/set/p999.png) |\n+| SET p99 Min-Max  | SET p99.9 Min-Max |\n+| ![Plot](resources/07/single/parallel-5x/set/p99-min-mean-max.png) | ![Plot](resources/07/single/parallel-5x/set/p999-min-mean-max.png) |\n+\n+\n+\n+**Heap Usage and GC Pauses**\n+\n+![Plot](resources/07/single/parallel-5x/heap.png)\n+\n+| GC Summary  | |\n+|:-------------|-------------:|\n+| Full collections\t| 26\n+| Mean garbage collection pause (ms)\t| 104\n+| Mean interval between collections (ms)\t| 56512\n+| Proportion of time spent in garbage collection pauses (%)\t| 4.27\n+| Proportion of time spent unpaused (%)\t| 95.73\n+\n+---\n+\n+#### Parallel Migrations - 20x\n+\n+20x parallelization was used: `hazelcast.partition.max.parallel.migrations=20`.\n+\n+- *Scale down from 10 to 9 members*: `14000` migrations were completed in **3 minutes and 22 seconds**. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 414}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjA4NDQ3OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/client/impl/ClusterViewListenerService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDoyNjowOVrOHDBFtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo0OTozNlrOHDB23g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyNTYyMQ==", "bodyText": "it can be better to document when this partitionTableVersion can be deleted from code.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472925621", "createdAt": "2020-08-19T10:26:09Z", "author": {"login": "ahmetmircik"}, "path": "hazelcast/src/main/java/com/hazelcast/client/impl/ClusterViewListenerService.java", "diffHunk": "@@ -57,6 +60,12 @@\n     private final boolean advancedNetworkConfigEnabled;\n     private final AtomicBoolean pushScheduled = new AtomicBoolean();\n     private final CoalescingDelayedTrigger delayedPartitionUpdateTrigger;\n+    // This is an emulation of the pre-4.1 partition state version.\n+    // We will increment this version if a partition table change is detected\n+    // while sending partition table to the client.\n+    private final AtomicInteger partitionTableVersion = new AtomicInteger();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzODIwNg==", "bodyText": "Most probably it will stay until the next major version of the client protocol. I'll add this.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472938206", "createdAt": "2020-08-19T10:49:36Z", "author": {"login": "mdogan"}, "path": "hazelcast/src/main/java/com/hazelcast/client/impl/ClusterViewListenerService.java", "diffHunk": "@@ -57,6 +60,12 @@\n     private final boolean advancedNetworkConfigEnabled;\n     private final AtomicBoolean pushScheduled = new AtomicBoolean();\n     private final CoalescingDelayedTrigger delayedPartitionUpdateTrigger;\n+    // This is an emulation of the pre-4.1 partition state version.\n+    // We will increment this version if a partition table change is detected\n+    // while sending partition table to the client.\n+    private final AtomicInteger partitionTableVersion = new AtomicInteger();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyNTYyMQ=="}, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjA5MDI4OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/client/impl/spi/impl/ClientPartitionServiceImpl.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDoyNzo0NlrOHDBJJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDoyNzo0NlrOHDBJJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyNjUwMg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            logFailure(connection, partitionStateVersion, current, \"response state version is old\");\n          \n          \n            \n                            logFailure(connection, partitionStateVersion, current, \"response partition state version is old\");", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472926502", "createdAt": "2020-08-19T10:27:46Z", "author": {"login": "ahmetmircik"}, "path": "hazelcast/src/main/java/com/hazelcast/client/impl/spi/impl/ClientPartitionServiceImpl.java", "diffHunk": "@@ -106,10 +106,9 @@ private boolean shouldBeApplied(Connection connection, Collection<Map.Entry<UUID\n             }\n             return true;\n         }\n-        if (partitionStateVersion <= current.partitionSateVersion) {\n+        if (partitionStateVersion == current.partitionSateVersion) {\n             if (logger.isFinestEnabled()) {\n-                logFailure(connection, partitionStateVersion, current,\n-                        \"response state version is old\");\n+                logFailure(connection, partitionStateVersion, current, \"response state version is old\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjA5ODY2OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/internal/cluster/impl/ClusterStateManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDozMDowOVrOHDBOOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo1OTozOVrOHDCK0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyNzgwMw==", "bodyText": "can't we remove partitionStateStamp from code-base and continue to use partitionStateVersion instead? It also helps hot-restart partial restart case.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472927803", "createdAt": "2020-08-19T10:30:09Z", "author": {"login": "ahmetmircik"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/cluster/impl/ClusterStateManager.java", "diffHunk": "@@ -271,17 +272,22 @@ private void validateClusterVersionChange(Version newClusterVersion) {\n         }\n     }\n \n-    private void checkMigrationsAndPartitionStateVersion(ClusterStateChange stateChange, int partitionStateVersion) {\n-        final InternalPartitionService partitionService = node.getPartitionService();\n-        final int thisPartitionStateVersion = partitionService.getPartitionStateVersion();\n+    private void checkMigrationsAndPartitionStateStamp(ClusterStateChange stateChange, long partitionStateStamp) {\n+        InternalPartitionService partitionService = node.getPartitionService();\n+        long thisPartitionStateStamp;\n+        if (clusterVersion.isGreaterOrEqual(Versions.V4_1)) {\n+            thisPartitionStateStamp = partitionService.getPartitionStateStamp();\n+        } else {\n+            thisPartitionStateStamp = partitionService.getPartitionStateVersion();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjk0MzMxMw==", "bodyText": "Problem is partitionStateVersion indicates a monotonic order between different partition states. But with the new mechanism, there's no such an order, members may receive partition updates in different order but still can have the same version. partitionStateStamp tries to solve this problem.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472943313", "createdAt": "2020-08-19T10:59:39Z", "author": {"login": "mdogan"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/cluster/impl/ClusterStateManager.java", "diffHunk": "@@ -271,17 +272,22 @@ private void validateClusterVersionChange(Version newClusterVersion) {\n         }\n     }\n \n-    private void checkMigrationsAndPartitionStateVersion(ClusterStateChange stateChange, int partitionStateVersion) {\n-        final InternalPartitionService partitionService = node.getPartitionService();\n-        final int thisPartitionStateVersion = partitionService.getPartitionStateVersion();\n+    private void checkMigrationsAndPartitionStateStamp(ClusterStateChange stateChange, long partitionStateStamp) {\n+        InternalPartitionService partitionService = node.getPartitionService();\n+        long thisPartitionStateStamp;\n+        if (clusterVersion.isGreaterOrEqual(Versions.V4_1)) {\n+            thisPartitionStateStamp = partitionService.getPartitionStateStamp();\n+        } else {\n+            thisPartitionStateStamp = partitionService.getPartitionStateVersion();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyNzgwMw=="}, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjExMzAxOnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/InternalPartitionService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDozNDowOFrOHDBWtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMTowMDowNFrOHDCL5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyOTk3NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                boolean isPartitionStateInitialized();\n          \n          \n            \n                boolean isPartitionTableInitialized();", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472929975", "createdAt": "2020-08-19T10:34:08Z", "author": {"login": "ahmetmircik"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/InternalPartitionService.java", "diffHunk": "@@ -79,6 +79,11 @@\n      */\n     void memberRemoved(Member member);\n \n+    /**\n+     * Returns whether partition table is initialized or not.\n+     */\n+    boolean isPartitionStateInitialized();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjk0MzU4OA==", "bodyText": "I'll remove this method, as it's not used apart from a test.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472943588", "createdAt": "2020-08-19T11:00:04Z", "author": {"login": "mdogan"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/InternalPartitionService.java", "diffHunk": "@@ -79,6 +79,11 @@\n      */\n     void memberRemoved(Member member);\n \n+    /**\n+     * Returns whether partition table is initialized or not.\n+     */\n+    boolean isPartitionStateInitialized();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkyOTk3NQ=="}, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjExODA5OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/PartitionRuntimeState.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDozNToyMlrOHDBZYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDozNToyMlrOHDBZYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzMDY1OA==", "bodyText": "can we have a short javaDoc for what is encodedPartitionTable?", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472930658", "createdAt": "2020-08-19T10:35:22Z", "author": {"login": "ahmetmircik"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/PartitionRuntimeState.java", "diffHunk": "@@ -34,27 +38,42 @@\n import static com.hazelcast.internal.serialization.impl.SerializationUtil.writeNullableCollection;\n import static com.hazelcast.internal.util.StringUtil.LINE_SEPARATOR;\n \n-public final class PartitionRuntimeState implements IdentifiedDataSerializable {\n+public final class PartitionRuntimeState implements IdentifiedDataSerializable, Versioned {\n \n-    private PartitionReplica[] replicas;\n-    private int[][] minimizedPartitionTable;\n+    private PartitionReplica[] allReplicas;\n+    private int[][] encodedPartitionTable;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjE0NzM1OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/operation/PromotionCommitOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo0MzoyM1rOHDBqTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMTowOTozNlrOHDCeqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzNDk4OA==", "bodyText": "are these commented lines leftover?", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472934988", "createdAt": "2020-08-19T10:43:23Z", "author": {"login": "ahmetmircik"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/operation/PromotionCommitOperation.java", "diffHunk": "@@ -180,6 +195,46 @@ private CallStatus beforePromotion() {\n         return CallStatus.VOID;\n     }\n \n+    private CallStatus alreadyAppliedAllPromotions() {\n+        getLogger().warning(\"Already applied all promotions to the partition state. Promotion state stamp: \"\n+                + partitionState.getStamp());\n+        InternalPartitionServiceImpl partitionService = getService();\n+        partitionService.getMigrationManager().releasePromotionPermit();\n+        success = true;\n+        return CallStatus.RESPONSE;\n+    }\n+\n+    //RU_COMPAT_4_0\n+    private void filterAlreadyAppliedPromotions() {\n+        if (getNodeEngine().getClusterService().getClusterVersion().isUnknownOrLessOrEqual(Versions.V4_0)) {\n+            return;\n+        }\n+        InternalPartitionServiceImpl partitionService = getService();\n+        PartitionStateManager stateManager = partitionService.getPartitionStateManager();\n+        Iterator<MigrationInfo> iter = promotions.iterator();\n+        while (iter.hasNext()) {\n+            MigrationInfo promotion = iter.next();\n+            InternalPartitionImpl partition = stateManager.getPartitionImpl(promotion.getPartitionId());\n+\n+            if (partition.version() >= promotion.getFinalPartitionVersion()) {\n+                // ? we can't assume this, since partition might be further updated...\n+//                if (promotion.getDestination().equals(partition.getOwnerReplicaOrNull())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjk0ODM5Mw==", "bodyText": "yes \ud83d\ude03, I'll remove them.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472948393", "createdAt": "2020-08-19T11:09:36Z", "author": {"login": "mdogan"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/operation/PromotionCommitOperation.java", "diffHunk": "@@ -180,6 +195,46 @@ private CallStatus beforePromotion() {\n         return CallStatus.VOID;\n     }\n \n+    private CallStatus alreadyAppliedAllPromotions() {\n+        getLogger().warning(\"Already applied all promotions to the partition state. Promotion state stamp: \"\n+                + partitionState.getStamp());\n+        InternalPartitionServiceImpl partitionService = getService();\n+        partitionService.getMigrationManager().releasePromotionPermit();\n+        success = true;\n+        return CallStatus.RESPONSE;\n+    }\n+\n+    //RU_COMPAT_4_0\n+    private void filterAlreadyAppliedPromotions() {\n+        if (getNodeEngine().getClusterService().getClusterVersion().isUnknownOrLessOrEqual(Versions.V4_0)) {\n+            return;\n+        }\n+        InternalPartitionServiceImpl partitionService = getService();\n+        PartitionStateManager stateManager = partitionService.getPartitionStateManager();\n+        Iterator<MigrationInfo> iter = promotions.iterator();\n+        while (iter.hasNext()) {\n+            MigrationInfo promotion = iter.next();\n+            InternalPartitionImpl partition = stateManager.getPartitionImpl(promotion.getPartitionId());\n+\n+            if (partition.version() >= promotion.getFinalPartitionVersion()) {\n+                // ? we can't assume this, since partition might be further updated...\n+//                if (promotion.getDestination().equals(partition.getOwnerReplicaOrNull())) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzNDk4OA=="}, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjE0ODcyOnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/spi/impl/operationservice/impl/Invocation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo0Mzo0N1rOHDBrGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMToxNDozNVrOHDCofA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzNTE5Mw==", "bodyText": "TODO?", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472935193", "createdAt": "2020-08-19T10:43:47Z", "author": {"login": "ahmetmircik"}, "path": "hazelcast/src/main/java/com/hazelcast/spi/impl/operationservice/impl/Invocation.java", "diffHunk": "@@ -283,6 +283,7 @@ final void initInvocationTarget() throws Exception {\n             if (previousTargetMember != null) {\n                 // If a target member was found earlier but current target member is null\n                 // then it means a member left.\n+                // TODO: not always true ??", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjk1MDkwOA==", "bodyText": "hmm this is a left-over comment from a CP testing, not relevant for this PR. but still the comment is true...", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472950908", "createdAt": "2020-08-19T11:14:35Z", "author": {"login": "mdogan"}, "path": "hazelcast/src/main/java/com/hazelcast/spi/impl/operationservice/impl/Invocation.java", "diffHunk": "@@ -283,6 +283,7 @@ final void initInvocationTarget() throws Exception {\n             if (previousTargetMember != null) {\n                 // If a target member was found earlier but current target member is null\n                 // then it means a member left.\n+                // TODO: not always true ??", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzNTE5Mw=="}, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1NjE1ODIxOnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/PartitionStateManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo0NjozM1rOHDBwoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxMDo0NjozM1rOHDBwoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjkzNjYwOA==", "bodyText": "leftover?", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r472936608", "createdAt": "2020-08-19T10:46:33Z", "author": {"login": "ahmetmircik"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/PartitionStateManager.java", "diffHunk": "@@ -214,16 +225,20 @@ void setInitialState(PartitionTableView partitionTable) {\n         PartitionReplica localReplica = PartitionReplica.from(node.getLocalMember());\n         for (int partitionId = 0; partitionId < partitionCount; partitionId++) {\n             InternalPartitionImpl partition = partitions[partitionId];\n-            PartitionReplica[] replicas = partitionTable.getReplicas(partitionId);\n-            if (!foundReplica && replicas != null) {\n+            InternalPartition newPartition = partitionTable.getPartition(partitionId);\n+            if (!foundReplica && newPartition != null) {\n                 for (int i = 0; i < InternalPartition.MAX_REPLICA_COUNT; i++) {\n-                    foundReplica |= replicas[i] != null;\n+                    foundReplica |= newPartition.getReplica(i) != null;\n                 }\n             }\n             partition.reset(localReplica);\n-            partition.setInitialReplicas(replicas);\n+//            partition.setInitialReplicas(replicas);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd95e5aced863bda0595e3320769ddb7691d726c"}, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3NzY4MTk1OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/client/impl/spi/impl/ClientPartitionServiceImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwOTowMTowOFrOHGOmNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwODoxNjoyNVrOHIFbMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjI5MjY2Mw==", "bodyText": "is the equals comparison valid for both RU-compatibility and 4.1-version-stamp modes?", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r476292663", "createdAt": "2020-08-25T09:01:08Z", "author": {"login": "vbekiaris"}, "path": "hazelcast/src/main/java/com/hazelcast/client/impl/spi/impl/ClientPartitionServiceImpl.java", "diffHunk": "@@ -106,10 +106,9 @@ private boolean shouldBeApplied(Connection connection, Collection<Map.Entry<UUID\n             }\n             return true;\n         }\n-        if (partitionStateVersion <= current.partitionSateVersion) {\n+        if (partitionStateVersion == current.partitionSateVersion) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODIzOTUzOQ==", "bodyText": "hmm.. good catch! I think I should revert this change back. I was planning to send partition stamp to the clients but couldn't find a way because of client compatibility requirement.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478239539", "createdAt": "2020-08-27T08:16:25Z", "author": {"login": "mdogan"}, "path": "hazelcast/src/main/java/com/hazelcast/client/impl/spi/impl/ClientPartitionServiceImpl.java", "diffHunk": "@@ -106,10 +106,9 @@ private boolean shouldBeApplied(Connection connection, Collection<Map.Entry<UUID\n             }\n             return true;\n         }\n-        if (partitionStateVersion <= current.partitionSateVersion) {\n+        if (partitionStateVersion == current.partitionSateVersion) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjI5MjY2Mw=="}, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3NzkxMDAwOnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/IPartition.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwOTo1NjozM1rOHGQwjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwODoxOToyNlrOHIFihg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjMyODA3OA==", "bodyText": "can you clarify what \"update on the partition\" means in this context? (maybe I 'm just silly, but it feels like this statement could be misinterpreted as \"data-in-the-partition version\", similar to replica versions we use for anti-entropy)", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r476328078", "createdAt": "2020-08-25T09:56:33Z", "author": {"login": "vbekiaris"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/IPartition.java", "diffHunk": "@@ -98,4 +98,13 @@\n      * @return {@code true} if address is owner or backup, {@code false} otherwise\n      */\n     boolean isOwnerOrBackup(Address address);\n+\n+    /**\n+     * Returns the version of the partition.\n+     * Partition version is incremented by one on each update\n+     * on the partition.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI0MTQxNA==", "bodyText": "\ud83d\udc4d I'll elaborate more.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478241414", "createdAt": "2020-08-27T08:19:26Z", "author": {"login": "mdogan"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/IPartition.java", "diffHunk": "@@ -98,4 +98,13 @@\n      * @return {@code true} if address is owner or backup, {@code false} otherwise\n      */\n     boolean isOwnerOrBackup(Address address);\n+\n+    /**\n+     * Returns the version of the partition.\n+     * Partition version is incremented by one on each update\n+     * on the partition.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjMyODA3OA=="}, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3ODE5MDgyOnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/PartitionStateVersionMismatchException.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxMToyMzoxMFrOHGTddA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxMToyMzoxMFrOHGTddA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjM3MjM0MA==", "bodyText": "add an // RU_COMPAT comment here so the class can be located & renamed in next major version?", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r476372340", "createdAt": "2020-08-25T11:23:10Z", "author": {"login": "vbekiaris"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/PartitionStateVersionMismatchException.java", "diffHunk": "@@ -19,17 +19,19 @@\n import com.hazelcast.core.HazelcastException;\n \n /**\n- * Thrown when local partition-state version doesn't match the version\n+ * Thrown when local partition stamp doesn't match the stamp\n  * of master member while running a migration/replication operation.\n+ * <p>\n+ * PartitionStateVersionMismatchException name is kept to provide rolling upgrade\n+ * guarantees. Otherwise this class would be renamed to PartitionStateStampMismatchException.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3ODMyOTM2OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/ReadonlyInternalPartition.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxMjowMDo0OVrOHGUw-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxMjowMDo0OVrOHGUw-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjM5MzcyMQ==", "bodyText": "Aways returning false from ReadonlyInternalPartition#isLocal() method seems unexpected. If the information is not available, is it probably best to throw an UnsupportedOperationException?", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r476393721", "createdAt": "2020-08-25T12:00:49Z", "author": {"login": "vbekiaris"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/ReadonlyInternalPartition.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.internal.partition;\n+\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+\n+/**\n+ * Readonly/immutable implementation of {@link InternalPartition} interface.\n+ */\n+public class ReadonlyInternalPartition extends AbstractInternalPartition {\n+\n+    private final PartitionReplica[] replicas;\n+    private final int version;\n+\n+    @SuppressFBWarnings(\"EI_EXPOSE_REP2\")\n+    public ReadonlyInternalPartition(PartitionReplica[] replicas, int partitionId, int version) {\n+        super(partitionId);\n+        this.replicas = replicas;\n+        this.version = version;\n+    }\n+\n+    public ReadonlyInternalPartition(InternalPartition partition) {\n+        super(partition.getPartitionId());\n+        this.replicas = partition.getReplicasCopy();\n+        this.version = partition.version();\n+    }\n+\n+    @Override\n+    public boolean isLocal() {\n+        return false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4Mjc2ODg4OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationStats.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwNzo1OTo0MFrOHHAapQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwOToxODo0MFrOHIHs2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzEwODkwMQ==", "bodyText": "Could we keep the elapsedMigrationTime & totalElapsedMigrationTime rendered in all cases so it is visible in \"All migration tasks have been completed\" log entry? IMHO it's useful info", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477108901", "createdAt": "2020-08-26T07:59:40Z", "author": {"login": "vbekiaris"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationStats.java", "diffHunk": "@@ -208,11 +208,10 @@ public String formatToString(boolean detailed) {\n             s.append(\", elapsedMigrationOperationTime=\").append(getElapsedMigrationOperationTime()).append(\"ms\")\n                     .append(\", totalElapsedMigrationOperationTime=\").append(getTotalElapsedMigrationOperationTime()).append(\"ms\")\n                     .append(\", elapsedDestinationCommitTime=\").append(getElapsedDestinationCommitTime()).append(\"ms\")\n-                    .append(\", totalElapsedDestinationCommitTime=\").append(getTotalElapsedDestinationCommitTime()).append(\"ms\");\n+                    .append(\", totalElapsedDestinationCommitTime=\").append(getTotalElapsedDestinationCommitTime()).append(\"ms\")\n+                    .append(\", elapsedMigrationTime=\").append(getElapsedMigrationTime()).append(\"ms\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI2OTYzMQ==", "bodyText": "These numbers show the total elapsed time of the individual migrations. But when migrations are concurrent, they might be misleading. Because they don't show the absolute elapsed time between start and complete. Still I can revert this change, if you think it's useful...", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478269631", "createdAt": "2020-08-27T09:06:26Z", "author": {"login": "mdogan"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationStats.java", "diffHunk": "@@ -208,11 +208,10 @@ public String formatToString(boolean detailed) {\n             s.append(\", elapsedMigrationOperationTime=\").append(getElapsedMigrationOperationTime()).append(\"ms\")\n                     .append(\", totalElapsedMigrationOperationTime=\").append(getTotalElapsedMigrationOperationTime()).append(\"ms\")\n                     .append(\", elapsedDestinationCommitTime=\").append(getElapsedDestinationCommitTime()).append(\"ms\")\n-                    .append(\", totalElapsedDestinationCommitTime=\").append(getTotalElapsedDestinationCommitTime()).append(\"ms\");\n+                    .append(\", totalElapsedDestinationCommitTime=\").append(getTotalElapsedDestinationCommitTime()).append(\"ms\")\n+                    .append(\", elapsedMigrationTime=\").append(getElapsedMigrationTime()).append(\"ms\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzEwODkwMQ=="}, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI3NjgyNA==", "bodyText": "oh right, so with concurrent migrations it will be more like \"cpu time\" rather than wall clock time. Let's leave this only in detailed == true case since it can be confusing.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478276824", "createdAt": "2020-08-27T09:18:40Z", "author": {"login": "vbekiaris"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationStats.java", "diffHunk": "@@ -208,11 +208,10 @@ public String formatToString(boolean detailed) {\n             s.append(\", elapsedMigrationOperationTime=\").append(getElapsedMigrationOperationTime()).append(\"ms\")\n                     .append(\", totalElapsedMigrationOperationTime=\").append(getTotalElapsedMigrationOperationTime()).append(\"ms\")\n                     .append(\", elapsedDestinationCommitTime=\").append(getElapsedDestinationCommitTime()).append(\"ms\")\n-                    .append(\", totalElapsedDestinationCommitTime=\").append(getTotalElapsedDestinationCommitTime()).append(\"ms\");\n+                    .append(\", totalElapsedDestinationCommitTime=\").append(getTotalElapsedDestinationCommitTime()).append(\"ms\")\n+                    .append(\", elapsedMigrationTime=\").append(getElapsedMigrationTime()).append(\"ms\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzEwODkwMQ=="}, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MzE4Mzc3OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/PartitionReplicaStateChecker.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwOTo0MzoyM1rOHHEaFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwOTo0MzoyM1rOHHEaFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE3NDI5Mw==", "bodyText": "can we document the special 0 value in getStamp() method javadoc (and maybe define it as a constant?) ?", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477174293", "createdAt": "2020-08-26T09:43:23Z", "author": {"login": "vbekiaris"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/PartitionReplicaStateChecker.java", "diffHunk": "@@ -82,6 +82,10 @@ public PartitionServiceState getPartitionServiceState() {\n             return FETCHING_PARTITION_TABLE;\n         }\n \n+        if (partitionStateManager.getStamp() != 0 && !partitionStateManager.isInitialized()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MzU5NTAwOnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/operation/MigrationRequestOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxMTo0ODoyMVrOHHIXKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwOToxNToyM1rOHIHlNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzIzOTA4MQ==", "bodyText": "is this change in log level intentional? It looks like it can result in logging migration failures during shutdown at WARNING level.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477239081", "createdAt": "2020-08-26T11:48:21Z", "author": {"login": "vbekiaris"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/operation/MigrationRequestOperation.java", "diffHunk": "@@ -277,13 +264,7 @@ private void logThrowable(Throwable t) {\n         if (throwableToLog instanceof ExecutionException) {\n             throwableToLog = throwableToLog.getCause() != null ? throwableToLog.getCause() : throwableToLog;\n         }\n-        Level level = getLogLevel(throwableToLog);\n-        getLogger().log(level, throwableToLog.getMessage(), throwableToLog);\n-    }\n-\n-    private Level getLogLevel(Throwable e) {\n-        return (e instanceof MemberLeftException || e instanceof InterruptedException)\n-                || !getNodeEngine().isRunning() ? Level.INFO : Level.WARNING;\n+        getLogger().warning(\"Failure while executing \" + migrationInfo, throwableToLog);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI3NDg3MA==", "bodyText": "It was for analyzing a failure. I'll revert back.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478274870", "createdAt": "2020-08-27T09:15:23Z", "author": {"login": "mdogan"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/operation/MigrationRequestOperation.java", "diffHunk": "@@ -277,13 +264,7 @@ private void logThrowable(Throwable t) {\n         if (throwableToLog instanceof ExecutionException) {\n             throwableToLog = throwableToLog.getCause() != null ? throwableToLog.getCause() : throwableToLog;\n         }\n-        Level level = getLogLevel(throwableToLog);\n-        getLogger().log(level, throwableToLog.getMessage(), throwableToLog);\n-    }\n-\n-    private Level getLogLevel(Throwable e) {\n-        return (e instanceof MemberLeftException || e instanceof InterruptedException)\n-                || !getNodeEngine().isRunning() ? Level.INFO : Level.WARNING;\n+        getLogger().warning(\"Failure while executing \" + migrationInfo, throwableToLog);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzIzOTA4MQ=="}, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MzY5ODA0OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/spi/properties/ClusterProperty.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxMjoxODowOVrOHHJVpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxMjoxODowOVrOHHJVpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI1NTA3OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Maximum number of partition migrations to be executed on a member.\n          \n          \n            \n                 * Maximum number of partition migrations to be executed concurrently on a member.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477255078", "createdAt": "2020-08-26T12:18:09Z", "author": {"login": "vbekiaris"}, "path": "hazelcast/src/main/java/com/hazelcast/spi/properties/ClusterProperty.java", "diffHunk": "@@ -652,8 +652,24 @@ private int getWhenNoSSLDetected() {\n             = new HazelcastProperty(\"hazelcast.partition.table.send.interval\", 15, SECONDS);\n     public static final HazelcastProperty PARTITION_BACKUP_SYNC_INTERVAL\n             = new HazelcastProperty(\"hazelcast.partition.backup.sync.interval\", 30, SECONDS);\n+    /**\n+     * Maximum number of partition migrations to be executed on a member.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MzcwODQ1OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/spi/impl/operationservice/impl/Invocation.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxMjoyMDo1MlrOHHJb-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwOTo1MjoyNVrOHII7cQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI1NjY5Nw==", "bodyText": "If there is an associated gh issue to address what needs to be done, I 'd rather we don't add yet another todo in the codebase :-)", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477256697", "createdAt": "2020-08-26T12:20:52Z", "author": {"login": "vbekiaris"}, "path": "hazelcast/src/main/java/com/hazelcast/spi/impl/operationservice/impl/Invocation.java", "diffHunk": "@@ -283,6 +283,7 @@ final void initInvocationTarget() throws Exception {\n             if (previousTargetMember != null) {\n                 // If a target member was found earlier but current target member is null\n                 // then it means a member left.\n+                // TODO: Above comment is not always true for RaftInvocation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI3NjQ2Ng==", "bodyText": "I'll create an issue. \ud83d\udc4d", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478276466", "createdAt": "2020-08-27T09:18:02Z", "author": {"login": "mdogan"}, "path": "hazelcast/src/main/java/com/hazelcast/spi/impl/operationservice/impl/Invocation.java", "diffHunk": "@@ -283,6 +283,7 @@ final void initInvocationTarget() throws Exception {\n             if (previousTargetMember != null) {\n                 // If a target member was found earlier but current target member is null\n                 // then it means a member left.\n+                // TODO: Above comment is not always true for RaftInvocation.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI1NjY5Nw=="}, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI5Njk0NQ==", "bodyText": "#17420", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478296945", "createdAt": "2020-08-27T09:52:25Z", "author": {"login": "mdogan"}, "path": "hazelcast/src/main/java/com/hazelcast/spi/impl/operationservice/impl/Invocation.java", "diffHunk": "@@ -283,6 +283,7 @@ final void initInvocationTarget() throws Exception {\n             if (previousTargetMember != null) {\n                 // If a target member was found earlier but current target member is null\n                 // then it means a member left.\n+                // TODO: Above comment is not always true for RaftInvocation.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI1NjY5Nw=="}, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4Mzg0MzAwOnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxMjo1NToxMVrOHHKtfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxMjo1NToxMVrOHHKtfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI3NzU2NA==", "bodyText": "checkstyle \ud83d\ude09\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                                if (t instanceof  OperationTimeoutException || t.getCause() instanceof OperationTimeoutException) {\n          \n          \n            \n                                if (t instanceof OperationTimeoutException || t.getCause() instanceof OperationTimeoutException) {", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477277564", "createdAt": "2020-08-26T12:55:11Z", "author": {"login": "vbekiaris"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationManager.java", "diffHunk": "@@ -423,6 +408,86 @@ private boolean commitMigrationToDestination(MigrationInfo migration) {\n         return false;\n     }\n \n+    /**\n+     * Sends a {@link MigrationCommitOperation} to the destination and returns {@code true} if the new partition state\n+     * was applied on the destination.\n+     */\n+    @SuppressWarnings({\"checkstyle:npathcomplexity\", \"checkstyle:cyclomaticcomplexity\", \"checkstyle:methodlength\"})\n+    private CompletionStage<Boolean> commitMigrationToDestinationAsync(MigrationInfo migration) {\n+        PartitionReplica destination = migration.getDestination();\n+\n+        if (destination.isIdentical(node.getLocalMember())) {\n+            if (logger.isFinestEnabled()) {\n+                logger.finest(\"Shortcutting migration commit, since destination is master. -> \" + migration);\n+            }\n+            return CompletableFuture.completedFuture(Boolean.TRUE);\n+        }\n+\n+        Member member = node.getClusterService().getMember(destination.address(), destination.uuid());\n+        if (member == null) {\n+            logger.warning(\"Cannot commit \" + migration + \". Destination \" + destination + \" is not a member anymore\");\n+            return CompletableFuture.completedFuture(Boolean.FALSE);\n+        }\n+\n+        try {\n+            if (logger.isFinestEnabled()) {\n+                logger.finest(\"Sending migration commit operation to \" + destination + \" for \" + migration);\n+            }\n+            migration.setStatus(MigrationStatus.SUCCESS);\n+            UUID destinationUuid = member.getUuid();\n+\n+            MigrationCommitOperation operation = new MigrationCommitOperation(migration, destinationUuid);\n+            InvocationFuture<Boolean> future = nodeEngine.getOperationService()\n+                    .createInvocationBuilder(SERVICE_NAME, operation, destination.address())\n+                    .setTryCount(Integer.MAX_VALUE)\n+                    .setCallTimeout(memberHeartbeatTimeoutMillis).invoke();\n+\n+            final String successResult = \"SUCCESS\";\n+            final String failureResult = \"FAIL\";\n+            final String retryResult = \"RETRY\";\n+\n+            return future.handle((done, t) -> {\n+                // Inspect commit result;\n+                // - if there's an exception, either retry or fail\n+                // - if result is true then success, otherwise failure\n+                logger.fine(\"Migration commit response received -> \" + migration + \", success: \" + done + \", failure: \" + t);\n+                if (t != null) {\n+                    logMigrationCommitFailure(migration, t);\n+                    if (t instanceof  OperationTimeoutException || t.getCause() instanceof OperationTimeoutException) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 268}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MzkxODY4OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxMzoxNDowMVrOHHLcuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwOToyMDo0NFrOHIHxvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI4OTY1Nw==", "bodyText": "refactor to static final constants? Also, we could use a more succinct result encoding, like plain ints?", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477289657", "createdAt": "2020-08-26T13:14:01Z", "author": {"login": "vbekiaris"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationManager.java", "diffHunk": "@@ -423,6 +408,86 @@ private boolean commitMigrationToDestination(MigrationInfo migration) {\n         return false;\n     }\n \n+    /**\n+     * Sends a {@link MigrationCommitOperation} to the destination and returns {@code true} if the new partition state\n+     * was applied on the destination.\n+     */\n+    @SuppressWarnings({\"checkstyle:npathcomplexity\", \"checkstyle:cyclomaticcomplexity\", \"checkstyle:methodlength\"})\n+    private CompletionStage<Boolean> commitMigrationToDestinationAsync(MigrationInfo migration) {\n+        PartitionReplica destination = migration.getDestination();\n+\n+        if (destination.isIdentical(node.getLocalMember())) {\n+            if (logger.isFinestEnabled()) {\n+                logger.finest(\"Shortcutting migration commit, since destination is master. -> \" + migration);\n+            }\n+            return CompletableFuture.completedFuture(Boolean.TRUE);\n+        }\n+\n+        Member member = node.getClusterService().getMember(destination.address(), destination.uuid());\n+        if (member == null) {\n+            logger.warning(\"Cannot commit \" + migration + \". Destination \" + destination + \" is not a member anymore\");\n+            return CompletableFuture.completedFuture(Boolean.FALSE);\n+        }\n+\n+        try {\n+            if (logger.isFinestEnabled()) {\n+                logger.finest(\"Sending migration commit operation to \" + destination + \" for \" + migration);\n+            }\n+            migration.setStatus(MigrationStatus.SUCCESS);\n+            UUID destinationUuid = member.getUuid();\n+\n+            MigrationCommitOperation operation = new MigrationCommitOperation(migration, destinationUuid);\n+            InvocationFuture<Boolean> future = nodeEngine.getOperationService()\n+                    .createInvocationBuilder(SERVICE_NAME, operation, destination.address())\n+                    .setTryCount(Integer.MAX_VALUE)\n+                    .setCallTimeout(memberHeartbeatTimeoutMillis).invoke();\n+\n+            final String successResult = \"SUCCESS\";\n+            final String failureResult = \"FAIL\";\n+            final String retryResult = \"RETRY\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 259}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODI3ODA3Nw==", "bodyText": "\ud83d\udc4d I was logging the responses while testing, that's why I've used string values. I'll convert to ints.", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r478278077", "createdAt": "2020-08-27T09:20:44Z", "author": {"login": "mdogan"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationManager.java", "diffHunk": "@@ -423,6 +408,86 @@ private boolean commitMigrationToDestination(MigrationInfo migration) {\n         return false;\n     }\n \n+    /**\n+     * Sends a {@link MigrationCommitOperation} to the destination and returns {@code true} if the new partition state\n+     * was applied on the destination.\n+     */\n+    @SuppressWarnings({\"checkstyle:npathcomplexity\", \"checkstyle:cyclomaticcomplexity\", \"checkstyle:methodlength\"})\n+    private CompletionStage<Boolean> commitMigrationToDestinationAsync(MigrationInfo migration) {\n+        PartitionReplica destination = migration.getDestination();\n+\n+        if (destination.isIdentical(node.getLocalMember())) {\n+            if (logger.isFinestEnabled()) {\n+                logger.finest(\"Shortcutting migration commit, since destination is master. -> \" + migration);\n+            }\n+            return CompletableFuture.completedFuture(Boolean.TRUE);\n+        }\n+\n+        Member member = node.getClusterService().getMember(destination.address(), destination.uuid());\n+        if (member == null) {\n+            logger.warning(\"Cannot commit \" + migration + \". Destination \" + destination + \" is not a member anymore\");\n+            return CompletableFuture.completedFuture(Boolean.FALSE);\n+        }\n+\n+        try {\n+            if (logger.isFinestEnabled()) {\n+                logger.finest(\"Sending migration commit operation to \" + destination + \" for \" + migration);\n+            }\n+            migration.setStatus(MigrationStatus.SUCCESS);\n+            UUID destinationUuid = member.getUuid();\n+\n+            MigrationCommitOperation operation = new MigrationCommitOperation(migration, destinationUuid);\n+            InvocationFuture<Boolean> future = nodeEngine.getOperationService()\n+                    .createInvocationBuilder(SERVICE_NAME, operation, destination.address())\n+                    .setTryCount(Integer.MAX_VALUE)\n+                    .setCallTimeout(memberHeartbeatTimeoutMillis).invoke();\n+\n+            final String successResult = \"SUCCESS\";\n+            final String failureResult = \"FAIL\";\n+            final String retryResult = \"RETRY\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI4OTY1Nw=="}, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 259}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4NDU1NTg3OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationManager.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxNToyODoyOVrOHHRmvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxNToyODoyOVrOHHRmvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzM5MDUyNQ==", "bodyText": "minor: could also use the static factory methods from InternalCompletableFuture for completed futures", "url": "https://github.com/hazelcast/hazelcast/pull/17197#discussion_r477390525", "createdAt": "2020-08-26T15:28:29Z", "author": {"login": "vbekiaris"}, "path": "hazelcast/src/main/java/com/hazelcast/internal/partition/impl/MigrationManager.java", "diffHunk": "@@ -947,17 +1048,529 @@ public void migrate(PartitionReplica source, int sourceCurrentReplicaIndex, int\n                 }\n             }\n         }\n+    }\n+\n+    class MigrationPlanTask implements MigrationRunnable {\n+        /** List of migration queues per-partition */\n+        private final List<Queue<MigrationInfo>> migrationQs;\n+        /**\n+         * Queue for completed migrations.\n+         * It will be processed concurrently while migrations are running.\n+         * */\n+        private final BlockingQueue<MigrationInfo> completed;\n+        /**\n+         * Set of currently migrating partition IDs.\n+         * It's illegal to have concurrent migrations on the same partition.\n+         */\n+        private final Set<Integer> migratingPartitions = new HashSet<>();\n+        /**\n+         * Map of endpoint -> migration-count.\n+         * Only {@link #maxParallelMigrations} number of migrations are allowed on a single member.\n+         */\n+        private final Map<Address, Integer> endpoint2MigrationCount = new HashMap<>();\n+        private int ongoingMigrationCount;\n+        private boolean failed;\n+        private volatile boolean aborted;\n+\n+        MigrationPlanTask(List<Queue<MigrationInfo>> migrationQs) {\n+            this.migrationQs = migrationQs;\n+            this.completed = new ArrayBlockingQueue<>(migrationQs.size());\n+        }\n+\n+        @Override\n+        public void run() {\n+            migrationCount.set(migrationQs.stream().mapToInt(Collection::size).sum());\n+\n+            while (true) {\n+                MigrationInfo migration = next();\n+                if (migration == null) {\n+                    break;\n+                }\n+\n+                if (failed | aborted) {\n+                    break;\n+                }\n+\n+                onStart(migration);\n+\n+                try {\n+                    CompletionStage<Boolean> f = new AsyncMigrationTask(migration).run();\n+                    f.thenRun(() -> {\n+                        logger.fine(\"AsyncMigrationTask completed: \" + migration);\n+                        boolean offered = completed.offer(migration);\n+                        assert offered : \"Failed to offer completed migration: \" + migration;\n+                    });\n+                } catch (Throwable e) {\n+                    logger.warning(\"AsyncMigrationTask failed: \" + migration, e);\n+                    boolean offered = completed.offer(migration);\n+                    assert offered : \"Failed to offer completed migration: \" + migration;\n+                }\n+\n+                if (!migrationDelay()) {\n+                    break;\n+                }\n+            }\n+\n+            waitOngoingMigrations();\n+\n+            if (failed || aborted) {\n+                logger.info(\"Rebalance process was \" + (failed ? \" failed\" : \"aborted\")\n+                        + \". Ignoring remaining migrations. Will recalculate the new migration plan. (\"\n+                        + stats.formatToString(logger.isFineEnabled()) + \")\");\n+                migrationCount.set(0);\n+                migrationQs.clear();\n+            } else {\n+                logger.info(\"All migration tasks have been completed. (\" + stats.formatToString(logger.isFineEnabled()) + \")\");\n+            }\n+        }\n+\n+        private void onStart(MigrationInfo migration) {\n+            boolean added = migratingPartitions.add(migration.getPartitionId());\n+            assert added : \"Couldn't add partitionId to migrating partitions set: \" + migration;\n+\n+            BiFunction<Address, Integer, Integer> inc = (address, current) -> current != null ? current + 1 : 1;\n+\n+            int count = endpoint2MigrationCount.compute(migration.getDestinationAddress(), inc);\n+            assert count > 0 && count <= maxParallelMigrations : \"Count: \" + count + \" -> \" + migration;\n+\n+            count = endpoint2MigrationCount.compute(sourceAddress(migration), inc);\n+            assert count > 0 && count <= maxParallelMigrations : \"Count: \" + count + \" -> \" + migration;\n \n+            ongoingMigrationCount++;\n+            migrationCount.decrementAndGet();\n+        }\n+\n+        private void onComplete(MigrationInfo migration) {\n+            boolean removed = migratingPartitions.remove(migration.getPartitionId());\n+            assert removed : \"Couldn't remove partitionId from migrating partitions set: \" + migration;\n+\n+            BiFunction<Address, Integer, Integer> dec = (address, current) -> current != null ? current - 1 : -1;\n+\n+            long count = endpoint2MigrationCount.compute(migration.getDestinationAddress(), dec);\n+            assert count >= 0 && count < maxParallelMigrations : \"Count: \" + count + \" -> \" + migration;\n+\n+            count = endpoint2MigrationCount.compute(sourceAddress(migration), dec);\n+            assert count >= 0 && count < maxParallelMigrations : \"Count: \" + count + \" -> \" + migration;\n+\n+            if (migration.getStatus() != MigrationStatus.SUCCESS) {\n+                failed = true;\n+            }\n+\n+            ongoingMigrationCount--;\n+        }\n+\n+        private boolean processCompleted() {\n+            boolean ok = false;\n+            MigrationInfo migration;\n+            while ((migration = completed.poll()) != null) {\n+                onComplete(migration);\n+                ok = true;\n+            }\n+            return ok;\n+        }\n+\n+        private MigrationInfo next() {\n+            MigrationInfo m;\n+            while ((m = next0()) == null) {\n+                if (migrationQs.isEmpty()) {\n+                    break;\n+                }\n+\n+                if (!processCompleted()) {\n+                    try {\n+                        MigrationInfo migration = completed.take();\n+                        onComplete(migration);\n+                    } catch (InterruptedException e) {\n+                        onInterrupted(e);\n+                        break;\n+                    }\n+                }\n+\n+                if (failed | aborted) {\n+                    break;\n+                }\n+            }\n+            return m;\n+        }\n+\n+        private MigrationInfo next0() {\n+            Iterator<Queue<MigrationInfo>> iter = migrationQs.iterator();\n+            while (iter.hasNext()) {\n+                Queue<MigrationInfo> q = iter.next();\n+                if (q.isEmpty()) {\n+                    iter.remove();\n+                    continue;\n+                }\n+\n+                if (!select(q.peek())) {\n+                    continue;\n+                }\n+\n+                return q.poll();\n+            }\n+            return null;\n+        }\n+\n+        private boolean select(MigrationInfo m) {\n+            if (m == null) {\n+                return true;\n+            }\n+\n+            if (migratingPartitions.contains(m.getPartitionId())) {\n+                return false;\n+            }\n+            if (endpoint2MigrationCount.getOrDefault(m.getDestinationAddress(), 0) == maxParallelMigrations) {\n+                return false;\n+            }\n+            return endpoint2MigrationCount.getOrDefault(sourceAddress(m), 0) < maxParallelMigrations;\n+        }\n+\n+        private Address sourceAddress(MigrationInfo m) {\n+            if (m.getSourceCurrentReplicaIndex() == 0) {\n+                return m.getSourceAddress();\n+            }\n+            InternalPartitionImpl partition = partitionStateManager.getPartitionImpl(m.getPartitionId());\n+            return partition.getOwnerOrNull();\n+        }\n+\n+        private boolean migrationDelay() {\n+            if (partitionMigrationInterval > 0) {\n+                try {\n+                    Thread.sleep(partitionMigrationInterval);\n+                } catch (InterruptedException e) {\n+                    onInterrupted(e);\n+                    return false;\n+                }\n+            }\n+            return true;\n+        }\n+\n+        private void waitOngoingMigrations() {\n+            boolean interrupted = false;\n+            while (ongoingMigrationCount > 0) {\n+                try {\n+                    MigrationInfo migration = completed.take();\n+                    onComplete(migration);\n+                } catch (InterruptedException ignored) {\n+                    interrupted = true;\n+                }\n+            }\n+            if (interrupted) {\n+                Thread.currentThread().interrupt();\n+            }\n+        }\n+\n+        private void onInterrupted(InterruptedException e) {\n+            logger.info(\"MigrationProcessTask is interrupted! Ignoring remaining migrations...\", e);\n+            Thread.currentThread().interrupt();\n+            abort();\n+        }\n+\n+        void abort() {\n+            aborted = true;\n+        }\n+    }\n+\n+    /**\n+     * Invoked on the master node to migrate a partition (excluding promotions).\n+     * It will execute the {@link MigrationRequestOperation} on the partition owner.\n+     */\n+    private class AsyncMigrationTask {\n+        private final MigrationInfo migration;\n+\n+        AsyncMigrationTask(MigrationInfo migration) {\n+            this.migration = migration;\n+            migration.setMaster(node.getThisAddress());\n+        }\n+\n+        CompletionStage<Boolean> run() {\n+            if (!partitionService.isLocalMemberMaster()) {\n+                return CompletableFuture.completedFuture(Boolean.FALSE);\n+            }\n+\n+            if (migration.getSource() == null\n+                    && migration.getDestinationCurrentReplicaIndex() > 0\n+                    && migration.getDestinationNewReplicaIndex() == 0) {\n+\n+                throw new IllegalStateException(\"Promotion migrations should be handled by \"\n+                        + RepairPartitionTableTask.class.getSimpleName() + \" -> \" + migration);\n+            }\n+\n+            Member partitionOwner = checkMigrationParticipantsAndGetPartitionOwner();\n+            if (partitionOwner == null) {\n+                return CompletableFuture.completedFuture(Boolean.FALSE);\n+            }\n+\n+            return executeMigrateOperation(partitionOwner);\n+        }\n+\n+        private void beforeMigration() {\n+            migration.setInitialPartitionVersion(partitionStateManager.getPartitionVersion(migration.getPartitionId()));\n+            migrationInterceptor.onMigrationStart(MigrationParticipant.MASTER, migration);\n+            if (logger.isFineEnabled()) {\n+                logger.fine(\"Starting Migration: \" + migration);\n+            }\n+        }\n+\n+        /**\n+         * Checks if the partition owner is not {@code null}, the source and destinations are still members and returns the owner.\n+         * Returns {@code null} and reschedules the {@link ControlTask} if the checks failed.\n+         */\n+        private Member checkMigrationParticipantsAndGetPartitionOwner() {\n+            Member partitionOwner = getPartitionOwner();\n+            if (partitionOwner == null) {\n+                logger.fine(\"Partition owner is null. Ignoring \" + migration);\n+                triggerRepartitioningAfterMigrationFailure();\n+                return null;\n+            }\n+            if (migration.getSource() != null) {\n+                PartitionReplica source = migration.getSource();\n+                if (node.getClusterService().getMember(source.address(), source.uuid()) == null) {\n+                    logger.fine(\"Source is not a member anymore. Ignoring \" + migration);\n+                    triggerRepartitioningAfterMigrationFailure();\n+                    return null;\n+                }\n+            }\n+            PartitionReplica destination = migration.getDestination();\n+            if (node.getClusterService().getMember(destination.address(), destination.uuid()) == null) {\n+                logger.fine(\"Destination is not a member anymore. Ignoring \" + migration);\n+                triggerRepartitioningAfterMigrationFailure();\n+                return null;\n+            }\n+            return partitionOwner;\n+        }\n+\n+        /** Returns the partition owner or {@code null} if it is not set. */\n+        private Member getPartitionOwner() {\n+            InternalPartitionImpl partition = partitionStateManager.getPartitionImpl(migration.getPartitionId());\n+            PartitionReplica owner = partition.getOwnerReplicaOrNull();\n+            if (owner == null) {\n+                if (migration.isValid()) {\n+                    logger.severe(\"Skipping migration! Partition owner is not set! -> partitionId=\"\n+                            + migration.getPartitionId()\n+                            + \", \" + partition + \" -VS- \" + migration);\n+                }\n+                return null;\n+            }\n+            return node.getClusterService().getMember(owner.address(), owner.uuid());\n+        }\n+\n+        /**\n+         * Sends a {@link MigrationRequestOperation} to the {@code fromMember} and returns the migration result if the\n+         * migration was successful.\n+         */\n+        @SuppressWarnings({\"checkstyle:npathcomplexity\", \"checkstyle:cyclomaticcomplexity\"})\n+        private CompletionStage<Boolean> executeMigrateOperation(Member fromMember) {\n+            long start = Timer.nanos();\n+            CompletableFuture<Boolean> future;\n+            try {\n+                beforeMigration();\n+\n+                List<MigrationInfo> completedMigrations = getCompletedMigrations(migration.getPartitionId());\n+                Operation op = new MigrationRequestOperation(migration, completedMigrations, 0, fragmentedMigrationEnabled);\n+                future = nodeEngine.getOperationService()\n+                        .createInvocationBuilder(SERVICE_NAME, op, fromMember.getAddress())\n+                        .setCallTimeout(partitionMigrationTimeout)\n+                        .invoke();\n+            } catch (Throwable t) {\n+                Level level = migration.isValid() ? Level.WARNING : Level.FINE;\n+                logger.log(level, \"Error during \" + migration, t);\n+                future = new CompletableFuture<>();\n+                future.completeExceptionally(t);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "869cd7f44a82f572cf57e40e6d5e791fd62af919"}, "originalPosition": 810}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 459, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}