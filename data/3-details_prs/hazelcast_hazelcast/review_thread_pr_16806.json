{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk0MTkzMTQ0", "number": 16806, "reviewThreads": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QwODozNToyMFrODr-ADg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxMDo1MDowM1rODsnc3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3NDMxMTgyOnYy", "diffSide": "RIGHT", "path": "docs/design/sql/04-mulitthreaded-execution.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QwODozNToyMVrOF8nflg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QwODo1MTo0OVrOF8oBEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTEwNTk0Mg==", "bodyText": "Probably \"..., which operate on separate ...\" is a better wording, \"update\" sounds strange when applied to GET.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399105942", "createdAt": "2020-03-27T08:35:21Z", "author": {"login": "taburet"}, "path": "docs/design/sql/04-mulitthreaded-execution.md", "diffHunk": "@@ -0,0 +1,215 @@\n+# SQL Multithreaded Execution\n+\n+## Overview\n+\n+The Hazelcast Mustang engine executes queries in parallel. This document describes the design of the multithreaded execution\n+environment of the engine.\n+\n+The document doesn't discuss operator-level parallelism, which is a different topic.\n+\n+The rest of this document is organized as follows. In section 1 we discuss the existing threading infrastructure of Hazelcast\n+IMDG and Hazelcast Jet. In section 2 we analyze why the existing infrastructure is inappropriate for query execution and then\n+describe the design of the Hazelcast Mustang execution environment. In section 3 we discuss possible alternative approaches\n+that were rejected.\n+\n+## 1 Existing Infrastructure\n+\n+Hazelcast IMDG uses staged event-driven architecture (SEDA) for message processing. During execution, a message passes through\n+several thread pools (stages), each optimized for a specific type of workload. We now describe stages that exist in Hazelcast.\n+\n+### 1.1 IO Pool\n+\n+Hazelcast uses a dedicated thread pool for message send and receive, which will be referred to as **IO Pool** in this paper.\n+Each thread from the IO pool maintains a subset of connections to remote members. Consider that we have a sender member (S)\n+and a receiver member (R). The typical execution flow is organized as follows:\n+1. The message is added to the queue of a single IO thread, and the thread is notified\n+1. The sender IO thread wakes up and sends the message over the network\n+1. A receiver IO thread is notified by the operating system on receive\n+1. The receiver IO thread wakes up, determines the next execution stage, adds the message to the stage's queue and notifies the\n+stage\n+1. The next execution stage processes the message\n+\n+*Snippet 1: Message execution flow*\n+```\n+Stage(S)                 IO(S)        IO(R)                Stage(R)\n+   |----enqueue/notify->--|            |                      |\n+   |                      |----send->--|                      |\n+   |                      |            |----enqueue/notify->--|\n+```\n+\n+We now discuss the organization of different execution stages.\n+\n+### 1.2 Partition Pool\n+\n+A message may have a logical **partition**, which is a positive integer number. Messages with defined partition are routed to\n+a special thread pool, which we refer to as **partition pool**. The pool has several threads. Every thread has a dedicated task\n+queue. Partition of the message is used to determine the exact thread which will process the message:\n+`threadIndex = partition % threadCount`.\n+\n+The partition pool has the following advantages:\n+1. Only one thread processes messages with the given partition so that processing logic may use less synchronization\n+1. Dedicated thread queues reduce contention on enqueue/deque operations\n+\n+The partition pool has the following disadvantage:\n+1. There is no balancing between threads: a single long-running task may delay other tasks from the same partition\n+indefinitely; likewise, an imbalance between partitions may cause resource underutilization\n+\n+The partition pool is thus most suitable for small tasks that operate on independent physical resources, and that are\n+distributed equally between logical partitions. An example is `IMap` operations, which update separate physical partitions,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fb09342b6e66b06d587040b4b8d137d0a11764c"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTExNDUxNA==", "bodyText": "Rephrased.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399114514", "createdAt": "2020-03-27T08:51:49Z", "author": {"login": "devozerov"}, "path": "docs/design/sql/04-mulitthreaded-execution.md", "diffHunk": "@@ -0,0 +1,215 @@\n+# SQL Multithreaded Execution\n+\n+## Overview\n+\n+The Hazelcast Mustang engine executes queries in parallel. This document describes the design of the multithreaded execution\n+environment of the engine.\n+\n+The document doesn't discuss operator-level parallelism, which is a different topic.\n+\n+The rest of this document is organized as follows. In section 1 we discuss the existing threading infrastructure of Hazelcast\n+IMDG and Hazelcast Jet. In section 2 we analyze why the existing infrastructure is inappropriate for query execution and then\n+describe the design of the Hazelcast Mustang execution environment. In section 3 we discuss possible alternative approaches\n+that were rejected.\n+\n+## 1 Existing Infrastructure\n+\n+Hazelcast IMDG uses staged event-driven architecture (SEDA) for message processing. During execution, a message passes through\n+several thread pools (stages), each optimized for a specific type of workload. We now describe stages that exist in Hazelcast.\n+\n+### 1.1 IO Pool\n+\n+Hazelcast uses a dedicated thread pool for message send and receive, which will be referred to as **IO Pool** in this paper.\n+Each thread from the IO pool maintains a subset of connections to remote members. Consider that we have a sender member (S)\n+and a receiver member (R). The typical execution flow is organized as follows:\n+1. The message is added to the queue of a single IO thread, and the thread is notified\n+1. The sender IO thread wakes up and sends the message over the network\n+1. A receiver IO thread is notified by the operating system on receive\n+1. The receiver IO thread wakes up, determines the next execution stage, adds the message to the stage's queue and notifies the\n+stage\n+1. The next execution stage processes the message\n+\n+*Snippet 1: Message execution flow*\n+```\n+Stage(S)                 IO(S)        IO(R)                Stage(R)\n+   |----enqueue/notify->--|            |                      |\n+   |                      |----send->--|                      |\n+   |                      |            |----enqueue/notify->--|\n+```\n+\n+We now discuss the organization of different execution stages.\n+\n+### 1.2 Partition Pool\n+\n+A message may have a logical **partition**, which is a positive integer number. Messages with defined partition are routed to\n+a special thread pool, which we refer to as **partition pool**. The pool has several threads. Every thread has a dedicated task\n+queue. Partition of the message is used to determine the exact thread which will process the message:\n+`threadIndex = partition % threadCount`.\n+\n+The partition pool has the following advantages:\n+1. Only one thread processes messages with the given partition so that processing logic may use less synchronization\n+1. Dedicated thread queues reduce contention on enqueue/deque operations\n+\n+The partition pool has the following disadvantage:\n+1. There is no balancing between threads: a single long-running task may delay other tasks from the same partition\n+indefinitely; likewise, an imbalance between partitions may cause resource underutilization\n+\n+The partition pool is thus most suitable for small tasks that operate on independent physical resources, and that are\n+distributed equally between logical partitions. An example is `IMap` operations, which update separate physical partitions,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTEwNTk0Mg=="}, "originalCommit": {"oid": "8fb09342b6e66b06d587040b4b8d137d0a11764c"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3NDM5NzYwOnYy", "diffSide": "RIGHT", "path": "docs/design/sql/04-mulitthreaded-execution.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QwOTowMTowN1rOF8oTxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QxMTo0MDo1NFrOF8tkcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTExOTMwMA==", "bodyText": "\"... should be ordered ...\"", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399119300", "createdAt": "2020-03-27T09:01:07Z", "author": {"login": "taburet"}, "path": "docs/design/sql/04-mulitthreaded-execution.md", "diffHunk": "@@ -0,0 +1,215 @@\n+# SQL Multithreaded Execution\n+\n+## Overview\n+\n+The Hazelcast Mustang engine executes queries in parallel. This document describes the design of the multithreaded execution\n+environment of the engine.\n+\n+The document doesn't discuss operator-level parallelism, which is a different topic.\n+\n+The rest of this document is organized as follows. In section 1 we discuss the existing threading infrastructure of Hazelcast\n+IMDG and Hazelcast Jet. In section 2 we analyze why the existing infrastructure is inappropriate for query execution and then\n+describe the design of the Hazelcast Mustang execution environment. In section 3 we discuss possible alternative approaches\n+that were rejected.\n+\n+## 1 Existing Infrastructure\n+\n+Hazelcast IMDG uses staged event-driven architecture (SEDA) for message processing. During execution, a message passes through\n+several thread pools (stages), each optimized for a specific type of workload. We now describe stages that exist in Hazelcast.\n+\n+### 1.1 IO Pool\n+\n+Hazelcast uses a dedicated thread pool for message send and receive, which will be referred to as **IO Pool** in this paper.\n+Each thread from the IO pool maintains a subset of connections to remote members. Consider that we have a sender member (S)\n+and a receiver member (R). The typical execution flow is organized as follows:\n+1. The message is added to the queue of a single IO thread, and the thread is notified\n+1. The sender IO thread wakes up and sends the message over the network\n+1. A receiver IO thread is notified by the operating system on receive\n+1. The receiver IO thread wakes up, determines the next execution stage, adds the message to the stage's queue and notifies the\n+stage\n+1. The next execution stage processes the message\n+\n+*Snippet 1: Message execution flow*\n+```\n+Stage(S)                 IO(S)        IO(R)                Stage(R)\n+   |----enqueue/notify->--|            |                      |\n+   |                      |----send->--|                      |\n+   |                      |            |----enqueue/notify->--|\n+```\n+\n+We now discuss the organization of different execution stages.\n+\n+### 1.2 Partition Pool\n+\n+A message may have a logical **partition**, which is a positive integer number. Messages with defined partition are routed to\n+a special thread pool, which we refer to as **partition pool**. The pool has several threads. Every thread has a dedicated task\n+queue. Partition of the message is used to determine the exact thread which will process the message:\n+`threadIndex = partition % threadCount`.\n+\n+The partition pool has the following advantages:\n+1. Only one thread processes messages with the given partition so that processing logic may use less synchronization\n+1. Dedicated thread queues reduce contention on enqueue/deque operations\n+\n+The partition pool has the following disadvantage:\n+1. There is no balancing between threads: a single long-running task may delay other tasks from the same partition\n+indefinitely; likewise, an imbalance between partitions may cause resource underutilization\n+\n+The partition pool is thus most suitable for small tasks that operate on independent physical resources, and that are\n+distributed equally between logical partitions. An example is `IMap` operations, which operate on separate physical\n+partitions, such as `GET` and `PUT`.\n+\n+Since the partition is a logical notion, it is possible to multiplex tasks from different components to a single partition pool.\n+For example, CP Subsystem schedules tasks, all with the same partition, to the partition pool to ensure total processing order.\n+\n+### 1.3 Generic Pool\n+\n+If a message doesn't have a logical partition, it is submitted to the **generic pool**. This is a conventional thread pool with\n+a shared blocking queue. It has inherent balancing capabilities. But at the same time, this pool may demonstrate less than\n+optimal throughput when a lot of small tasks are submitted due to contention on the queue.\n+\n+### 1.4 Hazelcast Jet Pool\n+\n+Hazelcast Jet uses its own cooperative pool to execute Jet jobs. Every thread has its own queue of jobs that are executed\n+cooperatively. There is no balancing: once the job is submitted to a specific thread, it is always executed in that thread.\n+\n+IO pool doesn't notify the Jet pool about new data batch (\"push\"). Instead, the message is just enqueued, and the Jet thread\n+checks the queue periodically (\"poll\").\n+\n+## 2 Design\n+\n+We now define the requirements to Hazelcast Mustang threading model, analyze them concerning existing infrastructure, and\n+define the design.\n+\n+### 2.1 Requirements\n+\n+The requirements are thread safety, load balancing, and ordered message processing.\n+\n+First, the infrastructure must guarantee that operator execution is thread-safe. That is, the stateful operator should not be\n+executed by multiple threads simultaneously. This simplifies operator implementations and makes them more performant.\n+Hazelcast Jet follows this principle, as only one thread may execute a particular job. However, Hazelcast Jet pool doesn't\n+satisfy the load balancing requirement discussed below.\n+\n+Second, the execution environment must support load balancing. Query execution may take a long time to complete. If several query\n+fragments have been assigned to a single execution thread, it should be possible to reassign them to idle threads dynamically.\n+Hence neither partition pool nor Hazelcast Jet pool designs are applicable to Hazelcast Mustang because they lack balancing\n+capabilities.\n+\n+Third, it should be possible to execute some tasks in order. That is, if task `A` is received before task `B`, then it\n+should be executed before `B`. It is always possible to implement an ordering guarantee with the help of additional\n+synchronization primitives, but it increases complexity and typically reduces performance. So we prefer to have a threading\n+infrastructure with ordering guarantees, such as in the partition pool. Examples of tasks requiring ordered processing:\n+1. Query cancel should be executed after query start to minimize resource leaks\n+1. The N-th batch from the stream should be processed before the (N+1)-th batch, as described in [[1]] (p. 1.3)\n+\n+### 2.1 General Design\n+\n+We define the taxonomy of tasks related to query execution.\n+\n+First, the engine must execute query fragments, i.e., advance Volcano-style operators, as explained in [[2]] (p. 3). Fragment\n+execution is initiated in response to query start or data messages. Fragment execution may take significant time to complete.\n+\n+Second, the engine must process query operations described in [[1]] (p. 3), such as query start, query cancel, batch arrival,\n+and flow control. These operations take short time to complete. Moreover, query start operation may trigger the parallel\n+execution of several fragments.\n+\n+Given the different nature of query operations and fragment execution, we split them into independent stages, called\n+**operation pool** and **fragment pool**. The former executes query operations, and the latter executes fragments. This design\n+provides a clear separation of concerns and allows us to optimize stages for their tasks as described below, which improves\n+performance. On the other hand, this design introduces an additional thread notification, as shown in the snippet below, which\n+may negatively affect performance. Nevertheless, we think that the advantages of this approach outweigh the disadvantages.\n+\n+*Snippet 2: Query start flow (receiver only)*\n+```\n+IO               Operation pool         Fragment pool\n+|----enqueue/notify-->-|                      |\n+|                      |----enqueue/notify-->-|\n+```\n+\n+### 2.2 Operation Pool\n+\n+Every network message received by the IO pool is first submitted to the operation pool.\n+\n+The pool is organized as a fixed pool of threads with dedicated per-thread queues. Dedicated queues reduce contention and\n+increase the throughput, which is important given that processing of a single message takes little time.\n+\n+Every message may define an optional logical partition. If the partition is not defined, a random thread is picked for message\n+processing. If the partition is defined, the index of executing thread is defined as `threadIndex = partition % threadCount`.\n+Two messages with the same partition are guaranteed to be processed by the same thread, thus providing ordering guarantees.\n+\n+Some messages may trigger query fragment execution, as described below. In this case, a fragment execution task is created and\n+submitted for execution to the fragment pool.\n+1. `execute` - triggers the execution of one or more fragments\n+1. `batch` and `flow_control` - trigger execution of one fragment defined by query ID and edge ID, as described in [[1]]\n+\n+### 2.3 Fragment Pool\n+\n+The fragment pool is responsible for the execution of individual query fragments. Fragment execution is always initiated by\n+a task from the operation pool.\n+\n+Unlike operations, fragment execution may take arbitrary time depending on the query structure. It is, therefore, important to\n+guarantee high throughput for short fragments, while still providing load balancing for long fragments. The ideal candidate\n+is a thread pool with dedicated per-thread queues and work-stealing. For this reason, we choose JDK's `ForkJoinPool` as a\n+backbone of the fragment pool. We create a separate instance of the `ForkJoinPool` rather than using `ForkJoinPool.commonPool()`\n+to avoid interference with user workloads.\n+\n+We may decide to introduce multiple fragment pools for better resource management in the future. This will help limit the\n+maximum number of CPU cores dedicated to a particular workload. An example of this approach is **Resource Pools** in MemSQL [[3]].\n+\n+## 3 Alternative Approaches\n+\n+Several other approaches were considered but then rejected. This section explains these approaches and the rejection\n+reasons.\n+\n+### 3.1 Use Single Pool\n+\n+It is possible to use a single pool for both operation and fragment processing and remove the additional notifications for\n+fragment execution. The problem is that an operation task is short and requires fast response, while fragment execution is\n+potentially a long-running task. Consider two examples that demonstrate when this approach doesn't work well.\n+\n+**Example 1**:\n+\n+Consider that long-running fragments occupied all threads in the pool. Then a `cancel` message arrives, but it cannot be\n+processed because all threads are busy. To fix it, we would have to introduce a separate priority queue, which should be\n+checked periodically by fragment tasks. This may lead to wasted CPU cycles and frequent interrupts to fragment tasks.\n+\n+**Example 2**\n+\n+Consider a fragment which occupied the thread `A`. Then a `batch` message arrives, which should resume execution of another\n+fragment. Given that the processing of `batch` message should is ordered, it has logical partition defined. It may happen that", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e1f504f79698e4cca5f07840c15055c176ba77a2"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIwNTQ5MQ==", "bodyText": "Fixed", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399205491", "createdAt": "2020-03-27T11:40:54Z", "author": {"login": "devozerov"}, "path": "docs/design/sql/04-mulitthreaded-execution.md", "diffHunk": "@@ -0,0 +1,215 @@\n+# SQL Multithreaded Execution\n+\n+## Overview\n+\n+The Hazelcast Mustang engine executes queries in parallel. This document describes the design of the multithreaded execution\n+environment of the engine.\n+\n+The document doesn't discuss operator-level parallelism, which is a different topic.\n+\n+The rest of this document is organized as follows. In section 1 we discuss the existing threading infrastructure of Hazelcast\n+IMDG and Hazelcast Jet. In section 2 we analyze why the existing infrastructure is inappropriate for query execution and then\n+describe the design of the Hazelcast Mustang execution environment. In section 3 we discuss possible alternative approaches\n+that were rejected.\n+\n+## 1 Existing Infrastructure\n+\n+Hazelcast IMDG uses staged event-driven architecture (SEDA) for message processing. During execution, a message passes through\n+several thread pools (stages), each optimized for a specific type of workload. We now describe stages that exist in Hazelcast.\n+\n+### 1.1 IO Pool\n+\n+Hazelcast uses a dedicated thread pool for message send and receive, which will be referred to as **IO Pool** in this paper.\n+Each thread from the IO pool maintains a subset of connections to remote members. Consider that we have a sender member (S)\n+and a receiver member (R). The typical execution flow is organized as follows:\n+1. The message is added to the queue of a single IO thread, and the thread is notified\n+1. The sender IO thread wakes up and sends the message over the network\n+1. A receiver IO thread is notified by the operating system on receive\n+1. The receiver IO thread wakes up, determines the next execution stage, adds the message to the stage's queue and notifies the\n+stage\n+1. The next execution stage processes the message\n+\n+*Snippet 1: Message execution flow*\n+```\n+Stage(S)                 IO(S)        IO(R)                Stage(R)\n+   |----enqueue/notify->--|            |                      |\n+   |                      |----send->--|                      |\n+   |                      |            |----enqueue/notify->--|\n+```\n+\n+We now discuss the organization of different execution stages.\n+\n+### 1.2 Partition Pool\n+\n+A message may have a logical **partition**, which is a positive integer number. Messages with defined partition are routed to\n+a special thread pool, which we refer to as **partition pool**. The pool has several threads. Every thread has a dedicated task\n+queue. Partition of the message is used to determine the exact thread which will process the message:\n+`threadIndex = partition % threadCount`.\n+\n+The partition pool has the following advantages:\n+1. Only one thread processes messages with the given partition so that processing logic may use less synchronization\n+1. Dedicated thread queues reduce contention on enqueue/deque operations\n+\n+The partition pool has the following disadvantage:\n+1. There is no balancing between threads: a single long-running task may delay other tasks from the same partition\n+indefinitely; likewise, an imbalance between partitions may cause resource underutilization\n+\n+The partition pool is thus most suitable for small tasks that operate on independent physical resources, and that are\n+distributed equally between logical partitions. An example is `IMap` operations, which operate on separate physical\n+partitions, such as `GET` and `PUT`.\n+\n+Since the partition is a logical notion, it is possible to multiplex tasks from different components to a single partition pool.\n+For example, CP Subsystem schedules tasks, all with the same partition, to the partition pool to ensure total processing order.\n+\n+### 1.3 Generic Pool\n+\n+If a message doesn't have a logical partition, it is submitted to the **generic pool**. This is a conventional thread pool with\n+a shared blocking queue. It has inherent balancing capabilities. But at the same time, this pool may demonstrate less than\n+optimal throughput when a lot of small tasks are submitted due to contention on the queue.\n+\n+### 1.4 Hazelcast Jet Pool\n+\n+Hazelcast Jet uses its own cooperative pool to execute Jet jobs. Every thread has its own queue of jobs that are executed\n+cooperatively. There is no balancing: once the job is submitted to a specific thread, it is always executed in that thread.\n+\n+IO pool doesn't notify the Jet pool about new data batch (\"push\"). Instead, the message is just enqueued, and the Jet thread\n+checks the queue periodically (\"poll\").\n+\n+## 2 Design\n+\n+We now define the requirements to Hazelcast Mustang threading model, analyze them concerning existing infrastructure, and\n+define the design.\n+\n+### 2.1 Requirements\n+\n+The requirements are thread safety, load balancing, and ordered message processing.\n+\n+First, the infrastructure must guarantee that operator execution is thread-safe. That is, the stateful operator should not be\n+executed by multiple threads simultaneously. This simplifies operator implementations and makes them more performant.\n+Hazelcast Jet follows this principle, as only one thread may execute a particular job. However, Hazelcast Jet pool doesn't\n+satisfy the load balancing requirement discussed below.\n+\n+Second, the execution environment must support load balancing. Query execution may take a long time to complete. If several query\n+fragments have been assigned to a single execution thread, it should be possible to reassign them to idle threads dynamically.\n+Hence neither partition pool nor Hazelcast Jet pool designs are applicable to Hazelcast Mustang because they lack balancing\n+capabilities.\n+\n+Third, it should be possible to execute some tasks in order. That is, if task `A` is received before task `B`, then it\n+should be executed before `B`. It is always possible to implement an ordering guarantee with the help of additional\n+synchronization primitives, but it increases complexity and typically reduces performance. So we prefer to have a threading\n+infrastructure with ordering guarantees, such as in the partition pool. Examples of tasks requiring ordered processing:\n+1. Query cancel should be executed after query start to minimize resource leaks\n+1. The N-th batch from the stream should be processed before the (N+1)-th batch, as described in [[1]] (p. 1.3)\n+\n+### 2.1 General Design\n+\n+We define the taxonomy of tasks related to query execution.\n+\n+First, the engine must execute query fragments, i.e., advance Volcano-style operators, as explained in [[2]] (p. 3). Fragment\n+execution is initiated in response to query start or data messages. Fragment execution may take significant time to complete.\n+\n+Second, the engine must process query operations described in [[1]] (p. 3), such as query start, query cancel, batch arrival,\n+and flow control. These operations take short time to complete. Moreover, query start operation may trigger the parallel\n+execution of several fragments.\n+\n+Given the different nature of query operations and fragment execution, we split them into independent stages, called\n+**operation pool** and **fragment pool**. The former executes query operations, and the latter executes fragments. This design\n+provides a clear separation of concerns and allows us to optimize stages for their tasks as described below, which improves\n+performance. On the other hand, this design introduces an additional thread notification, as shown in the snippet below, which\n+may negatively affect performance. Nevertheless, we think that the advantages of this approach outweigh the disadvantages.\n+\n+*Snippet 2: Query start flow (receiver only)*\n+```\n+IO               Operation pool         Fragment pool\n+|----enqueue/notify-->-|                      |\n+|                      |----enqueue/notify-->-|\n+```\n+\n+### 2.2 Operation Pool\n+\n+Every network message received by the IO pool is first submitted to the operation pool.\n+\n+The pool is organized as a fixed pool of threads with dedicated per-thread queues. Dedicated queues reduce contention and\n+increase the throughput, which is important given that processing of a single message takes little time.\n+\n+Every message may define an optional logical partition. If the partition is not defined, a random thread is picked for message\n+processing. If the partition is defined, the index of executing thread is defined as `threadIndex = partition % threadCount`.\n+Two messages with the same partition are guaranteed to be processed by the same thread, thus providing ordering guarantees.\n+\n+Some messages may trigger query fragment execution, as described below. In this case, a fragment execution task is created and\n+submitted for execution to the fragment pool.\n+1. `execute` - triggers the execution of one or more fragments\n+1. `batch` and `flow_control` - trigger execution of one fragment defined by query ID and edge ID, as described in [[1]]\n+\n+### 2.3 Fragment Pool\n+\n+The fragment pool is responsible for the execution of individual query fragments. Fragment execution is always initiated by\n+a task from the operation pool.\n+\n+Unlike operations, fragment execution may take arbitrary time depending on the query structure. It is, therefore, important to\n+guarantee high throughput for short fragments, while still providing load balancing for long fragments. The ideal candidate\n+is a thread pool with dedicated per-thread queues and work-stealing. For this reason, we choose JDK's `ForkJoinPool` as a\n+backbone of the fragment pool. We create a separate instance of the `ForkJoinPool` rather than using `ForkJoinPool.commonPool()`\n+to avoid interference with user workloads.\n+\n+We may decide to introduce multiple fragment pools for better resource management in the future. This will help limit the\n+maximum number of CPU cores dedicated to a particular workload. An example of this approach is **Resource Pools** in MemSQL [[3]].\n+\n+## 3 Alternative Approaches\n+\n+Several other approaches were considered but then rejected. This section explains these approaches and the rejection\n+reasons.\n+\n+### 3.1 Use Single Pool\n+\n+It is possible to use a single pool for both operation and fragment processing and remove the additional notifications for\n+fragment execution. The problem is that an operation task is short and requires fast response, while fragment execution is\n+potentially a long-running task. Consider two examples that demonstrate when this approach doesn't work well.\n+\n+**Example 1**:\n+\n+Consider that long-running fragments occupied all threads in the pool. Then a `cancel` message arrives, but it cannot be\n+processed because all threads are busy. To fix it, we would have to introduce a separate priority queue, which should be\n+checked periodically by fragment tasks. This may lead to wasted CPU cycles and frequent interrupts to fragment tasks.\n+\n+**Example 2**\n+\n+Consider a fragment which occupied the thread `A`. Then a `batch` message arrives, which should resume execution of another\n+fragment. Given that the processing of `batch` message should is ordered, it has logical partition defined. It may happen that", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTExOTMwMA=="}, "originalCommit": {"oid": "e1f504f79698e4cca5f07840c15055c176ba77a2"}, "originalPosition": 178}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3NDczNTg1OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentWorkerPool.java", "isResolved": false, "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QxMDozNzoyM1rOF8rmGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QxNDowNDoxOVrOF8yknA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTE3MzE0Nw==", "bodyText": "It's not a good idea to pass null as UncaughtExceptionHandler, all exceptions will be silenced.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399173147", "createdAt": "2020-03-27T10:37:23Z", "author": {"login": "taburet"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentWorkerPool.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.sql.impl.QueryUtils;\n+\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.concurrent.ForkJoinWorkerThread;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_FRAGMENT;\n+\n+/**\n+ * Thread pool that executes query fragments.\n+ */\n+public class QueryFragmentWorkerPool {\n+\n+    private final ForkJoinPool pool;\n+\n+    public QueryFragmentWorkerPool(String instanceName, int threadCount) {\n+        pool = new ForkJoinPool(threadCount, new WorkerThreadFactory(instanceName), null, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e1f504f79698e4cca5f07840c15055c176ba77a2"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTE3NDUxNg==", "bodyText": "... QueryOperationWorkerPool also silences all exceptions.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399174516", "createdAt": "2020-03-27T10:39:51Z", "author": {"login": "taburet"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentWorkerPool.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.sql.impl.QueryUtils;\n+\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.concurrent.ForkJoinWorkerThread;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_FRAGMENT;\n+\n+/**\n+ * Thread pool that executes query fragments.\n+ */\n+public class QueryFragmentWorkerPool {\n+\n+    private final ForkJoinPool pool;\n+\n+    public QueryFragmentWorkerPool(String instanceName, int threadCount) {\n+        pool = new ForkJoinPool(threadCount, new WorkerThreadFactory(instanceName), null, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTE3MzE0Nw=="}, "originalCommit": {"oid": "e1f504f79698e4cca5f07840c15055c176ba77a2"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIwMzM2Ng==", "bodyText": "We do not need anything to handle here since exceptions are handled in other places (e.g. QueryFragmentExecutable.run()).\nEven JDK doesn't use any handler by default.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399203366", "createdAt": "2020-03-27T11:36:39Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentWorkerPool.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.sql.impl.QueryUtils;\n+\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.concurrent.ForkJoinWorkerThread;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_FRAGMENT;\n+\n+/**\n+ * Thread pool that executes query fragments.\n+ */\n+public class QueryFragmentWorkerPool {\n+\n+    private final ForkJoinPool pool;\n+\n+    public QueryFragmentWorkerPool(String instanceName, int threadCount) {\n+        pool = new ForkJoinPool(threadCount, new WorkerThreadFactory(instanceName), null, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTE3MzE0Nw=="}, "originalCommit": {"oid": "e1f504f79698e4cca5f07840c15055c176ba77a2"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIxNjU3MA==", "bodyText": "JDK doesn't handle uncaught exceptions because it doesn't know what to do with them, there is no good default strategy that works for everyone. To decide on the strategy they provide an extension point.\nQueryFragmentExecutable doesn't handle all exceptions, even basic failed assert is swallowed silently because it results in AssertionError, which is not a subclass of Exception. Besides that, the unschedule() call is not wrapped in try-catch. In any case, it's a very bad practice to leave uncaught exception handlers empty.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399216570", "createdAt": "2020-03-27T12:03:49Z", "author": {"login": "taburet"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentWorkerPool.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.sql.impl.QueryUtils;\n+\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.concurrent.ForkJoinWorkerThread;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_FRAGMENT;\n+\n+/**\n+ * Thread pool that executes query fragments.\n+ */\n+public class QueryFragmentWorkerPool {\n+\n+    private final ForkJoinPool pool;\n+\n+    public QueryFragmentWorkerPool(String instanceName, int threadCount) {\n+        pool = new ForkJoinPool(threadCount, new WorkerThreadFactory(instanceName), null, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTE3MzE0Nw=="}, "originalCommit": {"oid": "e1f504f79698e4cca5f07840c15055c176ba77a2"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTIyMTAzMA==", "bodyText": "Note these are not generic thread pools that execute arbitrary tasks. The semantics is fully under our control. Moreover, FJP re-creates failed threads, when something nasty happens. What kind of handling do you suggest to add here?\nI think that one thing that is definitely worth adding is \"panic\" handler, such as in com.hazelcast.spi.impl.operationexecutor.impl.OperationThread#run, but it should be installed on the thread level, not thread pool.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399221030", "createdAt": "2020-03-27T12:12:51Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentWorkerPool.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.sql.impl.QueryUtils;\n+\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.concurrent.ForkJoinWorkerThread;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_FRAGMENT;\n+\n+/**\n+ * Thread pool that executes query fragments.\n+ */\n+public class QueryFragmentWorkerPool {\n+\n+    private final ForkJoinPool pool;\n+\n+    public QueryFragmentWorkerPool(String instanceName, int threadCount) {\n+        pool = new ForkJoinPool(threadCount, new WorkerThreadFactory(instanceName), null, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTE3MzE0Nw=="}, "originalCommit": {"oid": "e1f504f79698e4cca5f07840c15055c176ba77a2"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTI0NDU0OA==", "bodyText": "Yes, it's under our control, but that doesn't mean we should treat all the code base as fully verified and 100% bug free. We can even invoke user code under the hood: every map scan potentially results in accessing user supplied classes, we read attribute values from them.\nThe usual practice is to at least log the errors you can't handle, looks like we can't do nothing more.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399244548", "createdAt": "2020-03-27T12:57:23Z", "author": {"login": "taburet"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentWorkerPool.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.sql.impl.QueryUtils;\n+\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.concurrent.ForkJoinWorkerThread;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_FRAGMENT;\n+\n+/**\n+ * Thread pool that executes query fragments.\n+ */\n+public class QueryFragmentWorkerPool {\n+\n+    private final ForkJoinPool pool;\n+\n+    public QueryFragmentWorkerPool(String instanceName, int threadCount) {\n+        pool = new ForkJoinPool(threadCount, new WorkerThreadFactory(instanceName), null, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTE3MzE0Nw=="}, "originalCommit": {"oid": "e1f504f79698e4cca5f07840c15055c176ba77a2"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTI1ODI1Mw==", "bodyText": "The asyncMode parameter is basically a switch between FIFO and LIFO modes, false = LIFO. It should not affect the execution, since we don't fork any tasks, but Executors.newWorkStealingPool sets the parameter to true, probably we should do the same.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399258253", "createdAt": "2020-03-27T13:20:45Z", "author": {"login": "taburet"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentWorkerPool.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.sql.impl.QueryUtils;\n+\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.concurrent.ForkJoinWorkerThread;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_FRAGMENT;\n+\n+/**\n+ * Thread pool that executes query fragments.\n+ */\n+public class QueryFragmentWorkerPool {\n+\n+    private final ForkJoinPool pool;\n+\n+    public QueryFragmentWorkerPool(String instanceName, int threadCount) {\n+        pool = new ForkJoinPool(threadCount, new WorkerThreadFactory(instanceName), null, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTE3MzE0Nw=="}, "originalCommit": {"oid": "e1f504f79698e4cca5f07840c15055c176ba77a2"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTI4NjcwOQ==", "bodyText": "Added exception handling to both pools. It follows the common approach we use in other pools - to log an error, and possibly handle OOME. It is far from ideal, but it is what it is.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399286709", "createdAt": "2020-03-27T14:03:15Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentWorkerPool.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.sql.impl.QueryUtils;\n+\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.concurrent.ForkJoinWorkerThread;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_FRAGMENT;\n+\n+/**\n+ * Thread pool that executes query fragments.\n+ */\n+public class QueryFragmentWorkerPool {\n+\n+    private final ForkJoinPool pool;\n+\n+    public QueryFragmentWorkerPool(String instanceName, int threadCount) {\n+        pool = new ForkJoinPool(threadCount, new WorkerThreadFactory(instanceName), null, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTE3MzE0Nw=="}, "originalCommit": {"oid": "e1f504f79698e4cca5f07840c15055c176ba77a2"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTI4NzQ1Mg==", "bodyText": "Regarding asyncMode - great catch, LIFO is dangerous for us because it may lead to starvation. Changed to true.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399287452", "createdAt": "2020-03-27T14:04:19Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentWorkerPool.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.sql.impl.QueryUtils;\n+\n+import java.util.concurrent.ForkJoinPool;\n+import java.util.concurrent.ForkJoinWorkerThread;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_FRAGMENT;\n+\n+/**\n+ * Thread pool that executes query fragments.\n+ */\n+public class QueryFragmentWorkerPool {\n+\n+    private final ForkJoinPool pool;\n+\n+    public QueryFragmentWorkerPool(String instanceName, int threadCount) {\n+        pool = new ForkJoinPool(threadCount, new WorkerThreadFactory(instanceName), null, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTE3MzE0Nw=="}, "originalCommit": {"oid": "e1f504f79698e4cca5f07840c15055c176ba77a2"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3NjI1NzMyOnYy", "diffSide": "RIGHT", "path": "docs/design/sql/04-mulitthreaded-execution.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QxNzowNzowM1rOF86VGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOFQxMTo0NToyMVrOF9I4wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQxNDU1Mg==", "bodyText": "should BE ordered?", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399414552", "createdAt": "2020-03-27T17:07:03Z", "author": {"login": "petrpleshachkov"}, "path": "docs/design/sql/04-mulitthreaded-execution.md", "diffHunk": "@@ -0,0 +1,215 @@\n+# SQL Multithreaded Execution\n+\n+## Overview\n+\n+The Hazelcast Mustang engine executes queries in parallel. This document describes the design of the multithreaded execution\n+environment of the engine.\n+\n+The document doesn't discuss operator-level parallelism, which is a different topic.\n+\n+The rest of this document is organized as follows. In section 1 we discuss the existing threading infrastructure of Hazelcast\n+IMDG and Hazelcast Jet. In section 2 we analyze why the existing infrastructure is inappropriate for query execution and then\n+describe the design of the Hazelcast Mustang execution environment. In section 3 we discuss possible alternative approaches\n+that were rejected.\n+\n+## 1 Existing Infrastructure\n+\n+Hazelcast IMDG uses staged event-driven architecture (SEDA) for message processing. During execution, a message passes through\n+several thread pools (stages), each optimized for a specific type of workload. We now describe stages that exist in Hazelcast.\n+\n+### 1.1 IO Pool\n+\n+Hazelcast uses a dedicated thread pool for message send and receive, which will be referred to as **IO Pool** in this paper.\n+Each thread from the IO pool maintains a subset of connections to remote members. Consider that we have a sender member (S)\n+and a receiver member (R). The typical execution flow is organized as follows:\n+1. The message is added to the queue of a single IO thread, and the thread is notified\n+1. The sender IO thread wakes up and sends the message over the network\n+1. A receiver IO thread is notified by the operating system on receive\n+1. The receiver IO thread wakes up, determines the next execution stage, adds the message to the stage's queue and notifies the\n+stage\n+1. The next execution stage processes the message\n+\n+*Snippet 1: Message execution flow*\n+```\n+Stage(S)                 IO(S)        IO(R)                Stage(R)\n+   |----enqueue/notify->--|            |                      |\n+   |                      |----send->--|                      |\n+   |                      |            |----enqueue/notify->--|\n+```\n+\n+We now discuss the organization of different execution stages.\n+\n+### 1.2 Partition Pool\n+\n+A message may have a logical **partition**, which is a positive integer number. Messages with defined partition are routed to\n+a special thread pool, which we refer to as **partition pool**. The pool has several threads. Every thread has a dedicated task\n+queue. Partition of the message is used to determine the exact thread which will process the message:\n+`threadIndex = partition % threadCount`.\n+\n+The partition pool has the following advantages:\n+1. Only one thread processes messages with the given partition so that processing logic may use less synchronization\n+1. Dedicated thread queues reduce contention on enqueue/deque operations\n+\n+The partition pool has the following disadvantage:\n+1. There is no balancing between threads: a single long-running task may delay other tasks from the same partition\n+indefinitely; likewise, an imbalance between partitions may cause resource underutilization\n+\n+The partition pool is thus most suitable for small tasks that operate on independent physical resources, and that are\n+distributed equally between logical partitions. An example is `IMap` operations, which update separate physical partitions,\n+such as `GET` and `PUT`.\n+\n+Since the partition is a logical notion, it is possible to multiplex tasks from different components to a single partition pool.\n+For example, CP Subsystem schedules tasks, all with the same partition, to the partition pool to ensure total processing order.\n+\n+### 1.3 Generic Pool\n+\n+If a message doesn't have a logical partition, it is submitted to the **generic pool**. This is a conventional thread pool with\n+a shared blocking queue. It has inherent balancing capabilities. But at the same time, this pool may demonstrate less than\n+optimal throughput when a lot of small tasks are submitted due to contention on the queue.\n+\n+### 1.4 Hazelcast Jet Pool\n+\n+Hazelcast Jet uses its own cooperative pool to execute Jet jobs. Every thread has its own queue of jobs that are executed\n+cooperatively. There is no balancing: once the job is submitted to a specific thread, it is always executed in that thread.\n+\n+IO pool doesn't notify the Jet pool about new data batch (\"push\"). Instead, the message is just enqueued, and the Jet thread\n+checks the queue periodically (\"poll\").\n+\n+## 2 Design\n+\n+We now define the requirements to Hazelcast Mustang threading model, analyze them concerning existing infrastructure, and\n+define the design.\n+\n+### 2.1 Requirements\n+\n+The requirements are thread safety, load balancing, and ordered message processing.\n+\n+First, the infrastructure must guarantee that operator execution is thread-safe. That is, the stateful operator should not be\n+executed by multiple threads simultaneously. This simplifies operator implementations and makes them more performant.\n+Hazelcast Jet follows this principle, as only one thread may execute a particular job. However, Hazelcast Jet pool doesn't\n+satisfy the load balancing requirement discussed below.\n+\n+Second, the execution environment must support load balancing. Query execution may take a long time to complete. If several query\n+fragments have been assigned to a single execution thread, it should be possible to reassign them to available free thread\n+dynamically. Hence neither partition pool nor Hazelcast Jet pool designs are applicable to Hazelcast Mustang because they lack\n+balancing capabilities.\n+\n+Third, it should be possible to execute some tasks in order. That is, if task `A` is received before task `B`, then it\n+should be executed before `B`. It is always possible to implement an ordering guarantee with the help of additional\n+synchronization primitives, but it increases complexity and typically reduces performance. So we prefer to have a threading\n+infrastructure with ordering guarantees, such as in the partition pool. Examples of tasks requiring ordered processing:\n+1. Query cancel should be executed after query start to minimize resource leaks\n+1. The N-th batch from the stream should be processed before the (N+1)-th batch, as described in [[1]] (p. 1.3)\n+\n+### 2.1 General Design\n+\n+We define the taxonomy of tasks related to query execution.\n+\n+First, the engine must execute query fragments, i.e., advance Volcano-style operators, as explained in [[2]] (p. 3). Fragment\n+execution is initiated in response to query start or data messages. Fragment execution may take significant time to complete.\n+\n+Second, the engine must process query operations described in [[1]] (p. 3), such as query start, query cancel, batch arrival,\n+and flow control. These operations take short time to complete. Moreover, query start operation may trigger the parallel\n+execution of several fragments.\n+\n+Given the different nature of query operations and fragment execution, we split them into independent stages, called\n+**operation pool** and **fragment pool**. The former executes query operations, and the latter executes fragments. This design\n+provides a clear separation of concerns and allows us to optimize stages for their tasks as described below, which improves\n+performance. On the other hand, this design introduces an additional thread notification, as shown in the snippet below, which\n+may negatively affect performance. Nevertheless, we think that the advantages of this approach outweigh the disadvantages.\n+\n+*Snippet 2: Query start flow (receiver only)*\n+```\n+IO               Operation pool         Fragment pool\n+|----enqueue/notify-->-|                      |\n+|                      |----enqueue/notify-->-|\n+```\n+\n+### 2.2 Operation Pool\n+\n+Every network message received by the IO pool is first submitted to the operation pool.\n+\n+The pool is organized as a fixed pool of threads with dedicated per-thread queues. Dedicated queues reduce contention and\n+increase the throughput, which is important given that processing of a single message takes little time.\n+\n+Every message may define an optional logical partition. If the partition is not defined, a random thread is picked for message\n+processing. If the partition is defined, the index of executing thread is defined as `threadIndex = partition % threadCount`.\n+Two messages with the same partition are guaranteed to be processed by the same thread, thus providing ordering guarantees.\n+\n+Some messages may trigger query fragment execution, as described below. In this case, a fragment execution task is created and\n+submitted for execution to the fragment pool.\n+1. `execute` - triggers the execution of one or more fragments\n+1. `batch` and `flow_control` - trigger execution of one fragment defined by query ID and edge ID, as described in [[1]]\n+\n+### 2.3 Fragment Pool\n+\n+The fragment pool is responsible for the execution of individual query fragments. Fragment execution is always initiated by\n+a task from the operation pool.\n+\n+Unlike operations, fragment execution may take arbitrary time depending on the query structure. It is, therefore, important to\n+guarantee high throughput for short fragments, while still providing load balancing for long fragments. The ideal candidate\n+is a thread pool with dedicated per-thread queues and work-stealing. For this reason, we choose JDK's `ForkJoinPool` as a\n+backbone of the fragment pool. We create a separate instance of the `ForkJoinPool` rather than using `ForkJoinPool.commonPool()`\n+to avoid interference with user workloads.\n+\n+We may decide to introduce multiple fragment pools for better resource management in the future. This will help limit the\n+maximum number of CPU cores dedicated to a particular workload. An example of this approach is **Resource Pools** in MemSQL [[3]].\n+\n+## 3 Alternative Approaches\n+\n+Several other approaches were considered but then rejected. This section explains these approaches and the rejection\n+reasons.\n+\n+### 3.1 Use Single Pool\n+\n+It is possible to use a single pool for both operation and fragment processing and remove the additional notifications for\n+fragment execution. The problem is that an operation task is short and requires fast response, while fragment execution is\n+potentially a long-running task. Consider two examples that demonstrate when this approach doesn't work well.\n+\n+**Example 1**:\n+\n+Consider that long-running fragments occupied all threads in the pool. Then a `cancel` message arrives, but it cannot be\n+processed because all threads are busy. To fix it, we would have to introduce a separate priority queue, which should be\n+checked periodically by fragment tasks. This may lead to wasted CPU cycles and frequent interrupts to fragment tasks.\n+\n+**Example 2**\n+\n+Consider a fragment which occupied the thread `A`. Then a `batch` message arrives, which should resume execution of another\n+fragment. Given that the processing of `batch` message should is ordered, it has logical partition defined. It may happen that", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fb09342b6e66b06d587040b4b8d137d0a11764c"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY1MzA1OA==", "bodyText": "Fixed.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399653058", "createdAt": "2020-03-28T11:45:21Z", "author": {"login": "devozerov"}, "path": "docs/design/sql/04-mulitthreaded-execution.md", "diffHunk": "@@ -0,0 +1,215 @@\n+# SQL Multithreaded Execution\n+\n+## Overview\n+\n+The Hazelcast Mustang engine executes queries in parallel. This document describes the design of the multithreaded execution\n+environment of the engine.\n+\n+The document doesn't discuss operator-level parallelism, which is a different topic.\n+\n+The rest of this document is organized as follows. In section 1 we discuss the existing threading infrastructure of Hazelcast\n+IMDG and Hazelcast Jet. In section 2 we analyze why the existing infrastructure is inappropriate for query execution and then\n+describe the design of the Hazelcast Mustang execution environment. In section 3 we discuss possible alternative approaches\n+that were rejected.\n+\n+## 1 Existing Infrastructure\n+\n+Hazelcast IMDG uses staged event-driven architecture (SEDA) for message processing. During execution, a message passes through\n+several thread pools (stages), each optimized for a specific type of workload. We now describe stages that exist in Hazelcast.\n+\n+### 1.1 IO Pool\n+\n+Hazelcast uses a dedicated thread pool for message send and receive, which will be referred to as **IO Pool** in this paper.\n+Each thread from the IO pool maintains a subset of connections to remote members. Consider that we have a sender member (S)\n+and a receiver member (R). The typical execution flow is organized as follows:\n+1. The message is added to the queue of a single IO thread, and the thread is notified\n+1. The sender IO thread wakes up and sends the message over the network\n+1. A receiver IO thread is notified by the operating system on receive\n+1. The receiver IO thread wakes up, determines the next execution stage, adds the message to the stage's queue and notifies the\n+stage\n+1. The next execution stage processes the message\n+\n+*Snippet 1: Message execution flow*\n+```\n+Stage(S)                 IO(S)        IO(R)                Stage(R)\n+   |----enqueue/notify->--|            |                      |\n+   |                      |----send->--|                      |\n+   |                      |            |----enqueue/notify->--|\n+```\n+\n+We now discuss the organization of different execution stages.\n+\n+### 1.2 Partition Pool\n+\n+A message may have a logical **partition**, which is a positive integer number. Messages with defined partition are routed to\n+a special thread pool, which we refer to as **partition pool**. The pool has several threads. Every thread has a dedicated task\n+queue. Partition of the message is used to determine the exact thread which will process the message:\n+`threadIndex = partition % threadCount`.\n+\n+The partition pool has the following advantages:\n+1. Only one thread processes messages with the given partition so that processing logic may use less synchronization\n+1. Dedicated thread queues reduce contention on enqueue/deque operations\n+\n+The partition pool has the following disadvantage:\n+1. There is no balancing between threads: a single long-running task may delay other tasks from the same partition\n+indefinitely; likewise, an imbalance between partitions may cause resource underutilization\n+\n+The partition pool is thus most suitable for small tasks that operate on independent physical resources, and that are\n+distributed equally between logical partitions. An example is `IMap` operations, which update separate physical partitions,\n+such as `GET` and `PUT`.\n+\n+Since the partition is a logical notion, it is possible to multiplex tasks from different components to a single partition pool.\n+For example, CP Subsystem schedules tasks, all with the same partition, to the partition pool to ensure total processing order.\n+\n+### 1.3 Generic Pool\n+\n+If a message doesn't have a logical partition, it is submitted to the **generic pool**. This is a conventional thread pool with\n+a shared blocking queue. It has inherent balancing capabilities. But at the same time, this pool may demonstrate less than\n+optimal throughput when a lot of small tasks are submitted due to contention on the queue.\n+\n+### 1.4 Hazelcast Jet Pool\n+\n+Hazelcast Jet uses its own cooperative pool to execute Jet jobs. Every thread has its own queue of jobs that are executed\n+cooperatively. There is no balancing: once the job is submitted to a specific thread, it is always executed in that thread.\n+\n+IO pool doesn't notify the Jet pool about new data batch (\"push\"). Instead, the message is just enqueued, and the Jet thread\n+checks the queue periodically (\"poll\").\n+\n+## 2 Design\n+\n+We now define the requirements to Hazelcast Mustang threading model, analyze them concerning existing infrastructure, and\n+define the design.\n+\n+### 2.1 Requirements\n+\n+The requirements are thread safety, load balancing, and ordered message processing.\n+\n+First, the infrastructure must guarantee that operator execution is thread-safe. That is, the stateful operator should not be\n+executed by multiple threads simultaneously. This simplifies operator implementations and makes them more performant.\n+Hazelcast Jet follows this principle, as only one thread may execute a particular job. However, Hazelcast Jet pool doesn't\n+satisfy the load balancing requirement discussed below.\n+\n+Second, the execution environment must support load balancing. Query execution may take a long time to complete. If several query\n+fragments have been assigned to a single execution thread, it should be possible to reassign them to available free thread\n+dynamically. Hence neither partition pool nor Hazelcast Jet pool designs are applicable to Hazelcast Mustang because they lack\n+balancing capabilities.\n+\n+Third, it should be possible to execute some tasks in order. That is, if task `A` is received before task `B`, then it\n+should be executed before `B`. It is always possible to implement an ordering guarantee with the help of additional\n+synchronization primitives, but it increases complexity and typically reduces performance. So we prefer to have a threading\n+infrastructure with ordering guarantees, such as in the partition pool. Examples of tasks requiring ordered processing:\n+1. Query cancel should be executed after query start to minimize resource leaks\n+1. The N-th batch from the stream should be processed before the (N+1)-th batch, as described in [[1]] (p. 1.3)\n+\n+### 2.1 General Design\n+\n+We define the taxonomy of tasks related to query execution.\n+\n+First, the engine must execute query fragments, i.e., advance Volcano-style operators, as explained in [[2]] (p. 3). Fragment\n+execution is initiated in response to query start or data messages. Fragment execution may take significant time to complete.\n+\n+Second, the engine must process query operations described in [[1]] (p. 3), such as query start, query cancel, batch arrival,\n+and flow control. These operations take short time to complete. Moreover, query start operation may trigger the parallel\n+execution of several fragments.\n+\n+Given the different nature of query operations and fragment execution, we split them into independent stages, called\n+**operation pool** and **fragment pool**. The former executes query operations, and the latter executes fragments. This design\n+provides a clear separation of concerns and allows us to optimize stages for their tasks as described below, which improves\n+performance. On the other hand, this design introduces an additional thread notification, as shown in the snippet below, which\n+may negatively affect performance. Nevertheless, we think that the advantages of this approach outweigh the disadvantages.\n+\n+*Snippet 2: Query start flow (receiver only)*\n+```\n+IO               Operation pool         Fragment pool\n+|----enqueue/notify-->-|                      |\n+|                      |----enqueue/notify-->-|\n+```\n+\n+### 2.2 Operation Pool\n+\n+Every network message received by the IO pool is first submitted to the operation pool.\n+\n+The pool is organized as a fixed pool of threads with dedicated per-thread queues. Dedicated queues reduce contention and\n+increase the throughput, which is important given that processing of a single message takes little time.\n+\n+Every message may define an optional logical partition. If the partition is not defined, a random thread is picked for message\n+processing. If the partition is defined, the index of executing thread is defined as `threadIndex = partition % threadCount`.\n+Two messages with the same partition are guaranteed to be processed by the same thread, thus providing ordering guarantees.\n+\n+Some messages may trigger query fragment execution, as described below. In this case, a fragment execution task is created and\n+submitted for execution to the fragment pool.\n+1. `execute` - triggers the execution of one or more fragments\n+1. `batch` and `flow_control` - trigger execution of one fragment defined by query ID and edge ID, as described in [[1]]\n+\n+### 2.3 Fragment Pool\n+\n+The fragment pool is responsible for the execution of individual query fragments. Fragment execution is always initiated by\n+a task from the operation pool.\n+\n+Unlike operations, fragment execution may take arbitrary time depending on the query structure. It is, therefore, important to\n+guarantee high throughput for short fragments, while still providing load balancing for long fragments. The ideal candidate\n+is a thread pool with dedicated per-thread queues and work-stealing. For this reason, we choose JDK's `ForkJoinPool` as a\n+backbone of the fragment pool. We create a separate instance of the `ForkJoinPool` rather than using `ForkJoinPool.commonPool()`\n+to avoid interference with user workloads.\n+\n+We may decide to introduce multiple fragment pools for better resource management in the future. This will help limit the\n+maximum number of CPU cores dedicated to a particular workload. An example of this approach is **Resource Pools** in MemSQL [[3]].\n+\n+## 3 Alternative Approaches\n+\n+Several other approaches were considered but then rejected. This section explains these approaches and the rejection\n+reasons.\n+\n+### 3.1 Use Single Pool\n+\n+It is possible to use a single pool for both operation and fragment processing and remove the additional notifications for\n+fragment execution. The problem is that an operation task is short and requires fast response, while fragment execution is\n+potentially a long-running task. Consider two examples that demonstrate when this approach doesn't work well.\n+\n+**Example 1**:\n+\n+Consider that long-running fragments occupied all threads in the pool. Then a `cancel` message arrives, but it cannot be\n+processed because all threads are busy. To fix it, we would have to introduce a separate priority queue, which should be\n+checked periodically by fragment tasks. This may lead to wasted CPU cycles and frequent interrupts to fragment tasks.\n+\n+**Example 2**\n+\n+Consider a fragment which occupied the thread `A`. Then a `batch` message arrives, which should resume execution of another\n+fragment. Given that the processing of `batch` message should is ordered, it has logical partition defined. It may happen that", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQxNDU1Mg=="}, "originalCommit": {"oid": "8fb09342b6e66b06d587040b4b8d137d0a11764c"}, "originalPosition": 178}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3NjI3ODk5OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/mailbox/InboundBatch.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QxNzoxMzowMVrOF86iyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOFQxMTo0NToxNlrOF9I4tQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQxODA1OQ==", "bodyText": "Minor: make it final?", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399418059", "createdAt": "2020-03-27T17:13:01Z", "author": {"login": "petrpleshachkov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/mailbox/InboundBatch.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.mailbox;\n+\n+import com.hazelcast.sql.impl.row.RowBatch;\n+\n+import java.util.UUID;\n+\n+/**\n+ * Mailbox batch received from the remote member.\n+ */\n+public class InboundBatch {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fb09342b6e66b06d587040b4b8d137d0a11764c"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY1MzA0NQ==", "bodyText": "Makes sense, fixed.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399653045", "createdAt": "2020-03-28T11:45:16Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/mailbox/InboundBatch.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.mailbox;\n+\n+import com.hazelcast.sql.impl.row.RowBatch;\n+\n+import java.util.UUID;\n+\n+/**\n+ * Mailbox batch received from the remote member.\n+ */\n+public class InboundBatch {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQxODA1OQ=="}, "originalCommit": {"oid": "8fb09342b6e66b06d587040b4b8d137d0a11764c"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3NjI4NTQxOnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/mailbox/OutboundHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QxNzoxNDozMlrOF86miQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOFQxMTo0NTowOVrOF9I4rg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQxOTAxNw==", "bodyText": "Minor: which is available...", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399419017", "createdAt": "2020-03-27T17:14:32Z", "author": {"login": "petrpleshachkov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/mailbox/OutboundHandler.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.mailbox;\n+\n+/**\n+ * Core interface for outbound message processing.\n+ */\n+public interface OutboundHandler {\n+    /**\n+     * Handle flow control response from the remote inbound handler.\n+     *\n+     * @param remainingMemory Amount of memory which available on the remote end.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fb09342b6e66b06d587040b4b8d137d0a11764c"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY1MzAzOA==", "bodyText": "Fixed.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399653038", "createdAt": "2020-03-28T11:45:09Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/mailbox/OutboundHandler.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.mailbox;\n+\n+/**\n+ * Core interface for outbound message processing.\n+ */\n+public interface OutboundHandler {\n+    /**\n+     * Handle flow control response from the remote inbound handler.\n+     *\n+     * @param remainingMemory Amount of memory which available on the remote end.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQxOTAxNw=="}, "originalCommit": {"oid": "8fb09342b6e66b06d587040b4b8d137d0a11764c"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3NjI5MTIxOnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/operation/QueryOperationHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QxNzoxNjoxMVrOF86qaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOFQxMTo0NTowM1rOF9I4ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQyMDAwOA==", "bodyText": "Minor: True-> true", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399420008", "createdAt": "2020-03-27T17:16:11Z", "author": {"login": "petrpleshachkov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/operation/QueryOperationHandler.java", "diffHunk": "@@ -0,0 +1,40 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.operation;\n+\n+import java.util.UUID;\n+\n+/**\n+ * Query operation executor.\n+ */\n+public interface QueryOperationHandler {\n+    /**\n+     * Submit operation for execution on a member.\n+     *\n+     * @param memberId ID of the member.\n+     * @param operation Operation to be executed.\n+     * @return {@code True} if operation is triggered, {@code false} if target member is not available.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fb09342b6e66b06d587040b4b8d137d0a11764c"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY1MzAyMg==", "bodyText": "Fixed.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399653022", "createdAt": "2020-03-28T11:45:03Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/operation/QueryOperationHandler.java", "diffHunk": "@@ -0,0 +1,40 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.operation;\n+\n+import java.util.UUID;\n+\n+/**\n+ * Query operation executor.\n+ */\n+public interface QueryOperationHandler {\n+    /**\n+     * Submit operation for execution on a member.\n+     *\n+     * @param memberId ID of the member.\n+     * @param operation Operation to be executed.\n+     * @return {@code True} if operation is triggered, {@code false} if target member is not available.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQyMDAwOA=="}, "originalCommit": {"oid": "8fb09342b6e66b06d587040b4b8d137d0a11764c"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3NjI5NjU3OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/state/QueryStateCallback.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QxNzoxNzo0NVrOF86t7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOFQxMTo0NDo1OVrOF9I4mQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQyMDkxMQ==", "bodyText": "Minor: and-> an?", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399420911", "createdAt": "2020-03-27T17:17:45Z", "author": {"login": "petrpleshachkov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/state/QueryStateCallback.java", "diffHunk": "@@ -0,0 +1,40 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.state;\n+\n+/**\n+ * Callback to perform various actions on the query state.\n+ */\n+public interface QueryStateCallback {\n+    /**\n+     * Notify the query that fragment execution has finished.\n+     */\n+    void onFragmentFinished();\n+\n+    /**\n+     * Cancel the query with error.\n+     *\n+     * @param e Error.\n+     */\n+    void cancel(Exception e);\n+\n+    /**\n+     * Check whether the query is cancelled. If the query is not cancelled, the method returns with no side effects.\n+     * Otherwise and exception is thrown.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fb09342b6e66b06d587040b4b8d137d0a11764c"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY1MzAxNw==", "bodyText": "Fixed.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399653017", "createdAt": "2020-03-28T11:44:59Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/state/QueryStateCallback.java", "diffHunk": "@@ -0,0 +1,40 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.state;\n+\n+/**\n+ * Callback to perform various actions on the query state.\n+ */\n+public interface QueryStateCallback {\n+    /**\n+     * Notify the query that fragment execution has finished.\n+     */\n+    void onFragmentFinished();\n+\n+    /**\n+     * Cancel the query with error.\n+     *\n+     * @param e Error.\n+     */\n+    void cancel(Exception e);\n+\n+    /**\n+     * Check whether the query is cancelled. If the query is not cancelled, the method returns with no side effects.\n+     * Otherwise and exception is thrown.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQyMDkxMQ=="}, "originalCommit": {"oid": "8fb09342b6e66b06d587040b4b8d137d0a11764c"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3NjM0MDkxOnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentExecutable.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QxNzoyOTo1NFrOF87KWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOFQxMToyMDoxOFrOF9IwDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQyODE4Ng==", "bodyText": "Do we have to send flow control if res is WAIT?", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399428186", "createdAt": "2020-03-27T17:29:54Z", "author": {"login": "petrpleshachkov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentExecutable.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.sql.impl.exec.Exec;\n+import com.hazelcast.sql.impl.exec.IterationResult;\n+import com.hazelcast.sql.impl.fragment.QueryFragmentContext;\n+import com.hazelcast.sql.impl.mailbox.InboundHandler;\n+import com.hazelcast.sql.impl.mailbox.InboundBatch;\n+import com.hazelcast.sql.impl.mailbox.OutboundHandler;\n+import com.hazelcast.sql.impl.operation.QueryBatchExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryAbstractExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryFlowControlExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+import com.hazelcast.sql.impl.state.QueryStateCallback;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentLinkedDeque;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Query fragment executable that advances the top-level operator, consumes data operations, and manages scheduling.\n+ */\n+public class QueryFragmentExecutable implements QueryFragmentScheduleCallback {\n+\n+    private final QueryStateCallback stateCallback;\n+    private final List<Object> arguments;\n+    private final Exec exec;\n+    private final Map<Integer, InboundHandler> inboxes;\n+    private final Map<Integer, Map<UUID, OutboundHandler>> outboxes;\n+    private final QueryFragmentWorkerPool fragmentPool;\n+\n+    /** Operations to be processed. */\n+    private final ConcurrentLinkedDeque<QueryAbstractExchangeOperation> operations = new ConcurrentLinkedDeque<>();\n+\n+    /** Batch count which we used instead of batches.size() (which is O(N)) to prevent starvation. */\n+    private final AtomicInteger operationCount = new AtomicInteger();\n+\n+    /** Schedule flag. */\n+    private final AtomicBoolean scheduled = new AtomicBoolean();\n+\n+    /** Whether the fragment is initialized. */\n+    private volatile boolean initialized;\n+\n+    /** Whether the fragment has completed. */\n+    private volatile boolean completed;\n+\n+    public QueryFragmentExecutable(\n+        QueryStateCallback stateCallback,\n+        List<Object> arguments,\n+        Exec exec,\n+        Map<Integer, InboundHandler> inboxes,\n+        Map<Integer, Map<UUID, OutboundHandler>> outboxes,\n+        QueryFragmentWorkerPool fragmentPool\n+    ) {\n+        this.stateCallback = stateCallback;\n+        this.arguments = arguments;\n+        this.exec = exec;\n+        this.inboxes = inboxes;\n+        this.outboxes = outboxes;\n+        this.fragmentPool = fragmentPool;\n+    }\n+\n+    public Collection<Integer> getInboxEdgeIds() {\n+        return inboxes.keySet();\n+    }\n+\n+    public Collection<Integer> getOutboxEdgeIds() {\n+        return outboxes.keySet();\n+    }\n+\n+    /**\n+     * Add operation to be processed.\n+     */\n+    public void addOperation(QueryAbstractExchangeOperation operation) {\n+        operationCount.incrementAndGet();\n+        operations.addLast(operation);\n+    }\n+\n+    public void run() {\n+        try {\n+            if (completed) {\n+                return;\n+            }\n+\n+            // Setup the executor if needed.\n+            setupExecutor();\n+\n+            // Feed all batches to relevant inboxes first. Set the upper boundary on the number of batches to avoid\n+            // starvation when batches arrive quicker than we are able to process them.\n+            int maxOperationCount = operationCount.get();\n+            int processedBatchCount = 0;\n+\n+            QueryOperation operation;\n+\n+            while ((operation = operations.pollFirst()) != null) {\n+                if (operation instanceof QueryBatchExchangeOperation) {\n+                    QueryBatchExchangeOperation operation0 = (QueryBatchExchangeOperation) operation;\n+\n+                    InboundHandler inbox = inboxes.get(operation0.getEdgeId());\n+                    assert inbox != null;\n+\n+                    InboundBatch batch = new InboundBatch(\n+                        operation0.getBatch(),\n+                        operation0.isLast(),\n+                        operation0.getCallerId()\n+                    );\n+\n+                    inbox.onBatch(batch, operation0.getRemainingMemory());\n+                } else {\n+                    assert operation instanceof QueryFlowControlExchangeOperation;\n+\n+                    QueryFlowControlExchangeOperation operation0 = (QueryFlowControlExchangeOperation) operation;\n+\n+                    Map<UUID, OutboundHandler> edgeOutboxes = outboxes.get(operation0.getEdgeId());\n+                    assert edgeOutboxes != null;\n+\n+                    OutboundHandler outbox = edgeOutboxes.get(operation.getCallerId());\n+                    assert outbox != null;\n+\n+                    outbox.onFlowControl(operation0.getRemainingMemory());\n+                }\n+\n+                if (++processedBatchCount >= maxOperationCount) {\n+                    break;\n+                }\n+            }\n+\n+            operationCount.addAndGet(-1 * processedBatchCount);\n+\n+            IterationResult res = exec.advance();\n+\n+            // Send flow control messages if needed.\n+            if (res != IterationResult.FETCHED_DONE) {\n+                for (InboundHandler inbox : inboxes.values()) {\n+                    inbox.sendFlowControl();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c758ffc74823617bad0ce51e6838d2c4fe57ed0c"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY1MDgzMA==", "bodyText": "Yes. E.g. if we have a sort operator which is fetching a stream of rows from the remote operator, then we will return WAIT for all batches except for the last one. Nevertheless, flow control should work during this process as usual.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399650830", "createdAt": "2020-03-28T11:20:18Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentExecutable.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.sql.impl.exec.Exec;\n+import com.hazelcast.sql.impl.exec.IterationResult;\n+import com.hazelcast.sql.impl.fragment.QueryFragmentContext;\n+import com.hazelcast.sql.impl.mailbox.InboundHandler;\n+import com.hazelcast.sql.impl.mailbox.InboundBatch;\n+import com.hazelcast.sql.impl.mailbox.OutboundHandler;\n+import com.hazelcast.sql.impl.operation.QueryBatchExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryAbstractExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryFlowControlExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+import com.hazelcast.sql.impl.state.QueryStateCallback;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentLinkedDeque;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Query fragment executable that advances the top-level operator, consumes data operations, and manages scheduling.\n+ */\n+public class QueryFragmentExecutable implements QueryFragmentScheduleCallback {\n+\n+    private final QueryStateCallback stateCallback;\n+    private final List<Object> arguments;\n+    private final Exec exec;\n+    private final Map<Integer, InboundHandler> inboxes;\n+    private final Map<Integer, Map<UUID, OutboundHandler>> outboxes;\n+    private final QueryFragmentWorkerPool fragmentPool;\n+\n+    /** Operations to be processed. */\n+    private final ConcurrentLinkedDeque<QueryAbstractExchangeOperation> operations = new ConcurrentLinkedDeque<>();\n+\n+    /** Batch count which we used instead of batches.size() (which is O(N)) to prevent starvation. */\n+    private final AtomicInteger operationCount = new AtomicInteger();\n+\n+    /** Schedule flag. */\n+    private final AtomicBoolean scheduled = new AtomicBoolean();\n+\n+    /** Whether the fragment is initialized. */\n+    private volatile boolean initialized;\n+\n+    /** Whether the fragment has completed. */\n+    private volatile boolean completed;\n+\n+    public QueryFragmentExecutable(\n+        QueryStateCallback stateCallback,\n+        List<Object> arguments,\n+        Exec exec,\n+        Map<Integer, InboundHandler> inboxes,\n+        Map<Integer, Map<UUID, OutboundHandler>> outboxes,\n+        QueryFragmentWorkerPool fragmentPool\n+    ) {\n+        this.stateCallback = stateCallback;\n+        this.arguments = arguments;\n+        this.exec = exec;\n+        this.inboxes = inboxes;\n+        this.outboxes = outboxes;\n+        this.fragmentPool = fragmentPool;\n+    }\n+\n+    public Collection<Integer> getInboxEdgeIds() {\n+        return inboxes.keySet();\n+    }\n+\n+    public Collection<Integer> getOutboxEdgeIds() {\n+        return outboxes.keySet();\n+    }\n+\n+    /**\n+     * Add operation to be processed.\n+     */\n+    public void addOperation(QueryAbstractExchangeOperation operation) {\n+        operationCount.incrementAndGet();\n+        operations.addLast(operation);\n+    }\n+\n+    public void run() {\n+        try {\n+            if (completed) {\n+                return;\n+            }\n+\n+            // Setup the executor if needed.\n+            setupExecutor();\n+\n+            // Feed all batches to relevant inboxes first. Set the upper boundary on the number of batches to avoid\n+            // starvation when batches arrive quicker than we are able to process them.\n+            int maxOperationCount = operationCount.get();\n+            int processedBatchCount = 0;\n+\n+            QueryOperation operation;\n+\n+            while ((operation = operations.pollFirst()) != null) {\n+                if (operation instanceof QueryBatchExchangeOperation) {\n+                    QueryBatchExchangeOperation operation0 = (QueryBatchExchangeOperation) operation;\n+\n+                    InboundHandler inbox = inboxes.get(operation0.getEdgeId());\n+                    assert inbox != null;\n+\n+                    InboundBatch batch = new InboundBatch(\n+                        operation0.getBatch(),\n+                        operation0.isLast(),\n+                        operation0.getCallerId()\n+                    );\n+\n+                    inbox.onBatch(batch, operation0.getRemainingMemory());\n+                } else {\n+                    assert operation instanceof QueryFlowControlExchangeOperation;\n+\n+                    QueryFlowControlExchangeOperation operation0 = (QueryFlowControlExchangeOperation) operation;\n+\n+                    Map<UUID, OutboundHandler> edgeOutboxes = outboxes.get(operation0.getEdgeId());\n+                    assert edgeOutboxes != null;\n+\n+                    OutboundHandler outbox = edgeOutboxes.get(operation.getCallerId());\n+                    assert outbox != null;\n+\n+                    outbox.onFlowControl(operation0.getRemainingMemory());\n+                }\n+\n+                if (++processedBatchCount >= maxOperationCount) {\n+                    break;\n+                }\n+            }\n+\n+            operationCount.addAndGet(-1 * processedBatchCount);\n+\n+            IterationResult res = exec.advance();\n+\n+            // Send flow control messages if needed.\n+            if (res != IterationResult.FETCHED_DONE) {\n+                for (InboundHandler inbox : inboxes.values()) {\n+                    inbox.sendFlowControl();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQyODE4Ng=="}, "originalCommit": {"oid": "c758ffc74823617bad0ce51e6838d2c4fe57ed0c"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3NjM2MjM1OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentExecutable.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QxNzozNTozM1rOF87X2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOFQxMTo0NDo0MlrOF9I4dQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQzMTY0Mg==", "bodyText": "It seems this check is not necessary because it is covered by the scheduled.compareAndSet logic.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399431642", "createdAt": "2020-03-27T17:35:33Z", "author": {"login": "petrpleshachkov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentExecutable.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.sql.impl.exec.Exec;\n+import com.hazelcast.sql.impl.exec.IterationResult;\n+import com.hazelcast.sql.impl.fragment.QueryFragmentContext;\n+import com.hazelcast.sql.impl.mailbox.InboundHandler;\n+import com.hazelcast.sql.impl.mailbox.InboundBatch;\n+import com.hazelcast.sql.impl.mailbox.OutboundHandler;\n+import com.hazelcast.sql.impl.operation.QueryBatchExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryAbstractExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryFlowControlExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+import com.hazelcast.sql.impl.state.QueryStateCallback;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentLinkedDeque;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Query fragment executable that advances the top-level operator, consumes data operations, and manages scheduling.\n+ */\n+public class QueryFragmentExecutable implements QueryFragmentScheduleCallback {\n+\n+    private final QueryStateCallback stateCallback;\n+    private final List<Object> arguments;\n+    private final Exec exec;\n+    private final Map<Integer, InboundHandler> inboxes;\n+    private final Map<Integer, Map<UUID, OutboundHandler>> outboxes;\n+    private final QueryFragmentWorkerPool fragmentPool;\n+\n+    /** Operations to be processed. */\n+    private final ConcurrentLinkedDeque<QueryAbstractExchangeOperation> operations = new ConcurrentLinkedDeque<>();\n+\n+    /** Batch count which we used instead of batches.size() (which is O(N)) to prevent starvation. */\n+    private final AtomicInteger operationCount = new AtomicInteger();\n+\n+    /** Schedule flag. */\n+    private final AtomicBoolean scheduled = new AtomicBoolean();\n+\n+    /** Whether the fragment is initialized. */\n+    private volatile boolean initialized;\n+\n+    /** Whether the fragment has completed. */\n+    private volatile boolean completed;\n+\n+    public QueryFragmentExecutable(\n+        QueryStateCallback stateCallback,\n+        List<Object> arguments,\n+        Exec exec,\n+        Map<Integer, InboundHandler> inboxes,\n+        Map<Integer, Map<UUID, OutboundHandler>> outboxes,\n+        QueryFragmentWorkerPool fragmentPool\n+    ) {\n+        this.stateCallback = stateCallback;\n+        this.arguments = arguments;\n+        this.exec = exec;\n+        this.inboxes = inboxes;\n+        this.outboxes = outboxes;\n+        this.fragmentPool = fragmentPool;\n+    }\n+\n+    public Collection<Integer> getInboxEdgeIds() {\n+        return inboxes.keySet();\n+    }\n+\n+    public Collection<Integer> getOutboxEdgeIds() {\n+        return outboxes.keySet();\n+    }\n+\n+    /**\n+     * Add operation to be processed.\n+     */\n+    public void addOperation(QueryAbstractExchangeOperation operation) {\n+        operationCount.incrementAndGet();\n+        operations.addLast(operation);\n+    }\n+\n+    public void run() {\n+        try {\n+            if (completed) {\n+                return;\n+            }\n+\n+            // Setup the executor if needed.\n+            setupExecutor();\n+\n+            // Feed all batches to relevant inboxes first. Set the upper boundary on the number of batches to avoid\n+            // starvation when batches arrive quicker than we are able to process them.\n+            int maxOperationCount = operationCount.get();\n+            int processedBatchCount = 0;\n+\n+            QueryOperation operation;\n+\n+            while ((operation = operations.pollFirst()) != null) {\n+                if (operation instanceof QueryBatchExchangeOperation) {\n+                    QueryBatchExchangeOperation operation0 = (QueryBatchExchangeOperation) operation;\n+\n+                    InboundHandler inbox = inboxes.get(operation0.getEdgeId());\n+                    assert inbox != null;\n+\n+                    InboundBatch batch = new InboundBatch(\n+                        operation0.getBatch(),\n+                        operation0.isLast(),\n+                        operation0.getCallerId()\n+                    );\n+\n+                    inbox.onBatch(batch, operation0.getRemainingMemory());\n+                } else {\n+                    assert operation instanceof QueryFlowControlExchangeOperation;\n+\n+                    QueryFlowControlExchangeOperation operation0 = (QueryFlowControlExchangeOperation) operation;\n+\n+                    Map<UUID, OutboundHandler> edgeOutboxes = outboxes.get(operation0.getEdgeId());\n+                    assert edgeOutboxes != null;\n+\n+                    OutboundHandler outbox = edgeOutboxes.get(operation.getCallerId());\n+                    assert outbox != null;\n+\n+                    outbox.onFlowControl(operation0.getRemainingMemory());\n+                }\n+\n+                if (++processedBatchCount >= maxOperationCount) {\n+                    break;\n+                }\n+            }\n+\n+            operationCount.addAndGet(-1 * processedBatchCount);\n+\n+            IterationResult res = exec.advance();\n+\n+            // Send flow control messages if needed.\n+            if (res != IterationResult.FETCHED_DONE) {\n+                for (InboundHandler inbox : inboxes.values()) {\n+                    inbox.sendFlowControl();\n+                }\n+            }\n+\n+            // If executor finished, notify the state.\n+            if (res == IterationResult.FETCHED_DONE) {\n+                completed = true;\n+\n+                stateCallback.onFragmentFinished();\n+            }\n+        } catch (Exception e) {\n+            // Prevent subsequent invocations.\n+            completed = true;\n+\n+            // Notify state about the exception to trigger cancel operation.\n+            stateCallback.cancel(e);\n+        }\n+\n+        // Unschedule the fragment with double-check for new batches.\n+        unschedule();\n+    }\n+\n+    @Override\n+    public boolean schedule() {\n+        // If the fragment is already scheduled, we do not need to do anything else, because executor will re-check queue state\n+        // before exiting.\n+        if (scheduled.get()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c758ffc74823617bad0ce51e6838d2c4fe57ed0c"}, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY1Mjk1Ng==", "bodyText": "CAS is typically implemented as a write operation even if it doesn't change the value, causing contention on the bus. So here we employ test-and-test-and-set rather than test-and-set (CAS), to minimize the number of writes on a shared variable.\nThis should be considered as a micro-optimization, and it is hard to prove that it adds value with some numbers. But in general, it is assumed that there should be fewer fragment execution attempts than batches, so IMO it should pay off.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399652956", "createdAt": "2020-03-28T11:44:12Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentExecutable.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.sql.impl.exec.Exec;\n+import com.hazelcast.sql.impl.exec.IterationResult;\n+import com.hazelcast.sql.impl.fragment.QueryFragmentContext;\n+import com.hazelcast.sql.impl.mailbox.InboundHandler;\n+import com.hazelcast.sql.impl.mailbox.InboundBatch;\n+import com.hazelcast.sql.impl.mailbox.OutboundHandler;\n+import com.hazelcast.sql.impl.operation.QueryBatchExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryAbstractExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryFlowControlExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+import com.hazelcast.sql.impl.state.QueryStateCallback;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentLinkedDeque;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Query fragment executable that advances the top-level operator, consumes data operations, and manages scheduling.\n+ */\n+public class QueryFragmentExecutable implements QueryFragmentScheduleCallback {\n+\n+    private final QueryStateCallback stateCallback;\n+    private final List<Object> arguments;\n+    private final Exec exec;\n+    private final Map<Integer, InboundHandler> inboxes;\n+    private final Map<Integer, Map<UUID, OutboundHandler>> outboxes;\n+    private final QueryFragmentWorkerPool fragmentPool;\n+\n+    /** Operations to be processed. */\n+    private final ConcurrentLinkedDeque<QueryAbstractExchangeOperation> operations = new ConcurrentLinkedDeque<>();\n+\n+    /** Batch count which we used instead of batches.size() (which is O(N)) to prevent starvation. */\n+    private final AtomicInteger operationCount = new AtomicInteger();\n+\n+    /** Schedule flag. */\n+    private final AtomicBoolean scheduled = new AtomicBoolean();\n+\n+    /** Whether the fragment is initialized. */\n+    private volatile boolean initialized;\n+\n+    /** Whether the fragment has completed. */\n+    private volatile boolean completed;\n+\n+    public QueryFragmentExecutable(\n+        QueryStateCallback stateCallback,\n+        List<Object> arguments,\n+        Exec exec,\n+        Map<Integer, InboundHandler> inboxes,\n+        Map<Integer, Map<UUID, OutboundHandler>> outboxes,\n+        QueryFragmentWorkerPool fragmentPool\n+    ) {\n+        this.stateCallback = stateCallback;\n+        this.arguments = arguments;\n+        this.exec = exec;\n+        this.inboxes = inboxes;\n+        this.outboxes = outboxes;\n+        this.fragmentPool = fragmentPool;\n+    }\n+\n+    public Collection<Integer> getInboxEdgeIds() {\n+        return inboxes.keySet();\n+    }\n+\n+    public Collection<Integer> getOutboxEdgeIds() {\n+        return outboxes.keySet();\n+    }\n+\n+    /**\n+     * Add operation to be processed.\n+     */\n+    public void addOperation(QueryAbstractExchangeOperation operation) {\n+        operationCount.incrementAndGet();\n+        operations.addLast(operation);\n+    }\n+\n+    public void run() {\n+        try {\n+            if (completed) {\n+                return;\n+            }\n+\n+            // Setup the executor if needed.\n+            setupExecutor();\n+\n+            // Feed all batches to relevant inboxes first. Set the upper boundary on the number of batches to avoid\n+            // starvation when batches arrive quicker than we are able to process them.\n+            int maxOperationCount = operationCount.get();\n+            int processedBatchCount = 0;\n+\n+            QueryOperation operation;\n+\n+            while ((operation = operations.pollFirst()) != null) {\n+                if (operation instanceof QueryBatchExchangeOperation) {\n+                    QueryBatchExchangeOperation operation0 = (QueryBatchExchangeOperation) operation;\n+\n+                    InboundHandler inbox = inboxes.get(operation0.getEdgeId());\n+                    assert inbox != null;\n+\n+                    InboundBatch batch = new InboundBatch(\n+                        operation0.getBatch(),\n+                        operation0.isLast(),\n+                        operation0.getCallerId()\n+                    );\n+\n+                    inbox.onBatch(batch, operation0.getRemainingMemory());\n+                } else {\n+                    assert operation instanceof QueryFlowControlExchangeOperation;\n+\n+                    QueryFlowControlExchangeOperation operation0 = (QueryFlowControlExchangeOperation) operation;\n+\n+                    Map<UUID, OutboundHandler> edgeOutboxes = outboxes.get(operation0.getEdgeId());\n+                    assert edgeOutboxes != null;\n+\n+                    OutboundHandler outbox = edgeOutboxes.get(operation.getCallerId());\n+                    assert outbox != null;\n+\n+                    outbox.onFlowControl(operation0.getRemainingMemory());\n+                }\n+\n+                if (++processedBatchCount >= maxOperationCount) {\n+                    break;\n+                }\n+            }\n+\n+            operationCount.addAndGet(-1 * processedBatchCount);\n+\n+            IterationResult res = exec.advance();\n+\n+            // Send flow control messages if needed.\n+            if (res != IterationResult.FETCHED_DONE) {\n+                for (InboundHandler inbox : inboxes.values()) {\n+                    inbox.sendFlowControl();\n+                }\n+            }\n+\n+            // If executor finished, notify the state.\n+            if (res == IterationResult.FETCHED_DONE) {\n+                completed = true;\n+\n+                stateCallback.onFragmentFinished();\n+            }\n+        } catch (Exception e) {\n+            // Prevent subsequent invocations.\n+            completed = true;\n+\n+            // Notify state about the exception to trigger cancel operation.\n+            stateCallback.cancel(e);\n+        }\n+\n+        // Unschedule the fragment with double-check for new batches.\n+        unschedule();\n+    }\n+\n+    @Override\n+    public boolean schedule() {\n+        // If the fragment is already scheduled, we do not need to do anything else, because executor will re-check queue state\n+        // before exiting.\n+        if (scheduled.get()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQzMTY0Mg=="}, "originalCommit": {"oid": "c758ffc74823617bad0ce51e6838d2c4fe57ed0c"}, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY1Mjk4MQ==", "bodyText": "BTW, I slightly changed the reschedule algorithm, so that we reach this place even rarer.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399652981", "createdAt": "2020-03-28T11:44:42Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentExecutable.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.sql.impl.exec.Exec;\n+import com.hazelcast.sql.impl.exec.IterationResult;\n+import com.hazelcast.sql.impl.fragment.QueryFragmentContext;\n+import com.hazelcast.sql.impl.mailbox.InboundHandler;\n+import com.hazelcast.sql.impl.mailbox.InboundBatch;\n+import com.hazelcast.sql.impl.mailbox.OutboundHandler;\n+import com.hazelcast.sql.impl.operation.QueryBatchExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryAbstractExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryFlowControlExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+import com.hazelcast.sql.impl.state.QueryStateCallback;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentLinkedDeque;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Query fragment executable that advances the top-level operator, consumes data operations, and manages scheduling.\n+ */\n+public class QueryFragmentExecutable implements QueryFragmentScheduleCallback {\n+\n+    private final QueryStateCallback stateCallback;\n+    private final List<Object> arguments;\n+    private final Exec exec;\n+    private final Map<Integer, InboundHandler> inboxes;\n+    private final Map<Integer, Map<UUID, OutboundHandler>> outboxes;\n+    private final QueryFragmentWorkerPool fragmentPool;\n+\n+    /** Operations to be processed. */\n+    private final ConcurrentLinkedDeque<QueryAbstractExchangeOperation> operations = new ConcurrentLinkedDeque<>();\n+\n+    /** Batch count which we used instead of batches.size() (which is O(N)) to prevent starvation. */\n+    private final AtomicInteger operationCount = new AtomicInteger();\n+\n+    /** Schedule flag. */\n+    private final AtomicBoolean scheduled = new AtomicBoolean();\n+\n+    /** Whether the fragment is initialized. */\n+    private volatile boolean initialized;\n+\n+    /** Whether the fragment has completed. */\n+    private volatile boolean completed;\n+\n+    public QueryFragmentExecutable(\n+        QueryStateCallback stateCallback,\n+        List<Object> arguments,\n+        Exec exec,\n+        Map<Integer, InboundHandler> inboxes,\n+        Map<Integer, Map<UUID, OutboundHandler>> outboxes,\n+        QueryFragmentWorkerPool fragmentPool\n+    ) {\n+        this.stateCallback = stateCallback;\n+        this.arguments = arguments;\n+        this.exec = exec;\n+        this.inboxes = inboxes;\n+        this.outboxes = outboxes;\n+        this.fragmentPool = fragmentPool;\n+    }\n+\n+    public Collection<Integer> getInboxEdgeIds() {\n+        return inboxes.keySet();\n+    }\n+\n+    public Collection<Integer> getOutboxEdgeIds() {\n+        return outboxes.keySet();\n+    }\n+\n+    /**\n+     * Add operation to be processed.\n+     */\n+    public void addOperation(QueryAbstractExchangeOperation operation) {\n+        operationCount.incrementAndGet();\n+        operations.addLast(operation);\n+    }\n+\n+    public void run() {\n+        try {\n+            if (completed) {\n+                return;\n+            }\n+\n+            // Setup the executor if needed.\n+            setupExecutor();\n+\n+            // Feed all batches to relevant inboxes first. Set the upper boundary on the number of batches to avoid\n+            // starvation when batches arrive quicker than we are able to process them.\n+            int maxOperationCount = operationCount.get();\n+            int processedBatchCount = 0;\n+\n+            QueryOperation operation;\n+\n+            while ((operation = operations.pollFirst()) != null) {\n+                if (operation instanceof QueryBatchExchangeOperation) {\n+                    QueryBatchExchangeOperation operation0 = (QueryBatchExchangeOperation) operation;\n+\n+                    InboundHandler inbox = inboxes.get(operation0.getEdgeId());\n+                    assert inbox != null;\n+\n+                    InboundBatch batch = new InboundBatch(\n+                        operation0.getBatch(),\n+                        operation0.isLast(),\n+                        operation0.getCallerId()\n+                    );\n+\n+                    inbox.onBatch(batch, operation0.getRemainingMemory());\n+                } else {\n+                    assert operation instanceof QueryFlowControlExchangeOperation;\n+\n+                    QueryFlowControlExchangeOperation operation0 = (QueryFlowControlExchangeOperation) operation;\n+\n+                    Map<UUID, OutboundHandler> edgeOutboxes = outboxes.get(operation0.getEdgeId());\n+                    assert edgeOutboxes != null;\n+\n+                    OutboundHandler outbox = edgeOutboxes.get(operation.getCallerId());\n+                    assert outbox != null;\n+\n+                    outbox.onFlowControl(operation0.getRemainingMemory());\n+                }\n+\n+                if (++processedBatchCount >= maxOperationCount) {\n+                    break;\n+                }\n+            }\n+\n+            operationCount.addAndGet(-1 * processedBatchCount);\n+\n+            IterationResult res = exec.advance();\n+\n+            // Send flow control messages if needed.\n+            if (res != IterationResult.FETCHED_DONE) {\n+                for (InboundHandler inbox : inboxes.values()) {\n+                    inbox.sendFlowControl();\n+                }\n+            }\n+\n+            // If executor finished, notify the state.\n+            if (res == IterationResult.FETCHED_DONE) {\n+                completed = true;\n+\n+                stateCallback.onFragmentFinished();\n+            }\n+        } catch (Exception e) {\n+            // Prevent subsequent invocations.\n+            completed = true;\n+\n+            // Notify state about the exception to trigger cancel operation.\n+            stateCallback.cancel(e);\n+        }\n+\n+        // Unschedule the fragment with double-check for new batches.\n+        unschedule();\n+    }\n+\n+    @Override\n+    public boolean schedule() {\n+        // If the fragment is already scheduled, we do not need to do anything else, because executor will re-check queue state\n+        // before exiting.\n+        if (scheduled.get()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQzMTY0Mg=="}, "originalCommit": {"oid": "c758ffc74823617bad0ce51e6838d2c4fe57ed0c"}, "originalPosition": 180}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3NjM2NzI0OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentScheduleCallback.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QxNzozNjo1N1rOF87bJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOFQxMTo0NDo0OVrOF9I4jA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQzMjQ4Nw==", "bodyText": "True ->true", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399432487", "createdAt": "2020-03-27T17:36:57Z", "author": {"login": "petrpleshachkov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentScheduleCallback.java", "diffHunk": "@@ -0,0 +1,26 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+public interface QueryFragmentScheduleCallback {\n+    /**\n+     * Schedule the fragment for execution.\n+     *\n+     * @return {@code True} if the fragment was scheduled, {@code false} if already scheduled.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c758ffc74823617bad0ce51e6838d2c4fe57ed0c"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTY1MzAwNA==", "bodyText": "Fixed.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399653004", "createdAt": "2020-03-28T11:44:49Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryFragmentScheduleCallback.java", "diffHunk": "@@ -0,0 +1,26 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+public interface QueryFragmentScheduleCallback {\n+    /**\n+     * Schedule the fragment for execution.\n+     *\n+     * @return {@code True} if the fragment was scheduled, {@code false} if already scheduled.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQzMjQ4Nw=="}, "originalCommit": {"oid": "c758ffc74823617bad0ce51e6838d2c4fe57ed0c"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MDI3OTU2OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryOperationExecutable.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNzoxMDowN1rOF9cOyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNzoyODozOFrOF9cyFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk2OTk5Mw==", "bodyText": "It'd be nice to see a bit mode documentation about local and remote operations.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399969993", "createdAt": "2020-03-30T07:10:07Z", "author": {"login": "petrpleshachkov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryOperationExecutable.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.internal.nio.Packet;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+\n+/**\n+ * Operation descriptor.\n+ */\n+public final class QueryOperationExecutable {\n+    private final QueryOperation localOperation;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk3OTAyOQ==", "bodyText": "Add JavaDoc.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399979029", "createdAt": "2020-03-30T07:28:38Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryOperationExecutable.java", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.internal.nio.Packet;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+\n+/**\n+ * Operation descriptor.\n+ */\n+public final class QueryOperationExecutable {\n+    private final QueryOperation localOperation;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk2OTk5Mw=="}, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MDI4ODI4OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/operation/QueryOperationHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNzoxMjo1OVrOF9cT_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNzoyODozM1rOF9cx8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk3MTMyNA==", "bodyText": "Is null operation allowed here?", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399971324", "createdAt": "2020-03-30T07:12:59Z", "author": {"login": "petrpleshachkov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/operation/QueryOperationHandler.java", "diffHunk": "@@ -0,0 +1,40 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.operation;\n+\n+import java.util.UUID;\n+\n+/**\n+ * Query operation executor.\n+ */\n+public interface QueryOperationHandler {\n+    /**\n+     * Submit operation for execution on a member.\n+     *\n+     * @param memberId ID of the member.\n+     * @param operation Operation to be executed.\n+     * @return {@code true} if operation is triggered, {@code false} if target member is not available.\n+     */\n+    boolean submit(UUID memberId, QueryOperation operation);\n+\n+    /**\n+     * Execute the operation synchronously.\n+     *\n+     * @param operation Operation.\n+     */\n+    void execute(QueryOperation operation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk3ODk5NA==", "bodyText": "It should not be possible. Fixed the code.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399978994", "createdAt": "2020-03-30T07:28:33Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/operation/QueryOperationHandler.java", "diffHunk": "@@ -0,0 +1,40 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.operation;\n+\n+import java.util.UUID;\n+\n+/**\n+ * Query operation executor.\n+ */\n+public interface QueryOperationHandler {\n+    /**\n+     * Submit operation for execution on a member.\n+     *\n+     * @param memberId ID of the member.\n+     * @param operation Operation to be executed.\n+     * @return {@code true} if operation is triggered, {@code false} if target member is not available.\n+     */\n+    boolean submit(UUID memberId, QueryOperation operation);\n+\n+    /**\n+     * Execute the operation synchronously.\n+     *\n+     * @param operation Operation.\n+     */\n+    void execute(QueryOperation operation);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk3MTMyNA=="}, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MDI5MjYyOnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryOperationWorker.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNzoxNDoxOFrOF9cWng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNzoyODoyNVrOF9cxpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk3MTk5OA==", "bodyText": "It's possible to pass null to the execute. Is it allowed? Is it documented anywhere?", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399971998", "createdAt": "2020-03-30T07:14:18Z", "author": {"login": "petrpleshachkov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryOperationWorker.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.internal.nio.Packet;\n+import com.hazelcast.internal.serialization.SerializationService;\n+import com.hazelcast.internal.util.concurrent.MPSCQueue;\n+import com.hazelcast.logging.ILogger;\n+import com.hazelcast.sql.HazelcastSqlException;\n+import com.hazelcast.sql.impl.QueryId;\n+import com.hazelcast.sql.impl.QueryUtils;\n+import com.hazelcast.sql.impl.operation.QueryCancelOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperationDeserializationException;\n+import com.hazelcast.sql.impl.operation.QueryOperationHandler;\n+\n+import java.util.UUID;\n+\n+import static com.hazelcast.instance.impl.OutOfMemoryErrorDispatcher.inspectOutOfMemoryError;\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_OPERATION;\n+\n+/**\n+ * Worker responsible for operation processing.\n+ */\n+public class QueryOperationWorker implements Runnable {\n+\n+    private static final Object POISON = new Object();\n+\n+    private final QueryOperationHandler operationHandler;\n+    private final SerializationService ss;\n+    private final Thread thread;\n+    private final MPSCQueue<Object> queue;\n+    private final ILogger logger;\n+\n+    private UUID localMemberId;\n+\n+    public QueryOperationWorker(\n+        QueryOperationHandler operationHandler,\n+        SerializationService ss,\n+        String instanceName,\n+        int index,\n+        ILogger logger\n+    ) {\n+        this.operationHandler = operationHandler;\n+        this.ss = ss;\n+        this.logger = logger;\n+\n+        thread = new Thread(this,  QueryUtils.workerName(instanceName, WORKER_TYPE_OPERATION, index));\n+        queue = new MPSCQueue<>(thread, null);\n+\n+        thread.start();\n+    }\n+\n+    public void init(UUID localMemberId) {\n+        this.localMemberId = localMemberId;\n+    }\n+\n+    public void submit(QueryOperationExecutable task) {\n+        queue.add(task);\n+    }\n+\n+    public void stop() {\n+        queue.clear();\n+        queue.add(POISON);\n+\n+        thread.interrupt();\n+    }\n+\n+    @Override\n+    public void run() {\n+        try {\n+            run0();\n+        } catch (Throwable t) {\n+            inspectOutOfMemoryError(t);\n+            logger.severe(t);\n+        }\n+    }\n+\n+    private void run0() {\n+        try {\n+            while (true) {\n+                Object task = queue.take();\n+\n+                if (task == POISON) {\n+                    break;\n+                } else {\n+                    assert task instanceof QueryOperationExecutable;\n+\n+                    execute((QueryOperationExecutable) task);\n+                }\n+            }\n+        } catch (InterruptedException e) {\n+            // No-op.\n+        }\n+    }\n+\n+    private void execute(QueryOperationExecutable task) {\n+        QueryOperation operation = task.getLocalOperation();\n+\n+        if (operation == null) {\n+            operation = deserialize(task.getRemoteOperation());\n+        }\n+\n+        operationHandler.execute(operation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk3ODkxNw==", "bodyText": "It should not be possible. Fixed the code.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399978917", "createdAt": "2020-03-30T07:28:25Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryOperationWorker.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.internal.nio.Packet;\n+import com.hazelcast.internal.serialization.SerializationService;\n+import com.hazelcast.internal.util.concurrent.MPSCQueue;\n+import com.hazelcast.logging.ILogger;\n+import com.hazelcast.sql.HazelcastSqlException;\n+import com.hazelcast.sql.impl.QueryId;\n+import com.hazelcast.sql.impl.QueryUtils;\n+import com.hazelcast.sql.impl.operation.QueryCancelOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperationDeserializationException;\n+import com.hazelcast.sql.impl.operation.QueryOperationHandler;\n+\n+import java.util.UUID;\n+\n+import static com.hazelcast.instance.impl.OutOfMemoryErrorDispatcher.inspectOutOfMemoryError;\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_OPERATION;\n+\n+/**\n+ * Worker responsible for operation processing.\n+ */\n+public class QueryOperationWorker implements Runnable {\n+\n+    private static final Object POISON = new Object();\n+\n+    private final QueryOperationHandler operationHandler;\n+    private final SerializationService ss;\n+    private final Thread thread;\n+    private final MPSCQueue<Object> queue;\n+    private final ILogger logger;\n+\n+    private UUID localMemberId;\n+\n+    public QueryOperationWorker(\n+        QueryOperationHandler operationHandler,\n+        SerializationService ss,\n+        String instanceName,\n+        int index,\n+        ILogger logger\n+    ) {\n+        this.operationHandler = operationHandler;\n+        this.ss = ss;\n+        this.logger = logger;\n+\n+        thread = new Thread(this,  QueryUtils.workerName(instanceName, WORKER_TYPE_OPERATION, index));\n+        queue = new MPSCQueue<>(thread, null);\n+\n+        thread.start();\n+    }\n+\n+    public void init(UUID localMemberId) {\n+        this.localMemberId = localMemberId;\n+    }\n+\n+    public void submit(QueryOperationExecutable task) {\n+        queue.add(task);\n+    }\n+\n+    public void stop() {\n+        queue.clear();\n+        queue.add(POISON);\n+\n+        thread.interrupt();\n+    }\n+\n+    @Override\n+    public void run() {\n+        try {\n+            run0();\n+        } catch (Throwable t) {\n+            inspectOutOfMemoryError(t);\n+            logger.severe(t);\n+        }\n+    }\n+\n+    private void run0() {\n+        try {\n+            while (true) {\n+                Object task = queue.take();\n+\n+                if (task == POISON) {\n+                    break;\n+                } else {\n+                    assert task instanceof QueryOperationExecutable;\n+\n+                    execute((QueryOperationExecutable) task);\n+                }\n+            }\n+        } catch (InterruptedException e) {\n+            // No-op.\n+        }\n+    }\n+\n+    private void execute(QueryOperationExecutable task) {\n+        QueryOperation operation = task.getLocalOperation();\n+\n+        if (operation == null) {\n+            operation = deserialize(task.getRemoteOperation());\n+        }\n+\n+        operationHandler.execute(operation);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk3MTk5OA=="}, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MDI5ODYzOnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryOperationWorker.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNzoxNjoxOVrOF9caQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNzo1MTowNlrOF9dgaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk3MjkzMQ==", "bodyText": "As part of the exception handling, we cancel the query here. In the else branch we don't do this. Is there any specific reason for this?", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399972931", "createdAt": "2020-03-30T07:16:19Z", "author": {"login": "petrpleshachkov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryOperationWorker.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.internal.nio.Packet;\n+import com.hazelcast.internal.serialization.SerializationService;\n+import com.hazelcast.internal.util.concurrent.MPSCQueue;\n+import com.hazelcast.logging.ILogger;\n+import com.hazelcast.sql.HazelcastSqlException;\n+import com.hazelcast.sql.impl.QueryId;\n+import com.hazelcast.sql.impl.QueryUtils;\n+import com.hazelcast.sql.impl.operation.QueryCancelOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperationDeserializationException;\n+import com.hazelcast.sql.impl.operation.QueryOperationHandler;\n+\n+import java.util.UUID;\n+\n+import static com.hazelcast.instance.impl.OutOfMemoryErrorDispatcher.inspectOutOfMemoryError;\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_OPERATION;\n+\n+/**\n+ * Worker responsible for operation processing.\n+ */\n+public class QueryOperationWorker implements Runnable {\n+\n+    private static final Object POISON = new Object();\n+\n+    private final QueryOperationHandler operationHandler;\n+    private final SerializationService ss;\n+    private final Thread thread;\n+    private final MPSCQueue<Object> queue;\n+    private final ILogger logger;\n+\n+    private UUID localMemberId;\n+\n+    public QueryOperationWorker(\n+        QueryOperationHandler operationHandler,\n+        SerializationService ss,\n+        String instanceName,\n+        int index,\n+        ILogger logger\n+    ) {\n+        this.operationHandler = operationHandler;\n+        this.ss = ss;\n+        this.logger = logger;\n+\n+        thread = new Thread(this,  QueryUtils.workerName(instanceName, WORKER_TYPE_OPERATION, index));\n+        queue = new MPSCQueue<>(thread, null);\n+\n+        thread.start();\n+    }\n+\n+    public void init(UUID localMemberId) {\n+        this.localMemberId = localMemberId;\n+    }\n+\n+    public void submit(QueryOperationExecutable task) {\n+        queue.add(task);\n+    }\n+\n+    public void stop() {\n+        queue.clear();\n+        queue.add(POISON);\n+\n+        thread.interrupt();\n+    }\n+\n+    @Override\n+    public void run() {\n+        try {\n+            run0();\n+        } catch (Throwable t) {\n+            inspectOutOfMemoryError(t);\n+            logger.severe(t);\n+        }\n+    }\n+\n+    private void run0() {\n+        try {\n+            while (true) {\n+                Object task = queue.take();\n+\n+                if (task == POISON) {\n+                    break;\n+                } else {\n+                    assert task instanceof QueryOperationExecutable;\n+\n+                    execute((QueryOperationExecutable) task);\n+                }\n+            }\n+        } catch (InterruptedException e) {\n+            // No-op.\n+        }\n+    }\n+\n+    private void execute(QueryOperationExecutable task) {\n+        QueryOperation operation = task.getLocalOperation();\n+\n+        if (operation == null) {\n+            operation = deserialize(task.getRemoteOperation());\n+        }\n+\n+        operationHandler.execute(operation);\n+    }\n+\n+    /**\n+     * Deserializes the packet into operation. If an exception happens, the query is cancelled.\n+     *\n+     * @param packet Packet packet.\n+     * @return Query operation.\n+     */\n+    private QueryOperation deserialize(Packet packet) {\n+        try {\n+            return ss.toObject(packet);\n+        } catch (Exception e) {\n+            if (e.getCause() instanceof QueryOperationDeserializationException) {\n+                QueryOperationDeserializationException error = (QueryOperationDeserializationException) e.getCause();\n+\n+                // We assume that only ID aware operations may hold user data. Other operations contain only HZ classes and\n+                // we should never see deserialization errors for them.\n+                sendDeserializationError(error);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk3ODgxNg==", "bodyText": "The \"else\" branch is used for the situation where we cannot even extract the query ID (or the operation doesn't have query ID at all). Without it, we cannot cancel anything.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399978816", "createdAt": "2020-03-30T07:28:13Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryOperationWorker.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.internal.nio.Packet;\n+import com.hazelcast.internal.serialization.SerializationService;\n+import com.hazelcast.internal.util.concurrent.MPSCQueue;\n+import com.hazelcast.logging.ILogger;\n+import com.hazelcast.sql.HazelcastSqlException;\n+import com.hazelcast.sql.impl.QueryId;\n+import com.hazelcast.sql.impl.QueryUtils;\n+import com.hazelcast.sql.impl.operation.QueryCancelOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperationDeserializationException;\n+import com.hazelcast.sql.impl.operation.QueryOperationHandler;\n+\n+import java.util.UUID;\n+\n+import static com.hazelcast.instance.impl.OutOfMemoryErrorDispatcher.inspectOutOfMemoryError;\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_OPERATION;\n+\n+/**\n+ * Worker responsible for operation processing.\n+ */\n+public class QueryOperationWorker implements Runnable {\n+\n+    private static final Object POISON = new Object();\n+\n+    private final QueryOperationHandler operationHandler;\n+    private final SerializationService ss;\n+    private final Thread thread;\n+    private final MPSCQueue<Object> queue;\n+    private final ILogger logger;\n+\n+    private UUID localMemberId;\n+\n+    public QueryOperationWorker(\n+        QueryOperationHandler operationHandler,\n+        SerializationService ss,\n+        String instanceName,\n+        int index,\n+        ILogger logger\n+    ) {\n+        this.operationHandler = operationHandler;\n+        this.ss = ss;\n+        this.logger = logger;\n+\n+        thread = new Thread(this,  QueryUtils.workerName(instanceName, WORKER_TYPE_OPERATION, index));\n+        queue = new MPSCQueue<>(thread, null);\n+\n+        thread.start();\n+    }\n+\n+    public void init(UUID localMemberId) {\n+        this.localMemberId = localMemberId;\n+    }\n+\n+    public void submit(QueryOperationExecutable task) {\n+        queue.add(task);\n+    }\n+\n+    public void stop() {\n+        queue.clear();\n+        queue.add(POISON);\n+\n+        thread.interrupt();\n+    }\n+\n+    @Override\n+    public void run() {\n+        try {\n+            run0();\n+        } catch (Throwable t) {\n+            inspectOutOfMemoryError(t);\n+            logger.severe(t);\n+        }\n+    }\n+\n+    private void run0() {\n+        try {\n+            while (true) {\n+                Object task = queue.take();\n+\n+                if (task == POISON) {\n+                    break;\n+                } else {\n+                    assert task instanceof QueryOperationExecutable;\n+\n+                    execute((QueryOperationExecutable) task);\n+                }\n+            }\n+        } catch (InterruptedException e) {\n+            // No-op.\n+        }\n+    }\n+\n+    private void execute(QueryOperationExecutable task) {\n+        QueryOperation operation = task.getLocalOperation();\n+\n+        if (operation == null) {\n+            operation = deserialize(task.getRemoteOperation());\n+        }\n+\n+        operationHandler.execute(operation);\n+    }\n+\n+    /**\n+     * Deserializes the packet into operation. If an exception happens, the query is cancelled.\n+     *\n+     * @param packet Packet packet.\n+     * @return Query operation.\n+     */\n+    private QueryOperation deserialize(Packet packet) {\n+        try {\n+            return ss.toObject(packet);\n+        } catch (Exception e) {\n+            if (e.getCause() instanceof QueryOperationDeserializationException) {\n+                QueryOperationDeserializationException error = (QueryOperationDeserializationException) e.getCause();\n+\n+                // We assume that only ID aware operations may hold user data. Other operations contain only HZ classes and\n+                // we should never see deserialization errors for them.\n+                sendDeserializationError(error);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk3MjkzMQ=="}, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk4OTkwNg==", "bodyText": "Is there any other mechanism which will eventually cancel the query to avoid resources leak? If yes, I'd add this to the comment.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399989906", "createdAt": "2020-03-30T07:49:16Z", "author": {"login": "petrpleshachkov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryOperationWorker.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.internal.nio.Packet;\n+import com.hazelcast.internal.serialization.SerializationService;\n+import com.hazelcast.internal.util.concurrent.MPSCQueue;\n+import com.hazelcast.logging.ILogger;\n+import com.hazelcast.sql.HazelcastSqlException;\n+import com.hazelcast.sql.impl.QueryId;\n+import com.hazelcast.sql.impl.QueryUtils;\n+import com.hazelcast.sql.impl.operation.QueryCancelOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperationDeserializationException;\n+import com.hazelcast.sql.impl.operation.QueryOperationHandler;\n+\n+import java.util.UUID;\n+\n+import static com.hazelcast.instance.impl.OutOfMemoryErrorDispatcher.inspectOutOfMemoryError;\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_OPERATION;\n+\n+/**\n+ * Worker responsible for operation processing.\n+ */\n+public class QueryOperationWorker implements Runnable {\n+\n+    private static final Object POISON = new Object();\n+\n+    private final QueryOperationHandler operationHandler;\n+    private final SerializationService ss;\n+    private final Thread thread;\n+    private final MPSCQueue<Object> queue;\n+    private final ILogger logger;\n+\n+    private UUID localMemberId;\n+\n+    public QueryOperationWorker(\n+        QueryOperationHandler operationHandler,\n+        SerializationService ss,\n+        String instanceName,\n+        int index,\n+        ILogger logger\n+    ) {\n+        this.operationHandler = operationHandler;\n+        this.ss = ss;\n+        this.logger = logger;\n+\n+        thread = new Thread(this,  QueryUtils.workerName(instanceName, WORKER_TYPE_OPERATION, index));\n+        queue = new MPSCQueue<>(thread, null);\n+\n+        thread.start();\n+    }\n+\n+    public void init(UUID localMemberId) {\n+        this.localMemberId = localMemberId;\n+    }\n+\n+    public void submit(QueryOperationExecutable task) {\n+        queue.add(task);\n+    }\n+\n+    public void stop() {\n+        queue.clear();\n+        queue.add(POISON);\n+\n+        thread.interrupt();\n+    }\n+\n+    @Override\n+    public void run() {\n+        try {\n+            run0();\n+        } catch (Throwable t) {\n+            inspectOutOfMemoryError(t);\n+            logger.severe(t);\n+        }\n+    }\n+\n+    private void run0() {\n+        try {\n+            while (true) {\n+                Object task = queue.take();\n+\n+                if (task == POISON) {\n+                    break;\n+                } else {\n+                    assert task instanceof QueryOperationExecutable;\n+\n+                    execute((QueryOperationExecutable) task);\n+                }\n+            }\n+        } catch (InterruptedException e) {\n+            // No-op.\n+        }\n+    }\n+\n+    private void execute(QueryOperationExecutable task) {\n+        QueryOperation operation = task.getLocalOperation();\n+\n+        if (operation == null) {\n+            operation = deserialize(task.getRemoteOperation());\n+        }\n+\n+        operationHandler.execute(operation);\n+    }\n+\n+    /**\n+     * Deserializes the packet into operation. If an exception happens, the query is cancelled.\n+     *\n+     * @param packet Packet packet.\n+     * @return Query operation.\n+     */\n+    private QueryOperation deserialize(Packet packet) {\n+        try {\n+            return ss.toObject(packet);\n+        } catch (Exception e) {\n+            if (e.getCause() instanceof QueryOperationDeserializationException) {\n+                QueryOperationDeserializationException error = (QueryOperationDeserializationException) e.getCause();\n+\n+                // We assume that only ID aware operations may hold user data. Other operations contain only HZ classes and\n+                // we should never see deserialization errors for them.\n+                sendDeserializationError(error);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk3MjkzMQ=="}, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk5MDg4OA==", "bodyText": "No additional mechanisms in this specific PR.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399990888", "createdAt": "2020-03-30T07:51:06Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryOperationWorker.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.internal.nio.Packet;\n+import com.hazelcast.internal.serialization.SerializationService;\n+import com.hazelcast.internal.util.concurrent.MPSCQueue;\n+import com.hazelcast.logging.ILogger;\n+import com.hazelcast.sql.HazelcastSqlException;\n+import com.hazelcast.sql.impl.QueryId;\n+import com.hazelcast.sql.impl.QueryUtils;\n+import com.hazelcast.sql.impl.operation.QueryCancelOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperationDeserializationException;\n+import com.hazelcast.sql.impl.operation.QueryOperationHandler;\n+\n+import java.util.UUID;\n+\n+import static com.hazelcast.instance.impl.OutOfMemoryErrorDispatcher.inspectOutOfMemoryError;\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_OPERATION;\n+\n+/**\n+ * Worker responsible for operation processing.\n+ */\n+public class QueryOperationWorker implements Runnable {\n+\n+    private static final Object POISON = new Object();\n+\n+    private final QueryOperationHandler operationHandler;\n+    private final SerializationService ss;\n+    private final Thread thread;\n+    private final MPSCQueue<Object> queue;\n+    private final ILogger logger;\n+\n+    private UUID localMemberId;\n+\n+    public QueryOperationWorker(\n+        QueryOperationHandler operationHandler,\n+        SerializationService ss,\n+        String instanceName,\n+        int index,\n+        ILogger logger\n+    ) {\n+        this.operationHandler = operationHandler;\n+        this.ss = ss;\n+        this.logger = logger;\n+\n+        thread = new Thread(this,  QueryUtils.workerName(instanceName, WORKER_TYPE_OPERATION, index));\n+        queue = new MPSCQueue<>(thread, null);\n+\n+        thread.start();\n+    }\n+\n+    public void init(UUID localMemberId) {\n+        this.localMemberId = localMemberId;\n+    }\n+\n+    public void submit(QueryOperationExecutable task) {\n+        queue.add(task);\n+    }\n+\n+    public void stop() {\n+        queue.clear();\n+        queue.add(POISON);\n+\n+        thread.interrupt();\n+    }\n+\n+    @Override\n+    public void run() {\n+        try {\n+            run0();\n+        } catch (Throwable t) {\n+            inspectOutOfMemoryError(t);\n+            logger.severe(t);\n+        }\n+    }\n+\n+    private void run0() {\n+        try {\n+            while (true) {\n+                Object task = queue.take();\n+\n+                if (task == POISON) {\n+                    break;\n+                } else {\n+                    assert task instanceof QueryOperationExecutable;\n+\n+                    execute((QueryOperationExecutable) task);\n+                }\n+            }\n+        } catch (InterruptedException e) {\n+            // No-op.\n+        }\n+    }\n+\n+    private void execute(QueryOperationExecutable task) {\n+        QueryOperation operation = task.getLocalOperation();\n+\n+        if (operation == null) {\n+            operation = deserialize(task.getRemoteOperation());\n+        }\n+\n+        operationHandler.execute(operation);\n+    }\n+\n+    /**\n+     * Deserializes the packet into operation. If an exception happens, the query is cancelled.\n+     *\n+     * @param packet Packet packet.\n+     * @return Query operation.\n+     */\n+    private QueryOperation deserialize(Packet packet) {\n+        try {\n+            return ss.toObject(packet);\n+        } catch (Exception e) {\n+            if (e.getCause() instanceof QueryOperationDeserializationException) {\n+                QueryOperationDeserializationException error = (QueryOperationDeserializationException) e.getCause();\n+\n+                // We assume that only ID aware operations may hold user data. Other operations contain only HZ classes and\n+                // we should never see deserialization errors for them.\n+                sendDeserializationError(error);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk3MjkzMQ=="}, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 136}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MDMxMjI1OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryOperationWorker.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNzoyMDoyOVrOF9ciAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNzo1MTo1N1rOF9diGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk3NDkxNA==", "bodyText": "!thread.isAlive() ?", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399974914", "createdAt": "2020-03-30T07:20:29Z", "author": {"login": "petrpleshachkov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryOperationWorker.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.internal.nio.Packet;\n+import com.hazelcast.internal.serialization.SerializationService;\n+import com.hazelcast.internal.util.concurrent.MPSCQueue;\n+import com.hazelcast.logging.ILogger;\n+import com.hazelcast.sql.HazelcastSqlException;\n+import com.hazelcast.sql.impl.QueryId;\n+import com.hazelcast.sql.impl.QueryUtils;\n+import com.hazelcast.sql.impl.operation.QueryCancelOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperationDeserializationException;\n+import com.hazelcast.sql.impl.operation.QueryOperationHandler;\n+\n+import java.util.UUID;\n+\n+import static com.hazelcast.instance.impl.OutOfMemoryErrorDispatcher.inspectOutOfMemoryError;\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_OPERATION;\n+\n+/**\n+ * Worker responsible for operation processing.\n+ */\n+public class QueryOperationWorker implements Runnable {\n+\n+    private static final Object POISON = new Object();\n+\n+    private final QueryOperationHandler operationHandler;\n+    private final SerializationService ss;\n+    private final Thread thread;\n+    private final MPSCQueue<Object> queue;\n+    private final ILogger logger;\n+\n+    private UUID localMemberId;\n+\n+    public QueryOperationWorker(\n+        QueryOperationHandler operationHandler,\n+        SerializationService ss,\n+        String instanceName,\n+        int index,\n+        ILogger logger\n+    ) {\n+        this.operationHandler = operationHandler;\n+        this.ss = ss;\n+        this.logger = logger;\n+\n+        thread = new Thread(this,  QueryUtils.workerName(instanceName, WORKER_TYPE_OPERATION, index));\n+        queue = new MPSCQueue<>(thread, null);\n+\n+        thread.start();\n+    }\n+\n+    public void init(UUID localMemberId) {\n+        this.localMemberId = localMemberId;\n+    }\n+\n+    public void submit(QueryOperationExecutable task) {\n+        queue.add(task);\n+    }\n+\n+    public void stop() {\n+        queue.clear();\n+        queue.add(POISON);\n+\n+        thread.interrupt();\n+    }\n+\n+    @Override\n+    public void run() {\n+        try {\n+            run0();\n+        } catch (Throwable t) {\n+            inspectOutOfMemoryError(t);\n+            logger.severe(t);\n+        }\n+    }\n+\n+    private void run0() {\n+        try {\n+            while (true) {\n+                Object task = queue.take();\n+\n+                if (task == POISON) {\n+                    break;\n+                } else {\n+                    assert task instanceof QueryOperationExecutable;\n+\n+                    execute((QueryOperationExecutable) task);\n+                }\n+            }\n+        } catch (InterruptedException e) {\n+            // No-op.\n+        }\n+    }\n+\n+    private void execute(QueryOperationExecutable task) {\n+        QueryOperation operation = task.getLocalOperation();\n+\n+        if (operation == null) {\n+            operation = deserialize(task.getRemoteOperation());\n+        }\n+\n+        operationHandler.execute(operation);\n+    }\n+\n+    /**\n+     * Deserializes the packet into operation. If an exception happens, the query is cancelled.\n+     *\n+     * @param packet Packet packet.\n+     * @return Query operation.\n+     */\n+    private QueryOperation deserialize(Packet packet) {\n+        try {\n+            return ss.toObject(packet);\n+        } catch (Exception e) {\n+            if (e.getCause() instanceof QueryOperationDeserializationException) {\n+                QueryOperationDeserializationException error = (QueryOperationDeserializationException) e.getCause();\n+\n+                // We assume that only ID aware operations may hold user data. Other operations contain only HZ classes and\n+                // we should never see deserialization errors for them.\n+                sendDeserializationError(error);\n+            } else {\n+                // It is not easy to decide how to handle an arbitrary exception. We do not have caller coordinates, so\n+                // we do not know how to notify it. We also cannot panic (i.e. kill local member), because it would be a\n+                // security threat. So the only sensible solution is to log the error.\n+                logger.severe(\"Failed to deserialize query operation received from \" + packet.getConn().getEndPoint()\n+                    + \" (will be ignored)\", e);\n+            }\n+        }\n+\n+        return null;\n+    }\n+\n+    private void sendDeserializationError(QueryOperationDeserializationException e) {\n+        QueryId queryId = e.getQueryId();\n+        UUID callerId = e.getCallerId();\n+\n+        HazelcastSqlException error = HazelcastSqlException.error(\"Failed to deserialize \" + e.getOperationClassName()\n+            + \" received from \" + callerId + \": \" + e.getMessage(), e);\n+\n+        QueryCancelOperation cancelOperation =\n+            new QueryCancelOperation(queryId, error.getCode(), error.getMessage(), localMemberId);\n+\n+        try {\n+            operationHandler.submit(queryId.getMemberId(), cancelOperation);\n+        } catch (Exception ignore) {\n+            // This should never happen, since we do not transmit user objects.\n+        }\n+    }\n+\n+    /**\n+     * For testing only.\n+     */\n+    boolean isThreadTerminated() {\n+        return thread.getState() == Thread.State.TERMINATED;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk5MTMyMA==", "bodyText": "It doesn't matter much because the method is used for testing only.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399991320", "createdAt": "2020-03-30T07:51:57Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/main/java/com/hazelcast/sql/impl/worker/QueryOperationWorker.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.internal.nio.Packet;\n+import com.hazelcast.internal.serialization.SerializationService;\n+import com.hazelcast.internal.util.concurrent.MPSCQueue;\n+import com.hazelcast.logging.ILogger;\n+import com.hazelcast.sql.HazelcastSqlException;\n+import com.hazelcast.sql.impl.QueryId;\n+import com.hazelcast.sql.impl.QueryUtils;\n+import com.hazelcast.sql.impl.operation.QueryCancelOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperationDeserializationException;\n+import com.hazelcast.sql.impl.operation.QueryOperationHandler;\n+\n+import java.util.UUID;\n+\n+import static com.hazelcast.instance.impl.OutOfMemoryErrorDispatcher.inspectOutOfMemoryError;\n+import static com.hazelcast.sql.impl.QueryUtils.WORKER_TYPE_OPERATION;\n+\n+/**\n+ * Worker responsible for operation processing.\n+ */\n+public class QueryOperationWorker implements Runnable {\n+\n+    private static final Object POISON = new Object();\n+\n+    private final QueryOperationHandler operationHandler;\n+    private final SerializationService ss;\n+    private final Thread thread;\n+    private final MPSCQueue<Object> queue;\n+    private final ILogger logger;\n+\n+    private UUID localMemberId;\n+\n+    public QueryOperationWorker(\n+        QueryOperationHandler operationHandler,\n+        SerializationService ss,\n+        String instanceName,\n+        int index,\n+        ILogger logger\n+    ) {\n+        this.operationHandler = operationHandler;\n+        this.ss = ss;\n+        this.logger = logger;\n+\n+        thread = new Thread(this,  QueryUtils.workerName(instanceName, WORKER_TYPE_OPERATION, index));\n+        queue = new MPSCQueue<>(thread, null);\n+\n+        thread.start();\n+    }\n+\n+    public void init(UUID localMemberId) {\n+        this.localMemberId = localMemberId;\n+    }\n+\n+    public void submit(QueryOperationExecutable task) {\n+        queue.add(task);\n+    }\n+\n+    public void stop() {\n+        queue.clear();\n+        queue.add(POISON);\n+\n+        thread.interrupt();\n+    }\n+\n+    @Override\n+    public void run() {\n+        try {\n+            run0();\n+        } catch (Throwable t) {\n+            inspectOutOfMemoryError(t);\n+            logger.severe(t);\n+        }\n+    }\n+\n+    private void run0() {\n+        try {\n+            while (true) {\n+                Object task = queue.take();\n+\n+                if (task == POISON) {\n+                    break;\n+                } else {\n+                    assert task instanceof QueryOperationExecutable;\n+\n+                    execute((QueryOperationExecutable) task);\n+                }\n+            }\n+        } catch (InterruptedException e) {\n+            // No-op.\n+        }\n+    }\n+\n+    private void execute(QueryOperationExecutable task) {\n+        QueryOperation operation = task.getLocalOperation();\n+\n+        if (operation == null) {\n+            operation = deserialize(task.getRemoteOperation());\n+        }\n+\n+        operationHandler.execute(operation);\n+    }\n+\n+    /**\n+     * Deserializes the packet into operation. If an exception happens, the query is cancelled.\n+     *\n+     * @param packet Packet packet.\n+     * @return Query operation.\n+     */\n+    private QueryOperation deserialize(Packet packet) {\n+        try {\n+            return ss.toObject(packet);\n+        } catch (Exception e) {\n+            if (e.getCause() instanceof QueryOperationDeserializationException) {\n+                QueryOperationDeserializationException error = (QueryOperationDeserializationException) e.getCause();\n+\n+                // We assume that only ID aware operations may hold user data. Other operations contain only HZ classes and\n+                // we should never see deserialization errors for them.\n+                sendDeserializationError(error);\n+            } else {\n+                // It is not easy to decide how to handle an arbitrary exception. We do not have caller coordinates, so\n+                // we do not know how to notify it. We also cannot panic (i.e. kill local member), because it would be a\n+                // security threat. So the only sensible solution is to log the error.\n+                logger.severe(\"Failed to deserialize query operation received from \" + packet.getConn().getEndPoint()\n+                    + \" (will be ignored)\", e);\n+            }\n+        }\n+\n+        return null;\n+    }\n+\n+    private void sendDeserializationError(QueryOperationDeserializationException e) {\n+        QueryId queryId = e.getQueryId();\n+        UUID callerId = e.getCallerId();\n+\n+        HazelcastSqlException error = HazelcastSqlException.error(\"Failed to deserialize \" + e.getOperationClassName()\n+            + \" received from \" + callerId + \": \" + e.getMessage(), e);\n+\n+        QueryCancelOperation cancelOperation =\n+            new QueryCancelOperation(queryId, error.getCode(), error.getMessage(), localMemberId);\n+\n+        try {\n+            operationHandler.submit(queryId.getMemberId(), cancelOperation);\n+        } catch (Exception ignore) {\n+            // This should never happen, since we do not transmit user objects.\n+        }\n+    }\n+\n+    /**\n+     * For testing only.\n+     */\n+    boolean isThreadTerminated() {\n+        return thread.getState() == Thread.State.TERMINATED;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk3NDkxNA=="}, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 170}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MDM1ODc5OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/test/java/com/hazelcast/sql/impl/worker/QueryFragmentExecutableTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNzozNDozNFrOF9c95Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNzo1NTowOVrOF9dpGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk4MjA1Mw==", "bodyText": "You can use hazelcastTestSupport#assertOpenEventually which has a timeout to avoid indefinite blocks...", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399982053", "createdAt": "2020-03-30T07:34:34Z", "author": {"login": "petrpleshachkov"}, "path": "hazelcast/src/test/java/com/hazelcast/sql/impl/worker/QueryFragmentExecutableTest.java", "diffHunk": "@@ -0,0 +1,478 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.logging.NoLogFactory;\n+import com.hazelcast.sql.impl.QueryId;\n+import com.hazelcast.sql.impl.exec.AbstractExec;\n+import com.hazelcast.sql.impl.exec.IterationResult;\n+import com.hazelcast.sql.impl.fragment.QueryFragmentContext;\n+import com.hazelcast.sql.impl.mailbox.InboundBatch;\n+import com.hazelcast.sql.impl.mailbox.InboundHandler;\n+import com.hazelcast.sql.impl.mailbox.OutboundHandler;\n+import com.hazelcast.sql.impl.operation.QueryAbstractExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryBatchExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryFlowControlExchangeOperation;\n+import com.hazelcast.sql.impl.row.EmptyRowBatch;\n+import com.hazelcast.sql.impl.row.RowBatch;\n+import com.hazelcast.sql.impl.state.QueryStateCallback;\n+import com.hazelcast.test.HazelcastParallelClassRunner;\n+import com.hazelcast.test.HazelcastTestSupport;\n+import com.hazelcast.test.annotation.ParallelJVMTest;\n+import com.hazelcast.test.annotation.QuickTest;\n+import org.junit.After;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+\n+import java.util.Collections;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertSame;\n+import static org.junit.Assert.assertTrue;\n+\n+@RunWith(HazelcastParallelClassRunner.class)\n+@Category({QuickTest.class, ParallelJVMTest.class})\n+public class QueryFragmentExecutableTest extends HazelcastTestSupport {\n+\n+    private QueryFragmentWorkerPool pool;\n+\n+    @After\n+    public void after() {\n+        if (pool != null) {\n+            pool.stop();\n+\n+            pool = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testSetupIsCalledOnlyOnce() {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec().setPayload(new ResultExecPayload(IterationResult.FETCHED));\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.emptyMap(),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getSetupInvocationCount());\n+\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getSetupInvocationCount());\n+    }\n+\n+    @Test\n+    public void testAdvanceIsCalledUntilCompletion() {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec();\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.emptyMap(),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        exec.setPayload(new ResultExecPayload(IterationResult.WAIT));\n+        fragmentExecutable.run();\n+        fragmentExecutable.run();\n+        assertEquals(2, exec.getAdvanceInvocationCount());\n+\n+        exec.setPayload(new ResultExecPayload(IterationResult.FETCHED));\n+        fragmentExecutable.run();\n+        fragmentExecutable.run();\n+        assertEquals(4, exec.getAdvanceInvocationCount());\n+        assertEquals(0, stateCallback.getFragmentFinishedInvocationCount());\n+\n+        exec.setPayload(new ResultExecPayload(IterationResult.FETCHED_DONE));\n+        fragmentExecutable.run();\n+        fragmentExecutable.run();\n+        assertEquals(5, exec.getAdvanceInvocationCount());\n+        assertEquals(1, stateCallback.getFragmentFinishedInvocationCount());\n+    }\n+\n+    @Test\n+    public void testExceptionDuringExecution() {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec();\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.emptyMap(),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        // Throw an exception.\n+        exec.setPayload(new ExceptionExecPayload());\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getAdvanceInvocationCount());\n+        assertSame(ExceptionExecPayload.ERROR, stateCallback.getCancelException());\n+\n+        // Make sure that no advance is possible after that.\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getAdvanceInvocationCount());\n+        assertEquals(0, stateCallback.getFragmentFinishedInvocationCount());\n+    }\n+\n+    @Test\n+    public void testSchedule() throws Exception {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec();\n+\n+        AtomicBoolean flowControlNotified = new AtomicBoolean();\n+\n+        InboundHandler inboundHandler = new InboundHandler() {\n+            @Override\n+            public void onBatch(InboundBatch batch, long remainingMemory) {\n+                // No-op.\n+            }\n+\n+            @Override\n+            public void sendFlowControl() {\n+                flowControlNotified.set(true);\n+            }\n+        };\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.singletonMap(1, inboundHandler),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        CountDownLatch startLatch = new CountDownLatch(1);\n+        CountDownLatch stopLatch = new CountDownLatch(1);\n+\n+        exec.setPayload(() -> {\n+            startLatch.countDown();\n+            stopLatch.await();\n+\n+            return IterationResult.FETCHED;\n+        });\n+\n+        assertTrue(fragmentExecutable.schedule());\n+        startLatch.await();\n+\n+        assertFalse(fragmentExecutable.schedule());\n+        stopLatch.countDown();\n+\n+        assertTrueEventually(() -> assertTrue(flowControlNotified.get()));\n+    }\n+\n+    /**\n+     * Concurrent test which submit messages from different threads and see if all of them are processed.\n+     */\n+    @Test\n+    public void testMessages() throws Exception {\n+        int repeatCount = 100;\n+\n+        for (int i = 0; i < repeatCount; i++) {\n+            // Prepare data structures for messages.\n+            ConcurrentLinkedQueue<Long> inboundQueue = new ConcurrentLinkedQueue<>();\n+            ConcurrentLinkedQueue<Long> outboundQueue = new ConcurrentLinkedQueue<>();\n+\n+            Set<Long> inboundSet = Collections.newSetFromMap(new ConcurrentHashMap<>());\n+            Set<Long> outboundSet = Collections.newSetFromMap(new ConcurrentHashMap<>());\n+\n+            InboundHandler inboundHandler = new InboundHandler() {\n+                @Override\n+                public void onBatch(InboundBatch batch, long remainingMemory) {\n+                    inboundQueue.add(remainingMemory);\n+                }\n+\n+                @Override\n+                public void sendFlowControl() {\n+                    // No-op.\n+                }\n+            };\n+\n+            OutboundHandler outboundHandler = outboundQueue::add;\n+\n+            // Prepare executable.\n+            QueryId queryId = QueryId.create(UUID.randomUUID());\n+            UUID callerId = UUID.randomUUID();\n+            int edgeId = 1;\n+\n+            pool = createPool();\n+\n+            TestStateCallback stateCallback = new TestStateCallback();\n+\n+            TestExec exec = new TestExec().setPayload(() -> {\n+                while (!inboundQueue.isEmpty()) {\n+                    inboundSet.add(inboundQueue.poll());\n+                }\n+\n+                while (!outboundQueue.isEmpty()) {\n+                    outboundSet.add(outboundQueue.poll());\n+                }\n+\n+                return IterationResult.FETCHED;\n+            });\n+\n+            QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+                stateCallback,\n+                Collections.emptyList(),\n+                exec,\n+                Collections.singletonMap(edgeId, inboundHandler),\n+                Collections.singletonMap(edgeId, Collections.singletonMap(callerId, outboundHandler)),\n+                pool\n+            );\n+\n+            assertEquals(1, fragmentExecutable.getInboxEdgeIds().size());\n+            assertEquals(1, fragmentExecutable.getOutboxEdgeIds().size());\n+            assertEquals(edgeId, (int) fragmentExecutable.getInboxEdgeIds().iterator().next());\n+            assertEquals(edgeId, (int) fragmentExecutable.getOutboxEdgeIds().iterator().next());\n+\n+            // Start threads and wait for completion.\n+            int operationCount = 10_000;\n+            int threadCount = 8;\n+\n+            AtomicInteger doneOperationCount = new AtomicInteger();\n+            CountDownLatch doneLatch = new CountDownLatch(threadCount);\n+\n+            Runnable run = () -> {\n+                try {\n+                    while (true) {\n+                        int counter = doneOperationCount.getAndIncrement();\n+\n+                        if (counter >= operationCount) {\n+                            break;\n+                        }\n+\n+                        QueryAbstractExchangeOperation operation;\n+\n+                        if (ThreadLocalRandom.current().nextBoolean()) {\n+                            operation = new QueryBatchExchangeOperation(queryId, edgeId, EmptyRowBatch.INSTANCE, false, counter);\n+                        } else {\n+                            operation = new QueryFlowControlExchangeOperation(queryId, edgeId, counter);\n+                        }\n+\n+                        operation.setCallerId(callerId);\n+\n+                        fragmentExecutable.addOperation(operation);\n+                        fragmentExecutable.schedule();\n+                    }\n+                } finally {\n+                    doneLatch.countDown();\n+                }\n+            };\n+\n+            for (int j = 0; j < threadCount; j++) {\n+                new Thread(run).start();\n+            }\n+\n+            doneLatch.await();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 308}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk5MzExNA==", "bodyText": "The release of this latch is part of the test infrastructure, not testing logic. Asserting test code doesn't seem natural. In any case, the timeout worker will cause thread interrupt anyway in case of bugs in the test itself, so we are safe here.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399993114", "createdAt": "2020-03-30T07:55:09Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/test/java/com/hazelcast/sql/impl/worker/QueryFragmentExecutableTest.java", "diffHunk": "@@ -0,0 +1,478 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.logging.NoLogFactory;\n+import com.hazelcast.sql.impl.QueryId;\n+import com.hazelcast.sql.impl.exec.AbstractExec;\n+import com.hazelcast.sql.impl.exec.IterationResult;\n+import com.hazelcast.sql.impl.fragment.QueryFragmentContext;\n+import com.hazelcast.sql.impl.mailbox.InboundBatch;\n+import com.hazelcast.sql.impl.mailbox.InboundHandler;\n+import com.hazelcast.sql.impl.mailbox.OutboundHandler;\n+import com.hazelcast.sql.impl.operation.QueryAbstractExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryBatchExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryFlowControlExchangeOperation;\n+import com.hazelcast.sql.impl.row.EmptyRowBatch;\n+import com.hazelcast.sql.impl.row.RowBatch;\n+import com.hazelcast.sql.impl.state.QueryStateCallback;\n+import com.hazelcast.test.HazelcastParallelClassRunner;\n+import com.hazelcast.test.HazelcastTestSupport;\n+import com.hazelcast.test.annotation.ParallelJVMTest;\n+import com.hazelcast.test.annotation.QuickTest;\n+import org.junit.After;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+\n+import java.util.Collections;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertSame;\n+import static org.junit.Assert.assertTrue;\n+\n+@RunWith(HazelcastParallelClassRunner.class)\n+@Category({QuickTest.class, ParallelJVMTest.class})\n+public class QueryFragmentExecutableTest extends HazelcastTestSupport {\n+\n+    private QueryFragmentWorkerPool pool;\n+\n+    @After\n+    public void after() {\n+        if (pool != null) {\n+            pool.stop();\n+\n+            pool = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testSetupIsCalledOnlyOnce() {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec().setPayload(new ResultExecPayload(IterationResult.FETCHED));\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.emptyMap(),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getSetupInvocationCount());\n+\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getSetupInvocationCount());\n+    }\n+\n+    @Test\n+    public void testAdvanceIsCalledUntilCompletion() {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec();\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.emptyMap(),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        exec.setPayload(new ResultExecPayload(IterationResult.WAIT));\n+        fragmentExecutable.run();\n+        fragmentExecutable.run();\n+        assertEquals(2, exec.getAdvanceInvocationCount());\n+\n+        exec.setPayload(new ResultExecPayload(IterationResult.FETCHED));\n+        fragmentExecutable.run();\n+        fragmentExecutable.run();\n+        assertEquals(4, exec.getAdvanceInvocationCount());\n+        assertEquals(0, stateCallback.getFragmentFinishedInvocationCount());\n+\n+        exec.setPayload(new ResultExecPayload(IterationResult.FETCHED_DONE));\n+        fragmentExecutable.run();\n+        fragmentExecutable.run();\n+        assertEquals(5, exec.getAdvanceInvocationCount());\n+        assertEquals(1, stateCallback.getFragmentFinishedInvocationCount());\n+    }\n+\n+    @Test\n+    public void testExceptionDuringExecution() {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec();\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.emptyMap(),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        // Throw an exception.\n+        exec.setPayload(new ExceptionExecPayload());\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getAdvanceInvocationCount());\n+        assertSame(ExceptionExecPayload.ERROR, stateCallback.getCancelException());\n+\n+        // Make sure that no advance is possible after that.\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getAdvanceInvocationCount());\n+        assertEquals(0, stateCallback.getFragmentFinishedInvocationCount());\n+    }\n+\n+    @Test\n+    public void testSchedule() throws Exception {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec();\n+\n+        AtomicBoolean flowControlNotified = new AtomicBoolean();\n+\n+        InboundHandler inboundHandler = new InboundHandler() {\n+            @Override\n+            public void onBatch(InboundBatch batch, long remainingMemory) {\n+                // No-op.\n+            }\n+\n+            @Override\n+            public void sendFlowControl() {\n+                flowControlNotified.set(true);\n+            }\n+        };\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.singletonMap(1, inboundHandler),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        CountDownLatch startLatch = new CountDownLatch(1);\n+        CountDownLatch stopLatch = new CountDownLatch(1);\n+\n+        exec.setPayload(() -> {\n+            startLatch.countDown();\n+            stopLatch.await();\n+\n+            return IterationResult.FETCHED;\n+        });\n+\n+        assertTrue(fragmentExecutable.schedule());\n+        startLatch.await();\n+\n+        assertFalse(fragmentExecutable.schedule());\n+        stopLatch.countDown();\n+\n+        assertTrueEventually(() -> assertTrue(flowControlNotified.get()));\n+    }\n+\n+    /**\n+     * Concurrent test which submit messages from different threads and see if all of them are processed.\n+     */\n+    @Test\n+    public void testMessages() throws Exception {\n+        int repeatCount = 100;\n+\n+        for (int i = 0; i < repeatCount; i++) {\n+            // Prepare data structures for messages.\n+            ConcurrentLinkedQueue<Long> inboundQueue = new ConcurrentLinkedQueue<>();\n+            ConcurrentLinkedQueue<Long> outboundQueue = new ConcurrentLinkedQueue<>();\n+\n+            Set<Long> inboundSet = Collections.newSetFromMap(new ConcurrentHashMap<>());\n+            Set<Long> outboundSet = Collections.newSetFromMap(new ConcurrentHashMap<>());\n+\n+            InboundHandler inboundHandler = new InboundHandler() {\n+                @Override\n+                public void onBatch(InboundBatch batch, long remainingMemory) {\n+                    inboundQueue.add(remainingMemory);\n+                }\n+\n+                @Override\n+                public void sendFlowControl() {\n+                    // No-op.\n+                }\n+            };\n+\n+            OutboundHandler outboundHandler = outboundQueue::add;\n+\n+            // Prepare executable.\n+            QueryId queryId = QueryId.create(UUID.randomUUID());\n+            UUID callerId = UUID.randomUUID();\n+            int edgeId = 1;\n+\n+            pool = createPool();\n+\n+            TestStateCallback stateCallback = new TestStateCallback();\n+\n+            TestExec exec = new TestExec().setPayload(() -> {\n+                while (!inboundQueue.isEmpty()) {\n+                    inboundSet.add(inboundQueue.poll());\n+                }\n+\n+                while (!outboundQueue.isEmpty()) {\n+                    outboundSet.add(outboundQueue.poll());\n+                }\n+\n+                return IterationResult.FETCHED;\n+            });\n+\n+            QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+                stateCallback,\n+                Collections.emptyList(),\n+                exec,\n+                Collections.singletonMap(edgeId, inboundHandler),\n+                Collections.singletonMap(edgeId, Collections.singletonMap(callerId, outboundHandler)),\n+                pool\n+            );\n+\n+            assertEquals(1, fragmentExecutable.getInboxEdgeIds().size());\n+            assertEquals(1, fragmentExecutable.getOutboxEdgeIds().size());\n+            assertEquals(edgeId, (int) fragmentExecutable.getInboxEdgeIds().iterator().next());\n+            assertEquals(edgeId, (int) fragmentExecutable.getOutboxEdgeIds().iterator().next());\n+\n+            // Start threads and wait for completion.\n+            int operationCount = 10_000;\n+            int threadCount = 8;\n+\n+            AtomicInteger doneOperationCount = new AtomicInteger();\n+            CountDownLatch doneLatch = new CountDownLatch(threadCount);\n+\n+            Runnable run = () -> {\n+                try {\n+                    while (true) {\n+                        int counter = doneOperationCount.getAndIncrement();\n+\n+                        if (counter >= operationCount) {\n+                            break;\n+                        }\n+\n+                        QueryAbstractExchangeOperation operation;\n+\n+                        if (ThreadLocalRandom.current().nextBoolean()) {\n+                            operation = new QueryBatchExchangeOperation(queryId, edgeId, EmptyRowBatch.INSTANCE, false, counter);\n+                        } else {\n+                            operation = new QueryFlowControlExchangeOperation(queryId, edgeId, counter);\n+                        }\n+\n+                        operation.setCallerId(callerId);\n+\n+                        fragmentExecutable.addOperation(operation);\n+                        fragmentExecutable.schedule();\n+                    }\n+                } finally {\n+                    doneLatch.countDown();\n+                }\n+            };\n+\n+            for (int j = 0; j < threadCount; j++) {\n+                new Thread(run).start();\n+            }\n+\n+            doneLatch.await();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk4MjA1Mw=="}, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 308}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MDM2MzM2OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/test/java/com/hazelcast/sql/impl/worker/QueryFragmentExecutableTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNzozNTo1OVrOF9dArw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwNzo1Njo0MVrOF9dsSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk4Mjc2Nw==", "bodyText": "The same", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399982767", "createdAt": "2020-03-30T07:35:59Z", "author": {"login": "petrpleshachkov"}, "path": "hazelcast/src/test/java/com/hazelcast/sql/impl/worker/QueryFragmentExecutableTest.java", "diffHunk": "@@ -0,0 +1,478 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.logging.NoLogFactory;\n+import com.hazelcast.sql.impl.QueryId;\n+import com.hazelcast.sql.impl.exec.AbstractExec;\n+import com.hazelcast.sql.impl.exec.IterationResult;\n+import com.hazelcast.sql.impl.fragment.QueryFragmentContext;\n+import com.hazelcast.sql.impl.mailbox.InboundBatch;\n+import com.hazelcast.sql.impl.mailbox.InboundHandler;\n+import com.hazelcast.sql.impl.mailbox.OutboundHandler;\n+import com.hazelcast.sql.impl.operation.QueryAbstractExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryBatchExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryFlowControlExchangeOperation;\n+import com.hazelcast.sql.impl.row.EmptyRowBatch;\n+import com.hazelcast.sql.impl.row.RowBatch;\n+import com.hazelcast.sql.impl.state.QueryStateCallback;\n+import com.hazelcast.test.HazelcastParallelClassRunner;\n+import com.hazelcast.test.HazelcastTestSupport;\n+import com.hazelcast.test.annotation.ParallelJVMTest;\n+import com.hazelcast.test.annotation.QuickTest;\n+import org.junit.After;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+\n+import java.util.Collections;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertSame;\n+import static org.junit.Assert.assertTrue;\n+\n+@RunWith(HazelcastParallelClassRunner.class)\n+@Category({QuickTest.class, ParallelJVMTest.class})\n+public class QueryFragmentExecutableTest extends HazelcastTestSupport {\n+\n+    private QueryFragmentWorkerPool pool;\n+\n+    @After\n+    public void after() {\n+        if (pool != null) {\n+            pool.stop();\n+\n+            pool = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testSetupIsCalledOnlyOnce() {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec().setPayload(new ResultExecPayload(IterationResult.FETCHED));\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.emptyMap(),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getSetupInvocationCount());\n+\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getSetupInvocationCount());\n+    }\n+\n+    @Test\n+    public void testAdvanceIsCalledUntilCompletion() {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec();\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.emptyMap(),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        exec.setPayload(new ResultExecPayload(IterationResult.WAIT));\n+        fragmentExecutable.run();\n+        fragmentExecutable.run();\n+        assertEquals(2, exec.getAdvanceInvocationCount());\n+\n+        exec.setPayload(new ResultExecPayload(IterationResult.FETCHED));\n+        fragmentExecutable.run();\n+        fragmentExecutable.run();\n+        assertEquals(4, exec.getAdvanceInvocationCount());\n+        assertEquals(0, stateCallback.getFragmentFinishedInvocationCount());\n+\n+        exec.setPayload(new ResultExecPayload(IterationResult.FETCHED_DONE));\n+        fragmentExecutable.run();\n+        fragmentExecutable.run();\n+        assertEquals(5, exec.getAdvanceInvocationCount());\n+        assertEquals(1, stateCallback.getFragmentFinishedInvocationCount());\n+    }\n+\n+    @Test\n+    public void testExceptionDuringExecution() {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec();\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.emptyMap(),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        // Throw an exception.\n+        exec.setPayload(new ExceptionExecPayload());\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getAdvanceInvocationCount());\n+        assertSame(ExceptionExecPayload.ERROR, stateCallback.getCancelException());\n+\n+        // Make sure that no advance is possible after that.\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getAdvanceInvocationCount());\n+        assertEquals(0, stateCallback.getFragmentFinishedInvocationCount());\n+    }\n+\n+    @Test\n+    public void testSchedule() throws Exception {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec();\n+\n+        AtomicBoolean flowControlNotified = new AtomicBoolean();\n+\n+        InboundHandler inboundHandler = new InboundHandler() {\n+            @Override\n+            public void onBatch(InboundBatch batch, long remainingMemory) {\n+                // No-op.\n+            }\n+\n+            @Override\n+            public void sendFlowControl() {\n+                flowControlNotified.set(true);\n+            }\n+        };\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.singletonMap(1, inboundHandler),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        CountDownLatch startLatch = new CountDownLatch(1);\n+        CountDownLatch stopLatch = new CountDownLatch(1);\n+\n+        exec.setPayload(() -> {\n+            startLatch.countDown();\n+            stopLatch.await();\n+\n+            return IterationResult.FETCHED;\n+        });\n+\n+        assertTrue(fragmentExecutable.schedule());\n+        startLatch.await();\n+\n+        assertFalse(fragmentExecutable.schedule());\n+        stopLatch.countDown();\n+\n+        assertTrueEventually(() -> assertTrue(flowControlNotified.get()));\n+    }\n+\n+    /**\n+     * Concurrent test which submit messages from different threads and see if all of them are processed.\n+     */\n+    @Test\n+    public void testMessages() throws Exception {\n+        int repeatCount = 100;\n+\n+        for (int i = 0; i < repeatCount; i++) {\n+            // Prepare data structures for messages.\n+            ConcurrentLinkedQueue<Long> inboundQueue = new ConcurrentLinkedQueue<>();\n+            ConcurrentLinkedQueue<Long> outboundQueue = new ConcurrentLinkedQueue<>();\n+\n+            Set<Long> inboundSet = Collections.newSetFromMap(new ConcurrentHashMap<>());\n+            Set<Long> outboundSet = Collections.newSetFromMap(new ConcurrentHashMap<>());\n+\n+            InboundHandler inboundHandler = new InboundHandler() {\n+                @Override\n+                public void onBatch(InboundBatch batch, long remainingMemory) {\n+                    inboundQueue.add(remainingMemory);\n+                }\n+\n+                @Override\n+                public void sendFlowControl() {\n+                    // No-op.\n+                }\n+            };\n+\n+            OutboundHandler outboundHandler = outboundQueue::add;\n+\n+            // Prepare executable.\n+            QueryId queryId = QueryId.create(UUID.randomUUID());\n+            UUID callerId = UUID.randomUUID();\n+            int edgeId = 1;\n+\n+            pool = createPool();\n+\n+            TestStateCallback stateCallback = new TestStateCallback();\n+\n+            TestExec exec = new TestExec().setPayload(() -> {\n+                while (!inboundQueue.isEmpty()) {\n+                    inboundSet.add(inboundQueue.poll());\n+                }\n+\n+                while (!outboundQueue.isEmpty()) {\n+                    outboundSet.add(outboundQueue.poll());\n+                }\n+\n+                return IterationResult.FETCHED;\n+            });\n+\n+            QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+                stateCallback,\n+                Collections.emptyList(),\n+                exec,\n+                Collections.singletonMap(edgeId, inboundHandler),\n+                Collections.singletonMap(edgeId, Collections.singletonMap(callerId, outboundHandler)),\n+                pool\n+            );\n+\n+            assertEquals(1, fragmentExecutable.getInboxEdgeIds().size());\n+            assertEquals(1, fragmentExecutable.getOutboxEdgeIds().size());\n+            assertEquals(edgeId, (int) fragmentExecutable.getInboxEdgeIds().iterator().next());\n+            assertEquals(edgeId, (int) fragmentExecutable.getOutboxEdgeIds().iterator().next());\n+\n+            // Start threads and wait for completion.\n+            int operationCount = 10_000;\n+            int threadCount = 8;\n+\n+            AtomicInteger doneOperationCount = new AtomicInteger();\n+            CountDownLatch doneLatch = new CountDownLatch(threadCount);\n+\n+            Runnable run = () -> {\n+                try {\n+                    while (true) {\n+                        int counter = doneOperationCount.getAndIncrement();\n+\n+                        if (counter >= operationCount) {\n+                            break;\n+                        }\n+\n+                        QueryAbstractExchangeOperation operation;\n+\n+                        if (ThreadLocalRandom.current().nextBoolean()) {\n+                            operation = new QueryBatchExchangeOperation(queryId, edgeId, EmptyRowBatch.INSTANCE, false, counter);\n+                        } else {\n+                            operation = new QueryFlowControlExchangeOperation(queryId, edgeId, counter);\n+                        }\n+\n+                        operation.setCallerId(callerId);\n+\n+                        fragmentExecutable.addOperation(operation);\n+                        fragmentExecutable.schedule();\n+                    }\n+                } finally {\n+                    doneLatch.countDown();\n+                }\n+            };\n+\n+            for (int j = 0; j < threadCount; j++) {\n+                new Thread(run).start();\n+            }\n+\n+            doneLatch.await();\n+\n+            // Make sure that all batches were drained.\n+            assertTrueEventually(() -> assertTrue(inboundQueue.isEmpty()));\n+            assertTrueEventually(() -> assertTrue(outboundQueue.isEmpty()));\n+\n+            // Make sure that the executor observed all batches.\n+            assertTrueEventually(() -> assertEquals(operationCount, inboundSet.size() + outboundSet.size()));\n+        }\n+    }\n+\n+    /**\n+     * Test that ensures propagation of cancel.\n+     */\n+    @Test\n+    public void testShutdown() throws Exception {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec();\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.emptyMap(),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        CountDownLatch startLatch = new CountDownLatch(1);\n+        CountDownLatch blockLatch = new CountDownLatch(1);\n+        AtomicBoolean interruptCaught = new AtomicBoolean();\n+\n+        exec.setPayload(() -> {\n+            startLatch.countDown();\n+\n+            try {\n+                blockLatch.await();\n+            } catch (InterruptedException e) {\n+                interruptCaught.set(true);\n+\n+                Thread.currentThread().interrupt();\n+\n+                throw e;\n+            }\n+\n+            return IterationResult.FETCHED;\n+        });\n+\n+        // Await exec is reached.\n+        assertTrue(fragmentExecutable.schedule());\n+        startLatch.await();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 360}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk5MzkzMQ==", "bodyText": "Likewise, this latch doesn't test any assumptions. Instead, it just positions the threads at proper source code lines.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r399993931", "createdAt": "2020-03-30T07:56:41Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/test/java/com/hazelcast/sql/impl/worker/QueryFragmentExecutableTest.java", "diffHunk": "@@ -0,0 +1,478 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.logging.NoLogFactory;\n+import com.hazelcast.sql.impl.QueryId;\n+import com.hazelcast.sql.impl.exec.AbstractExec;\n+import com.hazelcast.sql.impl.exec.IterationResult;\n+import com.hazelcast.sql.impl.fragment.QueryFragmentContext;\n+import com.hazelcast.sql.impl.mailbox.InboundBatch;\n+import com.hazelcast.sql.impl.mailbox.InboundHandler;\n+import com.hazelcast.sql.impl.mailbox.OutboundHandler;\n+import com.hazelcast.sql.impl.operation.QueryAbstractExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryBatchExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryFlowControlExchangeOperation;\n+import com.hazelcast.sql.impl.row.EmptyRowBatch;\n+import com.hazelcast.sql.impl.row.RowBatch;\n+import com.hazelcast.sql.impl.state.QueryStateCallback;\n+import com.hazelcast.test.HazelcastParallelClassRunner;\n+import com.hazelcast.test.HazelcastTestSupport;\n+import com.hazelcast.test.annotation.ParallelJVMTest;\n+import com.hazelcast.test.annotation.QuickTest;\n+import org.junit.After;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+\n+import java.util.Collections;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertSame;\n+import static org.junit.Assert.assertTrue;\n+\n+@RunWith(HazelcastParallelClassRunner.class)\n+@Category({QuickTest.class, ParallelJVMTest.class})\n+public class QueryFragmentExecutableTest extends HazelcastTestSupport {\n+\n+    private QueryFragmentWorkerPool pool;\n+\n+    @After\n+    public void after() {\n+        if (pool != null) {\n+            pool.stop();\n+\n+            pool = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testSetupIsCalledOnlyOnce() {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec().setPayload(new ResultExecPayload(IterationResult.FETCHED));\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.emptyMap(),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getSetupInvocationCount());\n+\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getSetupInvocationCount());\n+    }\n+\n+    @Test\n+    public void testAdvanceIsCalledUntilCompletion() {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec();\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.emptyMap(),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        exec.setPayload(new ResultExecPayload(IterationResult.WAIT));\n+        fragmentExecutable.run();\n+        fragmentExecutable.run();\n+        assertEquals(2, exec.getAdvanceInvocationCount());\n+\n+        exec.setPayload(new ResultExecPayload(IterationResult.FETCHED));\n+        fragmentExecutable.run();\n+        fragmentExecutable.run();\n+        assertEquals(4, exec.getAdvanceInvocationCount());\n+        assertEquals(0, stateCallback.getFragmentFinishedInvocationCount());\n+\n+        exec.setPayload(new ResultExecPayload(IterationResult.FETCHED_DONE));\n+        fragmentExecutable.run();\n+        fragmentExecutable.run();\n+        assertEquals(5, exec.getAdvanceInvocationCount());\n+        assertEquals(1, stateCallback.getFragmentFinishedInvocationCount());\n+    }\n+\n+    @Test\n+    public void testExceptionDuringExecution() {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec();\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.emptyMap(),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        // Throw an exception.\n+        exec.setPayload(new ExceptionExecPayload());\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getAdvanceInvocationCount());\n+        assertSame(ExceptionExecPayload.ERROR, stateCallback.getCancelException());\n+\n+        // Make sure that no advance is possible after that.\n+        fragmentExecutable.run();\n+        assertEquals(1, exec.getAdvanceInvocationCount());\n+        assertEquals(0, stateCallback.getFragmentFinishedInvocationCount());\n+    }\n+\n+    @Test\n+    public void testSchedule() throws Exception {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec();\n+\n+        AtomicBoolean flowControlNotified = new AtomicBoolean();\n+\n+        InboundHandler inboundHandler = new InboundHandler() {\n+            @Override\n+            public void onBatch(InboundBatch batch, long remainingMemory) {\n+                // No-op.\n+            }\n+\n+            @Override\n+            public void sendFlowControl() {\n+                flowControlNotified.set(true);\n+            }\n+        };\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.singletonMap(1, inboundHandler),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        CountDownLatch startLatch = new CountDownLatch(1);\n+        CountDownLatch stopLatch = new CountDownLatch(1);\n+\n+        exec.setPayload(() -> {\n+            startLatch.countDown();\n+            stopLatch.await();\n+\n+            return IterationResult.FETCHED;\n+        });\n+\n+        assertTrue(fragmentExecutable.schedule());\n+        startLatch.await();\n+\n+        assertFalse(fragmentExecutable.schedule());\n+        stopLatch.countDown();\n+\n+        assertTrueEventually(() -> assertTrue(flowControlNotified.get()));\n+    }\n+\n+    /**\n+     * Concurrent test which submit messages from different threads and see if all of them are processed.\n+     */\n+    @Test\n+    public void testMessages() throws Exception {\n+        int repeatCount = 100;\n+\n+        for (int i = 0; i < repeatCount; i++) {\n+            // Prepare data structures for messages.\n+            ConcurrentLinkedQueue<Long> inboundQueue = new ConcurrentLinkedQueue<>();\n+            ConcurrentLinkedQueue<Long> outboundQueue = new ConcurrentLinkedQueue<>();\n+\n+            Set<Long> inboundSet = Collections.newSetFromMap(new ConcurrentHashMap<>());\n+            Set<Long> outboundSet = Collections.newSetFromMap(new ConcurrentHashMap<>());\n+\n+            InboundHandler inboundHandler = new InboundHandler() {\n+                @Override\n+                public void onBatch(InboundBatch batch, long remainingMemory) {\n+                    inboundQueue.add(remainingMemory);\n+                }\n+\n+                @Override\n+                public void sendFlowControl() {\n+                    // No-op.\n+                }\n+            };\n+\n+            OutboundHandler outboundHandler = outboundQueue::add;\n+\n+            // Prepare executable.\n+            QueryId queryId = QueryId.create(UUID.randomUUID());\n+            UUID callerId = UUID.randomUUID();\n+            int edgeId = 1;\n+\n+            pool = createPool();\n+\n+            TestStateCallback stateCallback = new TestStateCallback();\n+\n+            TestExec exec = new TestExec().setPayload(() -> {\n+                while (!inboundQueue.isEmpty()) {\n+                    inboundSet.add(inboundQueue.poll());\n+                }\n+\n+                while (!outboundQueue.isEmpty()) {\n+                    outboundSet.add(outboundQueue.poll());\n+                }\n+\n+                return IterationResult.FETCHED;\n+            });\n+\n+            QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+                stateCallback,\n+                Collections.emptyList(),\n+                exec,\n+                Collections.singletonMap(edgeId, inboundHandler),\n+                Collections.singletonMap(edgeId, Collections.singletonMap(callerId, outboundHandler)),\n+                pool\n+            );\n+\n+            assertEquals(1, fragmentExecutable.getInboxEdgeIds().size());\n+            assertEquals(1, fragmentExecutable.getOutboxEdgeIds().size());\n+            assertEquals(edgeId, (int) fragmentExecutable.getInboxEdgeIds().iterator().next());\n+            assertEquals(edgeId, (int) fragmentExecutable.getOutboxEdgeIds().iterator().next());\n+\n+            // Start threads and wait for completion.\n+            int operationCount = 10_000;\n+            int threadCount = 8;\n+\n+            AtomicInteger doneOperationCount = new AtomicInteger();\n+            CountDownLatch doneLatch = new CountDownLatch(threadCount);\n+\n+            Runnable run = () -> {\n+                try {\n+                    while (true) {\n+                        int counter = doneOperationCount.getAndIncrement();\n+\n+                        if (counter >= operationCount) {\n+                            break;\n+                        }\n+\n+                        QueryAbstractExchangeOperation operation;\n+\n+                        if (ThreadLocalRandom.current().nextBoolean()) {\n+                            operation = new QueryBatchExchangeOperation(queryId, edgeId, EmptyRowBatch.INSTANCE, false, counter);\n+                        } else {\n+                            operation = new QueryFlowControlExchangeOperation(queryId, edgeId, counter);\n+                        }\n+\n+                        operation.setCallerId(callerId);\n+\n+                        fragmentExecutable.addOperation(operation);\n+                        fragmentExecutable.schedule();\n+                    }\n+                } finally {\n+                    doneLatch.countDown();\n+                }\n+            };\n+\n+            for (int j = 0; j < threadCount; j++) {\n+                new Thread(run).start();\n+            }\n+\n+            doneLatch.await();\n+\n+            // Make sure that all batches were drained.\n+            assertTrueEventually(() -> assertTrue(inboundQueue.isEmpty()));\n+            assertTrueEventually(() -> assertTrue(outboundQueue.isEmpty()));\n+\n+            // Make sure that the executor observed all batches.\n+            assertTrueEventually(() -> assertEquals(operationCount, inboundSet.size() + outboundSet.size()));\n+        }\n+    }\n+\n+    /**\n+     * Test that ensures propagation of cancel.\n+     */\n+    @Test\n+    public void testShutdown() throws Exception {\n+        pool = createPool();\n+\n+        TestStateCallback stateCallback = new TestStateCallback();\n+        TestExec exec = new TestExec();\n+\n+        QueryFragmentExecutable fragmentExecutable = new QueryFragmentExecutable(\n+            stateCallback,\n+            Collections.emptyList(),\n+            exec,\n+            Collections.emptyMap(),\n+            Collections.emptyMap(),\n+            pool\n+        );\n+\n+        CountDownLatch startLatch = new CountDownLatch(1);\n+        CountDownLatch blockLatch = new CountDownLatch(1);\n+        AtomicBoolean interruptCaught = new AtomicBoolean();\n+\n+        exec.setPayload(() -> {\n+            startLatch.countDown();\n+\n+            try {\n+                blockLatch.await();\n+            } catch (InterruptedException e) {\n+                interruptCaught.set(true);\n+\n+                Thread.currentThread().interrupt();\n+\n+                throw e;\n+            }\n+\n+            return IterationResult.FETCHED;\n+        });\n+\n+        // Await exec is reached.\n+        assertTrue(fragmentExecutable.schedule());\n+        startLatch.await();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTk4Mjc2Nw=="}, "originalCommit": {"oid": "5f5e1c9a28091110ce12f774f18fd2d4dfcb7dcb"}, "originalPosition": 360}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MTA0MzU5OnYy", "diffSide": "RIGHT", "path": "hazelcast/src/test/java/com/hazelcast/sql/impl/worker/QueryOperationWorkerPoolTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxMDozMjo1OVrOF9je9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNzoyNjo1NlrOF90ZQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDA4ODgyMQ==", "bodyText": "Not 100% sure, but looks like the test doesn't assert the order of operations, it asserts only that they were executed on the same thread. The order guarantee is essential for the pool, so it makes sense to verify it directly.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r400088821", "createdAt": "2020-03-30T10:32:59Z", "author": {"login": "taburet"}, "path": "hazelcast/src/test/java/com/hazelcast/sql/impl/worker/QueryOperationWorkerPoolTest.java", "diffHunk": "@@ -0,0 +1,307 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.internal.nio.Packet;\n+import com.hazelcast.internal.serialization.impl.DefaultSerializationServiceBuilder;\n+import com.hazelcast.logging.NoLogFactory;\n+import com.hazelcast.nio.ObjectDataInput;\n+import com.hazelcast.nio.ObjectDataOutput;\n+import com.hazelcast.nio.serialization.DataSerializable;\n+import com.hazelcast.sql.SqlErrorCode;\n+import com.hazelcast.sql.impl.QueryId;\n+import com.hazelcast.sql.impl.operation.QueryBatchExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryCancelOperation;\n+import com.hazelcast.sql.impl.operation.QueryExecuteOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperationHandler;\n+import com.hazelcast.sql.impl.row.HeapRow;\n+import com.hazelcast.sql.impl.row.ListRowBatch;\n+import com.hazelcast.test.HazelcastParallelClassRunner;\n+import com.hazelcast.test.HazelcastTestSupport;\n+import com.hazelcast.test.annotation.ParallelJVMTest;\n+import com.hazelcast.test.annotation.QuickTest;\n+import org.junit.After;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.LinkedBlockingQueue;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertSame;\n+import static org.junit.Assert.assertTrue;\n+\n+@RunWith(HazelcastParallelClassRunner.class)\n+@Category({QuickTest.class, ParallelJVMTest.class})\n+public class QueryOperationWorkerPoolTest extends HazelcastTestSupport {\n+\n+    private static final int THREAD_COUNT = 4;\n+\n+    private QueryOperationWorkerPool pool;\n+\n+    @After\n+    public void after() {\n+        if (pool != null) {\n+            pool.stop();\n+\n+            pool = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testSubmitLocal() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa9606f6a17c5a1df5f735431282d4989d9c5e4c"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDM2NTg5MA==", "bodyText": "Improved the tests to check for order.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r400365890", "createdAt": "2020-03-30T17:26:56Z", "author": {"login": "devozerov"}, "path": "hazelcast/src/test/java/com/hazelcast/sql/impl/worker/QueryOperationWorkerPoolTest.java", "diffHunk": "@@ -0,0 +1,307 @@\n+/*\n+ * Copyright (c) 2008-2020, Hazelcast, Inc. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.hazelcast.sql.impl.worker;\n+\n+import com.hazelcast.internal.nio.Packet;\n+import com.hazelcast.internal.serialization.impl.DefaultSerializationServiceBuilder;\n+import com.hazelcast.logging.NoLogFactory;\n+import com.hazelcast.nio.ObjectDataInput;\n+import com.hazelcast.nio.ObjectDataOutput;\n+import com.hazelcast.nio.serialization.DataSerializable;\n+import com.hazelcast.sql.SqlErrorCode;\n+import com.hazelcast.sql.impl.QueryId;\n+import com.hazelcast.sql.impl.operation.QueryBatchExchangeOperation;\n+import com.hazelcast.sql.impl.operation.QueryCancelOperation;\n+import com.hazelcast.sql.impl.operation.QueryExecuteOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperation;\n+import com.hazelcast.sql.impl.operation.QueryOperationHandler;\n+import com.hazelcast.sql.impl.row.HeapRow;\n+import com.hazelcast.sql.impl.row.ListRowBatch;\n+import com.hazelcast.test.HazelcastParallelClassRunner;\n+import com.hazelcast.test.HazelcastTestSupport;\n+import com.hazelcast.test.annotation.ParallelJVMTest;\n+import com.hazelcast.test.annotation.QuickTest;\n+import org.junit.After;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+import org.junit.runner.RunWith;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.LinkedBlockingQueue;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertSame;\n+import static org.junit.Assert.assertTrue;\n+\n+@RunWith(HazelcastParallelClassRunner.class)\n+@Category({QuickTest.class, ParallelJVMTest.class})\n+public class QueryOperationWorkerPoolTest extends HazelcastTestSupport {\n+\n+    private static final int THREAD_COUNT = 4;\n+\n+    private QueryOperationWorkerPool pool;\n+\n+    @After\n+    public void after() {\n+        if (pool != null) {\n+            pool.stop();\n+\n+            pool = null;\n+        }\n+    }\n+\n+    @Test\n+    public void testSubmitLocal() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDA4ODgyMQ=="}, "originalCommit": {"oid": "aa9606f6a17c5a1df5f735431282d4989d9c5e4c"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MTEwMzAyOnYy", "diffSide": "RIGHT", "path": "docs/design/sql/04-mulitthreaded-execution.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxMDo1MDowM1rOF9kDQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNzo0MzozM1rOF91DJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDA5ODExNQ==", "bodyText": "There is one potential issue with ForkJoinPool. If I understand its internals correctly, under the load and uneven task sizes the pool prefers new task submissions over the old ones. Consider a pool of two queues with a constant flow of new submissions, the thread associated with the first queue is busy with the execution of some big task, the second thread finishes with another task execution, inspects its local queue, there is always something in it, since we have a constant flow of submissions, the tasks submitted to the first queue (potentially long time ago) are postponed.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r400098115", "createdAt": "2020-03-30T10:50:03Z", "author": {"login": "taburet"}, "path": "docs/design/sql/04-mulitthreaded-execution.md", "diffHunk": "@@ -0,0 +1,215 @@\n+# SQL Multithreaded Execution\n+\n+## Overview\n+\n+The Hazelcast Mustang engine executes queries in parallel. This document describes the design of the multithreaded execution\n+environment of the engine.\n+\n+The document doesn't discuss operator-level parallelism, which is a different topic.\n+\n+The rest of this document is organized as follows. In section 1 we discuss the existing threading infrastructure of Hazelcast\n+IMDG and Hazelcast Jet. In section 2 we analyze why the existing infrastructure is inappropriate for query execution and then\n+describe the design of the Hazelcast Mustang execution environment. In section 3 we discuss possible alternative approaches\n+that were rejected.\n+\n+## 1 Existing Infrastructure\n+\n+Hazelcast IMDG uses staged event-driven architecture (SEDA) for message processing. During execution, a message passes through\n+several thread pools (stages), each optimized for a specific type of workload. We now describe stages that exist in Hazelcast.\n+\n+### 1.1 IO Pool\n+\n+Hazelcast uses a dedicated thread pool for message send and receive, which will be referred to as **IO Pool** in this paper.\n+Each thread from the IO pool maintains a subset of connections to remote members. Consider that we have a sender member (S)\n+and a receiver member (R). The typical execution flow is organized as follows:\n+1. The message is added to the queue of a single IO thread, and the thread is notified\n+1. The sender IO thread wakes up and sends the message over the network\n+1. A receiver IO thread is notified by the operating system on receive\n+1. The receiver IO thread wakes up, determines the next execution stage, adds the message to the stage's queue and notifies the\n+stage\n+1. The next execution stage processes the message\n+\n+*Snippet 1: Message execution flow*\n+```\n+Stage(S)                 IO(S)        IO(R)                Stage(R)\n+   |----enqueue/notify->--|            |                      |\n+   |                      |----send->--|                      |\n+   |                      |            |----enqueue/notify->--|\n+```\n+\n+We now discuss the organization of different execution stages.\n+\n+### 1.2 Partition Pool\n+\n+A message may have a logical **partition**, which is a positive integer number. Messages with defined partition are routed to\n+a special thread pool, which we refer to as **partition pool**. The pool has several threads. Every thread has a dedicated task\n+queue. Partition of the message is used to determine the exact thread which will process the message:\n+`threadIndex = partition % threadCount`.\n+\n+The partition pool has the following advantages:\n+1. Only one thread processes messages with the given partition so that processing logic may use less synchronization\n+1. Dedicated thread queues reduce contention on enqueue/deque operations\n+\n+The partition pool has the following disadvantage:\n+1. There is no balancing between threads: a single long-running task may delay other tasks from the same partition\n+indefinitely; likewise, an imbalance between partitions may cause resource underutilization\n+\n+The partition pool is thus most suitable for small tasks that operate on independent physical resources, and that are\n+distributed equally between logical partitions. An example is `IMap` operations, which operate on separate physical\n+partitions, such as `GET` and `PUT`.\n+\n+Since the partition is a logical notion, it is possible to multiplex tasks from different components to a single partition pool.\n+For example, CP Subsystem schedules tasks, all with the same partition, to the partition pool to ensure total processing order.\n+\n+### 1.3 Generic Pool\n+\n+If a message doesn't have a logical partition, it is submitted to the **generic pool**. This is a conventional thread pool with\n+a shared blocking queue. It has inherent balancing capabilities. But at the same time, this pool may demonstrate less than\n+optimal throughput when a lot of small tasks are submitted due to contention on the queue.\n+\n+### 1.4 Hazelcast Jet Pool\n+\n+Hazelcast Jet uses its own cooperative pool to execute Jet jobs. Every thread has its own queue of jobs that are executed\n+cooperatively. There is no balancing: once the job is submitted to a specific thread, it is always executed in that thread.\n+\n+IO pool doesn't notify the Jet pool about new data batch (\"push\"). Instead, the message is just enqueued, and the Jet thread\n+checks the queue periodically (\"poll\").\n+\n+## 2 Design\n+\n+We now define the requirements to Hazelcast Mustang threading model, analyze them concerning existing infrastructure, and\n+define the design.\n+\n+### 2.1 Requirements\n+\n+The requirements are thread safety, load balancing, and ordered message processing.\n+\n+First, the infrastructure must guarantee that operator execution is thread-safe. That is, the stateful operator should not be\n+executed by multiple threads simultaneously. This simplifies operator implementations and makes them more performant.\n+Hazelcast Jet follows this principle, as only one thread may execute a particular job. However, Hazelcast Jet pool doesn't\n+satisfy the load balancing requirement discussed below.\n+\n+Second, the execution environment must support load balancing. Query execution may take a long time to complete. If several query\n+fragments have been assigned to a single execution thread, it should be possible to reassign them to idle threads dynamically.\n+Hence neither partition pool nor Hazelcast Jet pool designs are applicable to Hazelcast Mustang because they lack balancing\n+capabilities.\n+\n+Third, it should be possible to execute some tasks in order. That is, if task `A` is received before task `B`, then it\n+should be executed before `B`. It is always possible to implement an ordering guarantee with the help of additional\n+synchronization primitives, but it increases complexity and typically reduces performance. So we prefer to have a threading\n+infrastructure with ordering guarantees, such as in the partition pool. Examples of tasks requiring ordered processing:\n+1. Query cancel should be executed after query start to minimize resource leaks\n+1. The N-th batch from the stream should be processed before the (N+1)-th batch, as described in [[1]] (p. 1.3)\n+\n+### 2.1 General Design\n+\n+We define the taxonomy of tasks related to query execution.\n+\n+First, the engine must execute query fragments, i.e., advance Volcano-style operators, as explained in [[2]] (p. 3). Fragment\n+execution is initiated in response to query start or data messages. Fragment execution may take significant time to complete.\n+\n+Second, the engine must process query operations described in [[1]] (p. 3), such as query start, query cancel, batch arrival,\n+and flow control. These operations take short time to complete. Moreover, query start operation may trigger the parallel\n+execution of several fragments.\n+\n+Given the different nature of query operations and fragment execution, we split them into independent stages, called\n+**operation pool** and **fragment pool**. The former executes query operations, and the latter executes fragments. This design\n+provides a clear separation of concerns and allows us to optimize stages for their tasks as described below, which improves\n+performance. On the other hand, this design introduces an additional thread notification, as shown in the snippet below, which\n+may negatively affect performance. Nevertheless, we think that the advantages of this approach outweigh the disadvantages.\n+\n+*Snippet 2: Query start flow (receiver only)*\n+```\n+IO               Operation pool         Fragment pool\n+|----enqueue/notify-->-|                      |\n+|                      |----enqueue/notify-->-|\n+```\n+\n+### 2.2 Operation Pool\n+\n+Every network message received by the IO pool is first submitted to the operation pool.\n+\n+The pool is organized as a fixed pool of threads with dedicated per-thread queues. Dedicated queues reduce contention and\n+increase the throughput, which is important given that processing of a single message takes little time.\n+\n+Every message may define an optional logical partition. If the partition is not defined, a random thread is picked for message\n+processing. If the partition is defined, the index of executing thread is defined as `threadIndex = partition % threadCount`.\n+Two messages with the same partition are guaranteed to be processed by the same thread, thus providing ordering guarantees.\n+\n+Some messages may trigger query fragment execution, as described below. In this case, a fragment execution task is created and\n+submitted for execution to the fragment pool.\n+1. `execute` - triggers the execution of one or more fragments\n+1. `batch` and `flow_control` - trigger execution of one fragment defined by query ID and edge ID, as described in [[1]]\n+\n+### 2.3 Fragment Pool\n+\n+The fragment pool is responsible for the execution of individual query fragments. Fragment execution is always initiated by\n+a task from the operation pool.\n+\n+Unlike operations, fragment execution may take arbitrary time depending on the query structure. It is, therefore, important to\n+guarantee high throughput for short fragments, while still providing load balancing for long fragments. The ideal candidate\n+is a thread pool with dedicated per-thread queues and work-stealing. For this reason, we choose JDK's `ForkJoinPool` as a", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa9606f6a17c5a1df5f735431282d4989d9c5e4c"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDM3NjYxMg==", "bodyText": "We do not consider any kind of prioritization at the moment. The system is optimized for throughput, so if there is a constant flow of tasks, and the older task is executed after the newer ones, it is still ok, because it was postponed due to another work of equal priority.\nIf this will be the problem in the future, we may consider cooperative interrupts, where a task finishes its execution after some time.", "url": "https://github.com/hazelcast/hazelcast/pull/16806#discussion_r400376612", "createdAt": "2020-03-30T17:43:33Z", "author": {"login": "devozerov"}, "path": "docs/design/sql/04-mulitthreaded-execution.md", "diffHunk": "@@ -0,0 +1,215 @@\n+# SQL Multithreaded Execution\n+\n+## Overview\n+\n+The Hazelcast Mustang engine executes queries in parallel. This document describes the design of the multithreaded execution\n+environment of the engine.\n+\n+The document doesn't discuss operator-level parallelism, which is a different topic.\n+\n+The rest of this document is organized as follows. In section 1 we discuss the existing threading infrastructure of Hazelcast\n+IMDG and Hazelcast Jet. In section 2 we analyze why the existing infrastructure is inappropriate for query execution and then\n+describe the design of the Hazelcast Mustang execution environment. In section 3 we discuss possible alternative approaches\n+that were rejected.\n+\n+## 1 Existing Infrastructure\n+\n+Hazelcast IMDG uses staged event-driven architecture (SEDA) for message processing. During execution, a message passes through\n+several thread pools (stages), each optimized for a specific type of workload. We now describe stages that exist in Hazelcast.\n+\n+### 1.1 IO Pool\n+\n+Hazelcast uses a dedicated thread pool for message send and receive, which will be referred to as **IO Pool** in this paper.\n+Each thread from the IO pool maintains a subset of connections to remote members. Consider that we have a sender member (S)\n+and a receiver member (R). The typical execution flow is organized as follows:\n+1. The message is added to the queue of a single IO thread, and the thread is notified\n+1. The sender IO thread wakes up and sends the message over the network\n+1. A receiver IO thread is notified by the operating system on receive\n+1. The receiver IO thread wakes up, determines the next execution stage, adds the message to the stage's queue and notifies the\n+stage\n+1. The next execution stage processes the message\n+\n+*Snippet 1: Message execution flow*\n+```\n+Stage(S)                 IO(S)        IO(R)                Stage(R)\n+   |----enqueue/notify->--|            |                      |\n+   |                      |----send->--|                      |\n+   |                      |            |----enqueue/notify->--|\n+```\n+\n+We now discuss the organization of different execution stages.\n+\n+### 1.2 Partition Pool\n+\n+A message may have a logical **partition**, which is a positive integer number. Messages with defined partition are routed to\n+a special thread pool, which we refer to as **partition pool**. The pool has several threads. Every thread has a dedicated task\n+queue. Partition of the message is used to determine the exact thread which will process the message:\n+`threadIndex = partition % threadCount`.\n+\n+The partition pool has the following advantages:\n+1. Only one thread processes messages with the given partition so that processing logic may use less synchronization\n+1. Dedicated thread queues reduce contention on enqueue/deque operations\n+\n+The partition pool has the following disadvantage:\n+1. There is no balancing between threads: a single long-running task may delay other tasks from the same partition\n+indefinitely; likewise, an imbalance between partitions may cause resource underutilization\n+\n+The partition pool is thus most suitable for small tasks that operate on independent physical resources, and that are\n+distributed equally between logical partitions. An example is `IMap` operations, which operate on separate physical\n+partitions, such as `GET` and `PUT`.\n+\n+Since the partition is a logical notion, it is possible to multiplex tasks from different components to a single partition pool.\n+For example, CP Subsystem schedules tasks, all with the same partition, to the partition pool to ensure total processing order.\n+\n+### 1.3 Generic Pool\n+\n+If a message doesn't have a logical partition, it is submitted to the **generic pool**. This is a conventional thread pool with\n+a shared blocking queue. It has inherent balancing capabilities. But at the same time, this pool may demonstrate less than\n+optimal throughput when a lot of small tasks are submitted due to contention on the queue.\n+\n+### 1.4 Hazelcast Jet Pool\n+\n+Hazelcast Jet uses its own cooperative pool to execute Jet jobs. Every thread has its own queue of jobs that are executed\n+cooperatively. There is no balancing: once the job is submitted to a specific thread, it is always executed in that thread.\n+\n+IO pool doesn't notify the Jet pool about new data batch (\"push\"). Instead, the message is just enqueued, and the Jet thread\n+checks the queue periodically (\"poll\").\n+\n+## 2 Design\n+\n+We now define the requirements to Hazelcast Mustang threading model, analyze them concerning existing infrastructure, and\n+define the design.\n+\n+### 2.1 Requirements\n+\n+The requirements are thread safety, load balancing, and ordered message processing.\n+\n+First, the infrastructure must guarantee that operator execution is thread-safe. That is, the stateful operator should not be\n+executed by multiple threads simultaneously. This simplifies operator implementations and makes them more performant.\n+Hazelcast Jet follows this principle, as only one thread may execute a particular job. However, Hazelcast Jet pool doesn't\n+satisfy the load balancing requirement discussed below.\n+\n+Second, the execution environment must support load balancing. Query execution may take a long time to complete. If several query\n+fragments have been assigned to a single execution thread, it should be possible to reassign them to idle threads dynamically.\n+Hence neither partition pool nor Hazelcast Jet pool designs are applicable to Hazelcast Mustang because they lack balancing\n+capabilities.\n+\n+Third, it should be possible to execute some tasks in order. That is, if task `A` is received before task `B`, then it\n+should be executed before `B`. It is always possible to implement an ordering guarantee with the help of additional\n+synchronization primitives, but it increases complexity and typically reduces performance. So we prefer to have a threading\n+infrastructure with ordering guarantees, such as in the partition pool. Examples of tasks requiring ordered processing:\n+1. Query cancel should be executed after query start to minimize resource leaks\n+1. The N-th batch from the stream should be processed before the (N+1)-th batch, as described in [[1]] (p. 1.3)\n+\n+### 2.1 General Design\n+\n+We define the taxonomy of tasks related to query execution.\n+\n+First, the engine must execute query fragments, i.e., advance Volcano-style operators, as explained in [[2]] (p. 3). Fragment\n+execution is initiated in response to query start or data messages. Fragment execution may take significant time to complete.\n+\n+Second, the engine must process query operations described in [[1]] (p. 3), such as query start, query cancel, batch arrival,\n+and flow control. These operations take short time to complete. Moreover, query start operation may trigger the parallel\n+execution of several fragments.\n+\n+Given the different nature of query operations and fragment execution, we split them into independent stages, called\n+**operation pool** and **fragment pool**. The former executes query operations, and the latter executes fragments. This design\n+provides a clear separation of concerns and allows us to optimize stages for their tasks as described below, which improves\n+performance. On the other hand, this design introduces an additional thread notification, as shown in the snippet below, which\n+may negatively affect performance. Nevertheless, we think that the advantages of this approach outweigh the disadvantages.\n+\n+*Snippet 2: Query start flow (receiver only)*\n+```\n+IO               Operation pool         Fragment pool\n+|----enqueue/notify-->-|                      |\n+|                      |----enqueue/notify-->-|\n+```\n+\n+### 2.2 Operation Pool\n+\n+Every network message received by the IO pool is first submitted to the operation pool.\n+\n+The pool is organized as a fixed pool of threads with dedicated per-thread queues. Dedicated queues reduce contention and\n+increase the throughput, which is important given that processing of a single message takes little time.\n+\n+Every message may define an optional logical partition. If the partition is not defined, a random thread is picked for message\n+processing. If the partition is defined, the index of executing thread is defined as `threadIndex = partition % threadCount`.\n+Two messages with the same partition are guaranteed to be processed by the same thread, thus providing ordering guarantees.\n+\n+Some messages may trigger query fragment execution, as described below. In this case, a fragment execution task is created and\n+submitted for execution to the fragment pool.\n+1. `execute` - triggers the execution of one or more fragments\n+1. `batch` and `flow_control` - trigger execution of one fragment defined by query ID and edge ID, as described in [[1]]\n+\n+### 2.3 Fragment Pool\n+\n+The fragment pool is responsible for the execution of individual query fragments. Fragment execution is always initiated by\n+a task from the operation pool.\n+\n+Unlike operations, fragment execution may take arbitrary time depending on the query structure. It is, therefore, important to\n+guarantee high throughput for short fragments, while still providing load balancing for long fragments. The ideal candidate\n+is a thread pool with dedicated per-thread queues and work-stealing. For this reason, we choose JDK's `ForkJoinPool` as a", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDA5ODExNQ=="}, "originalCommit": {"oid": "aa9606f6a17c5a1df5f735431282d4989d9c5e4c"}, "originalPosition": 151}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 774, "cost": 1, "resetAt": "2021-11-12T20:28:25Z"}}}