{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk1ODY1ODAw", "number": 4686, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxMjoyMjo1N1rODtFSLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxMjoyMzo0N1rODtFThA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NTk5MDg1OnYy", "diffSide": "RIGHT", "path": "timelock-server/src/suiteTest/java/com/palantir/atlasdb/timelock/MultiNodePaxosTimeLockServerIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxMjoyMjo1N1rOF-TGhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxMjo0OToyNlrOF-UIVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDg2ODk5OA==", "bodyText": "This test will fail even if the buildup occurs in the warmup period. Maybe at least indicate in the reason for failure if it occurred during warmup or after we simulated shutting down?", "url": "https://github.com/palantir/atlasdb/pull/4686#discussion_r400868998", "createdAt": "2020-03-31T12:22:57Z", "author": {"login": "gmaretic"}, "path": "timelock-server/src/suiteTest/java/com/palantir/atlasdb/timelock/MultiNodePaxosTimeLockServerIntegrationTest.java", "diffHunk": "@@ -282,6 +285,30 @@ public void multipleLockRequestsWithTheSameIdAreGranted() {\n         }\n     }\n \n+    @Test\n+    @Ignore // TODO (jkong): Fix this test by reworking the threading model.\n+    public void stressTest() {\n+        TestableTimelockServer nonLeader = Iterables.getFirst(cluster.nonLeaders(client.namespace()).values(), null);\n+        int startingNumThreads = ManagementFactory.getThreadMXBean().getThreadCount();\n+        try {\n+            for (int i = 0; i < 10_000; i++) { // Needed as it takes a while for the thread buildup to occur\n+                client.getFreshTimestamp();\n+                assertThat(ManagementFactory.getThreadMXBean().getThreadCount())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09e6bcded6b01dfc022d1ee16c693a325d694b7a"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDg4NTg0Nw==", "bodyText": "Alright, will do. Yeah, that seems unlikely, but good to catch :)", "url": "https://github.com/palantir/atlasdb/pull/4686#discussion_r400885847", "createdAt": "2020-03-31T12:49:26Z", "author": {"login": "jeremyk-91"}, "path": "timelock-server/src/suiteTest/java/com/palantir/atlasdb/timelock/MultiNodePaxosTimeLockServerIntegrationTest.java", "diffHunk": "@@ -282,6 +285,30 @@ public void multipleLockRequestsWithTheSameIdAreGranted() {\n         }\n     }\n \n+    @Test\n+    @Ignore // TODO (jkong): Fix this test by reworking the threading model.\n+    public void stressTest() {\n+        TestableTimelockServer nonLeader = Iterables.getFirst(cluster.nonLeaders(client.namespace()).values(), null);\n+        int startingNumThreads = ManagementFactory.getThreadMXBean().getThreadCount();\n+        try {\n+            for (int i = 0; i < 10_000; i++) { // Needed as it takes a while for the thread buildup to occur\n+                client.getFreshTimestamp();\n+                assertThat(ManagementFactory.getThreadMXBean().getThreadCount())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDg2ODk5OA=="}, "originalCommit": {"oid": "09e6bcded6b01dfc022d1ee16c693a325d694b7a"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4NTk5NDI4OnYy", "diffSide": "RIGHT", "path": "timelock-server/src/suiteTest/java/com/palantir/atlasdb/timelock/MultiNodePaxosTimeLockServerIntegrationTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxMjoyMzo0N1rOF-TIjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQxMjo0OTozMVrOF-UIhg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDg2OTUxNg==", "bodyText": "Extract a method indicating that this simulates the node shutting down?", "url": "https://github.com/palantir/atlasdb/pull/4686#discussion_r400869516", "createdAt": "2020-03-31T12:23:47Z", "author": {"login": "gmaretic"}, "path": "timelock-server/src/suiteTest/java/com/palantir/atlasdb/timelock/MultiNodePaxosTimeLockServerIntegrationTest.java", "diffHunk": "@@ -282,6 +285,30 @@ public void multipleLockRequestsWithTheSameIdAreGranted() {\n         }\n     }\n \n+    @Test\n+    @Ignore // TODO (jkong): Fix this test by reworking the threading model.\n+    public void stressTest() {\n+        TestableTimelockServer nonLeader = Iterables.getFirst(cluster.nonLeaders(client.namespace()).values(), null);\n+        int startingNumThreads = ManagementFactory.getThreadMXBean().getThreadCount();\n+        try {\n+            for (int i = 0; i < 10_000; i++) { // Needed as it takes a while for the thread buildup to occur\n+                client.getFreshTimestamp();\n+                assertThat(ManagementFactory.getThreadMXBean().getThreadCount())\n+                        .as(\"should not additionally spin up too many threads\")\n+                        .isLessThanOrEqualTo(startingNumThreads + 200);\n+                if (i == 1_000) {\n+                    nonLeader.serverHolder().wireMock().register(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09e6bcded6b01dfc022d1ee16c693a325d694b7a"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDg4NTg5NA==", "bodyText": "Will do!", "url": "https://github.com/palantir/atlasdb/pull/4686#discussion_r400885894", "createdAt": "2020-03-31T12:49:31Z", "author": {"login": "jeremyk-91"}, "path": "timelock-server/src/suiteTest/java/com/palantir/atlasdb/timelock/MultiNodePaxosTimeLockServerIntegrationTest.java", "diffHunk": "@@ -282,6 +285,30 @@ public void multipleLockRequestsWithTheSameIdAreGranted() {\n         }\n     }\n \n+    @Test\n+    @Ignore // TODO (jkong): Fix this test by reworking the threading model.\n+    public void stressTest() {\n+        TestableTimelockServer nonLeader = Iterables.getFirst(cluster.nonLeaders(client.namespace()).values(), null);\n+        int startingNumThreads = ManagementFactory.getThreadMXBean().getThreadCount();\n+        try {\n+            for (int i = 0; i < 10_000; i++) { // Needed as it takes a while for the thread buildup to occur\n+                client.getFreshTimestamp();\n+                assertThat(ManagementFactory.getThreadMXBean().getThreadCount())\n+                        .as(\"should not additionally spin up too many threads\")\n+                        .isLessThanOrEqualTo(startingNumThreads + 200);\n+                if (i == 1_000) {\n+                    nonLeader.serverHolder().wireMock().register(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDg2OTUxNg=="}, "originalCommit": {"oid": "09e6bcded6b01dfc022d1ee16c693a325d694b7a"}, "originalPosition": 64}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3026, "cost": 1, "resetAt": "2021-11-12T12:57:47Z"}}}