{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcxNTY5MjUw", "number": 4552, "title": "[PDS-109527 FLUP] TimeLock Invariant Enforcement Part 1: Only One Leader Allowed", "bodyText": "Goals (and why):\n\nTimeLock should be able to detect its corruption reasonably quickly and stop itself.\nOf course, any such mechanism needs to avoid false positives.\n\nImplementation Description (bullets):\n\nWhenever timelock checks its health, if it detects that there were multiple leaders for a given client C, it performs the following:\n\nit requests a fresh timestamp for the namespace C on each service node, in the order S1, S2 ... Sn\nit then verifies that if multiple timestamps were returned, S1 < S2 < ... < Sn, and S1 > Q where Q was the last value of Sn - if this fails, we declare an error state\nif only one node replies, we accept that timelock is consistent\notherwise, we try again; after five cases where multiple nodes reply and everything is consistent, we give up: the cluster might legitimately be heavily loaded with leader elections, and there's no reason to believe that there is corruption anyway.\n\n\nThe aforementioned checks take place on a separate thread, to avoid having health checks block for long amounts of time.\n\nTesting (What was existing testing like?  What have you done to improve it?):\n\nNew classes have unit tests added.\nI manually verified that removing the leadership proxy from TimeLock will cause the error message to be printed in MultiNodePaxosTimeLockServerIntegrationTest, though in general testing this is messy.\n\nConcerns (what feedback would you like?):\n\nIt is possible for verifications to queue up in the event that a node is very slow to respond (along the lines of the client lock-ups ticket). Could this be a problem? Should the executor have a bounded work queue, and/or coalesce results for each client? Is this too much work?\nIn the case where we don't have multiple leaders, might it make sense to test a random client? Possibly not in the scope of this PR.\n\nWhere should we start reviewing?: NoSimultaneousServiceCheck\nPriority (whenever / two weeks / yesterday): this week or early next week.", "createdAt": "2020-02-05T20:26:09Z", "url": "https://github.com/palantir/atlasdb/pull/4552", "merged": true, "mergeCommit": {"oid": "6991f66c569db5388a8b77659dd39cec25962781"}, "closed": true, "closedAt": "2020-02-07T14:42:17Z", "author": {"login": "jeremyk-91"}, "timelineItems": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb_dSNvAH2gAyMzcxNTY5MjUwOjYyMWY1Zjc3NjY0NzhkODUxYWViMzZlZmM5NDliMTIzNjljYTkyMzg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcCANZlAFqTM1NTE5MTMzNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "621f5f7766478d851aeb36efc949b12369ca9238", "author": {"user": {"login": "jeremyk-91", "name": "Jeremy Kong"}}, "url": "https://github.com/palantir/atlasdb/commit/621f5f7766478d851aeb36efc949b12369ca9238", "committedDate": "2020-01-30T16:39:18Z", "message": "Invariants"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "05a665172a2cb7efcbe60f096b55289848214ea9", "author": {"user": {"login": "jeremyk-91", "name": "Jeremy Kong"}}, "url": "https://github.com/palantir/atlasdb/commit/05a665172a2cb7efcbe60f096b55289848214ea9", "committedDate": "2020-02-04T21:34:14Z", "message": "stash"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2314f1eedff796d9bebf94cb40ef907501481f96", "author": {"user": {"login": "jeremyk-91", "name": "Jeremy Kong"}}, "url": "https://github.com/palantir/atlasdb/commit/2314f1eedff796d9bebf94cb40ef907501481f96", "committedDate": "2020-02-05T11:25:27Z", "message": "Wire up check"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5bfc940bbca8a9c17832d234d7c7380137dfaf95", "author": {"user": {"login": "jeremyk-91", "name": "Jeremy Kong"}}, "url": "https://github.com/palantir/atlasdb/commit/5bfc940bbca8a9c17832d234d7c7380137dfaf95", "committedDate": "2020-02-05T11:33:38Z", "message": "Checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "64f884d8c7a6a24e6830c67a65657b1d87c68458", "author": {"user": {"login": "jeremyk-91", "name": "Jeremy Kong"}}, "url": "https://github.com/palantir/atlasdb/commit/64f884d8c7a6a24e6830c67a65657b1d87c68458", "committedDate": "2020-02-05T13:56:21Z", "message": "Switch to delegating model"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7aed4df64644be774052c6ff8cc70df5fa8f3018", "author": {"user": {"login": "jeremyk-91", "name": "Jeremy Kong"}}, "url": "https://github.com/palantir/atlasdb/commit/7aed4df64644be774052c6ff8cc70df5fa8f3018", "committedDate": "2020-02-05T18:30:30Z", "message": "Tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6a8d72f7392a1ea8ea36682ccee4e85d5fd9138e", "author": {"user": {"login": "jeremyk-91", "name": "Jeremy Kong"}}, "url": "https://github.com/palantir/atlasdb/commit/6a8d72f7392a1ea8ea36682ccee4e85d5fd9138e", "committedDate": "2020-02-05T19:05:12Z", "message": "Refactor"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1df297ede1602eda8c4fe77b99488a08eb3e429d", "author": {"user": {"login": "jeremyk-91", "name": "Jeremy Kong"}}, "url": "https://github.com/palantir/atlasdb/commit/1df297ede1602eda8c4fe77b99488a08eb3e429d", "committedDate": "2020-02-05T20:14:33Z", "message": "Add generated changelog entries"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "46d4eadd2e8b9815f1b9374f7d7d489d57c36528", "author": {"user": {"login": "jeremyk-91", "name": "Jeremy Kong"}}, "url": "https://github.com/palantir/atlasdb/commit/46d4eadd2e8b9815f1b9374f7d7d489d57c36528", "committedDate": "2020-02-05T20:21:47Z", "message": "bugfix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "86c86dab09f06aa684755e4fa86b7310dd1c374b", "author": {"user": {"login": "jeremyk-91", "name": "Jeremy Kong"}}, "url": "https://github.com/palantir/atlasdb/commit/86c86dab09f06aa684755e4fa86b7310dd1c374b", "committedDate": "2020-02-05T20:27:50Z", "message": "Imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b97d147f2b23fdcf46b00676a6dcffd296f591ad", "author": {"user": {"login": "jeremyk-91", "name": "Jeremy Kong"}}, "url": "https://github.com/palantir/atlasdb/commit/b97d147f2b23fdcf46b00676a6dcffd296f591ad", "committedDate": "2020-02-05T20:28:22Z", "message": "Merge branch 'jkong/timelock-sanity-checker' of github.com:palantir/atlasdb into jkong/timelock-sanity-checker"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU1MDY3MTE3", "url": "https://github.com/palantir/atlasdb/pull/4552#pullrequestreview-355067117", "createdAt": "2020-02-07T10:34:58Z", "commit": {"oid": "b97d147f2b23fdcf46b00676a6dcffd296f591ad"}, "state": "APPROVED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxMDozNDo1OFrOFm46Rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxMTozNToxMFrOFm6YOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjMyMjYzMA==", "bodyText": "It's not really clear why you would want to just log at error. Maybe say something like\n\nTimeLock now logs ERROR messages if multiple nodes are believed to be concurrently servicing timestamps for a given namespace.\n\nAnd maybe also stating that this is not more aggressive while we gather confidence in lack of false positives?", "url": "https://github.com/palantir/atlasdb/pull/4552#discussion_r376322630", "createdAt": "2020-02-07T10:34:58Z", "author": {"login": "gmaretic"}, "path": "changelog/@unreleased/pr-4552.v2.yml", "diffHunk": "@@ -0,0 +1,6 @@\n+type: improvement\n+improvement:\n+  description: TimeLock now logs ERROR messages if multiple nodes are detected to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b97d147f2b23fdcf46b00676a6dcffd296f591ad"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjMzOTUzMg==", "bodyText": "Is this actually reachable under some conditions or is it to make IntelliJ happy?", "url": "https://github.com/palantir/atlasdb/pull/4552#discussion_r376339532", "createdAt": "2020-02-07T11:15:21Z", "author": {"login": "gmaretic"}, "path": "timelock-agent/src/main/java/com/palantir/timelock/invariants/ServerKiller.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * (c) Copyright 2020 Palantir Technologies Inc. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.palantir.timelock.invariants;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.palantir.logsafe.exceptions.SafeIllegalStateException;\n+\n+public final class ServerKiller {\n+    private static final Logger log = LoggerFactory.getLogger(ServerKiller.class);\n+\n+    private ServerKiller() {\n+        // no\n+    }\n+\n+    public static Error killMeNow(Throwable error) {\n+        log.error(\"Something bad happened and we can't continue safely, so we're preemptively killing the server.\",\n+                error);\n+        System.exit(1);\n+        throw new SafeIllegalStateException(\"We should have exited before we get to this point\", error);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b97d147f2b23fdcf46b00676a6dcffd296f591ad"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjM0MTA5NA==", "bodyText": "Ah, so the TACs are single node clients, which will throw NCLEs when we request a timestamp if they are not leaders, correct?", "url": "https://github.com/palantir/atlasdb/pull/4552#discussion_r376341094", "createdAt": "2020-02-07T11:19:34Z", "author": {"login": "gmaretic"}, "path": "timelock-agent/src/main/java/com/palantir/timelock/invariants/TimeLockActivityCheckerFactory.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ * (c) Copyright 2020 Palantir Technologies Inc. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.palantir.timelock.invariants;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+import com.palantir.atlasdb.config.ImmutableServerListConfig;\n+import com.palantir.atlasdb.config.RemotingClientConfigs;\n+import com.palantir.atlasdb.config.ServerListConfig;\n+import com.palantir.atlasdb.factory.ServiceCreator;\n+import com.palantir.atlasdb.util.MetricsManager;\n+import com.palantir.conjure.java.api.config.service.UserAgent;\n+import com.palantir.lock.v2.TimelockRpcClient;\n+import com.palantir.timelock.config.TimeLockInstallConfiguration;\n+import com.palantir.timelock.paxos.PaxosRemotingUtils;\n+\n+public class TimeLockActivityCheckerFactory {\n+    private final TimeLockInstallConfiguration installConfiguration;\n+    private final MetricsManager metricsManager;\n+    private final UserAgent userAgent;\n+\n+    public TimeLockActivityCheckerFactory(\n+            TimeLockInstallConfiguration installConfiguration, MetricsManager metricsManager, UserAgent userAgent) {\n+        this.installConfiguration = installConfiguration;\n+        this.metricsManager = metricsManager;\n+        this.userAgent = userAgent;\n+    }\n+\n+    public List<TimeLockActivityChecker> getTimeLockActivityCheckers() {\n+        return installConfiguration.cluster()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b97d147f2b23fdcf46b00676a6dcffd296f591ad"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjM0NDQ1MQ==", "bodyText": "just filter instead of map + filter", "url": "https://github.com/palantir/atlasdb/pull/4552#discussion_r376344451", "createdAt": "2020-02-07T11:28:44Z", "author": {"login": "gmaretic"}, "path": "timelock-agent/src/main/java/com/palantir/timelock/invariants/NoSimultaneousServiceCheck.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * (c) Copyright 2020 Palantir Technologies Inc. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.palantir.timelock.invariants;\n+\n+import java.time.Duration;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.util.concurrent.Uninterruptibles;\n+import com.palantir.atlasdb.timelock.paxos.Client;\n+import com.palantir.common.concurrent.PTExecutors;\n+import com.palantir.logsafe.SafeArg;\n+import com.palantir.timelock.TimeLockStatus;\n+import com.palantir.timelock.paxos.HealthCheckDigest;\n+\n+public final class NoSimultaneousServiceCheck {\n+    private static final Logger log = LoggerFactory.getLogger(NoSimultaneousServiceCheck.class);\n+\n+    private static final int REQUIRED_CONSECUTIVE_VIOLATIONS_BEFORE_FAIL = 5;\n+    private static final Duration BACKOFF = Duration.ofMillis(1337);\n+\n+    private final List<TimeLockActivityChecker> timeLockActivityCheckers;\n+    private final Consumer<String> failureMechanism;\n+    private final ExecutorService executorService;\n+\n+    @VisibleForTesting\n+    NoSimultaneousServiceCheck(\n+            List<TimeLockActivityChecker> timeLockActivityCheckers,\n+            Consumer<String> failureMechanism,\n+            ExecutorService executorService) {\n+        this.timeLockActivityCheckers = timeLockActivityCheckers;\n+        this.failureMechanism = failureMechanism;\n+        this.executorService = executorService;\n+    }\n+\n+    public static NoSimultaneousServiceCheck create(List<TimeLockActivityChecker> timeLockActivityCheckers) {\n+        ExecutorService executorService = PTExecutors.newSingleThreadExecutor(\n+                PTExecutors.newNamedThreadFactory(false));\n+        return new NoSimultaneousServiceCheck(timeLockActivityCheckers,\n+                client -> {\n+                    // TODO (jkong): Gather confidence and then change to ServerKiller, so that we ACTUALLY shoot\n+                    // ourselves in the head.\n+                    log.error(\"We observed that multiple services were consistently serving timestamps, for the\"\n+                            + \" client {}. This is potentially indicative of SEVERE DATA CORRUPTION, and should\"\n+                            + \" never happen in a correct TimeLock implementation. If you see this message, please\"\n+                            + \" check the frequency of leader elections on your stack: if they are very frequent,\"\n+                            + \" consider increasing the leader election timeout. Otherwise, please contact support -\"\n+                            + \" your stack may have been compromised\",\n+                            SafeArg.of(\"client\", client));\n+                },\n+                executorService);\n+    }\n+\n+    public void processHealthCheckDigest(HealthCheckDigest digest) {\n+        Set<Client> clientsWithMultipleLeaders = digest.statusesToClient().get(TimeLockStatus.MULTIPLE_LEADERS);\n+        if (clientsWithMultipleLeaders.isEmpty()) {\n+            return;\n+        }\n+\n+        log.info(\"Clients {} appear to have multiple leaders based on the leader ping health check. Scheduling\"\n+                + \" checks on these specific clients now.\", SafeArg.of(\"clients\", clientsWithMultipleLeaders));\n+        clientsWithMultipleLeaders.forEach(this::scheduleCheckOnSpecificClient);\n+    }\n+\n+    private void scheduleCheckOnSpecificClient(Client client) {\n+        executorService.submit(() -> {\n+            try {\n+                performCheckOnSpecificClientUnsafe(client);\n+            } catch (Exception e) {\n+                log.info(\"No-simultaneous service check failed, suppressing exception to allow future checks\", e);\n+            }\n+        });\n+    }\n+\n+    private void performCheckOnSpecificClientUnsafe(Client client) {\n+        // Only fail on repeated violations, since it is possible for there to be a leader election between checks that\n+        // could legitimately cause false positives if we failed after one such issue. However, given the number of\n+        // checks it is unlikely that *that* many elections would occur.\n+        for (int attempt = 1; attempt <= REQUIRED_CONSECUTIVE_VIOLATIONS_BEFORE_FAIL; attempt++) {\n+            long numberOfNodesServingTimestamps = timeLockActivityCheckers.stream()\n+                    .map(timeLockActivityChecker ->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b97d147f2b23fdcf46b00676a6dcffd296f591ad"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjM0NjMzOA==", "bodyText": "Is this actually useful? We don't need to suppress because there are not scheduled. Also if something went wrong with TACs but we keep trying to check, we will never really figure out something is wrong", "url": "https://github.com/palantir/atlasdb/pull/4552#discussion_r376346338", "createdAt": "2020-02-07T11:34:09Z", "author": {"login": "gmaretic"}, "path": "timelock-agent/src/main/java/com/palantir/timelock/invariants/NoSimultaneousServiceCheck.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * (c) Copyright 2020 Palantir Technologies Inc. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.palantir.timelock.invariants;\n+\n+import java.time.Duration;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.util.concurrent.Uninterruptibles;\n+import com.palantir.atlasdb.timelock.paxos.Client;\n+import com.palantir.common.concurrent.PTExecutors;\n+import com.palantir.logsafe.SafeArg;\n+import com.palantir.timelock.TimeLockStatus;\n+import com.palantir.timelock.paxos.HealthCheckDigest;\n+\n+public final class NoSimultaneousServiceCheck {\n+    private static final Logger log = LoggerFactory.getLogger(NoSimultaneousServiceCheck.class);\n+\n+    private static final int REQUIRED_CONSECUTIVE_VIOLATIONS_BEFORE_FAIL = 5;\n+    private static final Duration BACKOFF = Duration.ofMillis(1337);\n+\n+    private final List<TimeLockActivityChecker> timeLockActivityCheckers;\n+    private final Consumer<String> failureMechanism;\n+    private final ExecutorService executorService;\n+\n+    @VisibleForTesting\n+    NoSimultaneousServiceCheck(\n+            List<TimeLockActivityChecker> timeLockActivityCheckers,\n+            Consumer<String> failureMechanism,\n+            ExecutorService executorService) {\n+        this.timeLockActivityCheckers = timeLockActivityCheckers;\n+        this.failureMechanism = failureMechanism;\n+        this.executorService = executorService;\n+    }\n+\n+    public static NoSimultaneousServiceCheck create(List<TimeLockActivityChecker> timeLockActivityCheckers) {\n+        ExecutorService executorService = PTExecutors.newSingleThreadExecutor(\n+                PTExecutors.newNamedThreadFactory(false));\n+        return new NoSimultaneousServiceCheck(timeLockActivityCheckers,\n+                client -> {\n+                    // TODO (jkong): Gather confidence and then change to ServerKiller, so that we ACTUALLY shoot\n+                    // ourselves in the head.\n+                    log.error(\"We observed that multiple services were consistently serving timestamps, for the\"\n+                            + \" client {}. This is potentially indicative of SEVERE DATA CORRUPTION, and should\"\n+                            + \" never happen in a correct TimeLock implementation. If you see this message, please\"\n+                            + \" check the frequency of leader elections on your stack: if they are very frequent,\"\n+                            + \" consider increasing the leader election timeout. Otherwise, please contact support -\"\n+                            + \" your stack may have been compromised\",\n+                            SafeArg.of(\"client\", client));\n+                },\n+                executorService);\n+    }\n+\n+    public void processHealthCheckDigest(HealthCheckDigest digest) {\n+        Set<Client> clientsWithMultipleLeaders = digest.statusesToClient().get(TimeLockStatus.MULTIPLE_LEADERS);\n+        if (clientsWithMultipleLeaders.isEmpty()) {\n+            return;\n+        }\n+\n+        log.info(\"Clients {} appear to have multiple leaders based on the leader ping health check. Scheduling\"\n+                + \" checks on these specific clients now.\", SafeArg.of(\"clients\", clientsWithMultipleLeaders));\n+        clientsWithMultipleLeaders.forEach(this::scheduleCheckOnSpecificClient);\n+    }\n+\n+    private void scheduleCheckOnSpecificClient(Client client) {\n+        executorService.submit(() -> {\n+            try {\n+                performCheckOnSpecificClientUnsafe(client);\n+            } catch (Exception e) {\n+                log.info(\"No-simultaneous service check failed, suppressing exception to allow future checks\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b97d147f2b23fdcf46b00676a6dcffd296f591ad"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjM0NjY4Mw==", "bodyText": "You are already in a for loop, you don't need the if/else", "url": "https://github.com/palantir/atlasdb/pull/4552#discussion_r376346683", "createdAt": "2020-02-07T11:35:10Z", "author": {"login": "gmaretic"}, "path": "timelock-agent/src/main/java/com/palantir/timelock/invariants/NoSimultaneousServiceCheck.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * (c) Copyright 2020 Palantir Technologies Inc. All rights reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package com.palantir.timelock.invariants;\n+\n+import java.time.Duration;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.util.concurrent.Uninterruptibles;\n+import com.palantir.atlasdb.timelock.paxos.Client;\n+import com.palantir.common.concurrent.PTExecutors;\n+import com.palantir.logsafe.SafeArg;\n+import com.palantir.timelock.TimeLockStatus;\n+import com.palantir.timelock.paxos.HealthCheckDigest;\n+\n+public final class NoSimultaneousServiceCheck {\n+    private static final Logger log = LoggerFactory.getLogger(NoSimultaneousServiceCheck.class);\n+\n+    private static final int REQUIRED_CONSECUTIVE_VIOLATIONS_BEFORE_FAIL = 5;\n+    private static final Duration BACKOFF = Duration.ofMillis(1337);\n+\n+    private final List<TimeLockActivityChecker> timeLockActivityCheckers;\n+    private final Consumer<String> failureMechanism;\n+    private final ExecutorService executorService;\n+\n+    @VisibleForTesting\n+    NoSimultaneousServiceCheck(\n+            List<TimeLockActivityChecker> timeLockActivityCheckers,\n+            Consumer<String> failureMechanism,\n+            ExecutorService executorService) {\n+        this.timeLockActivityCheckers = timeLockActivityCheckers;\n+        this.failureMechanism = failureMechanism;\n+        this.executorService = executorService;\n+    }\n+\n+    public static NoSimultaneousServiceCheck create(List<TimeLockActivityChecker> timeLockActivityCheckers) {\n+        ExecutorService executorService = PTExecutors.newSingleThreadExecutor(\n+                PTExecutors.newNamedThreadFactory(false));\n+        return new NoSimultaneousServiceCheck(timeLockActivityCheckers,\n+                client -> {\n+                    // TODO (jkong): Gather confidence and then change to ServerKiller, so that we ACTUALLY shoot\n+                    // ourselves in the head.\n+                    log.error(\"We observed that multiple services were consistently serving timestamps, for the\"\n+                            + \" client {}. This is potentially indicative of SEVERE DATA CORRUPTION, and should\"\n+                            + \" never happen in a correct TimeLock implementation. If you see this message, please\"\n+                            + \" check the frequency of leader elections on your stack: if they are very frequent,\"\n+                            + \" consider increasing the leader election timeout. Otherwise, please contact support -\"\n+                            + \" your stack may have been compromised\",\n+                            SafeArg.of(\"client\", client));\n+                },\n+                executorService);\n+    }\n+\n+    public void processHealthCheckDigest(HealthCheckDigest digest) {\n+        Set<Client> clientsWithMultipleLeaders = digest.statusesToClient().get(TimeLockStatus.MULTIPLE_LEADERS);\n+        if (clientsWithMultipleLeaders.isEmpty()) {\n+            return;\n+        }\n+\n+        log.info(\"Clients {} appear to have multiple leaders based on the leader ping health check. Scheduling\"\n+                + \" checks on these specific clients now.\", SafeArg.of(\"clients\", clientsWithMultipleLeaders));\n+        clientsWithMultipleLeaders.forEach(this::scheduleCheckOnSpecificClient);\n+    }\n+\n+    private void scheduleCheckOnSpecificClient(Client client) {\n+        executorService.submit(() -> {\n+            try {\n+                performCheckOnSpecificClientUnsafe(client);\n+            } catch (Exception e) {\n+                log.info(\"No-simultaneous service check failed, suppressing exception to allow future checks\", e);\n+            }\n+        });\n+    }\n+\n+    private void performCheckOnSpecificClientUnsafe(Client client) {\n+        // Only fail on repeated violations, since it is possible for there to be a leader election between checks that\n+        // could legitimately cause false positives if we failed after one such issue. However, given the number of\n+        // checks it is unlikely that *that* many elections would occur.\n+        for (int attempt = 1; attempt <= REQUIRED_CONSECUTIVE_VIOLATIONS_BEFORE_FAIL; attempt++) {\n+            long numberOfNodesServingTimestamps = timeLockActivityCheckers.stream()\n+                    .map(timeLockActivityChecker ->\n+                            timeLockActivityChecker.isThisNodeActivelyServingTimestampsForClient(client.value()))\n+                    .filter(x -> x)\n+                    .count();\n+            if (numberOfNodesServingTimestamps <= 1) {\n+                // Accept 0: the cluster being in such a bad state is not a terminal condition, could just be a\n+                // network partition or legitimate no-quorum situation. No reason to kill the server then.\n+                log.info(\"We don't think services were simultaneously serving timestamps for client {}\",\n+                        SafeArg.of(\"client\", client));\n+                return;\n+            }\n+\n+            if (attempt < REQUIRED_CONSECUTIVE_VIOLATIONS_BEFORE_FAIL) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b97d147f2b23fdcf46b00676a6dcffd296f591ad"}, "originalPosition": 114}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ea327bcec31d75f1c29ef55c45232e3674525db5", "author": {"user": {"login": "gmaretic", "name": null}}, "url": "https://github.com/palantir/atlasdb/commit/ea327bcec31d75f1c29ef55c45232e3674525db5", "committedDate": "2020-02-07T13:29:10Z", "message": "Actually track issued timestamps, update tests to reflect that"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ad651dd364a2566de152090c8788e5a24cdbf3c3", "author": {"user": {"login": "gmaretic", "name": null}}, "url": "https://github.com/palantir/atlasdb/commit/ad651dd364a2566de152090c8788e5a24cdbf3c3", "committedDate": "2020-02-07T13:29:10Z", "message": "Add generated changelog entries"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "19922c22c3ebb1179da86cb247287beaf25abc32", "author": {"user": {"login": "gmaretic", "name": null}}, "url": "https://github.com/palantir/atlasdb/commit/19922c22c3ebb1179da86cb247287beaf25abc32", "committedDate": "2020-02-07T13:29:10Z", "message": "Add generated changelog entries"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU1MTkxMzM1", "url": "https://github.com/palantir/atlasdb/pull/4552#pullrequestreview-355191335", "createdAt": "2020-02-07T14:27:27Z", "commit": {"oid": "ea327bcec31d75f1c29ef55c45232e3674525db5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxNDoyNzoyN1rOFm-sBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wN1QxNDoyNzoyN1rOFm-sBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NjQxNzI4Ng==", "bodyText": "I think we want the attempt number?", "url": "https://github.com/palantir/atlasdb/pull/4552#discussion_r376417286", "createdAt": "2020-02-07T14:27:27Z", "author": {"login": "jeremyk-91"}, "path": "timelock-agent/src/main/java/com/palantir/timelock/invariants/NoSimultaneousServiceCheck.java", "diffHunk": "@@ -97,30 +103,36 @@ private void performCheckOnSpecificClientUnsafe(Client client) {\n         // Only fail on repeated violations, since it is possible for there to be a leader election between checks that\n         // could legitimately cause false positives if we failed after one such issue. However, given the number of\n         // checks it is unlikely that *that* many elections would occur.\n-        for (int attempt = 1; attempt <= REQUIRED_CONSECUTIVE_VIOLATIONS_BEFORE_FAIL; attempt++) {\n-            long numberOfNodesServingTimestamps = timeLockActivityCheckers.stream()\n+        long timestampBound = Long.MIN_VALUE;\n+\n+        for (int attempt = 1; attempt <= REQUIRED_ATTEMPTS_BEFORE_GIVING_UP; attempt++) {\n+            List<Long> timestamps = timeLockActivityCheckers.stream()\n                     .map(timeLockActivityChecker ->\n-                            timeLockActivityChecker.isThisNodeActivelyServingTimestampsForClient(client.value()))\n-                    .filter(x -> x)\n-                    .count();\n-            if (numberOfNodesServingTimestamps <= 1) {\n+                            timeLockActivityChecker.getFreshTimestampFromNodeForClient(client.value()))\n+                    .filter(OptionalLong::isPresent)\n+                    .map(OptionalLong::getAsLong)\n+                    .collect(Collectors.toList());\n+            if (timestamps.size() <= 1) {\n                 // Accept 0: the cluster being in such a bad state is not a terminal condition, could just be a\n                 // network partition or legitimate no-quorum situation. No reason to kill the server then.\n                 log.info(\"We don't think services were simultaneously serving timestamps for client {}\",\n                         SafeArg.of(\"client\", client));\n                 return;\n             }\n \n-            if (attempt < REQUIRED_CONSECUTIVE_VIOLATIONS_BEFORE_FAIL) {\n-                log.info(\"We observed on attempt {} of {} that multiple services were serving timestamps. We'll try\"\n-                                + \" again in {} ms to see if this remains the case.\",\n-                        SafeArg.of(\"attemptNumber\", attempt),\n-                        SafeArg.of(\"maximumAttempts\", REQUIRED_CONSECUTIVE_VIOLATIONS_BEFORE_FAIL),\n-                        SafeArg.of(\"backoffMillis\", BACKOFF.toMillis()));\n-                Uninterruptibles.sleepUninterruptibly(BACKOFF.toMillis(), TimeUnit.MILLISECONDS);\n-            } else {\n+            if (!Ordering.natural().isStrictlyOrdered(timestamps) || timestamps.get(0) <= timestampBound) {\n                 failureMechanism.accept(client.value());\n+                return;\n             }\n+\n+            timestampBound = timestamps.get(timestamps.size() - 1);\n+            log.info(\"We observed on attempt that multiple services were serving timestamps, but the timestamps were\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ea327bcec31d75f1c29ef55c45232e3674525db5"}, "originalPosition": 97}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2238, "cost": 1, "resetAt": "2021-11-01T16:19:10Z"}}}