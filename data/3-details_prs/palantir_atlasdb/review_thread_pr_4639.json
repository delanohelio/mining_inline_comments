{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg0NTAzOTYy", "number": 4639, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQyMToxNjowM1rODlls2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxODowNToxNFrODl5AFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQwNzQxNTk1OnYy", "diffSide": "RIGHT", "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQyMToxNjowM1rOFykbVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxNzo0MTozNlrOFzBjeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODU2OTk0MQ==", "bodyText": "should I have passed in the cache key that generated this request? I think it's probably fine, but might make sense", "url": "https://github.com/palantir/atlasdb/pull/4639#discussion_r388569941", "createdAt": "2020-03-05T21:16:03Z", "author": {"login": "felixdesouza"}, "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "diffHunk": "@@ -16,92 +16,159 @@\n \n package com.palantir.atlasdb.timelock.paxos;\n \n+import java.time.Duration;\n+import java.util.Comparator;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Stream;\n \n-import javax.annotation.Nullable;\n-import javax.annotation.concurrent.NotThreadSafe;\n+import javax.annotation.concurrent.ThreadSafe;\n \n+import org.immutables.value.Value;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import com.github.benmanes.caffeine.cache.LoadingCache;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Maps;\n import com.google.common.collect.Sets;\n import com.palantir.atlasdb.autobatch.CoalescingRequestFunction;\n import com.palantir.common.streams.KeyedStream;\n+import com.palantir.logsafe.SafeArg;\n+import com.palantir.logsafe.exceptions.SafeIllegalArgumentException;\n+import com.palantir.logsafe.exceptions.SafeIllegalStateException;\n import com.palantir.paxos.PaxosLong;\n \n-/*\n-    This is not thread safe, but it is okay because it is run within an autobatcher, which is configured to not process\n-    multiple batches in parallel.\n- */\n-@NotThreadSafe\n+@ThreadSafe\n final class BatchingPaxosLatestSequenceCache implements CoalescingRequestFunction<Client, PaxosLong> {\n \n     private static final Logger log = LoggerFactory.getLogger(BatchingPaxosLatestSequenceCache.class);\n     private static final PaxosLong DEFAULT_VALUE = PaxosLong.of(BatchPaxosAcceptor.NO_LOG_ENTRY);\n \n-    @Nullable\n-    private AcceptorCacheKey cacheKey = null;\n-    private Map<Client, PaxosLong> cachedEntries = Maps.newHashMap();\n-    private BatchPaxosAcceptor delegate;\n+    private final BatchPaxosAcceptor delegate;\n+\n+    private final Set<Client> clientsSeenSoFar = Sets.newConcurrentHashSet();\n+\n+    private final AtomicReference<TimestampedAcceptorCacheKey> cacheKey = new AtomicReference<>();\n+    private final LoadingCache<TimestampedAcceptorCacheKey, ConcurrentMap<Client, PaxosLong>> cacheKeysToCaches =\n+            Caffeine.newBuilder()\n+                    .expireAfterAccess(Duration.ofMinutes(1))\n+                    .build($ -> Maps.newConcurrentMap());\n \n     BatchingPaxosLatestSequenceCache(BatchPaxosAcceptor delegate) {\n         this.delegate = delegate;\n     }\n \n     @Override\n-    public Map<Client, PaxosLong> apply(Set<Client> clients) {\n-        try {\n-            return unsafeGetLatest(clients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.info(\"Cache key is invalid, invalidating cache - using deprecated detection method\");\n-            return handleCacheMiss(clients);\n-        }\n-    }\n+    public Map<Client, PaxosLong> apply(Set<Client> requestedClients) {\n+        // always add requested clients so we can easily query with everything we've ever seen when our cache is invalid\n+        clientsSeenSoFar.addAll(requestedClients);\n \n-    private Map<Client, PaxosLong> handleCacheMiss(Set<Client> requestedClients) {\n-        cacheKey = null;\n-        Set<Client> allClients = ImmutableSet.<Client>builder()\n-                .addAll(requestedClients)\n-                .addAll(cachedEntries.keySet())\n-                .build();\n-        cachedEntries.clear();\n-        try {\n-            return unsafeGetLatest(allClients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.warn(\"Empty cache key is still invalid indicates product bug, failing request.\");\n-            throw new RuntimeException(e);\n+        int attempt = 0;\n+        while (attempt < 3) {\n+            TimestampedAcceptorCacheKey timestampedCacheKey = cacheKey.get();\n+            try {\n+                if (timestampedCacheKey == null) {\n+                    return populateNewCache(requestedClients);\n+                } else {\n+                    return populateExistingCache(\n+                            timestampedCacheKey,\n+                            cacheKeysToCaches.get(timestampedCacheKey),\n+                            requestedClients);\n+                }\n+            } catch (InvalidAcceptorCacheKeyException e) {\n+                log.info(\"Cache key is invalid, invalidating cache and retrying\",\n+                        SafeArg.of(\"attempt\", attempt),\n+                        e);\n+                cacheKey.compareAndSet(timestampedCacheKey, null);\n+                attempt++;\n+            }\n         }\n+\n+        throw new SafeIllegalStateException(\"could not request complete request due to contention in the cache\");\n     }\n \n-    private Map<Client, PaxosLong> unsafeGetLatest(Set<Client> clients) throws InvalidAcceptorCacheKeyException {\n-        if (cacheKey == null) {\n-            processDigest(delegate.latestSequencesPreparedOrAccepted(Optional.empty(), clients));\n-            return getResponseMap(clients);\n-        }\n+    private Map<Client, PaxosLong> populateNewCache(Set<Client> requestedClients)\n+            throws InvalidAcceptorCacheKeyException {\n+        AcceptorCacheDigest digest = delegate.latestSequencesPreparedOrAccepted(Optional.empty(), clientsSeenSoFar);\n+        ConcurrentMap<Client, PaxosLong> newEntriesToCache =\n+                cacheKeysToCaches.get(TimestampedAcceptorCacheKey.of(digest));\n+        processDigest(newEntriesToCache, digest);\n+        return getResponseMap(newEntriesToCache, requestedClients);\n+    }\n \n-        Set<Client> newClients = Sets.difference(clients, cachedEntries.keySet());\n+    private Map<Client, PaxosLong> populateExistingCache(\n+            TimestampedAcceptorCacheKey timestampedCacheKey,\n+            ConcurrentMap<Client, PaxosLong> currentCachedEntries,\n+            Set<Client> requestedClients)\n+            throws InvalidAcceptorCacheKeyException {\n+        Set<Client> newClients = ImmutableSet.copyOf(Sets.difference(requestedClients, currentCachedEntries.keySet()));\n         if (newClients.isEmpty()) {\n-            delegate.latestSequencesPreparedOrAcceptedCached(cacheKey).ifPresent(this::processDigest);\n-            return getResponseMap(clients);\n+            delegate.latestSequencesPreparedOrAcceptedCached(timestampedCacheKey.cacheKey())\n+                    .ifPresent(digest -> processDigest(currentCachedEntries, digest));\n+            return getResponseMap(currentCachedEntries, requestedClients);\n         } else {\n-            processDigest(delegate.latestSequencesPreparedOrAccepted(Optional.of(cacheKey), newClients));\n-            return getResponseMap(clients);\n+            processDigest(currentCachedEntries, delegate.latestSequencesPreparedOrAccepted(\n+                    Optional.of(timestampedCacheKey.cacheKey()),\n+                    newClients));\n+            return getResponseMap(currentCachedEntries, requestedClients);\n+        }\n+    }\n+\n+    private void processDigest(ConcurrentMap<Client, PaxosLong> currentCachedEntries, AcceptorCacheDigest digest) {\n+        TimestampedAcceptorCacheKey newCacheKey = TimestampedAcceptorCacheKey.of(digest);\n+        // this shares the same map with \"previous\" cache keys, if it's too confusing we can always copy it potentially\n+        ConcurrentMap<Client, PaxosLong> newCachedEntries =\n+                cacheKeysToCaches.get(newCacheKey, $ -> currentCachedEntries);\n+        KeyedStream.stream(digest.updates())\n+                .map(PaxosLong::of)\n+                .forEach((client, paxosLong) ->\n+                        newCachedEntries.merge(client, paxosLong, BatchingPaxosLatestSequenceCache::max));\n+\n+        // for a *new* mapping, setting the cache key must happen *after* we've setup the mapping, so that concurrent\n+        // clients will not reference an in-progress populating map which can be empty.\n+        maybeSetNewCacheKey(newCacheKey);\n+    }\n+\n+    private void maybeSetNewCacheKey(TimestampedAcceptorCacheKey newCacheKey) {\n+        while (true) {\n+            TimestampedAcceptorCacheKey current = cacheKey.get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1ad8bfc46dbac3ae2c8933d6020e71c73868edb"}, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTA0NzE2MQ==", "bodyText": "I think this is fine, as written we have a standard CAS algorithm and it makes the most current role of cacheKey more obvious", "url": "https://github.com/palantir/atlasdb/pull/4639#discussion_r389047161", "createdAt": "2020-03-06T17:41:36Z", "author": {"login": "jeremyk-91"}, "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "diffHunk": "@@ -16,92 +16,159 @@\n \n package com.palantir.atlasdb.timelock.paxos;\n \n+import java.time.Duration;\n+import java.util.Comparator;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Stream;\n \n-import javax.annotation.Nullable;\n-import javax.annotation.concurrent.NotThreadSafe;\n+import javax.annotation.concurrent.ThreadSafe;\n \n+import org.immutables.value.Value;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import com.github.benmanes.caffeine.cache.LoadingCache;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Maps;\n import com.google.common.collect.Sets;\n import com.palantir.atlasdb.autobatch.CoalescingRequestFunction;\n import com.palantir.common.streams.KeyedStream;\n+import com.palantir.logsafe.SafeArg;\n+import com.palantir.logsafe.exceptions.SafeIllegalArgumentException;\n+import com.palantir.logsafe.exceptions.SafeIllegalStateException;\n import com.palantir.paxos.PaxosLong;\n \n-/*\n-    This is not thread safe, but it is okay because it is run within an autobatcher, which is configured to not process\n-    multiple batches in parallel.\n- */\n-@NotThreadSafe\n+@ThreadSafe\n final class BatchingPaxosLatestSequenceCache implements CoalescingRequestFunction<Client, PaxosLong> {\n \n     private static final Logger log = LoggerFactory.getLogger(BatchingPaxosLatestSequenceCache.class);\n     private static final PaxosLong DEFAULT_VALUE = PaxosLong.of(BatchPaxosAcceptor.NO_LOG_ENTRY);\n \n-    @Nullable\n-    private AcceptorCacheKey cacheKey = null;\n-    private Map<Client, PaxosLong> cachedEntries = Maps.newHashMap();\n-    private BatchPaxosAcceptor delegate;\n+    private final BatchPaxosAcceptor delegate;\n+\n+    private final Set<Client> clientsSeenSoFar = Sets.newConcurrentHashSet();\n+\n+    private final AtomicReference<TimestampedAcceptorCacheKey> cacheKey = new AtomicReference<>();\n+    private final LoadingCache<TimestampedAcceptorCacheKey, ConcurrentMap<Client, PaxosLong>> cacheKeysToCaches =\n+            Caffeine.newBuilder()\n+                    .expireAfterAccess(Duration.ofMinutes(1))\n+                    .build($ -> Maps.newConcurrentMap());\n \n     BatchingPaxosLatestSequenceCache(BatchPaxosAcceptor delegate) {\n         this.delegate = delegate;\n     }\n \n     @Override\n-    public Map<Client, PaxosLong> apply(Set<Client> clients) {\n-        try {\n-            return unsafeGetLatest(clients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.info(\"Cache key is invalid, invalidating cache - using deprecated detection method\");\n-            return handleCacheMiss(clients);\n-        }\n-    }\n+    public Map<Client, PaxosLong> apply(Set<Client> requestedClients) {\n+        // always add requested clients so we can easily query with everything we've ever seen when our cache is invalid\n+        clientsSeenSoFar.addAll(requestedClients);\n \n-    private Map<Client, PaxosLong> handleCacheMiss(Set<Client> requestedClients) {\n-        cacheKey = null;\n-        Set<Client> allClients = ImmutableSet.<Client>builder()\n-                .addAll(requestedClients)\n-                .addAll(cachedEntries.keySet())\n-                .build();\n-        cachedEntries.clear();\n-        try {\n-            return unsafeGetLatest(allClients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.warn(\"Empty cache key is still invalid indicates product bug, failing request.\");\n-            throw new RuntimeException(e);\n+        int attempt = 0;\n+        while (attempt < 3) {\n+            TimestampedAcceptorCacheKey timestampedCacheKey = cacheKey.get();\n+            try {\n+                if (timestampedCacheKey == null) {\n+                    return populateNewCache(requestedClients);\n+                } else {\n+                    return populateExistingCache(\n+                            timestampedCacheKey,\n+                            cacheKeysToCaches.get(timestampedCacheKey),\n+                            requestedClients);\n+                }\n+            } catch (InvalidAcceptorCacheKeyException e) {\n+                log.info(\"Cache key is invalid, invalidating cache and retrying\",\n+                        SafeArg.of(\"attempt\", attempt),\n+                        e);\n+                cacheKey.compareAndSet(timestampedCacheKey, null);\n+                attempt++;\n+            }\n         }\n+\n+        throw new SafeIllegalStateException(\"could not request complete request due to contention in the cache\");\n     }\n \n-    private Map<Client, PaxosLong> unsafeGetLatest(Set<Client> clients) throws InvalidAcceptorCacheKeyException {\n-        if (cacheKey == null) {\n-            processDigest(delegate.latestSequencesPreparedOrAccepted(Optional.empty(), clients));\n-            return getResponseMap(clients);\n-        }\n+    private Map<Client, PaxosLong> populateNewCache(Set<Client> requestedClients)\n+            throws InvalidAcceptorCacheKeyException {\n+        AcceptorCacheDigest digest = delegate.latestSequencesPreparedOrAccepted(Optional.empty(), clientsSeenSoFar);\n+        ConcurrentMap<Client, PaxosLong> newEntriesToCache =\n+                cacheKeysToCaches.get(TimestampedAcceptorCacheKey.of(digest));\n+        processDigest(newEntriesToCache, digest);\n+        return getResponseMap(newEntriesToCache, requestedClients);\n+    }\n \n-        Set<Client> newClients = Sets.difference(clients, cachedEntries.keySet());\n+    private Map<Client, PaxosLong> populateExistingCache(\n+            TimestampedAcceptorCacheKey timestampedCacheKey,\n+            ConcurrentMap<Client, PaxosLong> currentCachedEntries,\n+            Set<Client> requestedClients)\n+            throws InvalidAcceptorCacheKeyException {\n+        Set<Client> newClients = ImmutableSet.copyOf(Sets.difference(requestedClients, currentCachedEntries.keySet()));\n         if (newClients.isEmpty()) {\n-            delegate.latestSequencesPreparedOrAcceptedCached(cacheKey).ifPresent(this::processDigest);\n-            return getResponseMap(clients);\n+            delegate.latestSequencesPreparedOrAcceptedCached(timestampedCacheKey.cacheKey())\n+                    .ifPresent(digest -> processDigest(currentCachedEntries, digest));\n+            return getResponseMap(currentCachedEntries, requestedClients);\n         } else {\n-            processDigest(delegate.latestSequencesPreparedOrAccepted(Optional.of(cacheKey), newClients));\n-            return getResponseMap(clients);\n+            processDigest(currentCachedEntries, delegate.latestSequencesPreparedOrAccepted(\n+                    Optional.of(timestampedCacheKey.cacheKey()),\n+                    newClients));\n+            return getResponseMap(currentCachedEntries, requestedClients);\n+        }\n+    }\n+\n+    private void processDigest(ConcurrentMap<Client, PaxosLong> currentCachedEntries, AcceptorCacheDigest digest) {\n+        TimestampedAcceptorCacheKey newCacheKey = TimestampedAcceptorCacheKey.of(digest);\n+        // this shares the same map with \"previous\" cache keys, if it's too confusing we can always copy it potentially\n+        ConcurrentMap<Client, PaxosLong> newCachedEntries =\n+                cacheKeysToCaches.get(newCacheKey, $ -> currentCachedEntries);\n+        KeyedStream.stream(digest.updates())\n+                .map(PaxosLong::of)\n+                .forEach((client, paxosLong) ->\n+                        newCachedEntries.merge(client, paxosLong, BatchingPaxosLatestSequenceCache::max));\n+\n+        // for a *new* mapping, setting the cache key must happen *after* we've setup the mapping, so that concurrent\n+        // clients will not reference an in-progress populating map which can be empty.\n+        maybeSetNewCacheKey(newCacheKey);\n+    }\n+\n+    private void maybeSetNewCacheKey(TimestampedAcceptorCacheKey newCacheKey) {\n+        while (true) {\n+            TimestampedAcceptorCacheKey current = cacheKey.get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODU2OTk0MQ=="}, "originalCommit": {"oid": "f1ad8bfc46dbac3ae2c8933d6020e71c73868edb"}, "originalPosition": 165}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQwOTQ1NjAwOnYy", "diffSide": "RIGHT", "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxMjo0MzozM1rOFy3dgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxMjo0MzozM1rOFy3dgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg4MTc5NA==", "bodyText": "The new and old cache key are never the same at this point", "url": "https://github.com/palantir/atlasdb/pull/4639#discussion_r388881794", "createdAt": "2020-03-06T12:43:33Z", "author": {"login": "jeremyk-91"}, "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "diffHunk": "@@ -16,92 +16,145 @@\n \n package com.palantir.atlasdb.timelock.paxos;\n \n+import java.time.Duration;\n+import java.util.Comparator;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Stream;\n \n-import javax.annotation.Nullable;\n-import javax.annotation.concurrent.NotThreadSafe;\n+import javax.annotation.concurrent.ThreadSafe;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import com.github.benmanes.caffeine.cache.LoadingCache;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Maps;\n import com.google.common.collect.Sets;\n import com.palantir.atlasdb.autobatch.CoalescingRequestFunction;\n import com.palantir.common.streams.KeyedStream;\n+import com.palantir.logsafe.SafeArg;\n+import com.palantir.logsafe.exceptions.SafeIllegalArgumentException;\n+import com.palantir.logsafe.exceptions.SafeIllegalStateException;\n import com.palantir.paxos.PaxosLong;\n \n-/*\n-    This is not thread safe, but it is okay because it is run within an autobatcher, which is configured to not process\n-    multiple batches in parallel.\n- */\n-@NotThreadSafe\n+@ThreadSafe\n final class BatchingPaxosLatestSequenceCache implements CoalescingRequestFunction<Client, PaxosLong> {\n \n     private static final Logger log = LoggerFactory.getLogger(BatchingPaxosLatestSequenceCache.class);\n     private static final PaxosLong DEFAULT_VALUE = PaxosLong.of(BatchPaxosAcceptor.NO_LOG_ENTRY);\n \n-    @Nullable\n-    private AcceptorCacheKey cacheKey = null;\n-    private Map<Client, PaxosLong> cachedEntries = Maps.newHashMap();\n-    private BatchPaxosAcceptor delegate;\n+    private final BatchPaxosAcceptor delegate;\n+\n+    private final Set<Client> clientsSeenSoFar = Sets.newConcurrentHashSet();\n+\n+    private final AtomicReference<TimestampedAcceptorCacheKey> cacheKey = new AtomicReference<>();\n+    private final LoadingCache<AcceptorCacheKey, ConcurrentMap<Client, PaxosLong>> cacheKeysToCaches =\n+            Caffeine.newBuilder()\n+                    .expireAfterAccess(Duration.ofMinutes(1))\n+                    .build($ -> Maps.newConcurrentMap());\n \n     BatchingPaxosLatestSequenceCache(BatchPaxosAcceptor delegate) {\n         this.delegate = delegate;\n     }\n \n     @Override\n-    public Map<Client, PaxosLong> apply(Set<Client> clients) {\n-        try {\n-            return unsafeGetLatest(clients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.info(\"Cache key is invalid, invalidating cache - using deprecated detection method\");\n-            return handleCacheMiss(clients);\n-        }\n-    }\n+    public Map<Client, PaxosLong> apply(Set<Client> requestedClients) {\n+        // always add requested clients so we can easily query with everything we've ever seen when our cache is invalid\n+        clientsSeenSoFar.addAll(requestedClients);\n \n-    private Map<Client, PaxosLong> handleCacheMiss(Set<Client> requestedClients) {\n-        cacheKey = null;\n-        Set<Client> allClients = ImmutableSet.<Client>builder()\n-                .addAll(requestedClients)\n-                .addAll(cachedEntries.keySet())\n-                .build();\n-        cachedEntries.clear();\n-        try {\n-            return unsafeGetLatest(allClients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.warn(\"Empty cache key is still invalid indicates product bug, failing request.\");\n-            throw new RuntimeException(e);\n+        int attempt = 0;\n+        while (attempt < 3) {\n+            TimestampedAcceptorCacheKey timestampedCacheKey = cacheKey.get();\n+            try {\n+                if (timestampedCacheKey == null) {\n+                    return populateNewCache(requestedClients);\n+                } else {\n+                    return populateExistingCache(\n+                            timestampedCacheKey,\n+                            cacheKeysToCaches.get(timestampedCacheKey.cacheKey()),\n+                            requestedClients);\n+                }\n+            } catch (InvalidAcceptorCacheKeyException e) {\n+                log.info(\"Cache key is invalid, invalidating cache and retrying\",\n+                        SafeArg.of(\"attempt\", attempt),\n+                        e);\n+                cacheKey.compareAndSet(timestampedCacheKey, null);\n+                attempt++;\n+            }\n         }\n+\n+        throw new SafeIllegalStateException(\"could not request complete request due to contention in the cache\");\n     }\n \n-    private Map<Client, PaxosLong> unsafeGetLatest(Set<Client> clients) throws InvalidAcceptorCacheKeyException {\n-        if (cacheKey == null) {\n-            processDigest(delegate.latestSequencesPreparedOrAccepted(Optional.empty(), clients));\n-            return getResponseMap(clients);\n-        }\n+    private Map<Client, PaxosLong> populateNewCache(Set<Client> requestedClients)\n+            throws InvalidAcceptorCacheKeyException {\n+        AcceptorCacheDigest digest = delegate.latestSequencesPreparedOrAccepted(Optional.empty(), clientsSeenSoFar);\n+        ConcurrentMap<Client, PaxosLong> newEntriesToCache =\n+                cacheKeysToCaches.get(digest.newCacheKey());\n+        processDigest(newEntriesToCache, digest);\n+        return getResponseMap(newEntriesToCache, requestedClients);\n+    }\n \n-        Set<Client> newClients = Sets.difference(clients, cachedEntries.keySet());\n+    private Map<Client, PaxosLong> populateExistingCache(\n+            TimestampedAcceptorCacheKey timestampedCacheKey,\n+            ConcurrentMap<Client, PaxosLong> currentCachedEntries,\n+            Set<Client> requestedClients)\n+            throws InvalidAcceptorCacheKeyException {\n+        Set<Client> newClients = ImmutableSet.copyOf(Sets.difference(requestedClients, currentCachedEntries.keySet()));\n         if (newClients.isEmpty()) {\n-            delegate.latestSequencesPreparedOrAcceptedCached(cacheKey).ifPresent(this::processDigest);\n-            return getResponseMap(clients);\n+            delegate.latestSequencesPreparedOrAcceptedCached(timestampedCacheKey.cacheKey())\n+                    .ifPresent(digest -> processDigest(currentCachedEntries, digest));\n+            return getResponseMap(currentCachedEntries, requestedClients);\n         } else {\n-            processDigest(delegate.latestSequencesPreparedOrAccepted(Optional.of(cacheKey), newClients));\n-            return getResponseMap(clients);\n+            processDigest(currentCachedEntries, delegate.latestSequencesPreparedOrAccepted(\n+                    Optional.of(timestampedCacheKey.cacheKey()),\n+                    newClients));\n+            return getResponseMap(currentCachedEntries, requestedClients);\n         }\n     }\n \n-    private Map<Client, PaxosLong> getResponseMap(Set<Client> clientsInRequest) {\n-        return Maps.toMap(clientsInRequest, client -> cachedEntries.getOrDefault(client, DEFAULT_VALUE));\n+    private void processDigest(ConcurrentMap<Client, PaxosLong> currentCachedEntries, AcceptorCacheDigest digest) {\n+        TimestampedAcceptorCacheKey newCacheKey = TimestampedAcceptorCacheKey.of(digest);\n+        // this shares the same map with \"previous\" cache keys, if it's too confusing we can always copy it potentially\n+        ConcurrentMap<Client, PaxosLong> newCachedEntries =\n+                cacheKeysToCaches.get(newCacheKey.cacheKey(), $ -> currentCachedEntries);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7daa4c3499227e18bc8d885fc1e210e5686156dc"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQwOTQ2NTAxOnYy", "diffSide": "RIGHT", "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxMjo0NjozM1rOFy3iqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxMjo0NjozM1rOFy3iqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODg4MzExNQ==", "bodyText": "so this map stores the largest known version of the updates from the reference point indicated by the cache key", "url": "https://github.com/palantir/atlasdb/pull/4639#discussion_r388883115", "createdAt": "2020-03-06T12:46:33Z", "author": {"login": "jeremyk-91"}, "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "diffHunk": "@@ -16,92 +16,145 @@\n \n package com.palantir.atlasdb.timelock.paxos;\n \n+import java.time.Duration;\n+import java.util.Comparator;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Stream;\n \n-import javax.annotation.Nullable;\n-import javax.annotation.concurrent.NotThreadSafe;\n+import javax.annotation.concurrent.ThreadSafe;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import com.github.benmanes.caffeine.cache.LoadingCache;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Maps;\n import com.google.common.collect.Sets;\n import com.palantir.atlasdb.autobatch.CoalescingRequestFunction;\n import com.palantir.common.streams.KeyedStream;\n+import com.palantir.logsafe.SafeArg;\n+import com.palantir.logsafe.exceptions.SafeIllegalArgumentException;\n+import com.palantir.logsafe.exceptions.SafeIllegalStateException;\n import com.palantir.paxos.PaxosLong;\n \n-/*\n-    This is not thread safe, but it is okay because it is run within an autobatcher, which is configured to not process\n-    multiple batches in parallel.\n- */\n-@NotThreadSafe\n+@ThreadSafe\n final class BatchingPaxosLatestSequenceCache implements CoalescingRequestFunction<Client, PaxosLong> {\n \n     private static final Logger log = LoggerFactory.getLogger(BatchingPaxosLatestSequenceCache.class);\n     private static final PaxosLong DEFAULT_VALUE = PaxosLong.of(BatchPaxosAcceptor.NO_LOG_ENTRY);\n \n-    @Nullable\n-    private AcceptorCacheKey cacheKey = null;\n-    private Map<Client, PaxosLong> cachedEntries = Maps.newHashMap();\n-    private BatchPaxosAcceptor delegate;\n+    private final BatchPaxosAcceptor delegate;\n+\n+    private final Set<Client> clientsSeenSoFar = Sets.newConcurrentHashSet();\n+\n+    private final AtomicReference<TimestampedAcceptorCacheKey> cacheKey = new AtomicReference<>();\n+    private final LoadingCache<AcceptorCacheKey, ConcurrentMap<Client, PaxosLong>> cacheKeysToCaches =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7daa4c3499227e18bc8d885fc1e210e5686156dc"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMDQ4OTI4OnYy", "diffSide": "RIGHT", "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxNzozNjo1MVrOFzBYaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxNzozNjo1MVrOFzBYaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTA0NDMyOA==", "bodyText": "nit: double request in message\nI think this is safe even with 3 attempts, that usually indicates leader churn or getting strangely far behind.", "url": "https://github.com/palantir/atlasdb/pull/4639#discussion_r389044328", "createdAt": "2020-03-06T17:36:51Z", "author": {"login": "jeremyk-91"}, "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "diffHunk": "@@ -16,92 +16,145 @@\n \n package com.palantir.atlasdb.timelock.paxos;\n \n+import java.time.Duration;\n+import java.util.Comparator;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Stream;\n \n-import javax.annotation.Nullable;\n-import javax.annotation.concurrent.NotThreadSafe;\n+import javax.annotation.concurrent.ThreadSafe;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import com.github.benmanes.caffeine.cache.LoadingCache;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Maps;\n import com.google.common.collect.Sets;\n import com.palantir.atlasdb.autobatch.CoalescingRequestFunction;\n import com.palantir.common.streams.KeyedStream;\n+import com.palantir.logsafe.SafeArg;\n+import com.palantir.logsafe.exceptions.SafeIllegalArgumentException;\n+import com.palantir.logsafe.exceptions.SafeIllegalStateException;\n import com.palantir.paxos.PaxosLong;\n \n-/*\n-    This is not thread safe, but it is okay because it is run within an autobatcher, which is configured to not process\n-    multiple batches in parallel.\n- */\n-@NotThreadSafe\n+@ThreadSafe\n final class BatchingPaxosLatestSequenceCache implements CoalescingRequestFunction<Client, PaxosLong> {\n \n     private static final Logger log = LoggerFactory.getLogger(BatchingPaxosLatestSequenceCache.class);\n     private static final PaxosLong DEFAULT_VALUE = PaxosLong.of(BatchPaxosAcceptor.NO_LOG_ENTRY);\n \n-    @Nullable\n-    private AcceptorCacheKey cacheKey = null;\n-    private Map<Client, PaxosLong> cachedEntries = Maps.newHashMap();\n-    private BatchPaxosAcceptor delegate;\n+    private final BatchPaxosAcceptor delegate;\n+\n+    private final Set<Client> clientsSeenSoFar = Sets.newConcurrentHashSet();\n+\n+    private final AtomicReference<TimestampedAcceptorCacheKey> cacheKey = new AtomicReference<>();\n+    private final LoadingCache<AcceptorCacheKey, ConcurrentMap<Client, PaxosLong>> cacheKeysToCaches =\n+            Caffeine.newBuilder()\n+                    .expireAfterAccess(Duration.ofMinutes(1))\n+                    .build($ -> Maps.newConcurrentMap());\n \n     BatchingPaxosLatestSequenceCache(BatchPaxosAcceptor delegate) {\n         this.delegate = delegate;\n     }\n \n     @Override\n-    public Map<Client, PaxosLong> apply(Set<Client> clients) {\n-        try {\n-            return unsafeGetLatest(clients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.info(\"Cache key is invalid, invalidating cache - using deprecated detection method\");\n-            return handleCacheMiss(clients);\n-        }\n-    }\n+    public Map<Client, PaxosLong> apply(Set<Client> requestedClients) {\n+        // always add requested clients so we can easily query with everything we've ever seen when our cache is invalid\n+        clientsSeenSoFar.addAll(requestedClients);\n \n-    private Map<Client, PaxosLong> handleCacheMiss(Set<Client> requestedClients) {\n-        cacheKey = null;\n-        Set<Client> allClients = ImmutableSet.<Client>builder()\n-                .addAll(requestedClients)\n-                .addAll(cachedEntries.keySet())\n-                .build();\n-        cachedEntries.clear();\n-        try {\n-            return unsafeGetLatest(allClients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.warn(\"Empty cache key is still invalid indicates product bug, failing request.\");\n-            throw new RuntimeException(e);\n+        int attempt = 0;\n+        while (attempt < 3) {\n+            TimestampedAcceptorCacheKey timestampedCacheKey = cacheKey.get();\n+            try {\n+                if (timestampedCacheKey == null) {\n+                    return populateNewCache(requestedClients);\n+                } else {\n+                    return populateExistingCache(\n+                            timestampedCacheKey,\n+                            cacheKeysToCaches.get(timestampedCacheKey.cacheKey()),\n+                            requestedClients);\n+                }\n+            } catch (InvalidAcceptorCacheKeyException e) {\n+                log.info(\"Cache key is invalid, invalidating cache and retrying\",\n+                        SafeArg.of(\"attempt\", attempt),\n+                        e);\n+                cacheKey.compareAndSet(timestampedCacheKey, null);\n+                attempt++;\n+            }\n         }\n+\n+        throw new SafeIllegalStateException(\"could not request complete request due to contention in the cache\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7daa4c3499227e18bc8d885fc1e210e5686156dc"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMDU3ODEzOnYy", "diffSide": "RIGHT", "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxODowNToxNFrOFzCPXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxODowNToxNFrOFzCPXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTA1ODM5Nw==", "bodyText": "nit: accumulateLatestCacheKey?", "url": "https://github.com/palantir/atlasdb/pull/4639#discussion_r389058397", "createdAt": "2020-03-06T18:05:14Z", "author": {"login": "jeremyk-91"}, "path": "timelock-impl/src/main/java/com/palantir/atlasdb/timelock/paxos/BatchingPaxosLatestSequenceCache.java", "diffHunk": "@@ -16,92 +16,145 @@\n \n package com.palantir.atlasdb.timelock.paxos;\n \n+import java.time.Duration;\n+import java.util.Comparator;\n import java.util.Map;\n import java.util.Optional;\n import java.util.Set;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Stream;\n \n-import javax.annotation.Nullable;\n-import javax.annotation.concurrent.NotThreadSafe;\n+import javax.annotation.concurrent.ThreadSafe;\n \n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.github.benmanes.caffeine.cache.Caffeine;\n+import com.github.benmanes.caffeine.cache.LoadingCache;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Maps;\n import com.google.common.collect.Sets;\n import com.palantir.atlasdb.autobatch.CoalescingRequestFunction;\n import com.palantir.common.streams.KeyedStream;\n+import com.palantir.logsafe.SafeArg;\n+import com.palantir.logsafe.exceptions.SafeIllegalArgumentException;\n+import com.palantir.logsafe.exceptions.SafeIllegalStateException;\n import com.palantir.paxos.PaxosLong;\n \n-/*\n-    This is not thread safe, but it is okay because it is run within an autobatcher, which is configured to not process\n-    multiple batches in parallel.\n- */\n-@NotThreadSafe\n+@ThreadSafe\n final class BatchingPaxosLatestSequenceCache implements CoalescingRequestFunction<Client, PaxosLong> {\n \n     private static final Logger log = LoggerFactory.getLogger(BatchingPaxosLatestSequenceCache.class);\n     private static final PaxosLong DEFAULT_VALUE = PaxosLong.of(BatchPaxosAcceptor.NO_LOG_ENTRY);\n \n-    @Nullable\n-    private AcceptorCacheKey cacheKey = null;\n-    private Map<Client, PaxosLong> cachedEntries = Maps.newHashMap();\n-    private BatchPaxosAcceptor delegate;\n+    private final BatchPaxosAcceptor delegate;\n+\n+    private final Set<Client> clientsSeenSoFar = Sets.newConcurrentHashSet();\n+\n+    private final AtomicReference<TimestampedAcceptorCacheKey> cacheKey = new AtomicReference<>();\n+    private final LoadingCache<AcceptorCacheKey, ConcurrentMap<Client, PaxosLong>> cacheKeysToCaches =\n+            Caffeine.newBuilder()\n+                    .expireAfterAccess(Duration.ofMinutes(1))\n+                    .build($ -> Maps.newConcurrentMap());\n \n     BatchingPaxosLatestSequenceCache(BatchPaxosAcceptor delegate) {\n         this.delegate = delegate;\n     }\n \n     @Override\n-    public Map<Client, PaxosLong> apply(Set<Client> clients) {\n-        try {\n-            return unsafeGetLatest(clients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.info(\"Cache key is invalid, invalidating cache - using deprecated detection method\");\n-            return handleCacheMiss(clients);\n-        }\n-    }\n+    public Map<Client, PaxosLong> apply(Set<Client> requestedClients) {\n+        // always add requested clients so we can easily query with everything we've ever seen when our cache is invalid\n+        clientsSeenSoFar.addAll(requestedClients);\n \n-    private Map<Client, PaxosLong> handleCacheMiss(Set<Client> requestedClients) {\n-        cacheKey = null;\n-        Set<Client> allClients = ImmutableSet.<Client>builder()\n-                .addAll(requestedClients)\n-                .addAll(cachedEntries.keySet())\n-                .build();\n-        cachedEntries.clear();\n-        try {\n-            return unsafeGetLatest(allClients);\n-        } catch (InvalidAcceptorCacheKeyException e) {\n-            log.warn(\"Empty cache key is still invalid indicates product bug, failing request.\");\n-            throw new RuntimeException(e);\n+        int attempt = 0;\n+        while (attempt < 3) {\n+            TimestampedAcceptorCacheKey timestampedCacheKey = cacheKey.get();\n+            try {\n+                if (timestampedCacheKey == null) {\n+                    return populateNewCache(requestedClients);\n+                } else {\n+                    return populateExistingCache(\n+                            timestampedCacheKey,\n+                            cacheKeysToCaches.get(timestampedCacheKey.cacheKey()),\n+                            requestedClients);\n+                }\n+            } catch (InvalidAcceptorCacheKeyException e) {\n+                log.info(\"Cache key is invalid, invalidating cache and retrying\",\n+                        SafeArg.of(\"attempt\", attempt),\n+                        e);\n+                cacheKey.compareAndSet(timestampedCacheKey, null);\n+                attempt++;\n+            }\n         }\n+\n+        throw new SafeIllegalStateException(\"could not request complete request due to contention in the cache\");\n     }\n \n-    private Map<Client, PaxosLong> unsafeGetLatest(Set<Client> clients) throws InvalidAcceptorCacheKeyException {\n-        if (cacheKey == null) {\n-            processDigest(delegate.latestSequencesPreparedOrAccepted(Optional.empty(), clients));\n-            return getResponseMap(clients);\n-        }\n+    private Map<Client, PaxosLong> populateNewCache(Set<Client> requestedClients)\n+            throws InvalidAcceptorCacheKeyException {\n+        AcceptorCacheDigest digest = delegate.latestSequencesPreparedOrAccepted(Optional.empty(), clientsSeenSoFar);\n+        ConcurrentMap<Client, PaxosLong> newEntriesToCache =\n+                cacheKeysToCaches.get(digest.newCacheKey());\n+        processDigest(newEntriesToCache, digest);\n+        return getResponseMap(newEntriesToCache, requestedClients);\n+    }\n \n-        Set<Client> newClients = Sets.difference(clients, cachedEntries.keySet());\n+    private Map<Client, PaxosLong> populateExistingCache(\n+            TimestampedAcceptorCacheKey timestampedCacheKey,\n+            ConcurrentMap<Client, PaxosLong> currentCachedEntries,\n+            Set<Client> requestedClients)\n+            throws InvalidAcceptorCacheKeyException {\n+        Set<Client> newClients = ImmutableSet.copyOf(Sets.difference(requestedClients, currentCachedEntries.keySet()));\n         if (newClients.isEmpty()) {\n-            delegate.latestSequencesPreparedOrAcceptedCached(cacheKey).ifPresent(this::processDigest);\n-            return getResponseMap(clients);\n+            delegate.latestSequencesPreparedOrAcceptedCached(timestampedCacheKey.cacheKey())\n+                    .ifPresent(digest -> processDigest(currentCachedEntries, digest));\n+            return getResponseMap(currentCachedEntries, requestedClients);\n         } else {\n-            processDigest(delegate.latestSequencesPreparedOrAccepted(Optional.of(cacheKey), newClients));\n-            return getResponseMap(clients);\n+            processDigest(currentCachedEntries, delegate.latestSequencesPreparedOrAccepted(\n+                    Optional.of(timestampedCacheKey.cacheKey()),\n+                    newClients));\n+            return getResponseMap(currentCachedEntries, requestedClients);\n         }\n     }\n \n-    private Map<Client, PaxosLong> getResponseMap(Set<Client> clientsInRequest) {\n-        return Maps.toMap(clientsInRequest, client -> cachedEntries.getOrDefault(client, DEFAULT_VALUE));\n+    private void processDigest(ConcurrentMap<Client, PaxosLong> currentCachedEntries, AcceptorCacheDigest digest) {\n+        TimestampedAcceptorCacheKey newCacheKey = TimestampedAcceptorCacheKey.of(digest);\n+        // this shares the same map with \"previous\" cache keys, if it's too confusing we can always copy it potentially\n+        ConcurrentMap<Client, PaxosLong> newCachedEntries =\n+                cacheKeysToCaches.get(newCacheKey.cacheKey(), $ -> currentCachedEntries);\n+        KeyedStream.stream(digest.updates())\n+                .map(PaxosLong::of)\n+                .forEach((client, paxosLong) ->\n+                        newCachedEntries.merge(client, paxosLong, BatchingPaxosLatestSequenceCache::max));\n+\n+        // for a *new* mapping, setting the cache key must happen *after* we've setup the mapping, so that concurrent\n+        // clients will not reference an in-progress populating map which can be empty.\n+        maybeSetNewCacheKey(newCacheKey);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7daa4c3499227e18bc8d885fc1e210e5686156dc"}, "originalPosition": 161}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3116, "cost": 1, "resetAt": "2021-11-12T12:57:47Z"}}}