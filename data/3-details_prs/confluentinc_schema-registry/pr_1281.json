{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYwMjE4NzIy", "number": 1281, "title": "Reflection serde", "bodyText": "The patch adds dynamic resolution of the Avro schema when Reflection API is used.\nIt solves one of the limitations of the initial implementation of the feature. The change allows to use the Reflection API deserialization in the following scenarios:\n\nReflectionAvroSerde is defined globally for KStreams\nthe source topic contains messages of various types (as per TopicRecordNameStrategy)", "createdAt": "2020-01-07T23:32:14Z", "url": "https://github.com/confluentinc/schema-registry/pull/1281", "merged": true, "mergeCommit": {"oid": "f8ca0ce3ac70b04245ba194c6bcd1adf5f872cbc"}, "closed": true, "closedAt": "2020-01-13T18:56:37Z", "author": {"login": "piotrsmolinski"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb4JIBqAH2gAyMzYwMjE4NzIyOmI4NTAwMWEyODFhZWJkMjdlODRlNTljMWUwMzk5Njk4NzkyOTQ1N2Q=", "endCursor": "Y3Vyc29yOnYyOpPPAAABb6BDr2AFqTM0MjA2NDMwOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "b85001a281aebd27e84e59c1e03996987929457d", "author": {"user": {"login": "piotrsmolinski", "name": null}}, "url": "https://github.com/confluentinc/schema-registry/commit/b85001a281aebd27e84e59c1e03996987929457d", "committedDate": "2020-01-07T23:12:36Z", "message": "enabled default constructor\n\nDefault constructor allows the SerDe to be used as default serde in KStreams config."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "642d3f73f277a4c3b7f898df1312db175a3cd32d", "author": {"user": {"login": "piotrsmolinski", "name": null}}, "url": "https://github.com/confluentinc/schema-registry/commit/642d3f73f277a4c3b7f898df1312db175a3cd32d", "committedDate": "2020-01-07T23:25:52Z", "message": "added logic to resolve the reflection schema when it is not provided during construction"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzM5NjE5ODk2", "url": "https://github.com/confluentinc/schema-registry/pull/1281#pullrequestreview-339619896", "createdAt": "2020-01-08T02:37:07Z", "commit": {"oid": "642d3f73f277a4c3b7f898df1312db175a3cd32d"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wOFQwMjozNzowN1rOFbLYQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0wOFQwMjozOTowMVrOFbLZqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDA0MjMwNw==", "bodyText": "nit: The build is complaining that whitespace is needed around !=", "url": "https://github.com/confluentinc/schema-registry/pull/1281#discussion_r364042307", "createdAt": "2020-01-08T02:37:07Z", "author": {"login": "rayokota"}, "path": "avro-serializer/src/main/java/io/confluent/kafka/serializers/AbstractKafkaAvroDeserializer.java", "diffHunk": "@@ -173,58 +173,91 @@ protected GenericContainerWithVersion deserializeWithSchemaAndVersion(\n   }\n \n   protected DatumReader<?> getDatumReader(Schema writerSchema, Schema readerSchema) {\n+    // normalize reader schema\n+    readerSchema = getReaderSchema(writerSchema, readerSchema);\n     boolean writerSchemaIsPrimitive =\n         AvroSchemaUtils.getPrimitiveSchemas().values().contains(writerSchema);\n-    // do not use SpecificDatumReader if writerSchema is a primitive\n-    if (useSchemaReflection && !writerSchemaIsPrimitive) {\n-      if (readerSchema == null) {\n-        throw new SerializationException(\n-            \"Reader schema cannot be null when using Avro schema reflection\");\n-      }\n+    if (writerSchemaIsPrimitive) {\n+      return new GenericDatumReader<>(writerSchema, readerSchema);\n+    } else if (useSchemaReflection) {\n       return new ReflectDatumReader<>(writerSchema, readerSchema);\n-    } else if (useSpecificAvroReader && !writerSchemaIsPrimitive) {\n-      if (readerSchema == null) {\n-        readerSchema = getReaderSchema(writerSchema);\n-      }\n+    } else if (useSpecificAvroReader) {\n       return new SpecificDatumReader<>(writerSchema, readerSchema);\n     } else {\n-      if (readerSchema == null) {\n-        return new GenericDatumReader<>(writerSchema);\n-      }\n       return new GenericDatumReader<>(writerSchema, readerSchema);\n     }\n   }\n \n-  @SuppressWarnings(\"unchecked\")\n-  private Schema getReaderSchema(Schema writerSchema) {\n-    Schema readerSchema = readerSchemaCache.get(writerSchema.getFullName());\n-    if (readerSchema == null) {\n-      Class<SpecificRecord> readerClass = SpecificData.get().getClass(writerSchema);\n-      if (readerClass != null) {\n-        try {\n-          readerSchema = readerClass.newInstance().getSchema();\n-        } catch (InstantiationException e) {\n-          throw new SerializationException(writerSchema.getFullName()\n-                                           + \" specified by the \"\n-                                           + \"writers schema could not be instantiated to \"\n-                                           + \"find the readers schema.\");\n-        } catch (IllegalAccessException e) {\n-          throw new SerializationException(writerSchema.getFullName()\n-                                           + \" specified by the \"\n-                                           + \"writers schema is not allowed to be instantiated \"\n-                                           + \"to find the readers schema.\");\n-        }\n-        readerSchemaCache.put(writerSchema.getFullName(), readerSchema);\n-      } else {\n-        throw new SerializationException(\"Could not find class \"\n-                                         + writerSchema.getFullName()\n-                                         + \" specified in writer's schema whilst finding reader's \"\n-                                         + \"schema for a SpecificRecord.\");\n-      }\n+  /**\n+   * Normalizes the reader schema, puts the resolved schema into the cache. \n+   * <li>\n+   * <ul>if the reader schema is provided, use the provided one</ul>\n+   * <ul>if the reader schema is cached for the writer schema full name, use the cached value</ul>\n+   * <ul>if the writer schema is primitive, use the writer one</ul>\n+   * <ul>if schema reflection is used, generate one from the class referred by writer schema</ul>\n+   * <ul>if generated classes are used, query the class referred by writer schema</ul>\n+   * <ul>otherwise use the writer schema</ul>\n+   * </li>\n+   */\n+  private Schema getReaderSchema(Schema writerSchema, Schema readerSchema) {\n+    if (readerSchema!=null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "642d3f73f277a4c3b7f898df1312db175a3cd32d"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDA0MjM1MQ==", "bodyText": "nit: The build is complaining that whitespace is needed around !=", "url": "https://github.com/confluentinc/schema-registry/pull/1281#discussion_r364042351", "createdAt": "2020-01-08T02:37:22Z", "author": {"login": "rayokota"}, "path": "avro-serializer/src/main/java/io/confluent/kafka/serializers/AbstractKafkaAvroDeserializer.java", "diffHunk": "@@ -173,58 +173,91 @@ protected GenericContainerWithVersion deserializeWithSchemaAndVersion(\n   }\n \n   protected DatumReader<?> getDatumReader(Schema writerSchema, Schema readerSchema) {\n+    // normalize reader schema\n+    readerSchema = getReaderSchema(writerSchema, readerSchema);\n     boolean writerSchemaIsPrimitive =\n         AvroSchemaUtils.getPrimitiveSchemas().values().contains(writerSchema);\n-    // do not use SpecificDatumReader if writerSchema is a primitive\n-    if (useSchemaReflection && !writerSchemaIsPrimitive) {\n-      if (readerSchema == null) {\n-        throw new SerializationException(\n-            \"Reader schema cannot be null when using Avro schema reflection\");\n-      }\n+    if (writerSchemaIsPrimitive) {\n+      return new GenericDatumReader<>(writerSchema, readerSchema);\n+    } else if (useSchemaReflection) {\n       return new ReflectDatumReader<>(writerSchema, readerSchema);\n-    } else if (useSpecificAvroReader && !writerSchemaIsPrimitive) {\n-      if (readerSchema == null) {\n-        readerSchema = getReaderSchema(writerSchema);\n-      }\n+    } else if (useSpecificAvroReader) {\n       return new SpecificDatumReader<>(writerSchema, readerSchema);\n     } else {\n-      if (readerSchema == null) {\n-        return new GenericDatumReader<>(writerSchema);\n-      }\n       return new GenericDatumReader<>(writerSchema, readerSchema);\n     }\n   }\n \n-  @SuppressWarnings(\"unchecked\")\n-  private Schema getReaderSchema(Schema writerSchema) {\n-    Schema readerSchema = readerSchemaCache.get(writerSchema.getFullName());\n-    if (readerSchema == null) {\n-      Class<SpecificRecord> readerClass = SpecificData.get().getClass(writerSchema);\n-      if (readerClass != null) {\n-        try {\n-          readerSchema = readerClass.newInstance().getSchema();\n-        } catch (InstantiationException e) {\n-          throw new SerializationException(writerSchema.getFullName()\n-                                           + \" specified by the \"\n-                                           + \"writers schema could not be instantiated to \"\n-                                           + \"find the readers schema.\");\n-        } catch (IllegalAccessException e) {\n-          throw new SerializationException(writerSchema.getFullName()\n-                                           + \" specified by the \"\n-                                           + \"writers schema is not allowed to be instantiated \"\n-                                           + \"to find the readers schema.\");\n-        }\n-        readerSchemaCache.put(writerSchema.getFullName(), readerSchema);\n-      } else {\n-        throw new SerializationException(\"Could not find class \"\n-                                         + writerSchema.getFullName()\n-                                         + \" specified in writer's schema whilst finding reader's \"\n-                                         + \"schema for a SpecificRecord.\");\n-      }\n+  /**\n+   * Normalizes the reader schema, puts the resolved schema into the cache. \n+   * <li>\n+   * <ul>if the reader schema is provided, use the provided one</ul>\n+   * <ul>if the reader schema is cached for the writer schema full name, use the cached value</ul>\n+   * <ul>if the writer schema is primitive, use the writer one</ul>\n+   * <ul>if schema reflection is used, generate one from the class referred by writer schema</ul>\n+   * <ul>if generated classes are used, query the class referred by writer schema</ul>\n+   * <ul>otherwise use the writer schema</ul>\n+   * </li>\n+   */\n+  private Schema getReaderSchema(Schema writerSchema, Schema readerSchema) {\n+    if (readerSchema!=null) {\n+      return readerSchema;\n+    }\n+    readerSchema = readerSchemaCache.get(writerSchema.getFullName());\n+    if (readerSchema!=null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "642d3f73f277a4c3b7f898df1312db175a3cd32d"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDA0MjYzNw==", "bodyText": "Are there tests that use this?", "url": "https://github.com/confluentinc/schema-registry/pull/1281#discussion_r364042637", "createdAt": "2020-01-08T02:38:52Z", "author": {"login": "rayokota"}, "path": "avro-serde/src/main/java/io/confluent/kafka/streams/serdes/avro/ReflectionAvroDeserializer.java", "diffHunk": "@@ -40,11 +40,24 @@\n   private final KafkaAvroDeserializer inner;\n   private final Schema schema;\n \n+  public ReflectionAvroDeserializer() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "642d3f73f277a4c3b7f898df1312db175a3cd32d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NDA0MjY2NQ==", "bodyText": "Are there tests that use this?", "url": "https://github.com/confluentinc/schema-registry/pull/1281#discussion_r364042665", "createdAt": "2020-01-08T02:39:01Z", "author": {"login": "rayokota"}, "path": "avro-serde/src/main/java/io/confluent/kafka/streams/serdes/avro/ReflectionAvroDeserializer.java", "diffHunk": "@@ -40,11 +40,24 @@\n   private final KafkaAvroDeserializer inner;\n   private final Schema schema;\n \n+  public ReflectionAvroDeserializer() {\n+    this.schema = null;\n+    this.inner = new KafkaAvroDeserializer();\n+  }\n+\n   public ReflectionAvroDeserializer(Class<T> type) {\n     this.schema = ReflectData.get().getSchema(type);\n     this.inner = new KafkaAvroDeserializer();\n   }\n \n+  /**\n+   * For testing purposes only.\n+   */\n+  ReflectionAvroDeserializer(final SchemaRegistryClient client) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "642d3f73f277a4c3b7f898df1312db175a3cd32d"}, "originalPosition": 17}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "99eea98abed0947c170434e25f7e8dbdb2b64cd9", "author": {"user": {"login": "piotrsmolinski", "name": null}}, "url": "https://github.com/confluentinc/schema-registry/commit/99eea98abed0947c170434e25f7e8dbdb2b64cd9", "committedDate": "2020-01-08T10:00:07Z", "message": "fixed whitespace warning"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "17a3989a3bd2f19d4f40a6ab85c9b2e050a538ad", "author": {"user": {"login": "piotrsmolinski", "name": null}}, "url": "https://github.com/confluentinc/schema-registry/commit/17a3989a3bd2f19d4f40a6ab85c9b2e050a538ad", "committedDate": "2020-01-08T13:42:01Z", "message": "added missing constructors"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d113ffeed42dbb18352783a2157a220bdf3bf72b", "author": {"user": {"login": "piotrsmolinski", "name": null}}, "url": "https://github.com/confluentinc/schema-registry/commit/d113ffeed42dbb18352783a2157a220bdf3bf72b", "committedDate": "2020-01-08T13:43:00Z", "message": "cloned the original test case to support dynamic type recognition"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7fed48e8e000103c346c511194e13643a21dd911", "author": {"user": {"login": "piotrsmolinski", "name": null}}, "url": "https://github.com/confluentinc/schema-registry/commit/7fed48e8e000103c346c511194e13643a21dd911", "committedDate": "2020-01-13T14:10:19Z", "message": "cache only generated schemas\n\nThe failure in testVersionMaintained was caused by caching the schema for given name. In case\nof GenericDatumReader the schema is as it comes from the serialized record. This means\nif the schema evolves, there could be multiple representations of the same avro record."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQyMDY0MzA5", "url": "https://github.com/confluentinc/schema-registry/pull/1281#pullrequestreview-342064309", "createdAt": "2020-01-13T18:56:28Z", "commit": {"oid": "7fed48e8e000103c346c511194e13643a21dd911"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4495, "cost": 1, "resetAt": "2021-11-01T13:51:04Z"}}}