{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDcwNjg1NTQ5", "number": 1591, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwNTo0MzoyOVrOEca7mQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwNTo1MjowM1rOEcbC2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MjM2ODI1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/io/confluent/kafka/schemaregistry/storage/OffsetCheckpoint.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwNTo0MzoyOVrOHG8pMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxNjoyMDoxMFrOHHTxmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA0NzA5MA==", "bodyText": "What if it exists but it's a file instead ?", "url": "https://github.com/confluentinc/schema-registry/pull/1591#discussion_r477047090", "createdAt": "2020-08-26T05:43:29Z", "author": {"login": "dragosvictor"}, "path": "core/src/main/java/io/confluent/kafka/schemaregistry/storage/OffsetCheckpoint.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.storage;\n+\n+import java.io.BufferedReader;\n+import java.io.BufferedWriter;\n+import java.io.Closeable;\n+import java.io.EOFException;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.FileLock;\n+import java.nio.channels.OverlappingFileLockException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.regex.Pattern;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.utils.Utils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This class saves out a map of topic/partition=&gt;offsets to a file.\n+ * The format of the file is UTF-8 text containing the following:\n+ * <pre>\n+ *   &lt;version&gt;\n+ *   &lt;n&gt;\n+ *   &lt;topic_name_1&gt; &lt;partition_1&gt; &lt;offset_1&gt;\n+ *   .\n+ *   .\n+ *   .\n+ *   &lt;topic_name_n&gt; &lt;partition_n&gt; &lt;offset_n&gt;\n+ * </pre>\n+ * The first line contains a number designating the format version (currently 0),\n+ * the get line contains a number giving the total number of offsets.\n+ * Each successive line gives a topic/partition/offset triple separated by spaces.\n+ */\n+public class OffsetCheckpoint implements Closeable {\n+  private static final Logger LOG = LoggerFactory.getLogger(OffsetCheckpoint.class);\n+\n+  public static final String CHECKPOINT_FILE_NAME = \".checkpoint\";\n+\n+  public static final String LOCK_FILE_NAME = \".lock\";\n+\n+  private static final Pattern WHITESPACE_MINIMUM_ONCE = Pattern.compile(\"\\\\s+\");\n+\n+  private final File file;\n+  private final Object lock;\n+  private FileChannel channel;\n+  private FileLock fileLock;\n+  private int version;\n+\n+  public OffsetCheckpoint(String checkpointDir, int version, String topic) throws IOException {\n+    File baseDir = baseDir(checkpointDir, topic);\n+    this.file = new File(baseDir, CHECKPOINT_FILE_NAME);\n+    lock = new Object();\n+\n+    final File lockFile = new File(baseDir, LOCK_FILE_NAME);\n+    final FileChannel channel =\n+        FileChannel.open(lockFile.toPath(), StandardOpenOption.CREATE, StandardOpenOption.WRITE);\n+    final FileLock fileLock = tryLock(channel);\n+    if (fileLock == null) {\n+      channel.close();\n+      throw new IOException(\"Could not obtain file lock\");\n+    }\n+    this.channel = channel;\n+    this.fileLock = fileLock;\n+    this.version = version;\n+  }\n+\n+  private File baseDir(final String checkpointDir, String topic) throws IOException {\n+    final File dir = new File(checkpointDir, topic);\n+    if (!dir.exists() && !dir.mkdirs()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ec301d494509f8a763b06e3c8ff4139ecee5685"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQyNjA3Mg==", "bodyText": "Thanks, I'll fix", "url": "https://github.com/confluentinc/schema-registry/pull/1591#discussion_r477426072", "createdAt": "2020-08-26T16:20:10Z", "author": {"login": "rayokota"}, "path": "core/src/main/java/io/confluent/kafka/schemaregistry/storage/OffsetCheckpoint.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.storage;\n+\n+import java.io.BufferedReader;\n+import java.io.BufferedWriter;\n+import java.io.Closeable;\n+import java.io.EOFException;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.FileLock;\n+import java.nio.channels.OverlappingFileLockException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.regex.Pattern;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.utils.Utils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This class saves out a map of topic/partition=&gt;offsets to a file.\n+ * The format of the file is UTF-8 text containing the following:\n+ * <pre>\n+ *   &lt;version&gt;\n+ *   &lt;n&gt;\n+ *   &lt;topic_name_1&gt; &lt;partition_1&gt; &lt;offset_1&gt;\n+ *   .\n+ *   .\n+ *   .\n+ *   &lt;topic_name_n&gt; &lt;partition_n&gt; &lt;offset_n&gt;\n+ * </pre>\n+ * The first line contains a number designating the format version (currently 0),\n+ * the get line contains a number giving the total number of offsets.\n+ * Each successive line gives a topic/partition/offset triple separated by spaces.\n+ */\n+public class OffsetCheckpoint implements Closeable {\n+  private static final Logger LOG = LoggerFactory.getLogger(OffsetCheckpoint.class);\n+\n+  public static final String CHECKPOINT_FILE_NAME = \".checkpoint\";\n+\n+  public static final String LOCK_FILE_NAME = \".lock\";\n+\n+  private static final Pattern WHITESPACE_MINIMUM_ONCE = Pattern.compile(\"\\\\s+\");\n+\n+  private final File file;\n+  private final Object lock;\n+  private FileChannel channel;\n+  private FileLock fileLock;\n+  private int version;\n+\n+  public OffsetCheckpoint(String checkpointDir, int version, String topic) throws IOException {\n+    File baseDir = baseDir(checkpointDir, topic);\n+    this.file = new File(baseDir, CHECKPOINT_FILE_NAME);\n+    lock = new Object();\n+\n+    final File lockFile = new File(baseDir, LOCK_FILE_NAME);\n+    final FileChannel channel =\n+        FileChannel.open(lockFile.toPath(), StandardOpenOption.CREATE, StandardOpenOption.WRITE);\n+    final FileLock fileLock = tryLock(channel);\n+    if (fileLock == null) {\n+      channel.close();\n+      throw new IOException(\"Could not obtain file lock\");\n+    }\n+    this.channel = channel;\n+    this.fileLock = fileLock;\n+    this.version = version;\n+  }\n+\n+  private File baseDir(final String checkpointDir, String topic) throws IOException {\n+    final File dir = new File(checkpointDir, topic);\n+    if (!dir.exists() && !dir.mkdirs()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA0NzA5MA=="}, "originalCommit": {"oid": "3ec301d494509f8a763b06e3c8ff4139ecee5685"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MjM4NjgwOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/io/confluent/kafka/schemaregistry/storage/OffsetCheckpoint.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwNTo1MjowM1rOHG8z7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxNjoyMDo1MlrOHHTzTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA0OTgzOQ==", "bodyText": "Does fileOutputStream need to be closed as well ?", "url": "https://github.com/confluentinc/schema-registry/pull/1591#discussion_r477049839", "createdAt": "2020-08-26T05:52:03Z", "author": {"login": "dragosvictor"}, "path": "core/src/main/java/io/confluent/kafka/schemaregistry/storage/OffsetCheckpoint.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.storage;\n+\n+import java.io.BufferedReader;\n+import java.io.BufferedWriter;\n+import java.io.Closeable;\n+import java.io.EOFException;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.FileLock;\n+import java.nio.channels.OverlappingFileLockException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.regex.Pattern;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.utils.Utils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This class saves out a map of topic/partition=&gt;offsets to a file.\n+ * The format of the file is UTF-8 text containing the following:\n+ * <pre>\n+ *   &lt;version&gt;\n+ *   &lt;n&gt;\n+ *   &lt;topic_name_1&gt; &lt;partition_1&gt; &lt;offset_1&gt;\n+ *   .\n+ *   .\n+ *   .\n+ *   &lt;topic_name_n&gt; &lt;partition_n&gt; &lt;offset_n&gt;\n+ * </pre>\n+ * The first line contains a number designating the format version (currently 0),\n+ * the get line contains a number giving the total number of offsets.\n+ * Each successive line gives a topic/partition/offset triple separated by spaces.\n+ */\n+public class OffsetCheckpoint implements Closeable {\n+  private static final Logger LOG = LoggerFactory.getLogger(OffsetCheckpoint.class);\n+\n+  public static final String CHECKPOINT_FILE_NAME = \".checkpoint\";\n+\n+  public static final String LOCK_FILE_NAME = \".lock\";\n+\n+  private static final Pattern WHITESPACE_MINIMUM_ONCE = Pattern.compile(\"\\\\s+\");\n+\n+  private final File file;\n+  private final Object lock;\n+  private FileChannel channel;\n+  private FileLock fileLock;\n+  private int version;\n+\n+  public OffsetCheckpoint(String checkpointDir, int version, String topic) throws IOException {\n+    File baseDir = baseDir(checkpointDir, topic);\n+    this.file = new File(baseDir, CHECKPOINT_FILE_NAME);\n+    lock = new Object();\n+\n+    final File lockFile = new File(baseDir, LOCK_FILE_NAME);\n+    final FileChannel channel =\n+        FileChannel.open(lockFile.toPath(), StandardOpenOption.CREATE, StandardOpenOption.WRITE);\n+    final FileLock fileLock = tryLock(channel);\n+    if (fileLock == null) {\n+      channel.close();\n+      throw new IOException(\"Could not obtain file lock\");\n+    }\n+    this.channel = channel;\n+    this.fileLock = fileLock;\n+    this.version = version;\n+  }\n+\n+  private File baseDir(final String checkpointDir, String topic) throws IOException {\n+    final File dir = new File(checkpointDir, topic);\n+    if (!dir.exists() && !dir.mkdirs()) {\n+      throw new IOException(\n+          String.format(\n+              \"checkpoint directory [%s] doesn't exist and couldn't be created\", dir.getPath()));\n+    }\n+    return dir;\n+  }\n+\n+  private FileLock tryLock(final FileChannel channel) throws IOException {\n+    try {\n+      return channel.tryLock();\n+    } catch (final OverlappingFileLockException e) {\n+      return null;\n+    }\n+  }\n+\n+  /**\n+   * Write the given offsets to the checkpoint file. All offsets should be non-negative.\n+   *\n+   * @throws IOException if any file operation fails with an IO exception\n+   */\n+  public void write(final Map<TopicPartition, Long> offsets) throws IOException {\n+    // if the version is negative, skip\n+    if (version < 0) {\n+      return;\n+    }\n+    // if there is no offsets, skip writing the file to save disk IOs\n+    if (offsets.isEmpty()) {\n+      return;\n+    }\n+\n+    synchronized (lock) {\n+      // write to temp file and then swap with the existing file\n+      final File temp = new File(file.getAbsolutePath() + \".tmp\");\n+      LOG.trace(\"Writing tmp checkpoint file {}\", temp.getAbsolutePath());\n+\n+      final FileOutputStream fileOutputStream = new FileOutputStream(temp);\n+      try (final BufferedWriter writer = new BufferedWriter(\n+          new OutputStreamWriter(fileOutputStream, StandardCharsets.UTF_8))) {\n+        writeIntLine(writer, version);\n+        writeIntLine(writer, offsets.size());\n+\n+        for (final Map.Entry<TopicPartition, Long> entry : offsets.entrySet()) {\n+          final TopicPartition tp = entry.getKey();\n+          final Long offset = entry.getValue();\n+          if (offset >= 0L) {\n+            writeEntry(writer, tp, offset);\n+          } else {\n+            LOG.warn(\"Received offset={} to write to checkpoint file for {}\", offset, tp);\n+          }\n+        }\n+\n+        writer.flush();\n+        fileOutputStream.getFD().sync();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ec301d494509f8a763b06e3c8ff4139ecee5685"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzQyNjUwOA==", "bodyText": "No, it will be closed by the wrapping BufferedWriter", "url": "https://github.com/confluentinc/schema-registry/pull/1591#discussion_r477426508", "createdAt": "2020-08-26T16:20:52Z", "author": {"login": "rayokota"}, "path": "core/src/main/java/io/confluent/kafka/schemaregistry/storage/OffsetCheckpoint.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Copyright 2020 Confluent Inc.\n+ *\n+ * Licensed under the Confluent Community License (the \"License\"); you may not use\n+ * this file except in compliance with the License.  You may obtain a copy of the\n+ * License at\n+ *\n+ * http://www.confluent.io/confluent-community-license\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n+ * WARRANTIES OF ANY KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ */\n+\n+package io.confluent.kafka.schemaregistry.storage;\n+\n+import java.io.BufferedReader;\n+import java.io.BufferedWriter;\n+import java.io.Closeable;\n+import java.io.EOFException;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStreamWriter;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.FileLock;\n+import java.nio.channels.OverlappingFileLockException;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.StandardOpenOption;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.regex.Pattern;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.utils.Utils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This class saves out a map of topic/partition=&gt;offsets to a file.\n+ * The format of the file is UTF-8 text containing the following:\n+ * <pre>\n+ *   &lt;version&gt;\n+ *   &lt;n&gt;\n+ *   &lt;topic_name_1&gt; &lt;partition_1&gt; &lt;offset_1&gt;\n+ *   .\n+ *   .\n+ *   .\n+ *   &lt;topic_name_n&gt; &lt;partition_n&gt; &lt;offset_n&gt;\n+ * </pre>\n+ * The first line contains a number designating the format version (currently 0),\n+ * the get line contains a number giving the total number of offsets.\n+ * Each successive line gives a topic/partition/offset triple separated by spaces.\n+ */\n+public class OffsetCheckpoint implements Closeable {\n+  private static final Logger LOG = LoggerFactory.getLogger(OffsetCheckpoint.class);\n+\n+  public static final String CHECKPOINT_FILE_NAME = \".checkpoint\";\n+\n+  public static final String LOCK_FILE_NAME = \".lock\";\n+\n+  private static final Pattern WHITESPACE_MINIMUM_ONCE = Pattern.compile(\"\\\\s+\");\n+\n+  private final File file;\n+  private final Object lock;\n+  private FileChannel channel;\n+  private FileLock fileLock;\n+  private int version;\n+\n+  public OffsetCheckpoint(String checkpointDir, int version, String topic) throws IOException {\n+    File baseDir = baseDir(checkpointDir, topic);\n+    this.file = new File(baseDir, CHECKPOINT_FILE_NAME);\n+    lock = new Object();\n+\n+    final File lockFile = new File(baseDir, LOCK_FILE_NAME);\n+    final FileChannel channel =\n+        FileChannel.open(lockFile.toPath(), StandardOpenOption.CREATE, StandardOpenOption.WRITE);\n+    final FileLock fileLock = tryLock(channel);\n+    if (fileLock == null) {\n+      channel.close();\n+      throw new IOException(\"Could not obtain file lock\");\n+    }\n+    this.channel = channel;\n+    this.fileLock = fileLock;\n+    this.version = version;\n+  }\n+\n+  private File baseDir(final String checkpointDir, String topic) throws IOException {\n+    final File dir = new File(checkpointDir, topic);\n+    if (!dir.exists() && !dir.mkdirs()) {\n+      throw new IOException(\n+          String.format(\n+              \"checkpoint directory [%s] doesn't exist and couldn't be created\", dir.getPath()));\n+    }\n+    return dir;\n+  }\n+\n+  private FileLock tryLock(final FileChannel channel) throws IOException {\n+    try {\n+      return channel.tryLock();\n+    } catch (final OverlappingFileLockException e) {\n+      return null;\n+    }\n+  }\n+\n+  /**\n+   * Write the given offsets to the checkpoint file. All offsets should be non-negative.\n+   *\n+   * @throws IOException if any file operation fails with an IO exception\n+   */\n+  public void write(final Map<TopicPartition, Long> offsets) throws IOException {\n+    // if the version is negative, skip\n+    if (version < 0) {\n+      return;\n+    }\n+    // if there is no offsets, skip writing the file to save disk IOs\n+    if (offsets.isEmpty()) {\n+      return;\n+    }\n+\n+    synchronized (lock) {\n+      // write to temp file and then swap with the existing file\n+      final File temp = new File(file.getAbsolutePath() + \".tmp\");\n+      LOG.trace(\"Writing tmp checkpoint file {}\", temp.getAbsolutePath());\n+\n+      final FileOutputStream fileOutputStream = new FileOutputStream(temp);\n+      try (final BufferedWriter writer = new BufferedWriter(\n+          new OutputStreamWriter(fileOutputStream, StandardCharsets.UTF_8))) {\n+        writeIntLine(writer, version);\n+        writeIntLine(writer, offsets.size());\n+\n+        for (final Map.Entry<TopicPartition, Long> entry : offsets.entrySet()) {\n+          final TopicPartition tp = entry.getKey();\n+          final Long offset = entry.getValue();\n+          if (offset >= 0L) {\n+            writeEntry(writer, tp, offset);\n+          } else {\n+            LOG.warn(\"Received offset={} to write to checkpoint file for {}\", offset, tp);\n+          }\n+        }\n+\n+        writer.flush();\n+        fileOutputStream.getFD().sync();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA0OTgzOQ=="}, "originalCommit": {"oid": "3ec301d494509f8a763b06e3c8ff4139ecee5685"}, "originalPosition": 146}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1816, "cost": 1, "resetAt": "2021-11-12T19:05:54Z"}}}