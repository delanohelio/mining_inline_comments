{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMxOTQxMDkz", "number": 1738, "title": "HIVE-24481: Skipped compaction can cause data corruption with streaming", "bodyText": "What changes were proposed in this pull request?\nSee the details in  HIVE-24481\nWhy are the changes needed?\nFix the data corruption issue.\nDoes this PR introduce any user-facing change?\nNo\nHow was this patch tested?\nUnit test", "createdAt": "2020-12-03T17:13:00Z", "url": "https://github.com/apache/hive/pull/1738", "merged": true, "mergeCommit": {"oid": "a1c6127440c0d2540700042c025014d86383a5cc"}, "closed": true, "closedAt": "2020-12-13T19:47:55Z", "author": {"login": "pvargacl"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdimWXegH2gAyNTMxOTQxMDkzOjg0YWU1ZDJiNDVkYTNmZjljMjZkYzFjMTNmY2MwNWVkZGMxNWNmMzQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdk0O8tAH2gAyNTMxOTQxMDkzOjAzZmE0ZTk3OTlmOTk5NWU3YTE2NDIyZTY1YjUyZjE0N2FkM2U3YTE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34", "author": {"user": {"login": "pvargacl", "name": "Peter Varga"}}, "url": "https://github.com/apache/hive/commit/84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34", "committedDate": "2020-12-03T17:11:29Z", "message": "HIVE-24481: Skipped compaction can cause data corruption with streaming"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NzY2NzY0", "url": "https://github.com/apache/hive/pull/1738#pullrequestreview-544766764", "createdAt": "2020-12-04T08:59:39Z", "commit": {"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwODo1OTozOVrOH_HJAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwODo1OTozOVrOH_HJAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTkzOTMyOQ==", "bodyText": "In a follow up Jira it might be worth change this whole AcidUtils approach and start to put everything from the beginning in a DirectoryImpl, so the argument count could be decreased to a sane amount.", "url": "https://github.com/apache/hive/pull/1738#discussion_r535939329", "createdAt": "2020-12-04T08:59:39Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -1369,14 +1383,14 @@ private static Directory getAcidState(FileSystem fileSystem, Path candidateDirec\n     if (childrenWithId != null) {\n       for (HdfsFileStatusWithId child : childrenWithId) {\n         getChildState(child, writeIdList, working, originalDirectories, original, obsolete,\n-            bestBase, ignoreEmptyFiles, abortedDirectories, abortedWriteIds, fs, validTxnList);\n+            bestBase, ignoreEmptyFiles, abortedDirectories, abortedWriteIds, uncompactedAborts, fs, validTxnList);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34"}, "originalPosition": 75}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0Nzk4MTMw", "url": "https://github.com/apache/hive/pull/1738#pullrequestreview-544798130", "createdAt": "2020-12-04T09:40:19Z", "commit": {"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwOTo0MDoyMFrOH_IuuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwOTo0MDoyMFrOH_IuuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk2NTM2OA==", "bodyText": "It's starting to affect readability, maybe refactor in the following patches.", "url": "https://github.com/apache/hive/pull/1738#discussion_r535965368", "createdAt": "2020-12-04T09:40:20Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -566,20 +567,20 @@ else if (filename.startsWith(BUCKET_PREFIX)) {\n   public static final class DirectoryImpl implements Directory {\n     private final List<Path> abortedDirectories;\n     private final Set<Long> abortedWriteIds;\n+    private final boolean uncompactedAborts;\n     private final boolean isBaseInRawFormat;\n     private final List<HdfsFileStatusWithId> original;\n     private final List<Path> obsolete;\n     private final List<ParsedDelta> deltas;\n     private final Path base;\n     private List<HdfsFileStatusWithId> baseFiles;\n \n-    public DirectoryImpl(List<Path> abortedDirectories, Set<Long> abortedWriteIds,\n-        boolean isBaseInRawFormat, List<HdfsFileStatusWithId> original,\n-        List<Path> obsolete, List<ParsedDelta> deltas, Path base) {\n-      this.abortedDirectories = abortedDirectories == null ?\n-          Collections.emptyList() : abortedDirectories;\n-      this.abortedWriteIds = abortedWriteIds == null ?\n-        Collections.emptySet() : abortedWriteIds;\n+    public DirectoryImpl(List<Path> abortedDirectories, Set<Long> abortedWriteIds, boolean uncompactedAborts,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34"}, "originalPosition": 27}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6a4cb91c2bd1eaaba88cafbff3446904ea69a5f2", "author": {"user": {"login": "pvargacl", "name": "Peter Varga"}}, "url": "https://github.com/apache/hive/commit/6a4cb91c2bd1eaaba88cafbff3446904ea69a5f2", "committedDate": "2020-12-04T10:24:32Z", "message": "Merge remote-tracking branch 'origin/master' into HIVE-24481-skipped-compaction"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5MjAyMzU3", "url": "https://github.com/apache/hive/pull/1738#pullrequestreview-549202357", "createdAt": "2020-12-10T13:37:59Z", "commit": {"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMzozNzo1OVrOIDJofw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMzozNzo1OVrOIDJofw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDE3NDQ2Mw==", "bodyText": "expectation msg copy-paste - \"there should be single record for the 2nd aborted txn\"", "url": "https://github.com/apache/hive/pull/1738#discussion_r540174463", "createdAt": "2020-12-10T13:37:59Z", "author": {"login": "deniskuzZ"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "diffHunk": "@@ -1177,6 +1177,104 @@ private HiveStreamingConnection prepareTableTwoPartitionsAndConnection(String db\n         .connect();\n   }\n \n+  /**\n+   * There is a special case handled in Compaction Worker that will skip compaction\n+   * if there is only one valid delta. But this compaction will be still cleaned up, if there are aborted directories.\n+   * @see Worker.isEnoughToCompact\n+   * However if no compaction was done, deltas containing mixed aborted / committed writes from streaming can not be cleaned\n+   * and the metadata belonging to those aborted transactions can not be removed.\n+   * @throws Exception ex\n+   */\n+  @Test\n+  public void testSkippedCompactionCleanerKeepsAborted() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    String agentInfo = \"UT_\" + Thread.currentThread().getName();\n+    TxnStore txnHandler = TxnUtils.getTxnStore(conf);\n+\n+    executeStatementOnDriver(\"drop table if exists \" + tblName, driver);\n+    executeStatementOnDriver(\"CREATE TABLE \" + tblName + \"(b STRING) \" +\n+        \" PARTITIONED BY (a INT) STORED AS ORC  TBLPROPERTIES ('transactional'='true')\", driver);\n+    executeStatementOnDriver(\"alter table \" + tblName + \" add partition(a=1)\", driver);\n+\n+    StrictDelimitedInputWriter writer = StrictDelimitedInputWriter.newBuilder()\n+        .withFieldDelimiter(',')\n+        .build();\n+\n+    // Create initial aborted txn\n+    HiveStreamingConnection connection = HiveStreamingConnection.newBuilder()\n+        .withDatabase(dbName)\n+        .withTable(tblName)\n+        .withStaticPartitionValues(Collections.singletonList(\"1\"))\n+        .withAgentInfo(agentInfo)\n+        .withHiveConf(conf)\n+        .withRecordWriter(writer)\n+        .withStreamingOptimizations(true)\n+        .withTransactionBatchSize(1)\n+        .connect();\n+\n+    connection.beginTransaction();\n+    connection.write(\"3,1\".getBytes());\n+    connection.write(\"4,1\".getBytes());\n+    connection.abortTransaction();\n+\n+    connection.close();\n+\n+    // Create a sequence of commit, abort, commit to the same delta folder\n+    connection = HiveStreamingConnection.newBuilder()\n+        .withDatabase(dbName)\n+        .withTable(tblName)\n+        .withStaticPartitionValues(Collections.singletonList(\"1\"))\n+        .withAgentInfo(agentInfo)\n+        .withHiveConf(conf)\n+        .withRecordWriter(writer)\n+        .withStreamingOptimizations(true)\n+        .withTransactionBatchSize(3)\n+        .connect();\n+\n+    connection.beginTransaction();\n+    connection.write(\"1,1\".getBytes());\n+    connection.write(\"2,1\".getBytes());\n+    connection.commitTransaction();\n+\n+    connection.beginTransaction();\n+    connection.write(\"3,1\".getBytes());\n+    connection.write(\"4,1\".getBytes());\n+    connection.abortTransaction();\n+\n+    connection.beginTransaction();\n+    connection.write(\"5,1\".getBytes());\n+    connection.write(\"6,1\".getBytes());\n+    connection.commitTransaction();\n+\n+    connection.close();\n+\n+    // Check that aborted are not read back\n+    driver.run(\"select * from cws\");\n+    List res = new ArrayList();\n+    driver.getFetchTask().fetch(res);\n+    Assert.assertEquals(4, res.size());\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(\"There should be 2 record for two aborted transaction\", 2, count);\n+\n+    // Start a compaction, that will be skipped, because only one valid delta is there\n+    driver.run(\"alter table cws partition(a='1') compact 'minor'\");\n+    runWorker(conf);\n+    // Cleaner should not delete info about aborted txn 2\n+    runCleaner(conf);\n+    txnHandler.cleanEmptyAbortedAndCommittedTxns();\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(\"There should be 1 record for two aborted transaction\", 1, count);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34"}, "originalPosition": 93}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5MjIyNDkw", "url": "https://github.com/apache/hive/pull/1738#pullrequestreview-549222490", "createdAt": "2020-12-10T14:00:09Z", "commit": {"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNDowMDowOVrOIDKn-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNDowMDowOVrOIDKn-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDE5MDcxNQ==", "bodyText": "fix the javadoc", "url": "https://github.com/apache/hive/pull/1738#discussion_r540190715", "createdAt": "2020-12-10T14:00:09Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -923,6 +929,13 @@ public String toString() {\n      * @return the list of aborted writeIds\n      */\n     Set<Long> getAbortedWriteIds();\n+\n+    /**\n+     * Get the list of writeIds that belong to aborted transactions, but can not be cleaned,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34"}, "originalPosition": 54}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5MjI1MjAw", "url": "https://github.com/apache/hive/pull/1738#pullrequestreview-549225200", "createdAt": "2020-12-10T14:03:03Z", "commit": {"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03fa4e9799f9995e7a16422e65b52f147ad3e7a1", "author": {"user": {"login": "pvargacl", "name": "Peter Varga"}}, "url": "https://github.com/apache/hive/commit/03fa4e9799f9995e7a16422e65b52f147ad3e7a1", "committedDate": "2020-12-10T14:29:54Z", "message": "Fix review comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2868, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}