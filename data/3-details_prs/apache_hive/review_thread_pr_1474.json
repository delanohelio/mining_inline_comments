{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgxMzEzMjM1", "number": 1474, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMTo0Njo1MVrOEg55_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxNjozMzo1MVrOEg_S4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyOTM4NjIzOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionDagSubmitPlugin.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMTo0Njo1MVrOHN8TPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMjoxMDoyMFrOHN8-_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDM4MTUwMA==", "bodyText": "please change this to retryPossible ( and remove maxExecutions field)", "url": "https://github.com/apache/hive/pull/1474#discussion_r484381500", "createdAt": "2020-09-07T11:46:51Z", "author": {"login": "kgyrtkirk"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionDagSubmitPlugin.java", "diffHunk": "@@ -76,7 +76,7 @@ public void beforeExecute(int executionIndex, boolean explainReOptimization) {\n   }\n \n   @Override\n-  public boolean shouldReExecute(int executionNum, CommandProcessorException ex) {\n+  public boolean shouldReExecute(int executionNum) {\n     return (executionNum < maxExecutions) && retryPossible;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7cbe43992ff6e11ff2d361bd2b637faf50af2044"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDM5MjcwMQ==", "bodyText": "changed", "url": "https://github.com/apache/hive/pull/1474#discussion_r484392701", "createdAt": "2020-09-07T12:10:20Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionDagSubmitPlugin.java", "diffHunk": "@@ -76,7 +76,7 @@ public void beforeExecute(int executionIndex, boolean explainReOptimization) {\n   }\n \n   @Override\n-  public boolean shouldReExecute(int executionNum, CommandProcessorException ex) {\n+  public boolean shouldReExecute(int executionNum) {\n     return (executionNum < maxExecutions) && retryPossible;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDM4MTUwMA=="}, "originalCommit": {"oid": "7cbe43992ff6e11ff2d361bd2b637faf50af2044"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyOTQzMjUyOnYy", "diffSide": "LEFT", "path": "ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMjowMToyMlrOHN8ueA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMjowMzowNFrOHN8xhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDM4ODQ3Mg==", "bodyText": "These tests should be added back in your next change, when we recompile without the reexec driver, to verify the dyn partitioning use-cases", "url": "https://github.com/apache/hive/pull/1474#discussion_r484388472", "createdAt": "2020-09-07T12:01:22Z", "author": {"login": "pvargacl"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java", "diffHunk": "@@ -2324,142 +2315,6 @@ private void testConcurrentMergeInsertNoDuplicates(String query, boolean sharedW\n     List res = new ArrayList();\n     driver.getFetchTask().fetch(res);\n     Assert.assertEquals(\"Duplicate records found\", 4, res.size());\n-    dropTable(new String[]{\"target\", \"source\"});\n-  }\n-\n-  /**\n-   * ValidTxnManager.isValidTxnListState can invalidate a snapshot if a relevant write transaction was committed\n-   * between a query compilation and lock acquisition. When this happens we have to recompile the given query,\n-   * otherwise we can miss reading partitions created between. The following three cases test these scenarios.\n-   * @throws Exception ex\n-   */\n-  @Test\n-  public void testMergeInsertDynamicPartitioningSequential() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7cbe43992ff6e11ff2d361bd2b637faf50af2044"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDM4OTI1Mw==", "bodyText": "Ack.", "url": "https://github.com/apache/hive/pull/1474#discussion_r484389253", "createdAt": "2020-09-07T12:03:04Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java", "diffHunk": "@@ -2324,142 +2315,6 @@ private void testConcurrentMergeInsertNoDuplicates(String query, boolean sharedW\n     List res = new ArrayList();\n     driver.getFetchTask().fetch(res);\n     Assert.assertEquals(\"Duplicate records found\", 4, res.size());\n-    dropTable(new String[]{\"target\", \"source\"});\n-  }\n-\n-  /**\n-   * ValidTxnManager.isValidTxnListState can invalidate a snapshot if a relevant write transaction was committed\n-   * between a query compilation and lock acquisition. When this happens we have to recompile the given query,\n-   * otherwise we can miss reading partitions created between. The following three cases test these scenarios.\n-   * @throws Exception ex\n-   */\n-  @Test\n-  public void testMergeInsertDynamicPartitioningSequential() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDM4ODQ3Mg=="}, "originalCommit": {"oid": "7cbe43992ff6e11ff2d361bd2b637faf50af2044"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDI0MTIzOnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxNjoyMTowMFrOHOESdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwOTowODo1OVrOHOT5kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxMjM3Mw==", "bodyText": "This should be added as the last parameter to not break backward compatibility no?", "url": "https://github.com/apache/hive/pull/1474#discussion_r484512373", "createdAt": "2020-09-07T16:21:00Z", "author": {"login": "pvargacl"}, "path": "standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift", "diffHunk": "@@ -1012,8 +1012,9 @@ struct CommitTxnRequest {\n     3: optional list<WriteEventInfo> writeEventInfos,\n     // Information to update the last repl id of table/partition along with commit txn (replication from 2.6 to 3.0)\n     4: optional ReplLastIdInfo replLastIdInfo,\n+    5: optional bool exclWriteEnabled = true,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec16413c622225240d436850097412af8e5e47d9"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDc2ODE0Ng==", "bodyText": "we need to keep it consistent with downstream, see HIVE-23759: refactor field order of CommitTxnRequest", "url": "https://github.com/apache/hive/pull/1474#discussion_r484768146", "createdAt": "2020-09-08T09:08:59Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift", "diffHunk": "@@ -1012,8 +1012,9 @@ struct CommitTxnRequest {\n     3: optional list<WriteEventInfo> writeEventInfos,\n     // Information to update the last repl id of table/partition along with commit txn (replication from 2.6 to 3.0)\n     4: optional ReplLastIdInfo replLastIdInfo,\n+    5: optional bool exclWriteEnabled = true,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxMjM3Mw=="}, "originalCommit": {"oid": "ec16413c622225240d436850097412af8e5e47d9"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDI0ODcyOnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxNjoyNDozMFrOHOEWwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwOTowODowNlrOHOT3xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxMzQ3Mw==", "bodyText": "This Reexec part is not really relevant now, we don't need the reexec driver for this to work properly", "url": "https://github.com/apache/hive/pull/1474#discussion_r484513473", "createdAt": "2020-09-07T16:24:30Z", "author": {"login": "pvargacl"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java", "diffHunk": "@@ -2315,6 +2386,139 @@ private void testConcurrentMergeInsertNoDuplicates(String query, boolean sharedW\n     List res = new ArrayList();\n     driver.getFetchTask().fetch(res);\n     Assert.assertEquals(\"Duplicate records found\", 4, res.size());\n+    dropTable(new String[]{\"target\", \"source\"});\n+  }\n+\n+  /**\n+   * ValidTxnManager.isValidTxnListState can invalidate a snapshot if a relevant write transaction was committed\n+   * between a query compilation and lock acquisition. When this happens we have to recompile the given query,\n+   * otherwise we can miss reading partitions created between. The following three cases test these scenarios.\n+   * @throws Exception ex\n+   */\n+  @Test\n+  public void testMergeInsertDynamicPartitioningSequential() throws Exception {\n+    dropTable(new String[]{\"target\", \"source\"});\n+    conf.setBoolVar(HiveConf.ConfVars.TXN_WRITE_X_LOCK, false);\n+\n+    // Create partition c=1\n+    driver.run(\"create table target (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')\");\n+    driver.run(\"insert into target values (1,1,1), (2,2,1)\");\n+    //Create partition c=2\n+    driver.run(\"create table source (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')\");\n+    driver.run(\"insert into source values (3,3,2), (4,4,2)\");\n+\n+    // txn 1 inserts data to an old and a new partition\n+    driver.run(\"insert into source values (5,5,2), (6,6,3)\");\n+\n+    // txn 2 inserts into the target table into a new partition ( and a duplicate considering the source table)\n+    driver.run(\"insert into target values (3, 3, 2)\");\n+\n+    // txn3 merge\n+    driver.run(\"merge into target t using source s on t.a = s.a \" +\n+      \"when not matched then insert values (s.a, s.b, s.c)\");\n+    driver.run(\"select * from target\");\n+    List res = new ArrayList();\n+    driver.getFetchTask().fetch(res);\n+    // The merge should see all three partition and not create duplicates\n+    Assert.assertEquals(\"Duplicate records found\", 6, res.size());\n+    Assert.assertTrue(\"Partition 3 was skipped\", res.contains(\"6\\t6\\t3\"));\n+    dropTable(new String[]{\"target\", \"source\"});\n+  }\n+\n+  @Test\n+  public void testMergeInsertDynamicPartitioningSnapshotInvalidatedWithOldCommit() throws Exception {\n+    // By creating the driver with the factory, we should have a ReExecDriver\n+    IDriver driver3 = DriverFactory.newDriver(conf);\n+    Assert.assertTrue(\"ReExecDriver was expected\", driver3 instanceof ReExecDriver);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec16413c622225240d436850097412af8e5e47d9"}, "originalPosition": 252}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDc2NzY4Nw==", "bodyText": "changed", "url": "https://github.com/apache/hive/pull/1474#discussion_r484767687", "createdAt": "2020-09-08T09:08:06Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java", "diffHunk": "@@ -2315,6 +2386,139 @@ private void testConcurrentMergeInsertNoDuplicates(String query, boolean sharedW\n     List res = new ArrayList();\n     driver.getFetchTask().fetch(res);\n     Assert.assertEquals(\"Duplicate records found\", 4, res.size());\n+    dropTable(new String[]{\"target\", \"source\"});\n+  }\n+\n+  /**\n+   * ValidTxnManager.isValidTxnListState can invalidate a snapshot if a relevant write transaction was committed\n+   * between a query compilation and lock acquisition. When this happens we have to recompile the given query,\n+   * otherwise we can miss reading partitions created between. The following three cases test these scenarios.\n+   * @throws Exception ex\n+   */\n+  @Test\n+  public void testMergeInsertDynamicPartitioningSequential() throws Exception {\n+    dropTable(new String[]{\"target\", \"source\"});\n+    conf.setBoolVar(HiveConf.ConfVars.TXN_WRITE_X_LOCK, false);\n+\n+    // Create partition c=1\n+    driver.run(\"create table target (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')\");\n+    driver.run(\"insert into target values (1,1,1), (2,2,1)\");\n+    //Create partition c=2\n+    driver.run(\"create table source (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')\");\n+    driver.run(\"insert into source values (3,3,2), (4,4,2)\");\n+\n+    // txn 1 inserts data to an old and a new partition\n+    driver.run(\"insert into source values (5,5,2), (6,6,3)\");\n+\n+    // txn 2 inserts into the target table into a new partition ( and a duplicate considering the source table)\n+    driver.run(\"insert into target values (3, 3, 2)\");\n+\n+    // txn3 merge\n+    driver.run(\"merge into target t using source s on t.a = s.a \" +\n+      \"when not matched then insert values (s.a, s.b, s.c)\");\n+    driver.run(\"select * from target\");\n+    List res = new ArrayList();\n+    driver.getFetchTask().fetch(res);\n+    // The merge should see all three partition and not create duplicates\n+    Assert.assertEquals(\"Duplicate records found\", 6, res.size());\n+    Assert.assertTrue(\"Partition 3 was skipped\", res.contains(\"6\\t6\\t3\"));\n+    dropTable(new String[]{\"target\", \"source\"});\n+  }\n+\n+  @Test\n+  public void testMergeInsertDynamicPartitioningSnapshotInvalidatedWithOldCommit() throws Exception {\n+    // By creating the driver with the factory, we should have a ReExecDriver\n+    IDriver driver3 = DriverFactory.newDriver(conf);\n+    Assert.assertTrue(\"ReExecDriver was expected\", driver3 instanceof ReExecDriver);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxMzQ3Mw=="}, "originalCommit": {"oid": "ec16413c622225240d436850097412af8e5e47d9"}, "originalPosition": 252}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzMDI2OTEyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxNjozMzo1MVrOHOEiGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQwOTowODowMlrOHOT3lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxNjM3OA==", "bodyText": "I think this code should be removed. If the alreadyCompiled was false, we already compiled it once line 473. This should not matter when we are in the invalid snapshot case, you already recompile the query if it is neccessarry", "url": "https://github.com/apache/hive/pull/1474#discussion_r484516378", "createdAt": "2020-09-07T16:33:51Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -488,30 +489,40 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       lockAndRespond();\n \n+      int retryShapshotCnt = 0;\n+      int maxRetrySnapshotCnt = HiveConf.getIntVar(driverContext.getConf(),\n+        HiveConf.ConfVars.HIVE_TXN_MAX_RETRYSNAPSHOT_COUNT);\n+\n       try {\n-        if (!driverTxnHandler.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+        while (!driverTxnHandler.isValidTxnListState() && ++retryShapshotCnt <= maxRetrySnapshotCnt) {\n+          LOG.info(\"Compiling after acquiring locks, attempt #\" + retryShapshotCnt);\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n           // txn list and retry\n           // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n           // Currently, we acquire a snapshot, we compile the query wrt that snapshot,\n           // and then, we acquire locks. If snapshot is still valid, we continue as usual.\n           // But if snapshot is not valid, we recompile the query.\n           if (driverContext.isOutdatedTxn()) {\n+            LOG.info(\"Snapshot is outdated, re-initiating transaction ...\");\n             driverContext.getTxnManager().rollbackTxn();\n \n             String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n             driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n             lockAndRespond();\n           }\n+\n           driverContext.setRetrial(true);\n           driverContext.getBackupContext().addSubContext(context);\n           driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n           context = driverContext.getBackupContext();\n+\n           driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n             driverContext.getTxnManager().getValidTxns().toString());\n+\n           if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n+            compileInternal(context.getCmd(), true);\n             driverTxnHandler.recordValidWriteIds();\n+            driverTxnHandler.setWriteIdForAcidFileSinks();\n           }\n \n           if (!alreadyCompiled) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec16413c622225240d436850097412af8e5e47d9"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDc2NzYzNg==", "bodyText": "removed", "url": "https://github.com/apache/hive/pull/1474#discussion_r484767636", "createdAt": "2020-09-08T09:08:02Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -488,30 +489,40 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       lockAndRespond();\n \n+      int retryShapshotCnt = 0;\n+      int maxRetrySnapshotCnt = HiveConf.getIntVar(driverContext.getConf(),\n+        HiveConf.ConfVars.HIVE_TXN_MAX_RETRYSNAPSHOT_COUNT);\n+\n       try {\n-        if (!driverTxnHandler.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+        while (!driverTxnHandler.isValidTxnListState() && ++retryShapshotCnt <= maxRetrySnapshotCnt) {\n+          LOG.info(\"Compiling after acquiring locks, attempt #\" + retryShapshotCnt);\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n           // txn list and retry\n           // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n           // Currently, we acquire a snapshot, we compile the query wrt that snapshot,\n           // and then, we acquire locks. If snapshot is still valid, we continue as usual.\n           // But if snapshot is not valid, we recompile the query.\n           if (driverContext.isOutdatedTxn()) {\n+            LOG.info(\"Snapshot is outdated, re-initiating transaction ...\");\n             driverContext.getTxnManager().rollbackTxn();\n \n             String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n             driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n             lockAndRespond();\n           }\n+\n           driverContext.setRetrial(true);\n           driverContext.getBackupContext().addSubContext(context);\n           driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n           context = driverContext.getBackupContext();\n+\n           driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n             driverContext.getTxnManager().getValidTxns().toString());\n+\n           if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n+            compileInternal(context.getCmd(), true);\n             driverTxnHandler.recordValidWriteIds();\n+            driverTxnHandler.setWriteIdForAcidFileSinks();\n           }\n \n           if (!alreadyCompiled) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDUxNjM3OA=="}, "originalCommit": {"oid": "ec16413c622225240d436850097412af8e5e47d9"}, "originalPosition": 50}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 424, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}