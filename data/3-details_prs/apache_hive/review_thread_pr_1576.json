{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAyMjg0NDM1", "number": 1576, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxMzoyMTowMVrOEtCTuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxMzoyNjo1NVrOEtCeeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NjU5MTkzOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxMzoyMTowMVrOHgmZiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxMzoyMTowMVrOHgmZiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk0NTYwOQ==", "bodyText": "nit: space", "url": "https://github.com/apache/hive/pull/1576#discussion_r503945609", "createdAt": "2020-10-13T13:21:01Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "diffHunk": "@@ -1156,13 +1157,36 @@ public BISplitStrategy(Context context, FileSystem fs, Path dir,\n           } else {\n             TreeMap<Long, BlockLocation> blockOffsets = SHIMS.getLocationsWithOffset(fs, fileStatus);\n             for (Map.Entry<Long, BlockLocation> entry : blockOffsets.entrySet()) {\n-              if (entry.getKey() + entry.getValue().getLength() > logicalLen) {\n+              long blockOffset = entry.getKey();\n+              long blockLength = entry.getValue().getLength();\n+              if(blockOffset > logicalLen) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66d61ad6f9be0c15ddc08a9b1f1c9fc4b19c3253"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NjYxOTQ1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxMzoyNjo1NVrOHgmq2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwOTo1MTozN1rOHhLOqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk1MDA0MA==", "bodyText": "Why is this capped with blockSize?\nIsn't there a case where we are at the end of the block but still writing?\nSplitLength might be this and it is easier to understad:\nlogicalLen - blockOffset", "url": "https://github.com/apache/hive/pull/1576#discussion_r503950040", "createdAt": "2020-10-13T13:26:55Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "diffHunk": "@@ -1156,13 +1157,36 @@ public BISplitStrategy(Context context, FileSystem fs, Path dir,\n           } else {\n             TreeMap<Long, BlockLocation> blockOffsets = SHIMS.getLocationsWithOffset(fs, fileStatus);\n             for (Map.Entry<Long, BlockLocation> entry : blockOffsets.entrySet()) {\n-              if (entry.getKey() + entry.getValue().getLength() > logicalLen) {\n+              long blockOffset = entry.getKey();\n+              long blockLength = entry.getValue().getLength();\n+              if(blockOffset > logicalLen) {\n                 //don't create splits for anything past logical EOF\n-                continue;\n+                //map is ordered, thus any possible entry in the iteration after this is bound to be > logicalLen\n+                break;\n               }\n-              OrcSplit orcSplit = new OrcSplit(fileStatus.getPath(), fileKey, entry.getKey(),\n-                entry.getValue().getLength(), entry.getValue().getHosts(), null, isOriginal, true,\n-                deltas, -1, logicalLen, dir, offsetAndBucket);\n+              long splitLength = blockLength;\n+\n+              long blockEndOvershoot = (blockOffset + blockLength) - logicalLen;\n+              if (blockEndOvershoot > 0) {\n+                // if logicalLen is placed within a block, we should make (this last) split out of the part of this block\n+                // -> we should read less than block end\n+                splitLength -= blockEndOvershoot;\n+              } else if (blockOffsets.lastKey() == blockOffset && blockEndOvershoot < 0) {\n+                // This is the last block but it ends before logicalLen\n+                // This can happen with HDFS if hflush was called and blocks are not persisted to disk yet, but content\n+                // is otherwise available for readers, as DNs have these buffers in memory at this time.\n+                // -> we should read more than (persisted) block end, but surely not more than the whole block\n+                if (fileStatus instanceof HdfsLocatedFileStatus) {\n+                  HdfsLocatedFileStatus hdfsFileStatus = (HdfsLocatedFileStatus)fileStatus;\n+                  if (hdfsFileStatus.getLocatedBlocks().isUnderConstruction()) {\n+                    // blockEndOvershoot is negative here...\n+                    splitLength = Math.min(splitLength - blockEndOvershoot, hdfsFileStatus.getBlockSize());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66d61ad6f9be0c15ddc08a9b1f1c9fc4b19c3253"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk3MDk2MA==", "bodyText": "hdfsFileStatus.blockSize() is not the block length, but the configured (max) block size (e.g. 256MB) - that's how big a block can be max, and that's why we shouldn't read past that in this split", "url": "https://github.com/apache/hive/pull/1576#discussion_r503970960", "createdAt": "2020-10-13T13:53:44Z", "author": {"login": "szlta"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "diffHunk": "@@ -1156,13 +1157,36 @@ public BISplitStrategy(Context context, FileSystem fs, Path dir,\n           } else {\n             TreeMap<Long, BlockLocation> blockOffsets = SHIMS.getLocationsWithOffset(fs, fileStatus);\n             for (Map.Entry<Long, BlockLocation> entry : blockOffsets.entrySet()) {\n-              if (entry.getKey() + entry.getValue().getLength() > logicalLen) {\n+              long blockOffset = entry.getKey();\n+              long blockLength = entry.getValue().getLength();\n+              if(blockOffset > logicalLen) {\n                 //don't create splits for anything past logical EOF\n-                continue;\n+                //map is ordered, thus any possible entry in the iteration after this is bound to be > logicalLen\n+                break;\n               }\n-              OrcSplit orcSplit = new OrcSplit(fileStatus.getPath(), fileKey, entry.getKey(),\n-                entry.getValue().getLength(), entry.getValue().getHosts(), null, isOriginal, true,\n-                deltas, -1, logicalLen, dir, offsetAndBucket);\n+              long splitLength = blockLength;\n+\n+              long blockEndOvershoot = (blockOffset + blockLength) - logicalLen;\n+              if (blockEndOvershoot > 0) {\n+                // if logicalLen is placed within a block, we should make (this last) split out of the part of this block\n+                // -> we should read less than block end\n+                splitLength -= blockEndOvershoot;\n+              } else if (blockOffsets.lastKey() == blockOffset && blockEndOvershoot < 0) {\n+                // This is the last block but it ends before logicalLen\n+                // This can happen with HDFS if hflush was called and blocks are not persisted to disk yet, but content\n+                // is otherwise available for readers, as DNs have these buffers in memory at this time.\n+                // -> we should read more than (persisted) block end, but surely not more than the whole block\n+                if (fileStatus instanceof HdfsLocatedFileStatus) {\n+                  HdfsLocatedFileStatus hdfsFileStatus = (HdfsLocatedFileStatus)fileStatus;\n+                  if (hdfsFileStatus.getLocatedBlocks().isUnderConstruction()) {\n+                    // blockEndOvershoot is negative here...\n+                    splitLength = Math.min(splitLength - blockEndOvershoot, hdfsFileStatus.getBlockSize());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk1MDA0MA=="}, "originalCommit": {"oid": "66d61ad6f9be0c15ddc08a9b1f1c9fc4b19c3253"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDUzNDkxMg==", "bodyText": "Maybe this is just a theoretical problem, but if the blockOffset + hdfsFileStatus.getBlockSize() is greater in the last block than the logicalLen, then we should throw an exception, and then we do not need the min here", "url": "https://github.com/apache/hive/pull/1576#discussion_r504534912", "createdAt": "2020-10-14T09:28:50Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "diffHunk": "@@ -1156,13 +1157,36 @@ public BISplitStrategy(Context context, FileSystem fs, Path dir,\n           } else {\n             TreeMap<Long, BlockLocation> blockOffsets = SHIMS.getLocationsWithOffset(fs, fileStatus);\n             for (Map.Entry<Long, BlockLocation> entry : blockOffsets.entrySet()) {\n-              if (entry.getKey() + entry.getValue().getLength() > logicalLen) {\n+              long blockOffset = entry.getKey();\n+              long blockLength = entry.getValue().getLength();\n+              if(blockOffset > logicalLen) {\n                 //don't create splits for anything past logical EOF\n-                continue;\n+                //map is ordered, thus any possible entry in the iteration after this is bound to be > logicalLen\n+                break;\n               }\n-              OrcSplit orcSplit = new OrcSplit(fileStatus.getPath(), fileKey, entry.getKey(),\n-                entry.getValue().getLength(), entry.getValue().getHosts(), null, isOriginal, true,\n-                deltas, -1, logicalLen, dir, offsetAndBucket);\n+              long splitLength = blockLength;\n+\n+              long blockEndOvershoot = (blockOffset + blockLength) - logicalLen;\n+              if (blockEndOvershoot > 0) {\n+                // if logicalLen is placed within a block, we should make (this last) split out of the part of this block\n+                // -> we should read less than block end\n+                splitLength -= blockEndOvershoot;\n+              } else if (blockOffsets.lastKey() == blockOffset && blockEndOvershoot < 0) {\n+                // This is the last block but it ends before logicalLen\n+                // This can happen with HDFS if hflush was called and blocks are not persisted to disk yet, but content\n+                // is otherwise available for readers, as DNs have these buffers in memory at this time.\n+                // -> we should read more than (persisted) block end, but surely not more than the whole block\n+                if (fileStatus instanceof HdfsLocatedFileStatus) {\n+                  HdfsLocatedFileStatus hdfsFileStatus = (HdfsLocatedFileStatus)fileStatus;\n+                  if (hdfsFileStatus.getLocatedBlocks().isUnderConstruction()) {\n+                    // blockEndOvershoot is negative here...\n+                    splitLength = Math.min(splitLength - blockEndOvershoot, hdfsFileStatus.getBlockSize());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk1MDA0MA=="}, "originalCommit": {"oid": "66d61ad6f9be0c15ddc08a9b1f1c9fc4b19c3253"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDU0OTAzNQ==", "bodyText": "okay, that makes sense", "url": "https://github.com/apache/hive/pull/1576#discussion_r504549035", "createdAt": "2020-10-14T09:51:37Z", "author": {"login": "szlta"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "diffHunk": "@@ -1156,13 +1157,36 @@ public BISplitStrategy(Context context, FileSystem fs, Path dir,\n           } else {\n             TreeMap<Long, BlockLocation> blockOffsets = SHIMS.getLocationsWithOffset(fs, fileStatus);\n             for (Map.Entry<Long, BlockLocation> entry : blockOffsets.entrySet()) {\n-              if (entry.getKey() + entry.getValue().getLength() > logicalLen) {\n+              long blockOffset = entry.getKey();\n+              long blockLength = entry.getValue().getLength();\n+              if(blockOffset > logicalLen) {\n                 //don't create splits for anything past logical EOF\n-                continue;\n+                //map is ordered, thus any possible entry in the iteration after this is bound to be > logicalLen\n+                break;\n               }\n-              OrcSplit orcSplit = new OrcSplit(fileStatus.getPath(), fileKey, entry.getKey(),\n-                entry.getValue().getLength(), entry.getValue().getHosts(), null, isOriginal, true,\n-                deltas, -1, logicalLen, dir, offsetAndBucket);\n+              long splitLength = blockLength;\n+\n+              long blockEndOvershoot = (blockOffset + blockLength) - logicalLen;\n+              if (blockEndOvershoot > 0) {\n+                // if logicalLen is placed within a block, we should make (this last) split out of the part of this block\n+                // -> we should read less than block end\n+                splitLength -= blockEndOvershoot;\n+              } else if (blockOffsets.lastKey() == blockOffset && blockEndOvershoot < 0) {\n+                // This is the last block but it ends before logicalLen\n+                // This can happen with HDFS if hflush was called and blocks are not persisted to disk yet, but content\n+                // is otherwise available for readers, as DNs have these buffers in memory at this time.\n+                // -> we should read more than (persisted) block end, but surely not more than the whole block\n+                if (fileStatus instanceof HdfsLocatedFileStatus) {\n+                  HdfsLocatedFileStatus hdfsFileStatus = (HdfsLocatedFileStatus)fileStatus;\n+                  if (hdfsFileStatus.getLocatedBlocks().isUnderConstruction()) {\n+                    // blockEndOvershoot is negative here...\n+                    splitLength = Math.min(splitLength - blockEndOvershoot, hdfsFileStatus.getBlockSize());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzk1MDA0MA=="}, "originalCommit": {"oid": "66d61ad6f9be0c15ddc08a9b1f1c9fc4b19c3253"}, "originalPosition": 40}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 342, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}