{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk2OTE3MzI3", "number": 1548, "reviewThreads": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxMDo0NzoxMlrOEqAkTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNzoyNTowNFrOEv5d3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNDg0OTQzOnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxMDo0NzoxMlrOHcA7sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMTo1OToxNVrOHcZ0Jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEzNzQ1OQ==", "bodyText": "Here we need some correction in expected values, as these tests were written wrt to branch-3.1 in which we don't support minor compaction for MM tables.", "url": "https://github.com/apache/hive/pull/1548#discussion_r499137459", "createdAt": "2020-10-03T10:47:12Z", "author": {"login": "vpnvishv"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2128,6 +2128,395 @@ public void testCleanerForTxnToWriteId() throws Exception {\n             0, TxnDbUtil.countQueryAgent(hiveConf, \"select count(*) from TXN_TO_WRITE_ID\"));\n   }\n \n+  @Test\n+    public void testMmTableAbortWithCompaction() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 4. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData2);\n+    verifyBaseDirAndResult(0, Table.MMTBL.toString(), \"\", resultData2);\n+    // 5. Run Cleaner. Shouldn't impact anything.\n+    runCleaner(hiveConf);\n+    // 6. Run initiator remove aborted entry from TXNS table\n+    runInitiator(hiveConf);\n+\n+    // Verify query result\n+    List<String> rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData2), rs);\n+\n+    int [][] resultData3 = new int[][] {{1,2}, {5,6}, {7,8}};\n+    // 7. add few more rows\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(7,8)\");\n+    // 8. add one more aborted delta\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(9,10)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+\n+    // 9. Perform a MAJOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    verifyDeltaDirAndResult(4, Table.MMTBL.toString(), \"\", resultData3);\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runCleaner(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runInitiator(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // Verify query result\n+    rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData3), rs);\n+  }\n+  @Test\n+  public void testMmTableAbortWithCompactionNoCleanup() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}, {5,6}};\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \" values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData1);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 3. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU0NTEyNw==", "bodyText": "fixed", "url": "https://github.com/apache/hive/pull/1548#discussion_r499545127", "createdAt": "2020-10-05T11:59:15Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2128,6 +2128,395 @@ public void testCleanerForTxnToWriteId() throws Exception {\n             0, TxnDbUtil.countQueryAgent(hiveConf, \"select count(*) from TXN_TO_WRITE_ID\"));\n   }\n \n+  @Test\n+    public void testMmTableAbortWithCompaction() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 4. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData2);\n+    verifyBaseDirAndResult(0, Table.MMTBL.toString(), \"\", resultData2);\n+    // 5. Run Cleaner. Shouldn't impact anything.\n+    runCleaner(hiveConf);\n+    // 6. Run initiator remove aborted entry from TXNS table\n+    runInitiator(hiveConf);\n+\n+    // Verify query result\n+    List<String> rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData2), rs);\n+\n+    int [][] resultData3 = new int[][] {{1,2}, {5,6}, {7,8}};\n+    // 7. add few more rows\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(7,8)\");\n+    // 8. add one more aborted delta\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(9,10)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+\n+    // 9. Perform a MAJOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    verifyDeltaDirAndResult(4, Table.MMTBL.toString(), \"\", resultData3);\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runCleaner(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runInitiator(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // Verify query result\n+    rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData3), rs);\n+  }\n+  @Test\n+  public void testMmTableAbortWithCompactionNoCleanup() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}, {5,6}};\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \" values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData1);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 3. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEzNzQ1OQ=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNDg1MDY5OnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxMDo0OTo1OVrOHcA8Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMTo1ODoxN1rOHcZyAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEzNzYyNg==", "bodyText": "Here expected value looks correct to me and this looks like a genuine issue. Is this due to some incorrect stats, any idea?", "url": "https://github.com/apache/hive/pull/1548#discussion_r499137626", "createdAt": "2020-10-03T10:49:59Z", "author": {"login": "vpnvishv"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2128,6 +2128,395 @@ public void testCleanerForTxnToWriteId() throws Exception {\n             0, TxnDbUtil.countQueryAgent(hiveConf, \"select count(*) from TXN_TO_WRITE_ID\"));\n   }\n \n+  @Test\n+    public void testMmTableAbortWithCompaction() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU0NDU3OQ==", "bodyText": "fixed, turned off StatsOptimizer", "url": "https://github.com/apache/hive/pull/1548#discussion_r499544579", "createdAt": "2020-10-05T11:58:17Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2128,6 +2128,395 @@ public void testCleanerForTxnToWriteId() throws Exception {\n             0, TxnDbUtil.countQueryAgent(hiveConf, \"select count(*) from TXN_TO_WRITE_ID\"));\n   }\n \n+  @Test\n+    public void testMmTableAbortWithCompaction() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEzNzYyNg=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNDg1MTM1OnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxMDo1MToxOFrOHcA8sA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxMTo1ODoyMlrOHcZyKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEzNzcxMg==", "bodyText": "Again, here expected value looks correct to me and this looks like a genuine issue. Is this due to some incorrect stats, any idea?", "url": "https://github.com/apache/hive/pull/1548#discussion_r499137712", "createdAt": "2020-10-03T10:51:18Z", "author": {"login": "vpnvishv"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2128,6 +2128,395 @@ public void testCleanerForTxnToWriteId() throws Exception {\n             0, TxnDbUtil.countQueryAgent(hiveConf, \"select count(*) from TXN_TO_WRITE_ID\"));\n   }\n \n+  @Test\n+    public void testMmTableAbortWithCompaction() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 4. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData2);\n+    verifyBaseDirAndResult(0, Table.MMTBL.toString(), \"\", resultData2);\n+    // 5. Run Cleaner. Shouldn't impact anything.\n+    runCleaner(hiveConf);\n+    // 6. Run initiator remove aborted entry from TXNS table\n+    runInitiator(hiveConf);\n+\n+    // Verify query result\n+    List<String> rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData2), rs);\n+\n+    int [][] resultData3 = new int[][] {{1,2}, {5,6}, {7,8}};\n+    // 7. add few more rows\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(7,8)\");\n+    // 8. add one more aborted delta\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(9,10)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+\n+    // 9. Perform a MAJOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    verifyDeltaDirAndResult(4, Table.MMTBL.toString(), \"\", resultData3);\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runCleaner(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runInitiator(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // Verify query result\n+    rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData3), rs);\n+  }\n+  @Test\n+  public void testMmTableAbortWithCompactionNoCleanup() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}, {5,6}};\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \" values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData1);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 3. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+    verifyBaseDirAndResult(0, Table.MMTBL.toString(), \"\", resultData1);\n+    // Verify query result\n+    List<String> rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData1), rs);\n+\n+    int [][] resultData3 = new int[][] {{1,2}, {5,6}, {7,8}};\n+    // 4. add few more rows\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(7,8)\");\n+    // 5. add one more aborted delta\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(9,10)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    verifyDeltaDirAndResult(4, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // 6. Perform a MAJOR compaction, expectation is it should remove aborted delta dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // 7. Run one more Major compaction this should not have any affect\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    runCleaner(hiveConf);\n+\n+    // Verify query result\n+    rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData3), rs);\n+  }\n+\n+  @Test\n+  public void testFullACIDAbortWithMinorMajorCompaction() throws Exception {\n+    // 1. Insert some rows into acid table\n+    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.ACIDTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.ACIDTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 2 delta directories.\n+    verifyDeltaDirAndResult(2, Table.ACIDTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.ACIDTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+    // 3. insert few more rows in acid table\n+    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.ACIDTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.ACIDTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTU0NDYxNw==", "bodyText": "fixed, turned of StatsOptimizer", "url": "https://github.com/apache/hive/pull/1548#discussion_r499544617", "createdAt": "2020-10-05T11:58:22Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2128,6 +2128,395 @@ public void testCleanerForTxnToWriteId() throws Exception {\n             0, TxnDbUtil.countQueryAgent(hiveConf, \"select count(*) from TXN_TO_WRITE_ID\"));\n   }\n \n+  @Test\n+    public void testMmTableAbortWithCompaction() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 4. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData2);\n+    verifyBaseDirAndResult(0, Table.MMTBL.toString(), \"\", resultData2);\n+    // 5. Run Cleaner. Shouldn't impact anything.\n+    runCleaner(hiveConf);\n+    // 6. Run initiator remove aborted entry from TXNS table\n+    runInitiator(hiveConf);\n+\n+    // Verify query result\n+    List<String> rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData2), rs);\n+\n+    int [][] resultData3 = new int[][] {{1,2}, {5,6}, {7,8}};\n+    // 7. add few more rows\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(7,8)\");\n+    // 8. add one more aborted delta\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(9,10)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+\n+    // 9. Perform a MAJOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    verifyDeltaDirAndResult(4, Table.MMTBL.toString(), \"\", resultData3);\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runCleaner(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runInitiator(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // Verify query result\n+    rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData3), rs);\n+  }\n+  @Test\n+  public void testMmTableAbortWithCompactionNoCleanup() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}, {5,6}};\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \" values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData1);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 3. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+    verifyBaseDirAndResult(0, Table.MMTBL.toString(), \"\", resultData1);\n+    // Verify query result\n+    List<String> rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData1), rs);\n+\n+    int [][] resultData3 = new int[][] {{1,2}, {5,6}, {7,8}};\n+    // 4. add few more rows\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(7,8)\");\n+    // 5. add one more aborted delta\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(9,10)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    verifyDeltaDirAndResult(4, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // 6. Perform a MAJOR compaction, expectation is it should remove aborted delta dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // 7. Run one more Major compaction this should not have any affect\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    runCleaner(hiveConf);\n+\n+    // Verify query result\n+    rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData3), rs);\n+  }\n+\n+  @Test\n+  public void testFullACIDAbortWithMinorMajorCompaction() throws Exception {\n+    // 1. Insert some rows into acid table\n+    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.ACIDTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.ACIDTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 2 delta directories.\n+    verifyDeltaDirAndResult(2, Table.ACIDTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.ACIDTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+    // 3. insert few more rows in acid table\n+    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.ACIDTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.ACIDTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEzNzcxMg=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 156}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjczNTEyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzoyMTozMFrOHcQPxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzoyMDowNlrOHcmkng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM4ODM1Ng==", "bodyText": "This is going issue many filesystem listing on a table with many partitions, that is going to be very slow on S3. I think you should consider changing this logic to be similar to getHdfsDirSnapshots that would do 1 recursive listing, iterate all the files and collect the deltas that needs to be deleted and delete them at the end (possible concurrently)", "url": "https://github.com/apache/hive/pull/1548#discussion_r499388356", "createdAt": "2020-10-05T07:21:30Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n+          return true;\n+        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    };\n+    List<FileStatus> deleted = new ArrayList<>();\n+    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n+    return deleted;\n+  }\n+\n+  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYyMDUxOA==", "bodyText": "getHdfsDirSnapshots does the same recursive listing, isn't it?\nRemoteIterator<LocatedFileStatus> itr = fs.listFiles(path, true);\nwhile (itr.hasNext()) {", "url": "https://github.com/apache/hive/pull/1548#discussion_r499620518", "createdAt": "2020-10-05T14:00:12Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n+          return true;\n+        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    };\n+    List<FileStatus> deleted = new ArrayList<>();\n+    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n+    return deleted;\n+  }\n+\n+  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM4ODM1Ng=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc1NDE0Mg==", "bodyText": "changed to use getHdfsDirSnapshots,\n@pvargacl do you know. if i should access cached data somehow?", "url": "https://github.com/apache/hive/pull/1548#discussion_r499754142", "createdAt": "2020-10-05T17:20:06Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n+          return true;\n+        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    };\n+    List<FileStatus> deleted = new ArrayList<>();\n+    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n+    return deleted;\n+  }\n+\n+  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM4ODM1Ng=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjgwODQ4OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzo0NDo0NVrOHcQ8HQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzoyMDozN1rOHcmltw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM5OTcwOQ==", "bodyText": "Why the contains \"=\", are we checking for a partition where the user named the column exactly like a valid delta dir? I don't think we should support that", "url": "https://github.com/apache/hive/pull/1548#discussion_r499399709", "createdAt": "2020-10-05T07:44:45Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1NjM5Nw==", "bodyText": "I was also wondering the same, as this code was there in the earlier patches so I have just kept it. We can remove this.", "url": "https://github.com/apache/hive/pull/1548#discussion_r499656397", "createdAt": "2020-10-05T14:49:00Z", "author": {"login": "vpnvishv"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM5OTcwOQ=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc1NDQyMw==", "bodyText": "changed, included delete_delta as well", "url": "https://github.com/apache/hive/pull/1548#discussion_r499754423", "createdAt": "2020-10-05T17:20:37Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM5OTcwOQ=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjgyOTM5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzo1MDoxMFrOHcRISg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNToxMDowMVrOHchjKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwMjgyNg==", "bodyText": "I am wondering are we covering all the use cases here? Is it possible that this dynamic part query was writing to an existing partition with existing older writes and a compaction was running before we managed to delete the aborted delta? I think in this case sadly, we still going to read the aborted data as valid. Could you add a test case to check if it is indeed a problem or not? (I do not have an idea for a solution...)", "url": "https://github.com/apache/hive/pull/1548#discussion_r499402826", "createdAt": "2020-10-05T07:50:10Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYyMzg2NA==", "bodyText": "Why would it read the aborted data as valid if txn is in still in aborted state?", "url": "https://github.com/apache/hive/pull/1548#discussion_r499623864", "createdAt": "2020-10-05T14:05:03Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwMjgyNg=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY1NTMwNg==", "bodyText": "@pvargacl Sorry I may be missing something here, but with this change, how can compactor read the data of an aborted delta. It should be in the aborted list right, due to this dummy p type entry in TXN_COMPONENTS?", "url": "https://github.com/apache/hive/pull/1548#discussion_r499655306", "createdAt": "2020-10-05T14:47:31Z", "author": {"login": "vpnvishv"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwMjgyNg=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY3MTg0OA==", "bodyText": "You are right, I got confused, the p entry will solve this.", "url": "https://github.com/apache/hive/pull/1548#discussion_r499671848", "createdAt": "2020-10-05T15:10:01Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwMjgyNg=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjg0MDU5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzo1Mjo1OFrOHcROsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzoxMjoyMlrOHcmTdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwNDQ2Ng==", "bodyText": "Are we doing this to delete newly created partitions if there are no other writes? Is this ok, what if we found a valid empty partition that is registered in the HMS? We should not delete that. I think this can be skipped all together, the empty partition dir will not bother anybody", "url": "https://github.com/apache/hive/pull/1548#discussion_r499404466", "createdAt": "2020-10-05T07:52:58Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n+          return true;\n+        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    };\n+    List<FileStatus> deleted = new ArrayList<>();\n+    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n+    return deleted;\n+  }\n+\n+  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)\n+      throws IOException {\n+    RemoteIterator<FileStatus> it = listIterator(fs, root, null);\n+\n+    while (it.hasNext()) {\n+      FileStatus fStatus = it.next();\n+      if (fStatus.isDirectory()) {\n+        if (filter.accept(fStatus.getPath())) {\n+          fs.delete(fStatus.getPath(), true);\n+          deleted.add(fStatus);\n+        } else {\n+          deleteDeltaDirectoriesAux(fStatus.getPath(), fs, filter, deleted);\n+          if (isDirectoryEmpty(fs, fStatus.getPath())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTYzOTgzMQ==", "bodyText": "agree, that would simplify re-use of getHdfsDirSnapshots", "url": "https://github.com/apache/hive/pull/1548#discussion_r499639831", "createdAt": "2020-10-05T14:27:03Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n+          return true;\n+        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    };\n+    List<FileStatus> deleted = new ArrayList<>();\n+    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n+    return deleted;\n+  }\n+\n+  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)\n+      throws IOException {\n+    RemoteIterator<FileStatus> it = listIterator(fs, root, null);\n+\n+    while (it.hasNext()) {\n+      FileStatus fStatus = it.next();\n+      if (fStatus.isDirectory()) {\n+        if (filter.accept(fStatus.getPath())) {\n+          fs.delete(fStatus.getPath(), true);\n+          deleted.add(fStatus);\n+        } else {\n+          deleteDeltaDirectoriesAux(fStatus.getPath(), fs, filter, deleted);\n+          if (isDirectoryEmpty(fs, fStatus.getPath())) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwNDQ2Ng=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc0OTc0OA==", "bodyText": "partitions are not removed in HMS", "url": "https://github.com/apache/hive/pull/1548#discussion_r499749748", "createdAt": "2020-10-05T17:12:22Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n+          return true;\n+        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    };\n+    List<FileStatus> deleted = new ArrayList<>();\n+    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n+    return deleted;\n+  }\n+\n+  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)\n+      throws IOException {\n+    RemoteIterator<FileStatus> it = listIterator(fs, root, null);\n+\n+    while (it.hasNext()) {\n+      FileStatus fStatus = it.next();\n+      if (fStatus.isDirectory()) {\n+        if (filter.accept(fStatus.getPath())) {\n+          fs.delete(fStatus.getPath(), true);\n+          deleted.add(fStatus);\n+        } else {\n+          deleteDeltaDirectoriesAux(fStatus.getPath(), fs, filter, deleted);\n+          if (isDirectoryEmpty(fs, fStatus.getPath())) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwNDQ2Ng=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNjg0OTU3OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzo1NToxMFrOHcRToA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNzoyMTozMVrOHcmnwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwNTcyOA==", "bodyText": "This should be similar to tryListLocatedHdfsStatus don't catch all Throwable. And maybe add all this to the HdfsUtils class", "url": "https://github.com/apache/hive/pull/1548#discussion_r499405728", "createdAt": "2020-10-05T07:55:10Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n+          return true;\n+        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    };\n+    List<FileStatus> deleted = new ArrayList<>();\n+    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n+    return deleted;\n+  }\n+\n+  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)\n+      throws IOException {\n+    RemoteIterator<FileStatus> it = listIterator(fs, root, null);\n+\n+    while (it.hasNext()) {\n+      FileStatus fStatus = it.next();\n+      if (fStatus.isDirectory()) {\n+        if (filter.accept(fStatus.getPath())) {\n+          fs.delete(fStatus.getPath(), true);\n+          deleted.add(fStatus);\n+        } else {\n+          deleteDeltaDirectoriesAux(fStatus.getPath(), fs, filter, deleted);\n+          if (isDirectoryEmpty(fs, fStatus.getPath())) {\n+            fs.delete(fStatus.getPath(), false);\n+            deleted.add(fStatus);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private static boolean isDirectoryEmpty(FileSystem fs, Path path) throws IOException {\n+    RemoteIterator<FileStatus> it = listIterator(fs, path, null);\n+    return !it.hasNext();\n+  }\n+\n+  private static RemoteIterator<FileStatus> listIterator(FileSystem fs, Path path, PathFilter filter)\n+      throws IOException {\n+    try {\n+      return new ToFileStatusIterator(SHIMS.listLocatedHdfsStatusIterator(fs, path, filter));\n+    } catch (Throwable t) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc1NDk0Ng==", "bodyText": "removed it", "url": "https://github.com/apache/hive/pull/1548#discussion_r499754946", "createdAt": "2020-10-05T17:21:31Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n+          return true;\n+        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    };\n+    List<FileStatus> deleted = new ArrayList<>();\n+    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n+    return deleted;\n+  }\n+\n+  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)\n+      throws IOException {\n+    RemoteIterator<FileStatus> it = listIterator(fs, root, null);\n+\n+    while (it.hasNext()) {\n+      FileStatus fStatus = it.next();\n+      if (fStatus.isDirectory()) {\n+        if (filter.accept(fStatus.getPath())) {\n+          fs.delete(fStatus.getPath(), true);\n+          deleted.add(fStatus);\n+        } else {\n+          deleteDeltaDirectoriesAux(fStatus.getPath(), fs, filter, deleted);\n+          if (isDirectoryEmpty(fs, fStatus.getPath())) {\n+            fs.delete(fStatus.getPath(), false);\n+            deleted.add(fStatus);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private static boolean isDirectoryEmpty(FileSystem fs, Path path) throws IOException {\n+    RemoteIterator<FileStatus> it = listIterator(fs, path, null);\n+    return !it.hasNext();\n+  }\n+\n+  private static RemoteIterator<FileStatus> listIterator(FileSystem fs, Path path, PathFilter filter)\n+      throws IOException {\n+    try {\n+      return new ToFileStatusIterator(SHIMS.listLocatedHdfsStatusIterator(fs, path, filter));\n+    } catch (Throwable t) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwNTcyOA=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNzEzMjM4OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwOTowNzo1NFrOHcT_JA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzozMjoyMVrOHeevow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0OTYzNg==", "bodyText": "Two questions here:\n\nIn the original Jira there was discussion about not allowing concurrent cleanings of the same stuff (partition / table). Should we worry about this?\nThe slow cleanAborted will clog the executor service, we should do something about this, either in this patch, or follow up something like https://issues.apache.org/jira/browse/HIVE-21150 immediately after this.", "url": "https://github.com/apache/hive/pull/1548#discussion_r499449636", "createdAt": "2020-10-05T09:07:54Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -97,9 +100,9 @@ public void run() {\n           long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n           LOG.info(\"Cleaning based on min open txn id: \" + minOpenTxnId);\n           List<CompletableFuture> cleanerList = new ArrayList<>();\n-          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+          for (CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n             cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n-                    clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY0Njg2MQ==", "bodyText": "In original patch Map<String, NonReentrantReadWriteLock> tableLock = new ConcurrentHashMap<>() was used to prevent  a concurrent p-clean (where the whole table will be scanned). I think, that is resolved by grouping p-cleans and recording list of writeIds that needs to be removed:\nhttps://github.com/apache/hive/pull/1548/files#diff-9cf3ae764b7a33b568a984d695aff837R328\n@vpnvishv is that correct? Also we do not allow concurrent Cleaners, their execution is mutexed.\n\n\nwas related to the following issue based on Map<String, NonReentrantReadWriteLock> tableLock = new ConcurrentHashMap<>()  design:\n\"Suppose you have p-type clean on table T that is running (i.e. has the Write lock) and you have 30 different partition clean requests (in T).  The 30 per partition cleans will get blocked but they will tie up every thread in the pool while they are blocked, right?  If so, no other clean (on any other table) will actually make progress until the p-type on T is done.\"\n\n\nYes, it's still the case that we'll have to wait for all tasks to complete and if there is one long-running task, we won't be able to submit new ones. However not sure if it's a critical issue. I think, we can address it in a separate jira.", "url": "https://github.com/apache/hive/pull/1548#discussion_r501646861", "createdAt": "2020-10-08T11:29:00Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -97,9 +100,9 @@ public void run() {\n           long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n           LOG.info(\"Cleaning based on min open txn id: \" + minOpenTxnId);\n           List<CompletableFuture> cleanerList = new ArrayList<>();\n-          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+          for (CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n             cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n-                    clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0OTYzNg=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcyMzA0Mw==", "bodyText": "I agree, it can be addressed in a follow up Jira.", "url": "https://github.com/apache/hive/pull/1548#discussion_r501723043", "createdAt": "2020-10-08T13:32:21Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -97,9 +100,9 @@ public void run() {\n           long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n           LOG.info(\"Cleaning based on min open txn id: \" + minOpenTxnId);\n           List<CompletableFuture> cleanerList = new ArrayList<>();\n-          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+          for (CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n             cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n-                    clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0OTYzNg=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNzE4MzYyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwOToyMDoyMlrOHcUd1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxOTozODo0MlrOHcrDvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ1NzQ5NA==", "bodyText": "@vpnvishv Why do we do this here? I understand we can, but why don't we let the Cleaner to delete the files? This just makes the compactor slower. Do we have a functionality reason for this?\nAfter this change it will run in CompactorMR and in MMQueryCompactors, but not in normal QueryCompactors?", "url": "https://github.com/apache/hive/pull/1548#discussion_r499457494", "createdAt": "2020-10-05T09:20:22Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java", "diffHunk": "@@ -237,6 +237,7 @@ void run(HiveConf conf, String jobName, Table t, Partition p, StorageDescriptor\n     }\n \n     JobConf job = createBaseJobConf(conf, jobName, t, sd, writeIds, ci);\n+    QueryCompactor.Util.removeAbortedDirsForAcidTable(conf, dir);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY0OTkxOQ==", "bodyText": "@pvargacl You are right, this is not required, as now compactor run in a transaction and the cleaner has validTxnList with aborted bits set. This we have added wrt to Hive-3, in which cleaner doesn't have aborted bits set, as we create validWriteIdList for cleaner based on the highestWriteId.", "url": "https://github.com/apache/hive/pull/1548#discussion_r499649919", "createdAt": "2020-10-05T14:40:28Z", "author": {"login": "vpnvishv"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java", "diffHunk": "@@ -237,6 +237,7 @@ void run(HiveConf conf, String jobName, Table t, Partition p, StorageDescriptor\n     }\n \n     JobConf job = createBaseJobConf(conf, jobName, t, sd, writeIds, ci);\n+    QueryCompactor.Util.removeAbortedDirsForAcidTable(conf, dir);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ1NzQ5NA=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTgyNzY0NA==", "bodyText": "removed", "url": "https://github.com/apache/hive/pull/1548#discussion_r499827644", "createdAt": "2020-10-05T19:38:42Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java", "diffHunk": "@@ -237,6 +237,7 @@ void run(HiveConf conf, String jobName, Table t, Partition p, StorageDescriptor\n     }\n \n     JobConf job = createBaseJobConf(conf, jobName, t, sd, writeIds, ci);\n+    QueryCompactor.Util.removeAbortedDirsForAcidTable(conf, dir);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ1NzQ5NA=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyNzMwNDEzOnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwOTo0OTozN1rOHcVlVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxNzoxOTowNFrOHeom9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ3NTc5Ng==", "bodyText": "I might be mistaken here, but does this mean, that if we have many \"normal\" aborted txn and 1 aborted dynpart txn, we will not initiate a normal compaction until the dynpart stuff is not cleaned up? Is this ok, shouldn't we doing both?", "url": "https://github.com/apache/hive/pull/1548#discussion_r499475796", "createdAt": "2020-10-05T09:49:37Z", "author": {"login": "pvargacl"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -107,11 +107,12 @@ public CompactionTxnHandler() {\n         // Check for aborted txns: number of aborted txns past threshold and age of aborted txns\n         // past time threshold\n         boolean checkAbortedTimeThreshold = abortedTimeThreshold >= 0;\n-        final String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\",\"\n-            + \"MIN(\\\"TXN_STARTED\\\"), COUNT(*)\"\n+        String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\", \"\n+            + \"MIN(\\\"TXN_STARTED\\\"), COUNT(*), \"\n+            + \"MAX(CASE WHEN \\\"TC_OPERATION_TYPE\\\" = \" + OperationType.DYNPART + \" THEN 1 ELSE 0 END) AS \\\"IS_DP\\\" \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTc0NzMzNA==", "bodyText": "why is that? aborted dynPart is just a special case that would be handled separately (IS_DP=1).", "url": "https://github.com/apache/hive/pull/1548#discussion_r499747334", "createdAt": "2020-10-05T17:07:45Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -107,11 +107,12 @@ public CompactionTxnHandler() {\n         // Check for aborted txns: number of aborted txns past threshold and age of aborted txns\n         // past time threshold\n         boolean checkAbortedTimeThreshold = abortedTimeThreshold >= 0;\n-        final String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\",\"\n-            + \"MIN(\\\"TXN_STARTED\\\"), COUNT(*)\"\n+        String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\", \"\n+            + \"MIN(\\\"TXN_STARTED\\\"), COUNT(*), \"\n+            + \"MAX(CASE WHEN \\\"TC_OPERATION_TYPE\\\" = \" + OperationType.DYNPART + \" THEN 1 ELSE 0 END) AS \\\"IS_DP\\\" \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ3NTc5Ng=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTczMjc5NA==", "bodyText": "Previously if you had aborted txn above threshold this would generate a \"normal\" compaction that would clean up everything. However now if you have one dynpart aborted the type will be CLEAN_ABORTED that will only clean the writeids belonging to p-type records and leave everything else. This will delay the normal cleaning. I am not sure that is a problem or not.", "url": "https://github.com/apache/hive/pull/1548#discussion_r501732794", "createdAt": "2020-10-08T13:45:21Z", "author": {"login": "pvargacl"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -107,11 +107,12 @@ public CompactionTxnHandler() {\n         // Check for aborted txns: number of aborted txns past threshold and age of aborted txns\n         // past time threshold\n         boolean checkAbortedTimeThreshold = abortedTimeThreshold >= 0;\n-        final String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\",\"\n-            + \"MIN(\\\"TXN_STARTED\\\"), COUNT(*)\"\n+        String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\", \"\n+            + \"MIN(\\\"TXN_STARTED\\\"), COUNT(*), \"\n+            + \"MAX(CASE WHEN \\\"TC_OPERATION_TYPE\\\" = \" + OperationType.DYNPART + \" THEN 1 ELSE 0 END) AS \\\"IS_DP\\\" \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ3NTc5Ng=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTg4MzQyNw==", "bodyText": "I still don't follow. Aborted txn check is done per db/table/partition, so if you have db1/tbl1/p1/type=NOT_DP and db1/tbl1/null/type=DP - that should generate 2 entries in potential compactions.", "url": "https://github.com/apache/hive/pull/1548#discussion_r501883427", "createdAt": "2020-10-08T17:16:58Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -107,11 +107,12 @@ public CompactionTxnHandler() {\n         // Check for aborted txns: number of aborted txns past threshold and age of aborted txns\n         // past time threshold\n         boolean checkAbortedTimeThreshold = abortedTimeThreshold >= 0;\n-        final String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\",\"\n-            + \"MIN(\\\"TXN_STARTED\\\"), COUNT(*)\"\n+        String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\", \"\n+            + \"MIN(\\\"TXN_STARTED\\\"), COUNT(*), \"\n+            + \"MAX(CASE WHEN \\\"TC_OPERATION_TYPE\\\" = \" + OperationType.DYNPART + \" THEN 1 ELSE 0 END) AS \\\"IS_DP\\\" \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ3NTc5Ng=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTg4NDY2MA==", "bodyText": "oh, sorry, I only considered time based threshold for DYN_PART", "url": "https://github.com/apache/hive/pull/1548#discussion_r501884660", "createdAt": "2020-10-08T17:19:04Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -107,11 +107,12 @@ public CompactionTxnHandler() {\n         // Check for aborted txns: number of aborted txns past threshold and age of aborted txns\n         // past time threshold\n         boolean checkAbortedTimeThreshold = abortedTimeThreshold >= 0;\n-        final String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\",\"\n-            + \"MIN(\\\"TXN_STARTED\\\"), COUNT(*)\"\n+        String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\", \"\n+            + \"MIN(\\\"TXN_STARTED\\\"), COUNT(*), \"\n+            + \"MAX(CASE WHEN \\\"TC_OPERATION_TYPE\\\" = \" + OperationType.DYNPART + \" THEN 1 ELSE 0 END) AS \\\"IS_DP\\\" \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ3NTc5Ng=="}, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTYzNzY0OnYy", "diffSide": "RIGHT", "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzoxOToxM1rOHeeKNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzo0NDoxMVrOHefSHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcxMzQ2Mw==", "bodyText": "I think this assert is quit misleading. I might be wrong but the recursive listing skips empty directories, and actually this new version of cleaning will keep the partition directories (it should) and only delete the delta dirs and files.", "url": "https://github.com/apache/hive/pull/1548#discussion_r501713463", "createdAt": "2020-10-08T13:19:13Z", "author": {"login": "pvargacl"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "diffHunk": "@@ -853,6 +857,273 @@ public void majorCompactAfterAbort() throws Exception {\n             Lists.newArrayList(5, 6), 1);\n   }\n \n+  @Test\n+  public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactAfterAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    // Create three folders with two different transactions\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  private void assertAndCompactCleanAbort(String dbName, String tblName) throws Exception {\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    TxnStore txnHandler = TxnUtils.getTxnStore(conf);\n+    Table table = msClient.getTable(dbName, tblName);\n+    FileSystem fs = FileSystem.get(conf);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table.getSd().getLocation()));\n+    if (3 != stat.length) {\n+      Assert.fail(\"Expecting three directories corresponding to three partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 1, count);\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.CLEANING_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTczMTg3MQ==", "bodyText": "yes, recursive listing skips empty directories - that's was done intentionally. changed the comment", "url": "https://github.com/apache/hive/pull/1548#discussion_r501731871", "createdAt": "2020-10-08T13:44:11Z", "author": {"login": "deniskuzZ"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "diffHunk": "@@ -853,6 +857,273 @@ public void majorCompactAfterAbort() throws Exception {\n             Lists.newArrayList(5, 6), 1);\n   }\n \n+  @Test\n+  public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactAfterAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    // Create three folders with two different transactions\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  private void assertAndCompactCleanAbort(String dbName, String tblName) throws Exception {\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    TxnStore txnHandler = TxnUtils.getTxnStore(conf);\n+    Table table = msClient.getTable(dbName, tblName);\n+    FileSystem fs = FileSystem.get(conf);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table.getSd().getLocation()));\n+    if (3 != stat.length) {\n+      Assert.fail(\"Expecting three directories corresponding to three partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 1, count);\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.CLEANING_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcxMzQ2Mw=="}, "originalCommit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTY0OTg5OnYy", "diffSide": "RIGHT", "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzoyMTo0MFrOHeeRbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzo0NDowNVrOHefR2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcxNTMxMQ==", "bodyText": "this comment is copied I guess", "url": "https://github.com/apache/hive/pull/1548#discussion_r501715311", "createdAt": "2020-10-08T13:21:40Z", "author": {"login": "pvargacl"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "diffHunk": "@@ -853,6 +857,273 @@ public void majorCompactAfterAbort() throws Exception {\n             Lists.newArrayList(5, 6), 1);\n   }\n \n+  @Test\n+  public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactAfterAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    // Create three folders with two different transactions\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  private void assertAndCompactCleanAbort(String dbName, String tblName) throws Exception {\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    TxnStore txnHandler = TxnUtils.getTxnStore(conf);\n+    Table table = msClient.getTable(dbName, tblName);\n+    FileSystem fs = FileSystem.get(conf);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table.getSd().getLocation()));\n+    if (3 != stat.length) {\n+      Assert.fail(\"Expecting three directories corresponding to three partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 1, count);\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.CLEANING_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.SUCCEEDED_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactSeveralTables() throws Exception {\n+    String dbName = \"default\";\n+    String tblName1 = \"cws1\";\n+    String tblName2 = \"cws2\";\n+\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName1, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName2, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,1\".getBytes());\n+    connection2.write(\"2,2\".getBytes());\n+    connection2.abortTransaction();\n+\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    FileSystem fs = FileSystem.get(conf);\n+    Table table1 = msClient.getTable(dbName, tblName1);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table1.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+    Table table2 = msClient.getTable(dbName, tblName2);\n+    stat = fs.listStatus(new Path(table2.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 2, count);\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table1.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCorrectlyCleaned() throws Exception {\n+    // Test that at commit the tables are cleaned properly\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+    HiveStreamingConnection connection = prepareTableAndConnection(dbName, tblName, 1);\n+    connection.beginTransaction();\n+    connection.write(\"1,1\".getBytes());\n+    connection.write(\"2,2\".getBytes());\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTczMTgwMg==", "bodyText": "fixed", "url": "https://github.com/apache/hive/pull/1548#discussion_r501731802", "createdAt": "2020-10-08T13:44:05Z", "author": {"login": "deniskuzZ"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "diffHunk": "@@ -853,6 +857,273 @@ public void majorCompactAfterAbort() throws Exception {\n             Lists.newArrayList(5, 6), 1);\n   }\n \n+  @Test\n+  public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactAfterAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    // Create three folders with two different transactions\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  private void assertAndCompactCleanAbort(String dbName, String tblName) throws Exception {\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    TxnStore txnHandler = TxnUtils.getTxnStore(conf);\n+    Table table = msClient.getTable(dbName, tblName);\n+    FileSystem fs = FileSystem.get(conf);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table.getSd().getLocation()));\n+    if (3 != stat.length) {\n+      Assert.fail(\"Expecting three directories corresponding to three partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 1, count);\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.CLEANING_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.SUCCEEDED_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactSeveralTables() throws Exception {\n+    String dbName = \"default\";\n+    String tblName1 = \"cws1\";\n+    String tblName2 = \"cws2\";\n+\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName1, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName2, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,1\".getBytes());\n+    connection2.write(\"2,2\".getBytes());\n+    connection2.abortTransaction();\n+\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    FileSystem fs = FileSystem.get(conf);\n+    Table table1 = msClient.getTable(dbName, tblName1);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table1.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+    Table table2 = msClient.getTable(dbName, tblName2);\n+    stat = fs.listStatus(new Path(table2.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 2, count);\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table1.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCorrectlyCleaned() throws Exception {\n+    // Test that at commit the tables are cleaned properly\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+    HiveStreamingConnection connection = prepareTableAndConnection(dbName, tblName, 1);\n+    connection.beginTransaction();\n+    connection.write(\"1,1\".getBytes());\n+    connection.write(\"2,2\".getBytes());\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcxNTMxMQ=="}, "originalCommit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "originalPosition": 211}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTY2ODY0OnYy", "diffSide": "RIGHT", "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzoyNTozNVrOHeec6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxNDoxNjo1MVrOHeg0ww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcxODI0OA==", "bodyText": "Could you add two more test with batch size 2. first batch writes to p1 and p2 and commits, second batch writes to p2 and p3 and aborts. And one with the aborted / committed order changed.", "url": "https://github.com/apache/hive/pull/1548#discussion_r501718248", "createdAt": "2020-10-08T13:25:35Z", "author": {"login": "pvargacl"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "diffHunk": "@@ -853,6 +857,273 @@ public void majorCompactAfterAbort() throws Exception {\n             Lists.newArrayList(5, 6), 1);\n   }\n \n+  @Test\n+  public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactAfterAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    // Create three folders with two different transactions\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  private void assertAndCompactCleanAbort(String dbName, String tblName) throws Exception {\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    TxnStore txnHandler = TxnUtils.getTxnStore(conf);\n+    Table table = msClient.getTable(dbName, tblName);\n+    FileSystem fs = FileSystem.get(conf);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table.getSd().getLocation()));\n+    if (3 != stat.length) {\n+      Assert.fail(\"Expecting three directories corresponding to three partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 1, count);\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.CLEANING_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.SUCCEEDED_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactSeveralTables() throws Exception {\n+    String dbName = \"default\";\n+    String tblName1 = \"cws1\";\n+    String tblName2 = \"cws2\";\n+\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName1, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName2, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,1\".getBytes());\n+    connection2.write(\"2,2\".getBytes());\n+    connection2.abortTransaction();\n+\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    FileSystem fs = FileSystem.get(conf);\n+    Table table1 = msClient.getTable(dbName, tblName1);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table1.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+    Table table2 = msClient.getTable(dbName, tblName2);\n+    stat = fs.listStatus(new Path(table2.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 2, count);\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table1.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCorrectlyCleaned() throws Exception {\n+    // Test that at commit the tables are cleaned properly\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+    HiveStreamingConnection connection = prepareTableAndConnection(dbName, tblName, 1);\n+    connection.beginTransaction();\n+    connection.write(\"1,1\".getBytes());\n+    connection.write(\"2,2\".getBytes());\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 1, count);\n+\n+    connection.commitTransaction();\n+\n+    // After commit the row should have been deleted\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+  }\n+\n+  @Test\n+  public void testCleanAbortAndMinorCompact() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection.beginTransaction();\n+    connection.write(\"1,1\".getBytes());\n+    connection.write(\"2,2\".getBytes());\n+    connection.abortTransaction();\n+\n+    executeStatementOnDriver(\"insert into \" + tblName + \" partition (a) values (1, '1')\", driver);\n+    executeStatementOnDriver(\"delete from \" + tblName + \" where b=1\", driver);\n+\n+    conf.setIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_DELTA_NUM_THRESHOLD, 0);\n+    runInitiator(conf);\n+    runWorker(conf);\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 2, count);\n+    // Cleaning should happen in threads concurrently for the minor compaction and the clean abort one.\n+    runCleaner(conf);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+  }\n+\n+  private HiveStreamingConnection prepareTableAndConnection(String dbName, String tblName, int batchSize) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "originalPosition": 254}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc1NzEyMw==", "bodyText": "added", "url": "https://github.com/apache/hive/pull/1548#discussion_r501757123", "createdAt": "2020-10-08T14:16:51Z", "author": {"login": "deniskuzZ"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "diffHunk": "@@ -853,6 +857,273 @@ public void majorCompactAfterAbort() throws Exception {\n             Lists.newArrayList(5, 6), 1);\n   }\n \n+  @Test\n+  public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactAfterAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    // Create three folders with two different transactions\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  private void assertAndCompactCleanAbort(String dbName, String tblName) throws Exception {\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    TxnStore txnHandler = TxnUtils.getTxnStore(conf);\n+    Table table = msClient.getTable(dbName, tblName);\n+    FileSystem fs = FileSystem.get(conf);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table.getSd().getLocation()));\n+    if (3 != stat.length) {\n+      Assert.fail(\"Expecting three directories corresponding to three partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 1, count);\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.CLEANING_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.SUCCEEDED_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactSeveralTables() throws Exception {\n+    String dbName = \"default\";\n+    String tblName1 = \"cws1\";\n+    String tblName2 = \"cws2\";\n+\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName1, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName2, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,1\".getBytes());\n+    connection2.write(\"2,2\".getBytes());\n+    connection2.abortTransaction();\n+\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    FileSystem fs = FileSystem.get(conf);\n+    Table table1 = msClient.getTable(dbName, tblName1);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table1.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+    Table table2 = msClient.getTable(dbName, tblName2);\n+    stat = fs.listStatus(new Path(table2.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 2, count);\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table1.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCorrectlyCleaned() throws Exception {\n+    // Test that at commit the tables are cleaned properly\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+    HiveStreamingConnection connection = prepareTableAndConnection(dbName, tblName, 1);\n+    connection.beginTransaction();\n+    connection.write(\"1,1\".getBytes());\n+    connection.write(\"2,2\".getBytes());\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 1, count);\n+\n+    connection.commitTransaction();\n+\n+    // After commit the row should have been deleted\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+  }\n+\n+  @Test\n+  public void testCleanAbortAndMinorCompact() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection.beginTransaction();\n+    connection.write(\"1,1\".getBytes());\n+    connection.write(\"2,2\".getBytes());\n+    connection.abortTransaction();\n+\n+    executeStatementOnDriver(\"insert into \" + tblName + \" partition (a) values (1, '1')\", driver);\n+    executeStatementOnDriver(\"delete from \" + tblName + \" where b=1\", driver);\n+\n+    conf.setIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_DELTA_NUM_THRESHOLD, 0);\n+    runInitiator(conf);\n+    runWorker(conf);\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 2, count);\n+    // Cleaning should happen in threads concurrently for the minor compaction and the clean abort one.\n+    runCleaner(conf);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+  }\n+\n+  private HiveStreamingConnection prepareTableAndConnection(String dbName, String tblName, int batchSize) throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcxODI0OA=="}, "originalCommit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "originalPosition": 254}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTcwMzY0OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzozMzozMFrOHeeyzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxNDoyODo0MFrOHehZ3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcyMzg1Mw==", "bodyText": "Shouldn't you mark the compaction failed or cleaned?", "url": "https://github.com/apache/hive/pull/1548#discussion_r501723853", "createdAt": "2020-10-08T13:33:30Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -232,6 +240,51 @@ public Object run() throws Exception {\n   private static String idWatermark(CompactionInfo ci) {\n     return \" id=\" + ci.id;\n   }\n+\n+  private void cleanAborted(CompactionInfo ci) throws MetaException {\n+    if (ci.writeIds == null || ci.writeIds.size() == 0) {\n+      LOG.warn(\"Attempted cleaning aborted transaction with empty writeId list\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTczMTY5Nw==", "bodyText": "yep, good catch!", "url": "https://github.com/apache/hive/pull/1548#discussion_r501731697", "createdAt": "2020-10-08T13:43:56Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -232,6 +240,51 @@ public Object run() throws Exception {\n   private static String idWatermark(CompactionInfo ci) {\n     return \" id=\" + ci.id;\n   }\n+\n+  private void cleanAborted(CompactionInfo ci) throws MetaException {\n+    if (ci.writeIds == null || ci.writeIds.size() == 0) {\n+      LOG.warn(\"Attempted cleaning aborted transaction with empty writeId list\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcyMzg1Mw=="}, "originalCommit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTc2NjYyMA==", "bodyText": "fixed", "url": "https://github.com/apache/hive/pull/1548#discussion_r501766620", "createdAt": "2020-10-08T14:28:40Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -232,6 +240,51 @@ public Object run() throws Exception {\n   private static String idWatermark(CompactionInfo ci) {\n     return \" id=\" + ci.id;\n   }\n+\n+  private void cleanAborted(CompactionInfo ci) throws MetaException {\n+    if (ci.writeIds == null || ci.writeIds.size() == 0) {\n+      LOG.warn(\"Attempted cleaning aborted transaction with empty writeId list\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcyMzg1Mw=="}, "originalCommit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTc5MjkzOnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzo1MTo1NlrOHefpww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzo1MTo1NlrOHefpww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTczNzkyMw==", "bodyText": "cool", "url": "https://github.com/apache/hive/pull/1548#discussion_r501737923", "createdAt": "2020-10-08T13:51:56Z", "author": {"login": "pvargacl"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -414,77 +436,56 @@ public void markCleaned(CompactionInfo info) throws MetaException {\n          * aborted TXN_COMPONENTS above tc_writeid (and consequently about aborted txns).\n          * See {@link ql.txn.compactor.Cleaner.removeFiles()}\n          */\n-        s = \"SELECT DISTINCT \\\"TXN_ID\\\" FROM \\\"TXNS\\\", \\\"TXN_COMPONENTS\\\" WHERE \\\"TXN_ID\\\" = \\\"TC_TXNID\\\" \"\n-            + \"AND \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \" AND \\\"TC_DATABASE\\\" = ? AND \\\"TC_TABLE\\\" = ?\";\n-        if (info.highestWriteId != 0) s += \" AND \\\"TC_WRITEID\\\" <= ?\";\n-        if (info.partName != null) s += \" AND \\\"TC_PARTITION\\\" = ?\";\n+        List<String> queries = new ArrayList<>();\n+        Iterator<Long> writeIdsIter = null;\n+        List<Integer> counts = null;\n \n-        pStmt = dbConn.prepareStatement(s);\n-        paramCount = 1;\n-        pStmt.setString(paramCount++, info.dbname);\n-        pStmt.setString(paramCount++, info.tableName);\n-        if(info.highestWriteId != 0) {\n-          pStmt.setLong(paramCount++, info.highestWriteId);\n+        s = \"DELETE FROM \\\"TXN_COMPONENTS\\\" WHERE \\\"TC_TXNID\\\" IN (\" +\n+          \"   SELECT \\\"TXN_ID\\\" FROM \\\"TXNS\\\" WHERE \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \") \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "originalPosition": 197}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NjE4NjAwOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNTo1NDozNVrOHlFAmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQyMDoxNDo0M1rOHlPS2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0MTQzMw==", "bodyText": "This can be consolidated with most of isDynPartIngest in CompactionUtils", "url": "https://github.com/apache/hive/pull/1548#discussion_r508641433", "createdAt": "2020-10-20T15:54:35Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java", "diffHunk": "@@ -589,4 +593,9 @@ private void checkInterrupt() throws InterruptedException {\n       throw new InterruptedException(\"Compaction execution is interrupted\");\n     }\n   }\n-}\n+\n+  private static boolean isDynPartAbort(Table t, CompactionInfo ci) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODgwOTk0NA==", "bodyText": "those are actually 2 diff methods the only common part is the check for isDynPart. Also there is no CompactionUtils only CompactorUtil, that contains thread factory stuff.", "url": "https://github.com/apache/hive/pull/1548#discussion_r508809944", "createdAt": "2020-10-20T20:14:43Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java", "diffHunk": "@@ -589,4 +593,9 @@ private void checkInterrupt() throws InterruptedException {\n       throw new InterruptedException(\"Compaction execution is interrupted\");\n     }\n   }\n-}\n+\n+  private static boolean isDynPartAbort(Table t, CompactionInfo ci) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0MTQzMw=="}, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NjE5MDgzOnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNTo1NToxM1rOHlFDlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNTo1NToxM1rOHlFDlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0MjE5OQ==", "bodyText": "FYI MM tests are usually in TestTxnCommandsForMmTable.java but I don't really care about this", "url": "https://github.com/apache/hive/pull/1548#discussion_r508642199", "createdAt": "2020-10-20T15:55:13Z", "author": {"login": "klcopp"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2128,24 +2129,601 @@ public void testCleanerForTxnToWriteId() throws Exception {\n             0, TxnDbUtil.countQueryAgent(hiveConf, \"select count(*) from TXN_TO_WRITE_ID\"));\n   }\n \n-  private void verifyDirAndResult(int expectedDeltas) throws Exception {\n-    FileSystem fs = FileSystem.get(hiveConf);\n-    // Verify the content of subdirs\n-    FileStatus[] status = fs.listStatus(new Path(TEST_WAREHOUSE_DIR + \"/\" +\n-        (Table.MMTBL).toString().toLowerCase()), FileUtils.HIDDEN_FILES_PATH_FILTER);\n+  @Test\n+  public void testMmTableAbortWithCompaction() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NjIwNDYzOnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNTo1NzoxNlrOHlFM2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQyMDowMzozOVrOHlO7xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0NDU3MA==", "bodyText": "Why was this changed?", "url": "https://github.com/apache/hive/pull/1548#discussion_r508644570", "createdAt": "2020-10-20T15:57:16Z", "author": {"login": "klcopp"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -400,11 +389,11 @@ public void markCleaned(CompactionInfo info) throws MetaException {\n           pStmt.setString(paramCount++, info.partName);\n         }\n         if(info.highestWriteId != 0) {\n-          pStmt.setLong(paramCount++, info.highestWriteId);\n+          pStmt.setLong(paramCount, info.highestWriteId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODgwNDAzOQ==", "bodyText": "redundant post increment", "url": "https://github.com/apache/hive/pull/1548#discussion_r508804039", "createdAt": "2020-10-20T20:03:39Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -400,11 +389,11 @@ public void markCleaned(CompactionInfo info) throws MetaException {\n           pStmt.setString(paramCount++, info.partName);\n         }\n         if(info.highestWriteId != 0) {\n-          pStmt.setLong(paramCount++, info.highestWriteId);\n+          pStmt.setLong(paramCount, info.highestWriteId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0NDU3MA=="}, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NjIyMzg3OnYy", "diffSide": "LEFT", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNjowMDoyNlrOHlFYnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQyMDowMzo0NFrOHlO75g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0NzU4Mw==", "bodyText": "Any ideas about why this was here? Just curious", "url": "https://github.com/apache/hive/pull/1548#discussion_r508647583", "createdAt": "2020-10-20T16:00:26Z", "author": {"login": "klcopp"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -134,9 +132,6 @@ public CompactionTxnHandler() {\n             response.add(info);\n           }\n         }\n-\n-        LOG.debug(\"Going to rollback\");\n-        dbConn.rollback();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODgwNDA3MA==", "bodyText": "no idea :)", "url": "https://github.com/apache/hive/pull/1548#discussion_r508804070", "createdAt": "2020-10-20T20:03:44Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -134,9 +132,6 @@ public CompactionTxnHandler() {\n             response.add(info);\n           }\n         }\n-\n-        LOG.debug(\"Going to rollback\");\n-        dbConn.rollback();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0NzU4Mw=="}, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NjYwMDYyOnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNzoyNTowNFrOHlJGLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwNzozNzoyOVrOHleN3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODcwODM5OA==", "bodyText": "This is just refactoring right? LGTM but can you make sure @pvary sees this as well?", "url": "https://github.com/apache/hive/pull/1548#discussion_r508708398", "createdAt": "2020-10-20T17:25:04Z", "author": {"login": "klcopp"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -414,76 +403,30 @@ public void markCleaned(CompactionInfo info) throws MetaException {\n          * aborted TXN_COMPONENTS above tc_writeid (and consequently about aborted txns).\n          * See {@link ql.txn.compactor.Cleaner.removeFiles()}\n          */\n-        s = \"SELECT DISTINCT \\\"TXN_ID\\\" FROM \\\"TXNS\\\", \\\"TXN_COMPONENTS\\\" WHERE \\\"TXN_ID\\\" = \\\"TC_TXNID\\\" \"\n-            + \"AND \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \" AND \\\"TC_DATABASE\\\" = ? AND \\\"TC_TABLE\\\" = ?\";\n-        if (info.highestWriteId != 0) s += \" AND \\\"TC_WRITEID\\\" <= ?\";\n-        if (info.partName != null) s += \" AND \\\"TC_PARTITION\\\" = ?\";\n-\n+        s = \"DELETE FROM \\\"TXN_COMPONENTS\\\" WHERE \\\"TC_TXNID\\\" IN (\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODgwNTE2Mw==", "bodyText": "this is an optimization that makes everything in 1 db request instead of 2 (select + delete)", "url": "https://github.com/apache/hive/pull/1548#discussion_r508805163", "createdAt": "2020-10-20T20:05:35Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -414,76 +403,30 @@ public void markCleaned(CompactionInfo info) throws MetaException {\n          * aborted TXN_COMPONENTS above tc_writeid (and consequently about aborted txns).\n          * See {@link ql.txn.compactor.Cleaner.removeFiles()}\n          */\n-        s = \"SELECT DISTINCT \\\"TXN_ID\\\" FROM \\\"TXNS\\\", \\\"TXN_COMPONENTS\\\" WHERE \\\"TXN_ID\\\" = \\\"TC_TXNID\\\" \"\n-            + \"AND \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \" AND \\\"TC_DATABASE\\\" = ? AND \\\"TC_TABLE\\\" = ?\";\n-        if (info.highestWriteId != 0) s += \" AND \\\"TC_WRITEID\\\" <= ?\";\n-        if (info.partName != null) s += \" AND \\\"TC_PARTITION\\\" = ?\";\n-\n+        s = \"DELETE FROM \\\"TXN_COMPONENTS\\\" WHERE \\\"TC_TXNID\\\" IN (\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODcwODM5OA=="}, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODgwNzQ5OQ==", "bodyText": "@pvary, could you please take a quick look? thanks!", "url": "https://github.com/apache/hive/pull/1548#discussion_r508807499", "createdAt": "2020-10-20T20:10:03Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -414,76 +403,30 @@ public void markCleaned(CompactionInfo info) throws MetaException {\n          * aborted TXN_COMPONENTS above tc_writeid (and consequently about aborted txns).\n          * See {@link ql.txn.compactor.Cleaner.removeFiles()}\n          */\n-        s = \"SELECT DISTINCT \\\"TXN_ID\\\" FROM \\\"TXNS\\\", \\\"TXN_COMPONENTS\\\" WHERE \\\"TXN_ID\\\" = \\\"TC_TXNID\\\" \"\n-            + \"AND \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \" AND \\\"TC_DATABASE\\\" = ? AND \\\"TC_TABLE\\\" = ?\";\n-        if (info.highestWriteId != 0) s += \" AND \\\"TC_WRITEID\\\" <= ?\";\n-        if (info.partName != null) s += \" AND \\\"TC_PARTITION\\\" = ?\";\n-\n+        s = \"DELETE FROM \\\"TXN_COMPONENTS\\\" WHERE \\\"TC_TXNID\\\" IN (\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODcwODM5OA=="}, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTA1NDQzMA==", "bodyText": "never mind, LGTM", "url": "https://github.com/apache/hive/pull/1548#discussion_r509054430", "createdAt": "2020-10-21T07:37:29Z", "author": {"login": "klcopp"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -414,76 +403,30 @@ public void markCleaned(CompactionInfo info) throws MetaException {\n          * aborted TXN_COMPONENTS above tc_writeid (and consequently about aborted txns).\n          * See {@link ql.txn.compactor.Cleaner.removeFiles()}\n          */\n-        s = \"SELECT DISTINCT \\\"TXN_ID\\\" FROM \\\"TXNS\\\", \\\"TXN_COMPONENTS\\\" WHERE \\\"TXN_ID\\\" = \\\"TC_TXNID\\\" \"\n-            + \"AND \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \" AND \\\"TC_DATABASE\\\" = ? AND \\\"TC_TABLE\\\" = ?\";\n-        if (info.highestWriteId != 0) s += \" AND \\\"TC_WRITEID\\\" <= ?\";\n-        if (info.partName != null) s += \" AND \\\"TC_PARTITION\\\" = ?\";\n-\n+        s = \"DELETE FROM \\\"TXN_COMPONENTS\\\" WHERE \\\"TC_TXNID\\\" IN (\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODcwODM5OA=="}, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 125}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 318, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}