{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM3MDc2Mjgz", "number": 1151, "title": "HIVE-23725: ValidTxnManager snapshot outdating causing partial reads", "bodyText": "HIVE-23725: ValidTxnManager snapshot outdating causing partial reads in merge insert", "createdAt": "2020-06-19T12:33:16Z", "url": "https://github.com/apache/hive/pull/1151", "merged": true, "mergeCommit": {"oid": "e2a02f1b43cba657d4d1c16ead091072be5fe834"}, "closed": true, "closedAt": "2020-06-26T11:55:05Z", "author": {"login": "pvargacl"}, "timelineItems": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcsyM_2AH2gAyNDM3MDc2MjgzOmY3MDNmZjkxMzlkOWNlOWY5ZmQ0MTI4ZTliOTY2M2Q3MTgwOTg0MGQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcu_W13AH2gAyNDM3MDc2MjgzOjMxNGJlNTEyZDg0ZTViZjFhYTk1NjMxMzlmM2MzOGZmODk3NDg1NGI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "f703ff9139d9ce9f9fd4128e9b9663d71809840d", "author": {"user": {"login": "pvargacl", "name": "Peter Varga"}}, "url": "https://github.com/apache/hive/commit/f703ff9139d9ce9f9fd4128e9b9663d71809840d", "committedDate": "2020-06-19T12:28:12Z", "message": "HIVE-23725: ValidTxnManager snapshot outdating causing partial reads in merge insert"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0MDU3NzQ1", "url": "https://github.com/apache/hive/pull/1151#pullrequestreview-434057745", "createdAt": "2020-06-19T13:01:04Z", "commit": {"oid": "f703ff9139d9ce9f9fd4128e9b9663d71809840d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzowMTowNFrOGmT_oA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzowMTowNFrOGmT_oA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjgyNjY1Ng==", "bodyText": "I think retryLock would be a proper name.", "url": "https://github.com/apache/hive/pull/1151#discussion_r442826656", "createdAt": "2020-06-19T13:01:04Z", "author": {"login": "deniskuzZ"}, "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "diffHunk": "@@ -4979,10 +4979,11 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n \n     HIVE_QUERY_REEXECUTION_ENABLED(\"hive.query.reexecution.enabled\", true,\n         \"Enable query reexecutions\"),\n-    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize\",\n+    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize,lockacquisition\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f703ff9139d9ce9f9fd4128e9b9663d71809840d"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0MDYwNjA0", "url": "https://github.com/apache/hive/pull/1151#pullrequestreview-434060604", "createdAt": "2020-06-19T13:05:09Z", "commit": {"oid": "f703ff9139d9ce9f9fd4128e9b9663d71809840d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzowNToxMFrOGmUHug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzowNToxMFrOGmUHug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjgyODczMA==", "bodyText": "wrap releaseLocksAndCommitOrRollback(false) with try and throw exception in finally, instead of rollback null", "url": "https://github.com/apache/hive/pull/1151#discussion_r442828730", "createdAt": "2020-06-19T13:05:10Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,14 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n           // txn list and retry\n-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n-          // Currently, we acquire a snapshot, we compile the query wrt that snapshot,\n-          // and then, we acquire locks. If snapshot is still valid, we continue as usual.\n-          // But if snapshot is not valid, we recompile the query.\n-          if (driverContext.isOutdatedTxn()) {\n-            driverContext.getTxnManager().rollbackTxn();\n-\n-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n-            lockAndRespond();\n-          }\n-          driverContext.setRetrial(true);\n-          driverContext.getBackupContext().addSubContext(context);\n-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n-          context = driverContext.getBackupContext();\n-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n-            driverContext.getTxnManager().getValidTxns().toString());\n-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n-            validTxnManager.recordValidWriteIds();\n-          }\n-\n-          if (!alreadyCompiled) {\n-            // compile internal will automatically reset the perf logger\n-            compileInternal(command, true);\n-          } else {\n-            // Since we're reusing the compiled plan, we need to update its start time for current run\n-            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n-          }\n-\n-          if (!validTxnManager.isValidTxnListState()) {\n-            // Throw exception\n-            throw handleHiveException(new HiveException(\"Operation could not be executed\"), 14);\n-          }\n-\n-          //Reset the PerfLogger\n-          perfLogger = SessionState.getPerfLogger(true);\n-\n-          // the reason that we set the txn manager for the cxt here is because each\n-          // query has its own ctx object. The txn mgr is shared across the\n-          // same instance of Driver, which can run multiple queries.\n-          context.setHiveTxnManager(driverContext.getTxnManager());\n+          rollback(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f703ff9139d9ce9f9fd4128e9b9663d71809840d"}, "originalPosition": 59}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84", "author": {"user": {"login": "pvargacl", "name": "Peter Varga"}}, "url": "https://github.com/apache/hive/commit/b9cf35abae2e62b8d7f50971e115d1b5a3eabc84", "committedDate": "2020-06-21T20:52:56Z", "message": "Fix failing tests and review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM0NTQ0NjMx", "url": "https://github.com/apache/hive/pull/1151#pullrequestreview-434544631", "createdAt": "2020-06-21T23:59:21Z", "commit": {"oid": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMVQyMzo1OToyMVrOGmvCmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQwMDowOTozOFrOGmvGIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2OTc4NQ==", "bodyText": "Do we really want retrylock to be driven by a config? Shouldn't it just be enabled?", "url": "https://github.com/apache/hive/pull/1151#discussion_r443269785", "createdAt": "2020-06-21T23:59:21Z", "author": {"login": "jcamachor"}, "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "diffHunk": "@@ -4979,10 +4979,11 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n \n     HIVE_QUERY_REEXECUTION_ENABLED(\"hive.query.reexecution.enabled\", true,\n         \"Enable query reexecutions\"),\n-    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize\",\n+    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize,retrylock\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2OTk3MA==", "bodyText": "equals ?", "url": "https://github.com/apache/hive/pull/1151#discussion_r443269970", "createdAt": "2020-06-22T00:01:16Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java", "diffHunk": "@@ -64,6 +65,9 @@ private static IReExecutionPlugin buildReExecPlugin(String name) throws RuntimeE\n     if (name.equals(\"reoptimize\")) {\n       return new ReOptimizePlugin();\n     }\n+    if (name.endsWith(\"retrylock\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI3MDY4OA==", "bodyText": "I think this makes behavior wrt original logic slightly different. For instance, if another transaction obtains the locks in between the moment that this transaction releases them and is going to acquire them again, does this mean the transaction would fail for a second time? If that is the case, should we have a specific configuration for the number of retries in this case? It seems for the default re-execution the number of retries is 1, but in this case, we could retry several times before failing the query.", "url": "https://github.com/apache/hive/pull/1151#discussion_r443270688", "createdAt": "2020-06-22T00:09:38Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,18 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n-          // txn list and retry\n-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n-          // Currently, we acquire a snapshot, we compile the query wrt that snapshot,\n-          // and then, we acquire locks. If snapshot is still valid, we continue as usual.\n-          // But if snapshot is not valid, we recompile the query.\n-          if (driverContext.isOutdatedTxn()) {\n-            driverContext.getTxnManager().rollbackTxn();\n-\n-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n-            lockAndRespond();\n-          }\n-          driverContext.setRetrial(true);\n-          driverContext.getBackupContext().addSubContext(context);\n-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n-          context = driverContext.getBackupContext();\n-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n-            driverContext.getTxnManager().getValidTxns().toString());\n-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n-            validTxnManager.recordValidWriteIds();\n-          }\n-\n-          if (!alreadyCompiled) {\n-            // compile internal will automatically reset the perf logger\n-            compileInternal(command, true);\n-          } else {\n-            // Since we're reusing the compiled plan, we need to update its start time for current run\n-            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n-          }\n-\n-          if (!validTxnManager.isValidTxnListState()) {\n-            // Throw exception\n-            throw handleHiveException(new HiveException(\"Operation could not be executed\"), 14);\n+          // txn list and retry (see ReExecutionRetryLockPlugin)\n+          try {\n+            releaseLocksAndCommitOrRollback(false);\n+          } catch (LockException e) {\n+            handleHiveException(e, 12);\n           }\n-\n-          //Reset the PerfLogger\n-          perfLogger = SessionState.getPerfLogger(true);\n-\n-          // the reason that we set the txn manager for the cxt here is because each\n-          // query has its own ctx object. The txn mgr is shared across the\n-          // same instance of Driver, which can run multiple queries.\n-          context.setHiveTxnManager(driverContext.getTxnManager());\n+          throw handleHiveException(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84"}, "originalPosition": 64}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ba8c29266ee5ca8bd25b764f795f903505bda8a4", "author": {"user": {"login": "pvargacl", "name": "Peter Varga"}}, "url": "https://github.com/apache/hive/commit/ba8c29266ee5ca8bd25b764f795f903505bda8a4", "committedDate": "2020-06-23T09:37:49Z", "message": "Add new reexecution count config for retrylock"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1ODczNDMz", "url": "https://github.com/apache/hive/pull/1151#pullrequestreview-435873433", "createdAt": "2020-06-23T15:00:06Z", "commit": {"oid": "ba8c29266ee5ca8bd25b764f795f903505bda8a4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNTowMDowNlrOGntezg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNTowMDowNlrOGntezg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MjgxNA==", "bodyText": "Is this still needed in ReExecDriver? Shouldn't it be driven completely by the plugins implementation now, i.e., shouldReExecute will return false after the number of retries exceeds the max?", "url": "https://github.com/apache/hive/pull/1151#discussion_r444292814", "createdAt": "2020-06-23T15:00:06Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java", "diffHunk": "@@ -148,8 +148,7 @@ public void setOperationId(String operationId) {\n   @Override\n   public CommandProcessorResponse run() throws CommandProcessorException {\n     executionIndex = 0;\n-    int maxExecutuions = 1 + coreDriver.getConf().getIntVar(ConfVars.HIVE_QUERY_MAX_REEXECUTION_COUNT);\n-\n+    int maxExecutions = getMaxExecutions();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba8c29266ee5ca8bd25b764f795f903505bda8a4"}, "originalPosition": 6}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7", "author": {"user": {"login": "pvargacl", "name": "Peter Varga"}}, "url": "https://github.com/apache/hive/commit/4d421609193669b5a46c2f530f6c570de69ad4e7", "committedDate": "2020-06-23T15:21:29Z", "message": "review comment fix"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MzcyMzQ2", "url": "https://github.com/apache/hive/pull/1151#pullrequestreview-436372346", "createdAt": "2020-06-24T06:56:36Z", "commit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNjo1NjozNlrOGoFT6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNjo1NjozNlrOGoFT6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4MzI0MQ==", "bodyText": "should we use WARN here?", "url": "https://github.com/apache/hive/pull/1151#discussion_r444683241", "createdAt": "2020-06-24T06:56:36Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,18 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "originalPosition": 15}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MzczMDUz", "url": "https://github.com/apache/hive/pull/1151#pullrequestreview-436373053", "createdAt": "2020-06-24T06:57:52Z", "commit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNjo1Nzo1MlrOGoFWBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNjo1Nzo1MlrOGoFWBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4Mzc4Mg==", "bodyText": "what is this magic number 12? do we have enum for error codes?", "url": "https://github.com/apache/hive/pull/1151#discussion_r444683782", "createdAt": "2020-06-24T06:57:52Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,18 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n-          // txn list and retry\n-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n-          // Currently, we acquire a snapshot, we compile the query wrt that snapshot,\n-          // and then, we acquire locks. If snapshot is still valid, we continue as usual.\n-          // But if snapshot is not valid, we recompile the query.\n-          if (driverContext.isOutdatedTxn()) {\n-            driverContext.getTxnManager().rollbackTxn();\n-\n-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n-            lockAndRespond();\n-          }\n-          driverContext.setRetrial(true);\n-          driverContext.getBackupContext().addSubContext(context);\n-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n-          context = driverContext.getBackupContext();\n-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n-            driverContext.getTxnManager().getValidTxns().toString());\n-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n-            validTxnManager.recordValidWriteIds();\n-          }\n-\n-          if (!alreadyCompiled) {\n-            // compile internal will automatically reset the perf logger\n-            compileInternal(command, true);\n-          } else {\n-            // Since we're reusing the compiled plan, we need to update its start time for current run\n-            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n-          }\n-\n-          if (!validTxnManager.isValidTxnListState()) {\n-            // Throw exception\n-            throw handleHiveException(new HiveException(\"Operation could not be executed\"), 14);\n+          // txn list and retry (see ReExecutionRetryLockPlugin)\n+          try {\n+            releaseLocksAndCommitOrRollback(false);\n+          } catch (LockException e) {\n+            handleHiveException(e, 12);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "originalPosition": 54}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MzgwOTkx", "url": "https://github.com/apache/hive/pull/1151#pullrequestreview-436380991", "createdAt": "2020-06-24T07:12:03Z", "commit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNzoxMjowM1rOGoFuQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNzoxMjowM1rOGoFuQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4OTk4NQ==", "bodyText": "why do we need this? there is clean up at the beginning. same in other tests", "url": "https://github.com/apache/hive/pull/1151#discussion_r444689985", "createdAt": "2020-06-24T07:12:03Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java", "diffHunk": "@@ -2329,6 +2338,142 @@ private void testConcurrentMergeInsertNoDuplicates(String query, boolean sharedW\n     List res = new ArrayList();\n     driver.getFetchTask().fetch(res);\n     Assert.assertEquals(\"Duplicate records found\", 4, res.size());\n+    dropTable(new String[]{\"target\", \"source\"});\n+  }\n+\n+  /**\n+   * ValidTxnManager.isValidTxnListState can invalidate a snapshot if a relevant write transaction was committed\n+   * between a query compilation and lock acquisition. When this happens we have to recompile the given query,\n+   * otherwise we can miss reading partitions created between. The following three cases test these scenarios.\n+   * @throws Exception ex\n+   */\n+  @Test\n+  public void testMergeInsertDynamicPartitioningSequential() throws Exception {\n+\n+    dropTable(new String[]{\"target\", \"source\"});\n+    conf.setBoolVar(HiveConf.ConfVars.TXN_WRITE_X_LOCK, false);\n+\n+    // Create partition c=1\n+    driver.run(\"create table target (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')\");\n+    driver.run(\"insert into target values (1,1,1), (2,2,1)\");\n+    //Create partition c=2\n+    driver.run(\"create table source (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')\");\n+    driver.run(\"insert into source values (3,3,2), (4,4,2)\");\n+\n+    // txn 1 inserts data to an old and a new partition\n+    driver.run(\"insert into source values (5,5,2), (6,6,3)\");\n+\n+    // txn 2 inserts into the target table into a new partition ( and a duplicate considering the source table)\n+    driver.run(\"insert into target values (3, 3, 2)\");\n+\n+    // txn3 merge\n+    driver.run(\"merge into target t using source s on t.a = s.a \" +\n+        \"when not matched then insert values (s.a, s.b, s.c)\");\n+    driver.run(\"select * from target\");\n+    List res = new ArrayList();\n+    driver.getFetchTask().fetch(res);\n+    // The merge should see all three partition and not create duplicates\n+    Assert.assertEquals(\"Duplicate records found\", 6, res.size());\n+    Assert.assertTrue(\"Partition 3 was skipped\", res.contains(\"6\\t6\\t3\"));\n+    dropTable(new String[]{\"target\", \"source\"});", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "originalPosition": 115}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2Mzg2NjYx", "url": "https://github.com/apache/hive/pull/1151#pullrequestreview-436386661", "createdAt": "2020-06-24T07:21:29Z", "commit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNzoyMToyOVrOGoF_gQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNzoyMToyOVrOGoF_gQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5NDQwMQ==", "bodyText": "should we re-execute when here?", "url": "https://github.com/apache/hive/pull/1151#discussion_r444694401", "createdAt": "2020-06-24T07:21:29Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionRetryLockPlugin.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.reexec;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.Driver;\n+import org.apache.hadoop.hive.ql.plan.mapper.PlanMapper;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorException;\n+\n+public class ReExecutionRetryLockPlugin implements IReExecutionPlugin {\n+\n+  private Driver coreDriver;\n+  private int maxRetryLockExecutions = 1;\n+\n+  @Override\n+  public void initialize(Driver driver) {\n+    coreDriver = driver;\n+    maxRetryLockExecutions = 1 + coreDriver.getConf().getIntVar(HiveConf.ConfVars.HIVE_QUERY_MAX_REEXECUTION_RETRYLOCK_COUNT);\n+  }\n+\n+  @Override\n+  public void beforeExecute(int executionIndex, boolean explainReOptimization) {\n+  }\n+\n+  @Override\n+  public boolean shouldReExecute(int executionNum, CommandProcessorException ex) {\n+    return executionNum < maxRetryLockExecutions && ex != null &&\n+        ex.getMessage().contains(Driver.SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED);\n+  }\n+\n+  @Override\n+  public void prepareToReExecute() {\n+  }\n+\n+  @Override\n+  public boolean shouldReExecute(int executionNum, PlanMapper oldPlanMapper, PlanMapper newPlanMapper) {\n+    return executionNum < maxRetryLockExecutions;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "originalPosition": 53}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0427fe54a19bbe77a04feb9ef7a84ba9520563de", "author": {"user": {"login": "pvargacl", "name": "Peter Varga"}}, "url": "https://github.com/apache/hive/commit/0427fe54a19bbe77a04feb9ef7a84ba9520563de", "committedDate": "2020-06-24T12:30:00Z", "message": "Address review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2NjM0MzQ2", "url": "https://github.com/apache/hive/pull/1151#pullrequestreview-436634346", "createdAt": "2020-06-24T13:15:59Z", "commit": {"oid": "0427fe54a19bbe77a04feb9ef7a84ba9520563de"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "314be512d84e5bf1aa9563139f3c38ff8974854b", "author": {"user": {"login": "pvargacl", "name": "Peter Varga"}}, "url": "https://github.com/apache/hive/commit/314be512d84e5bf1aa9563139f3c38ff8974854b", "committedDate": "2020-06-26T08:55:34Z", "message": "Merge remote-tracking branch 'origin/master' into HIVE-23725-validtxnmanager\n\n# Conflicts:\n#\tcommon/src/java/org/apache/hadoop/hive/conf/HiveConf.java\n#\tql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3681, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}