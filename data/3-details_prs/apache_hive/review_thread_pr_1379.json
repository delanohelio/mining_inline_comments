{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY0NjQ5NjAy", "number": 1379, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QxNDo1Njo1OVrOEgB9UQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QxNDo1ODo0NFrOEgCAXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyMDIxOTY5OnYy", "diffSide": "RIGHT", "path": "pom.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QxNDo1Njo1OVrOHMqpQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQwOToyODoxMVrOHNGmuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzA0MzY0OQ==", "bodyText": "Kafka version is also declared specifically in kafka-handler/pom.xml Can we parmeterize there, so that all of Hive is referencing just one kafka version?", "url": "https://github.com/apache/hive/pull/1379#discussion_r483043649", "createdAt": "2020-09-03T14:56:59Z", "author": {"login": "ashutoshc"}, "path": "pom.xml", "diffHunk": "@@ -169,6 +169,7 @@\n     <junit.version>4.13</junit.version>\n     <junit.jupiter.version>5.6.2</junit.jupiter.version>\n     <junit.vintage.version>5.6.2</junit.vintage.version>\n+    <kafka.version>2.5.0</kafka.version>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "792f69499f1935cfaad743353630dc51835bd119"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzUwMTc1Mw==", "bodyText": "sure!", "url": "https://github.com/apache/hive/pull/1379#discussion_r483501753", "createdAt": "2020-09-04T09:28:11Z", "author": {"login": "abstractdog"}, "path": "pom.xml", "diffHunk": "@@ -169,6 +169,7 @@\n     <junit.version>4.13</junit.version>\n     <junit.jupiter.version>5.6.2</junit.jupiter.version>\n     <junit.vintage.version>5.6.2</junit.vintage.version>\n+    <kafka.version>2.5.0</kafka.version>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzA0MzY0OQ=="}, "originalCommit": {"oid": "792f69499f1935cfaad743353630dc51835bd119"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAyMDIyNzUxOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QxNDo1ODo0NFrOHMquVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQxMzoxMzo0MVrOHNNCHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzA0NDk0OQ==", "bodyText": "This is iterating over all partition objects in plan even when kafka is not used. This gets expensive when there are large number of partition objects. Is it possible to do a quick check to see if kafka is used before iterating over full list of parttions?", "url": "https://github.com/apache/hive/pull/1379#discussion_r483044949", "createdAt": "2020-09-03T14:58:44Z", "author": {"login": "ashutoshc"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java", "diffHunk": "@@ -265,11 +279,70 @@ public URI apply(Path path) {\n         }\n         dag.addURIsForCredentials(uris);\n       }\n+      getKafkaCredentials((MapWork)work, dag, conf);\n     }\n-\n     getCredentialsForFileSinks(work, dag);\n   }\n \n+  private void getKafkaCredentials(MapWork work, DAG dag, JobConf conf) {\n+    Token<?> tokenCheck = dag.getCredentials().getToken(KAFKA_DELEGATION_TOKEN_KEY);\n+    if (tokenCheck != null) {\n+      LOG.debug(\"Kafka credentials already added, skipping...\");\n+      return;\n+    }\n+    LOG.info(\"Getting kafka credentials for mapwork: \" + work.getName());\n+\n+    String kafkaBrokers = null;\n+    Map<String, PartitionDesc> partitions = work.getAliasToPartnInfo();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "792f69499f1935cfaad743353630dc51835bd119"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzEzOTA3Mw==", "bodyText": "good point, I was thinking about the same, this can become an expensive loop and 100% useless for users not using kafka, let me look for a short-circuit way", "url": "https://github.com/apache/hive/pull/1379#discussion_r483139073", "createdAt": "2020-09-03T17:23:04Z", "author": {"login": "abstractdog"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java", "diffHunk": "@@ -265,11 +279,70 @@ public URI apply(Path path) {\n         }\n         dag.addURIsForCredentials(uris);\n       }\n+      getKafkaCredentials((MapWork)work, dag, conf);\n     }\n-\n     getCredentialsForFileSinks(work, dag);\n   }\n \n+  private void getKafkaCredentials(MapWork work, DAG dag, JobConf conf) {\n+    Token<?> tokenCheck = dag.getCredentials().getToken(KAFKA_DELEGATION_TOKEN_KEY);\n+    if (tokenCheck != null) {\n+      LOG.debug(\"Kafka credentials already added, skipping...\");\n+      return;\n+    }\n+    LOG.info(\"Getting kafka credentials for mapwork: \" + work.getName());\n+\n+    String kafkaBrokers = null;\n+    Map<String, PartitionDesc> partitions = work.getAliasToPartnInfo();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzA0NDk0OQ=="}, "originalCommit": {"oid": "792f69499f1935cfaad743353630dc51835bd119"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzYwNzA3MQ==", "bodyText": "@ashutoshc : what do you think about this? edc4ad4#diff-d7b5b051769b68ed5dd602cc30744439R303-R306\n(tested on cluster)", "url": "https://github.com/apache/hive/pull/1379#discussion_r483607071", "createdAt": "2020-09-04T13:13:41Z", "author": {"login": "abstractdog"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java", "diffHunk": "@@ -265,11 +279,70 @@ public URI apply(Path path) {\n         }\n         dag.addURIsForCredentials(uris);\n       }\n+      getKafkaCredentials((MapWork)work, dag, conf);\n     }\n-\n     getCredentialsForFileSinks(work, dag);\n   }\n \n+  private void getKafkaCredentials(MapWork work, DAG dag, JobConf conf) {\n+    Token<?> tokenCheck = dag.getCredentials().getToken(KAFKA_DELEGATION_TOKEN_KEY);\n+    if (tokenCheck != null) {\n+      LOG.debug(\"Kafka credentials already added, skipping...\");\n+      return;\n+    }\n+    LOG.info(\"Getting kafka credentials for mapwork: \" + work.getName());\n+\n+    String kafkaBrokers = null;\n+    Map<String, PartitionDesc> partitions = work.getAliasToPartnInfo();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzA0NDk0OQ=="}, "originalCommit": {"oid": "792f69499f1935cfaad743353630dc51835bd119"}, "originalPosition": 92}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 502, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}