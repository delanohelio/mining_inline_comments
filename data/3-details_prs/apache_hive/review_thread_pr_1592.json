{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA2ODA2OTY1", "number": 1592, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxNTo0MToxOFrOEzW9gA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNToxMDowNVrOE0axZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyMjg5MDI0OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxNTo0MToxOFrOHqiElQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNDoyNjoxN1rOHrRcag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM2MDQ2OQ==", "bodyText": "It's not the end of the world to add the CQ_TXN_ID column, but we can avoid that and keep things more straightforward (i.e. keep compaction stuff out of generic TxnHandler and limit it to CompactionTxnHandler which was made specifically for updating compaction-related tables) by updating CQ_NEXT_TXN_ID in CompactionTxnHandler instead, and calling it straight from Worker, maybe right between commitTxn and markCompacted. It would be so much simpler.", "url": "https://github.com/apache/hive/pull/1592#discussion_r514360469", "createdAt": "2020-10-29T15:41:18Z", "author": {"login": "klcopp"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -1526,6 +1529,10 @@ private void updateWSCommitIdAndCleanUpMetadata(Statement stmt, long txnid, TxnT\n     if (txnType == TxnType.MATER_VIEW_REBUILD) {\n       queryBatch.add(\"DELETE FROM \\\"MATERIALIZATION_REBUILD_LOCKS\\\" WHERE \\\"MRL_TXN_ID\\\" = \" + txnid);\n     }\n+    if (txnType == TxnType.COMPACTION) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "40468c3ed3951680e95c1c6b9488044d96677093"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkyMzIwMw==", "bodyText": "I would prefer this method for two reasons:\n\nTo work correctly, the first point when we can update the CQ_NEXT_TXN_ID, is inside the commitTxn, when we already have the lock for the txnId sequence. Anywhere before that, there can be a txn, that will have a higher txnId than the saved CQ_NEXT_TXN_ID and still not see the compaction as committed -> causing the Cleaner to clean up deltas, that should be kept for that txn. So if we don't do it in commitTxn, we only can do it after that, but that would mean, that the update won't be part of the transaction, and can fail \"silently\"\nIn a follow up Jira, I need to provide a HMS notification for every committed compaction, for that I would also need CQ_TXN_ID", "url": "https://github.com/apache/hive/pull/1592#discussion_r514923203", "createdAt": "2020-10-30T07:55:28Z", "author": {"login": "pvargacl"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -1526,6 +1529,10 @@ private void updateWSCommitIdAndCleanUpMetadata(Statement stmt, long txnid, TxnT\n     if (txnType == TxnType.MATER_VIEW_REBUILD) {\n       queryBatch.add(\"DELETE FROM \\\"MATERIALIZATION_REBUILD_LOCKS\\\" WHERE \\\"MRL_TXN_ID\\\" = \" + txnid);\n     }\n+    if (txnType == TxnType.COMPACTION) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM2MDQ2OQ=="}, "originalCommit": {"oid": "40468c3ed3951680e95c1c6b9488044d96677093"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3OTY0MA==", "bodyText": "I think, you could make updateWSCommitIdAndCleanUpMetadata protected and overwrite it in CompactionTxnHandler, so we won't scatter compaction related stuff across many classes.", "url": "https://github.com/apache/hive/pull/1592#discussion_r514979640", "createdAt": "2020-10-30T09:48:51Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -1526,6 +1529,10 @@ private void updateWSCommitIdAndCleanUpMetadata(Statement stmt, long txnid, TxnT\n     if (txnType == TxnType.MATER_VIEW_REBUILD) {\n       queryBatch.add(\"DELETE FROM \\\"MATERIALIZATION_REBUILD_LOCKS\\\" WHERE \\\"MRL_TXN_ID\\\" = \" + txnid);\n     }\n+    if (txnType == TxnType.COMPACTION) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM2MDQ2OQ=="}, "originalCommit": {"oid": "40468c3ed3951680e95c1c6b9488044d96677093"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTEzNjYxOA==", "bodyText": "Done", "url": "https://github.com/apache/hive/pull/1592#discussion_r515136618", "createdAt": "2020-10-30T14:26:17Z", "author": {"login": "pvargacl"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -1526,6 +1529,10 @@ private void updateWSCommitIdAndCleanUpMetadata(Statement stmt, long txnid, TxnT\n     if (txnType == TxnType.MATER_VIEW_REBUILD) {\n       queryBatch.add(\"DELETE FROM \\\"MATERIALIZATION_REBUILD_LOCKS\\\" WHERE \\\"MRL_TXN_ID\\\" = \" + txnid);\n     }\n+    if (txnType == TxnType.COMPACTION) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM2MDQ2OQ=="}, "originalCommit": {"oid": "40468c3ed3951680e95c1c6b9488044d96677093"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyMjkwMjA4OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOVQxNTo0MzozN1rOHqiLxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQwNzo0NzoxNVrOHrEONw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM2MjMwOQ==", "bodyText": "Cleaner already knows this value, Cleaner#run calls CompactionTxnHandler#findMinOpenTxnIdForCleaner first, then findReadyToClean, so you can just pass it into findReadyToClean.\n(Btw findMinOpenTxnIdForCleaner doesn't filter out timed out txns like getMinOpenTxnIdWaterMark does, might want to change that? (AcidHouseKeeperService should take care of that, but who knows if it's on... on the other hand that's another query and would take longer))", "url": "https://github.com/apache/hive/pull/1592#discussion_r514362309", "createdAt": "2020-10-29T15:43:37Z", "author": {"login": "klcopp"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -281,9 +280,14 @@ public void markCompacted(CompactionInfo info) throws MetaException {\n       try {\n         dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n         stmt = dbConn.createStatement();\n-        String s = \"SELECT \\\"CQ_ID\\\", \\\"CQ_DATABASE\\\", \\\"CQ_TABLE\\\", \\\"CQ_PARTITION\\\", \" +\n-            \"\\\"CQ_TYPE\\\", \\\"CQ_RUN_AS\\\", \\\"CQ_HIGHEST_WRITE_ID\\\" FROM \\\"COMPACTION_QUEUE\\\" \" +\n-            \"WHERE \\\"CQ_STATE\\\" = '\" + READY_FOR_CLEANING + \"'\";\n+        /*\n+         * By filtering on minOpenTxnWaterMark, we will only cleanup after every transaction is committed, that could see\n+         * the uncompacted deltas. This way the cleaner can clean up everything that was made obsolete by this compaction.\n+         */\n+        long minOpenTxnWaterMark = getMinOpenTxnIdWaterMark(dbConn);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "40468c3ed3951680e95c1c6b9488044d96677093"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkxOTk5MQ==", "bodyText": "Passing the minOpenTxn as an argument now\nChanged the findMinOpenTxnIdForCleaner to use getMinOpenTxnIdWaterMark. The timeout boundary checking is needed, since HIVE-23084, because it might be possible for an open txn to appear later, that has txnId lower than the current minOpen and higher the timeout boundary. Probably it wouldn't cause any problem for the Cleaner, but better safe than sorry, this way it always gives correct result.\nThis also means that the max(cq_next_txnid) check is removed, but I think this will only mean, that if there were any txn after the compaction that were aborted, we are going to clean those up also, which is a good side effect.", "url": "https://github.com/apache/hive/pull/1592#discussion_r514919991", "createdAt": "2020-10-30T07:47:15Z", "author": {"login": "pvargacl"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -281,9 +280,14 @@ public void markCompacted(CompactionInfo info) throws MetaException {\n       try {\n         dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n         stmt = dbConn.createStatement();\n-        String s = \"SELECT \\\"CQ_ID\\\", \\\"CQ_DATABASE\\\", \\\"CQ_TABLE\\\", \\\"CQ_PARTITION\\\", \" +\n-            \"\\\"CQ_TYPE\\\", \\\"CQ_RUN_AS\\\", \\\"CQ_HIGHEST_WRITE_ID\\\" FROM \\\"COMPACTION_QUEUE\\\" \" +\n-            \"WHERE \\\"CQ_STATE\\\" = '\" + READY_FOR_CLEANING + \"'\";\n+        /*\n+         * By filtering on minOpenTxnWaterMark, we will only cleanup after every transaction is committed, that could see\n+         * the uncompacted deltas. This way the cleaner can clean up everything that was made obsolete by this compaction.\n+         */\n+        long minOpenTxnWaterMark = getMinOpenTxnIdWaterMark(dbConn);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM2MjMwOQ=="}, "originalCommit": {"oid": "40468c3ed3951680e95c1c6b9488044d96677093"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNjczMTA1OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxMDowNzoyN1rOHrIeoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNDoyODoyOFrOHrRiWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk4OTcyOA==", "bodyText": "Do we expect it to be <= 0? Could we have CQ_NEXT_TXN_ID = NULL?", "url": "https://github.com/apache/hive/pull/1592#discussion_r514989728", "createdAt": "2020-10-30T10:07:27Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -281,9 +281,16 @@ public void markCompacted(CompactionInfo info) throws MetaException {\n       try {\n         dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n         stmt = dbConn.createStatement();\n-        String s = \"SELECT \\\"CQ_ID\\\", \\\"CQ_DATABASE\\\", \\\"CQ_TABLE\\\", \\\"CQ_PARTITION\\\", \" +\n-            \"\\\"CQ_TYPE\\\", \\\"CQ_RUN_AS\\\", \\\"CQ_HIGHEST_WRITE_ID\\\" FROM \\\"COMPACTION_QUEUE\\\" \" +\n-            \"WHERE \\\"CQ_STATE\\\" = '\" + READY_FOR_CLEANING + \"'\";\n+        /*\n+         * By filtering on minOpenTxnWaterMark, we will only cleanup after every transaction is committed, that could see\n+         * the uncompacted deltas. This way the cleaner can clean up everything that was made obsolete by this compaction.\n+         */\n+        String s = \"SELECT \\\"CQ_ID\\\", \\\"CQ_DATABASE\\\", \\\"CQ_TABLE\\\", \\\"CQ_PARTITION\\\", \"\n+                + \"\\\"CQ_TYPE\\\", \\\"CQ_RUN_AS\\\", \\\"CQ_HIGHEST_WRITE_ID\\\" FROM \\\"COMPACTION_QUEUE\\\" WHERE \\\"CQ_STATE\\\" = '\"\n+                + READY_FOR_CLEANING;\n+        if (minOpenTxnWaterMark > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5388ecdc5c7c309f119e43d25f3f5ff0558e9e40"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTEzODEzNw==", "bodyText": "Both of them are here to support easier testing. In some of the compaction test, the compaction workflow is imitated, and it is not run in transaction, so this check would fail.", "url": "https://github.com/apache/hive/pull/1592#discussion_r515138137", "createdAt": "2020-10-30T14:28:28Z", "author": {"login": "pvargacl"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -281,9 +281,16 @@ public void markCompacted(CompactionInfo info) throws MetaException {\n       try {\n         dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n         stmt = dbConn.createStatement();\n-        String s = \"SELECT \\\"CQ_ID\\\", \\\"CQ_DATABASE\\\", \\\"CQ_TABLE\\\", \\\"CQ_PARTITION\\\", \" +\n-            \"\\\"CQ_TYPE\\\", \\\"CQ_RUN_AS\\\", \\\"CQ_HIGHEST_WRITE_ID\\\" FROM \\\"COMPACTION_QUEUE\\\" \" +\n-            \"WHERE \\\"CQ_STATE\\\" = '\" + READY_FOR_CLEANING + \"'\";\n+        /*\n+         * By filtering on minOpenTxnWaterMark, we will only cleanup after every transaction is committed, that could see\n+         * the uncompacted deltas. This way the cleaner can clean up everything that was made obsolete by this compaction.\n+         */\n+        String s = \"SELECT \\\"CQ_ID\\\", \\\"CQ_DATABASE\\\", \\\"CQ_TABLE\\\", \\\"CQ_PARTITION\\\", \"\n+                + \"\\\"CQ_TYPE\\\", \\\"CQ_RUN_AS\\\", \\\"CQ_HIGHEST_WRITE_ID\\\" FROM \\\"COMPACTION_QUEUE\\\" WHERE \\\"CQ_STATE\\\" = '\"\n+                + READY_FOR_CLEANING;\n+        if (minOpenTxnWaterMark > 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk4OTcyOA=="}, "originalCommit": {"oid": "5388ecdc5c7c309f119e43d25f3f5ff0558e9e40"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzMTI2MDQ1OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQxNTo0OTozM1rOHrwCXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQxNTo0OTozM1rOHrwCXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTYzNzg1Mg==", "bodyText": "Thanks for the change! It looks much cleaner and more readable to me. Could you please add \"@OverRide\" annotation as well.\nPending tests ...", "url": "https://github.com/apache/hive/pull/1592#discussion_r515637852", "createdAt": "2020-11-01T15:49:33Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -1120,50 +1128,34 @@ public void setHadoopJobId(String hadoopJobId, long id) {\n \n   @Override\n   @RetrySemantics.Idempotent\n-  public long findMinOpenTxnIdForCleaner() throws MetaException{\n+  public long findMinOpenTxnIdForCleaner() throws MetaException {\n     Connection dbConn = null;\n-    Statement stmt = null;\n-    ResultSet rs = null;\n     try {\n       try {\n         dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n-        stmt = dbConn.createStatement();\n-        String query = \"SELECT COUNT(\\\"TXN_ID\\\") FROM \\\"TXNS\\\" WHERE \\\"TXN_STATE\\\" = \" + TxnStatus.OPEN;\n-        LOG.debug(\"Going to execute query <\" + query + \">\");\n-        rs = stmt.executeQuery(query);\n-        if (!rs.next()) {\n-          throw new MetaException(\"Transaction tables not properly initialized.\");\n-        }\n-        long numOpenTxns = rs.getLong(1);\n-        if (numOpenTxns > 0) {\n-          query = \"SELECT MIN(\\\"RES\\\".\\\"ID\\\") FROM (\" +\n-              \"SELECT MIN(\\\"TXN_ID\\\") AS \\\"ID\\\" FROM \\\"TXNS\\\" WHERE \\\"TXN_STATE\\\" = \" + TxnStatus.OPEN +\n-              \" UNION \" +\n-              \"SELECT MAX(\\\"CQ_NEXT_TXN_ID\\\") AS \\\"ID\\\" FROM \\\"COMPACTION_QUEUE\\\" WHERE \\\"CQ_STATE\\\" = \"\n-              + quoteChar(READY_FOR_CLEANING) +\n-              \") \\\"RES\\\"\";\n-        } else {\n-          query = \"SELECT MAX(\\\"TXN_ID\\\") + 1 FROM \\\"TXNS\\\"\";\n-        }\n-        LOG.debug(\"Going to execute query <\" + query + \">\");\n-        rs = stmt.executeQuery(query);\n-        if (!rs.next()) {\n-          throw new MetaException(\"Transaction tables not properly initialized, no record found in TXNS\");\n-        }\n-        return rs.getLong(1);\n+        return getMinOpenTxnIdWaterMark(dbConn);\n       } catch (SQLException e) {\n         LOG.error(\"Unable to getMinOpenTxnIdForCleaner\", e);\n         rollbackDBConn(dbConn);\n         checkRetryable(dbConn, e, \"getMinOpenTxnForCleaner\");\n         throw new MetaException(\"Unable to execute getMinOpenTxnIfForCleaner() \" +\n             StringUtils.stringifyException(e));\n       } finally {\n-        close(rs, stmt, dbConn);\n+        closeDbConn(dbConn);\n       }\n     } catch (RetryException e) {\n       return findMinOpenTxnIdForCleaner();\n     }\n   }\n+\n+  protected void updateWSCommitIdAndCleanUpMetadata(Statement stmt, long txnid, TxnType txnType, Long commitId,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e1068364252429cd7f65eac7a5a3106d66e3a272"}, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzNDAwMDM5OnYy", "diffSide": "LEFT", "path": "ql/src/test/results/clientpositive/llap/cardinality_preserving_join_opt2.q.out", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNToxMDowNVrOHsIcLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNToxMDo0NFrOHsId6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjAzNzY3Nw==", "bodyText": "You can revert this, I disabled it a minute ago.", "url": "https://github.com/apache/hive/pull/1592#discussion_r516037677", "createdAt": "2020-11-02T15:10:05Z", "author": {"login": "klcopp"}, "path": "ql/src/test/results/clientpositive/llap/cardinality_preserving_join_opt2.q.out", "diffHunk": "@@ -239,7 +239,7 @@ HiveProject(c1=[$11], c5=[$13], c6=[$14], c3=[$1], c4=[$2], c51=[$3], c61=[$4],\n             HiveFilter(condition=[IS NOT NULL($0)])\n               HiveTableScan(table=[[mydb_e10, d4_tab_e10]], table:alias=[r])\n         HiveProject(c1=[$0], c2=[$1], c4=[$3], c5=[$4])\n-          HiveFilter(condition=[AND(OR(IS NULL($4), >($4, 2020-10-01)), IS NOT NULL($1), IS NOT NULL($0))])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3a3b512f7d97d0fc8d7e0e57f4a23a1d2b05000"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjAzODEyMg==", "bodyText": "And rebase of course", "url": "https://github.com/apache/hive/pull/1592#discussion_r516038122", "createdAt": "2020-11-02T15:10:44Z", "author": {"login": "klcopp"}, "path": "ql/src/test/results/clientpositive/llap/cardinality_preserving_join_opt2.q.out", "diffHunk": "@@ -239,7 +239,7 @@ HiveProject(c1=[$11], c5=[$13], c6=[$14], c3=[$1], c4=[$2], c51=[$3], c61=[$4],\n             HiveFilter(condition=[IS NOT NULL($0)])\n               HiveTableScan(table=[[mydb_e10, d4_tab_e10]], table:alias=[r])\n         HiveProject(c1=[$0], c2=[$1], c4=[$3], c5=[$4])\n-          HiveFilter(condition=[AND(OR(IS NULL($4), >($4, 2020-10-01)), IS NOT NULL($1), IS NOT NULL($0))])", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjAzNzY3Nw=="}, "originalCommit": {"oid": "b3a3b512f7d97d0fc8d7e0e57f4a23a1d2b05000"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 355, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}