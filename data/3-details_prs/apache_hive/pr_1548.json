{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk2OTE3MzI3", "number": 1548, "title": "HIVE-21052: Make sure transactions get cleaned if they are aborted be\u2026", "bodyText": "What changes were proposed in this pull request?\nBelow changes are only with respect to master.\nDesign: taken from https://issues.apache.org/jira/secure/attachment/12954375/Aborted%20Txn%20w_Direct%20Write.pdf\nOverview:\n\nadd a dummy row to TXN_COMPONENTS with operation type 'p' in enqueueLockWithRetry, which will be removed in addDynamicPartition\nIf anytime txn is aborted, this dummy entry will be block initiator to remove this txnId from TXNS\nInitiator will add a row in COMPACTION_QUEUE (with type 'p') for the above aborted txn with the state as READY_FOR_CLEANING, at a time there will be a single entry of this type for a table in COMPACTION_QUEUE.\nCleaner will directly pickup above request, and process it via new cleanAborted code path(scan all partitions and remove aborted dirs), once successful cleaner will remove dummy row from TXN_COMPONENTS\n\nCleaner Design:\n\nWe are keeping cleaner single thread, and this new type of cleanup will be handled similar to any regular cleanup\n\nAborted dirs cleanup:\n\nIn p-type cleanup, cleaner will iterate over all the partitions and remove all delta/base dirs with given aborted writeId list\nadded cleanup of aborted base/delta in the worker also\n\nTXN_COMPONENTS cleanup:\n\nIf successful, p-type entry will be removed from TXN_COMPONENTS during addDynamicPartitions\nIf aborted, cleaner will clean in markCleaned after successful processing of p-type cleanup\n\nTXNS cleanup:\n\nNo change, will be cleaned up by the initiator\n\n\nWhy are the changes needed?\nTo fix above mentioned issue.\n\nDoes this PR introduce any user-facing change?\nNo\n\nHow was this patch tested?\nunit-tests added", "createdAt": "2020-10-02T13:44:47Z", "url": "https://github.com/apache/hive/pull/1548", "merged": true, "mergeCommit": {"oid": "b7f39651c5cd74860b764490834ebfbcdf31978b"}, "closed": true, "closedAt": "2020-10-21T07:51:14Z", "author": {"login": "deniskuzZ"}, "timelineItems": {"totalCount": 50, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdO4ZUlgFqTUwMTUzNzcwMQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdUoX9zAFqTUxMzM4NzA3MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNTM3NzAx", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-501537701", "createdAt": "2020-10-03T10:47:12Z", "commit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxMDo0NzoxMlrOHcA7sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wM1QxMDo1MToxOFrOHcA8sA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEzNzQ1OQ==", "bodyText": "Here we need some correction in expected values, as these tests were written wrt to branch-3.1 in which we don't support minor compaction for MM tables.", "url": "https://github.com/apache/hive/pull/1548#discussion_r499137459", "createdAt": "2020-10-03T10:47:12Z", "author": {"login": "vpnvishv"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2128,6 +2128,395 @@ public void testCleanerForTxnToWriteId() throws Exception {\n             0, TxnDbUtil.countQueryAgent(hiveConf, \"select count(*) from TXN_TO_WRITE_ID\"));\n   }\n \n+  @Test\n+    public void testMmTableAbortWithCompaction() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 4. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData2);\n+    verifyBaseDirAndResult(0, Table.MMTBL.toString(), \"\", resultData2);\n+    // 5. Run Cleaner. Shouldn't impact anything.\n+    runCleaner(hiveConf);\n+    // 6. Run initiator remove aborted entry from TXNS table\n+    runInitiator(hiveConf);\n+\n+    // Verify query result\n+    List<String> rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData2), rs);\n+\n+    int [][] resultData3 = new int[][] {{1,2}, {5,6}, {7,8}};\n+    // 7. add few more rows\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(7,8)\");\n+    // 8. add one more aborted delta\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(9,10)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+\n+    // 9. Perform a MAJOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    verifyDeltaDirAndResult(4, Table.MMTBL.toString(), \"\", resultData3);\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runCleaner(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runInitiator(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // Verify query result\n+    rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData3), rs);\n+  }\n+  @Test\n+  public void testMmTableAbortWithCompactionNoCleanup() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}, {5,6}};\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \" values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData1);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 3. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEzNzYyNg==", "bodyText": "Here expected value looks correct to me and this looks like a genuine issue. Is this due to some incorrect stats, any idea?", "url": "https://github.com/apache/hive/pull/1548#discussion_r499137626", "createdAt": "2020-10-03T10:49:59Z", "author": {"login": "vpnvishv"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2128,6 +2128,395 @@ public void testCleanerForTxnToWriteId() throws Exception {\n             0, TxnDbUtil.countQueryAgent(hiveConf, \"select count(*) from TXN_TO_WRITE_ID\"));\n   }\n \n+  @Test\n+    public void testMmTableAbortWithCompaction() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTEzNzcxMg==", "bodyText": "Again, here expected value looks correct to me and this looks like a genuine issue. Is this due to some incorrect stats, any idea?", "url": "https://github.com/apache/hive/pull/1548#discussion_r499137712", "createdAt": "2020-10-03T10:51:18Z", "author": {"login": "vpnvishv"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2128,6 +2128,395 @@ public void testCleanerForTxnToWriteId() throws Exception {\n             0, TxnDbUtil.countQueryAgent(hiveConf, \"select count(*) from TXN_TO_WRITE_ID\"));\n   }\n \n+  @Test\n+    public void testMmTableAbortWithCompaction() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 4. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData2);\n+    verifyBaseDirAndResult(0, Table.MMTBL.toString(), \"\", resultData2);\n+    // 5. Run Cleaner. Shouldn't impact anything.\n+    runCleaner(hiveConf);\n+    // 6. Run initiator remove aborted entry from TXNS table\n+    runInitiator(hiveConf);\n+\n+    // Verify query result\n+    List<String> rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData2), rs);\n+\n+    int [][] resultData3 = new int[][] {{1,2}, {5,6}, {7,8}};\n+    // 7. add few more rows\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(7,8)\");\n+    // 8. add one more aborted delta\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(9,10)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+\n+    // 9. Perform a MAJOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    verifyDeltaDirAndResult(4, Table.MMTBL.toString(), \"\", resultData3);\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runCleaner(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+    runInitiator(hiveConf);\n+    verifyDeltaDirAndResult(0, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // Verify query result\n+    rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData3), rs);\n+  }\n+  @Test\n+  public void testMmTableAbortWithCompactionNoCleanup() throws Exception {\n+    // 1. Insert some rows into MM table\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(1,2)\");\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(5,6)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}, {5,6}};\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \" values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 1 delta and 1 base directory. The base one is the aborted one.\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData1);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.MMTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));\n+\n+    // 3. Perform a MINOR compaction, expectation is it should remove aborted base dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MINOR'\");\n+    // The worker should remove the subdir for aborted transaction\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(2, Table.MMTBL.toString(), \"\", resultData1);\n+    verifyBaseDirAndResult(0, Table.MMTBL.toString(), \"\", resultData1);\n+    // Verify query result\n+    List<String> rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData1), rs);\n+\n+    int [][] resultData3 = new int[][] {{1,2}, {5,6}, {7,8}};\n+    // 4. add few more rows\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(7,8)\");\n+    // 5. add one more aborted delta\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.MMTBL + \"(a,b) values(9,10)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    verifyDeltaDirAndResult(4, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // 6. Perform a MAJOR compaction, expectation is it should remove aborted delta dir\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    // 7. Run one more Major compaction this should not have any affect\n+    runStatementOnDriver(\"alter table \"+ Table.MMTBL + \" compact 'MAJOR'\");\n+    runWorker(hiveConf);\n+    verifyDeltaDirAndResult(3, Table.MMTBL.toString(), \"\", resultData3);\n+    verifyBaseDirAndResult(1, Table.MMTBL.toString(), \"\", resultData3);\n+\n+    runCleaner(hiveConf);\n+\n+    // Verify query result\n+    rs = runStatementOnDriver(\"select a,b from \" + Table.MMTBL + \" order by a\");\n+    Assert.assertEquals(stringifyValues(resultData3), rs);\n+  }\n+\n+  @Test\n+  public void testFullACIDAbortWithMinorMajorCompaction() throws Exception {\n+    // 1. Insert some rows into acid table\n+    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \"(a,b) values(1,2)\");\n+    // There should be 1 delta directory\n+    int [][] resultData1 =  new int[][] {{1,2}};\n+    verifyDeltaDirAndResult(1, Table.ACIDTBL.toString(), \"\", resultData1);\n+    List<String> r1 = runStatementOnDriver(\"select count(*) from \" + Table.ACIDTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // 2. Let a transaction be aborted\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, true);\n+    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \"(a,b) values(3,4)\");\n+    hiveConf.setBoolVar(HiveConf.ConfVars.HIVETESTMODEROLLBACKTXN, false);\n+    // There should be 2 delta directories.\n+    verifyDeltaDirAndResult(2, Table.ACIDTBL.toString(), \"\", resultData1);\n+\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.ACIDTBL);\n+    Assert.assertEquals(\"1\", r1.get(0));\n+\n+    // Verify query result\n+    int [][] resultData2 = new int[][] {{1,2}, {5,6}};\n+    // 3. insert few more rows in acid table\n+    runStatementOnDriver(\"insert into \" + Table.ACIDTBL + \"(a,b) values(5,6)\");\n+    verifyDeltaDirAndResult(3, Table.ACIDTBL.toString(), \"\", resultData2);\n+    r1 = runStatementOnDriver(\"select count(*) from \" + Table.ACIDTBL);\n+    Assert.assertEquals(\"2\", r1.get(0));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 156}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNzc3NTc4", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-501777578", "createdAt": "2020-10-05T07:21:29Z", "commit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzoyMTozMFrOHcQPxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzoyMTozMFrOHcQPxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM4ODM1Ng==", "bodyText": "This is going issue many filesystem listing on a table with many partitions, that is going to be very slow on S3. I think you should consider changing this logic to be similar to getHdfsDirSnapshots that would do 1 recursive listing, iterate all the files and collect the deltas that needs to be deleted and delete them at the end (possible concurrently)", "url": "https://github.com/apache/hive/pull/1548#discussion_r499388356", "createdAt": "2020-10-05T07:21:30Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n+          return true;\n+        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    };\n+    List<FileStatus> deleted = new ArrayList<>();\n+    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n+    return deleted;\n+  }\n+\n+  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 84}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNzkzMzE4", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-501793318", "createdAt": "2020-10-05T07:44:45Z", "commit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzo0NDo0NVrOHcQ8HQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzo0NDo0NVrOHcQ8HQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTM5OTcwOQ==", "bodyText": "Why the contains \"=\", are we checking for a partition where the user named the column exactly like a valid delta dir? I don't think we should support that", "url": "https://github.com/apache/hive/pull/1548#discussion_r499399709", "createdAt": "2020-10-05T07:44:45Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNzk3NDcw", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-501797470", "createdAt": "2020-10-05T07:50:09Z", "commit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzo1MDoxMFrOHcRISg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzo1MDoxMFrOHcRISg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwMjgyNg==", "bodyText": "I am wondering are we covering all the use cases here? Is it possible that this dynamic part query was writing to an existing partition with existing older writes and a compaction was running before we managed to delete the aborted delta? I think in this case sadly, we still going to read the aborted data as valid. Could you add a test case to check if it is indeed a problem or not? (I do not have an idea for a solution...)", "url": "https://github.com/apache/hive/pull/1548#discussion_r499402826", "createdAt": "2020-10-05T07:50:10Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxNzk5NjMw", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-501799630", "createdAt": "2020-10-05T07:52:58Z", "commit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzo1Mjo1OFrOHcROsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzo1Mjo1OFrOHcROsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwNDQ2Ng==", "bodyText": "Are we doing this to delete newly created partitions if there are no other writes? Is this ok, what if we found a valid empty partition that is registered in the HMS? We should not delete that. I think this can be skipped all together, the empty partition dir will not bother anybody", "url": "https://github.com/apache/hive/pull/1548#discussion_r499404466", "createdAt": "2020-10-05T07:52:58Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n+          return true;\n+        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    };\n+    List<FileStatus> deleted = new ArrayList<>();\n+    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n+    return deleted;\n+  }\n+\n+  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)\n+      throws IOException {\n+    RemoteIterator<FileStatus> it = listIterator(fs, root, null);\n+\n+    while (it.hasNext()) {\n+      FileStatus fStatus = it.next();\n+      if (fStatus.isDirectory()) {\n+        if (filter.accept(fStatus.getPath())) {\n+          fs.delete(fStatus.getPath(), true);\n+          deleted.add(fStatus);\n+        } else {\n+          deleteDeltaDirectoriesAux(fStatus.getPath(), fs, filter, deleted);\n+          if (isDirectoryEmpty(fs, fStatus.getPath())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 96}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxODAxMjYw", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-501801260", "createdAt": "2020-10-05T07:55:10Z", "commit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzo1NToxMFrOHcRToA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwNzo1NToxMFrOHcRToA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQwNTcyOA==", "bodyText": "This should be similar to tryListLocatedHdfsStatus don't catch all Throwable. And maybe add all this to the HdfsUtils class", "url": "https://github.com/apache/hive/pull/1548#discussion_r499405728", "createdAt": "2020-10-05T07:55:10Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2839,6 +2848,87 @@ public static void setNonTransactional(Map<String, String> tblProps) {\n     tblProps.remove(hive_metastoreConstants.TABLE_TRANSACTIONAL_PROPERTIES);\n   }\n \n+  /**\n+   * Look for delta directories matching the list of writeIds and deletes them.\n+   * @param rootPartition root partition to look for the delta directories\n+   * @param conf configuration\n+   * @param writeIds list of writeIds to look for in the delta directories\n+   * @return list of deleted directories.\n+   * @throws IOException\n+   */\n+  public static List<FileStatus> deleteDeltaDirectories(Path rootPartition, Configuration conf, Set<Long> writeIds)\n+      throws IOException {\n+    FileSystem fs = rootPartition.getFileSystem(conf);\n+\n+    PathFilter filter = (p) -> {\n+      String name = p.getName();\n+      for (Long wId : writeIds) {\n+        if (name.startsWith(deltaSubdir(wId, wId)) && !name.contains(\"=\")) {\n+          return true;\n+        } else if (name.startsWith(baseDir(wId)) && !name.contains(\"=\")) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    };\n+    List<FileStatus> deleted = new ArrayList<>();\n+    deleteDeltaDirectoriesAux(rootPartition, fs, filter, deleted);\n+    return deleted;\n+  }\n+\n+  private static void deleteDeltaDirectoriesAux(Path root, FileSystem fs, PathFilter filter, List<FileStatus> deleted)\n+      throws IOException {\n+    RemoteIterator<FileStatus> it = listIterator(fs, root, null);\n+\n+    while (it.hasNext()) {\n+      FileStatus fStatus = it.next();\n+      if (fStatus.isDirectory()) {\n+        if (filter.accept(fStatus.getPath())) {\n+          fs.delete(fStatus.getPath(), true);\n+          deleted.add(fStatus);\n+        } else {\n+          deleteDeltaDirectoriesAux(fStatus.getPath(), fs, filter, deleted);\n+          if (isDirectoryEmpty(fs, fStatus.getPath())) {\n+            fs.delete(fStatus.getPath(), false);\n+            deleted.add(fStatus);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private static boolean isDirectoryEmpty(FileSystem fs, Path path) throws IOException {\n+    RemoteIterator<FileStatus> it = listIterator(fs, path, null);\n+    return !it.hasNext();\n+  }\n+\n+  private static RemoteIterator<FileStatus> listIterator(FileSystem fs, Path path, PathFilter filter)\n+      throws IOException {\n+    try {\n+      return new ToFileStatusIterator(SHIMS.listLocatedHdfsStatusIterator(fs, path, filter));\n+    } catch (Throwable t) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 114}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxODU4MTE2", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-501858116", "createdAt": "2020-10-05T09:07:54Z", "commit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwOTowNzo1NFrOHcT_JA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwOTowNzo1NFrOHcT_JA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ0OTYzNg==", "bodyText": "Two questions here:\n\nIn the original Jira there was discussion about not allowing concurrent cleanings of the same stuff (partition / table). Should we worry about this?\nThe slow cleanAborted will clog the executor service, we should do something about this, either in this patch, or follow up something like https://issues.apache.org/jira/browse/HIVE-21150 immediately after this.", "url": "https://github.com/apache/hive/pull/1548#discussion_r499449636", "createdAt": "2020-10-05T09:07:54Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -97,9 +100,9 @@ public void run() {\n           long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n           LOG.info(\"Cleaning based on min open txn id: \" + minOpenTxnId);\n           List<CompletableFuture> cleanerList = new ArrayList<>();\n-          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+          for (CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n             cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n-                    clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxODY4MzI2", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-501868326", "createdAt": "2020-10-05T09:20:21Z", "commit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwOToyMDoyMlrOHcUd1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwOToyMDoyMlrOHcUd1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ1NzQ5NA==", "bodyText": "@vpnvishv Why do we do this here? I understand we can, but why don't we let the Cleaner to delete the files? This just makes the compactor slower. Do we have a functionality reason for this?\nAfter this change it will run in CompactorMR and in MMQueryCompactors, but not in normal QueryCompactors?", "url": "https://github.com/apache/hive/pull/1548#discussion_r499457494", "createdAt": "2020-10-05T09:20:22Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java", "diffHunk": "@@ -237,6 +237,7 @@ void run(HiveConf conf, String jobName, Table t, Partition p, StorageDescriptor\n     }\n \n     JobConf job = createBaseJobConf(conf, jobName, t, sd, writeIds, ci);\n+    QueryCompactor.Util.removeAbortedDirsForAcidTable(conf, dir);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxODkyMTE2", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-501892116", "createdAt": "2020-10-05T09:49:37Z", "commit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwOTo0OTozN1rOHcVlVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQwOTo0OTozN1rOHcVlVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTQ3NTc5Ng==", "bodyText": "I might be mistaken here, but does this mean, that if we have many \"normal\" aborted txn and 1 aborted dynpart txn, we will not initiate a normal compaction until the dynpart stuff is not cleaned up? Is this ok, shouldn't we doing both?", "url": "https://github.com/apache/hive/pull/1548#discussion_r499475796", "createdAt": "2020-10-05T09:49:37Z", "author": {"login": "pvargacl"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -107,11 +107,12 @@ public CompactionTxnHandler() {\n         // Check for aborted txns: number of aborted txns past threshold and age of aborted txns\n         // past time threshold\n         boolean checkAbortedTimeThreshold = abortedTimeThreshold >= 0;\n-        final String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\",\"\n-            + \"MIN(\\\"TXN_STARTED\\\"), COUNT(*)\"\n+        String sCheckAborted = \"SELECT \\\"TC_DATABASE\\\", \\\"TC_TABLE\\\", \\\"TC_PARTITION\\\", \"\n+            + \"MIN(\\\"TXN_STARTED\\\"), COUNT(*), \"\n+            + \"MAX(CASE WHEN \\\"TC_OPERATION_TYPE\\\" = \" + OperationType.DYNPART + \" THEN 1 ELSE 0 END) AS \\\"IS_DP\\\" \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8fe2b5fb8eda000403f43180e5706e3212b87e13"}, "originalPosition": 8}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d34e97481fe0003a55d51fb72306d788b435f021", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/d34e97481fe0003a55d51fb72306d788b435f021", "committedDate": "2020-10-06T17:57:44Z", "message": "added test for dynamic partition update"}, "afterCommit": {"oid": "bb7cca22d4dd91c5df48e217749a78e6e334fd17", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/bb7cca22d4dd91c5df48e217749a78e6e334fd17", "committedDate": "2020-10-06T18:09:35Z", "message": "added test for dynamic partition update"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bb7cca22d4dd91c5df48e217749a78e6e334fd17", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/bb7cca22d4dd91c5df48e217749a78e6e334fd17", "committedDate": "2020-10-06T18:09:35Z", "message": "added test for dynamic partition update"}, "afterCommit": {"oid": "eb221eb3cce4deed190438903dc73dee55e5fe30", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/eb221eb3cce4deed190438903dc73dee55e5fe30", "committedDate": "2020-10-06T21:41:18Z", "message": "added test for dynamic partition update"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "eb221eb3cce4deed190438903dc73dee55e5fe30", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/eb221eb3cce4deed190438903dc73dee55e5fe30", "committedDate": "2020-10-06T21:41:18Z", "message": "added test for dynamic partition update"}, "afterCommit": {"oid": "ee285982ccc8decdb3ebbbd8bad9ac2604235143", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/ee285982ccc8decdb3ebbbd8bad9ac2604235143", "committedDate": "2020-10-07T07:52:05Z", "message": "compilation failure due to wildcard import fix"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "95f322a3545990a445141d45e2df7dff5c6716cd", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/95f322a3545990a445141d45e2df7dff5c6716cd", "committedDate": "2020-10-07T20:25:58Z", "message": "refactored txn_components cleanup"}, "afterCommit": {"oid": "0731bdf6c7dab11d103e04e915e1d8dcacf7a56e", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/0731bdf6c7dab11d103e04e915e1d8dcacf7a56e", "committedDate": "2020-10-07T20:40:02Z", "message": "refactored txn_components cleanup"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0731bdf6c7dab11d103e04e915e1d8dcacf7a56e", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/0731bdf6c7dab11d103e04e915e1d8dcacf7a56e", "committedDate": "2020-10-07T20:40:02Z", "message": "refactored txn_components cleanup"}, "afterCommit": {"oid": "7307bd104f19233dee04c3196caf731c43b788f0", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/7307bd104f19233dee04c3196caf731c43b788f0", "committedDate": "2020-10-07T20:48:03Z", "message": "refactored txn_components cleanup"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7307bd104f19233dee04c3196caf731c43b788f0", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/7307bd104f19233dee04c3196caf731c43b788f0", "committedDate": "2020-10-07T20:48:03Z", "message": "refactored txn_components cleanup"}, "afterCommit": {"oid": "72c80a49b896b1fbda719188e9fbaf55b0e2887d", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/72c80a49b896b1fbda719188e9fbaf55b0e2887d", "committedDate": "2020-10-07T21:40:10Z", "message": "refactored txn_components cleanup"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0NzcyODQx", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-504772841", "createdAt": "2020-10-08T13:19:13Z", "commit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzoxOToxM1rOHeeKNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzoxOToxM1rOHeeKNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcxMzQ2Mw==", "bodyText": "I think this assert is quit misleading. I might be wrong but the recursive listing skips empty directories, and actually this new version of cleaning will keep the partition directories (it should) and only delete the delta dirs and files.", "url": "https://github.com/apache/hive/pull/1548#discussion_r501713463", "createdAt": "2020-10-08T13:19:13Z", "author": {"login": "pvargacl"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "diffHunk": "@@ -853,6 +857,273 @@ public void majorCompactAfterAbort() throws Exception {\n             Lists.newArrayList(5, 6), 1);\n   }\n \n+  @Test\n+  public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactAfterAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    // Create three folders with two different transactions\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  private void assertAndCompactCleanAbort(String dbName, String tblName) throws Exception {\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    TxnStore txnHandler = TxnUtils.getTxnStore(conf);\n+    Table table = msClient.getTable(dbName, tblName);\n+    FileSystem fs = FileSystem.get(conf);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table.getSd().getLocation()));\n+    if (3 != stat.length) {\n+      Assert.fail(\"Expecting three directories corresponding to three partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 1, count);\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.CLEANING_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "originalPosition": 127}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0Nzc1MjU3", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-504775257", "createdAt": "2020-10-08T13:21:39Z", "commit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzoyMTo0MFrOHeeRbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzoyMTo0MFrOHeeRbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcxNTMxMQ==", "bodyText": "this comment is copied I guess", "url": "https://github.com/apache/hive/pull/1548#discussion_r501715311", "createdAt": "2020-10-08T13:21:40Z", "author": {"login": "pvargacl"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "diffHunk": "@@ -853,6 +857,273 @@ public void majorCompactAfterAbort() throws Exception {\n             Lists.newArrayList(5, 6), 1);\n   }\n \n+  @Test\n+  public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactAfterAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    // Create three folders with two different transactions\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  private void assertAndCompactCleanAbort(String dbName, String tblName) throws Exception {\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    TxnStore txnHandler = TxnUtils.getTxnStore(conf);\n+    Table table = msClient.getTable(dbName, tblName);\n+    FileSystem fs = FileSystem.get(conf);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table.getSd().getLocation()));\n+    if (3 != stat.length) {\n+      Assert.fail(\"Expecting three directories corresponding to three partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 1, count);\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.CLEANING_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.SUCCEEDED_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactSeveralTables() throws Exception {\n+    String dbName = \"default\";\n+    String tblName1 = \"cws1\";\n+    String tblName2 = \"cws2\";\n+\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName1, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName2, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,1\".getBytes());\n+    connection2.write(\"2,2\".getBytes());\n+    connection2.abortTransaction();\n+\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    FileSystem fs = FileSystem.get(conf);\n+    Table table1 = msClient.getTable(dbName, tblName1);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table1.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+    Table table2 = msClient.getTable(dbName, tblName2);\n+    stat = fs.listStatus(new Path(table2.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 2, count);\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table1.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCorrectlyCleaned() throws Exception {\n+    // Test that at commit the tables are cleaned properly\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+    HiveStreamingConnection connection = prepareTableAndConnection(dbName, tblName, 1);\n+    connection.beginTransaction();\n+    connection.write(\"1,1\".getBytes());\n+    connection.write(\"2,2\".getBytes());\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "originalPosition": 211}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0Nzc5MDQw", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-504779040", "createdAt": "2020-10-08T13:25:35Z", "commit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzoyNTozNVrOHeec6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzoyNTozNVrOHeec6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcxODI0OA==", "bodyText": "Could you add two more test with batch size 2. first batch writes to p1 and p2 and commits, second batch writes to p2 and p3 and aborts. And one with the aborted / committed order changed.", "url": "https://github.com/apache/hive/pull/1548#discussion_r501718248", "createdAt": "2020-10-08T13:25:35Z", "author": {"login": "pvargacl"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "diffHunk": "@@ -853,6 +857,273 @@ public void majorCompactAfterAbort() throws Exception {\n             Lists.newArrayList(5, 6), 1);\n   }\n \n+  @Test\n+  public void testCleanAbortCompactAfterAbortTwoPartitions() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection1 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableTwoPartitionsAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactAfterAbort() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    // Create three folders with two different transactions\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,3\".getBytes());\n+    connection2.write(\"2,3\".getBytes());\n+    connection2.write(\"3,3\".getBytes());\n+    connection2.abortTransaction();\n+\n+    assertAndCompactCleanAbort(dbName, tblName);\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  private void assertAndCompactCleanAbort(String dbName, String tblName) throws Exception {\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    TxnStore txnHandler = TxnUtils.getTxnStore(conf);\n+    Table table = msClient.getTable(dbName, tblName);\n+    FileSystem fs = FileSystem.get(conf);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table.getSd().getLocation()));\n+    if (3 != stat.length) {\n+      Assert.fail(\"Expecting three directories corresponding to three partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 1, count);\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.CLEANING_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(1, rsp.getCompacts().size());\n+    Assert.assertEquals(TxnStore.SUCCEEDED_RESPONSE, rsp.getCompacts().get(0).getState());\n+    Assert.assertEquals(\"cws\", rsp.getCompacts().get(0).getTablename());\n+    Assert.assertEquals(CompactionType.CLEAN_ABORTED,\n+        rsp.getCompacts().get(0).getType());\n+  }\n+\n+  @Test\n+  public void testCleanAbortCompactSeveralTables() throws Exception {\n+    String dbName = \"default\";\n+    String tblName1 = \"cws1\";\n+    String tblName2 = \"cws2\";\n+\n+    HiveStreamingConnection connection1 = prepareTableAndConnection(dbName, tblName1, 1);\n+    HiveStreamingConnection connection2 = prepareTableAndConnection(dbName, tblName2, 1);\n+\n+    connection1.beginTransaction();\n+    connection1.write(\"1,1\".getBytes());\n+    connection1.write(\"2,2\".getBytes());\n+    connection1.abortTransaction();\n+\n+    connection2.beginTransaction();\n+    connection2.write(\"1,1\".getBytes());\n+    connection2.write(\"2,2\".getBytes());\n+    connection2.abortTransaction();\n+\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(conf);\n+    FileSystem fs = FileSystem.get(conf);\n+    Table table1 = msClient.getTable(dbName, tblName1);\n+    FileStatus[] stat =\n+        fs.listStatus(new Path(table1.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+    Table table2 = msClient.getTable(dbName, tblName2);\n+    stat = fs.listStatus(new Path(table2.getSd().getLocation()));\n+    if (2 != stat.length) {\n+      Assert.fail(\"Expecting two directories corresponding to two partitions, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 2, count);\n+\n+    runInitiator(conf);\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE where CQ_TYPE='p'\");\n+    // Only one job is added to the queue per table. This job corresponds to all the entries for a particular table\n+    // with rows in TXN_COMPONENTS\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 2, count);\n+\n+    runCleaner(conf);\n+\n+    // After the cleaner runs TXN_COMPONENTS and COMPACTION_QUEUE should have zero rows, also the folders should have been deleted.\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    RemoteIterator it =\n+        fs.listFiles(new Path(table1.getSd().getLocation()), true);\n+    if (it.hasNext()) {\n+      Assert.fail(\"Expecting compaction to have cleaned the directories, FileStatus[] stat \" + Arrays.toString(stat));\n+    }\n+\n+    connection1.close();\n+    connection2.close();\n+  }\n+\n+  @Test\n+  public void testCleanAbortCorrectlyCleaned() throws Exception {\n+    // Test that at commit the tables are cleaned properly\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+    HiveStreamingConnection connection = prepareTableAndConnection(dbName, tblName, 1);\n+    connection.beginTransaction();\n+    connection.write(\"1,1\".getBytes());\n+    connection.write(\"2,2\".getBytes());\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 1, count);\n+\n+    connection.commitTransaction();\n+\n+    // After commit the row should have been deleted\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS where TC_OPERATION_TYPE='p'\");\n+    // We should have two rows corresponding to the two aborted transactions\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+  }\n+\n+  @Test\n+  public void testCleanAbortAndMinorCompact() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    HiveStreamingConnection connection = prepareTableAndConnection(dbName, tblName, 1);\n+\n+    connection.beginTransaction();\n+    connection.write(\"1,1\".getBytes());\n+    connection.write(\"2,2\".getBytes());\n+    connection.abortTransaction();\n+\n+    executeStatementOnDriver(\"insert into \" + tblName + \" partition (a) values (1, '1')\", driver);\n+    executeStatementOnDriver(\"delete from \" + tblName + \" where b=1\", driver);\n+\n+    conf.setIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_DELTA_NUM_THRESHOLD, 0);\n+    runInitiator(conf);\n+    runWorker(conf);\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 2, count);\n+    // Cleaning should happen in threads concurrently for the minor compaction and the clean abort one.\n+    runCleaner(conf);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from COMPACTION_QUEUE\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from COMPACTION_QUEUE\"), 0, count);\n+\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(TxnDbUtil.queryToString(conf, \"select * from TXN_COMPONENTS\"), 0, count);\n+\n+  }\n+\n+  private HiveStreamingConnection prepareTableAndConnection(String dbName, String tblName, int batchSize) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "originalPosition": 254}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0Nzg2Nzgz", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-504786783", "createdAt": "2020-10-08T13:33:29Z", "commit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzozMzozMFrOHeeyzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzozMzozMFrOHeeyzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcyMzg1Mw==", "bodyText": "Shouldn't you mark the compaction failed or cleaned?", "url": "https://github.com/apache/hive/pull/1548#discussion_r501723853", "createdAt": "2020-10-08T13:33:30Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -232,6 +240,51 @@ public Object run() throws Exception {\n   private static String idWatermark(CompactionInfo ci) {\n     return \" id=\" + ci.id;\n   }\n+\n+  private void cleanAborted(CompactionInfo ci) throws MetaException {\n+    if (ci.writeIds == null || ci.writeIds.size() == 0) {\n+      LOG.warn(\"Attempted cleaning aborted transaction with empty writeId list\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0ODA1NTk2", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-504805596", "createdAt": "2020-10-08T13:51:56Z", "commit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzo1MTo1NlrOHefpww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzo1MTo1NlrOHefpww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTczNzkyMw==", "bodyText": "cool", "url": "https://github.com/apache/hive/pull/1548#discussion_r501737923", "createdAt": "2020-10-08T13:51:56Z", "author": {"login": "pvargacl"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -414,77 +436,56 @@ public void markCleaned(CompactionInfo info) throws MetaException {\n          * aborted TXN_COMPONENTS above tc_writeid (and consequently about aborted txns).\n          * See {@link ql.txn.compactor.Cleaner.removeFiles()}\n          */\n-        s = \"SELECT DISTINCT \\\"TXN_ID\\\" FROM \\\"TXNS\\\", \\\"TXN_COMPONENTS\\\" WHERE \\\"TXN_ID\\\" = \\\"TC_TXNID\\\" \"\n-            + \"AND \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \" AND \\\"TC_DATABASE\\\" = ? AND \\\"TC_TABLE\\\" = ?\";\n-        if (info.highestWriteId != 0) s += \" AND \\\"TC_WRITEID\\\" <= ?\";\n-        if (info.partName != null) s += \" AND \\\"TC_PARTITION\\\" = ?\";\n+        List<String> queries = new ArrayList<>();\n+        Iterator<Long> writeIdsIter = null;\n+        List<Integer> counts = null;\n \n-        pStmt = dbConn.prepareStatement(s);\n-        paramCount = 1;\n-        pStmt.setString(paramCount++, info.dbname);\n-        pStmt.setString(paramCount++, info.tableName);\n-        if(info.highestWriteId != 0) {\n-          pStmt.setLong(paramCount++, info.highestWriteId);\n+        s = \"DELETE FROM \\\"TXN_COMPONENTS\\\" WHERE \\\"TC_TXNID\\\" IN (\" +\n+          \"   SELECT \\\"TXN_ID\\\" FROM \\\"TXNS\\\" WHERE \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \") \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db551f03b7fec385c2291f88a7032a4f64966384"}, "originalPosition": 197}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "70a5aa1ed9f35a51125b14b35c17c4e1eae82adf", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/70a5aa1ed9f35a51125b14b35c17c4e1eae82adf", "committedDate": "2020-10-08T14:16:33Z", "message": "added tests"}, "afterCommit": {"oid": "0d96a45094ca0ddfaf8e2334987255483efa92f4", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/0d96a45094ca0ddfaf8e2334987255483efa92f4", "committedDate": "2020-10-08T19:56:31Z", "message": "added tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6e71901f2c713d39afa8a18f392348b4afd4be7c", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/6e71901f2c713d39afa8a18f392348b4afd4be7c", "committedDate": "2020-10-12T13:23:20Z", "message": "HIVE-21052: Make sure transactions get cleaned if they are aborted before addPartitions is called"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e9f9cd714d47eb8e66da712073045d76c04a5efc", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/e9f9cd714d47eb8e66da712073045d76c04a5efc", "committedDate": "2020-10-12T13:23:27Z", "message": "fixed tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "75ce19fc7fe2b38e8ec6946da357fd85f7a2e2fb", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/75ce19fc7fe2b38e8ec6946da357fd85f7a2e2fb", "committedDate": "2020-10-12T13:23:27Z", "message": "addressing comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cce9f358d5ee56b6ad0fe2dda3bcdac500c65df5", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/cce9f358d5ee56b6ad0fe2dda3bcdac500c65df5", "committedDate": "2020-10-12T13:23:27Z", "message": "removed cleanup from compactor"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c821f4acf97fc46f70a50bc25cbe177238cb76b5", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/c821f4acf97fc46f70a50bc25cbe177238cb76b5", "committedDate": "2020-10-12T13:23:27Z", "message": "restored aborted base dir cleanup in case of IOW, added test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3e567a93ceee36f922634fe591b8182a2e65f4e2", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/3e567a93ceee36f922634fe591b8182a2e65f4e2", "committedDate": "2020-10-12T13:23:27Z", "message": "added test for dynamic partition update"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3599350ffb042222cf857482b0d3eaffb52f1d92", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/3599350ffb042222cf857482b0d3eaffb52f1d92", "committedDate": "2020-10-12T13:23:27Z", "message": "compilation failure due to wildcard import fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b7406a693e6963c8b3e25b69095ed9b57fa8b375", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/b7406a693e6963c8b3e25b69095ed9b57fa8b375", "committedDate": "2020-10-12T13:23:28Z", "message": "ported tests from HIVE-3.1 pull request"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fab03787c7d4b5d32b5984a10ccaf73def06385c", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/fab03787c7d4b5d32b5984a10ccaf73def06385c", "committedDate": "2020-10-12T13:23:28Z", "message": "findReadyToClean optimization"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7f662b0048bf073315c25c172b6423467263b59b", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/7f662b0048bf073315c25c172b6423467263b59b", "committedDate": "2020-10-12T13:23:28Z", "message": "refactored txn_components cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4be7152eb58563f230b9646da6c3daf3546f805b", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/4be7152eb58563f230b9646da6c3daf3546f805b", "committedDate": "2020-10-12T13:23:28Z", "message": "fixed whether should replicate check"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "647dfa179d8e08ebfaecd3dd67ee000b1bc335cd", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/647dfa179d8e08ebfaecd3dd67ee000b1bc335cd", "committedDate": "2020-10-12T13:23:28Z", "message": "addressing comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ed9f748d77427241ded73b4defe078e70b4fe4e6", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/ed9f748d77427241ded73b4defe078e70b4fe4e6", "committedDate": "2020-10-12T13:23:28Z", "message": "added tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fb1ec01be132a00b148b20c9615fb0d4815aecf4", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/fb1ec01be132a00b148b20c9615fb0d4815aecf4", "committedDate": "2020-10-12T13:23:28Z", "message": "master rebase fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "68e53871d79094984d1092073297120d5a53297b", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/68e53871d79094984d1092073297120d5a53297b", "committedDate": "2020-10-12T13:38:33Z", "message": "merge with snapshot check changes"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0d96a45094ca0ddfaf8e2334987255483efa92f4", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/0d96a45094ca0ddfaf8e2334987255483efa92f4", "committedDate": "2020-10-08T19:56:31Z", "message": "added tests"}, "afterCommit": {"oid": "68e53871d79094984d1092073297120d5a53297b", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/68e53871d79094984d1092073297120d5a53297b", "committedDate": "2020-10-12T13:38:33Z", "message": "merge with snapshot check changes"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "193337f0b214c96b38fd9f109c46f152bc491ee9", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/193337f0b214c96b38fd9f109c46f152bc491ee9", "committedDate": "2020-10-12T14:37:36Z", "message": "removed aborted cleanup for MM tables"}, "afterCommit": {"oid": "e6f1c12208e4bddbeaf1e9ff96b1129669bf2599", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/e6f1c12208e4bddbeaf1e9ff96b1129669bf2599", "committedDate": "2020-10-13T09:11:18Z", "message": "fixed test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3a62c2a2b0141de1eae6cd18d104e205718c35a5", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/3a62c2a2b0141de1eae6cd18d104e205718c35a5", "committedDate": "2020-10-13T15:21:32Z", "message": "fixed test"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e6f1c12208e4bddbeaf1e9ff96b1129669bf2599", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/e6f1c12208e4bddbeaf1e9ff96b1129669bf2599", "committedDate": "2020-10-13T09:11:18Z", "message": "fixed test"}, "afterCommit": {"oid": "3a62c2a2b0141de1eae6cd18d104e205718c35a5", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/3a62c2a2b0141de1eae6cd18d104e205718c35a5", "committedDate": "2020-10-13T15:21:32Z", "message": "fixed test"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "88e11837f8fed43fadc4e4efcdc98e3eaceaac0b", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/88e11837f8fed43fadc4e4efcdc98e3eaceaac0b", "committedDate": "2020-10-19T14:38:16Z", "message": "address Karen's comments"}, "afterCommit": {"oid": "fac26064dcc8f39748fb0d08fcab899174b8f3b8", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/fac26064dcc8f39748fb0d08fcab899174b8f3b8", "committedDate": "2020-10-19T14:42:07Z", "message": "address Karen's comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fac26064dcc8f39748fb0d08fcab899174b8f3b8", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/fac26064dcc8f39748fb0d08fcab899174b8f3b8", "committedDate": "2020-10-19T14:42:07Z", "message": "address Karen's comments"}, "afterCommit": {"oid": "29d8a2343b7939d55453c542fd093d6403f10479", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/29d8a2343b7939d55453c542fd093d6403f10479", "committedDate": "2020-10-19T14:44:57Z", "message": "address Karen's comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "29d8a2343b7939d55453c542fd093d6403f10479", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/29d8a2343b7939d55453c542fd093d6403f10479", "committedDate": "2020-10-19T14:44:57Z", "message": "address Karen's comments"}, "afterCommit": {"oid": "3cc9e3b9bb1582524b4bfc2ef265e1605682ed4d", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/3cc9e3b9bb1582524b4bfc2ef265e1605682ed4d", "committedDate": "2020-10-19T14:47:01Z", "message": "address Karen's comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf3db3ef429f9b90cc6370e0312c51b5a6bbd770", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/cf3db3ef429f9b90cc6370e0312c51b5a6bbd770", "committedDate": "2020-10-19T17:55:54Z", "message": "address Karen's comments, recheck"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3cc9e3b9bb1582524b4bfc2ef265e1605682ed4d", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/3cc9e3b9bb1582524b4bfc2ef265e1605682ed4d", "committedDate": "2020-10-19T14:47:01Z", "message": "address Karen's comments"}, "afterCommit": {"oid": "cf3db3ef429f9b90cc6370e0312c51b5a6bbd770", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/cf3db3ef429f9b90cc6370e0312c51b5a6bbd770", "committedDate": "2020-10-19T17:55:54Z", "message": "address Karen's comments, recheck"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c36860e8142bc117fafbb127797bf7de90697711", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/c36860e8142bc117fafbb127797bf7de90697711", "committedDate": "2020-10-20T07:57:37Z", "message": "add partition filter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3006aa6464e83336fec309965b86f7904fd1db3", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/f3006aa6464e83336fec309965b86f7904fd1db3", "committedDate": "2020-10-20T08:05:25Z", "message": "fixed txn_components cleanup query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47", "author": {"user": {"login": "deniskuzZ", "name": "Denys Kuzmenko"}}, "url": "https://github.com/apache/hive/commit/d9d805d318cc5fd4ad39dc9449c8d061bfc62c47", "committedDate": "2020-10-20T08:59:07Z", "message": "partition condition fix"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEyODg5MDA1", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-512889005", "createdAt": "2020-10-20T15:54:34Z", "commit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNTo1NDozNVrOHlFAmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNzoyNTowNFrOHlJGLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0MTQzMw==", "bodyText": "This can be consolidated with most of isDynPartIngest in CompactionUtils", "url": "https://github.com/apache/hive/pull/1548#discussion_r508641433", "createdAt": "2020-10-20T15:54:35Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java", "diffHunk": "@@ -589,4 +593,9 @@ private void checkInterrupt() throws InterruptedException {\n       throw new InterruptedException(\"Compaction execution is interrupted\");\n     }\n   }\n-}\n+\n+  private static boolean isDynPartAbort(Table t, CompactionInfo ci) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0MjE5OQ==", "bodyText": "FYI MM tests are usually in TestTxnCommandsForMmTable.java but I don't really care about this", "url": "https://github.com/apache/hive/pull/1548#discussion_r508642199", "createdAt": "2020-10-20T15:55:13Z", "author": {"login": "klcopp"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands2.java", "diffHunk": "@@ -2128,24 +2129,601 @@ public void testCleanerForTxnToWriteId() throws Exception {\n             0, TxnDbUtil.countQueryAgent(hiveConf, \"select count(*) from TXN_TO_WRITE_ID\"));\n   }\n \n-  private void verifyDirAndResult(int expectedDeltas) throws Exception {\n-    FileSystem fs = FileSystem.get(hiveConf);\n-    // Verify the content of subdirs\n-    FileStatus[] status = fs.listStatus(new Path(TEST_WAREHOUSE_DIR + \"/\" +\n-        (Table.MMTBL).toString().toLowerCase()), FileUtils.HIDDEN_FILES_PATH_FILTER);\n+  @Test\n+  public void testMmTableAbortWithCompaction() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0NDU3MA==", "bodyText": "Why was this changed?", "url": "https://github.com/apache/hive/pull/1548#discussion_r508644570", "createdAt": "2020-10-20T15:57:16Z", "author": {"login": "klcopp"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -400,11 +389,11 @@ public void markCleaned(CompactionInfo info) throws MetaException {\n           pStmt.setString(paramCount++, info.partName);\n         }\n         if(info.highestWriteId != 0) {\n-          pStmt.setLong(paramCount++, info.highestWriteId);\n+          pStmt.setLong(paramCount, info.highestWriteId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODY0NzU4Mw==", "bodyText": "Any ideas about why this was here? Just curious", "url": "https://github.com/apache/hive/pull/1548#discussion_r508647583", "createdAt": "2020-10-20T16:00:26Z", "author": {"login": "klcopp"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -134,9 +132,6 @@ public CompactionTxnHandler() {\n             response.add(info);\n           }\n         }\n-\n-        LOG.debug(\"Going to rollback\");\n-        dbConn.rollback();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODcwODM5OA==", "bodyText": "This is just refactoring right? LGTM but can you make sure @pvary sees this as well?", "url": "https://github.com/apache/hive/pull/1548#discussion_r508708398", "createdAt": "2020-10-20T17:25:04Z", "author": {"login": "klcopp"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java", "diffHunk": "@@ -414,76 +403,30 @@ public void markCleaned(CompactionInfo info) throws MetaException {\n          * aborted TXN_COMPONENTS above tc_writeid (and consequently about aborted txns).\n          * See {@link ql.txn.compactor.Cleaner.removeFiles()}\n          */\n-        s = \"SELECT DISTINCT \\\"TXN_ID\\\" FROM \\\"TXNS\\\", \\\"TXN_COMPONENTS\\\" WHERE \\\"TXN_ID\\\" = \\\"TC_TXNID\\\" \"\n-            + \"AND \\\"TXN_STATE\\\" = \" + TxnStatus.ABORTED + \" AND \\\"TC_DATABASE\\\" = ? AND \\\"TC_TABLE\\\" = ?\";\n-        if (info.highestWriteId != 0) s += \" AND \\\"TC_WRITEID\\\" <= ?\";\n-        if (info.partName != null) s += \" AND \\\"TC_PARTITION\\\" = ?\";\n-\n+        s = \"DELETE FROM \\\"TXN_COMPONENTS\\\" WHERE \\\"TC_TXNID\\\" IN (\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "originalPosition": 125}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEzMzg3MDcw", "url": "https://github.com/apache/hive/pull/1548#pullrequestreview-513387070", "createdAt": "2020-10-21T07:38:06Z", "commit": {"oid": "d9d805d318cc5fd4ad39dc9449c8d061bfc62c47"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3132, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}