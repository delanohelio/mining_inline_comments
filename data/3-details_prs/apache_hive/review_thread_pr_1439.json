{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc0Nzk4Nzkx", "number": 1439, "reviewThreads": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxODo0MDoxMlrOEd2f_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwNDozMDo1MVrOEhgFEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzM3MDg3OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/cost/HiveOnTezCostModel.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxODo0MDoxMlrOHJQzWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMzozMzo0NlrOHN_jsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3NDUyMA==", "bodyText": "Not sure about this change. If the algorithm is sort-based, you will still sort the complete input, right?", "url": "https://github.com/apache/hive/pull/1439#discussion_r479474520", "createdAt": "2020-08-28T18:40:12Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/cost/HiveOnTezCostModel.java", "diffHunk": "@@ -89,22 +89,23 @@ public RelOptCost getAggregateCost(HiveAggregate aggregate) {\n     } else {\n       final RelMetadataQuery mq = aggregate.getCluster().getMetadataQuery();\n       // 1. Sum of input cardinalities\n-      final Double rCount = mq.getRowCount(aggregate.getInput());\n-      if (rCount == null) {\n+      final Double inputRowCount = mq.getRowCount(aggregate.getInput());\n+      final Double rowCount = mq.getRowCount(aggregate);\n+      if (inputRowCount == null || rowCount == null) {\n         return null;\n       }\n       // 2. CPU cost = sorting cost\n-      final double cpuCost = algoUtils.computeSortCPUCost(rCount);\n+      final double cpuCost = algoUtils.computeSortCPUCost(rowCount) + inputRowCount * algoUtils.getCpuUnitCost();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjExOTA0Mw==", "bodyText": "maybe...I'm trying to catch the case when inputRowCount >> outputRowCount; we are also grouping - so it will not be a full sort at all ; I was using the above to achieve:\nlog(outputRowCount)*outputRowCount + inputRowCount*COST\n\nthe rational behind this is that it needs to really sort oRC and read iRC rows - this could be an underestimation...but log(iRC)*iRC was highly overestimating the cost\none alternative for the above could be to use:\nlog(outputRowCount) * inputRowCount\n\nthe rational behind this:\nwe will need to find the place for every input row; but we also know that the output will be at most outputRowCount - so it shouldn't take more time to find the place for the actual row than log(outputRowCount)", "url": "https://github.com/apache/hive/pull/1439#discussion_r482119043", "createdAt": "2020-09-02T14:35:41Z", "author": {"login": "kgyrtkirk"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/cost/HiveOnTezCostModel.java", "diffHunk": "@@ -89,22 +89,23 @@ public RelOptCost getAggregateCost(HiveAggregate aggregate) {\n     } else {\n       final RelMetadataQuery mq = aggregate.getCluster().getMetadataQuery();\n       // 1. Sum of input cardinalities\n-      final Double rCount = mq.getRowCount(aggregate.getInput());\n-      if (rCount == null) {\n+      final Double inputRowCount = mq.getRowCount(aggregate.getInput());\n+      final Double rowCount = mq.getRowCount(aggregate);\n+      if (inputRowCount == null || rowCount == null) {\n         return null;\n       }\n       // 2. CPU cost = sorting cost\n-      final double cpuCost = algoUtils.computeSortCPUCost(rCount);\n+      final double cpuCost = algoUtils.computeSortCPUCost(rowCount) + inputRowCount * algoUtils.getCpuUnitCost();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3NDUyMA=="}, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjMwMTQwOQ==", "bodyText": "I think the problem is that we are trying to encapsulate here the algorithm selection too: The fact that we are grouping in each node before sorting the data (I think this is also somehow reflected in the isLe discussion above). However, that is not represented with precision by current model, since output rows is supposed to be the output of the final step in the aggregation.\nWrt read, there is also the IO part of the cost, I am trying to understand whether some of the cost representation that you are talking about is IO.\nThere is some more info about the original formulas that were used to compute this here: https://cwiki.apache.org/confluence/display/Hive/Cost-based+optimization+in+Hive\nCan we split this into two patches and have the changes to the cost model on their own? This should also help to discuss this in more detail.", "url": "https://github.com/apache/hive/pull/1439#discussion_r482301409", "createdAt": "2020-09-02T18:49:09Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/cost/HiveOnTezCostModel.java", "diffHunk": "@@ -89,22 +89,23 @@ public RelOptCost getAggregateCost(HiveAggregate aggregate) {\n     } else {\n       final RelMetadataQuery mq = aggregate.getCluster().getMetadataQuery();\n       // 1. Sum of input cardinalities\n-      final Double rCount = mq.getRowCount(aggregate.getInput());\n-      if (rCount == null) {\n+      final Double inputRowCount = mq.getRowCount(aggregate.getInput());\n+      final Double rowCount = mq.getRowCount(aggregate);\n+      if (inputRowCount == null || rowCount == null) {\n         return null;\n       }\n       // 2. CPU cost = sorting cost\n-      final double cpuCost = algoUtils.computeSortCPUCost(rCount);\n+      final double cpuCost = algoUtils.computeSortCPUCost(rowCount) + inputRowCount * algoUtils.getCpuUnitCost();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3NDUyMA=="}, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQzNDg2NQ==", "bodyText": "sure; I'll open a separate ticket for the cost model changes", "url": "https://github.com/apache/hive/pull/1439#discussion_r484434865", "createdAt": "2020-09-07T13:33:46Z", "author": {"login": "kgyrtkirk"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/cost/HiveOnTezCostModel.java", "diffHunk": "@@ -89,22 +89,23 @@ public RelOptCost getAggregateCost(HiveAggregate aggregate) {\n     } else {\n       final RelMetadataQuery mq = aggregate.getCluster().getMetadataQuery();\n       // 1. Sum of input cardinalities\n-      final Double rCount = mq.getRowCount(aggregate.getInput());\n-      if (rCount == null) {\n+      final Double inputRowCount = mq.getRowCount(aggregate.getInput());\n+      final Double rowCount = mq.getRowCount(aggregate);\n+      if (inputRowCount == null || rowCount == null) {\n         return null;\n       }\n       // 2. CPU cost = sorting cost\n-      final double cpuCost = algoUtils.computeSortCPUCost(rCount);\n+      final double cpuCost = algoUtils.computeSortCPUCost(rowCount) + inputRowCount * algoUtils.getCpuUnitCost();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3NDUyMA=="}, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzM3MzgzOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxODo0MToxNFrOHJQ1Dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQxNToxNzowN1rOHOiKuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3NDk1OQ==", "bodyText": "We could rely on mq.areColumnsUnique.", "url": "https://github.com/apache/hive/pull/1439#discussion_r479474959", "createdAt": "2020-08-28T18:41:14Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -303,6 +305,90 @@ public void onMatch(RelOptRuleCall call) {\n     }\n   }\n \n+  /**\n+   * Determines weather the give grouping is unique.\n+   *\n+   * Consider a join which might produce non-unique rows; but later the results are aggregated again.\n+   * This method determines if there are sufficient columns in the grouping which have been present previously as unique column(s).\n+   */\n+  private boolean isGroupingUnique(RelNode input, ImmutableBitSet groups) {\n+    if (groups.isEmpty()) {\n+      return false;\n+    }\n+    RelMetadataQuery mq = input.getCluster().getMetadataQuery();\n+    Set<ImmutableBitSet> uKeys = mq.getUniqueKeys(input);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUxMzY1NA==", "bodyText": "If the purpose of this method is to determine that given a set of columns are unique or not you can use areColumnsUnique as @jcamachor  suggested.", "url": "https://github.com/apache/hive/pull/1439#discussion_r479513654", "createdAt": "2020-08-28T20:12:15Z", "author": {"login": "vineetgarg02"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -303,6 +305,90 @@ public void onMatch(RelOptRuleCall call) {\n     }\n   }\n \n+  /**\n+   * Determines weather the give grouping is unique.\n+   *\n+   * Consider a join which might produce non-unique rows; but later the results are aggregated again.\n+   * This method determines if there are sufficient columns in the grouping which have been present previously as unique column(s).\n+   */\n+  private boolean isGroupingUnique(RelNode input, ImmutableBitSet groups) {\n+    if (groups.isEmpty()) {\n+      return false;\n+    }\n+    RelMetadataQuery mq = input.getCluster().getMetadataQuery();\n+    Set<ImmutableBitSet> uKeys = mq.getUniqueKeys(input);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3NDk1OQ=="}, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTAwMTkxMg==", "bodyText": "yes; I've explored using areColumnsUnique because it matches the usecase here - however for some tests it emitted some NPEs so I've gone back to the getUniqueKeys approach\nI'll file a jira for areColumnsUnique when I know what's wrong with it..", "url": "https://github.com/apache/hive/pull/1439#discussion_r485001912", "createdAt": "2020-09-08T15:17:07Z", "author": {"login": "kgyrtkirk"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -303,6 +305,90 @@ public void onMatch(RelOptRuleCall call) {\n     }\n   }\n \n+  /**\n+   * Determines weather the give grouping is unique.\n+   *\n+   * Consider a join which might produce non-unique rows; but later the results are aggregated again.\n+   * This method determines if there are sufficient columns in the grouping which have been present previously as unique column(s).\n+   */\n+  private boolean isGroupingUnique(RelNode input, ImmutableBitSet groups) {\n+    if (groups.isEmpty()) {\n+      return false;\n+    }\n+    RelMetadataQuery mq = input.getCluster().getMetadataQuery();\n+    Set<ImmutableBitSet> uKeys = mq.getUniqueKeys(input);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3NDk1OQ=="}, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzM5MjMxOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxODo0NzoxMVrOHJQ_4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wN1QxMzo0MjowOVrOHN_0Iw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3NzczMQ==", "bodyText": "The rule is enabled via config so we will only reach here if it is enabled.\nWe should be able to force the transform even if cost-based variant is disabled.", "url": "https://github.com/apache/hive/pull/1439#discussion_r479477731", "createdAt": "2020-08-28T18:47:11Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -290,7 +291,8 @@ public void onMatch(RelOptRuleCall call) {\n       RelNode r = relBuilder.build();\n       RelOptCost afterCost = mq.getCumulativeCost(r);\n       RelOptCost beforeCost = mq.getCumulativeCost(aggregate);\n-      if (afterCost.isLt(beforeCost)) {\n+      boolean shouldForceTransform = isGroupingUnique(join, aggregate.getGroupSet());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQzOTA3NQ==", "bodyText": "I've added a config: hive.transpose.aggr.join.unique to enable/disable this feature", "url": "https://github.com/apache/hive/pull/1439#discussion_r484439075", "createdAt": "2020-09-07T13:42:09Z", "author": {"login": "kgyrtkirk"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -290,7 +291,8 @@ public void onMatch(RelOptRuleCall call) {\n       RelNode r = relBuilder.build();\n       RelOptCost afterCost = mq.getCumulativeCost(r);\n       RelOptCost beforeCost = mq.getCumulativeCost(aggregate);\n-      if (afterCost.isLt(beforeCost)) {\n+      boolean shouldForceTransform = isGroupingUnique(join, aggregate.getGroupSet());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3NzczMQ=="}, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzQwMDk0OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "isResolved": true, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxODo1MDozMFrOHJRFRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxNTo0MzowN1rOHQkGxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3OTExMA==", "bodyText": "This could call mq.areColumnsUnique instead of making the recursive call.", "url": "https://github.com/apache/hive/pull/1439#discussion_r479479110", "createdAt": "2020-08-28T18:50:30Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -303,6 +305,90 @@ public void onMatch(RelOptRuleCall call) {\n     }\n   }\n \n+  /**\n+   * Determines weather the give grouping is unique.\n+   *\n+   * Consider a join which might produce non-unique rows; but later the results are aggregated again.\n+   * This method determines if there are sufficient columns in the grouping which have been present previously as unique column(s).\n+   */\n+  private boolean isGroupingUnique(RelNode input, ImmutableBitSet groups) {\n+    if (groups.isEmpty()) {\n+      return false;\n+    }\n+    RelMetadataQuery mq = input.getCluster().getMetadataQuery();\n+    Set<ImmutableBitSet> uKeys = mq.getUniqueKeys(input);\n+    for (ImmutableBitSet u : uKeys) {\n+      if (groups.contains(u)) {\n+        return true;\n+      }\n+    }\n+    if (input instanceof Join) {\n+      Join join = (Join) input;\n+      RexBuilder rexBuilder = input.getCluster().getRexBuilder();\n+      SimpleConditionInfo cond = new SimpleConditionInfo(join.getCondition(), rexBuilder);\n+\n+      if (cond.valid) {\n+        ImmutableBitSet newGroup = groups.intersect(ImmutableBitSet.fromBitSet(cond.fields));\n+        RelNode l = join.getLeft();\n+        RelNode r = join.getRight();\n+\n+        int joinFieldCount = join.getRowType().getFieldCount();\n+        int lFieldCount = l.getRowType().getFieldCount();\n+\n+        ImmutableBitSet groupL = newGroup.get(0, lFieldCount);\n+        ImmutableBitSet groupR = newGroup.get(lFieldCount, joinFieldCount).shift(-lFieldCount);\n+\n+        if (isGroupingUnique(l, groupL)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA5MDI4OQ==", "bodyText": "this method does a bit different thing - honestly I feeled like I'm in trouble when I've given this name to it :)\nthis method checks if the given columns contain an unique column somewhere in the covered joins; (this still sound fuzzy) so let's take an example\nconsider:\nselect c_id, sum(i_prize) from customer c join item i on(i.c_id=c.c_id)\n\n\ndo an aggregate grouping by the column C_ID  ; and sum up something\nbelow is a join which joins by C_ID\nasking wether C_ID  is a unique column on top of the join is false; but there is subtree in which C_ID is unique => so if we push the aggregate on that branch the aggregation will be a no-op\n\nI think this case is not handled by areColumnsUnique", "url": "https://github.com/apache/hive/pull/1439#discussion_r482090289", "createdAt": "2020-09-02T13:58:09Z", "author": {"login": "kgyrtkirk"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -303,6 +305,90 @@ public void onMatch(RelOptRuleCall call) {\n     }\n   }\n \n+  /**\n+   * Determines weather the give grouping is unique.\n+   *\n+   * Consider a join which might produce non-unique rows; but later the results are aggregated again.\n+   * This method determines if there are sufficient columns in the grouping which have been present previously as unique column(s).\n+   */\n+  private boolean isGroupingUnique(RelNode input, ImmutableBitSet groups) {\n+    if (groups.isEmpty()) {\n+      return false;\n+    }\n+    RelMetadataQuery mq = input.getCluster().getMetadataQuery();\n+    Set<ImmutableBitSet> uKeys = mq.getUniqueKeys(input);\n+    for (ImmutableBitSet u : uKeys) {\n+      if (groups.contains(u)) {\n+        return true;\n+      }\n+    }\n+    if (input instanceof Join) {\n+      Join join = (Join) input;\n+      RexBuilder rexBuilder = input.getCluster().getRexBuilder();\n+      SimpleConditionInfo cond = new SimpleConditionInfo(join.getCondition(), rexBuilder);\n+\n+      if (cond.valid) {\n+        ImmutableBitSet newGroup = groups.intersect(ImmutableBitSet.fromBitSet(cond.fields));\n+        RelNode l = join.getLeft();\n+        RelNode r = join.getRight();\n+\n+        int joinFieldCount = join.getRowType().getFieldCount();\n+        int lFieldCount = l.getRowType().getFieldCount();\n+\n+        ImmutableBitSet groupL = newGroup.get(0, lFieldCount);\n+        ImmutableBitSet groupR = newGroup.get(lFieldCount, joinFieldCount).shift(-lFieldCount);\n+\n+        if (isGroupingUnique(l, groupL)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3OTExMA=="}, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI0MzM3Ng==", "bodyText": "Could you execute areColumnsUnique on the join input then? Wouldn't that simplify this logic?", "url": "https://github.com/apache/hive/pull/1439#discussion_r482243376", "createdAt": "2020-09-02T17:27:25Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -303,6 +305,90 @@ public void onMatch(RelOptRuleCall call) {\n     }\n   }\n \n+  /**\n+   * Determines weather the give grouping is unique.\n+   *\n+   * Consider a join which might produce non-unique rows; but later the results are aggregated again.\n+   * This method determines if there are sufficient columns in the grouping which have been present previously as unique column(s).\n+   */\n+  private boolean isGroupingUnique(RelNode input, ImmutableBitSet groups) {\n+    if (groups.isEmpty()) {\n+      return false;\n+    }\n+    RelMetadataQuery mq = input.getCluster().getMetadataQuery();\n+    Set<ImmutableBitSet> uKeys = mq.getUniqueKeys(input);\n+    for (ImmutableBitSet u : uKeys) {\n+      if (groups.contains(u)) {\n+        return true;\n+      }\n+    }\n+    if (input instanceof Join) {\n+      Join join = (Join) input;\n+      RexBuilder rexBuilder = input.getCluster().getRexBuilder();\n+      SimpleConditionInfo cond = new SimpleConditionInfo(join.getCondition(), rexBuilder);\n+\n+      if (cond.valid) {\n+        ImmutableBitSet newGroup = groups.intersect(ImmutableBitSet.fromBitSet(cond.fields));\n+        RelNode l = join.getLeft();\n+        RelNode r = join.getRight();\n+\n+        int joinFieldCount = join.getRowType().getFieldCount();\n+        int lFieldCount = l.getRowType().getFieldCount();\n+\n+        ImmutableBitSet groupL = newGroup.get(0, lFieldCount);\n+        ImmutableBitSet groupR = newGroup.get(lFieldCount, joinFieldCount).shift(-lFieldCount);\n+\n+        if (isGroupingUnique(l, groupL)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3OTExMA=="}, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NDQxOTc3Ng==", "bodyText": "this method recursively checks that the above condition is satisfied or not - that's why it needs to call itself", "url": "https://github.com/apache/hive/pull/1439#discussion_r484419776", "createdAt": "2020-09-07T13:03:06Z", "author": {"login": "kgyrtkirk"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -303,6 +305,90 @@ public void onMatch(RelOptRuleCall call) {\n     }\n   }\n \n+  /**\n+   * Determines weather the give grouping is unique.\n+   *\n+   * Consider a join which might produce non-unique rows; but later the results are aggregated again.\n+   * This method determines if there are sufficient columns in the grouping which have been present previously as unique column(s).\n+   */\n+  private boolean isGroupingUnique(RelNode input, ImmutableBitSet groups) {\n+    if (groups.isEmpty()) {\n+      return false;\n+    }\n+    RelMetadataQuery mq = input.getCluster().getMetadataQuery();\n+    Set<ImmutableBitSet> uKeys = mq.getUniqueKeys(input);\n+    for (ImmutableBitSet u : uKeys) {\n+      if (groups.contains(u)) {\n+        return true;\n+      }\n+    }\n+    if (input instanceof Join) {\n+      Join join = (Join) input;\n+      RexBuilder rexBuilder = input.getCluster().getRexBuilder();\n+      SimpleConditionInfo cond = new SimpleConditionInfo(join.getCondition(), rexBuilder);\n+\n+      if (cond.valid) {\n+        ImmutableBitSet newGroup = groups.intersect(ImmutableBitSet.fromBitSet(cond.fields));\n+        RelNode l = join.getLeft();\n+        RelNode r = join.getRight();\n+\n+        int joinFieldCount = join.getRowType().getFieldCount();\n+        int lFieldCount = l.getRowType().getFieldCount();\n+\n+        ImmutableBitSet groupL = newGroup.get(0, lFieldCount);\n+        ImmutableBitSet groupR = newGroup.get(lFieldCount, joinFieldCount).shift(-lFieldCount);\n+\n+        if (isGroupingUnique(l, groupL)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3OTExMA=="}, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzNTA2NQ==", "bodyText": "As you go down recursively, you may start finding HepRelVertex as the rel node. I think you hit the instanceof Project below only for the first project because you create it using the builder.\nThat is why I think once you have gone through the first join, you will not hit another join; you could simply call areColumnsUnique in these if clauses, which could potentially uncover new cases.\nI may be wrong though, I just wanted to leave you a final note to make sure it was clear what I meant.", "url": "https://github.com/apache/hive/pull/1439#discussion_r485335065", "createdAt": "2020-09-09T04:47:02Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -303,6 +305,90 @@ public void onMatch(RelOptRuleCall call) {\n     }\n   }\n \n+  /**\n+   * Determines weather the give grouping is unique.\n+   *\n+   * Consider a join which might produce non-unique rows; but later the results are aggregated again.\n+   * This method determines if there are sufficient columns in the grouping which have been present previously as unique column(s).\n+   */\n+  private boolean isGroupingUnique(RelNode input, ImmutableBitSet groups) {\n+    if (groups.isEmpty()) {\n+      return false;\n+    }\n+    RelMetadataQuery mq = input.getCluster().getMetadataQuery();\n+    Set<ImmutableBitSet> uKeys = mq.getUniqueKeys(input);\n+    for (ImmutableBitSet u : uKeys) {\n+      if (groups.contains(u)) {\n+        return true;\n+      }\n+    }\n+    if (input instanceof Join) {\n+      Join join = (Join) input;\n+      RexBuilder rexBuilder = input.getCluster().getRexBuilder();\n+      SimpleConditionInfo cond = new SimpleConditionInfo(join.getCondition(), rexBuilder);\n+\n+      if (cond.valid) {\n+        ImmutableBitSet newGroup = groups.intersect(ImmutableBitSet.fromBitSet(cond.fields));\n+        RelNode l = join.getLeft();\n+        RelNode r = join.getRight();\n+\n+        int joinFieldCount = join.getRowType().getFieldCount();\n+        int lFieldCount = l.getRowType().getFieldCount();\n+\n+        ImmutableBitSet groupL = newGroup.get(0, lFieldCount);\n+        ImmutableBitSet groupR = newGroup.get(lFieldCount, joinFieldCount).shift(-lFieldCount);\n+\n+        if (isGroupingUnique(l, groupL)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3OTExMA=="}, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzExNDYyMA==", "bodyText": "That could be done; and I'm sure it was true in this - but this logic will work better if it could walk down as many joins as it could - we might have an aggregate on top in the meantime a bunch of joins under it...so I feel that it will be beneficial to retain it.\nI feeled tempted to write a RelMd handler - however I don't think I could just introduce a new one easily.\nRelShuttle doesn't look like a good match - I'll leave it as a set of instanceof calls for now.\nI'll upload a new patch to see if digging deeper in the tree could do more or not.", "url": "https://github.com/apache/hive/pull/1439#discussion_r487114620", "createdAt": "2020-09-11T15:16:35Z", "author": {"login": "kgyrtkirk"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -303,6 +305,90 @@ public void onMatch(RelOptRuleCall call) {\n     }\n   }\n \n+  /**\n+   * Determines weather the give grouping is unique.\n+   *\n+   * Consider a join which might produce non-unique rows; but later the results are aggregated again.\n+   * This method determines if there are sufficient columns in the grouping which have been present previously as unique column(s).\n+   */\n+  private boolean isGroupingUnique(RelNode input, ImmutableBitSet groups) {\n+    if (groups.isEmpty()) {\n+      return false;\n+    }\n+    RelMetadataQuery mq = input.getCluster().getMetadataQuery();\n+    Set<ImmutableBitSet> uKeys = mq.getUniqueKeys(input);\n+    for (ImmutableBitSet u : uKeys) {\n+      if (groups.contains(u)) {\n+        return true;\n+      }\n+    }\n+    if (input instanceof Join) {\n+      Join join = (Join) input;\n+      RexBuilder rexBuilder = input.getCluster().getRexBuilder();\n+      SimpleConditionInfo cond = new SimpleConditionInfo(join.getCondition(), rexBuilder);\n+\n+      if (cond.valid) {\n+        ImmutableBitSet newGroup = groups.intersect(ImmutableBitSet.fromBitSet(cond.fields));\n+        RelNode l = join.getLeft();\n+        RelNode r = join.getRight();\n+\n+        int joinFieldCount = join.getRowType().getFieldCount();\n+        int lFieldCount = l.getRowType().getFieldCount();\n+\n+        ImmutableBitSet groupL = newGroup.get(0, lFieldCount);\n+        ImmutableBitSet groupR = newGroup.get(lFieldCount, joinFieldCount).shift(-lFieldCount);\n+\n+        if (isGroupingUnique(l, groupL)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3OTExMA=="}, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzEzMDgyMA==", "bodyText": "OK. If it turns out there are many changes and it may need some time to be fixed, feel free to defer to follow-up JIRA and let's merge this one.", "url": "https://github.com/apache/hive/pull/1439#discussion_r487130820", "createdAt": "2020-09-11T15:43:07Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -303,6 +305,90 @@ public void onMatch(RelOptRuleCall call) {\n     }\n   }\n \n+  /**\n+   * Determines weather the give grouping is unique.\n+   *\n+   * Consider a join which might produce non-unique rows; but later the results are aggregated again.\n+   * This method determines if there are sufficient columns in the grouping which have been present previously as unique column(s).\n+   */\n+  private boolean isGroupingUnique(RelNode input, ImmutableBitSet groups) {\n+    if (groups.isEmpty()) {\n+      return false;\n+    }\n+    RelMetadataQuery mq = input.getCluster().getMetadataQuery();\n+    Set<ImmutableBitSet> uKeys = mq.getUniqueKeys(input);\n+    for (ImmutableBitSet u : uKeys) {\n+      if (groups.contains(u)) {\n+        return true;\n+      }\n+    }\n+    if (input instanceof Join) {\n+      Join join = (Join) input;\n+      RexBuilder rexBuilder = input.getCluster().getRexBuilder();\n+      SimpleConditionInfo cond = new SimpleConditionInfo(join.getCondition(), rexBuilder);\n+\n+      if (cond.valid) {\n+        ImmutableBitSet newGroup = groups.intersect(ImmutableBitSet.fromBitSet(cond.fields));\n+        RelNode l = join.getLeft();\n+        RelNode r = join.getRight();\n+\n+        int joinFieldCount = join.getRowType().getFieldCount();\n+        int lFieldCount = l.getRowType().getFieldCount();\n+\n+        ImmutableBitSet groupL = newGroup.get(0, lFieldCount);\n+        ImmutableBitSet groupR = newGroup.get(lFieldCount, joinFieldCount).shift(-lFieldCount);\n+\n+        if (isGroupingUnique(l, groupL)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3OTExMA=="}, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzQwMTE1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxODo1MDozOFrOHJRFcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxODo1MDozOFrOHJRFcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ3OTE1Mw==", "bodyText": "Same as above.", "url": "https://github.com/apache/hive/pull/1439#discussion_r479479153", "createdAt": "2020-08-28T18:50:38Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -303,6 +305,90 @@ public void onMatch(RelOptRuleCall call) {\n     }\n   }\n \n+  /**\n+   * Determines weather the give grouping is unique.\n+   *\n+   * Consider a join which might produce non-unique rows; but later the results are aggregated again.\n+   * This method determines if there are sufficient columns in the grouping which have been present previously as unique column(s).\n+   */\n+  private boolean isGroupingUnique(RelNode input, ImmutableBitSet groups) {\n+    if (groups.isEmpty()) {\n+      return false;\n+    }\n+    RelMetadataQuery mq = input.getCluster().getMetadataQuery();\n+    Set<ImmutableBitSet> uKeys = mq.getUniqueKeys(input);\n+    for (ImmutableBitSet u : uKeys) {\n+      if (groups.contains(u)) {\n+        return true;\n+      }\n+    }\n+    if (input instanceof Join) {\n+      Join join = (Join) input;\n+      RexBuilder rexBuilder = input.getCluster().getRexBuilder();\n+      SimpleConditionInfo cond = new SimpleConditionInfo(join.getCondition(), rexBuilder);\n+\n+      if (cond.valid) {\n+        ImmutableBitSet newGroup = groups.intersect(ImmutableBitSet.fromBitSet(cond.fields));\n+        RelNode l = join.getLeft();\n+        RelNode r = join.getRight();\n+\n+        int joinFieldCount = join.getRowType().getFieldCount();\n+        int lFieldCount = l.getRowType().getFieldCount();\n+\n+        ImmutableBitSet groupL = newGroup.get(0, lFieldCount);\n+        ImmutableBitSet groupR = newGroup.get(lFieldCount, joinFieldCount).shift(-lFieldCount);\n+\n+        if (isGroupingUnique(l, groupL)) {\n+          return true;\n+        }\n+        if (isGroupingUnique(r, groupR)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzQzNjg3OnYy", "diffSide": "LEFT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxOTowNDoxNlrOHJRb9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNDowMDozOVrOHLwk3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ4NDkxOA==", "bodyText": "I think you suggested changing this... Maybe isLe if we do not introduce an additional aggregate on top?", "url": "https://github.com/apache/hive/pull/1439#discussion_r479484918", "createdAt": "2020-08-28T19:04:16Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -290,7 +291,8 @@ public void onMatch(RelOptRuleCall call) {\n       RelNode r = relBuilder.build();\n       RelOptCost afterCost = mq.getCumulativeCost(r);\n       RelOptCost beforeCost = mq.getCumulativeCost(aggregate);\n-      if (afterCost.isLt(beforeCost)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA5MjI1NQ==", "bodyText": "yes; if we use isLe the current cost model which only takes rowcount into account will prefer the pushing aggregates further\nthere is another alternative to the force based approach: the rule can be configured to use the more advanced cost system - so that it could take cpu/io cost into account", "url": "https://github.com/apache/hive/pull/1439#discussion_r482092255", "createdAt": "2020-09-02T14:00:39Z", "author": {"login": "kgyrtkirk"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -290,7 +291,8 @@ public void onMatch(RelOptRuleCall call) {\n       RelNode r = relBuilder.build();\n       RelOptCost afterCost = mq.getCumulativeCost(r);\n       RelOptCost beforeCost = mq.getCumulativeCost(aggregate);\n-      if (afterCost.isLt(beforeCost)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ4NDkxOA=="}, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzQ2MDk5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxOToxMjoyNVrOHJRqLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxOToxMjoyNVrOHJRqLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ4ODU1Ng==", "bodyText": "Additionally, when we force triggering the transform, does it make sense to verify that we are not creating an aggregate on top (i.e., we end up with agg before join and after join?\nThat may narrow it down even further to a case when the pushdown should always be beneficial.", "url": "https://github.com/apache/hive/pull/1439#discussion_r479488556", "createdAt": "2020-08-28T19:12:25Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -290,7 +291,8 @@ public void onMatch(RelOptRuleCall call) {\n       RelNode r = relBuilder.build();\n       RelOptCost afterCost = mq.getCumulativeCost(r);\n       RelOptCost beforeCost = mq.getCumulativeCost(aggregate);\n-      if (afterCost.isLt(beforeCost)) {\n+      boolean shouldForceTransform = isGroupingUnique(join, aggregate.getGroupSet());\n+      if (shouldForceTransform || afterCost.isLt(beforeCost)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzQ2MzU2OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxOToxMzoyMlrOHJRrvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxOToxMzoyMlrOHJRrvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQ4ODk1OQ==", "bodyText": "Can you use JoinInfo.of for this? It seems it does something very similar.", "url": "https://github.com/apache/hive/pull/1439#discussion_r479488959", "createdAt": "2020-08-28T19:13:22Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAggregateJoinTransposeRule.java", "diffHunk": "@@ -303,6 +305,90 @@ public void onMatch(RelOptRuleCall call) {\n     }\n   }\n \n+  /**\n+   * Determines weather the give grouping is unique.\n+   *\n+   * Consider a join which might produce non-unique rows; but later the results are aggregated again.\n+   * This method determines if there are sufficient columns in the grouping which have been present previously as unique column(s).\n+   */\n+  private boolean isGroupingUnique(RelNode input, ImmutableBitSet groups) {\n+    if (groups.isEmpty()) {\n+      return false;\n+    }\n+    RelMetadataQuery mq = input.getCluster().getMetadataQuery();\n+    Set<ImmutableBitSet> uKeys = mq.getUniqueKeys(input);\n+    for (ImmutableBitSet u : uKeys) {\n+      if (groups.contains(u)) {\n+        return true;\n+      }\n+    }\n+    if (input instanceof Join) {\n+      Join join = (Join) input;\n+      RexBuilder rexBuilder = input.getCluster().getRexBuilder();\n+      SimpleConditionInfo cond = new SimpleConditionInfo(join.getCondition(), rexBuilder);\n+\n+      if (cond.valid) {\n+        ImmutableBitSet newGroup = groups.intersect(ImmutableBitSet.fromBitSet(cond.fields));\n+        RelNode l = join.getLeft();\n+        RelNode r = join.getRight();\n+\n+        int joinFieldCount = join.getRowType().getFieldCount();\n+        int lFieldCount = l.getRowType().getFieldCount();\n+\n+        ImmutableBitSet groupL = newGroup.get(0, lFieldCount);\n+        ImmutableBitSet groupR = newGroup.get(lFieldCount, joinFieldCount).shift(-lFieldCount);\n+\n+        if (isGroupingUnique(l, groupL)) {\n+          return true;\n+        }\n+        if (isGroupingUnique(r, groupR)) {\n+          return true;\n+        }\n+      }\n+    }\n+    return false;\n+  }\n+\n+  static class SimpleConditionInfo {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzU5MjEyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/cost/HiveOnTezCostModel.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxOTo1OTo0NVrOHJS4ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxOTo1OTo0NVrOHJS4ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwODYxOA==", "bodyText": "Can we change rowCount to outputRowCount? This will make the change more readable.", "url": "https://github.com/apache/hive/pull/1439#discussion_r479508618", "createdAt": "2020-08-28T19:59:45Z", "author": {"login": "vineetgarg02"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/cost/HiveOnTezCostModel.java", "diffHunk": "@@ -89,22 +89,23 @@ public RelOptCost getAggregateCost(HiveAggregate aggregate) {\n     } else {\n       final RelMetadataQuery mq = aggregate.getCluster().getMetadataQuery();\n       // 1. Sum of input cardinalities\n-      final Double rCount = mq.getRowCount(aggregate.getInput());\n-      if (rCount == null) {\n+      final Double inputRowCount = mq.getRowCount(aggregate.getInput());\n+      final Double rowCount = mq.getRowCount(aggregate);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzU5NTAxOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/cost/HiveOnTezCostModel.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMDowMDo1MFrOHJS6SA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNDo0MTozMVrOHLyfFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwOTA2NA==", "bodyText": "rAverageSize is based on input row count but rowCount is output row count. Is this intended or should average row size be computed based on output row count?", "url": "https://github.com/apache/hive/pull/1439#discussion_r479509064", "createdAt": "2020-08-28T20:00:50Z", "author": {"login": "vineetgarg02"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/cost/HiveOnTezCostModel.java", "diffHunk": "@@ -89,22 +89,23 @@ public RelOptCost getAggregateCost(HiveAggregate aggregate) {\n     } else {\n       final RelMetadataQuery mq = aggregate.getCluster().getMetadataQuery();\n       // 1. Sum of input cardinalities\n-      final Double rCount = mq.getRowCount(aggregate.getInput());\n-      if (rCount == null) {\n+      final Double inputRowCount = mq.getRowCount(aggregate.getInput());\n+      final Double rowCount = mq.getRowCount(aggregate);\n+      if (inputRowCount == null || rowCount == null) {\n         return null;\n       }\n       // 2. CPU cost = sorting cost\n-      final double cpuCost = algoUtils.computeSortCPUCost(rCount);\n+      final double cpuCost = algoUtils.computeSortCPUCost(rowCount) + inputRowCount * algoUtils.getCpuUnitCost();\n       // 3. IO cost = cost of writing intermediary results to local FS +\n       //              cost of reading from local FS for transferring to GBy +\n       //              cost of transferring map outputs to GBy operator\n       final Double rAverageSize = mq.getAverageRowSize(aggregate.getInput());\n       if (rAverageSize == null) {\n         return null;\n       }\n-      final double ioCost = algoUtils.computeSortIOCost(new Pair<Double,Double>(rCount,rAverageSize));\n+      final double ioCost = algoUtils.computeSortIOCost(new Pair<Double, Double>(rowCount, rAverageSize));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjEyMzU0MA==", "bodyText": "if we will be doing a 2 phase groupby: every mapper will do some grouping before it starts emitting; in case iRC >> oRC the mappers could eliminate a lot of rows ; and they will most likely utilize O(oRC) io\nthis is an underestimation ; I wanted to multiply it with the number of mappers - but I don't think that's known at this point....I can add a config key for a fixed multiplier.", "url": "https://github.com/apache/hive/pull/1439#discussion_r482123540", "createdAt": "2020-09-02T14:41:31Z", "author": {"login": "kgyrtkirk"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/cost/HiveOnTezCostModel.java", "diffHunk": "@@ -89,22 +89,23 @@ public RelOptCost getAggregateCost(HiveAggregate aggregate) {\n     } else {\n       final RelMetadataQuery mq = aggregate.getCluster().getMetadataQuery();\n       // 1. Sum of input cardinalities\n-      final Double rCount = mq.getRowCount(aggregate.getInput());\n-      if (rCount == null) {\n+      final Double inputRowCount = mq.getRowCount(aggregate.getInput());\n+      final Double rowCount = mq.getRowCount(aggregate);\n+      if (inputRowCount == null || rowCount == null) {\n         return null;\n       }\n       // 2. CPU cost = sorting cost\n-      final double cpuCost = algoUtils.computeSortCPUCost(rCount);\n+      final double cpuCost = algoUtils.computeSortCPUCost(rowCount) + inputRowCount * algoUtils.getCpuUnitCost();\n       // 3. IO cost = cost of writing intermediary results to local FS +\n       //              cost of reading from local FS for transferring to GBy +\n       //              cost of transferring map outputs to GBy operator\n       final Double rAverageSize = mq.getAverageRowSize(aggregate.getInput());\n       if (rAverageSize == null) {\n         return null;\n       }\n-      final double ioCost = algoUtils.computeSortIOCost(new Pair<Double,Double>(rCount,rAverageSize));\n+      final double ioCost = algoUtils.computeSortIOCost(new Pair<Double, Double>(rowCount, rAverageSize));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwOTA2NA=="}, "originalCommit": {"oid": "e72417eee27e091c5a3e8ce4367bd8497f1c6d08"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTYwMjQyOnYy", "diffSide": "RIGHT", "path": "ql/src/test/results/clientpositive/llap/constraints_optimization.q.out", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwNDowOToxM1rOHO16Gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwNDowOToxM1rOHO16Gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMyNTMzOA==", "bodyText": "This is quite neat.", "url": "https://github.com/apache/hive/pull/1439#discussion_r485325338", "createdAt": "2020-09-09T04:09:13Z", "author": {"login": "jcamachor"}, "path": "ql/src/test/results/clientpositive/llap/constraints_optimization.q.out", "diffHunk": "@@ -2631,13 +2631,12 @@ POSTHOOK: Input: default@customer\n POSTHOOK: Input: default@store_sales\n #### A masked pattern was here ####\n CBO PLAN:\n-HiveAggregate(group=[{0}])\n-  HiveJoin(condition=[=($0, $8)], joinType=[inner], algorithm=[none], cost=[not available])\n-    HiveProject(c_customer_sk=[$0], c_customer_id=[$1], c_first_name=[$8], c_last_name=[$9], c_preferred_cust_flag=[$10], c_birth_country=[$14], c_login=[$15], c_email_address=[$16])\n-      HiveTableScan(table=[[default, customer]], table:alias=[customer])\n-    HiveProject(ss_customer_sk=[$3])\n-      HiveFilter(condition=[IS NOT NULL($3)])\n-        HiveTableScan(table=[[default, store_sales]], table:alias=[store_sales])\n+HiveSemiJoin(condition=[=($0, $1)], joinType=[semi])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62480d110acb48fdc98c08a43dc3166d3df457c5"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTYwNDkzOnYy", "diffSide": "RIGHT", "path": "ql/src/test/results/clientpositive/perf/tez/constraints/cbo_query23.q.out", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwNDoxMDoyNFrOHO17Yg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwNDoxMDoyNFrOHO17Yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMyNTY2Ng==", "bodyText": "Cool!", "url": "https://github.com/apache/hive/pull/1439#discussion_r485325666", "createdAt": "2020-09-09T04:10:24Z", "author": {"login": "jcamachor"}, "path": "ql/src/test/results/clientpositive/perf/tez/constraints/cbo_query23.q.out", "diffHunk": "@@ -155,18 +155,19 @@ HiveAggregate(group=[{}], agg#0=[sum($0)])\n                                     HiveTableScan(table=[[default, date_dim]], table:alias=[date_dim])\n           HiveProject($f1=[$0])\n             HiveFilter(condition=[>($2, 4)])\n-              HiveProject(i_item_sk=[$1], d_date=[$0], $f2=[$2])\n-                HiveAggregate(group=[{3, 4}], agg#0=[count()])\n-                  HiveJoin(condition=[=($1, $4)], joinType=[inner], algorithm=[none], cost=[not available])\n-                    HiveJoin(condition=[=($0, $2)], joinType=[inner], algorithm=[none], cost=[not available])\n-                      HiveProject(ss_sold_date_sk=[$0], ss_item_sk=[$2])\n-                        HiveFilter(condition=[IS NOT NULL($0)])\n-                          HiveTableScan(table=[[default, store_sales]], table:alias=[store_sales])\n-                      HiveProject(d_date_sk=[$0], d_date=[$2])\n-                        HiveFilter(condition=[IN($6, 1999, 2000, 2001, 2002)])\n-                          HiveTableScan(table=[[default, date_dim]], table:alias=[date_dim])\n-                    HiveProject(i_item_sk=[$0], substr=[substr($4, 1, 30)])\n-                      HiveTableScan(table=[[default, item]], table:alias=[item])\n+              HiveProject(i_item_sk=[$3], d_date=[$1], $f2=[$2])\n+                HiveJoin(condition=[=($0, $3)], joinType=[inner], algorithm=[none], cost=[not available])\n+                  HiveProject(ss_item_sk=[$0], d_date=[$1], $f2=[$2])\n+                    HiveAggregate(group=[{1, 3}], agg#0=[count()])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62480d110acb48fdc98c08a43dc3166d3df457c5"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTY0MDUwOnYy", "diffSide": "RIGHT", "path": "ql/src/test/queries/clientpositive/tpch18.q", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwNDozMDo1MVrOHO2PTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxNTo0OTowMlrOHQkUBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzMDc2Ng==", "bodyText": "Can we declare {{(L_ORDERKEY, L_LINENUMBER)}} as primary key? That way we will be able to verify whether HIVE-24087 is kicking in and removing the unnecessary join.\nWe can also try the variant with {{L_ORDERKEY NOT NULL}} constraint.", "url": "https://github.com/apache/hive/pull/1439#discussion_r485330766", "createdAt": "2020-09-09T04:30:51Z", "author": {"login": "jcamachor"}, "path": "ql/src/test/queries/clientpositive/tpch18.q", "diffHunk": "@@ -0,0 +1,133 @@\n+--! qt:dataset:tpch_0_001.customer\n+--! qt:dataset:tpch_0_001.lineitem\n+--! qt:dataset:tpch_0_001.nation\n+--! qt:dataset:tpch_0_001.orders\n+--! qt:dataset:tpch_0_001.part\n+--! qt:dataset:tpch_0_001.partsupp\n+--! qt:dataset:tpch_0_001.region\n+--! qt:dataset:tpch_0_001.supplier\n+\n+\n+use tpch_0_001;\n+\n+set hive.transpose.aggr.join=true;\n+set hive.transpose.aggr.join.unique=true;\n+set hive.mapred.mode=nonstrict;\n+\n+create view q18_tmp_cached as\n+select\n+\tl_orderkey,\n+\tsum(l_quantity) as t_sum_quantity\n+from\n+\tlineitem\n+where\n+\tl_orderkey is not null\n+group by\n+\tl_orderkey;\n+\n+\n+\n+explain cbo select\n+c_name,\n+c_custkey,\n+o_orderkey,\n+o_orderdate,\n+o_totalprice,\n+sum(l_quantity)\n+from\n+\tcustomer,\n+\torders,\n+\tq18_tmp_cached t,\n+\tlineitem l\n+where\n+c_custkey = o_custkey\n+and o_orderkey = t.l_orderkey\n+and o_orderkey is not null\n+and t.t_sum_quantity > 300\n+and o_orderkey = l.l_orderkey\n+and l.l_orderkey is not null\n+group by\n+c_name,\n+c_custkey,\n+o_orderkey,\n+o_orderdate,\n+o_totalprice\n+order by\n+o_totalprice desc,\n+o_orderdate\n+limit 100;\n+\n+\n+\n+select 'add constraints';\n+\n+alter table orders add constraint pk_o primary key (o_orderkey) disable novalidate rely;\n+alter table customer add constraint pk_c primary key (c_custkey) disable novalidate rely;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62480d110acb48fdc98c08a43dc3166d3df457c5"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzA5NjEwMw==", "bodyText": "I've added both constraints - it only removed the IS NOT NULL filter\nit seems to me that 1 of the sum() is used as an output and the other is being used to filter by >300 - so both of them is being \"used\"", "url": "https://github.com/apache/hive/pull/1439#discussion_r487096103", "createdAt": "2020-09-11T14:47:19Z", "author": {"login": "kgyrtkirk"}, "path": "ql/src/test/queries/clientpositive/tpch18.q", "diffHunk": "@@ -0,0 +1,133 @@\n+--! qt:dataset:tpch_0_001.customer\n+--! qt:dataset:tpch_0_001.lineitem\n+--! qt:dataset:tpch_0_001.nation\n+--! qt:dataset:tpch_0_001.orders\n+--! qt:dataset:tpch_0_001.part\n+--! qt:dataset:tpch_0_001.partsupp\n+--! qt:dataset:tpch_0_001.region\n+--! qt:dataset:tpch_0_001.supplier\n+\n+\n+use tpch_0_001;\n+\n+set hive.transpose.aggr.join=true;\n+set hive.transpose.aggr.join.unique=true;\n+set hive.mapred.mode=nonstrict;\n+\n+create view q18_tmp_cached as\n+select\n+\tl_orderkey,\n+\tsum(l_quantity) as t_sum_quantity\n+from\n+\tlineitem\n+where\n+\tl_orderkey is not null\n+group by\n+\tl_orderkey;\n+\n+\n+\n+explain cbo select\n+c_name,\n+c_custkey,\n+o_orderkey,\n+o_orderdate,\n+o_totalprice,\n+sum(l_quantity)\n+from\n+\tcustomer,\n+\torders,\n+\tq18_tmp_cached t,\n+\tlineitem l\n+where\n+c_custkey = o_custkey\n+and o_orderkey = t.l_orderkey\n+and o_orderkey is not null\n+and t.t_sum_quantity > 300\n+and o_orderkey = l.l_orderkey\n+and l.l_orderkey is not null\n+group by\n+c_name,\n+c_custkey,\n+o_orderkey,\n+o_orderdate,\n+o_totalprice\n+order by\n+o_totalprice desc,\n+o_orderdate\n+limit 100;\n+\n+\n+\n+select 'add constraints';\n+\n+alter table orders add constraint pk_o primary key (o_orderkey) disable novalidate rely;\n+alter table customer add constraint pk_c primary key (c_custkey) disable novalidate rely;\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzMDc2Ng=="}, "originalCommit": {"oid": "62480d110acb48fdc98c08a43dc3166d3df457c5"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzEzNDIxMw==", "bodyText": "Thanks @kgyrtkirk .\nThis seems to need further exploration, we thought https://issues.apache.org/jira/browse/HIVE-24087 was going to help here. @vineetgarg02 , could you take a look at this once this patch is merged? Maybe the shape of the plan is slightly different to the one we anticipated.", "url": "https://github.com/apache/hive/pull/1439#discussion_r487134213", "createdAt": "2020-09-11T15:49:02Z", "author": {"login": "jcamachor"}, "path": "ql/src/test/queries/clientpositive/tpch18.q", "diffHunk": "@@ -0,0 +1,133 @@\n+--! qt:dataset:tpch_0_001.customer\n+--! qt:dataset:tpch_0_001.lineitem\n+--! qt:dataset:tpch_0_001.nation\n+--! qt:dataset:tpch_0_001.orders\n+--! qt:dataset:tpch_0_001.part\n+--! qt:dataset:tpch_0_001.partsupp\n+--! qt:dataset:tpch_0_001.region\n+--! qt:dataset:tpch_0_001.supplier\n+\n+\n+use tpch_0_001;\n+\n+set hive.transpose.aggr.join=true;\n+set hive.transpose.aggr.join.unique=true;\n+set hive.mapred.mode=nonstrict;\n+\n+create view q18_tmp_cached as\n+select\n+\tl_orderkey,\n+\tsum(l_quantity) as t_sum_quantity\n+from\n+\tlineitem\n+where\n+\tl_orderkey is not null\n+group by\n+\tl_orderkey;\n+\n+\n+\n+explain cbo select\n+c_name,\n+c_custkey,\n+o_orderkey,\n+o_orderdate,\n+o_totalprice,\n+sum(l_quantity)\n+from\n+\tcustomer,\n+\torders,\n+\tq18_tmp_cached t,\n+\tlineitem l\n+where\n+c_custkey = o_custkey\n+and o_orderkey = t.l_orderkey\n+and o_orderkey is not null\n+and t.t_sum_quantity > 300\n+and o_orderkey = l.l_orderkey\n+and l.l_orderkey is not null\n+group by\n+c_name,\n+c_custkey,\n+o_orderkey,\n+o_orderdate,\n+o_totalprice\n+order by\n+o_totalprice desc,\n+o_orderdate\n+limit 100;\n+\n+\n+\n+select 'add constraints';\n+\n+alter table orders add constraint pk_o primary key (o_orderkey) disable novalidate rely;\n+alter table customer add constraint pk_c primary key (c_custkey) disable novalidate rely;\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTMzMDc2Ng=="}, "originalCommit": {"oid": "62480d110acb48fdc98c08a43dc3166d3df457c5"}, "originalPosition": 66}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 395, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}