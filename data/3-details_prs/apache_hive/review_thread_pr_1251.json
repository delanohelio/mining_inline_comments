{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ4NzczNDI4", "number": 1251, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxNDoxODowMlrOEOR19g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxNDozNTo0NlrOEOSXoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNDA3ODYyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxNDoxODowMlrOGxVynQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxOTo0ODowNlrOGxiv5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM5MDQyOQ==", "bodyText": "We could spare the deserialization of MapWork from JobConf here, if we pass the MapWork instance already present in LlapRecordReader to VectorizedOrcAcidRowBatchReader ctor. (Downside is that in turn we would need to adjust the other ctor's of VectorizedOrcAcidRowBatchReader too)", "url": "https://github.com/apache/hive/pull/1251#discussion_r454390429", "createdAt": "2020-07-14T14:18:02Z", "author": {"login": "szlta"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -232,6 +250,17 @@ private VectorizedOrcAcidRowBatchReader(JobConf conf, OrcSplit orcSplit, Reporte\n \n     this.syntheticProps = orcSplit.getSyntheticAcidProps();\n \n+    if (LlapHiveUtils.isLlapMode(conf) && LlapProxy.isDaemon()\n+            && HiveConf.getBoolVar(conf, ConfVars.LLAP_TRACK_CACHE_USAGE))\n+    {\n+      MapWork mapWork = LlapHiveUtils.findMapWork(conf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYwMjcyNw==", "bodyText": "Good idea, done!", "url": "https://github.com/apache/hive/pull/1251#discussion_r454602727", "createdAt": "2020-07-14T19:48:06Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -232,6 +250,17 @@ private VectorizedOrcAcidRowBatchReader(JobConf conf, OrcSplit orcSplit, Reporte\n \n     this.syntheticProps = orcSplit.getSyntheticAcidProps();\n \n+    if (LlapHiveUtils.isLlapMode(conf) && LlapProxy.isDaemon()\n+            && HiveConf.getBoolVar(conf, ConfVars.LLAP_TRACK_CACHE_USAGE))\n+    {\n+      MapWork mapWork = LlapHiveUtils.findMapWork(conf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM5MDQyOQ=="}, "originalCommit": {"oid": "c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNDA5ODM0OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxNDoyMjozMFrOGxV_FQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxOTo0ODoyNVrOGxiwmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM5MzYyMQ==", "bodyText": "Initialized to true on purpose for now? If not, I don't see it getting set to false.", "url": "https://github.com/apache/hive/pull/1251#discussion_r454393621", "createdAt": "2020-07-14T14:22:30Z", "author": {"login": "szlta"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -129,6 +137,16 @@\n    */\n   private SearchArgument deleteEventSarg = null;\n \n+  /**\n+   * Cachetag associated with the Split\n+   */\n+  private final CacheTag cacheTag;\n+\n+  /**\n+   * Skip using Llap IO cache for checking delete_delta files if the configuration is not correct\n+   */\n+  private static boolean skipLlapCache = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYwMjkwNA==", "bodyText": "That was a mistake. Corrected, and initialized as false", "url": "https://github.com/apache/hive/pull/1251#discussion_r454602904", "createdAt": "2020-07-14T19:48:25Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -129,6 +137,16 @@\n    */\n   private SearchArgument deleteEventSarg = null;\n \n+  /**\n+   * Cachetag associated with the Split\n+   */\n+  private final CacheTag cacheTag;\n+\n+  /**\n+   * Skip using Llap IO cache for checking delete_delta files if the configuration is not correct\n+   */\n+  private static boolean skipLlapCache = true;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM5MzYyMQ=="}, "originalCommit": {"oid": "c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNDE2NDgwOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxNDozNTo0NlrOGxWnTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxOTo0ODozOVrOGxixIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQwMzkxOQ==", "bodyText": "nit: comment could be more verbose, like: Reader can be reused if it was created before: only for non-LLAP cache cases, otherwise we need to create it here", "url": "https://github.com/apache/hive/pull/1251#discussion_r454403919", "createdAt": "2020-07-14T14:35:46Z", "author": {"login": "szlta"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1562,20 +1580,31 @@ public int compareTo(CompressedOwid other) {\n       try {\n         final Path[] deleteDeltaDirs = getDeleteDeltaDirsFromSplit(orcSplit);\n         if (deleteDeltaDirs.length > 0) {\n+          FileSystem fs = orcSplit.getPath().getFileSystem(conf);\n+          AcidOutputFormat.Options orcSplitMinMaxWriteIds =\n+              AcidUtils.parseBaseOrDeltaBucketFilename(orcSplit.getPath(), conf);\n           int totalDeleteEventCount = 0;\n           for (Path deleteDeltaDir : deleteDeltaDirs) {\n-            FileSystem fs = deleteDeltaDir.getFileSystem(conf);\n+            if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n+              continue;\n+            }\n             Path[] deleteDeltaFiles = OrcRawRecordMerger.getDeltaFiles(deleteDeltaDir, bucket,\n                 new OrcRawRecordMerger.Options().isCompacting(false), null);\n             for (Path deleteDeltaFile : deleteDeltaFiles) {\n               try {\n-                /**\n-                 * todo: we have OrcSplit.orcTail so we should be able to get stats from there\n-                 */\n-                Reader deleteDeltaReader = OrcFile.createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n-                if (deleteDeltaReader.getNumberOfRows() <= 0) {\n+                ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag);\n+                OrcTail orcTail = readerData.orcTail;\n+                if (orcTail.getFooter().getNumberOfRows() <= 0) {\n                   continue; // just a safe check to ensure that we are not reading empty delete files.\n                 }\n+                OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n+                if (!deleteKeyInterval.isIntersects(keyInterval)) {\n+                  // If there is no intersection between data and delete delta, do not read delete file\n+                  continue;\n+                }\n+                // Create the reader if we got the OrcTail from cache", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8"}, "originalPosition": 318}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDYwMzA0Mg==", "bodyText": "Added more comment", "url": "https://github.com/apache/hive/pull/1251#discussion_r454603042", "createdAt": "2020-07-14T19:48:39Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1562,20 +1580,31 @@ public int compareTo(CompressedOwid other) {\n       try {\n         final Path[] deleteDeltaDirs = getDeleteDeltaDirsFromSplit(orcSplit);\n         if (deleteDeltaDirs.length > 0) {\n+          FileSystem fs = orcSplit.getPath().getFileSystem(conf);\n+          AcidOutputFormat.Options orcSplitMinMaxWriteIds =\n+              AcidUtils.parseBaseOrDeltaBucketFilename(orcSplit.getPath(), conf);\n           int totalDeleteEventCount = 0;\n           for (Path deleteDeltaDir : deleteDeltaDirs) {\n-            FileSystem fs = deleteDeltaDir.getFileSystem(conf);\n+            if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n+              continue;\n+            }\n             Path[] deleteDeltaFiles = OrcRawRecordMerger.getDeltaFiles(deleteDeltaDir, bucket,\n                 new OrcRawRecordMerger.Options().isCompacting(false), null);\n             for (Path deleteDeltaFile : deleteDeltaFiles) {\n               try {\n-                /**\n-                 * todo: we have OrcSplit.orcTail so we should be able to get stats from there\n-                 */\n-                Reader deleteDeltaReader = OrcFile.createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n-                if (deleteDeltaReader.getNumberOfRows() <= 0) {\n+                ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag);\n+                OrcTail orcTail = readerData.orcTail;\n+                if (orcTail.getFooter().getNumberOfRows() <= 0) {\n                   continue; // just a safe check to ensure that we are not reading empty delete files.\n                 }\n+                OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n+                if (!deleteKeyInterval.isIntersects(keyInterval)) {\n+                  // If there is no intersection between data and delete delta, do not read delete file\n+                  continue;\n+                }\n+                // Create the reader if we got the OrcTail from cache", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDQwMzkxOQ=="}, "originalCommit": {"oid": "c23f9dfe6facb9b8dbb03e5716d4e616997d4ac8"}, "originalPosition": 318}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 573, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}