{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDUxMjgxNTkx", "number": 1275, "reviewThreads": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwOTo0Mjo0N1rOEQi8Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwODoxNzozNlrOETJXNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1Nzg1MTkxOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwOTo0Mjo0N1rOG0wUBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNzowNjo0NlrOG1BoCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzk3MDY5NA==", "bodyText": "I think it would be a good idea to shut down the executor when we finished the run loop.\nWhat do you think?", "url": "https://github.com/apache/hive/pull/1275#discussion_r457970694", "createdAt": "2020-07-21T09:42:47Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -64,13 +67,15 @@\n   static final private String CLASS_NAME = Cleaner.class.getName();\n   static final private Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n   private long cleanerCheckInterval = 0;\n+  private Executor cleanerExecutor;\n \n   private ReplChangeManager replChangeManager;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n+    cleanerExecutor = Executors.newFixedThreadPool(conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a94d0aacdf61a220514850b414ba0c126ef99f7e"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODAxMzM3OA==", "bodyText": "Yeah, makes sense. Added it.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458013378", "createdAt": "2020-07-21T11:02:17Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -64,13 +67,15 @@\n   static final private String CLASS_NAME = Cleaner.class.getName();\n   static final private Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n   private long cleanerCheckInterval = 0;\n+  private Executor cleanerExecutor;\n \n   private ReplChangeManager replChangeManager;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n+    cleanerExecutor = Executors.newFixedThreadPool(conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzk3MDY5NA=="}, "originalCommit": {"oid": "a94d0aacdf61a220514850b414ba0c126ef99f7e"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODAyNzkxMw==", "bodyText": "I prefer the way how it was done in Worker.java:\n\nNamed threads\nPriority down\nSet daemon on/off\nStopped in finally\n\nShall we do it here, or should we create a follow-up jira for creating and cleaning up the executor threads for Cleaner and Initiator as well?\nThanks,\nPeter", "url": "https://github.com/apache/hive/pull/1275#discussion_r458027913", "createdAt": "2020-07-21T11:31:40Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -64,13 +67,15 @@\n   static final private String CLASS_NAME = Cleaner.class.getName();\n   static final private Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n   private long cleanerCheckInterval = 0;\n+  private Executor cleanerExecutor;\n \n   private ReplChangeManager replChangeManager;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n+    cleanerExecutor = Executors.newFixedThreadPool(conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzk3MDY5NA=="}, "originalCommit": {"oid": "a94d0aacdf61a220514850b414ba0c126ef99f7e"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODA5Njc2NQ==", "bodyText": "Just saw the approach in Worker.java, it looks much better/cleaner. I will make the changes here only ()for both cleaner/intiator) and update once it is ready.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458096765", "createdAt": "2020-07-21T13:29:27Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -64,13 +67,15 @@\n   static final private String CLASS_NAME = Cleaner.class.getName();\n   static final private Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n   private long cleanerCheckInterval = 0;\n+  private Executor cleanerExecutor;\n \n   private ReplChangeManager replChangeManager;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n+    cleanerExecutor = Executors.newFixedThreadPool(conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzk3MDY5NA=="}, "originalCommit": {"oid": "a94d0aacdf61a220514850b414ba0c126ef99f7e"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI1NDM0NA==", "bodyText": "Done.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458254344", "createdAt": "2020-07-21T17:06:46Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -64,13 +67,15 @@\n   static final private String CLASS_NAME = Cleaner.class.getName();\n   static final private Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n   private long cleanerCheckInterval = 0;\n+  private Executor cleanerExecutor;\n \n   private ReplChangeManager replChangeManager;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n+    cleanerExecutor = Executors.newFixedThreadPool(conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzk3MDY5NA=="}, "originalCommit": {"oid": "a94d0aacdf61a220514850b414ba0c126ef99f7e"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1Nzg5NTY1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwOTo1NTowN1rOG0wvbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxMDozNzo0MVrOG0yKaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzk3NzcwOQ==", "bodyText": "What happens if one of the futures throws the exception?\nAre the others continue to execute? Will we wait until all of the tasks are finished one way or another?\nWe do not want multiple Cleaning tasks running concurrently on the same partition.\nSeeing this the same problem might arise with the Initiator too. What do you think @deniskuzZ ?", "url": "https://github.com/apache/hive/pull/1275#discussion_r457977709", "createdAt": "2020-07-21T09:55:07Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -89,9 +94,12 @@ public void run() {\n         handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n         startedAt = System.currentTimeMillis();\n         long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+        List<CompletableFuture> cleanerList = new ArrayList<>();\n         for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+          cleanerList.add(CompletableFuture.runAsync(ThrowingRunnable.unchecked(() ->\n+            clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n         }\n+        CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a94d0aacdf61a220514850b414ba0c126ef99f7e"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODAwMTAwMQ==", "bodyText": "Talked with @deniskuzZ and he convinced me that join() will wait for all of the tasks to finish :)", "url": "https://github.com/apache/hive/pull/1275#discussion_r458001001", "createdAt": "2020-07-21T10:37:41Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -89,9 +94,12 @@ public void run() {\n         handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n         startedAt = System.currentTimeMillis();\n         long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+        List<CompletableFuture> cleanerList = new ArrayList<>();\n         for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+          cleanerList.add(CompletableFuture.runAsync(ThrowingRunnable.unchecked(() ->\n+            clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n         }\n+        CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzk3NzcwOQ=="}, "originalCommit": {"oid": "a94d0aacdf61a220514850b414ba0c126ef99f7e"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1ODkxNDgwOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNDoyMzoxM1rOG06cXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNzowNzoxMFrOG1BpCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODEzNjY3MA==", "bodyText": "Why not just declare cleanerExecutor as and ExecutorService to start?", "url": "https://github.com/apache/hive/pull/1275#discussion_r458136670", "createdAt": "2020-07-21T14:23:13Z", "author": {"login": "belugabehr"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -113,6 +122,8 @@ public void run() {\n         }\n       }\n     } while (!stop.get());\n+\n+    ((ExecutorService)cleanerExecutor).shutdown();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce1780f8f5e5e502a9c4e220df424b89fa3ab7bc"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI1NDYwMA==", "bodyText": "Modify it now.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458254600", "createdAt": "2020-07-21T17:07:10Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -113,6 +122,8 @@ public void run() {\n         }\n       }\n     } while (!stop.get());\n+\n+    ((ExecutorService)cleanerExecutor).shutdown();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODEzNjY3MA=="}, "originalCommit": {"oid": "ce1780f8f5e5e502a9c4e220df424b89fa3ab7bc"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTkzODQ0OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwODowMzoyMVrOG1XUFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwODowMzoyMVrOG1XUFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYwOTY4NQ==", "bodyText": "Minimally log the error on info level?", "url": "https://github.com/apache/hive/pull/1275#discussion_r458609685", "createdAt": "2020-07-22T08:03:21Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -80,39 +98,58 @@ public void run() {\n           HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n     }\n \n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n+        try {\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          int count = 0;\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            completionService.submit(() -> {\n+              ThrowingRunnable.unchecked(() -> clean(compactionInfo, minOpenTxnId));\n+              return null;\n+            });\n+            count++;\n+          }\n+\n+          for(int i=0; i<count; i++) {\n+            try {\n+              completionService.take().get();\n+            } catch (InterruptedException| ExecutionException ignore) {\n+              // What should we do here?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTk0MDExOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwODowMzo1MlrOG1XVIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwODowMzo1MlrOG1XVIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYwOTk1NA==", "bodyText": "nit of the nit: formatting:\nfor (int i = 0; i < count; i++) {", "url": "https://github.com/apache/hive/pull/1275#discussion_r458609954", "createdAt": "2020-07-22T08:03:52Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -80,39 +98,58 @@ public void run() {\n           HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n     }\n \n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n+        try {\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          int count = 0;\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            completionService.submit(() -> {\n+              ThrowingRunnable.unchecked(() -> clean(compactionInfo, minOpenTxnId));\n+              return null;\n+            });\n+            count++;\n+          }\n+\n+          for(int i=0; i<count; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTk0NTY5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwODowNToyM1rOG1XYew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwOTo1Mjo1NlrOG1bSnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYxMDgxMQ==", "bodyText": "Shall we move this to the run method too? It would make it easier to understand the code IMHO", "url": "https://github.com/apache/hive/pull/1275#discussion_r458610811", "createdAt": "2020-07-22T08:05:23Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -64,13 +72,23 @@\n   static final private String CLASS_NAME = Cleaner.class.getName();\n   static final private Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n   private long cleanerCheckInterval = 0;\n+  private ExecutorService cleanerExecutor;\n+  private CompletionService<Void> completionService;\n \n   private ReplChangeManager replChangeManager;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n+    ThreadFactory threadFactory = new ThreadFactoryBuilder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYxMTE4Nw==", "bodyText": "Or maybe use this as an utility method to create the executor like and use this in the run method to create the executor:\nWhateverUtil.createExecutor(String name, int size) {}", "url": "https://github.com/apache/hive/pull/1275#discussion_r458611187", "createdAt": "2020-07-22T08:06:04Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -64,13 +72,23 @@\n   static final private String CLASS_NAME = Cleaner.class.getName();\n   static final private Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n   private long cleanerCheckInterval = 0;\n+  private ExecutorService cleanerExecutor;\n+  private CompletionService<Void> completionService;\n \n   private ReplChangeManager replChangeManager;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n+    ThreadFactory threadFactory = new ThreadFactoryBuilder()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYxMDgxMQ=="}, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3NDg0Nw==", "bodyText": "Done.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458674847", "createdAt": "2020-07-22T09:52:56Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -64,13 +72,23 @@\n   static final private String CLASS_NAME = Cleaner.class.getName();\n   static final private Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n   private long cleanerCheckInterval = 0;\n+  private ExecutorService cleanerExecutor;\n+  private CompletionService<Void> completionService;\n \n   private ReplChangeManager replChangeManager;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n+    ThreadFactory threadFactory = new ThreadFactoryBuilder()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYxMDgxMQ=="}, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTk1MDI3OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwODowNjo0NFrOG1XbXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwOTo1NDoyNFrOG1bV4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYxMTU1MQ==", "bodyText": "What's the reasoning behind this change?", "url": "https://github.com/apache/hive/pull/1275#discussion_r458611551", "createdAt": "2020-07-22T08:06:44Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "diffHunk": "@@ -149,17 +156,25 @@ public void run() {\n               String runAs = resolveUserToRunAs(tblNameOwners, t, p);\n               /* checkForCompaction includes many file metadata checks and may be expensive.\n                * Therefore, using a thread pool here and running checkForCompactions in parallel */\n-              compactionList.add(CompletableFuture.runAsync(ThrowingRunnable.unchecked(() ->\n-                  scheduleCompactionIfRequired(ci, t, p, runAs)), compactionExecutor));\n+              completionService.submit(() -> {\n+                ThrowingRunnable.unchecked(() -> scheduleCompactionIfRequired(ci, t, p, runAs));\n+                return null;\n+              });\n+              count++;\n             } catch (Throwable t) {\n               LOG.error(\"Caught exception while trying to determine if we should compact {}. \" +\n                   \"Marking failed to avoid repeated failures, {}\", ci, t);\n               ci.errorMessage = t.getMessage();\n               txnHandler.markFailed(ci);\n             }\n           }\n-          CompletableFuture.allOf(compactionList.toArray(new CompletableFuture[0]))\n-            .join();\n+          for(int i=0; i<count; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3NTY4MA==", "bodyText": "Earlier I missed the fact that we don't care in how and when are task getting completed and using join is better in terms of readability and usability too. So I have reverted this change.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458675680", "createdAt": "2020-07-22T09:54:24Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "diffHunk": "@@ -149,17 +156,25 @@ public void run() {\n               String runAs = resolveUserToRunAs(tblNameOwners, t, p);\n               /* checkForCompaction includes many file metadata checks and may be expensive.\n                * Therefore, using a thread pool here and running checkForCompactions in parallel */\n-              compactionList.add(CompletableFuture.runAsync(ThrowingRunnable.unchecked(() ->\n-                  scheduleCompactionIfRequired(ci, t, p, runAs)), compactionExecutor));\n+              completionService.submit(() -> {\n+                ThrowingRunnable.unchecked(() -> scheduleCompactionIfRequired(ci, t, p, runAs));\n+                return null;\n+              });\n+              count++;\n             } catch (Throwable t) {\n               LOG.error(\"Caught exception while trying to determine if we should compact {}. \" +\n                   \"Marking failed to avoid repeated failures, {}\", ci, t);\n               ci.errorMessage = t.getMessage();\n               txnHandler.markFailed(ci);\n             }\n           }\n-          CompletableFuture.allOf(compactionList.toArray(new CompletableFuture[0]))\n-            .join();\n+          for(int i=0; i<count; i++) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYxMTU1MQ=="}, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MjAzMDc1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwODoyNzo0NlrOG1YMGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxNzozMzo1OVrOG1s_vA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYyNDAyNg==", "bodyText": "what's the reasoning behind this? if you want to call it, do it before releasing the lock!!", "url": "https://github.com/apache/hive/pull/1275#discussion_r458624026", "createdAt": "2020-07-22T08:27:46Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "diffHunk": "@@ -182,6 +197,10 @@ public void run() {\n     } catch (Throwable t) {\n       LOG.error(\"Caught an exception in the main loop of compactor initiator, exiting \" +\n           StringUtils.stringifyException(t));\n+    } finally {\n+      if (compactionExecutor != null) {\n+        compactionExecutor.shutdownNow();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3NTgyMw==", "bodyText": "Fixed this.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458675823", "createdAt": "2020-07-22T09:54:38Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "diffHunk": "@@ -182,6 +197,10 @@ public void run() {\n     } catch (Throwable t) {\n       LOG.error(\"Caught an exception in the main loop of compactor initiator, exiting \" +\n           StringUtils.stringifyException(t));\n+    } finally {\n+      if (compactionExecutor != null) {\n+        compactionExecutor.shutdownNow();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYyNDAyNg=="}, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk2NDkyNA==", "bodyText": "@adesh-rao, sorry that was actually a right spot, that's basically end of thread execution", "url": "https://github.com/apache/hive/pull/1275#discussion_r458964924", "createdAt": "2020-07-22T17:33:59Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "diffHunk": "@@ -182,6 +197,10 @@ public void run() {\n     } catch (Throwable t) {\n       LOG.error(\"Caught an exception in the main loop of compactor initiator, exiting \" +\n           StringUtils.stringifyException(t));\n+    } finally {\n+      if (compactionExecutor != null) {\n+        compactionExecutor.shutdownNow();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYyNDAyNg=="}, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MjA2Mjc3OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwODozNjoxNFrOG1YgGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwOTo1NDo1M1rOG1bXBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYyOTE0NA==", "bodyText": "what's the reasoning behind this change? returning some null doesn't add more code readability", "url": "https://github.com/apache/hive/pull/1275#discussion_r458629144", "createdAt": "2020-07-22T08:36:14Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "diffHunk": "@@ -149,17 +156,25 @@ public void run() {\n               String runAs = resolveUserToRunAs(tblNameOwners, t, p);\n               /* checkForCompaction includes many file metadata checks and may be expensive.\n                * Therefore, using a thread pool here and running checkForCompactions in parallel */\n-              compactionList.add(CompletableFuture.runAsync(ThrowingRunnable.unchecked(() ->\n-                  scheduleCompactionIfRequired(ci, t, p, runAs)), compactionExecutor));\n+              completionService.submit(() -> {\n+                ThrowingRunnable.unchecked(() -> scheduleCompactionIfRequired(ci, t, p, runAs));\n+                return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3NTk3Mg==", "bodyText": "Same as above. Removed it.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458675972", "createdAt": "2020-07-22T09:54:53Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "diffHunk": "@@ -149,17 +156,25 @@ public void run() {\n               String runAs = resolveUserToRunAs(tblNameOwners, t, p);\n               /* checkForCompaction includes many file metadata checks and may be expensive.\n                * Therefore, using a thread pool here and running checkForCompactions in parallel */\n-              compactionList.add(CompletableFuture.runAsync(ThrowingRunnable.unchecked(() ->\n-                  scheduleCompactionIfRequired(ci, t, p, runAs)), compactionExecutor));\n+              completionService.submit(() -> {\n+                ThrowingRunnable.unchecked(() -> scheduleCompactionIfRequired(ci, t, p, runAs));\n+                return null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODYyOTE0NA=="}, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MjE5MjkzOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwOTowOTo0NFrOG1ZvqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwOTo1NTowMlrOG1bXXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY0OTUxMw==", "bodyText": "CompletableFuture is a better choice", "url": "https://github.com/apache/hive/pull/1275#discussion_r458649513", "createdAt": "2020-07-22T09:09:44Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -80,39 +98,58 @@ public void run() {\n           HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n     }\n \n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n+        try {\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          int count = 0;\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            completionService.submit(() -> {\n+              ThrowingRunnable.unchecked(() -> clean(compactionInfo, minOpenTxnId));\n+              return null;\n+            });\n+            count++;\n+          }\n+\n+          for(int i=0; i<count; i++) {\n+            try {\n+              completionService.take().get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3NjA2Mg==", "bodyText": "Agree, changed it.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458676062", "createdAt": "2020-07-22T09:55:02Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -80,39 +98,58 @@ public void run() {\n           HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n     }\n \n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n+        try {\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          int count = 0;\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            completionService.submit(() -> {\n+              ThrowingRunnable.unchecked(() -> clean(compactionInfo, minOpenTxnId));\n+              return null;\n+            });\n+            count++;\n+          }\n+\n+          for(int i=0; i<count; i++) {\n+            try {\n+              completionService.take().get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY0OTUxMw=="}, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MjIwNzQ3OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwOToxMzo0M1rOG1Z4xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwOTo1NToxMFrOG1bXoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1MTg0Nw==", "bodyText": "shutdown should be called here, otherwise you can terminate threads from 2nd iteration", "url": "https://github.com/apache/hive/pull/1275#discussion_r458651847", "createdAt": "2020-07-22T09:13:43Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -80,39 +98,58 @@ public void run() {\n           HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n     }\n \n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n+        try {\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          int count = 0;\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            completionService.submit(() -> {\n+              ThrowingRunnable.unchecked(() -> clean(compactionInfo, minOpenTxnId));\n+              return null;\n+            });\n+            count++;\n+          }\n+\n+          for(int i=0; i<count; i++) {\n+            try {\n+              completionService.take().get();\n+            } catch (InterruptedException| ExecutionException ignore) {\n+              // What should we do here?\n+            }\n+          }\n+        } catch (Throwable t) {\n+          LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n+                  StringUtils.stringifyException(t));\n         }\n-      } catch (Throwable t) {\n-        LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n-        if (handle != null) {\n-          handle.releaseLocks();\n+        finally {\n+          if (handle != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3NjEyOQ==", "bodyText": "Fixed this.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458676129", "createdAt": "2020-07-22T09:55:10Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -80,39 +98,58 @@ public void run() {\n           HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n     }\n \n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n+        try {\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          int count = 0;\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            completionService.submit(() -> {\n+              ThrowingRunnable.unchecked(() -> clean(compactionInfo, minOpenTxnId));\n+              return null;\n+            });\n+            count++;\n+          }\n+\n+          for(int i=0; i<count; i++) {\n+            try {\n+              completionService.take().get();\n+            } catch (InterruptedException| ExecutionException ignore) {\n+              // What should we do here?\n+            }\n+          }\n+        } catch (Throwable t) {\n+          LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n+                  StringUtils.stringifyException(t));\n         }\n-      } catch (Throwable t) {\n-        LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n-        if (handle != null) {\n-          handle.releaseLocks();\n+        finally {\n+          if (handle != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1MTg0Nw=="}, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MjIyMjM2OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwOToxNzo0MVrOG1aB4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwOTo1NToxOVrOG1bX_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1NDE3OQ==", "bodyText": "could we refactor this block by negating if condition and removing continue part?", "url": "https://github.com/apache/hive/pull/1275#discussion_r458654179", "createdAt": "2020-07-22T09:17:41Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -80,39 +98,58 @@ public void run() {\n           HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n     }\n \n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n+        try {\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          int count = 0;\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            completionService.submit(() -> {\n+              ThrowingRunnable.unchecked(() -> clean(compactionInfo, minOpenTxnId));\n+              return null;\n+            });\n+            count++;\n+          }\n+\n+          for(int i=0; i<count; i++) {\n+            try {\n+              completionService.take().get();\n+            } catch (InterruptedException| ExecutionException ignore) {\n+              // What should we do here?\n+            }\n+          }\n+        } catch (Throwable t) {\n+          LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n+                  StringUtils.stringifyException(t));\n         }\n-      } catch (Throwable t) {\n-        LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n-        if (handle != null) {\n-          handle.releaseLocks();\n+        finally {\n+          if (handle != null) {\n+            handle.releaseLocks();\n+          }\n         }\n-      }\n-      // Now, go back to bed until it's time to do this again\n-      long elapsedTime = System.currentTimeMillis() - startedAt;\n-      if (elapsedTime >= cleanerCheckInterval || stop.get())  {\n-        continue;\n-      } else {\n-        try {\n-          Thread.sleep(cleanerCheckInterval - elapsedTime);\n-        } catch (InterruptedException ie) {\n-          // What can I do about it?\n+        // Now, go back to bed until it's time to do this again\n+        long elapsedTime = System.currentTimeMillis() - startedAt;\n+        if (elapsedTime >= cleanerCheckInterval || stop.get())  {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3NjIyMg==", "bodyText": "Done.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458676222", "createdAt": "2020-07-22T09:55:19Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -80,39 +98,58 @@ public void run() {\n           HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n     }\n \n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n+        try {\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          int count = 0;\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            completionService.submit(() -> {\n+              ThrowingRunnable.unchecked(() -> clean(compactionInfo, minOpenTxnId));\n+              return null;\n+            });\n+            count++;\n+          }\n+\n+          for(int i=0; i<count; i++) {\n+            try {\n+              completionService.take().get();\n+            } catch (InterruptedException| ExecutionException ignore) {\n+              // What should we do here?\n+            }\n+          }\n+        } catch (Throwable t) {\n+          LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n+                  StringUtils.stringifyException(t));\n         }\n-      } catch (Throwable t) {\n-        LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n-        if (handle != null) {\n-          handle.releaseLocks();\n+        finally {\n+          if (handle != null) {\n+            handle.releaseLocks();\n+          }\n         }\n-      }\n-      // Now, go back to bed until it's time to do this again\n-      long elapsedTime = System.currentTimeMillis() - startedAt;\n-      if (elapsedTime >= cleanerCheckInterval || stop.get())  {\n-        continue;\n-      } else {\n-        try {\n-          Thread.sleep(cleanerCheckInterval - elapsedTime);\n-        } catch (InterruptedException ie) {\n-          // What can I do about it?\n+        // Now, go back to bed until it's time to do this again\n+        long elapsedTime = System.currentTimeMillis() - startedAt;\n+        if (elapsedTime >= cleanerCheckInterval || stop.get())  {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY1NDE3OQ=="}, "originalCommit": {"oid": "b4c3d2e27d6b147f4cf8053b4388ad523ebdf7e1"}, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MjM4MjI5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxMDowMDoyNFrOG1bj6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxMDowMDoyNFrOG1bj6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODY3OTI3Mw==", "bodyText": "Could you please move this to constants", "url": "https://github.com/apache/hive/pull/1275#discussion_r458679273", "createdAt": "2020-07-22T10:00:24Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -79,7 +81,9 @@ public void run() {\n       cleanerCheckInterval = conf.getTimeVar(\n           HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n     }\n-\n+    String threadNameFormat = \"Cleaner-executor-thread-%d\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MjUzMDExOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": true, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxMDo0NToyNVrOG1c_gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwNzo0NToyM1rOG3XL9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwMjcyMw==", "bodyText": "what's the purpose of executor service shutdown? it would be needed only when Cleaner thread terminates", "url": "https://github.com/apache/hive/pull/1275#discussion_r458702723", "createdAt": "2020-07-22T10:45:25Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -89,23 +93,28 @@ public void run() {\n         handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n         startedAt = System.currentTimeMillis();\n         long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+        List<CompletableFuture> cleanerList = new ArrayList<>();\n         for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+          cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n         }\n+        CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n       } catch (Throwable t) {\n         LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n+                StringUtils.stringifyException(t));\n+        if (cleanerExecutor != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwNTY5NQ==", "bodyText": "This is to handle the case when threads in executors are lost due to exception.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458705695", "createdAt": "2020-07-22T10:51:20Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -89,23 +93,28 @@ public void run() {\n         handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n         startedAt = System.currentTimeMillis();\n         long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+        List<CompletableFuture> cleanerList = new ArrayList<>();\n         for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+          cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n         }\n+        CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n       } catch (Throwable t) {\n         LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n+                StringUtils.stringifyException(t));\n+        if (cleanerExecutor != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwMjcyMw=="}, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODc3Njc0Nw==", "bodyText": "You are submitting tasks to the executor inside the loop, all at once, so if there will be an exception in main loop - there won't be any running threads in executor.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458776747", "createdAt": "2020-07-22T13:07:01Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -89,23 +93,28 @@ public void run() {\n         handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n         startedAt = System.currentTimeMillis();\n         long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+        List<CompletableFuture> cleanerList = new ArrayList<>();\n         for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+          cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n         }\n+        CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n       } catch (Throwable t) {\n         LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n+                StringUtils.stringifyException(t));\n+        if (cleanerExecutor != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwMjcyMw=="}, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk0MDMzNA==", "bodyText": "Agreed. I have removed the executor shutdown.\nBut where/should we add executor shutdown? I think since the Cleaner is supposed to be running all the time. We can skip executor shutdown?", "url": "https://github.com/apache/hive/pull/1275#discussion_r458940334", "createdAt": "2020-07-22T16:53:14Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -89,23 +93,28 @@ public void run() {\n         handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n         startedAt = System.currentTimeMillis();\n         long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+        List<CompletableFuture> cleanerList = new ArrayList<>();\n         for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+          cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n         }\n+        CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n       } catch (Throwable t) {\n         LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n+                StringUtils.stringifyException(t));\n+        if (cleanerExecutor != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwMjcyMw=="}, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk2Nzg0Ng==", "bodyText": "@adesh-rao, sorry I think you have placed it originally at the right spot, at the end of run() method, you'll probably have to wrap it's internals (do while) with try and put shutdownNow in finally section. note: remove catch InterruptedException.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458967846", "createdAt": "2020-07-22T17:38:42Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -89,23 +93,28 @@ public void run() {\n         handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n         startedAt = System.currentTimeMillis();\n         long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+        List<CompletableFuture> cleanerList = new ArrayList<>();\n         for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+          cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n         }\n+        CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n       } catch (Throwable t) {\n         LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n+                StringUtils.stringifyException(t));\n+        if (cleanerExecutor != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwMjcyMw=="}, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQzNTEwNA==", "bodyText": "Which InterruptedException are you pointing at?  Also, I have moved the shutdown at the end of run method now.", "url": "https://github.com/apache/hive/pull/1275#discussion_r459435104", "createdAt": "2020-07-23T13:08:25Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -89,23 +93,28 @@ public void run() {\n         handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n         startedAt = System.currentTimeMillis();\n         long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+        List<CompletableFuture> cleanerList = new ArrayList<>();\n         for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+          cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n         }\n+        CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n       } catch (Throwable t) {\n         LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n+                StringUtils.stringifyException(t));\n+        if (cleanerExecutor != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwMjcyMw=="}, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcwNDc1Ng==", "bodyText": "@adesh-rao, exception from (see how it's done in Initiator)\ntry {\nThread.sleep(cleanerCheckInterval - elapsedTime);\n} catch (InterruptedException ie) {\n// What can I do about it?\n}\nrest looks good to me", "url": "https://github.com/apache/hive/pull/1275#discussion_r460704756", "createdAt": "2020-07-27T07:45:23Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -89,23 +93,28 @@ public void run() {\n         handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n         startedAt = System.currentTimeMillis();\n         long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+        List<CompletableFuture> cleanerList = new ArrayList<>();\n         for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+          cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n         }\n+        CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n       } catch (Throwable t) {\n         LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n+                StringUtils.stringifyException(t));\n+        if (cleanerExecutor != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwMjcyMw=="}, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MjUzOTk5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxMDo0ODo0MVrOG1dF0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxMzowNzowMFrOG2Jolg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwNDMzOQ==", "bodyText": "could you move negation inside: elapsedTime < cleanerCheckInterval && !stop.get()", "url": "https://github.com/apache/hive/pull/1275#discussion_r458704339", "createdAt": "2020-07-22T10:48:41Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -89,23 +93,28 @@ public void run() {\n         handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n         startedAt = System.currentTimeMillis();\n         long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+        List<CompletableFuture> cleanerList = new ArrayList<>();\n         for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+          cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n         }\n+        CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n       } catch (Throwable t) {\n         LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n+                StringUtils.stringifyException(t));\n+        if (cleanerExecutor != null) {\n+          cleanerExecutor.shutdownNow();\n+          cleanerExecutor = CompactorUtil.createExecutorWithThreadFactory(\n+                  conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE), threadNameFormat);\n+        }\n+      } finally {\n         if (handle != null) {\n           handle.releaseLocks();\n         }\n       }\n       // Now, go back to bed until it's time to do this again\n       long elapsedTime = System.currentTimeMillis() - startedAt;\n-      if (elapsedTime >= cleanerCheckInterval || stop.get())  {\n-        continue;\n-      } else {\n+      if (!(elapsedTime >= cleanerCheckInterval || stop.get())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODc1MTQ1OQ==", "bodyText": "fixed.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458751459", "createdAt": "2020-07-22T12:23:39Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -89,23 +93,28 @@ public void run() {\n         handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n         startedAt = System.currentTimeMillis();\n         long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+        List<CompletableFuture> cleanerList = new ArrayList<>();\n         for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+          cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n         }\n+        CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n       } catch (Throwable t) {\n         LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n+                StringUtils.stringifyException(t));\n+        if (cleanerExecutor != null) {\n+          cleanerExecutor.shutdownNow();\n+          cleanerExecutor = CompactorUtil.createExecutorWithThreadFactory(\n+                  conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE), threadNameFormat);\n+        }\n+      } finally {\n         if (handle != null) {\n           handle.releaseLocks();\n         }\n       }\n       // Now, go back to bed until it's time to do this again\n       long elapsedTime = System.currentTimeMillis() - startedAt;\n-      if (elapsedTime >= cleanerCheckInterval || stop.get())  {\n-        continue;\n-      } else {\n+      if (!(elapsedTime >= cleanerCheckInterval || stop.get())) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwNDMzOQ=="}, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk2OTQ1Mw==", "bodyText": "can't see the change", "url": "https://github.com/apache/hive/pull/1275#discussion_r458969453", "createdAt": "2020-07-22T17:41:22Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -89,23 +93,28 @@ public void run() {\n         handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n         startedAt = System.currentTimeMillis();\n         long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+        List<CompletableFuture> cleanerList = new ArrayList<>();\n         for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+          cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n         }\n+        CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n       } catch (Throwable t) {\n         LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n+                StringUtils.stringifyException(t));\n+        if (cleanerExecutor != null) {\n+          cleanerExecutor.shutdownNow();\n+          cleanerExecutor = CompactorUtil.createExecutorWithThreadFactory(\n+                  conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE), threadNameFormat);\n+        }\n+      } finally {\n         if (handle != null) {\n           handle.releaseLocks();\n         }\n       }\n       // Now, go back to bed until it's time to do this again\n       long elapsedTime = System.currentTimeMillis() - startedAt;\n-      if (elapsedTime >= cleanerCheckInterval || stop.get())  {\n-        continue;\n-      } else {\n+      if (!(elapsedTime >= cleanerCheckInterval || stop.get())) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwNDMzOQ=="}, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQzNDEzNA==", "bodyText": "This is outdated. I have added a comment at the right place.", "url": "https://github.com/apache/hive/pull/1275#discussion_r459434134", "createdAt": "2020-07-23T13:07:00Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -89,23 +93,28 @@ public void run() {\n         handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n         startedAt = System.currentTimeMillis();\n         long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+        List<CompletableFuture> cleanerList = new ArrayList<>();\n         for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n+          cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                  clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n         }\n+        CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n       } catch (Throwable t) {\n         LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n+                StringUtils.stringifyException(t));\n+        if (cleanerExecutor != null) {\n+          cleanerExecutor.shutdownNow();\n+          cleanerExecutor = CompactorUtil.createExecutorWithThreadFactory(\n+                  conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE), threadNameFormat);\n+        }\n+      } finally {\n         if (handle != null) {\n           handle.releaseLocks();\n         }\n       }\n       // Now, go back to bed until it's time to do this again\n       long elapsedTime = System.currentTimeMillis() - startedAt;\n-      if (elapsedTime >= cleanerCheckInterval || stop.get())  {\n-        continue;\n-      } else {\n+      if (!(elapsedTime >= cleanerCheckInterval || stop.get())) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwNDMzOQ=="}, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MjU0NzA4OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxMDo1MDo1MVrOG1dKMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxMjoxMDoxMlrOG1fiaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwNTQ1OQ==", "bodyText": "same as above, what's the purpose of executor service shutdown? it would be needed only when Initiator thread terminates", "url": "https://github.com/apache/hive/pull/1275#discussion_r458705459", "createdAt": "2020-07-22T10:50:51Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "diffHunk": "@@ -166,6 +167,11 @@ public void run() {\n         } catch (Throwable t) {\n           LOG.error(\"Initiator loop caught unexpected exception this time through the loop: \" +\n               StringUtils.stringifyException(t));\n+          if (compactionExecutor != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODc0NDQyNQ==", "bodyText": "same as above. This is to fix the number of threads in case threads of executor are lost. This is similarly done in Worker.java too.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458744425", "createdAt": "2020-07-22T12:10:12Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "diffHunk": "@@ -166,6 +167,11 @@ public void run() {\n         } catch (Throwable t) {\n           LOG.error(\"Initiator loop caught unexpected exception this time through the loop: \" +\n               StringUtils.stringifyException(t));\n+          if (compactionExecutor != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwNTQ1OQ=="}, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MjU1MzEwOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxMDo1Mjo1M1rOG1dN7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxMjoxMjo1NVrOG1fnzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwNjQxNA==", "bodyText": "should it be ThreadUtil?", "url": "https://github.com/apache/hive/pull/1275#discussion_r458706414", "createdAt": "2020-07-22T10:52:53Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorUtil.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.txn.compactor;\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+\n+public class CompactorUtil {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODc0NTgwNA==", "bodyText": "Currently this only contains thread utility methods, but intention of this class is to have compactor utility methods which might be needed in future.", "url": "https://github.com/apache/hive/pull/1275#discussion_r458745804", "createdAt": "2020-07-22T12:12:55Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorUtil.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.txn.compactor;\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+\n+public class CompactorUtil {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODcwNjQxNA=="}, "originalCommit": {"oid": "d36b661a6f02c0584bdbad59785ab1d5d38da07c"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NDI0MTkyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxNzo1Mjo0MVrOG1tsZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxMzowNjozNlrOG2JnmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk3NjM1OA==", "bodyText": "move this under init, including cleanerCheckInterval initialization", "url": "https://github.com/apache/hive/pull/1275#discussion_r458976358", "createdAt": "2020-07-22T17:52:41Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -79,7 +82,9 @@ public void run() {\n       cleanerCheckInterval = conf.getTimeVar(\n           HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n     }\n-\n+    ExecutorService cleanerExecutor = CompactorUtil.createExecutorWithThreadFactory(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1029d56a3867daa29035c67e8eca3fcec9080f7"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQzMzg4MQ==", "bodyText": "Done.", "url": "https://github.com/apache/hive/pull/1275#discussion_r459433881", "createdAt": "2020-07-23T13:06:36Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -79,7 +82,9 @@ public void run() {\n       cleanerCheckInterval = conf.getTimeVar(\n           HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n     }\n-\n+    ExecutorService cleanerExecutor = CompactorUtil.createExecutorWithThreadFactory(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk3NjM1OA=="}, "originalCommit": {"oid": "f1029d56a3867daa29035c67e8eca3fcec9080f7"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NDI1MjgyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxNzo1NToyNlrOG1tzQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxMzowNjoyOVrOG2JnSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk3ODExNQ==", "bodyText": "move this under init", "url": "https://github.com/apache/hive/pull/1275#discussion_r458978115", "createdAt": "2020-07-22T17:55:26Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "diffHunk": "@@ -87,6 +87,9 @@\n   public void run() {\n     // Make sure nothing escapes this run method and kills the metastore at large,\n     // so wrap it in a big catch Throwable statement.\n+    ExecutorService compactionExecutor = CompactorUtil.createExecutorWithThreadFactory(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1029d56a3867daa29035c67e8eca3fcec9080f7"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQzMzgwMQ==", "bodyText": "Done.", "url": "https://github.com/apache/hive/pull/1275#discussion_r459433801", "createdAt": "2020-07-23T13:06:29Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java", "diffHunk": "@@ -87,6 +87,9 @@\n   public void run() {\n     // Make sure nothing escapes this run method and kills the metastore at large,\n     // so wrap it in a big catch Throwable statement.\n+    ExecutorService compactionExecutor = CompactorUtil.createExecutorWithThreadFactory(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk3ODExNQ=="}, "originalCommit": {"oid": "f1029d56a3867daa29035c67e8eca3fcec9080f7"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NzIyMzUwOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxMzowNTozMVrOG2JkdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxMzowNTozMVrOG2JkdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTQzMzA3Nw==", "bodyText": "@deniskuzZ This condition is modified. The previous comment is there on the outdated code.", "url": "https://github.com/apache/hive/pull/1275#discussion_r459433077", "createdAt": "2020-07-23T13:05:31Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -66,53 +69,62 @@\n   private long cleanerCheckInterval = 0;\n \n   private ReplChangeManager replChangeManager;\n+  private ExecutorService cleanerExecutor;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n-  }\n-\n-  @Override\n-  public void run() {\n     if (cleanerCheckInterval == 0) {\n       cleanerCheckInterval = conf.getTimeVar(\n-          HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n+              HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n     }\n+    cleanerExecutor = CompactorUtil.createExecutorWithThreadFactory(\n+            conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE),\n+            COMPACTOR_CLEANER_THREAD_NAME_FORMAT);\n+  }\n \n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n-        }\n-      } catch (Throwable t) {\n-        LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n-        if (handle != null) {\n-          handle.releaseLocks();\n-        }\n-      }\n-      // Now, go back to bed until it's time to do this again\n-      long elapsedTime = System.currentTimeMillis() - startedAt;\n-      if (elapsedTime >= cleanerCheckInterval || stop.get())  {\n-        continue;\n-      } else {\n+  @Override\n+  public void run() {\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n         try {\n-          Thread.sleep(cleanerCheckInterval - elapsedTime);\n-        } catch (InterruptedException ie) {\n-          // What can I do about it?\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          List<CompletableFuture> cleanerList = new ArrayList<>();\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                    clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n+          }\n+          CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n+        } catch (Throwable t) {\n+          LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n+                  StringUtils.stringifyException(t));\n+        } finally {\n+          if (handle != null) {\n+            handle.releaseLocks();\n+          }\n+        }\n+        // Now, go back to bed until it's time to do this again\n+        long elapsedTime = System.currentTimeMillis() - startedAt;\n+        if (elapsedTime < cleanerCheckInterval && !stop.get()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "001595fed553b5eec519e244c74e75d071f3ddf4"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3NTk2ODYzOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwNzo0ODo0M1rOG3XSoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwNDoxNDoxNFrOG377lQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcwNjQ2NQ==", "bodyText": "i think, this if check is redundant.", "url": "https://github.com/apache/hive/pull/1275#discussion_r460706465", "createdAt": "2020-07-27T07:48:43Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -66,53 +69,62 @@\n   private long cleanerCheckInterval = 0;\n \n   private ReplChangeManager replChangeManager;\n+  private ExecutorService cleanerExecutor;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n-  }\n-\n-  @Override\n-  public void run() {\n     if (cleanerCheckInterval == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c0798db8730e422a9a4bbf00dd81472ee5e9825"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTMwNjc3Mw==", "bodyText": "Yes, this is redundant. Removed it.", "url": "https://github.com/apache/hive/pull/1275#discussion_r461306773", "createdAt": "2020-07-28T04:14:14Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -66,53 +69,62 @@\n   private long cleanerCheckInterval = 0;\n \n   private ReplChangeManager replChangeManager;\n+  private ExecutorService cleanerExecutor;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n-  }\n-\n-  @Override\n-  public void run() {\n     if (cleanerCheckInterval == 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDcwNjQ2NQ=="}, "originalCommit": {"oid": "2c0798db8730e422a9a4bbf00dd81472ee5e9825"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MDAwNDk1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "isResolved": true, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwNTozNTozMFrOG39TJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxMzo1Njo1MFrOG45rwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTMyOTE4OQ==", "bodyText": "InterruptedException catch redundant as well (see Initiator)", "url": "https://github.com/apache/hive/pull/1275#discussion_r461329189", "createdAt": "2020-07-28T05:35:30Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -66,53 +69,60 @@\n   private long cleanerCheckInterval = 0;\n \n   private ReplChangeManager replChangeManager;\n+  private ExecutorService cleanerExecutor;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n+    cleanerCheckInterval = conf.getTimeVar(\n+            HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n+    cleanerExecutor = CompactorUtil.createExecutorWithThreadFactory(\n+            conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE),\n+            COMPACTOR_CLEANER_THREAD_NAME_FORMAT);\n   }\n \n   @Override\n   public void run() {\n-    if (cleanerCheckInterval == 0) {\n-      cleanerCheckInterval = conf.getTimeVar(\n-          HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n-    }\n-\n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n-        }\n-      } catch (Throwable t) {\n-        LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n-        if (handle != null) {\n-          handle.releaseLocks();\n-        }\n-      }\n-      // Now, go back to bed until it's time to do this again\n-      long elapsedTime = System.currentTimeMillis() - startedAt;\n-      if (elapsedTime >= cleanerCheckInterval || stop.get())  {\n-        continue;\n-      } else {\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n         try {\n-          Thread.sleep(cleanerCheckInterval - elapsedTime);\n-        } catch (InterruptedException ie) {\n-          // What can I do about it?\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          List<CompletableFuture> cleanerList = new ArrayList<>();\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                    clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n+          }\n+          CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n+        } catch (Throwable t) {\n+          LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n+                  StringUtils.stringifyException(t));\n+        } finally {\n+          if (handle != null) {\n+            handle.releaseLocks();\n+          }\n         }\n+        // Now, go back to bed until it's time to do this again\n+        long elapsedTime = System.currentTimeMillis() - startedAt;\n+        if (elapsedTime < cleanerCheckInterval && !stop.get()) {\n+          try {\n+            Thread.sleep(cleanerCheckInterval - elapsedTime);\n+          } catch (InterruptedException ie) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6ef5ddca2ea3009ceb99140079a6d694c642c17"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTM0MDM1MQ==", "bodyText": "I don't think it is redundant.\nIn initiator, the try-catch (catching throwable) is applied on the complete while loop. Since thread.sleep is inside while loop, it was redundant in initiator. (Though, sleep throws an Interrupted exception, we will get out of while loop and initiator will exit).\nIn case of cleaner, the try-catch (catching throwable) is applied only on the main logic for cleaning directories. This is inside while loop (as compared to complete while loop for initiator). Here, even if sleep throws Interrupted exception, Cleaner won't exit because of a separate try-catch statement inside while loop.", "url": "https://github.com/apache/hive/pull/1275#discussion_r461340351", "createdAt": "2020-07-28T06:09:57Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -66,53 +69,60 @@\n   private long cleanerCheckInterval = 0;\n \n   private ReplChangeManager replChangeManager;\n+  private ExecutorService cleanerExecutor;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n+    cleanerCheckInterval = conf.getTimeVar(\n+            HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n+    cleanerExecutor = CompactorUtil.createExecutorWithThreadFactory(\n+            conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE),\n+            COMPACTOR_CLEANER_THREAD_NAME_FORMAT);\n   }\n \n   @Override\n   public void run() {\n-    if (cleanerCheckInterval == 0) {\n-      cleanerCheckInterval = conf.getTimeVar(\n-          HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n-    }\n-\n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n-        }\n-      } catch (Throwable t) {\n-        LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n-        if (handle != null) {\n-          handle.releaseLocks();\n-        }\n-      }\n-      // Now, go back to bed until it's time to do this again\n-      long elapsedTime = System.currentTimeMillis() - startedAt;\n-      if (elapsedTime >= cleanerCheckInterval || stop.get())  {\n-        continue;\n-      } else {\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n         try {\n-          Thread.sleep(cleanerCheckInterval - elapsedTime);\n-        } catch (InterruptedException ie) {\n-          // What can I do about it?\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          List<CompletableFuture> cleanerList = new ArrayList<>();\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                    clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n+          }\n+          CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n+        } catch (Throwable t) {\n+          LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n+                  StringUtils.stringifyException(t));\n+        } finally {\n+          if (handle != null) {\n+            handle.releaseLocks();\n+          }\n         }\n+        // Now, go back to bed until it's time to do this again\n+        long elapsedTime = System.currentTimeMillis() - startedAt;\n+        if (elapsedTime < cleanerCheckInterval && !stop.get()) {\n+          try {\n+            Thread.sleep(cleanerCheckInterval - elapsedTime);\n+          } catch (InterruptedException ie) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTMyOTE4OQ=="}, "originalCommit": {"oid": "c6ef5ddca2ea3009ceb99140079a6d694c642c17"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjA2NzE1Nw==", "bodyText": "@adesh-rao, it should behave same way as in Initiator. if you interrupt the thread - it should be cleanly interrupted. To be honest i don't see the difference. try-catch in Cleaner covers main do-while loop in Thread.run. Am I missing something here?", "url": "https://github.com/apache/hive/pull/1275#discussion_r462067157", "createdAt": "2020-07-29T06:28:31Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -66,53 +69,60 @@\n   private long cleanerCheckInterval = 0;\n \n   private ReplChangeManager replChangeManager;\n+  private ExecutorService cleanerExecutor;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n+    cleanerCheckInterval = conf.getTimeVar(\n+            HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n+    cleanerExecutor = CompactorUtil.createExecutorWithThreadFactory(\n+            conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE),\n+            COMPACTOR_CLEANER_THREAD_NAME_FORMAT);\n   }\n \n   @Override\n   public void run() {\n-    if (cleanerCheckInterval == 0) {\n-      cleanerCheckInterval = conf.getTimeVar(\n-          HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n-    }\n-\n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n-        }\n-      } catch (Throwable t) {\n-        LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n-        if (handle != null) {\n-          handle.releaseLocks();\n-        }\n-      }\n-      // Now, go back to bed until it's time to do this again\n-      long elapsedTime = System.currentTimeMillis() - startedAt;\n-      if (elapsedTime >= cleanerCheckInterval || stop.get())  {\n-        continue;\n-      } else {\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n         try {\n-          Thread.sleep(cleanerCheckInterval - elapsedTime);\n-        } catch (InterruptedException ie) {\n-          // What can I do about it?\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          List<CompletableFuture> cleanerList = new ArrayList<>();\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                    clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n+          }\n+          CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n+        } catch (Throwable t) {\n+          LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n+                  StringUtils.stringifyException(t));\n+        } finally {\n+          if (handle != null) {\n+            handle.releaseLocks();\n+          }\n         }\n+        // Now, go back to bed until it's time to do this again\n+        long elapsedTime = System.currentTimeMillis() - startedAt;\n+        if (elapsedTime < cleanerCheckInterval && !stop.get()) {\n+          try {\n+            Thread.sleep(cleanerCheckInterval - elapsedTime);\n+          } catch (InterruptedException ie) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTMyOTE4OQ=="}, "originalCommit": {"oid": "c6ef5ddca2ea3009ceb99140079a6d694c642c17"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjA3MzAyNw==", "bodyText": "There is no catch in Cleaner, its just try-finally.", "url": "https://github.com/apache/hive/pull/1275#discussion_r462073027", "createdAt": "2020-07-29T06:43:14Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -66,53 +69,60 @@\n   private long cleanerCheckInterval = 0;\n \n   private ReplChangeManager replChangeManager;\n+  private ExecutorService cleanerExecutor;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n+    cleanerCheckInterval = conf.getTimeVar(\n+            HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n+    cleanerExecutor = CompactorUtil.createExecutorWithThreadFactory(\n+            conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE),\n+            COMPACTOR_CLEANER_THREAD_NAME_FORMAT);\n   }\n \n   @Override\n   public void run() {\n-    if (cleanerCheckInterval == 0) {\n-      cleanerCheckInterval = conf.getTimeVar(\n-          HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n-    }\n-\n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n-        }\n-      } catch (Throwable t) {\n-        LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n-        if (handle != null) {\n-          handle.releaseLocks();\n-        }\n-      }\n-      // Now, go back to bed until it's time to do this again\n-      long elapsedTime = System.currentTimeMillis() - startedAt;\n-      if (elapsedTime >= cleanerCheckInterval || stop.get())  {\n-        continue;\n-      } else {\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n         try {\n-          Thread.sleep(cleanerCheckInterval - elapsedTime);\n-        } catch (InterruptedException ie) {\n-          // What can I do about it?\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          List<CompletableFuture> cleanerList = new ArrayList<>();\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                    clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n+          }\n+          CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n+        } catch (Throwable t) {\n+          LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n+                  StringUtils.stringifyException(t));\n+        } finally {\n+          if (handle != null) {\n+            handle.releaseLocks();\n+          }\n         }\n+        // Now, go back to bed until it's time to do this again\n+        long elapsedTime = System.currentTimeMillis() - startedAt;\n+        if (elapsedTime < cleanerCheckInterval && !stop.get()) {\n+          try {\n+            Thread.sleep(cleanerCheckInterval - elapsedTime);\n+          } catch (InterruptedException ie) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTMyOTE4OQ=="}, "originalCommit": {"oid": "c6ef5ddca2ea3009ceb99140079a6d694c642c17"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjA5MDM5Ng==", "bodyText": "Catch there is used to log the exceptions, you don't have any logging right now. I think, it's answer for \"// What can I do about it?\" comment.", "url": "https://github.com/apache/hive/pull/1275#discussion_r462090396", "createdAt": "2020-07-29T07:20:09Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -66,53 +69,60 @@\n   private long cleanerCheckInterval = 0;\n \n   private ReplChangeManager replChangeManager;\n+  private ExecutorService cleanerExecutor;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n+    cleanerCheckInterval = conf.getTimeVar(\n+            HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n+    cleanerExecutor = CompactorUtil.createExecutorWithThreadFactory(\n+            conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE),\n+            COMPACTOR_CLEANER_THREAD_NAME_FORMAT);\n   }\n \n   @Override\n   public void run() {\n-    if (cleanerCheckInterval == 0) {\n-      cleanerCheckInterval = conf.getTimeVar(\n-          HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n-    }\n-\n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n-        }\n-      } catch (Throwable t) {\n-        LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n-        if (handle != null) {\n-          handle.releaseLocks();\n-        }\n-      }\n-      // Now, go back to bed until it's time to do this again\n-      long elapsedTime = System.currentTimeMillis() - startedAt;\n-      if (elapsedTime >= cleanerCheckInterval || stop.get())  {\n-        continue;\n-      } else {\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n         try {\n-          Thread.sleep(cleanerCheckInterval - elapsedTime);\n-        } catch (InterruptedException ie) {\n-          // What can I do about it?\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          List<CompletableFuture> cleanerList = new ArrayList<>();\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                    clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n+          }\n+          CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n+        } catch (Throwable t) {\n+          LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n+                  StringUtils.stringifyException(t));\n+        } finally {\n+          if (handle != null) {\n+            handle.releaseLocks();\n+          }\n         }\n+        // Now, go back to bed until it's time to do this again\n+        long elapsedTime = System.currentTimeMillis() - startedAt;\n+        if (elapsedTime < cleanerCheckInterval && !stop.get()) {\n+          try {\n+            Thread.sleep(cleanerCheckInterval - elapsedTime);\n+          } catch (InterruptedException ie) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTMyOTE4OQ=="}, "originalCommit": {"oid": "c6ef5ddca2ea3009ceb99140079a6d694c642c17"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjEzMzAxOA==", "bodyText": "Added the logging in catch.", "url": "https://github.com/apache/hive/pull/1275#discussion_r462133018", "createdAt": "2020-07-29T08:34:53Z", "author": {"login": "adesh-rao"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -66,53 +69,60 @@\n   private long cleanerCheckInterval = 0;\n \n   private ReplChangeManager replChangeManager;\n+  private ExecutorService cleanerExecutor;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n+    cleanerCheckInterval = conf.getTimeVar(\n+            HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n+    cleanerExecutor = CompactorUtil.createExecutorWithThreadFactory(\n+            conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE),\n+            COMPACTOR_CLEANER_THREAD_NAME_FORMAT);\n   }\n \n   @Override\n   public void run() {\n-    if (cleanerCheckInterval == 0) {\n-      cleanerCheckInterval = conf.getTimeVar(\n-          HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n-    }\n-\n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n-        }\n-      } catch (Throwable t) {\n-        LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n-        if (handle != null) {\n-          handle.releaseLocks();\n-        }\n-      }\n-      // Now, go back to bed until it's time to do this again\n-      long elapsedTime = System.currentTimeMillis() - startedAt;\n-      if (elapsedTime >= cleanerCheckInterval || stop.get())  {\n-        continue;\n-      } else {\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n         try {\n-          Thread.sleep(cleanerCheckInterval - elapsedTime);\n-        } catch (InterruptedException ie) {\n-          // What can I do about it?\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          List<CompletableFuture> cleanerList = new ArrayList<>();\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                    clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n+          }\n+          CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n+        } catch (Throwable t) {\n+          LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n+                  StringUtils.stringifyException(t));\n+        } finally {\n+          if (handle != null) {\n+            handle.releaseLocks();\n+          }\n         }\n+        // Now, go back to bed until it's time to do this again\n+        long elapsedTime = System.currentTimeMillis() - startedAt;\n+        if (elapsedTime < cleanerCheckInterval && !stop.get()) {\n+          try {\n+            Thread.sleep(cleanerCheckInterval - elapsedTime);\n+          } catch (InterruptedException ie) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTMyOTE4OQ=="}, "originalCommit": {"oid": "c6ef5ddca2ea3009ceb99140079a6d694c642c17"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMxODUyOQ==", "bodyText": "do you think it's ok to have un-interruptible thread ? in current situation if someone interrupts the Cleaner thread it will just go right away with next clean attempt", "url": "https://github.com/apache/hive/pull/1275#discussion_r462318529", "createdAt": "2020-07-29T13:56:50Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java", "diffHunk": "@@ -66,53 +69,60 @@\n   private long cleanerCheckInterval = 0;\n \n   private ReplChangeManager replChangeManager;\n+  private ExecutorService cleanerExecutor;\n \n   @Override\n   public void init(AtomicBoolean stop) throws Exception {\n     super.init(stop);\n     replChangeManager = ReplChangeManager.getInstance(conf);\n+    cleanerCheckInterval = conf.getTimeVar(\n+            HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n+    cleanerExecutor = CompactorUtil.createExecutorWithThreadFactory(\n+            conf.getIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE),\n+            COMPACTOR_CLEANER_THREAD_NAME_FORMAT);\n   }\n \n   @Override\n   public void run() {\n-    if (cleanerCheckInterval == 0) {\n-      cleanerCheckInterval = conf.getTimeVar(\n-          HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n-    }\n-\n-    do {\n-      TxnStore.MutexAPI.LockHandle handle = null;\n-      long startedAt = -1;\n-      // Make sure nothing escapes this run method and kills the metastore at large,\n-      // so wrap it in a big catch Throwable statement.\n-      try {\n-        handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n-        startedAt = System.currentTimeMillis();\n-        long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n-        for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n-          clean(compactionInfo, minOpenTxnId);\n-        }\n-      } catch (Throwable t) {\n-        LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n-            StringUtils.stringifyException(t));\n-      }\n-      finally {\n-        if (handle != null) {\n-          handle.releaseLocks();\n-        }\n-      }\n-      // Now, go back to bed until it's time to do this again\n-      long elapsedTime = System.currentTimeMillis() - startedAt;\n-      if (elapsedTime >= cleanerCheckInterval || stop.get())  {\n-        continue;\n-      } else {\n+    try {\n+      do {\n+        TxnStore.MutexAPI.LockHandle handle = null;\n+        long startedAt = -1;\n+        // Make sure nothing escapes this run method and kills the metastore at large,\n+        // so wrap it in a big catch Throwable statement.\n         try {\n-          Thread.sleep(cleanerCheckInterval - elapsedTime);\n-        } catch (InterruptedException ie) {\n-          // What can I do about it?\n+          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Cleaner.name());\n+          startedAt = System.currentTimeMillis();\n+          long minOpenTxnId = txnHandler.findMinOpenTxnIdForCleaner();\n+          List<CompletableFuture> cleanerList = new ArrayList<>();\n+          for(CompactionInfo compactionInfo : txnHandler.findReadyToClean()) {\n+            cleanerList.add(CompletableFuture.runAsync(CompactorUtil.ThrowingRunnable.unchecked(() ->\n+                    clean(compactionInfo, minOpenTxnId)), cleanerExecutor));\n+          }\n+          CompletableFuture.allOf(cleanerList.toArray(new CompletableFuture[0])).join();\n+        } catch (Throwable t) {\n+          LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n+                  StringUtils.stringifyException(t));\n+        } finally {\n+          if (handle != null) {\n+            handle.releaseLocks();\n+          }\n         }\n+        // Now, go back to bed until it's time to do this again\n+        long elapsedTime = System.currentTimeMillis() - startedAt;\n+        if (elapsedTime < cleanerCheckInterval && !stop.get()) {\n+          try {\n+            Thread.sleep(cleanerCheckInterval - elapsedTime);\n+          } catch (InterruptedException ie) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTMyOTE4OQ=="}, "originalCommit": {"oid": "c6ef5ddca2ea3009ceb99140079a6d694c642c17"}, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MDU0MTgzOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorUtil.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwODozNDo0MFrOG4CYAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwODozNDo0MFrOG4CYAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQxMjM1Mg==", "bodyText": "nit: Make it 2 space tabs.", "url": "https://github.com/apache/hive/pull/1275#discussion_r461412352", "createdAt": "2020-07-28T08:34:40Z", "author": {"login": "sankarh"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorUtil.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.txn.compactor;\n+\n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+\n+public class CompactorUtil {\n+    public interface ThrowingRunnable<E extends Exception> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6ef5ddca2ea3009ceb99140079a6d694c642c17"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4MDU5NjM5OnYy", "diffSide": "RIGHT", "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwODo0OTowNlrOG4C6LQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQwODo0OTowNlrOG4C6LQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTQyMTEwMQ==", "bodyText": "The config name can be relevant. It actually represent how many threads that we use for parallelly run the cleaner. But, the name sounds like Queue name. Can we change it to \"HIVE_COMPACTOR_CLEANER_THREADS_NUM\"?", "url": "https://github.com/apache/hive/pull/1275#discussion_r461421101", "createdAt": "2020-07-28T08:49:06Z", "author": {"login": "sankarh"}, "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "diffHunk": "@@ -3028,6 +3028,9 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n \n     HIVE_COMPACTOR_CLEANER_RUN_INTERVAL(\"hive.compactor.cleaner.run.interval\", \"5000ms\",\n         new TimeValidator(TimeUnit.MILLISECONDS), \"Time between runs of the cleaner thread\"),\n+    HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE(\"hive.compactor.cleaner.request.queue\", 1,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6ef5ddca2ea3009ceb99140079a6d694c642c17"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NTEwNTA3OnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwODoxNDozN1rOG4tk8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwODoxNDozN1rOG4tk8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjEyMDE3Nw==", "bodyText": "nit: Make it \"for (int i = 0; i < 10; i++)\". Check other places in this patch.", "url": "https://github.com/apache/hive/pull/1275#discussion_r462120177", "createdAt": "2020-07-29T08:14:37Z", "author": {"login": "sankarh"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java", "diffHunk": "@@ -274,6 +285,55 @@ public void droppedPartition() throws Exception {\n     ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n     Assert.assertEquals(0, rsp.getCompactsSize());\n   }\n+\n+  @Test\n+  public void processCompactionCandidatesInParallel() throws Exception {\n+    Table t = newTable(\"default\", \"camipc\", true);\n+    List<Partition> partitions = new ArrayList<>();\n+    Partition p = null;\n+    for(int i=0; i<10; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f16fd20aafe062e5c70cfacc22a944591210f68"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NTExMjczOnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwODoxNjoyNVrOG4tpaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwODo0NjoyMFrOG4uwpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjEyMTMyMw==", "bodyText": "nit: Need a blank line after closing braces,", "url": "https://github.com/apache/hive/pull/1275#discussion_r462121323", "createdAt": "2020-07-29T08:16:25Z", "author": {"login": "sankarh"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java", "diffHunk": "@@ -274,6 +285,55 @@ public void droppedPartition() throws Exception {\n     ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n     Assert.assertEquals(0, rsp.getCompactsSize());\n   }\n+\n+  @Test\n+  public void processCompactionCandidatesInParallel() throws Exception {\n+    Table t = newTable(\"default\", \"camipc\", true);\n+    List<Partition> partitions = new ArrayList<>();\n+    Partition p = null;\n+    for(int i=0; i<10; i++) {\n+      p = newPartition(t, \"today\" + i);\n+\n+      addBaseFile(t, p, 20L, 20);\n+      addDeltaFile(t, p, 21L, 22L, 2);\n+      addDeltaFile(t, p, 23L, 24L, 2);\n+      addDeltaFile(t, p, 21L, 24L, 4);\n+      partitions.add(p);\n+    }\n+    burnThroughTransactions(\"default\", \"camipc\", 25);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f16fd20aafe062e5c70cfacc22a944591210f68"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjEzOTU1Ng==", "bodyText": "Done.", "url": "https://github.com/apache/hive/pull/1275#discussion_r462139556", "createdAt": "2020-07-29T08:46:20Z", "author": {"login": "adesh-rao"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java", "diffHunk": "@@ -274,6 +285,55 @@ public void droppedPartition() throws Exception {\n     ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n     Assert.assertEquals(0, rsp.getCompactsSize());\n   }\n+\n+  @Test\n+  public void processCompactionCandidatesInParallel() throws Exception {\n+    Table t = newTable(\"default\", \"camipc\", true);\n+    List<Partition> partitions = new ArrayList<>();\n+    Partition p = null;\n+    for(int i=0; i<10; i++) {\n+      p = newPartition(t, \"today\" + i);\n+\n+      addBaseFile(t, p, 20L, 20);\n+      addDeltaFile(t, p, 21L, 22L, 2);\n+      addDeltaFile(t, p, 23L, 24L, 2);\n+      addDeltaFile(t, p, 21L, 24L, 4);\n+      partitions.add(p);\n+    }\n+    burnThroughTransactions(\"default\", \"camipc\", 25);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjEyMTMyMw=="}, "originalCommit": {"oid": "5f16fd20aafe062e5c70cfacc22a944591210f68"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NTExNzk5OnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwODoxNzozNlrOG4tsdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQwODoxNzozNlrOG4tsdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjEyMjEwMA==", "bodyText": "nit: Keep the code block of \"if\", \"else if\" and \"else\" in new line with a tab space.", "url": "https://github.com/apache/hive/pull/1275#discussion_r462122100", "createdAt": "2020-07-29T08:17:36Z", "author": {"login": "sankarh"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java", "diffHunk": "@@ -274,6 +285,55 @@ public void droppedPartition() throws Exception {\n     ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n     Assert.assertEquals(0, rsp.getCompactsSize());\n   }\n+\n+  @Test\n+  public void processCompactionCandidatesInParallel() throws Exception {\n+    Table t = newTable(\"default\", \"camipc\", true);\n+    List<Partition> partitions = new ArrayList<>();\n+    Partition p = null;\n+    for(int i=0; i<10; i++) {\n+      p = newPartition(t, \"today\" + i);\n+\n+      addBaseFile(t, p, 20L, 20);\n+      addDeltaFile(t, p, 21L, 22L, 2);\n+      addDeltaFile(t, p, 23L, 24L, 2);\n+      addDeltaFile(t, p, 21L, 24L, 4);\n+      partitions.add(p);\n+    }\n+    burnThroughTransactions(\"default\", \"camipc\", 25);\n+    for(int i=0; i<10; i++) {\n+      CompactionRequest rqst = new CompactionRequest(\"default\", \"camipc\", CompactionType.MINOR);\n+      rqst.setPartitionname(\"ds=today\"+i);\n+      txnHandler.compact(rqst);\n+      CompactionInfo ci = txnHandler.findNextToCompact(\"fred\");\n+      ci.runAs = System.getProperty(\"user.name\");\n+      txnHandler.updateCompactorState(ci, openTxn());\n+      txnHandler.markCompacted(ci);\n+    }\n+\n+    conf.setIntVar(HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_REQUEST_QUEUE, 3);\n+    startCleaner();\n+\n+    // Check there are no compactions requests left.\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(10, rsp.getCompactsSize());\n+    Assert.assertTrue(TxnStore.SUCCEEDED_RESPONSE.equals(rsp.getCompacts().get(0).getState()));\n+\n+    // Check that the files are removed\n+    for (Partition pa : partitions) {\n+      List<Path> paths = getDirectories(conf, t, pa);\n+      Assert.assertEquals(2, paths.size());\n+      boolean sawBase = false, sawDelta = false;\n+      for (Path path : paths) {\n+        if (path.getName().equals(\"base_20\")) sawBase = true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f16fd20aafe062e5c70cfacc22a944591210f68"}, "originalPosition": 74}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 598, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}