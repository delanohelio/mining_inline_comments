{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE4NzQyMTkw", "number": 1660, "reviewThreads": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxMjoyMjozNVrOFBwKIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNTo0MjoyMFrOFB2BIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3MzgxOTIzOnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxMjoyMjozNVrOIAkPsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNDozOTo1NFrOIBgFjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzQ2NDc1Mw==", "bodyText": "Is this change expected? What are these records?", "url": "https://github.com/apache/hive/pull/1660#discussion_r537464753", "createdAt": "2020-12-07T12:22:35Z", "author": {"login": "pvargacl"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java", "diffHunk": "@@ -1747,15 +1747,15 @@ public void testMultiInsertOnDynamicallyPartitionedMmTable() throws Exception {\n     final String completedTxnComponentsContents =\n         TxnDbUtil.queryToString(conf, \"select * from \\\"COMPLETED_TXN_COMPONENTS\\\"\");\n     Assert.assertEquals(completedTxnComponentsContents,\n-        2, TxnDbUtil.countQueryAgent(conf, \"select count(*) from \\\"COMPLETED_TXN_COMPONENTS\\\"\"));\n+        4, TxnDbUtil.countQueryAgent(conf, \"select count(*) from \\\"COMPLETED_TXN_COMPONENTS\\\"\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ0NTE5Ng==", "bodyText": "Those records are duplicates. It is a \"side-effect\" of fixing the FileSinkOperator-MoveTask assignment.\nFor ACID tables for an insert like in the test, 4 records were created even before the direct insert got introduced. Because then the FSO-MoveTask assignment was based on the staging directories. And for insert like this there were 2 FSOs and 2 MoveTasks. Each MoveTasks called the metastore method which creates an entry in the TXN_COMPONENTS table for each partition. So there were 4 records at the end of the insert. But for MM tables (and later for direct insert) there is no staging directory and all MoveTasks and all FSOs will contain the table directory. So for every FSO it will find the same MoveTask (which is the first in the list) and only this one will be executed. This is not correct, but didn't cause any issue, so it was undetected until the direct delete and update came in. To make them work properly, had to fix the FSO-MoveTask assignment, but then for MM tables and with direct insert it will have duplicate records just like for ACID tables without direct insert. The Java doc of the TxnHandler.addDynamicPartitions method says that duplicates won't cause any trouble, but if you know issues with that, please share it with me.", "url": "https://github.com/apache/hive/pull/1660#discussion_r538445196", "createdAt": "2020-12-08T14:39:54Z", "author": {"login": "kuczoram"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java", "diffHunk": "@@ -1747,15 +1747,15 @@ public void testMultiInsertOnDynamicallyPartitionedMmTable() throws Exception {\n     final String completedTxnComponentsContents =\n         TxnDbUtil.queryToString(conf, \"select * from \\\"COMPLETED_TXN_COMPONENTS\\\"\");\n     Assert.assertEquals(completedTxnComponentsContents,\n-        2, TxnDbUtil.countQueryAgent(conf, \"select count(*) from \\\"COMPLETED_TXN_COMPONENTS\\\"\"));\n+        4, TxnDbUtil.countQueryAgent(conf, \"select count(*) from \\\"COMPLETED_TXN_COMPONENTS\\\"\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzQ2NDc1Mw=="}, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NDQ2NzI2OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/Context.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNDo0MzozN1rOIAqGOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNDo0NjozN1rOIBgh5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU2MDYzMw==", "bodyText": "Could we start it from 0?", "url": "https://github.com/apache/hive/pull/1660#discussion_r537560633", "createdAt": "2020-12-07T14:43:37Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Context.java", "diffHunk": "@@ -105,6 +105,7 @@\n \n   private Configuration conf;\n   protected int pathid = 10000;\n+  private int moveTaskId = 100;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ1MjQ1NQ==", "bodyText": "Sure! Fixed it.", "url": "https://github.com/apache/hive/pull/1660#discussion_r538452455", "createdAt": "2020-12-08T14:46:37Z", "author": {"login": "kuczoram"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Context.java", "diffHunk": "@@ -105,6 +105,7 @@\n \n   private Configuration conf;\n   protected int pathid = 10000;\n+  private int moveTaskId = 100;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU2MDYzMw=="}, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NDUwNjIxOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNDo1MDo1MVrOIAqczA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNDo0OToxOVrOIBgtqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU2NjQxMg==", "bodyText": "Maybe javadoc instead of a comment?", "url": "https://github.com/apache/hive/pull/1660#discussion_r537566412", "createdAt": "2020-12-07T14:50:51Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java", "diffHunk": "@@ -189,6 +191,49 @@ public WriteEntity getAcidAnalyzeTable() {\n     return acidSinks;\n   }\n \n+  public Integer getStatementIdForAcidWriteType(long writeId, String moveTaskId, AcidUtils.Operation acidOperation, Path path) {\n+    FileSinkDesc result = null;\n+    for (FileSinkDesc acidSink : acidSinks) {\n+      if (acidOperation.equals(acidSink.getAcidOperation()) && path.equals(acidSink.getDestPath())\n+          && acidSink.getTableWriteId() == writeId\n+          && (moveTaskId == null || acidSink.getMoveTaskId() == null || moveTaskId.equals(acidSink.getMoveTaskId()))) {\n+        // There is a problem with the union all optimisation. In this case, there will be multiple FileSinkOperators", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ1NTQ2NQ==", "bodyText": "Yeah, it would be better. Added Java doc.", "url": "https://github.com/apache/hive/pull/1660#discussion_r538455465", "createdAt": "2020-12-08T14:49:19Z", "author": {"login": "kuczoram"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java", "diffHunk": "@@ -189,6 +191,49 @@ public WriteEntity getAcidAnalyzeTable() {\n     return acidSinks;\n   }\n \n+  public Integer getStatementIdForAcidWriteType(long writeId, String moveTaskId, AcidUtils.Operation acidOperation, Path path) {\n+    FileSinkDesc result = null;\n+    for (FileSinkDesc acidSink : acidSinks) {\n+      if (acidOperation.equals(acidSink.getAcidOperation()) && path.equals(acidSink.getDestPath())\n+          && acidSink.getTableWriteId() == writeId\n+          && (moveTaskId == null || acidSink.getMoveTaskId() == null || moveTaskId.equals(acidSink.getMoveTaskId()))) {\n+        // There is a problem with the union all optimisation. In this case, there will be multiple FileSinkOperators", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU2NjQxMg=="}, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NDUxMTAxOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNDo1MTo0MlrOIAqfmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNTowNjowMVrOIBh2dQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU2NzEzMQ==", "bodyText": "Javadoc", "url": "https://github.com/apache/hive/pull/1660#discussion_r537567131", "createdAt": "2020-12-07T14:51:42Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java", "diffHunk": "@@ -189,6 +191,49 @@ public WriteEntity getAcidAnalyzeTable() {\n     return acidSinks;\n   }\n \n+  public Integer getStatementIdForAcidWriteType(long writeId, String moveTaskId, AcidUtils.Operation acidOperation, Path path) {\n+    FileSinkDesc result = null;\n+    for (FileSinkDesc acidSink : acidSinks) {\n+      if (acidOperation.equals(acidSink.getAcidOperation()) && path.equals(acidSink.getDestPath())\n+          && acidSink.getTableWriteId() == writeId\n+          && (moveTaskId == null || acidSink.getMoveTaskId() == null || moveTaskId.equals(acidSink.getMoveTaskId()))) {\n+        // There is a problem with the union all optimisation. In this case, there will be multiple FileSinkOperators\n+        // with the same operation, writeId and moveTaskId. But one of these FSOs doesn't write data and its statementId\n+        // is not valid, so if this FSO is selected and its statementId is returned, the file listing will find nothing.\n+        // So check the acidSinks and if two of them have the same writeId, path and moveTaskId, then return -1 as statementId.\n+        // Like this, the file listing will find all partitions and files correctly.\n+        if (result != null) {\n+          return -1;\n+        }\n+        result = acidSink;\n+      }\n+    }\n+    if (result != null) {\n+      return result.getStatementId();\n+    } else {\n+      return -1;\n+    }\n+  }\n+\n+  public Set<String> getDynamicPartitionSpecs(long writeId, String moveTaskId, AcidUtils.Operation acidOperation, Path path) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ3NDEwMQ==", "bodyText": "Added it.", "url": "https://github.com/apache/hive/pull/1660#discussion_r538474101", "createdAt": "2020-12-08T15:06:01Z", "author": {"login": "kuczoram"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java", "diffHunk": "@@ -189,6 +191,49 @@ public WriteEntity getAcidAnalyzeTable() {\n     return acidSinks;\n   }\n \n+  public Integer getStatementIdForAcidWriteType(long writeId, String moveTaskId, AcidUtils.Operation acidOperation, Path path) {\n+    FileSinkDesc result = null;\n+    for (FileSinkDesc acidSink : acidSinks) {\n+      if (acidOperation.equals(acidSink.getAcidOperation()) && path.equals(acidSink.getDestPath())\n+          && acidSink.getTableWriteId() == writeId\n+          && (moveTaskId == null || acidSink.getMoveTaskId() == null || moveTaskId.equals(acidSink.getMoveTaskId()))) {\n+        // There is a problem with the union all optimisation. In this case, there will be multiple FileSinkOperators\n+        // with the same operation, writeId and moveTaskId. But one of these FSOs doesn't write data and its statementId\n+        // is not valid, so if this FSO is selected and its statementId is returned, the file listing will find nothing.\n+        // So check the acidSinks and if two of them have the same writeId, path and moveTaskId, then return -1 as statementId.\n+        // Like this, the file listing will find all partitions and files correctly.\n+        if (result != null) {\n+          return -1;\n+        }\n+        result = acidSink;\n+      }\n+    }\n+    if (result != null) {\n+      return result.getStatementId();\n+    } else {\n+      return -1;\n+    }\n+  }\n+\n+  public Set<String> getDynamicPartitionSpecs(long writeId, String moveTaskId, AcidUtils.Operation acidOperation, Path path) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU2NzEzMQ=="}, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NDU0NjA4OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNDo1Nzo1MFrOIAqz9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNjoxODo1NVrOIBm4gA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU3MjM0Mw==", "bodyText": "Could we do something like a switch?", "url": "https://github.com/apache/hive/pull/1660#discussion_r537572343", "createdAt": "2020-12-07T14:57:50Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "diffHunk": "@@ -232,9 +236,25 @@ public void closeWriters(boolean abort) throws HiveException {\n       for (int i = 0; i < updaters.length; i++) {\n         if (updaters[i] != null) {\n           SerDeStats stats = updaters[i].getStats();\n-          // Ignore 0 row files except in case of insert overwrite\n-          if (isDirectInsert && (stats.getRowCount() > 0 || isInsertOverwrite)) {\n-            outPathsCommitted[i] = updaters[i].getUpdatedFilePath();\n+          // Ignore 0 row files except in case of insert overwrite or delete or update\n+          if (isDirectInsert\n+              && (stats.getRowCount() > 0 || isInsertOverwrite || AcidUtils.Operation.DELETE.equals(acidOperation)\n+                  || AcidUtils.Operation.UPDATE.equals(acidOperation))) {\n+            // In case of delete operation, the deleteFilePath has to be used, not the updatedFilePath\n+            // In case of update operation, we need both paths. The updateFilePath will be added\n+            // to the outPathsCommitted array and the deleteFilePath will be collected in a separate list.\n+            OrcRecordUpdater recordUpdater = (OrcRecordUpdater) updaters[i];\n+            outPathsCommitted[i] = recordUpdater.getUpdatedFilePath();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODU1NjU0NA==", "bodyText": "Sure! Fixed", "url": "https://github.com/apache/hive/pull/1660#discussion_r538556544", "createdAt": "2020-12-08T16:18:55Z", "author": {"login": "kuczoram"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "diffHunk": "@@ -232,9 +236,25 @@ public void closeWriters(boolean abort) throws HiveException {\n       for (int i = 0; i < updaters.length; i++) {\n         if (updaters[i] != null) {\n           SerDeStats stats = updaters[i].getStats();\n-          // Ignore 0 row files except in case of insert overwrite\n-          if (isDirectInsert && (stats.getRowCount() > 0 || isInsertOverwrite)) {\n-            outPathsCommitted[i] = updaters[i].getUpdatedFilePath();\n+          // Ignore 0 row files except in case of insert overwrite or delete or update\n+          if (isDirectInsert\n+              && (stats.getRowCount() > 0 || isInsertOverwrite || AcidUtils.Operation.DELETE.equals(acidOperation)\n+                  || AcidUtils.Operation.UPDATE.equals(acidOperation))) {\n+            // In case of delete operation, the deleteFilePath has to be used, not the updatedFilePath\n+            // In case of update operation, we need both paths. The updateFilePath will be added\n+            // to the outPathsCommitted array and the deleteFilePath will be collected in a separate list.\n+            OrcRecordUpdater recordUpdater = (OrcRecordUpdater) updaters[i];\n+            outPathsCommitted[i] = recordUpdater.getUpdatedFilePath();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU3MjM0Mw=="}, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NDU3MTE5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNTowMjozOVrOIArCgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyNjozMlrOIBjNsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU3NjA2Ng==", "bodyText": "Think through 1 more time. I can accept that this is the best solution, but this is UGLY \ud83d\ude04", "url": "https://github.com/apache/hive/pull/1660#discussion_r537576066", "createdAt": "2020-12-07T15:02:39Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "diffHunk": "@@ -251,7 +271,7 @@ public void closeWriters(boolean abort) throws HiveException {\n       }\n     }\n \n-    private void commit(FileSystem fs, List<Path> commitPaths) throws HiveException {\n+    private void commit(FileSystem fs, List<Path> commitPaths, List<Path> deleteDeltas) throws HiveException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5NjQzMg==", "bodyText": "I know, but I don't really know a better solution, only if we change the internal structures in FileSinkOperator. Like using Lists instead of arrays. But this could have unexpected side effects. I am open to try it but I would do it under a separate Jira. I create one about investigating this refactoring.\nhttps://issues.apache.org/jira/browse/HIVE-24505", "url": "https://github.com/apache/hive/pull/1660#discussion_r538496432", "createdAt": "2020-12-08T15:26:32Z", "author": {"login": "kuczoram"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "diffHunk": "@@ -251,7 +271,7 @@ public void closeWriters(boolean abort) throws HiveException {\n       }\n     }\n \n-    private void commit(FileSystem fs, List<Path> commitPaths) throws HiveException {\n+    private void commit(FileSystem fs, List<Path> commitPaths, List<Path> deleteDeltas) throws HiveException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU3NjA2Ng=="}, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NDY1MTQ5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNToxNzo1MlrOIArxmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNTo1MzoxNVrOIBlEnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU4ODEyMw==", "bodyText": "Javadoc please", "url": "https://github.com/apache/hive/pull/1660#discussion_r537588123", "createdAt": "2020-12-07T15:17:52Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -563,6 +564,21 @@ else if (filename.startsWith(BUCKET_PREFIX)) {\n     return result;\n   }\n \n+  public static Map<String, Integer> getDeltaToAttemptIdMap(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUyNjg3Nw==", "bodyText": "Done", "url": "https://github.com/apache/hive/pull/1660#discussion_r538526877", "createdAt": "2020-12-08T15:53:15Z", "author": {"login": "kuczoram"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -563,6 +564,21 @@ else if (filename.startsWith(BUCKET_PREFIX)) {\n     return result;\n   }\n \n+  public static Map<String, Integer> getDeltaToAttemptIdMap(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU4ODEyMw=="}, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NDY4NzMxOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNToyNDo0NVrOIAsGow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNTozMjowOFrOIBjnEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU5MzUwNw==", "bodyText": "Too long line", "url": "https://github.com/apache/hive/pull/1660#discussion_r537593507", "createdAt": "2020-12-07T15:24:45Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java", "diffHunk": "@@ -2694,7 +2699,7 @@ private void constructOneLBLocationMap(FileStatus fSta,\n    */\n   private Set<Path> getValidPartitionsInPath(\n       int numDP, int numLB, Path loadPath, Long writeId, int stmtId,\n-      boolean isMmTable, boolean isInsertOverwrite, boolean isDirectInsert) throws HiveException {\n+      boolean isMmTable, boolean isInsertOverwrite, boolean isDirectInsert, AcidUtils.Operation operation, Set<String> dynamiPartitionSpecs) throws HiveException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUwMjkzMQ==", "bodyText": "Fixed", "url": "https://github.com/apache/hive/pull/1660#discussion_r538502931", "createdAt": "2020-12-08T15:32:08Z", "author": {"login": "kuczoram"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java", "diffHunk": "@@ -2694,7 +2699,7 @@ private void constructOneLBLocationMap(FileStatus fSta,\n    */\n   private Set<Path> getValidPartitionsInPath(\n       int numDP, int numLB, Path loadPath, Long writeId, int stmtId,\n-      boolean isMmTable, boolean isInsertOverwrite, boolean isDirectInsert) throws HiveException {\n+      boolean isMmTable, boolean isInsertOverwrite, boolean isDirectInsert, AcidUtils.Operation operation, Set<String> dynamiPartitionSpecs) throws HiveException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU5MzUwNw=="}, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NDcxMTE1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNToyOTowOVrOIAsUlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNTo0Nzo1NFrOIBkswA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU5NzA3Nw==", "bodyText": "Double check if this is needed", "url": "https://github.com/apache/hive/pull/1660#discussion_r537597077", "createdAt": "2020-12-07T15:29:09Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "diffHunk": "@@ -1895,7 +1901,20 @@ public static boolean isSkewedStoredAsDirs(FileSinkDesc fsInputDesc) {\n       }\n \n       if ((srcDir != null) && srcDir.equals(fsopFinalDir)) {\n-        return mvTsk;\n+        if (isDirectInsert || isMmFsop) {\n+          if (moveTaskId != null && fsoMoveTaskId != null && moveTaskId.equals(fsoMoveTaskId)) {\n+            // If the ACID direct insert is on, the MoveTasks cannot be identified by the srcDir as\n+            // in this case the srcDir is always the root directory of the table.\n+            // We need to consider the ACID write type to identify the MoveTasks.\n+            return mvTsk;\n+          }\n+          if ((moveTaskId == null || fsoMoveTaskId == null) && moveTaskWriteType != null", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUyMDc2OA==", "bodyText": "There was a test which was failing if this was not there, but since then I think I fixed the moveTaskId generation, so cannot be null. It think this is not needed. I will remove it and let's see what the tests say.", "url": "https://github.com/apache/hive/pull/1660#discussion_r538520768", "createdAt": "2020-12-08T15:47:54Z", "author": {"login": "kuczoram"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "diffHunk": "@@ -1895,7 +1901,20 @@ public static boolean isSkewedStoredAsDirs(FileSinkDesc fsInputDesc) {\n       }\n \n       if ((srcDir != null) && srcDir.equals(fsopFinalDir)) {\n-        return mvTsk;\n+        if (isDirectInsert || isMmFsop) {\n+          if (moveTaskId != null && fsoMoveTaskId != null && moveTaskId.equals(fsoMoveTaskId)) {\n+            // If the ACID direct insert is on, the MoveTasks cannot be identified by the srcDir as\n+            // in this case the srcDir is always the root directory of the table.\n+            // We need to consider the ACID write type to identify the MoveTasks.\n+            return mvTsk;\n+          }\n+          if ((moveTaskId == null || fsoMoveTaskId == null) && moveTaskWriteType != null", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU5NzA3Nw=="}, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NDc0MTQzOnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNTozNDo1NlrOIAsmUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNTowNzoxNlrOIBh71g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYwMTYxNw==", "bodyText": "Remove", "url": "https://github.com/apache/hive/pull/1660#discussion_r537601617", "createdAt": "2020-12-07T15:34:56Z", "author": {"login": "pvary"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java", "diffHunk": "@@ -52,6 +52,8 @@\n   @Mock\n   private DataInput mockDataInput;\n \n+  // IRJUNK IDE TESZTET!!!", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ3NTQ3OA==", "bodyText": "Removed.", "url": "https://github.com/apache/hive/pull/1660#discussion_r538475478", "createdAt": "2020-12-08T15:07:16Z", "author": {"login": "kuczoram"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java", "diffHunk": "@@ -52,6 +52,8 @@\n   @Mock\n   private DataInput mockDataInput;\n \n+  // IRJUNK IDE TESZTET!!!", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYwMTYxNw=="}, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NDc3NTY2OnYy", "diffSide": "RIGHT", "path": "ql/src/test/queries/clientpositive/materialized_view_create_rewrite_4.q", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNTo0MTo0MlrOIAs6mA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNTozNjozN1rOIBj7Jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYwNjgwOA==", "bodyText": "Please file a jira", "url": "https://github.com/apache/hive/pull/1660#discussion_r537606808", "createdAt": "2020-12-07T15:41:42Z", "author": {"login": "pvary"}, "path": "ql/src/test/queries/clientpositive/materialized_view_create_rewrite_4.q", "diffHunk": "@@ -3,6 +3,7 @@ set hive.support.concurrency=true;\n set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;\n set hive.strict.checks.cartesian.product=false;\n set hive.materializedview.rewriting=true;\n+set hive.acid.direct.insert.enabled=false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUwODA3MQ==", "bodyText": "Done: https://issues.apache.org/jira/browse/HIVE-24506", "url": "https://github.com/apache/hive/pull/1660#discussion_r538508071", "createdAt": "2020-12-08T15:36:37Z", "author": {"login": "kuczoram"}, "path": "ql/src/test/queries/clientpositive/materialized_view_create_rewrite_4.q", "diffHunk": "@@ -3,6 +3,7 @@ set hive.support.concurrency=true;\n set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;\n set hive.strict.checks.cartesian.product=false;\n set hive.materializedview.rewriting=true;\n+set hive.acid.direct.insert.enabled=false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYwNjgwOA=="}, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NDc3OTIxOnYy", "diffSide": "RIGHT", "path": "ql/src/test/queries/clientpositive/sort_acid.q", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QxNTo0MjoyMFrOIAs8qw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNTozODo0NlrOIBkEzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYwNzMzOQ==", "bodyText": "Can we sort the result file instead?", "url": "https://github.com/apache/hive/pull/1660#discussion_r537607339", "createdAt": "2020-12-07T15:42:20Z", "author": {"login": "pvary"}, "path": "ql/src/test/queries/clientpositive/sort_acid.q", "diffHunk": "@@ -16,7 +16,7 @@ explain cbo\n update acidtlb set b=777;\n update acidtlb set b=777;\n \n-select * from acidtlb;\n+select * from acidtlb order by a;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUxMDU0Mg==", "bodyText": "Sure, fixed.", "url": "https://github.com/apache/hive/pull/1660#discussion_r538510542", "createdAt": "2020-12-08T15:38:46Z", "author": {"login": "kuczoram"}, "path": "ql/src/test/queries/clientpositive/sort_acid.q", "diffHunk": "@@ -16,7 +16,7 @@ explain cbo\n update acidtlb set b=777;\n update acidtlb set b=777;\n \n-select * from acidtlb;\n+select * from acidtlb order by a;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzYwNzMzOQ=="}, "originalCommit": {"oid": "6f9cfcd37341fe990b4411d56ad3a4d050ec2bca"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 233, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}