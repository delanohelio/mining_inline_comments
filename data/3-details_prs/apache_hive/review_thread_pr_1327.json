{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU3NjM4NzM2", "number": 1327, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwODo1Mzo1NFrOET-2Mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwOTowMDoyMFrOET--hA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mzg4MDgyOnYy", "diffSide": "RIGHT", "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwODo1Mzo1NFrOG6BHJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTo1NjozN1rOG6zeSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4ODgwNw==", "bodyText": "Can we leave a comment why are these settings needed?", "url": "https://github.com/apache/hive/pull/1327#discussion_r463488807", "createdAt": "2020-07-31T08:53:54Z", "author": {"login": "pvary"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java", "diffHunk": "@@ -95,6 +98,10 @@ private void setupTez(HiveConf conf) {\n     conf.set(\"hive.tez.container.size\", \"128\");\n     conf.setBoolean(\"hive.merge.tezfiles\", false);\n     conf.setBoolean(\"hive.in.tez.test\", true);\n+    if (!mmCompaction) {\n+      conf.set(\"tez.grouping.max-size\", \"1024\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxMzkzMQ==", "bodyText": "Sure, added a comment.", "url": "https://github.com/apache/hive/pull/1327#discussion_r464313931", "createdAt": "2020-08-03T09:56:37Z", "author": {"login": "kuczoram"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java", "diffHunk": "@@ -95,6 +98,10 @@ private void setupTez(HiveConf conf) {\n     conf.set(\"hive.tez.container.size\", \"128\");\n     conf.setBoolean(\"hive.merge.tezfiles\", false);\n     conf.setBoolean(\"hive.in.tez.test\", true);\n+    if (!mmCompaction) {\n+      conf.set(\"tez.grouping.max-size\", \"1024\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4ODgwNw=="}, "originalCommit": {"oid": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mzg4NDg4OnYy", "diffSide": "RIGHT", "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwODo1NTowMlrOG6BJhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMjowMDo0MVrOG62zhg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4OTQxNA==", "bodyText": "Could we leave a comment with the expected file structure?", "url": "https://github.com/apache/hive/pull/1327#discussion_r463489414", "createdAt": "2020-07-31T08:55:02Z", "author": {"login": "pvary"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java", "diffHunk": "@@ -217,6 +224,64 @@ void insertTestData(String dbName, String tblName) throws Exception {\n       executeStatementOnDriver(\"delete from \" + tblName + \" where a = '1'\", driver);\n     }\n \n+    void createTableWithoutBucketWithMultipleSplits(String dbName, String tblName, String tempTblName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM2ODUxOA==", "bodyText": "Sure, added a comment.", "url": "https://github.com/apache/hive/pull/1327#discussion_r464368518", "createdAt": "2020-08-03T12:00:41Z", "author": {"login": "kuczoram"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java", "diffHunk": "@@ -217,6 +224,64 @@ void insertTestData(String dbName, String tblName) throws Exception {\n       executeStatementOnDriver(\"delete from \" + tblName + \" where a = '1'\", driver);\n     }\n \n+    void createTableWithoutBucketWithMultipleSplits(String dbName, String tblName, String tempTblName,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4OTQxNA=="}, "originalCommit": {"oid": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mzg5MTQ0OnYy", "diffSide": "RIGHT", "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwODo1NzowNlrOG6BNgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMjowMjozNFrOG622jg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5MDQzNA==", "bodyText": "I have seen issues with queries using virtual columns (INPUT__FILE__NAME, ROW__ID). The row number in the results were different with and without the virtual columns. - This is just a note, maybe no action is needed here", "url": "https://github.com/apache/hive/pull/1327#discussion_r463490434", "createdAt": "2020-07-31T08:57:06Z", "author": {"login": "pvary"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java", "diffHunk": "@@ -261,22 +326,77 @@ protected void insertMmTestData(String tblName, int iterations) throws Exception\n     }\n \n     List<String> getAllData(String tblName) throws Exception {\n-      return getAllData(null, tblName);\n+      return getAllData(null, tblName, false);\n     }\n \n-    List<String> getAllData(String dbName, String tblName) throws Exception {\n+    List<String> getAllData(String tblName, boolean withRowId) throws Exception {\n+      return getAllData(null, tblName, withRowId);\n+    }\n+\n+    List<String> getAllData(String dbName, String tblName, boolean withRowId) throws Exception {\n       if (dbName != null) {\n         tblName = dbName + \".\" + tblName;\n       }\n-      List<String> result = executeStatementOnDriverAndReturnResults(\"select * from \" + tblName, driver);\n+      StringBuffer query = new StringBuffer();\n+      query.append(\"select \");\n+      if (withRowId) {\n+        query.append(\"ROW__ID, \");\n+      }\n+      query.append(\"* from \");\n+      query.append(tblName);\n+      List<String> result = executeStatementOnDriverAndReturnResults(query.toString(), driver);\n       Collections.sort(result);\n       return result;\n     }\n \n+    List<String> getDataWithInputFileNames(String dbName, String tblName) throws Exception {\n+      if (dbName != null) {\n+        tblName = dbName + \".\" + tblName;\n+      }\n+      StringBuffer query = new StringBuffer();\n+      query.append(\"select \");\n+      query.append(\"INPUT__FILE__NAME, a from \");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM2OTI5NA==", "bodyText": "Thanks a lot for the info. I will check, but I haven't seen any issues with this. Maybe because I don't check the row number here, just the file names?", "url": "https://github.com/apache/hive/pull/1327#discussion_r464369294", "createdAt": "2020-08-03T12:02:34Z", "author": {"login": "kuczoram"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorOnTezTest.java", "diffHunk": "@@ -261,22 +326,77 @@ protected void insertMmTestData(String tblName, int iterations) throws Exception\n     }\n \n     List<String> getAllData(String tblName) throws Exception {\n-      return getAllData(null, tblName);\n+      return getAllData(null, tblName, false);\n     }\n \n-    List<String> getAllData(String dbName, String tblName) throws Exception {\n+    List<String> getAllData(String tblName, boolean withRowId) throws Exception {\n+      return getAllData(null, tblName, withRowId);\n+    }\n+\n+    List<String> getAllData(String dbName, String tblName, boolean withRowId) throws Exception {\n       if (dbName != null) {\n         tblName = dbName + \".\" + tblName;\n       }\n-      List<String> result = executeStatementOnDriverAndReturnResults(\"select * from \" + tblName, driver);\n+      StringBuffer query = new StringBuffer();\n+      query.append(\"select \");\n+      if (withRowId) {\n+        query.append(\"ROW__ID, \");\n+      }\n+      query.append(\"* from \");\n+      query.append(tblName);\n+      List<String> result = executeStatementOnDriverAndReturnResults(query.toString(), driver);\n       Collections.sort(result);\n       return result;\n     }\n \n+    List<String> getDataWithInputFileNames(String dbName, String tblName) throws Exception {\n+      if (dbName != null) {\n+        tblName = dbName + \".\" + tblName;\n+      }\n+      StringBuffer query = new StringBuffer();\n+      query.append(\"select \");\n+      query.append(\"INPUT__FILE__NAME, a from \");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5MDQzNA=="}, "originalCommit": {"oid": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1"}, "originalPosition": 129}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mzg5ODgwOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwODo1OToyNlrOG6BSDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMjoxMToxNlrOG63F0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5MTU5OA==", "bodyText": "Are the results in order? Can we close writers for old buckets? That could save memory", "url": "https://github.com/apache/hive/pull/1327#discussion_r463491598", "createdAt": "2020-07-31T08:59:26Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "diffHunk": "@@ -1063,7 +1076,11 @@ public void process(Object row, int tag) throws HiveException {\n       // RecordUpdater expects to get the actual row, not a serialized version of it.  Thus we\n       // pass the row rather than recordValue.\n       if (conf.getWriteType() == AcidUtils.Operation.NOT_ACID || conf.isMmTable() || conf.isCompactionTable()) {\n-        rowOutWriters[findWriterOffset(row)].write(recordValue);\n+        writerOffset = bucketId;\n+        if (!conf.isCompactionTable()) {\n+          writerOffset = findWriterOffset(row);\n+        }\n+        rowOutWriters[writerOffset].write(recordValue);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDM3MzIwMg==", "bodyText": "They should be in order, because the result temp table for the compaction is created like \"clustered by (bucket) sorted by (bucket, originalTransaction, rowId) into 10 buckets\". I would assume that because of this, the rows in the table will be in order by bucket, originalTransaction and rowId. I haven't seen otherwise during my testing.\nI don't think we can close the writers here, because they will be used in the closeOp method as well and they are closed there.", "url": "https://github.com/apache/hive/pull/1327#discussion_r464373202", "createdAt": "2020-08-03T12:11:16Z", "author": {"login": "kuczoram"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "diffHunk": "@@ -1063,7 +1076,11 @@ public void process(Object row, int tag) throws HiveException {\n       // RecordUpdater expects to get the actual row, not a serialized version of it.  Thus we\n       // pass the row rather than recordValue.\n       if (conf.getWriteType() == AcidUtils.Operation.NOT_ACID || conf.isMmTable() || conf.isCompactionTable()) {\n-        rowOutWriters[findWriterOffset(row)].write(recordValue);\n+        writerOffset = bucketId;\n+        if (!conf.isCompactionTable()) {\n+          writerOffset = findWriterOffset(row);\n+        }\n+        rowOutWriters[writerOffset].write(recordValue);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5MTU5OA=="}, "originalCommit": {"oid": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MzkwMjEyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/QueryCompactor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwOTowMDoyMFrOG6BUBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNTo1MDoyM1rOG6-7pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5MjEwMw==", "bodyText": "Can we leave a comment why this settings are needed?", "url": "https://github.com/apache/hive/pull/1327#discussion_r463492103", "createdAt": "2020-07-31T09:00:20Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/QueryCompactor.java", "diffHunk": "@@ -115,6 +115,10 @@ void runCompactionQueries(HiveConf conf, String tmpTableName, StorageDescriptor\n       }\n       for (String query : compactionQueries) {\n         LOG.info(\"Running {} compaction via query: {}\", compactionInfo.isMajorCompaction() ? \"major\" : \"minor\", query);\n+        if (!compactionInfo.isMajorCompaction()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDUwMTY3MQ==", "bodyText": "Sure, added a comment.", "url": "https://github.com/apache/hive/pull/1327#discussion_r464501671", "createdAt": "2020-08-03T15:50:23Z", "author": {"login": "kuczoram"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/QueryCompactor.java", "diffHunk": "@@ -115,6 +115,10 @@ void runCompactionQueries(HiveConf conf, String tmpTableName, StorageDescriptor\n       }\n       for (String query : compactionQueries) {\n         LOG.info(\"Running {} compaction via query: {}\", compactionInfo.isMajorCompaction() ? \"major\" : \"minor\", query);\n+        if (!compactionInfo.isMajorCompaction()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5MjEwMw=="}, "originalCommit": {"oid": "f9f07ac6639a7fd652a31b92f708e73d33c3f4c1"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 465, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}