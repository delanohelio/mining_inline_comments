{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMxOTQxMDkz", "number": 1738, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwODo1OTozOVrOFAskAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNDowMDowOVrOFDfedw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM2Mjc0NDM0OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwODo1OTozOVrOH_HJAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwODo1OTozOVrOH_HJAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTkzOTMyOQ==", "bodyText": "In a follow up Jira it might be worth change this whole AcidUtils approach and start to put everything from the beginning in a DirectoryImpl, so the argument count could be decreased to a sane amount.", "url": "https://github.com/apache/hive/pull/1738#discussion_r535939329", "createdAt": "2020-12-04T08:59:39Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -1369,14 +1383,14 @@ private static Directory getAcidState(FileSystem fileSystem, Path candidateDirec\n     if (childrenWithId != null) {\n       for (HdfsFileStatusWithId child : childrenWithId) {\n         getChildState(child, writeIdList, working, originalDirectories, original, obsolete,\n-            bestBase, ignoreEmptyFiles, abortedDirectories, abortedWriteIds, fs, validTxnList);\n+            bestBase, ignoreEmptyFiles, abortedDirectories, abortedWriteIds, uncompactedAborts, fs, validTxnList);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM2MjkxOTY3OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwOTo0MDoyMFrOH_IuuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QwODo1MTozMVrOIAb99g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk2NTM2OA==", "bodyText": "It's starting to affect readability, maybe refactor in the following patches.", "url": "https://github.com/apache/hive/pull/1738#discussion_r535965368", "createdAt": "2020-12-04T09:40:20Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -566,20 +567,20 @@ else if (filename.startsWith(BUCKET_PREFIX)) {\n   public static final class DirectoryImpl implements Directory {\n     private final List<Path> abortedDirectories;\n     private final Set<Long> abortedWriteIds;\n+    private final boolean uncompactedAborts;\n     private final boolean isBaseInRawFormat;\n     private final List<HdfsFileStatusWithId> original;\n     private final List<Path> obsolete;\n     private final List<ParsedDelta> deltas;\n     private final Path base;\n     private List<HdfsFileStatusWithId> baseFiles;\n \n-    public DirectoryImpl(List<Path> abortedDirectories, Set<Long> abortedWriteIds,\n-        boolean isBaseInRawFormat, List<HdfsFileStatusWithId> original,\n-        List<Path> obsolete, List<ParsedDelta> deltas, Path base) {\n-      this.abortedDirectories = abortedDirectories == null ?\n-          Collections.emptyList() : abortedDirectories;\n-      this.abortedWriteIds = abortedWriteIds == null ?\n-        Collections.emptySet() : abortedWriteIds;\n+    public DirectoryImpl(List<Path> abortedDirectories, Set<Long> abortedWriteIds, boolean uncompactedAborts,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzMyOTE0Mg==", "bodyText": "Yes, I plan to do that. I think I will make DirectoryImpl mutable, and pass it down the getAcidState calls, to gather all the info in it along the way.", "url": "https://github.com/apache/hive/pull/1738#discussion_r537329142", "createdAt": "2020-12-07T08:51:31Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -566,20 +567,20 @@ else if (filename.startsWith(BUCKET_PREFIX)) {\n   public static final class DirectoryImpl implements Directory {\n     private final List<Path> abortedDirectories;\n     private final Set<Long> abortedWriteIds;\n+    private final boolean uncompactedAborts;\n     private final boolean isBaseInRawFormat;\n     private final List<HdfsFileStatusWithId> original;\n     private final List<Path> obsolete;\n     private final List<ParsedDelta> deltas;\n     private final Path base;\n     private List<HdfsFileStatusWithId> baseFiles;\n \n-    public DirectoryImpl(List<Path> abortedDirectories, Set<Long> abortedWriteIds,\n-        boolean isBaseInRawFormat, List<HdfsFileStatusWithId> original,\n-        List<Path> obsolete, List<ParsedDelta> deltas, Path base) {\n-      this.abortedDirectories = abortedDirectories == null ?\n-          Collections.emptyList() : abortedDirectories;\n-      this.abortedWriteIds = abortedWriteIds == null ?\n-        Collections.emptySet() : abortedWriteIds;\n+    public DirectoryImpl(List<Path> abortedDirectories, Set<Long> abortedWriteIds, boolean uncompactedAborts,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk2NTM2OA=="}, "originalCommit": {"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5MTk0ODM1OnYy", "diffSide": "RIGHT", "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMzozNzo1OVrOIDJofw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMzozNzo1OVrOIDJofw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDE3NDQ2Mw==", "bodyText": "expectation msg copy-paste - \"there should be single record for the 2nd aborted txn\"", "url": "https://github.com/apache/hive/pull/1738#discussion_r540174463", "createdAt": "2020-12-10T13:37:59Z", "author": {"login": "deniskuzZ"}, "path": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/txn/compactor/TestCompactor.java", "diffHunk": "@@ -1177,6 +1177,104 @@ private HiveStreamingConnection prepareTableTwoPartitionsAndConnection(String db\n         .connect();\n   }\n \n+  /**\n+   * There is a special case handled in Compaction Worker that will skip compaction\n+   * if there is only one valid delta. But this compaction will be still cleaned up, if there are aborted directories.\n+   * @see Worker.isEnoughToCompact\n+   * However if no compaction was done, deltas containing mixed aborted / committed writes from streaming can not be cleaned\n+   * and the metadata belonging to those aborted transactions can not be removed.\n+   * @throws Exception ex\n+   */\n+  @Test\n+  public void testSkippedCompactionCleanerKeepsAborted() throws Exception {\n+    String dbName = \"default\";\n+    String tblName = \"cws\";\n+\n+    String agentInfo = \"UT_\" + Thread.currentThread().getName();\n+    TxnStore txnHandler = TxnUtils.getTxnStore(conf);\n+\n+    executeStatementOnDriver(\"drop table if exists \" + tblName, driver);\n+    executeStatementOnDriver(\"CREATE TABLE \" + tblName + \"(b STRING) \" +\n+        \" PARTITIONED BY (a INT) STORED AS ORC  TBLPROPERTIES ('transactional'='true')\", driver);\n+    executeStatementOnDriver(\"alter table \" + tblName + \" add partition(a=1)\", driver);\n+\n+    StrictDelimitedInputWriter writer = StrictDelimitedInputWriter.newBuilder()\n+        .withFieldDelimiter(',')\n+        .build();\n+\n+    // Create initial aborted txn\n+    HiveStreamingConnection connection = HiveStreamingConnection.newBuilder()\n+        .withDatabase(dbName)\n+        .withTable(tblName)\n+        .withStaticPartitionValues(Collections.singletonList(\"1\"))\n+        .withAgentInfo(agentInfo)\n+        .withHiveConf(conf)\n+        .withRecordWriter(writer)\n+        .withStreamingOptimizations(true)\n+        .withTransactionBatchSize(1)\n+        .connect();\n+\n+    connection.beginTransaction();\n+    connection.write(\"3,1\".getBytes());\n+    connection.write(\"4,1\".getBytes());\n+    connection.abortTransaction();\n+\n+    connection.close();\n+\n+    // Create a sequence of commit, abort, commit to the same delta folder\n+    connection = HiveStreamingConnection.newBuilder()\n+        .withDatabase(dbName)\n+        .withTable(tblName)\n+        .withStaticPartitionValues(Collections.singletonList(\"1\"))\n+        .withAgentInfo(agentInfo)\n+        .withHiveConf(conf)\n+        .withRecordWriter(writer)\n+        .withStreamingOptimizations(true)\n+        .withTransactionBatchSize(3)\n+        .connect();\n+\n+    connection.beginTransaction();\n+    connection.write(\"1,1\".getBytes());\n+    connection.write(\"2,1\".getBytes());\n+    connection.commitTransaction();\n+\n+    connection.beginTransaction();\n+    connection.write(\"3,1\".getBytes());\n+    connection.write(\"4,1\".getBytes());\n+    connection.abortTransaction();\n+\n+    connection.beginTransaction();\n+    connection.write(\"5,1\".getBytes());\n+    connection.write(\"6,1\".getBytes());\n+    connection.commitTransaction();\n+\n+    connection.close();\n+\n+    // Check that aborted are not read back\n+    driver.run(\"select * from cws\");\n+    List res = new ArrayList();\n+    driver.getFetchTask().fetch(res);\n+    Assert.assertEquals(4, res.size());\n+\n+    int count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(\"There should be 2 record for two aborted transaction\", 2, count);\n+\n+    // Start a compaction, that will be skipped, because only one valid delta is there\n+    driver.run(\"alter table cws partition(a='1') compact 'minor'\");\n+    runWorker(conf);\n+    // Cleaner should not delete info about aborted txn 2\n+    runCleaner(conf);\n+    txnHandler.cleanEmptyAbortedAndCommittedTxns();\n+    count = TxnDbUtil.countQueryAgent(conf, \"select count(*) from TXN_COMPONENTS\");\n+    Assert.assertEquals(\"There should be 1 record for two aborted transaction\", 1, count);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5MjA1NzUxOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNDowMDowOVrOIDKn-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNDowMDowOVrOIDKn-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDE5MDcxNQ==", "bodyText": "fix the javadoc", "url": "https://github.com/apache/hive/pull/1738#discussion_r540190715", "createdAt": "2020-12-10T14:00:09Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -923,6 +929,13 @@ public String toString() {\n      * @return the list of aborted writeIds\n      */\n     Set<Long> getAbortedWriteIds();\n+\n+    /**\n+     * Get the list of writeIds that belong to aborted transactions, but can not be cleaned,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84ae5d2b45da3ff9c26dc1c13fcc05eddc15cf34"}, "originalPosition": 54}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 132, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}