{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMxMDUwMzI1", "number": 1073, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNToyNjozMFrOEEu3vA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTo0MzozOFrOEEveYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMzk3NjkyOnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/ACIDBenchmarks.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNToyNjozMFrOGiib9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxMzowMjozNVrOGzSIHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg2ODk4MQ==", "bodyText": "rename to doSetup as it's global and not iteration scoped", "url": "https://github.com/apache/hive/pull/1073#discussion_r438868981", "createdAt": "2020-06-11T15:26:30Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/ACIDBenchmarks.java", "diffHunk": "@@ -0,0 +1,247 @@\n+package org.apache.hadoop.hive.metastore.tools;\n+\n+import org.apache.hadoop.hive.metastore.api.DataOperationType;\n+import org.apache.hadoop.hive.metastore.api.LockComponent;\n+import org.apache.hadoop.hive.metastore.api.LockRequest;\n+import org.apache.logging.log4j.Level;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.core.LoggerContext;\n+import org.apache.logging.log4j.core.config.Configuration;\n+import org.apache.thrift.TException;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.TearDown;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.hadoop.hive.metastore.tools.BenchmarkUtils.createManyTables;\n+import static org.apache.hadoop.hive.metastore.tools.BenchmarkUtils.dropManyTables;\n+import static org.apache.hadoop.hive.metastore.tools.Util.throwingSupplierWrapper;\n+\n+public class ACIDBenchmarks {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(CoreContext.class);\n+\n+  @State(Scope.Benchmark)\n+  public static class CoreContext {\n+    @Param(\"1\")\n+    protected int howMany;\n+\n+    @State(Scope.Thread)\n+    public static class ThreadState {\n+      HMSClient client;\n+\n+      @Setup\n+      public void doSetup() throws Exception {\n+        LOG.debug(\"Creating client\");\n+        client = HMSConfig.getInstance().newClient();\n+      }\n+\n+      @TearDown\n+      public void doTearDown() throws Exception {\n+        client.close();\n+        LOG.debug(\"Closed a connection to metastore.\");\n+      }\n+    }\n+\n+    @Setup\n+    public void setup() {\n+      LoggerContext ctx = (LoggerContext) LogManager.getContext(false);\n+      Configuration ctxConfig = ctx.getConfiguration();\n+      ctxConfig.getLoggerConfig(CoreContext.class.getName()).setLevel(Level.INFO);\n+      ctx.updateLoggers(ctxConfig);\n+    }\n+  }\n+\n+  @State(Scope.Benchmark)\n+  public static class TestOpenTxn extends CoreContext {\n+\n+    @State(Scope.Thread)\n+    public static class ThreadState extends CoreContext.ThreadState {\n+      List<Long> openTxns = new ArrayList<>();\n+\n+      @TearDown\n+      public void doTearDown() throws Exception {\n+        client.abortTxns(openTxns);\n+        LOG.debug(\"aborted all opened txns\");\n+      }\n+\n+      void addTxn(List<Long> openTxn) {\n+        openTxns.addAll(openTxn);\n+      }\n+    }\n+\n+    @Benchmark\n+    public void openTxn(TestOpenTxn.ThreadState state) throws TException {\n+      state.addTxn(state.client.openTxn(howMany));\n+      LOG.debug(\"opened txns, count=\", howMany);\n+    }\n+  }\n+\n+  @State(Scope.Benchmark)\n+  public static class TestLocking extends CoreContext {\n+    private int nTables;\n+\n+    @Param(\"0\")\n+    private int nPartitions;\n+\n+    private List<LockComponent> lockComponents;\n+\n+    @Setup\n+    public void setup() {\n+      this.nTables = (nPartitions != 0) ? howMany / nPartitions : howMany;\n+      createLockComponents();\n+    }\n+\n+    @State(Scope.Thread)\n+    public static class ThreadState extends CoreContext.ThreadState {\n+      List<Long> openTxns = new ArrayList<>();\n+      long txnId;\n+\n+      @Setup(org.openjdk.jmh.annotations.Level.Invocation)\n+      public void iterSetup() {\n+        txnId = executeOpenTxnAndGetTxnId(client);\n+        LOG.debug(\"opened txn, id={}\", txnId);\n+        openTxns.add(txnId);\n+      }\n+\n+      @TearDown\n+      public void doTearDown() throws Exception {\n+        client.abortTxns(openTxns);\n+        if (BenchmarkUtils.checkTxnsCleaned(client, openTxns) == false) {\n+          LOG.error(\"Something went wrong with the cleanup of txns\");\n+        }\n+        LOG.debug(\"aborted all opened txns\");\n+      }\n+    }\n+\n+    @Benchmark\n+    public void lock(TestLocking.ThreadState state) {\n+      LOG.debug(\"sending lock request\");\n+      executeLock(state.client, state.txnId, lockComponents);\n+    }\n+\n+    private void createLockComponents() {\n+      lockComponents = new ArrayList<>();\n+\n+      for (int i = 0; i < nTables; i++) {\n+        for (int j = 0; j < nPartitions - (nPartitions > 1 ? 1 : 0); j++) {\n+          lockComponents.add(\n+            new Util.LockComponentBuilder()\n+              .setDbName(\"default\")\n+              .setTableName(String.format(\"tmp_table_%d\", i))\n+              .setPartitionName(\"p_\" + j)\n+              .setShared()\n+              .setOperationType(DataOperationType.SELECT)\n+              .build());\n+        }\n+        if (nPartitions != 1) {\n+          lockComponents.add(\n+            new Util.LockComponentBuilder()\n+              .setDbName(\"default\")\n+              .setTableName(String.format(\"tmp_table_%d\", i))\n+              .setShared()\n+              .setOperationType(DataOperationType.SELECT)\n+              .build());\n+        }\n+      }\n+    }\n+\n+    private static long executeOpenTxnAndGetTxnId(HMSClient client) {\n+      return throwingSupplierWrapper(() -> client.openTxn(1).get(0));\n+    }\n+\n+    private void executeLock(HMSClient client, long txnId, List<LockComponent> lockComponents) {\n+      LockRequest req = new LockRequest(lockComponents, \"hclient\", \"localhost\");\n+      req.setTxnid(txnId);\n+      throwingSupplierWrapper(() -> client.lock(req));\n+    }\n+  }\n+\n+  @State(Scope.Benchmark)\n+  public static class TestAllocateTableWriteIds extends CoreContext {\n+    String dbName = \"test_db\";\n+    String tblName = \"tmp_table\";\n+\n+    @State(Scope.Thread)\n+    public static class ThreadState extends CoreContext.ThreadState {\n+      List<Long> openTxns = new ArrayList<>();\n+      long txnId;\n+\n+      @Setup\n+      public void iterSetup() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd44b6227ea2c56386ab09970a3dfeb2016767d0"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDIyMTk1OQ==", "bodyText": "renaming to simple setup, as doSetup would overwrite the parent doSetup()", "url": "https://github.com/apache/hive/pull/1073#discussion_r440221959", "createdAt": "2020-06-15T14:35:51Z", "author": {"login": "zchovan"}, "path": "standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/ACIDBenchmarks.java", "diffHunk": "@@ -0,0 +1,247 @@\n+package org.apache.hadoop.hive.metastore.tools;\n+\n+import org.apache.hadoop.hive.metastore.api.DataOperationType;\n+import org.apache.hadoop.hive.metastore.api.LockComponent;\n+import org.apache.hadoop.hive.metastore.api.LockRequest;\n+import org.apache.logging.log4j.Level;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.core.LoggerContext;\n+import org.apache.logging.log4j.core.config.Configuration;\n+import org.apache.thrift.TException;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.TearDown;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.hadoop.hive.metastore.tools.BenchmarkUtils.createManyTables;\n+import static org.apache.hadoop.hive.metastore.tools.BenchmarkUtils.dropManyTables;\n+import static org.apache.hadoop.hive.metastore.tools.Util.throwingSupplierWrapper;\n+\n+public class ACIDBenchmarks {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(CoreContext.class);\n+\n+  @State(Scope.Benchmark)\n+  public static class CoreContext {\n+    @Param(\"1\")\n+    protected int howMany;\n+\n+    @State(Scope.Thread)\n+    public static class ThreadState {\n+      HMSClient client;\n+\n+      @Setup\n+      public void doSetup() throws Exception {\n+        LOG.debug(\"Creating client\");\n+        client = HMSConfig.getInstance().newClient();\n+      }\n+\n+      @TearDown\n+      public void doTearDown() throws Exception {\n+        client.close();\n+        LOG.debug(\"Closed a connection to metastore.\");\n+      }\n+    }\n+\n+    @Setup\n+    public void setup() {\n+      LoggerContext ctx = (LoggerContext) LogManager.getContext(false);\n+      Configuration ctxConfig = ctx.getConfiguration();\n+      ctxConfig.getLoggerConfig(CoreContext.class.getName()).setLevel(Level.INFO);\n+      ctx.updateLoggers(ctxConfig);\n+    }\n+  }\n+\n+  @State(Scope.Benchmark)\n+  public static class TestOpenTxn extends CoreContext {\n+\n+    @State(Scope.Thread)\n+    public static class ThreadState extends CoreContext.ThreadState {\n+      List<Long> openTxns = new ArrayList<>();\n+\n+      @TearDown\n+      public void doTearDown() throws Exception {\n+        client.abortTxns(openTxns);\n+        LOG.debug(\"aborted all opened txns\");\n+      }\n+\n+      void addTxn(List<Long> openTxn) {\n+        openTxns.addAll(openTxn);\n+      }\n+    }\n+\n+    @Benchmark\n+    public void openTxn(TestOpenTxn.ThreadState state) throws TException {\n+      state.addTxn(state.client.openTxn(howMany));\n+      LOG.debug(\"opened txns, count=\", howMany);\n+    }\n+  }\n+\n+  @State(Scope.Benchmark)\n+  public static class TestLocking extends CoreContext {\n+    private int nTables;\n+\n+    @Param(\"0\")\n+    private int nPartitions;\n+\n+    private List<LockComponent> lockComponents;\n+\n+    @Setup\n+    public void setup() {\n+      this.nTables = (nPartitions != 0) ? howMany / nPartitions : howMany;\n+      createLockComponents();\n+    }\n+\n+    @State(Scope.Thread)\n+    public static class ThreadState extends CoreContext.ThreadState {\n+      List<Long> openTxns = new ArrayList<>();\n+      long txnId;\n+\n+      @Setup(org.openjdk.jmh.annotations.Level.Invocation)\n+      public void iterSetup() {\n+        txnId = executeOpenTxnAndGetTxnId(client);\n+        LOG.debug(\"opened txn, id={}\", txnId);\n+        openTxns.add(txnId);\n+      }\n+\n+      @TearDown\n+      public void doTearDown() throws Exception {\n+        client.abortTxns(openTxns);\n+        if (BenchmarkUtils.checkTxnsCleaned(client, openTxns) == false) {\n+          LOG.error(\"Something went wrong with the cleanup of txns\");\n+        }\n+        LOG.debug(\"aborted all opened txns\");\n+      }\n+    }\n+\n+    @Benchmark\n+    public void lock(TestLocking.ThreadState state) {\n+      LOG.debug(\"sending lock request\");\n+      executeLock(state.client, state.txnId, lockComponents);\n+    }\n+\n+    private void createLockComponents() {\n+      lockComponents = new ArrayList<>();\n+\n+      for (int i = 0; i < nTables; i++) {\n+        for (int j = 0; j < nPartitions - (nPartitions > 1 ? 1 : 0); j++) {\n+          lockComponents.add(\n+            new Util.LockComponentBuilder()\n+              .setDbName(\"default\")\n+              .setTableName(String.format(\"tmp_table_%d\", i))\n+              .setPartitionName(\"p_\" + j)\n+              .setShared()\n+              .setOperationType(DataOperationType.SELECT)\n+              .build());\n+        }\n+        if (nPartitions != 1) {\n+          lockComponents.add(\n+            new Util.LockComponentBuilder()\n+              .setDbName(\"default\")\n+              .setTableName(String.format(\"tmp_table_%d\", i))\n+              .setShared()\n+              .setOperationType(DataOperationType.SELECT)\n+              .build());\n+        }\n+      }\n+    }\n+\n+    private static long executeOpenTxnAndGetTxnId(HMSClient client) {\n+      return throwingSupplierWrapper(() -> client.openTxn(1).get(0));\n+    }\n+\n+    private void executeLock(HMSClient client, long txnId, List<LockComponent> lockComponents) {\n+      LockRequest req = new LockRequest(lockComponents, \"hclient\", \"localhost\");\n+      req.setTxnid(txnId);\n+      throwingSupplierWrapper(() -> client.lock(req));\n+    }\n+  }\n+\n+  @State(Scope.Benchmark)\n+  public static class TestAllocateTableWriteIds extends CoreContext {\n+    String dbName = \"test_db\";\n+    String tblName = \"tmp_table\";\n+\n+    @State(Scope.Thread)\n+    public static class ThreadState extends CoreContext.ThreadState {\n+      List<Long> openTxns = new ArrayList<>();\n+      long txnId;\n+\n+      @Setup\n+      public void iterSetup() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg2ODk4MQ=="}, "originalCommit": {"oid": "cd44b6227ea2c56386ab09970a3dfeb2016767d0"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQyNzU1MA==", "bodyText": "i think, it will execute both, check for example doTearDown in ThreadState confs.", "url": "https://github.com/apache/hive/pull/1073#discussion_r456427550", "createdAt": "2020-07-17T13:02:35Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/ACIDBenchmarks.java", "diffHunk": "@@ -0,0 +1,247 @@\n+package org.apache.hadoop.hive.metastore.tools;\n+\n+import org.apache.hadoop.hive.metastore.api.DataOperationType;\n+import org.apache.hadoop.hive.metastore.api.LockComponent;\n+import org.apache.hadoop.hive.metastore.api.LockRequest;\n+import org.apache.logging.log4j.Level;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.core.LoggerContext;\n+import org.apache.logging.log4j.core.config.Configuration;\n+import org.apache.thrift.TException;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.TearDown;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.hadoop.hive.metastore.tools.BenchmarkUtils.createManyTables;\n+import static org.apache.hadoop.hive.metastore.tools.BenchmarkUtils.dropManyTables;\n+import static org.apache.hadoop.hive.metastore.tools.Util.throwingSupplierWrapper;\n+\n+public class ACIDBenchmarks {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(CoreContext.class);\n+\n+  @State(Scope.Benchmark)\n+  public static class CoreContext {\n+    @Param(\"1\")\n+    protected int howMany;\n+\n+    @State(Scope.Thread)\n+    public static class ThreadState {\n+      HMSClient client;\n+\n+      @Setup\n+      public void doSetup() throws Exception {\n+        LOG.debug(\"Creating client\");\n+        client = HMSConfig.getInstance().newClient();\n+      }\n+\n+      @TearDown\n+      public void doTearDown() throws Exception {\n+        client.close();\n+        LOG.debug(\"Closed a connection to metastore.\");\n+      }\n+    }\n+\n+    @Setup\n+    public void setup() {\n+      LoggerContext ctx = (LoggerContext) LogManager.getContext(false);\n+      Configuration ctxConfig = ctx.getConfiguration();\n+      ctxConfig.getLoggerConfig(CoreContext.class.getName()).setLevel(Level.INFO);\n+      ctx.updateLoggers(ctxConfig);\n+    }\n+  }\n+\n+  @State(Scope.Benchmark)\n+  public static class TestOpenTxn extends CoreContext {\n+\n+    @State(Scope.Thread)\n+    public static class ThreadState extends CoreContext.ThreadState {\n+      List<Long> openTxns = new ArrayList<>();\n+\n+      @TearDown\n+      public void doTearDown() throws Exception {\n+        client.abortTxns(openTxns);\n+        LOG.debug(\"aborted all opened txns\");\n+      }\n+\n+      void addTxn(List<Long> openTxn) {\n+        openTxns.addAll(openTxn);\n+      }\n+    }\n+\n+    @Benchmark\n+    public void openTxn(TestOpenTxn.ThreadState state) throws TException {\n+      state.addTxn(state.client.openTxn(howMany));\n+      LOG.debug(\"opened txns, count=\", howMany);\n+    }\n+  }\n+\n+  @State(Scope.Benchmark)\n+  public static class TestLocking extends CoreContext {\n+    private int nTables;\n+\n+    @Param(\"0\")\n+    private int nPartitions;\n+\n+    private List<LockComponent> lockComponents;\n+\n+    @Setup\n+    public void setup() {\n+      this.nTables = (nPartitions != 0) ? howMany / nPartitions : howMany;\n+      createLockComponents();\n+    }\n+\n+    @State(Scope.Thread)\n+    public static class ThreadState extends CoreContext.ThreadState {\n+      List<Long> openTxns = new ArrayList<>();\n+      long txnId;\n+\n+      @Setup(org.openjdk.jmh.annotations.Level.Invocation)\n+      public void iterSetup() {\n+        txnId = executeOpenTxnAndGetTxnId(client);\n+        LOG.debug(\"opened txn, id={}\", txnId);\n+        openTxns.add(txnId);\n+      }\n+\n+      @TearDown\n+      public void doTearDown() throws Exception {\n+        client.abortTxns(openTxns);\n+        if (BenchmarkUtils.checkTxnsCleaned(client, openTxns) == false) {\n+          LOG.error(\"Something went wrong with the cleanup of txns\");\n+        }\n+        LOG.debug(\"aborted all opened txns\");\n+      }\n+    }\n+\n+    @Benchmark\n+    public void lock(TestLocking.ThreadState state) {\n+      LOG.debug(\"sending lock request\");\n+      executeLock(state.client, state.txnId, lockComponents);\n+    }\n+\n+    private void createLockComponents() {\n+      lockComponents = new ArrayList<>();\n+\n+      for (int i = 0; i < nTables; i++) {\n+        for (int j = 0; j < nPartitions - (nPartitions > 1 ? 1 : 0); j++) {\n+          lockComponents.add(\n+            new Util.LockComponentBuilder()\n+              .setDbName(\"default\")\n+              .setTableName(String.format(\"tmp_table_%d\", i))\n+              .setPartitionName(\"p_\" + j)\n+              .setShared()\n+              .setOperationType(DataOperationType.SELECT)\n+              .build());\n+        }\n+        if (nPartitions != 1) {\n+          lockComponents.add(\n+            new Util.LockComponentBuilder()\n+              .setDbName(\"default\")\n+              .setTableName(String.format(\"tmp_table_%d\", i))\n+              .setShared()\n+              .setOperationType(DataOperationType.SELECT)\n+              .build());\n+        }\n+      }\n+    }\n+\n+    private static long executeOpenTxnAndGetTxnId(HMSClient client) {\n+      return throwingSupplierWrapper(() -> client.openTxn(1).get(0));\n+    }\n+\n+    private void executeLock(HMSClient client, long txnId, List<LockComponent> lockComponents) {\n+      LockRequest req = new LockRequest(lockComponents, \"hclient\", \"localhost\");\n+      req.setTxnid(txnId);\n+      throwingSupplierWrapper(() -> client.lock(req));\n+    }\n+  }\n+\n+  @State(Scope.Benchmark)\n+  public static class TestAllocateTableWriteIds extends CoreContext {\n+    String dbName = \"test_db\";\n+    String tblName = \"tmp_table\";\n+\n+    @State(Scope.Thread)\n+    public static class ThreadState extends CoreContext.ThreadState {\n+      List<Long> openTxns = new ArrayList<>();\n+      long txnId;\n+\n+      @Setup\n+      public void iterSetup() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg2ODk4MQ=="}, "originalCommit": {"oid": "cd44b6227ea2c56386ab09970a3dfeb2016767d0"}, "originalPosition": 178}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMzk5Mzk0OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/BenchmarkTool.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNToyOToyMVrOGiim7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxMjo1Mzo1NFrOGzR2tQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg3MTc4OA==", "bodyText": "I would use switch here and go with ALL by default", "url": "https://github.com/apache/hive/pull/1073#discussion_r438871788", "createdAt": "2020-06-11T15:29:21Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/BenchmarkTool.java", "diffHunk": "@@ -141,12 +175,62 @@ private static void saveDataFile(String location, String name,\n     }\n   }\n \n-\n   @Override\n   public void run() {\n-    LOG.info(\"Using warmup \" + warmup +\n-        \" spin \" + spinCount + \" nparams \" + nParameters + \" threads \" + nThreads);\n+    LOG.info(\"Using warmup \" + warmup + \" spin \" + spinCount + \" nparams \" + Arrays.toString(nParameters) + \" threads \"\n+        + nThreads);\n+    HMSConfig.getInstance().init(host, port, confDir);\n+\n+    if (runMode == RunModes.ALL) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd44b6227ea2c56386ab09970a3dfeb2016767d0"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQyMzA5Mw==", "bodyText": "can't see change here", "url": "https://github.com/apache/hive/pull/1073#discussion_r456423093", "createdAt": "2020-07-17T12:53:54Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/BenchmarkTool.java", "diffHunk": "@@ -141,12 +175,62 @@ private static void saveDataFile(String location, String name,\n     }\n   }\n \n-\n   @Override\n   public void run() {\n-    LOG.info(\"Using warmup \" + warmup +\n-        \" spin \" + spinCount + \" nparams \" + nParameters + \" threads \" + nThreads);\n+    LOG.info(\"Using warmup \" + warmup + \" spin \" + spinCount + \" nparams \" + Arrays.toString(nParameters) + \" threads \"\n+        + nThreads);\n+    HMSConfig.getInstance().init(host, port, confDir);\n+\n+    if (runMode == RunModes.ALL) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg3MTc4OA=="}, "originalCommit": {"oid": "cd44b6227ea2c56386ab09970a3dfeb2016767d0"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNDAxNDI4OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/BenchmarkUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTozMjozN1rOGiizyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTozMjozN1rOGiizyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg3NTA4Mg==", "bodyText": "many new lines", "url": "https://github.com/apache/hive/pull/1073#discussion_r438875082", "createdAt": "2020-06-11T15:32:37Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/BenchmarkUtils.java", "diffHunk": "@@ -0,0 +1,72 @@\n+package org.apache.hadoop.hive.metastore.tools;\n+\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.TxnInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.hadoop.hive.metastore.tools.Util.createSchema;\n+import static org.apache.hadoop.hive.metastore.tools.Util.throwingSupplierWrapper;\n+\n+public class BenchmarkUtils {\n+  private static final Logger LOG = LoggerFactory.getLogger(BenchmarkUtils.class);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd44b6227ea2c56386ab09970a3dfeb2016767d0"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNDAyMjc4OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/BenchmarkUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTozNDowMVrOGii41g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxNDowMjoyNVrOGysb7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg3NjM3NA==", "bodyText": "Arrays.asList returns ArrayList, why to pass it into constructor of another ArrayList?", "url": "https://github.com/apache/hive/pull/1073#discussion_r438876374", "createdAt": "2020-06-11T15:34:01Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/BenchmarkUtils.java", "diffHunk": "@@ -0,0 +1,72 @@\n+package org.apache.hadoop.hive.metastore.tools;\n+\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.TxnInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.hadoop.hive.metastore.tools.Util.createSchema;\n+import static org.apache.hadoop.hive.metastore.tools.Util.throwingSupplierWrapper;\n+\n+public class BenchmarkUtils {\n+  private static final Logger LOG = LoggerFactory.getLogger(BenchmarkUtils.class);\n+\n+\n+  static void createManyTables(HMSClient client, int howMany, String dbName, String format) {\n+    List<FieldSchema> columns = createSchema(new ArrayList<>(Arrays.asList(\"name\", \"string\")));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd44b6227ea2c56386ab09970a3dfeb2016767d0"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTgxMDAzMA==", "bodyText": "fixed", "url": "https://github.com/apache/hive/pull/1073#discussion_r455810030", "createdAt": "2020-07-16T14:02:25Z", "author": {"login": "zchovan"}, "path": "standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/BenchmarkUtils.java", "diffHunk": "@@ -0,0 +1,72 @@\n+package org.apache.hadoop.hive.metastore.tools;\n+\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.TxnInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.hadoop.hive.metastore.tools.Util.createSchema;\n+import static org.apache.hadoop.hive.metastore.tools.Util.throwingSupplierWrapper;\n+\n+public class BenchmarkUtils {\n+  private static final Logger LOG = LoggerFactory.getLogger(BenchmarkUtils.class);\n+\n+\n+  static void createManyTables(HMSClient client, int howMany, String dbName, String format) {\n+    List<FieldSchema> columns = createSchema(new ArrayList<>(Arrays.asList(\"name\", \"string\")));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg3NjM3NA=="}, "originalCommit": {"oid": "cd44b6227ea2c56386ab09970a3dfeb2016767d0"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNDA1NTQ3OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/BenchmarkUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTozOToyMlrOGijNQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxMjo1NDo0N1rOGzR4kA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg4MTYwMg==", "bodyText": "you can use txnInfos.stream().anyMatch(txnsOpenedByBenchmark::contains), change txnsOpenedByBenchmark to Set", "url": "https://github.com/apache/hive/pull/1073#discussion_r438881602", "createdAt": "2020-06-11T15:39:22Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/BenchmarkUtils.java", "diffHunk": "@@ -0,0 +1,72 @@\n+package org.apache.hadoop.hive.metastore.tools;\n+\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.TxnInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.hadoop.hive.metastore.tools.Util.createSchema;\n+import static org.apache.hadoop.hive.metastore.tools.Util.throwingSupplierWrapper;\n+\n+public class BenchmarkUtils {\n+  private static final Logger LOG = LoggerFactory.getLogger(BenchmarkUtils.class);\n+\n+\n+  static void createManyTables(HMSClient client, int howMany, String dbName, String format) {\n+    List<FieldSchema> columns = createSchema(new ArrayList<>(Arrays.asList(\"name\", \"string\")));\n+    List<FieldSchema> partitions = createSchema(new ArrayList<>(Arrays.asList(\"date\", \"string\")));\n+    IntStream.range(0, howMany)\n+        .forEach(i ->\n+            throwingSupplierWrapper(() -> client.createTable(\n+                new Util.TableBuilder(dbName, String.format(format, i))\n+                    .withType(TableType.MANAGED_TABLE)\n+                    .withColumns(columns)\n+                    .withPartitionKeys(partitions)\n+                    .build())));\n+  }\n+\n+  static void dropManyTables(HMSClient client, int howMany, String dbName, String format) {\n+    IntStream.range(0, howMany)\n+        .forEach(i ->\n+            throwingSupplierWrapper(() -> client.dropTable(dbName, String.format(format, i))));\n+  }\n+\n+  // Create a simple table with a single column and single partition\n+  static void createPartitionedTable(HMSClient client, String dbName, String tableName) {\n+    throwingSupplierWrapper(() -> client.createTable(\n+        new Util.TableBuilder(dbName, tableName)\n+            .withType(TableType.MANAGED_TABLE)\n+            .withColumns(createSchema(Collections.singletonList(\"name:string\")))\n+            .withPartitionKeys(createSchema(Collections.singletonList(\"date\")))\n+            .build()));\n+  }\n+\n+  static boolean checkTxnsCleaned(HMSClient client, List<Long> txnsOpenedByBenchmark) throws InterruptedException {\n+    // let's wait the default cleaner run period\n+    Thread.sleep(100000);\n+    List<Long> notCleanedTxns = new ArrayList<>();\n+    throwingSupplierWrapper(() -> {\n+      List<TxnInfo> txnInfos = client.getOpenTxnsInfo();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd44b6227ea2c56386ab09970a3dfeb2016767d0"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQyMzU2OA==", "bodyText": "can't see any change here", "url": "https://github.com/apache/hive/pull/1073#discussion_r456423568", "createdAt": "2020-07-17T12:54:47Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-tools/metastore-benchmarks/src/main/java/org/apache/hadoop/hive/metastore/tools/BenchmarkUtils.java", "diffHunk": "@@ -0,0 +1,72 @@\n+package org.apache.hadoop.hive.metastore.tools;\n+\n+import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.TxnInfo;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.IntStream;\n+\n+import static org.apache.hadoop.hive.metastore.tools.Util.createSchema;\n+import static org.apache.hadoop.hive.metastore.tools.Util.throwingSupplierWrapper;\n+\n+public class BenchmarkUtils {\n+  private static final Logger LOG = LoggerFactory.getLogger(BenchmarkUtils.class);\n+\n+\n+  static void createManyTables(HMSClient client, int howMany, String dbName, String format) {\n+    List<FieldSchema> columns = createSchema(new ArrayList<>(Arrays.asList(\"name\", \"string\")));\n+    List<FieldSchema> partitions = createSchema(new ArrayList<>(Arrays.asList(\"date\", \"string\")));\n+    IntStream.range(0, howMany)\n+        .forEach(i ->\n+            throwingSupplierWrapper(() -> client.createTable(\n+                new Util.TableBuilder(dbName, String.format(format, i))\n+                    .withType(TableType.MANAGED_TABLE)\n+                    .withColumns(columns)\n+                    .withPartitionKeys(partitions)\n+                    .build())));\n+  }\n+\n+  static void dropManyTables(HMSClient client, int howMany, String dbName, String format) {\n+    IntStream.range(0, howMany)\n+        .forEach(i ->\n+            throwingSupplierWrapper(() -> client.dropTable(dbName, String.format(format, i))));\n+  }\n+\n+  // Create a simple table with a single column and single partition\n+  static void createPartitionedTable(HMSClient client, String dbName, String tableName) {\n+    throwingSupplierWrapper(() -> client.createTable(\n+        new Util.TableBuilder(dbName, tableName)\n+            .withType(TableType.MANAGED_TABLE)\n+            .withColumns(createSchema(Collections.singletonList(\"name:string\")))\n+            .withPartitionKeys(createSchema(Collections.singletonList(\"date\")))\n+            .build()));\n+  }\n+\n+  static boolean checkTxnsCleaned(HMSClient client, List<Long> txnsOpenedByBenchmark) throws InterruptedException {\n+    // let's wait the default cleaner run period\n+    Thread.sleep(100000);\n+    List<Long> notCleanedTxns = new ArrayList<>();\n+    throwingSupplierWrapper(() -> {\n+      List<TxnInfo> txnInfos = client.getOpenTxnsInfo();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg4MTYwMg=="}, "originalCommit": {"oid": "cd44b6227ea2c56386ab09970a3dfeb2016767d0"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNDA3NTg3OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-tools/tools-common/src/main/java/org/apache/hadoop/hive/metastore/tools/HMSClient.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNTo0MzozOFrOGijahQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxMzoxNTo1OFrOGzSkEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg4NDk5Nw==", "bodyText": "I would expect this method to return list of validWriteIds, not just true. Should we change the name?", "url": "https://github.com/apache/hive/pull/1073#discussion_r438884997", "createdAt": "2020-06-11T15:43:38Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-tools/tools-common/src/main/java/org/apache/hadoop/hive/metastore/tools/HMSClient.java", "diffHunk": "@@ -345,21 +348,44 @@ boolean openTxn(int numTxns) throws TException {\n     return openTxns;\n   }\n \n+  List<TxnInfo> getOpenTxnsInfo() throws TException {\n+    return client.get_open_txns_info().getOpen_txns();\n+  }\n+\n   boolean commitTxn(long txnId) throws TException {\n     client.commit_txn(new CommitTxnRequest(txnId));\n     return true;\n   }\n \n-  boolean abortTxn(long txnId) throws TException {\n-    client.abort_txn(new AbortTxnRequest(txnId));\n+  boolean abortTxns(List<Long> txnIds) throws TException {\n+    client.abort_txns(new AbortTxnsRequest(txnIds));\n     return true;\n   }\n \n-  boolean abortTxns(List<Long> txnIds) throws TException {\n-    client.abort_txns(new AbortTxnsRequest(txnIds));\n+  boolean allocateTableWriteIds(String dbName, String tableName, List<Long> openTxns) throws TException {\n+    AllocateTableWriteIdsRequest awiRqst = new AllocateTableWriteIdsRequest(dbName, tableName);\n+    openTxns.forEach(t -> {\n+      awiRqst.addToTxnIds(t);\n+    });\n+\n+    client.allocate_table_write_ids(awiRqst);\n     return true;\n   }\n \n+  boolean getValidWriteIds(List<String> fullTableNames) throws TException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd44b6227ea2c56386ab09970a3dfeb2016767d0"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTgxMDUyMA==", "bodyText": "HMClient methods return boolean, so they can be invoked with throwingSupplierWrapper()", "url": "https://github.com/apache/hive/pull/1073#discussion_r455810520", "createdAt": "2020-07-16T14:03:04Z", "author": {"login": "zchovan"}, "path": "standalone-metastore/metastore-tools/tools-common/src/main/java/org/apache/hadoop/hive/metastore/tools/HMSClient.java", "diffHunk": "@@ -345,21 +348,44 @@ boolean openTxn(int numTxns) throws TException {\n     return openTxns;\n   }\n \n+  List<TxnInfo> getOpenTxnsInfo() throws TException {\n+    return client.get_open_txns_info().getOpen_txns();\n+  }\n+\n   boolean commitTxn(long txnId) throws TException {\n     client.commit_txn(new CommitTxnRequest(txnId));\n     return true;\n   }\n \n-  boolean abortTxn(long txnId) throws TException {\n-    client.abort_txn(new AbortTxnRequest(txnId));\n+  boolean abortTxns(List<Long> txnIds) throws TException {\n+    client.abort_txns(new AbortTxnsRequest(txnIds));\n     return true;\n   }\n \n-  boolean abortTxns(List<Long> txnIds) throws TException {\n-    client.abort_txns(new AbortTxnsRequest(txnIds));\n+  boolean allocateTableWriteIds(String dbName, String tableName, List<Long> openTxns) throws TException {\n+    AllocateTableWriteIdsRequest awiRqst = new AllocateTableWriteIdsRequest(dbName, tableName);\n+    openTxns.forEach(t -> {\n+      awiRqst.addToTxnIds(t);\n+    });\n+\n+    client.allocate_table_write_ids(awiRqst);\n     return true;\n   }\n \n+  boolean getValidWriteIds(List<String> fullTableNames) throws TException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg4NDk5Nw=="}, "originalCommit": {"oid": "cd44b6227ea2c56386ab09970a3dfeb2016767d0"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQyMTk4OQ==", "bodyText": "I don't get what does it have to do with throwingSupplierWrapper. throwingSupplierWrapper just handles checked exceptions. Could you please elaborate here?", "url": "https://github.com/apache/hive/pull/1073#discussion_r456421989", "createdAt": "2020-07-17T12:51:48Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-tools/tools-common/src/main/java/org/apache/hadoop/hive/metastore/tools/HMSClient.java", "diffHunk": "@@ -345,21 +348,44 @@ boolean openTxn(int numTxns) throws TException {\n     return openTxns;\n   }\n \n+  List<TxnInfo> getOpenTxnsInfo() throws TException {\n+    return client.get_open_txns_info().getOpen_txns();\n+  }\n+\n   boolean commitTxn(long txnId) throws TException {\n     client.commit_txn(new CommitTxnRequest(txnId));\n     return true;\n   }\n \n-  boolean abortTxn(long txnId) throws TException {\n-    client.abort_txn(new AbortTxnRequest(txnId));\n+  boolean abortTxns(List<Long> txnIds) throws TException {\n+    client.abort_txns(new AbortTxnsRequest(txnIds));\n     return true;\n   }\n \n-  boolean abortTxns(List<Long> txnIds) throws TException {\n-    client.abort_txns(new AbortTxnsRequest(txnIds));\n+  boolean allocateTableWriteIds(String dbName, String tableName, List<Long> openTxns) throws TException {\n+    AllocateTableWriteIdsRequest awiRqst = new AllocateTableWriteIdsRequest(dbName, tableName);\n+    openTxns.forEach(t -> {\n+      awiRqst.addToTxnIds(t);\n+    });\n+\n+    client.allocate_table_write_ids(awiRqst);\n     return true;\n   }\n \n+  boolean getValidWriteIds(List<String> fullTableNames) throws TException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg4NDk5Nw=="}, "originalCommit": {"oid": "cd44b6227ea2c56386ab09970a3dfeb2016767d0"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjQzNDcwNA==", "bodyText": "ah sorry, I was mistaken, the reason why it never returned the writeIds is because they are never used, the benchmark is just executing the api call. The return value from the hms is actually a GetValidWriteIdsResponse object, not a list. As it is never used I'm not sure if we need to change this.", "url": "https://github.com/apache/hive/pull/1073#discussion_r456434704", "createdAt": "2020-07-17T13:15:58Z", "author": {"login": "zchovan"}, "path": "standalone-metastore/metastore-tools/tools-common/src/main/java/org/apache/hadoop/hive/metastore/tools/HMSClient.java", "diffHunk": "@@ -345,21 +348,44 @@ boolean openTxn(int numTxns) throws TException {\n     return openTxns;\n   }\n \n+  List<TxnInfo> getOpenTxnsInfo() throws TException {\n+    return client.get_open_txns_info().getOpen_txns();\n+  }\n+\n   boolean commitTxn(long txnId) throws TException {\n     client.commit_txn(new CommitTxnRequest(txnId));\n     return true;\n   }\n \n-  boolean abortTxn(long txnId) throws TException {\n-    client.abort_txn(new AbortTxnRequest(txnId));\n+  boolean abortTxns(List<Long> txnIds) throws TException {\n+    client.abort_txns(new AbortTxnsRequest(txnIds));\n     return true;\n   }\n \n-  boolean abortTxns(List<Long> txnIds) throws TException {\n-    client.abort_txns(new AbortTxnsRequest(txnIds));\n+  boolean allocateTableWriteIds(String dbName, String tableName, List<Long> openTxns) throws TException {\n+    AllocateTableWriteIdsRequest awiRqst = new AllocateTableWriteIdsRequest(dbName, tableName);\n+    openTxns.forEach(t -> {\n+      awiRqst.addToTxnIds(t);\n+    });\n+\n+    client.allocate_table_write_ids(awiRqst);\n     return true;\n   }\n \n+  boolean getValidWriteIds(List<String> fullTableNames) throws TException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg4NDk5Nw=="}, "originalCommit": {"oid": "cd44b6227ea2c56386ab09970a3dfeb2016767d0"}, "originalPosition": 83}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 743, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}