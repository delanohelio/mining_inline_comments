{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk0MjgxMjU4", "number": 1533, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxMjo1MDo0OVrOEoiK1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxMzoxNTo1MVrOEoi1Ew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwOTM4MzI2OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxMjo1MDo0OVrOHZrikA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQwOTo0NDo0NVrOHaVsLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY4OTgwOA==", "bodyText": "That is an interesting change, do I understand correctly, that this says we only need to do a follow up check only, if the txn was outdated, because in the other cases the exclusive lock + the partition based writeset checking guarantees, that writeIds will be always valid at the second time? Could you add some comments here", "url": "https://github.com/apache/hive/pull/1533#discussion_r496689808", "createdAt": "2020-09-29T12:50:49Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -497,38 +497,41 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n         HiveConf.ConfVars.HIVE_TXN_MAX_RETRYSNAPSHOT_COUNT);\n \n       try {\n-        while (!driverTxnHandler.isValidTxnListState() && ++retryShapshotCnt <= maxRetrySnapshotCnt) {\n-          LOG.info(\"Re-compiling after acquiring locks, attempt #\" + retryShapshotCnt);\n-          // Snapshot was outdated when locks were acquired, hence regenerate context, txn list and retry.\n-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n-          // Currently, we acquire a snapshot, compile the query with that snapshot, and then - acquire locks.\n-          // If snapshot is still valid, we continue as usual.\n-          // But if snapshot is not valid, we recompile the query.\n-          if (driverContext.isOutdatedTxn()) {\n-            LOG.info(\"Snapshot is outdated, re-initiating transaction ...\");\n-            driverContext.getTxnManager().rollbackTxn();\n-\n-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n-            lockAndRespond();\n+        do {\n+          driverContext.setOutdatedTxn(false);\n+\n+          if (!driverTxnHandler.isValidTxnListState()) {\n+            LOG.info(\"Re-compiling after acquiring locks, attempt #\" + retryShapshotCnt);\n+            // Snapshot was outdated when locks were acquired, hence regenerate context, txn list and retry.\n+            // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n+            // Currently, we acquire a snapshot, compile the query with that snapshot, and then - acquire locks.\n+            // If snapshot is still valid, we continue as usual.\n+            // But if snapshot is not valid, we recompile the query.\n+            if (driverContext.isOutdatedTxn()) {\n+              LOG.info(\"Snapshot is outdated, re-initiating transaction ...\");\n+              driverContext.getTxnManager().rollbackTxn();\n+\n+              String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n+              driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n+              lockAndRespond();\n+            }\n+            driverContext.setRetrial(true);\n+            driverContext.getBackupContext().addSubContext(context);\n+            driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n+            context = driverContext.getBackupContext();\n+\n+            driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n+              driverContext.getTxnManager().getValidTxns().toString());\n+\n+            if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n+              compileInternal(context.getCmd(), true);\n+              driverTxnHandler.recordValidWriteIds();\n+              driverTxnHandler.setWriteIdForAcidFileSinks();\n+            }\n+            // Since we're reusing the compiled plan, we need to update its start time for current run\n+            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n           }\n-\n-          driverContext.setRetrial(true);\n-          driverContext.getBackupContext().addSubContext(context);\n-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n-          context = driverContext.getBackupContext();\n-\n-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n-            driverContext.getTxnManager().getValidTxns().toString());\n-\n-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n-            compileInternal(context.getCmd(), true);\n-            driverTxnHandler.recordValidWriteIds();\n-            driverTxnHandler.setWriteIdForAcidFileSinks();\n-          }\n-          // Since we're reusing the compiled plan, we need to update its start time for current run\n-          driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n-        }\n+        } while (driverContext.isOutdatedTxn() && ++retryShapshotCnt <= maxRetrySnapshotCnt);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6ea97470067a90fbc1a6d0b809ddb28c5a7502c"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzM4MDM5OQ==", "bodyText": "added comments", "url": "https://github.com/apache/hive/pull/1533#discussion_r497380399", "createdAt": "2020-09-30T09:44:45Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -497,38 +497,41 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n         HiveConf.ConfVars.HIVE_TXN_MAX_RETRYSNAPSHOT_COUNT);\n \n       try {\n-        while (!driverTxnHandler.isValidTxnListState() && ++retryShapshotCnt <= maxRetrySnapshotCnt) {\n-          LOG.info(\"Re-compiling after acquiring locks, attempt #\" + retryShapshotCnt);\n-          // Snapshot was outdated when locks were acquired, hence regenerate context, txn list and retry.\n-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n-          // Currently, we acquire a snapshot, compile the query with that snapshot, and then - acquire locks.\n-          // If snapshot is still valid, we continue as usual.\n-          // But if snapshot is not valid, we recompile the query.\n-          if (driverContext.isOutdatedTxn()) {\n-            LOG.info(\"Snapshot is outdated, re-initiating transaction ...\");\n-            driverContext.getTxnManager().rollbackTxn();\n-\n-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n-            lockAndRespond();\n+        do {\n+          driverContext.setOutdatedTxn(false);\n+\n+          if (!driverTxnHandler.isValidTxnListState()) {\n+            LOG.info(\"Re-compiling after acquiring locks, attempt #\" + retryShapshotCnt);\n+            // Snapshot was outdated when locks were acquired, hence regenerate context, txn list and retry.\n+            // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n+            // Currently, we acquire a snapshot, compile the query with that snapshot, and then - acquire locks.\n+            // If snapshot is still valid, we continue as usual.\n+            // But if snapshot is not valid, we recompile the query.\n+            if (driverContext.isOutdatedTxn()) {\n+              LOG.info(\"Snapshot is outdated, re-initiating transaction ...\");\n+              driverContext.getTxnManager().rollbackTxn();\n+\n+              String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n+              driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n+              lockAndRespond();\n+            }\n+            driverContext.setRetrial(true);\n+            driverContext.getBackupContext().addSubContext(context);\n+            driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n+            context = driverContext.getBackupContext();\n+\n+            driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n+              driverContext.getTxnManager().getValidTxns().toString());\n+\n+            if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n+              compileInternal(context.getCmd(), true);\n+              driverTxnHandler.recordValidWriteIds();\n+              driverTxnHandler.setWriteIdForAcidFileSinks();\n+            }\n+            // Since we're reusing the compiled plan, we need to update its start time for current run\n+            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n           }\n-\n-          driverContext.setRetrial(true);\n-          driverContext.getBackupContext().addSubContext(context);\n-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n-          context = driverContext.getBackupContext();\n-\n-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n-            driverContext.getTxnManager().getValidTxns().toString());\n-\n-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n-            compileInternal(context.getCmd(), true);\n-            driverTxnHandler.recordValidWriteIds();\n-            driverTxnHandler.setWriteIdForAcidFileSinks();\n-          }\n-          // Since we're reusing the compiled plan, we need to update its start time for current run\n-          driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n-        }\n+        } while (driverContext.isOutdatedTxn() && ++retryShapshotCnt <= maxRetrySnapshotCnt);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY4OTgwOA=="}, "originalCommit": {"oid": "d6ea97470067a90fbc1a6d0b809ddb28c5a7502c"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwOTQyNjI1OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxMzowMDo0NFrOHZr9BQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQxMDoyMjozMlrOHaXBdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5NjU4MQ==", "bodyText": "Maybe add some javadoc, what does it do. I would emphasise that this method call only makes sense if the caller holds an exclusive lock, that blocks other txns to commit writes. And also, that this deliberately ignores inserts, or maybe this should be added to the DriverTxnHandler, that inserts will not invalidate the snapshot, that can cause duplicates", "url": "https://github.com/apache/hive/pull/1533#discussion_r496696581", "createdAt": "2020-09-29T13:00:44Z", "author": {"login": "pvargacl"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -1408,6 +1410,43 @@ private boolean isUpdateOrDelete(Statement stmt, String conflictSQLSuffix) throw\n     }\n   }\n \n+  public long getLatestTxnInConflict(long txnid) throws MetaException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6ea97470067a90fbc1a6d0b809ddb28c5a7502c"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzQwMjIyOQ==", "bodyText": "added javadoc", "url": "https://github.com/apache/hive/pull/1533#discussion_r497402229", "createdAt": "2020-09-30T10:22:32Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -1408,6 +1410,43 @@ private boolean isUpdateOrDelete(Statement stmt, String conflictSQLSuffix) throw\n     }\n   }\n \n+  public long getLatestTxnInConflict(long txnid) throws MetaException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5NjU4MQ=="}, "originalCommit": {"oid": "d6ea97470067a90fbc1a6d0b809ddb28c5a7502c"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwOTQ0Nzc0OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxMzowNTo1M1rOHZsKRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQwNzoyMDoyMVrOHaQXTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5OTk3NQ==", "bodyText": "This fix is not related to the original problem, shouldn't it be committed separately?", "url": "https://github.com/apache/hive/pull/1533#discussion_r496699975", "createdAt": "2020-09-29T13:05:53Z", "author": {"login": "pvargacl"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -3501,6 +3541,10 @@ public void addDynamicPartitions(AddDynamicPartitions rqst)\n             pstmt.executeBatch();\n           }\n         }\n+        try (PreparedStatement pstmt = dbConn.prepareStatement(TXN_COMPONENTS_DP_DELETE_QUERY)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6ea97470067a90fbc1a6d0b809ddb28c5a7502c"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI5MzEzMw==", "bodyText": "It's actually related. Operation that involve dynamic partitioning doesn't generate TXN_COMPONENTS at the locking stage, making it impossible to check for write conflict.", "url": "https://github.com/apache/hive/pull/1533#discussion_r497293133", "createdAt": "2020-09-30T07:20:21Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -3501,6 +3541,10 @@ public void addDynamicPartitions(AddDynamicPartitions rqst)\n             pstmt.executeBatch();\n           }\n         }\n+        try (PreparedStatement pstmt = dbConn.prepareStatement(TXN_COMPONENTS_DP_DELETE_QUERY)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjY5OTk3NQ=="}, "originalCommit": {"oid": "d6ea97470067a90fbc1a6d0b809ddb28c5a7502c"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwOTQ5MTM5OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxMzoxNTo1MVrOHZslaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQwNzoyNDo0NlrOHaQgHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjcwNjkyMw==", "bodyText": "This is probably a big enough change, but I was wondering if you could return all the conflicting txnIds and writeIds. In that case if the max conflicting txnId is less than the current one, you could skip two other call to the HMS for validTxnList and validWriteIdList and just remove the exceptions from the list.", "url": "https://github.com/apache/hive/pull/1533#discussion_r496706923", "createdAt": "2020-09-29T13:15:51Z", "author": {"login": "pvargacl"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -1408,6 +1410,43 @@ private boolean isUpdateOrDelete(Statement stmt, String conflictSQLSuffix) throw\n     }\n   }\n \n+  public long getLatestTxnInConflict(long txnid) throws MetaException {\n+    Connection dbConn = null;\n+    Statement stmt = null;\n+\n+    try {\n+      dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n+      stmt = dbConn.createStatement();\n+\n+      String writeConflictQuery = \"SELECT MAX(\\\"COMMITTED\\\".\\\"WS_TXNID\\\")\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6ea97470067a90fbc1a6d0b809ddb28c5a7502c"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzI5NTM5MQ==", "bodyText": "In most of the cases it will just decrease the performance. It could only be useful if there is a conflict with already registered in snapshot txn. Also won't work with dynamic partitioning as we need to re-compile and get a fresh validWriteIdList list.", "url": "https://github.com/apache/hive/pull/1533#discussion_r497295391", "createdAt": "2020-09-30T07:24:46Z", "author": {"login": "deniskuzZ"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java", "diffHunk": "@@ -1408,6 +1410,43 @@ private boolean isUpdateOrDelete(Statement stmt, String conflictSQLSuffix) throw\n     }\n   }\n \n+  public long getLatestTxnInConflict(long txnid) throws MetaException {\n+    Connection dbConn = null;\n+    Statement stmt = null;\n+\n+    try {\n+      dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n+      stmt = dbConn.createStatement();\n+\n+      String writeConflictQuery = \"SELECT MAX(\\\"COMMITTED\\\".\\\"WS_TXNID\\\")\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjcwNjkyMw=="}, "originalCommit": {"oid": "d6ea97470067a90fbc1a6d0b809ddb28c5a7502c"}, "originalPosition": 21}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 305, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}