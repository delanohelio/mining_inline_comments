{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk1NjM5MjM1", "number": 1542, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxMzo0NDozOVrOEp0dFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDo1OTozN1rOEx8UAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMjg2NDg1OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxMzo0NDozOVrOHbuM4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNDo0ODoyMVrOHbwi3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODgzMDU2Mw==", "bodyText": "could you please follow the convention of other methods and define a struct for the requests arguments", "url": "https://github.com/apache/hive/pull/1542#discussion_r498830563", "createdAt": "2020-10-02T13:44:39Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift", "diffHunk": "@@ -2830,6 +2848,11 @@ PartitionsResponse get_partitions_req(1:PartitionsRequest req)\n   void add_replication_metrics(1: ReplicationMetricList replicationMetricList) throws(1:MetaException o1)\n   ReplicationMetricList get_replication_metrics(1: GetReplicationMetricsRequest rqst) throws(1:MetaException o1)\n   GetOpenTxnsResponse get_open_txns_req(1: GetOpenTxnsRequest getOpenTxnsRequest)\n+\n+  void create_stored_procedure(1: string catName, 2: StoredProcedure proc) throws(1:NoSuchObjectException o1, 2:MetaException o2)\n+  StoredProcedure get_stored_procedure(1: string catName, 2: string db, 3: string name) throws (1:MetaException o1, 2:NoSuchObjectException o2)\n+  void drop_stored_procedure(1: string catName, 2: string dbName, 3: string funcName) throws (1:MetaException o1, 2:NoSuchObjectException o2)\n+  list<StoredProcedure> get_all_stored_procedures(1: string catName) throws (1:MetaException o1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg2ODk1OQ==", "bodyText": "You mean putting (1: string catName, 2: string dbName, 3: string funcName) into a request object? I can do that. But if we have only one parameter, like in the last case that would be an overkill in my opinion.", "url": "https://github.com/apache/hive/pull/1542#discussion_r498868959", "createdAt": "2020-10-02T14:48:21Z", "author": {"login": "zeroflag"}, "path": "standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift", "diffHunk": "@@ -2830,6 +2848,11 @@ PartitionsResponse get_partitions_req(1:PartitionsRequest req)\n   void add_replication_metrics(1: ReplicationMetricList replicationMetricList) throws(1:MetaException o1)\n   ReplicationMetricList get_replication_metrics(1: GetReplicationMetricsRequest rqst) throws(1:MetaException o1)\n   GetOpenTxnsResponse get_open_txns_req(1: GetOpenTxnsRequest getOpenTxnsRequest)\n+\n+  void create_stored_procedure(1: string catName, 2: StoredProcedure proc) throws(1:NoSuchObjectException o1, 2:MetaException o2)\n+  StoredProcedure get_stored_procedure(1: string catName, 2: string db, 3: string name) throws (1:MetaException o1, 2:NoSuchObjectException o2)\n+  void drop_stored_procedure(1: string catName, 2: string dbName, 3: string funcName) throws (1:MetaException o1, 2:NoSuchObjectException o2)\n+  list<StoredProcedure> get_all_stored_procedures(1: string catName) throws (1:MetaException o1)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODgzMDU2Mw=="}, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMzA0MzY2OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "isResolved": true, "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNDozMjo1NVrOHbv-cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNTo1MTo1MFrOHlEzvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg1OTYzNA==", "bodyText": "I think instead of storing the return_type/argument types and such in the metastore - as they would never participate in a query or anything \"usefull\"; they will just travel as payload in the messages.\nGiven the fact that they are effectively implicit data which can be figured out from the function defintion - I think we may leave it to the execution engine; it should be able to figure it out (since it should be able to use it) .\noptionally; to give ourselfs(and users) some type of clarity we could add a \"signature\" string to the table - which could provide a human readable signature", "url": "https://github.com/apache/hive/pull/1542#discussion_r498859634", "createdAt": "2020-10-02T14:32:55Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,83 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">\n+      <datastore-identity>\n+        <column name=\"SP_ID\"/>\n+      </datastore-identity>\n+      <field name=\"createTime\">\n+        <column name=\"CREATE_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"lastAccessTime\">\n+        <column name=\"LAST_ACCESS_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"database\">\n+        <column name=\"DB_ID\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"name\">\n+        <column name=\"NAME\" length=\"256\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"owner\">\n+        <column name=\"OWNER_NAME\" length=\"128\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"source\">\n+        <column name=\"SOURCE\" jdbc-type=\"MEDIUMTEXT\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"language\">\n+        <column name=\"LANG\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"returnType\">\n+        <column name=\"RET_TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"true\"/>\n+      </field>\n+      <field name=\"parameters\" table=\"SP_POS_ARGS\" >\n+        <collection element-type=\"MPosParam\"/>\n+        <join>\n+            <column name=\"SP_ID\"/>\n+        </join>\n+        <element>\n+          <embedded>\n+            <field name=\"name\">\n+              <column name=\"NAME\" jdbc-type=\"varchar\" length=\"256\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"type\">\n+              <column name=\"TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"length\">\n+              <column name=\"length\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"scale\">\n+              <column name=\"scale\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"isOut\">\n+              <column name=\"OUT\" allows-null=\"false\"/>\n+            </field>\n+          </embedded>\n+        </element>\n+      </field>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg3Nzg5NA==", "bodyText": "The signature is already parsed by the time the procedure is being created. We would need to drop that information, get back the textual representation of the signature to store it in HMS, and reparse it on the client side when someone calls the procedure. That's maybe not a big deal but still unnecessary to parse it twice. Storing it in a structured way also ensures some degree of validity, you can't store a syntactically incorrect signature if we store it in a structured way.\nI'm not sure if they never participate in a query. If one wants to discover the stored procedures which are currently stored in a DB and find out on what data they operate they would need to do some clumsy string manipulations on the signature.\nConsidering that other DB engines also store these information separately I would like to keep it as it is for now and see how it works in practice. Later on when we have multi language support we can revisit this issue.", "url": "https://github.com/apache/hive/pull/1542#discussion_r498877894", "createdAt": "2020-10-02T15:02:51Z", "author": {"login": "zeroflag"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,83 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">\n+      <datastore-identity>\n+        <column name=\"SP_ID\"/>\n+      </datastore-identity>\n+      <field name=\"createTime\">\n+        <column name=\"CREATE_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"lastAccessTime\">\n+        <column name=\"LAST_ACCESS_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"database\">\n+        <column name=\"DB_ID\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"name\">\n+        <column name=\"NAME\" length=\"256\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"owner\">\n+        <column name=\"OWNER_NAME\" length=\"128\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"source\">\n+        <column name=\"SOURCE\" jdbc-type=\"MEDIUMTEXT\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"language\">\n+        <column name=\"LANG\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"returnType\">\n+        <column name=\"RET_TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"true\"/>\n+      </field>\n+      <field name=\"parameters\" table=\"SP_POS_ARGS\" >\n+        <collection element-type=\"MPosParam\"/>\n+        <join>\n+            <column name=\"SP_ID\"/>\n+        </join>\n+        <element>\n+          <embedded>\n+            <field name=\"name\">\n+              <column name=\"NAME\" jdbc-type=\"varchar\" length=\"256\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"type\">\n+              <column name=\"TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"length\">\n+              <column name=\"length\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"scale\">\n+              <column name=\"scale\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"isOut\">\n+              <column name=\"OUT\" allows-null=\"false\"/>\n+            </field>\n+          </embedded>\n+        </element>\n+      </field>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg1OTYzNA=="}, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjUxMjY0Mg==", "bodyText": "I'm not sure if they never participate in a query. If one wants to discover the stored procedures which are currently stored in a DB and find out on what data they operate they would need to do some clumsy string manipulations on the signature.\n\nI believe you are thinking about information_schema stuff - its not set in stone that we have to get all that data from the metastore db - for this case we might add a few UDFs parameter info into an array or something ; so we will still store simple things in the metastore - but we could transform it into more readable in.\n\nConsidering that other DB engines also store these information separately I would like to keep it as it is for now and see how it works in practice. Later on when we have multi language support we can revisit this issue.\n\nyes it might be..but it would be better to revisit stuff like this if its really needed; and not after we have introduced \"something\" which later we should care for even if we don't want to\nI still think there will be no real benefit of \"storing it\" in a decomposed manner - it will be harder to go forward in case stuff changes - and right now will not use it for anything ; so let's remove it..and add it only if there is a real need for it.", "url": "https://github.com/apache/hive/pull/1542#discussion_r502512642", "createdAt": "2020-10-09T15:33:46Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,83 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">\n+      <datastore-identity>\n+        <column name=\"SP_ID\"/>\n+      </datastore-identity>\n+      <field name=\"createTime\">\n+        <column name=\"CREATE_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"lastAccessTime\">\n+        <column name=\"LAST_ACCESS_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"database\">\n+        <column name=\"DB_ID\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"name\">\n+        <column name=\"NAME\" length=\"256\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"owner\">\n+        <column name=\"OWNER_NAME\" length=\"128\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"source\">\n+        <column name=\"SOURCE\" jdbc-type=\"MEDIUMTEXT\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"language\">\n+        <column name=\"LANG\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"returnType\">\n+        <column name=\"RET_TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"true\"/>\n+      </field>\n+      <field name=\"parameters\" table=\"SP_POS_ARGS\" >\n+        <collection element-type=\"MPosParam\"/>\n+        <join>\n+            <column name=\"SP_ID\"/>\n+        </join>\n+        <element>\n+          <embedded>\n+            <field name=\"name\">\n+              <column name=\"NAME\" jdbc-type=\"varchar\" length=\"256\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"type\">\n+              <column name=\"TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"length\">\n+              <column name=\"length\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"scale\">\n+              <column name=\"scale\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"isOut\">\n+              <column name=\"OUT\" allows-null=\"false\"/>\n+            </field>\n+          </embedded>\n+        </element>\n+      </field>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg1OTYzNA=="}, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDk2NDEzMg==", "bodyText": "Thanks @zeroflag for working on this , this is very cool.\nThere are a few challenges with keeping redundant information outside of the procedure text that I can think of. One of them is that while the semantics of the procedure definition may be well defined, the representation for the other fields may not be defined clearly. Another usual challenge is that if there are any changes in the future, you will have to ensure backwards compatibility for those fields too.\nGoing through the specific implementation of type handling, it seems you are keeping length and scale in different fields. This is not done when we store types in HMS for Hive. Any reason for that? Also, checking the documentation, it seems HPL/SQL can apply some transformations to the field type. Are those transformations applied before storing the definition or later on?\nBased on that, I think keeping a lean representation in HMS has its advantages like @kgyrtkirk mentioned, specifically if those fields are not actually being used for the time being. It's preferable to add those fields later on if needed, rather than backtracking and removing fields.", "url": "https://github.com/apache/hive/pull/1542#discussion_r504964132", "createdAt": "2020-10-14T20:50:31Z", "author": {"login": "jcamachor"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,83 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">\n+      <datastore-identity>\n+        <column name=\"SP_ID\"/>\n+      </datastore-identity>\n+      <field name=\"createTime\">\n+        <column name=\"CREATE_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"lastAccessTime\">\n+        <column name=\"LAST_ACCESS_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"database\">\n+        <column name=\"DB_ID\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"name\">\n+        <column name=\"NAME\" length=\"256\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"owner\">\n+        <column name=\"OWNER_NAME\" length=\"128\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"source\">\n+        <column name=\"SOURCE\" jdbc-type=\"MEDIUMTEXT\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"language\">\n+        <column name=\"LANG\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"returnType\">\n+        <column name=\"RET_TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"true\"/>\n+      </field>\n+      <field name=\"parameters\" table=\"SP_POS_ARGS\" >\n+        <collection element-type=\"MPosParam\"/>\n+        <join>\n+            <column name=\"SP_ID\"/>\n+        </join>\n+        <element>\n+          <embedded>\n+            <field name=\"name\">\n+              <column name=\"NAME\" jdbc-type=\"varchar\" length=\"256\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"type\">\n+              <column name=\"TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"length\">\n+              <column name=\"length\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"scale\">\n+              <column name=\"scale\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"isOut\">\n+              <column name=\"OUT\" allows-null=\"false\"/>\n+            </field>\n+          </embedded>\n+        </element>\n+      </field>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg1OTYzNA=="}, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTQ5OTk5NQ==", "bodyText": "Hey @jcamachor thanks for the feedback, I'm glad you chimed in. Maybe there is a misunderstanding, this information is not redundant in any way, we're making use of it when invoking a procedure. This information must be stored somewhere in some form the only question is what representation to use. Just to clarify, currently I store the signature in a structured way + procedure body (without the signature) in text.\nWhat was proposed (but @kgyrtkirk correct me if I'm wrong) as an alternative solution is store the signature together with the body and optionally add a signature string to the table as well (this would be redundant). I think regardless of the representation, backward compatibility will always be a concern.\nOne thing I don't like about storing the signature in text is that the assumption that invoking the procedure will always require parsing. This is only true for now because HPL/SQL is an AST interpreter but if we ever want to make this performant probably we'll need to introduce a byte code VM at some point in the near future. While creating a procedure needs parsing but invoking it wouldn't, if we stored the byte code. How would this work in this case? I suppose we can add runtime information into the byte code, but that's not always an option. For example postgres allows you to invoke procedures implemented in C where runtime information about the signature is not available. This might be one reason why they also choose to store the signature separately in a structured way.\nMulti language support was already raised by customers and adding it would be the easiest if we had common bytecode for different languages. One might want to call a procedure implemented in language A from a different language B. Then A would need to use the parser of language B to get the signature information, if the signature was stored in text.\nWe can keep speculating on this, but at this point this is still an experimental and undocumented feature, I'm open to change it later if we have proof that one way is better than the other.\nBut if we decide to go with the alternative solution from now on, I suggest we choose a language agnostic representation (JSON or whatever) of the signature, instead of the unparsed text.\n\nAlso, checking the documentation, it seems HPL/SQL can apply some transformations to the field type. Are those transformations applied before storing the definition or later on?\n\nYes, and that only affects create table statements.", "url": "https://github.com/apache/hive/pull/1542#discussion_r505499995", "createdAt": "2020-10-15T12:27:50Z", "author": {"login": "zeroflag"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,83 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">\n+      <datastore-identity>\n+        <column name=\"SP_ID\"/>\n+      </datastore-identity>\n+      <field name=\"createTime\">\n+        <column name=\"CREATE_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"lastAccessTime\">\n+        <column name=\"LAST_ACCESS_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"database\">\n+        <column name=\"DB_ID\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"name\">\n+        <column name=\"NAME\" length=\"256\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"owner\">\n+        <column name=\"OWNER_NAME\" length=\"128\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"source\">\n+        <column name=\"SOURCE\" jdbc-type=\"MEDIUMTEXT\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"language\">\n+        <column name=\"LANG\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"returnType\">\n+        <column name=\"RET_TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"true\"/>\n+      </field>\n+      <field name=\"parameters\" table=\"SP_POS_ARGS\" >\n+        <collection element-type=\"MPosParam\"/>\n+        <join>\n+            <column name=\"SP_ID\"/>\n+        </join>\n+        <element>\n+          <embedded>\n+            <field name=\"name\">\n+              <column name=\"NAME\" jdbc-type=\"varchar\" length=\"256\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"type\">\n+              <column name=\"TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"length\">\n+              <column name=\"length\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"scale\">\n+              <column name=\"scale\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"isOut\">\n+              <column name=\"OUT\" allows-null=\"false\"/>\n+            </field>\n+          </embedded>\n+        </element>\n+      </field>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg1OTYzNA=="}, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjE3NjY5OA==", "bodyText": "What was proposed (but @kgyrtkirk correct me if I'm wrong) as an alternative solution is store the signature together with the body and optionally add a signature string to the table as well (this would be redundant). I think regardless of the representation, backward compatibility will always be a concern.\n\nThat was a suggestion to provide a way to store a human readable 1 liner for the function - I never thinked that we should parse it; the system should rely on the definition - the human readable field would be just for \"show\" to be used sysdb related tables/etc.\n\nThis is only true for now because HPL/SQL is an AST interpreter but if we ever want to make this performant probably we'll need to introduce a byte code VM at some point in the near future.\n\nIf at some point in time the \"parsing\" will prove to be a bottle neck - then we should address it at that point ; rather than try to address a problem which we may not even face - correct me if I'm wrong but I think we will only parse the function 1 time during the HS2 life cycle - which doesn't seem to be a big deal to me.\n\nOne thing I don't like about storing the signature in text is that the assumption that invoking the procedure will always require parsing. This is only true for now because HPL/SQL is an AST interpreter but if we ever want to make this performant probably we'll need to introduce a byte code VM at some point in the near future.\n\nI think we should clarify/separate 2 things - for stored procedures usually there will be 2 languages at play:\n\nthe \"host\" language - in this case HiveQL - which will accept the function definition\nthe \"foreign\" language - which may or may not need parsing; in case of \"HPL/SQL\" it will but that will not be true for all languages\n\nbecause this patch doesn't contain any HiveQL parser changes - I don't know how that part will work; could you give some example how these functions could be used by the user?\ncreate procedure x(a integer) as $$\ninsert into t values(a);\n$$ langugage hplsql;\n\ncall x(12);\n\n\nFor example postgres allows you to invoke procedures implemented in C where runtime information about the signature is not available.\n\nI don't know what feature you are refering to ; but afaik to register a function in postgres you should run a create function stmt - which will contain all the arguments/etc; and because of that I don't see any issue with other languages - as a create procedure call for any language; must contain the full defintion - which includes argument types/etc - so we will store that for any supported language..", "url": "https://github.com/apache/hive/pull/1542#discussion_r506176698", "createdAt": "2020-10-16T08:39:14Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,83 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">\n+      <datastore-identity>\n+        <column name=\"SP_ID\"/>\n+      </datastore-identity>\n+      <field name=\"createTime\">\n+        <column name=\"CREATE_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"lastAccessTime\">\n+        <column name=\"LAST_ACCESS_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"database\">\n+        <column name=\"DB_ID\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"name\">\n+        <column name=\"NAME\" length=\"256\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"owner\">\n+        <column name=\"OWNER_NAME\" length=\"128\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"source\">\n+        <column name=\"SOURCE\" jdbc-type=\"MEDIUMTEXT\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"language\">\n+        <column name=\"LANG\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"returnType\">\n+        <column name=\"RET_TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"true\"/>\n+      </field>\n+      <field name=\"parameters\" table=\"SP_POS_ARGS\" >\n+        <collection element-type=\"MPosParam\"/>\n+        <join>\n+            <column name=\"SP_ID\"/>\n+        </join>\n+        <element>\n+          <embedded>\n+            <field name=\"name\">\n+              <column name=\"NAME\" jdbc-type=\"varchar\" length=\"256\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"type\">\n+              <column name=\"TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"length\">\n+              <column name=\"length\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"scale\">\n+              <column name=\"scale\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"isOut\">\n+              <column name=\"OUT\" allows-null=\"false\"/>\n+            </field>\n+          </embedded>\n+        </element>\n+      </field>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg1OTYzNA=="}, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzUzNDg2Nw==", "bodyText": "That was a suggestion to provide a way to store a human readable..\n\nI understand it is only for showing the user, but it it feels like an afterthought, I don't really like it. It adds redundancy and it only has a single use. The structured information can be used to generate this output, (or in fact other outputs, like showing the parameters in a table) or to supply meta programming information to various tools, like code completion tools or method finders.\nFor example, search for the return type:\nint i = fn.. // find every function that starts with \"fn\" and returns an int\n\nAlthough this is a search for the return type not on the parameter but it's the same problem since the return type is also part of the signature.\nI we had higher order functions (in fact JavaScript has and that might be the 2nd supported language) then:\nfilter(fn.., listOfString); // find every function that starts with \"fn\" and takes a single string parameter\n\nMethod finder:\nThis might look like a bit scifi, there are only 2 programming environments I'm aware of, which know how to do this.\nMethodFinder.methodFor('somefile.txt', 'txt'); // find which method returns the extension part of filename, by an by example,\n\nIt will return:\n\"FilenameUtils.getExtension()\"\n\nThese things might sound unimportant but I think language design and tool development shouldn't be separated. Tool support should be considered from day 1 when creating a language.\n\nIf at some point in time the \"parsing\" will prove to be a bottle neck\n\nI'm was not talking about parsing as a bottleneck (though it could be) but interpreting the AST (method body) as a bottleneck. I can't think of any imperative language that can be taken seriously that works that way. Perhaps shell scripts, or early versions of Ruby which was notoriously slow so later they changed it.\n\nI think we should clarify/separate 2 things\n\nIf we only allow defining procedures in terms of the \"host language\", then this is true indeed. My assumption was that we might want to accept procedure definitions in terms of the \"foreign language\". For example function(x,y) {}. But ok, let's say this is not allowed. Then you're right, using the alternative format seems to be viable if we don't count the other issues.\nIf we go to that route, what would you do with the other columns which are also part of the signature, like \"LANG\", \"RET_TYPE\", \"NAME\" ?\nCREATE FUNCTION func1(a int) RETURNS int LANGUAGE XX BEGIN ... END;\n\n\nI don't know what feature you are refering\n\nIt's probably that one. I think that illustrates nicely why parsing is not always a prerequisite of calling a procedure.", "url": "https://github.com/apache/hive/pull/1542#discussion_r507534867", "createdAt": "2020-10-19T07:40:40Z", "author": {"login": "zeroflag"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,83 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">\n+      <datastore-identity>\n+        <column name=\"SP_ID\"/>\n+      </datastore-identity>\n+      <field name=\"createTime\">\n+        <column name=\"CREATE_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"lastAccessTime\">\n+        <column name=\"LAST_ACCESS_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"database\">\n+        <column name=\"DB_ID\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"name\">\n+        <column name=\"NAME\" length=\"256\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"owner\">\n+        <column name=\"OWNER_NAME\" length=\"128\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"source\">\n+        <column name=\"SOURCE\" jdbc-type=\"MEDIUMTEXT\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"language\">\n+        <column name=\"LANG\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"returnType\">\n+        <column name=\"RET_TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"true\"/>\n+      </field>\n+      <field name=\"parameters\" table=\"SP_POS_ARGS\" >\n+        <collection element-type=\"MPosParam\"/>\n+        <join>\n+            <column name=\"SP_ID\"/>\n+        </join>\n+        <element>\n+          <embedded>\n+            <field name=\"name\">\n+              <column name=\"NAME\" jdbc-type=\"varchar\" length=\"256\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"type\">\n+              <column name=\"TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"length\">\n+              <column name=\"length\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"scale\">\n+              <column name=\"scale\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"isOut\">\n+              <column name=\"OUT\" allows-null=\"false\"/>\n+            </field>\n+          </embedded>\n+        </element>\n+      </field>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg1OTYzNA=="}, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzYzNDQ5OA==", "bodyText": "I understand it is only for showing the user, but it it feels like an afterthought, I don't really like it.\n\nOkay; then we don't need to add and store it - we could also use a simple UDF to get the signature if we want that.\nAbout  search for the return type I think we should come up with a \"working solution first\" and go after these things when we need it (maybe never?)\nI think that MethodFinder.methodFor will need a lot more than just a signature...so it's a little bit too much\n\nThese things might sound unimportant but I think language design and tool development shouldn't be separated. Tool support should be considered from day 1 when creating a language.\n\nWe may leave this thing out completely right now - because we don't need it right now.\nI think instead of adding something which may not perfectly align with our future needs ; leaving something out will not lock us in at all.\n\nbut interpreting the AST (method body) as a bottleneck.\n\nthat's the problem of the language implementation itself - I think in case the language function is defined in some kind of \"text\" then we should store it as \"text\" - to have a less convoluted contract with the embedded language.\n\nIf we only allow defining procedures in terms of the \"host language\", then this is true indeed. My assumption was that we might want to accept procedure definitions in terms of the \"foreign language\".\n\nyes; we should only store those procedures/functions which are usable from the HiveQL - don't we?\n\nCREATE FUNCTION func1(a int) RETURNS int LANGUAGE XX BEGIN ... END;\n\nI think we should simply store the whole definition from CREATE to the closing ;.\nStoring anything less could become unparsable.\n\n\nI don't know what feature you are refering\n\n\n\nIt's probably that one. I think that illustrates nicely why parsing is not always a prerequisite of calling a procedure.\n\nThere might be some misunderstanding here by \"parsing\" - I mean to process it with the host sql language.\nIf the client language has performance issues doing parsing/etc is then that issue belong to the language itself.\nIn case the language is an on-the fly interpreted language which also happens to have a compiled version ; then that other version could either be registered as a separate language (and refer to a binary somehow) or the language could add some caching/etc to avoid unneccessary parsing overhead.\nCouldn't we do something like this for hplsql?\nIn any case: I think that extending the basic implementation with a \"generic blob storage\" option to provide additional services for the stored procedures (which could potentially be used to speed up sql function execution) should be a separate feature - and as such; should be discussed(and implemented) separetly.\n(Honestly I think there would be marginal benefits implementing this - and could be dodged by the client language implementation with a few caches/etc.)", "url": "https://github.com/apache/hive/pull/1542#discussion_r507634498", "createdAt": "2020-10-19T10:19:44Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,83 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">\n+      <datastore-identity>\n+        <column name=\"SP_ID\"/>\n+      </datastore-identity>\n+      <field name=\"createTime\">\n+        <column name=\"CREATE_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"lastAccessTime\">\n+        <column name=\"LAST_ACCESS_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"database\">\n+        <column name=\"DB_ID\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"name\">\n+        <column name=\"NAME\" length=\"256\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"owner\">\n+        <column name=\"OWNER_NAME\" length=\"128\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"source\">\n+        <column name=\"SOURCE\" jdbc-type=\"MEDIUMTEXT\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"language\">\n+        <column name=\"LANG\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"returnType\">\n+        <column name=\"RET_TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"true\"/>\n+      </field>\n+      <field name=\"parameters\" table=\"SP_POS_ARGS\" >\n+        <collection element-type=\"MPosParam\"/>\n+        <join>\n+            <column name=\"SP_ID\"/>\n+        </join>\n+        <element>\n+          <embedded>\n+            <field name=\"name\">\n+              <column name=\"NAME\" jdbc-type=\"varchar\" length=\"256\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"type\">\n+              <column name=\"TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"length\">\n+              <column name=\"length\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"scale\">\n+              <column name=\"scale\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"isOut\">\n+              <column name=\"OUT\" allows-null=\"false\"/>\n+            </field>\n+          </embedded>\n+        </element>\n+      </field>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg1OTYzNA=="}, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODYzODE0Mw==", "bodyText": "Discussed this further offline, let's try the string based approach for now and see how it goes. I'll modify the patch.", "url": "https://github.com/apache/hive/pull/1542#discussion_r508638143", "createdAt": "2020-10-20T15:51:50Z", "author": {"login": "zeroflag"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,83 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">\n+      <datastore-identity>\n+        <column name=\"SP_ID\"/>\n+      </datastore-identity>\n+      <field name=\"createTime\">\n+        <column name=\"CREATE_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"lastAccessTime\">\n+        <column name=\"LAST_ACCESS_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"database\">\n+        <column name=\"DB_ID\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"name\">\n+        <column name=\"NAME\" length=\"256\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"owner\">\n+        <column name=\"OWNER_NAME\" length=\"128\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"source\">\n+        <column name=\"SOURCE\" jdbc-type=\"MEDIUMTEXT\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"language\">\n+        <column name=\"LANG\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"returnType\">\n+        <column name=\"RET_TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"true\"/>\n+      </field>\n+      <field name=\"parameters\" table=\"SP_POS_ARGS\" >\n+        <collection element-type=\"MPosParam\"/>\n+        <join>\n+            <column name=\"SP_ID\"/>\n+        </join>\n+        <element>\n+          <embedded>\n+            <field name=\"name\">\n+              <column name=\"NAME\" jdbc-type=\"varchar\" length=\"256\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"type\">\n+              <column name=\"TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"length\">\n+              <column name=\"length\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"scale\">\n+              <column name=\"scale\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"isOut\">\n+              <column name=\"OUT\" allows-null=\"false\"/>\n+            </field>\n+          </embedded>\n+        </element>\n+      </field>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg1OTYzNA=="}, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMzA1NDc1OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/sql/derby/hive-schema-4.0.0.derby.sql", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNDozNTo1N1rOHbwFLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNDo0NjowMVrOHbwdKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg2MTM1OA==", "bodyText": "I think we should only add fields which are actually usefull and in use - because right now the accesstime would not be updated at all I don't think we should add it.", "url": "https://github.com/apache/hive/pull/1542#discussion_r498861358", "createdAt": "2020-10-02T14:35:57Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-server/src/main/sql/derby/hive-schema-4.0.0.derby.sql", "diffHunk": "@@ -786,6 +786,35 @@ CREATE TABLE \"APP\".\"REPLICATION_METRICS\" (\n CREATE INDEX \"POLICY_IDX\" ON \"APP\".\"REPLICATION_METRICS\" (\"RM_POLICY\");\n CREATE INDEX \"DUMP_IDX\" ON \"APP\".\"REPLICATION_METRICS\" (\"RM_DUMP_EXECUTION_ID\");\n \n+-- Create stored procedure tables\n+CREATE TABLE \"APP\".\"STORED_PROCS\" (\n+  \"SP_ID\" BIGINT NOT NULL,\n+  \"CREATE_TIME\" INTEGER NOT NULL,\n+  \"LAST_ACCESS_TIME\" INTEGER NOT NULL,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg2NzQ5Ng==", "bodyText": "the intention was to have something the represents the last modification date (maybe the name was chosen poorly), but ok I'll remove it, it is not used", "url": "https://github.com/apache/hive/pull/1542#discussion_r498867496", "createdAt": "2020-10-02T14:46:01Z", "author": {"login": "zeroflag"}, "path": "standalone-metastore/metastore-server/src/main/sql/derby/hive-schema-4.0.0.derby.sql", "diffHunk": "@@ -786,6 +786,35 @@ CREATE TABLE \"APP\".\"REPLICATION_METRICS\" (\n CREATE INDEX \"POLICY_IDX\" ON \"APP\".\"REPLICATION_METRICS\" (\"RM_POLICY\");\n CREATE INDEX \"DUMP_IDX\" ON \"APP\".\"REPLICATION_METRICS\" (\"RM_DUMP_EXECUTION_ID\");\n \n+-- Create stored procedure tables\n+CREATE TABLE \"APP\".\"STORED_PROCS\" (\n+  \"SP_ID\" BIGINT NOT NULL,\n+  \"CREATE_TIME\" INTEGER NOT NULL,\n+  \"LAST_ACCESS_TIME\" INTEGER NOT NULL,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg2MTM1OA=="}, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMzA3NDI2OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNDo0MToyMFrOHbwRgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQxNDozMjo1OVrOHcf3cQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg2NDUxNQ==", "bodyText": "this is the first occurence of MEDIUMTEXT in package.jdo - I don't know how well that will work\nwe had quite a few problems with \"long\" tableproperty values - and PARAM_VALUE was updated to use CLOB in oracle/etc\nthe most important would be to make sure that we can store the procedure in all supported metastore databases - if possible this should also be tested in some way (at least by hand)", "url": "https://github.com/apache/hive/pull/1542#discussion_r498864515", "createdAt": "2020-10-02T14:41:20Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,83 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">\n+      <datastore-identity>\n+        <column name=\"SP_ID\"/>\n+      </datastore-identity>\n+      <field name=\"createTime\">\n+        <column name=\"CREATE_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"lastAccessTime\">\n+        <column name=\"LAST_ACCESS_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"database\">\n+        <column name=\"DB_ID\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"name\">\n+        <column name=\"NAME\" length=\"256\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"owner\">\n+        <column name=\"OWNER_NAME\" length=\"128\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"source\">\n+        <column name=\"SOURCE\" jdbc-type=\"MEDIUMTEXT\" allows-null=\"false\"/>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg3ODc1Ng==", "bodyText": "I'll double check it, I remember having some problem with another textual datatype, I can't remember which one was that, that's why MEDIUMTEXT was chosen.", "url": "https://github.com/apache/hive/pull/1542#discussion_r498878756", "createdAt": "2020-10-02T15:04:14Z", "author": {"login": "zeroflag"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,83 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">\n+      <datastore-identity>\n+        <column name=\"SP_ID\"/>\n+      </datastore-identity>\n+      <field name=\"createTime\">\n+        <column name=\"CREATE_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"lastAccessTime\">\n+        <column name=\"LAST_ACCESS_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"database\">\n+        <column name=\"DB_ID\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"name\">\n+        <column name=\"NAME\" length=\"256\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"owner\">\n+        <column name=\"OWNER_NAME\" length=\"128\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"source\">\n+        <column name=\"SOURCE\" jdbc-type=\"MEDIUMTEXT\" allows-null=\"false\"/>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg2NDUxNQ=="}, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTY0NDI3Mw==", "bodyText": "I changed it to CLOB, that is already used at multiple places.", "url": "https://github.com/apache/hive/pull/1542#discussion_r499644273", "createdAt": "2020-10-05T14:32:59Z", "author": {"login": "zeroflag"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,83 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">\n+      <datastore-identity>\n+        <column name=\"SP_ID\"/>\n+      </datastore-identity>\n+      <field name=\"createTime\">\n+        <column name=\"CREATE_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"lastAccessTime\">\n+        <column name=\"LAST_ACCESS_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"database\">\n+        <column name=\"DB_ID\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"name\">\n+        <column name=\"NAME\" length=\"256\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"owner\">\n+        <column name=\"OWNER_NAME\" length=\"128\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"source\">\n+        <column name=\"SOURCE\" jdbc-type=\"MEDIUMTEXT\" allows-null=\"false\"/>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg2NDUxNQ=="}, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzc2MTM4OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.2.0-to-4.0.0.mysql.sql", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDowNDo1NFrOHoRDhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxMDo1MzowMFrOHo2Bgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk4NDUxOA==", "bodyText": "what will happen when the db is dropped? wouldn't this FK will restrict the DB from being dropped?", "url": "https://github.com/apache/hive/pull/1542#discussion_r511984518", "createdAt": "2020-10-26T14:04:54Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.2.0-to-4.0.0.mysql.sql", "diffHunk": "@@ -109,6 +109,20 @@ CREATE TABLE IF NOT EXISTS REPLICATION_METRICS (\n CREATE INDEX POLICY_IDX ON REPLICATION_METRICS (RM_POLICY);\n CREATE INDEX DUMP_IDX ON REPLICATION_METRICS (RM_DUMP_EXECUTION_ID);\n \n+-- Create stored procedure tables\n+CREATE TABLE STORED_PROCS (\n+  `SP_ID` BIGINT(20) NOT NULL,\n+  `CREATE_TIME` INT(11) NOT NULL,\n+  `DB_ID` BIGINT(20) NOT NULL,\n+  `NAME` VARCHAR(256) NOT NULL,\n+  `OWNER_NAME` VARCHAR(128) NOT NULL,\n+  `SOURCE` LONGTEXT NOT NULL,\n+  PRIMARY KEY (`SP_ID`)\n+);\n+\n+CREATE UNIQUE INDEX UNIQUESTOREDPROC ON STORED_PROCS (NAME, DB_ID);\n+ALTER TABLE `STORED_PROCS` ADD CONSTRAINT `STOREDPROC_FK1` FOREIGN KEY (`DB_ID`) REFERENCES DBS (`DB_ID`);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5MDIxMQ==", "bodyText": "good catch, fixed it.", "url": "https://github.com/apache/hive/pull/1542#discussion_r512590211", "createdAt": "2020-10-27T10:53:00Z", "author": {"login": "zeroflag"}, "path": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.2.0-to-4.0.0.mysql.sql", "diffHunk": "@@ -109,6 +109,20 @@ CREATE TABLE IF NOT EXISTS REPLICATION_METRICS (\n CREATE INDEX POLICY_IDX ON REPLICATION_METRICS (RM_POLICY);\n CREATE INDEX DUMP_IDX ON REPLICATION_METRICS (RM_DUMP_EXECUTION_ID);\n \n+-- Create stored procedure tables\n+CREATE TABLE STORED_PROCS (\n+  `SP_ID` BIGINT(20) NOT NULL,\n+  `CREATE_TIME` INT(11) NOT NULL,\n+  `DB_ID` BIGINT(20) NOT NULL,\n+  `NAME` VARCHAR(256) NOT NULL,\n+  `OWNER_NAME` VARCHAR(128) NOT NULL,\n+  `SOURCE` LONGTEXT NOT NULL,\n+  PRIMARY KEY (`SP_ID`)\n+);\n+\n+CREATE UNIQUE INDEX UNIQUESTOREDPROC ON STORED_PROCS (NAME, DB_ID);\n+ALTER TABLE `STORED_PROCS` ADD CONSTRAINT `STOREDPROC_FK1` FOREIGN KEY (`DB_ID`) REFERENCES DBS (`DB_ID`);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk4NDUxOA=="}, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzc5MTExOnYy", "diffSide": "RIGHT", "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDoxMToxN1rOHoRWZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxMDo1MzoyMVrOHo2Cdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk4OTM0OQ==", "bodyText": "these lines start with * *", "url": "https://github.com/apache/hive/pull/1542#discussion_r511989349", "createdAt": "2020-10-26T14:11:17Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java", "diffHunk": "@@ -1,780 +1,30 @@\n /*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  * Licensed to the Apache Software Foundation (ASF) under one", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5MDQ1NA==", "bodyText": "fixed", "url": "https://github.com/apache/hive/pull/1542#discussion_r512590454", "createdAt": "2020-10-27T10:53:21Z", "author": {"login": "zeroflag"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java", "diffHunk": "@@ -1,780 +1,30 @@\n /*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  * Licensed to the Apache Software Foundation (ASF) under one", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk4OTM0OQ=="}, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzc5NTQ5OnYy", "diffSide": "RIGHT", "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDoxMjoxNlrOHoRZJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTozODowM1rOHoViqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5MDA1NA==", "bodyText": "what does this returned boolean mean?", "url": "https://github.com/apache/hive/pull/1542#discussion_r511990054", "createdAt": "2020-10-26T14:12:16Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java", "diffHunk": "@@ -1,780 +1,30 @@\n /*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n  *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n  */\n \n package org.apache.hive.hplsql.functions;\n \n-import java.sql.ResultSet;\n-import java.sql.Date;\n-import java.sql.SQLException;\n-import java.text.SimpleDateFormat;\n-import java.util.ArrayList;\n-import java.util.Calendar;\n-import java.util.HashMap;\n-import java.util.Map;\n-import java.util.TimeZone;\n-import java.util.regex.Matcher;\n-import java.util.regex.Pattern;\n+import org.apache.hive.hplsql.HplsqlParser;\n \n-import org.apache.commons.lang3.StringUtils;\n-import org.antlr.v4.runtime.ParserRuleContext;\n-import org.apache.hive.hplsql.*;\n-\n-interface FuncCommand {\n-  void run(HplsqlParser.Expr_func_paramsContext ctx);\n-}\n-\n-interface FuncSpecCommand {\n-  void run(HplsqlParser.Expr_spec_funcContext ctx);\n-}\n-\n-/**\n- * HPL/SQL functions\n- */\n-public class Function {\n-  Exec exec;\n-  HashMap<String, FuncCommand> map = new HashMap<String, FuncCommand>();  \n-  HashMap<String, FuncSpecCommand> specMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, FuncSpecCommand> specSqlMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, HplsqlParser.Create_function_stmtContext> userMap = new HashMap<String, HplsqlParser.Create_function_stmtContext>();\n-  HashMap<String, HplsqlParser.Create_procedure_stmtContext> procMap = new HashMap<String, HplsqlParser.Create_procedure_stmtContext>();\n-  boolean trace = false; \n-  \n-  public Function(Exec e) {\n-    exec = e;  \n-    trace = exec.getTrace();\n-  }\n-  \n-  /** \n-   * Register functions\n-   */\n-  public void register(Function f) {    \n-  }\n-  \n-  /**\n-   * Execute a function\n-   */\n-  public void exec(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUser(name, ctx)) {\n-      return;\n-    }\n-    else if (isProc(name) && execProc(name, ctx, null)) {\n-      return;\n-    }\n-    if (name.indexOf(\".\") != -1) {               // Name can be qualified and spaces are allowed between parts\n-      String[] parts = name.split(\"\\\\.\");\n-      StringBuilder str = new StringBuilder();\n-      for (int i = 0; i < parts.length; i++) {\n-        if (i > 0) {\n-          str.append(\".\");\n-        }\n-        str.append(parts[i].trim());        \n-      }\n-      name = str.toString();      \n-    } \n-    if (trace && ctx != null && ctx.parent != null && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncCommand func = map.get(name.toUpperCase());    \n-    if (func != null) {\n-      func.run(ctx);\n-    }    \n-    else {\n-      info(ctx, \"Function not found: \" + name);\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * User-defined function in a SQL query\n-   */\n-  public void execSql(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUserSql(ctx, name)) {\n-      return;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(name);\n-    sql.append(\"(\");\n-    if (ctx != null) {\n-      int cnt = ctx.func_param().size();\n-      for (int i = 0; i < cnt; i++) {\n-        sql.append(evalPop(ctx.func_param(i).expr()));\n-        if (i + 1 < cnt) {\n-          sql.append(\", \");\n-        }\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-  }\n-  \n-  /**\n-   * Aggregate or window function in a SQL query\n-   */\n-  public void execAggWindowSql(HplsqlParser.Expr_agg_window_funcContext ctx) {\n-    exec.stackPush(exec.getFormattedText(ctx));\n-  }\n-  \n-  /**\n-   * Execute a user-defined function\n-   */\n-  public boolean execUser(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    if (trace) {\n-      trace(ctx, \"EXEC FUNCTION \" + name);\n-    }\n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    setCallParameters(ctx, actualParams, userCtx.create_routine_params(), null);\n-    if (userCtx.declare_block_inplace() != null) {\n-      visit(userCtx.declare_block_inplace());\n-    }\n-    visit(userCtx.single_block_stmt());\n-    exec.leaveScope(); \n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a HPL/SQL user-defined function in a query \n-   */\n-  public boolean execUserSql(HplsqlParser.Expr_func_paramsContext ctx, String name) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(\"hplsql('\");\n-    sql.append(name);\n-    sql.append(\"(\");\n-    int cnt = ctx.func_param().size();\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(\":\" + (i + 1));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")'\");\n-    if (cnt > 0) {\n-      sql.append(\", \");\n-    }\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(evalPop(ctx.func_param(i).expr()));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-    exec.registerUdf();\n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure as the entry point of the script (defined by -main option)\n-   */\n-  public boolean execProc(String name) {\n-    if (trace) {\n-      trace(\"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(\"Procedure not found\");\n-      return false;\n-    }    \n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(procCtx.create_routine_params());\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    return true;\n-  }\n-  \n-  /**\n-   * Check if the stored procedure with the specified name is defined\n-   */\n-  public boolean isProc(String name) {\n-    if (procMap.get(name.toUpperCase()) != null) {\n-      return true;\n-    }\n-    return false;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure using CALL or EXEC statement passing parameters\n-   */\n-  public boolean execProc(String name, HplsqlParser.Expr_func_paramsContext ctx, ParserRuleContext callCtx) {\n-    if (trace) {\n-      trace(callCtx, \"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(callCtx, \"Procedure not found\");\n-      return false;\n-    }    \n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    HashMap<String, Var> out = new HashMap<String, Var>();\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.declare_block_inplace() != null) {\n-      visit(procCtx.declare_block_inplace());\n-    }\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(ctx, actualParams, procCtx.create_routine_params(), out);\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    for (Map.Entry<String, Var> i : out.entrySet()) {      // Set OUT parameters\n-      exec.setVariable(i.getKey(), i.getValue());\n-    }\n-    return true;\n-  }\n-  \n-  /**\n-   * Set parameters for user-defined function call\n-   */\n-  public void setCallParameters(HplsqlParser.Expr_func_paramsContext actual, ArrayList<Var> actualValues, \n-                         HplsqlParser.Create_routine_paramsContext formal,\n-                         HashMap<String, Var> out) {\n-    if (actual == null || actual.func_param() == null || actualValues == null) {\n-      return;\n-    }\n-    int actualCnt = actualValues.size();\n-    int formalCnt = formal.create_routine_param_item().size();\n-    for (int i = 0; i < actualCnt; i++) {\n-      if (i >= formalCnt) {\n-        break;\n-      }\n-      HplsqlParser.ExprContext a = actual.func_param(i).expr(); \n-      HplsqlParser.Create_routine_param_itemContext p = getCallParameter(actual, formal, i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var var = setCallParameter(name, type, len, scale, actualValues.get(i));\n-      if (trace) {\n-        trace(actual, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      } \n-      if (out != null && a.expr_atom() != null && a.expr_atom().ident() != null &&\n-          (p.T_OUT() != null || p.T_INOUT() != null)) {\n-        String actualName = a.expr_atom().ident().getText();\n-        if (actualName != null) {\n-          out.put(actualName, var);  \n-        }         \n-      }\n-    }\n-  }\n-  \n-  /**\n-   * Set parameters for entry-point call (Main procedure defined by -main option)\n-   */\n-  void setCallParameters(HplsqlParser.Create_routine_paramsContext ctx) {\n-    int cnt = ctx.create_routine_param_item().size();\n-    for (int i = 0; i < cnt; i++) {\n-      HplsqlParser.Create_routine_param_itemContext p = ctx.create_routine_param_item(i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var value = exec.findVariable(name);\n-      Var var = setCallParameter(name, type, len, scale, value);\n-      if (trace) {\n-        trace(ctx, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      }      \n-    }\n-  }\n-  \n-  /**\n-   * Create a function or procedure parameter and set its value\n-   */\n-  Var setCallParameter(String name, String type, String len, String scale, Var value) {\n-    Var var = new Var(name, type, len, scale, null);\n-    var.cast(value);\n-    exec.addVariable(var);    \n-    return var;\n-  }\n-  \n-  /**\n-   * Get call parameter definition by name (if specified) or position\n-   */\n-  HplsqlParser.Create_routine_param_itemContext getCallParameter(HplsqlParser.Expr_func_paramsContext actual, \n-      HplsqlParser.Create_routine_paramsContext formal, int pos) {\n-    String named = null;\n-    int out_pos = pos;\n-    if (actual.func_param(pos).ident() != null) {\n-      named = actual.func_param(pos).ident().getText(); \n-      int cnt = formal.create_routine_param_item().size();\n-      for (int i = 0; i < cnt; i++) {\n-        if (named.equalsIgnoreCase(formal.create_routine_param_item(i).ident().getText())) {\n-          out_pos = i;\n-          break;\n-        }\n-      }\n-    }\n-    return formal.create_routine_param_item(out_pos);\n-  }  \n-  \n-  /**\n-   * Evaluate actual call parameters\n-   */\n-  public ArrayList<Var> getActualCallParameters(HplsqlParser.Expr_func_paramsContext actual) {\n-    if (actual == null || actual.func_param() == null) {\n-      return null;\n-    }\n-    int cnt = actual.func_param().size();\n-    ArrayList<Var> values = new ArrayList<Var>(cnt);\n-    for (int i = 0; i < cnt; i++) {\n-      values.add(evalPop(actual.func_param(i).expr()));\n-    }\n-    return values;\n-  }\n-  \n-  /**\n-   * Add a user-defined function\n-   */\n-  public void addUserFunction(HplsqlParser.Create_function_stmtContext ctx) {\n-    String name = ctx.ident().getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE FUNCTION \" + name);\n-    }\n-    userMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Add a user-defined procedure\n-   */\n-  public void addUserProcedure(HplsqlParser.Create_procedure_stmtContext ctx) {\n-    String name = ctx.ident(0).getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE PROCEDURE \" + name);\n-    }\n-    procMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Get the number of parameters in function call\n-   */\n-  public int getParamCount(HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (ctx == null) {\n-      return 0;\n-    }\n-    return ctx.func_param().size();\n-  }\n-    \n-  /**\n-   * Execute a special function\n-   */\n-  public void specExec(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else if(ctx.T_MAX_PART_STRING() != null) {\n-      execMaxPartString(ctx);\n-    } else if(ctx.T_MIN_PART_STRING() != null) {\n-      execMinPartString(ctx);\n-    } else if(ctx.T_MAX_PART_INT() != null) {\n-      execMaxPartInt(ctx);\n-    } else if(ctx.T_MIN_PART_INT() != null) {\n-      execMinPartInt(ctx);\n-    } else if(ctx.T_MAX_PART_DATE() != null) {\n-      execMaxPartDate(ctx);\n-    } else if(ctx.T_MIN_PART_DATE() != null) {\n-      execMinPartDate(ctx);\n-    } else if(ctx.T_PART_LOC() != null) {\n-      execPartLoc(ctx);\n-    } else {\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * Execute a special function in executable SQL statement\n-   */\n-  public void specExecSql(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specSqlMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else {\n-      exec.stackPush(exec.getFormattedText(ctx));\n-    }\n-  }\n-  \n-  /**\n-   * Get the current date\n-   */\n-  public void execCurrentDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"CURRENT_DATE\");\n-    }\n-    SimpleDateFormat f = new SimpleDateFormat(\"yyyy-MM-dd\");\n-    String s = f.format(Calendar.getInstance().getTime());\n-    exec.stackPush(new Var(Var.Type.DATE, Utils.toDate(s))); \n-  }\n-  \n-  /**\n-   * Execute MAX_PART_STRING function\n-   */\n-  public void execMaxPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_STRING function\n-   */\n-  public void execMinPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_INT function\n-   */\n-  public void execMaxPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_INT function\n-   */\n-  public void execMinPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_DATE function\n-   */\n-  public void execMaxPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_DATE function\n-   */\n-  public void execMinPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, false /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN or MAX partition function\n-   */\n-  public void execMinMaxPart(HplsqlParser.Expr_spec_funcContext ctx, Var.Type type, boolean max) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"SHOW PARTITIONS \" + tabname;    \n-    String colname = null;    \n-    int colnum = -1;\n-    int exprnum = ctx.expr().size();    \n-    // Column name \n-    if (ctx.expr(1) != null) {\n-      colname = evalPop(ctx.expr(1)).toString();\n-    } else {\n-      colnum = 0;\n-    }\n-    // Partition filter\n-    if (exprnum >= 4) {\n-      sql += \" PARTITION (\";\n-      int i = 2;\n-      while (i + 1 < exprnum) {\n-        String fcol = evalPop(ctx.expr(i)).toString();\n-        String fval = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += fcol + \"=\" + fval;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      String resultString = null;\n-      Long resultInt = null;\n-      Date resultDate = null;      \n-      while (rs.next()) {\n-        String[] parts = rs.getString(1).split(\"/\");\n-        // Find partition column by name\n-        if (colnum == -1) {\n-          for (int i = 0; i < parts.length; i++) {\n-            String[] name = parts[i].split(\"=\");\n-            if (name[0].equalsIgnoreCase(colname)) {\n-              colnum = i;\n-              break;\n-            }\n-          }\n-          // No partition column with the specified name exists\n-          if (colnum == -1) {\n-            evalNullClose(query, exec.conf.defaultConnection);\n-            return;\n-          }\n-        }\n-        String[] pair = parts[colnum].split(\"=\");\n-        if (type == Var.Type.STRING) {\n-          resultString = Utils.minMaxString(resultString, pair[1], max);          \n-        } \n-        else if (type == Var.Type.BIGINT) {\n-          resultInt = Utils.minMaxInt(resultInt, pair[1], max);          \n-        } \n-        else if (type == Var.Type.DATE) {\n-          resultDate = Utils.minMaxDate(resultDate, pair[1], max);\n-        }\n-      }\n-      if (resultString != null) {\n-        evalString(resultString);\n-      } \n-      else if (resultInt != null) {\n-        evalInt(resultInt);\n-      } \n-      else if (resultDate != null) {\n-        evalDate(resultDate);\n-      } \n-      else {\n-        evalNull();\n-      }\n-    } catch (SQLException e) {}  \n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Execute PART_LOC function\n-   */\n-  public void execPartLoc(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"DESCRIBE EXTENDED \" + tabname;    \n-    int exprnum = ctx.expr().size();   \n-    boolean hostname = false;\n-    // Partition filter\n-    if (exprnum > 1) {\n-      sql += \" PARTITION (\";\n-      int i = 1;\n-      while (i + 1 < exprnum) {\n-        String col = evalPop(ctx.expr(i)).toString();\n-        String val = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += col + \"=\" + val;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    // With host name\n-    if (exprnum % 2 == 0 && evalPop(ctx.expr(exprnum - 1)).intValue() == 1) {\n-      hostname = true;\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    String result = null;\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      while (rs.next()) {\n-        if (rs.getString(1).startsWith(\"Detailed Partition Information\")) {\n-          Matcher m = Pattern.compile(\".*, location:(.*?),.*\").matcher(rs.getString(2));\n-          if (m.find()) {\n-            result = m.group(1);\n-          }    \n-        }\n-      }\n-    } catch (SQLException e) {}  \n-    if (result != null) {\n-      // Remove the host name\n-      if (!hostname) {\n-        Matcher m = Pattern.compile(\".*://.*?(/.*)\").matcher(result); \n-        if (m.find()) {\n-          result = m.group(1);\n-        }\n-      }\n-      evalString(result);\n-    }    \n-    else {\n-      evalNull();\n-    }\n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Evaluate the expression and push the value to the stack\n-   */\n-  void eval(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-  }\n-\n-  /**\n-   * Evaluate the expression to the specified variable\n-   */\n-  void evalVar(Var var) {\n-    exec.stackPush(var); \n-  }\n-\n-  /**\n-   * Evaluate the expression to NULL\n-   */\n-  void evalNull() {\n-    exec.stackPush(Var.Null); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified String value\n-   */\n-  void evalString(String string) {\n-    exec.stackPush(new Var(string)); \n-  }\n-  \n-  void evalString(StringBuilder string) {\n-    evalString(string.toString()); \n-  }\n-\n-  /**\n-   * Evaluate the expression to specified Int value\n-   */\n-  void evalInt(Long i) {\n-    exec.stackPush(new Var(i)); \n-  }\n-  \n-  void evalInt(int i) {\n-    evalInt(Long.valueOf(i));\n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified Date value\n-   */\n-  void evalDate(Date date) {\n-    exec.stackPush(new Var(Var.Type.DATE, date)); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to NULL and close the query\n-   */\n-  void evalNullClose(Query query, String conn) {\n-    exec.stackPush(Var.Null); \n-    exec.closeQuery(query, conn);\n-    if(trace) {\n-      query.printStackTrace();\n-    }\n-  }\n-  \n-  /**\n-   * Evaluate the expression and pop value from the stack\n-   */\n-  Var evalPop(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-    return exec.stackPop();  \n-  }\n-  \n-  Var evalPop(ParserRuleContext ctx, int value) {\n-    if (ctx != null) {\n-      return evalPop(ctx);\n-    }\n-    return new Var(Long.valueOf(value));\n-  }\n-  \n-  /**\n-   * Execute rules\n-   */\n-  Integer visit(ParserRuleContext ctx) {\n-    return exec.visit(ctx);  \n-  } \n- \n-  /**\n-   * Execute children rules\n-   */\n-  Integer visitChildren(ParserRuleContext ctx) {\n-    return exec.visitChildren(ctx);  \n-  }  \n-  \n-  /**\n-   * Trace information\n-   */\n-  public void trace(ParserRuleContext ctx, String message) {\n-    if (trace) {\n-      exec.trace(ctx, message);\n-    }\n-  }\n-  \n-  public void trace(String message) {\n-    trace(null, message);\n-  }\n-  \n-  public void info(ParserRuleContext ctx, String message) {\n-    exec.info(ctx, message);\n-  }\n+public interface Function {\n+  boolean exec(String name, HplsqlParser.Expr_func_paramsContext ctx);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 797}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA1ODAyNA==", "bodyText": "true: function was found and executed, false if it was not found. The return value was already used and expected by the existing code so I kept it. There are some issues with how the existing implementation works, for example calling an undefined function evaluates to null, instead of throwing an error. We might want to revisit these things later, but this wasn't the scope of this patch.", "url": "https://github.com/apache/hive/pull/1542#discussion_r512058024", "createdAt": "2020-10-26T15:38:03Z", "author": {"login": "zeroflag"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java", "diffHunk": "@@ -1,780 +1,30 @@\n /*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n  *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n  */\n \n package org.apache.hive.hplsql.functions;\n \n-import java.sql.ResultSet;\n-import java.sql.Date;\n-import java.sql.SQLException;\n-import java.text.SimpleDateFormat;\n-import java.util.ArrayList;\n-import java.util.Calendar;\n-import java.util.HashMap;\n-import java.util.Map;\n-import java.util.TimeZone;\n-import java.util.regex.Matcher;\n-import java.util.regex.Pattern;\n+import org.apache.hive.hplsql.HplsqlParser;\n \n-import org.apache.commons.lang3.StringUtils;\n-import org.antlr.v4.runtime.ParserRuleContext;\n-import org.apache.hive.hplsql.*;\n-\n-interface FuncCommand {\n-  void run(HplsqlParser.Expr_func_paramsContext ctx);\n-}\n-\n-interface FuncSpecCommand {\n-  void run(HplsqlParser.Expr_spec_funcContext ctx);\n-}\n-\n-/**\n- * HPL/SQL functions\n- */\n-public class Function {\n-  Exec exec;\n-  HashMap<String, FuncCommand> map = new HashMap<String, FuncCommand>();  \n-  HashMap<String, FuncSpecCommand> specMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, FuncSpecCommand> specSqlMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, HplsqlParser.Create_function_stmtContext> userMap = new HashMap<String, HplsqlParser.Create_function_stmtContext>();\n-  HashMap<String, HplsqlParser.Create_procedure_stmtContext> procMap = new HashMap<String, HplsqlParser.Create_procedure_stmtContext>();\n-  boolean trace = false; \n-  \n-  public Function(Exec e) {\n-    exec = e;  \n-    trace = exec.getTrace();\n-  }\n-  \n-  /** \n-   * Register functions\n-   */\n-  public void register(Function f) {    \n-  }\n-  \n-  /**\n-   * Execute a function\n-   */\n-  public void exec(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUser(name, ctx)) {\n-      return;\n-    }\n-    else if (isProc(name) && execProc(name, ctx, null)) {\n-      return;\n-    }\n-    if (name.indexOf(\".\") != -1) {               // Name can be qualified and spaces are allowed between parts\n-      String[] parts = name.split(\"\\\\.\");\n-      StringBuilder str = new StringBuilder();\n-      for (int i = 0; i < parts.length; i++) {\n-        if (i > 0) {\n-          str.append(\".\");\n-        }\n-        str.append(parts[i].trim());        \n-      }\n-      name = str.toString();      \n-    } \n-    if (trace && ctx != null && ctx.parent != null && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncCommand func = map.get(name.toUpperCase());    \n-    if (func != null) {\n-      func.run(ctx);\n-    }    \n-    else {\n-      info(ctx, \"Function not found: \" + name);\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * User-defined function in a SQL query\n-   */\n-  public void execSql(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUserSql(ctx, name)) {\n-      return;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(name);\n-    sql.append(\"(\");\n-    if (ctx != null) {\n-      int cnt = ctx.func_param().size();\n-      for (int i = 0; i < cnt; i++) {\n-        sql.append(evalPop(ctx.func_param(i).expr()));\n-        if (i + 1 < cnt) {\n-          sql.append(\", \");\n-        }\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-  }\n-  \n-  /**\n-   * Aggregate or window function in a SQL query\n-   */\n-  public void execAggWindowSql(HplsqlParser.Expr_agg_window_funcContext ctx) {\n-    exec.stackPush(exec.getFormattedText(ctx));\n-  }\n-  \n-  /**\n-   * Execute a user-defined function\n-   */\n-  public boolean execUser(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    if (trace) {\n-      trace(ctx, \"EXEC FUNCTION \" + name);\n-    }\n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    setCallParameters(ctx, actualParams, userCtx.create_routine_params(), null);\n-    if (userCtx.declare_block_inplace() != null) {\n-      visit(userCtx.declare_block_inplace());\n-    }\n-    visit(userCtx.single_block_stmt());\n-    exec.leaveScope(); \n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a HPL/SQL user-defined function in a query \n-   */\n-  public boolean execUserSql(HplsqlParser.Expr_func_paramsContext ctx, String name) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(\"hplsql('\");\n-    sql.append(name);\n-    sql.append(\"(\");\n-    int cnt = ctx.func_param().size();\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(\":\" + (i + 1));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")'\");\n-    if (cnt > 0) {\n-      sql.append(\", \");\n-    }\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(evalPop(ctx.func_param(i).expr()));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-    exec.registerUdf();\n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure as the entry point of the script (defined by -main option)\n-   */\n-  public boolean execProc(String name) {\n-    if (trace) {\n-      trace(\"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(\"Procedure not found\");\n-      return false;\n-    }    \n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(procCtx.create_routine_params());\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    return true;\n-  }\n-  \n-  /**\n-   * Check if the stored procedure with the specified name is defined\n-   */\n-  public boolean isProc(String name) {\n-    if (procMap.get(name.toUpperCase()) != null) {\n-      return true;\n-    }\n-    return false;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure using CALL or EXEC statement passing parameters\n-   */\n-  public boolean execProc(String name, HplsqlParser.Expr_func_paramsContext ctx, ParserRuleContext callCtx) {\n-    if (trace) {\n-      trace(callCtx, \"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(callCtx, \"Procedure not found\");\n-      return false;\n-    }    \n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    HashMap<String, Var> out = new HashMap<String, Var>();\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.declare_block_inplace() != null) {\n-      visit(procCtx.declare_block_inplace());\n-    }\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(ctx, actualParams, procCtx.create_routine_params(), out);\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    for (Map.Entry<String, Var> i : out.entrySet()) {      // Set OUT parameters\n-      exec.setVariable(i.getKey(), i.getValue());\n-    }\n-    return true;\n-  }\n-  \n-  /**\n-   * Set parameters for user-defined function call\n-   */\n-  public void setCallParameters(HplsqlParser.Expr_func_paramsContext actual, ArrayList<Var> actualValues, \n-                         HplsqlParser.Create_routine_paramsContext formal,\n-                         HashMap<String, Var> out) {\n-    if (actual == null || actual.func_param() == null || actualValues == null) {\n-      return;\n-    }\n-    int actualCnt = actualValues.size();\n-    int formalCnt = formal.create_routine_param_item().size();\n-    for (int i = 0; i < actualCnt; i++) {\n-      if (i >= formalCnt) {\n-        break;\n-      }\n-      HplsqlParser.ExprContext a = actual.func_param(i).expr(); \n-      HplsqlParser.Create_routine_param_itemContext p = getCallParameter(actual, formal, i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var var = setCallParameter(name, type, len, scale, actualValues.get(i));\n-      if (trace) {\n-        trace(actual, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      } \n-      if (out != null && a.expr_atom() != null && a.expr_atom().ident() != null &&\n-          (p.T_OUT() != null || p.T_INOUT() != null)) {\n-        String actualName = a.expr_atom().ident().getText();\n-        if (actualName != null) {\n-          out.put(actualName, var);  \n-        }         \n-      }\n-    }\n-  }\n-  \n-  /**\n-   * Set parameters for entry-point call (Main procedure defined by -main option)\n-   */\n-  void setCallParameters(HplsqlParser.Create_routine_paramsContext ctx) {\n-    int cnt = ctx.create_routine_param_item().size();\n-    for (int i = 0; i < cnt; i++) {\n-      HplsqlParser.Create_routine_param_itemContext p = ctx.create_routine_param_item(i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var value = exec.findVariable(name);\n-      Var var = setCallParameter(name, type, len, scale, value);\n-      if (trace) {\n-        trace(ctx, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      }      \n-    }\n-  }\n-  \n-  /**\n-   * Create a function or procedure parameter and set its value\n-   */\n-  Var setCallParameter(String name, String type, String len, String scale, Var value) {\n-    Var var = new Var(name, type, len, scale, null);\n-    var.cast(value);\n-    exec.addVariable(var);    \n-    return var;\n-  }\n-  \n-  /**\n-   * Get call parameter definition by name (if specified) or position\n-   */\n-  HplsqlParser.Create_routine_param_itemContext getCallParameter(HplsqlParser.Expr_func_paramsContext actual, \n-      HplsqlParser.Create_routine_paramsContext formal, int pos) {\n-    String named = null;\n-    int out_pos = pos;\n-    if (actual.func_param(pos).ident() != null) {\n-      named = actual.func_param(pos).ident().getText(); \n-      int cnt = formal.create_routine_param_item().size();\n-      for (int i = 0; i < cnt; i++) {\n-        if (named.equalsIgnoreCase(formal.create_routine_param_item(i).ident().getText())) {\n-          out_pos = i;\n-          break;\n-        }\n-      }\n-    }\n-    return formal.create_routine_param_item(out_pos);\n-  }  \n-  \n-  /**\n-   * Evaluate actual call parameters\n-   */\n-  public ArrayList<Var> getActualCallParameters(HplsqlParser.Expr_func_paramsContext actual) {\n-    if (actual == null || actual.func_param() == null) {\n-      return null;\n-    }\n-    int cnt = actual.func_param().size();\n-    ArrayList<Var> values = new ArrayList<Var>(cnt);\n-    for (int i = 0; i < cnt; i++) {\n-      values.add(evalPop(actual.func_param(i).expr()));\n-    }\n-    return values;\n-  }\n-  \n-  /**\n-   * Add a user-defined function\n-   */\n-  public void addUserFunction(HplsqlParser.Create_function_stmtContext ctx) {\n-    String name = ctx.ident().getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE FUNCTION \" + name);\n-    }\n-    userMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Add a user-defined procedure\n-   */\n-  public void addUserProcedure(HplsqlParser.Create_procedure_stmtContext ctx) {\n-    String name = ctx.ident(0).getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE PROCEDURE \" + name);\n-    }\n-    procMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Get the number of parameters in function call\n-   */\n-  public int getParamCount(HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (ctx == null) {\n-      return 0;\n-    }\n-    return ctx.func_param().size();\n-  }\n-    \n-  /**\n-   * Execute a special function\n-   */\n-  public void specExec(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else if(ctx.T_MAX_PART_STRING() != null) {\n-      execMaxPartString(ctx);\n-    } else if(ctx.T_MIN_PART_STRING() != null) {\n-      execMinPartString(ctx);\n-    } else if(ctx.T_MAX_PART_INT() != null) {\n-      execMaxPartInt(ctx);\n-    } else if(ctx.T_MIN_PART_INT() != null) {\n-      execMinPartInt(ctx);\n-    } else if(ctx.T_MAX_PART_DATE() != null) {\n-      execMaxPartDate(ctx);\n-    } else if(ctx.T_MIN_PART_DATE() != null) {\n-      execMinPartDate(ctx);\n-    } else if(ctx.T_PART_LOC() != null) {\n-      execPartLoc(ctx);\n-    } else {\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * Execute a special function in executable SQL statement\n-   */\n-  public void specExecSql(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specSqlMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else {\n-      exec.stackPush(exec.getFormattedText(ctx));\n-    }\n-  }\n-  \n-  /**\n-   * Get the current date\n-   */\n-  public void execCurrentDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"CURRENT_DATE\");\n-    }\n-    SimpleDateFormat f = new SimpleDateFormat(\"yyyy-MM-dd\");\n-    String s = f.format(Calendar.getInstance().getTime());\n-    exec.stackPush(new Var(Var.Type.DATE, Utils.toDate(s))); \n-  }\n-  \n-  /**\n-   * Execute MAX_PART_STRING function\n-   */\n-  public void execMaxPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_STRING function\n-   */\n-  public void execMinPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_INT function\n-   */\n-  public void execMaxPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_INT function\n-   */\n-  public void execMinPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_DATE function\n-   */\n-  public void execMaxPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_DATE function\n-   */\n-  public void execMinPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, false /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN or MAX partition function\n-   */\n-  public void execMinMaxPart(HplsqlParser.Expr_spec_funcContext ctx, Var.Type type, boolean max) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"SHOW PARTITIONS \" + tabname;    \n-    String colname = null;    \n-    int colnum = -1;\n-    int exprnum = ctx.expr().size();    \n-    // Column name \n-    if (ctx.expr(1) != null) {\n-      colname = evalPop(ctx.expr(1)).toString();\n-    } else {\n-      colnum = 0;\n-    }\n-    // Partition filter\n-    if (exprnum >= 4) {\n-      sql += \" PARTITION (\";\n-      int i = 2;\n-      while (i + 1 < exprnum) {\n-        String fcol = evalPop(ctx.expr(i)).toString();\n-        String fval = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += fcol + \"=\" + fval;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      String resultString = null;\n-      Long resultInt = null;\n-      Date resultDate = null;      \n-      while (rs.next()) {\n-        String[] parts = rs.getString(1).split(\"/\");\n-        // Find partition column by name\n-        if (colnum == -1) {\n-          for (int i = 0; i < parts.length; i++) {\n-            String[] name = parts[i].split(\"=\");\n-            if (name[0].equalsIgnoreCase(colname)) {\n-              colnum = i;\n-              break;\n-            }\n-          }\n-          // No partition column with the specified name exists\n-          if (colnum == -1) {\n-            evalNullClose(query, exec.conf.defaultConnection);\n-            return;\n-          }\n-        }\n-        String[] pair = parts[colnum].split(\"=\");\n-        if (type == Var.Type.STRING) {\n-          resultString = Utils.minMaxString(resultString, pair[1], max);          \n-        } \n-        else if (type == Var.Type.BIGINT) {\n-          resultInt = Utils.minMaxInt(resultInt, pair[1], max);          \n-        } \n-        else if (type == Var.Type.DATE) {\n-          resultDate = Utils.minMaxDate(resultDate, pair[1], max);\n-        }\n-      }\n-      if (resultString != null) {\n-        evalString(resultString);\n-      } \n-      else if (resultInt != null) {\n-        evalInt(resultInt);\n-      } \n-      else if (resultDate != null) {\n-        evalDate(resultDate);\n-      } \n-      else {\n-        evalNull();\n-      }\n-    } catch (SQLException e) {}  \n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Execute PART_LOC function\n-   */\n-  public void execPartLoc(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"DESCRIBE EXTENDED \" + tabname;    \n-    int exprnum = ctx.expr().size();   \n-    boolean hostname = false;\n-    // Partition filter\n-    if (exprnum > 1) {\n-      sql += \" PARTITION (\";\n-      int i = 1;\n-      while (i + 1 < exprnum) {\n-        String col = evalPop(ctx.expr(i)).toString();\n-        String val = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += col + \"=\" + val;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    // With host name\n-    if (exprnum % 2 == 0 && evalPop(ctx.expr(exprnum - 1)).intValue() == 1) {\n-      hostname = true;\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    String result = null;\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      while (rs.next()) {\n-        if (rs.getString(1).startsWith(\"Detailed Partition Information\")) {\n-          Matcher m = Pattern.compile(\".*, location:(.*?),.*\").matcher(rs.getString(2));\n-          if (m.find()) {\n-            result = m.group(1);\n-          }    \n-        }\n-      }\n-    } catch (SQLException e) {}  \n-    if (result != null) {\n-      // Remove the host name\n-      if (!hostname) {\n-        Matcher m = Pattern.compile(\".*://.*?(/.*)\").matcher(result); \n-        if (m.find()) {\n-          result = m.group(1);\n-        }\n-      }\n-      evalString(result);\n-    }    \n-    else {\n-      evalNull();\n-    }\n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Evaluate the expression and push the value to the stack\n-   */\n-  void eval(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-  }\n-\n-  /**\n-   * Evaluate the expression to the specified variable\n-   */\n-  void evalVar(Var var) {\n-    exec.stackPush(var); \n-  }\n-\n-  /**\n-   * Evaluate the expression to NULL\n-   */\n-  void evalNull() {\n-    exec.stackPush(Var.Null); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified String value\n-   */\n-  void evalString(String string) {\n-    exec.stackPush(new Var(string)); \n-  }\n-  \n-  void evalString(StringBuilder string) {\n-    evalString(string.toString()); \n-  }\n-\n-  /**\n-   * Evaluate the expression to specified Int value\n-   */\n-  void evalInt(Long i) {\n-    exec.stackPush(new Var(i)); \n-  }\n-  \n-  void evalInt(int i) {\n-    evalInt(Long.valueOf(i));\n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified Date value\n-   */\n-  void evalDate(Date date) {\n-    exec.stackPush(new Var(Var.Type.DATE, date)); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to NULL and close the query\n-   */\n-  void evalNullClose(Query query, String conn) {\n-    exec.stackPush(Var.Null); \n-    exec.closeQuery(query, conn);\n-    if(trace) {\n-      query.printStackTrace();\n-    }\n-  }\n-  \n-  /**\n-   * Evaluate the expression and pop value from the stack\n-   */\n-  Var evalPop(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-    return exec.stackPop();  \n-  }\n-  \n-  Var evalPop(ParserRuleContext ctx, int value) {\n-    if (ctx != null) {\n-      return evalPop(ctx);\n-    }\n-    return new Var(Long.valueOf(value));\n-  }\n-  \n-  /**\n-   * Execute rules\n-   */\n-  Integer visit(ParserRuleContext ctx) {\n-    return exec.visit(ctx);  \n-  } \n- \n-  /**\n-   * Execute children rules\n-   */\n-  Integer visitChildren(ParserRuleContext ctx) {\n-    return exec.visitChildren(ctx);  \n-  }  \n-  \n-  /**\n-   * Trace information\n-   */\n-  public void trace(ParserRuleContext ctx, String message) {\n-    if (trace) {\n-      exec.trace(ctx, message);\n-    }\n-  }\n-  \n-  public void trace(String message) {\n-    trace(null, message);\n-  }\n-  \n-  public void info(ParserRuleContext ctx, String message) {\n-    exec.info(ctx, message);\n-  }\n+public interface Function {\n+  boolean exec(String name, HplsqlParser.Expr_func_paramsContext ctx);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5MDA1NA=="}, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 797}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzgwMDMwOnYy", "diffSide": "RIGHT", "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDoxMzoyMVrOHoRcGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTozODoyMlrOHoVjsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5MDgxMQ==", "bodyText": "what does exists mean? an implementation of this interface is a Function - which should have a name ; so a getName would probably fit better", "url": "https://github.com/apache/hive/pull/1542#discussion_r511990811", "createdAt": "2020-10-26T14:13:21Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java", "diffHunk": "@@ -1,780 +1,30 @@\n /*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n  *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n  */\n \n package org.apache.hive.hplsql.functions;\n \n-import java.sql.ResultSet;\n-import java.sql.Date;\n-import java.sql.SQLException;\n-import java.text.SimpleDateFormat;\n-import java.util.ArrayList;\n-import java.util.Calendar;\n-import java.util.HashMap;\n-import java.util.Map;\n-import java.util.TimeZone;\n-import java.util.regex.Matcher;\n-import java.util.regex.Pattern;\n+import org.apache.hive.hplsql.HplsqlParser;\n \n-import org.apache.commons.lang3.StringUtils;\n-import org.antlr.v4.runtime.ParserRuleContext;\n-import org.apache.hive.hplsql.*;\n-\n-interface FuncCommand {\n-  void run(HplsqlParser.Expr_func_paramsContext ctx);\n-}\n-\n-interface FuncSpecCommand {\n-  void run(HplsqlParser.Expr_spec_funcContext ctx);\n-}\n-\n-/**\n- * HPL/SQL functions\n- */\n-public class Function {\n-  Exec exec;\n-  HashMap<String, FuncCommand> map = new HashMap<String, FuncCommand>();  \n-  HashMap<String, FuncSpecCommand> specMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, FuncSpecCommand> specSqlMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, HplsqlParser.Create_function_stmtContext> userMap = new HashMap<String, HplsqlParser.Create_function_stmtContext>();\n-  HashMap<String, HplsqlParser.Create_procedure_stmtContext> procMap = new HashMap<String, HplsqlParser.Create_procedure_stmtContext>();\n-  boolean trace = false; \n-  \n-  public Function(Exec e) {\n-    exec = e;  \n-    trace = exec.getTrace();\n-  }\n-  \n-  /** \n-   * Register functions\n-   */\n-  public void register(Function f) {    \n-  }\n-  \n-  /**\n-   * Execute a function\n-   */\n-  public void exec(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUser(name, ctx)) {\n-      return;\n-    }\n-    else if (isProc(name) && execProc(name, ctx, null)) {\n-      return;\n-    }\n-    if (name.indexOf(\".\") != -1) {               // Name can be qualified and spaces are allowed between parts\n-      String[] parts = name.split(\"\\\\.\");\n-      StringBuilder str = new StringBuilder();\n-      for (int i = 0; i < parts.length; i++) {\n-        if (i > 0) {\n-          str.append(\".\");\n-        }\n-        str.append(parts[i].trim());        \n-      }\n-      name = str.toString();      \n-    } \n-    if (trace && ctx != null && ctx.parent != null && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncCommand func = map.get(name.toUpperCase());    \n-    if (func != null) {\n-      func.run(ctx);\n-    }    \n-    else {\n-      info(ctx, \"Function not found: \" + name);\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * User-defined function in a SQL query\n-   */\n-  public void execSql(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUserSql(ctx, name)) {\n-      return;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(name);\n-    sql.append(\"(\");\n-    if (ctx != null) {\n-      int cnt = ctx.func_param().size();\n-      for (int i = 0; i < cnt; i++) {\n-        sql.append(evalPop(ctx.func_param(i).expr()));\n-        if (i + 1 < cnt) {\n-          sql.append(\", \");\n-        }\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-  }\n-  \n-  /**\n-   * Aggregate or window function in a SQL query\n-   */\n-  public void execAggWindowSql(HplsqlParser.Expr_agg_window_funcContext ctx) {\n-    exec.stackPush(exec.getFormattedText(ctx));\n-  }\n-  \n-  /**\n-   * Execute a user-defined function\n-   */\n-  public boolean execUser(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    if (trace) {\n-      trace(ctx, \"EXEC FUNCTION \" + name);\n-    }\n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    setCallParameters(ctx, actualParams, userCtx.create_routine_params(), null);\n-    if (userCtx.declare_block_inplace() != null) {\n-      visit(userCtx.declare_block_inplace());\n-    }\n-    visit(userCtx.single_block_stmt());\n-    exec.leaveScope(); \n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a HPL/SQL user-defined function in a query \n-   */\n-  public boolean execUserSql(HplsqlParser.Expr_func_paramsContext ctx, String name) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(\"hplsql('\");\n-    sql.append(name);\n-    sql.append(\"(\");\n-    int cnt = ctx.func_param().size();\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(\":\" + (i + 1));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")'\");\n-    if (cnt > 0) {\n-      sql.append(\", \");\n-    }\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(evalPop(ctx.func_param(i).expr()));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-    exec.registerUdf();\n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure as the entry point of the script (defined by -main option)\n-   */\n-  public boolean execProc(String name) {\n-    if (trace) {\n-      trace(\"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(\"Procedure not found\");\n-      return false;\n-    }    \n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(procCtx.create_routine_params());\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    return true;\n-  }\n-  \n-  /**\n-   * Check if the stored procedure with the specified name is defined\n-   */\n-  public boolean isProc(String name) {\n-    if (procMap.get(name.toUpperCase()) != null) {\n-      return true;\n-    }\n-    return false;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure using CALL or EXEC statement passing parameters\n-   */\n-  public boolean execProc(String name, HplsqlParser.Expr_func_paramsContext ctx, ParserRuleContext callCtx) {\n-    if (trace) {\n-      trace(callCtx, \"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(callCtx, \"Procedure not found\");\n-      return false;\n-    }    \n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    HashMap<String, Var> out = new HashMap<String, Var>();\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.declare_block_inplace() != null) {\n-      visit(procCtx.declare_block_inplace());\n-    }\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(ctx, actualParams, procCtx.create_routine_params(), out);\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    for (Map.Entry<String, Var> i : out.entrySet()) {      // Set OUT parameters\n-      exec.setVariable(i.getKey(), i.getValue());\n-    }\n-    return true;\n-  }\n-  \n-  /**\n-   * Set parameters for user-defined function call\n-   */\n-  public void setCallParameters(HplsqlParser.Expr_func_paramsContext actual, ArrayList<Var> actualValues, \n-                         HplsqlParser.Create_routine_paramsContext formal,\n-                         HashMap<String, Var> out) {\n-    if (actual == null || actual.func_param() == null || actualValues == null) {\n-      return;\n-    }\n-    int actualCnt = actualValues.size();\n-    int formalCnt = formal.create_routine_param_item().size();\n-    for (int i = 0; i < actualCnt; i++) {\n-      if (i >= formalCnt) {\n-        break;\n-      }\n-      HplsqlParser.ExprContext a = actual.func_param(i).expr(); \n-      HplsqlParser.Create_routine_param_itemContext p = getCallParameter(actual, formal, i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var var = setCallParameter(name, type, len, scale, actualValues.get(i));\n-      if (trace) {\n-        trace(actual, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      } \n-      if (out != null && a.expr_atom() != null && a.expr_atom().ident() != null &&\n-          (p.T_OUT() != null || p.T_INOUT() != null)) {\n-        String actualName = a.expr_atom().ident().getText();\n-        if (actualName != null) {\n-          out.put(actualName, var);  \n-        }         \n-      }\n-    }\n-  }\n-  \n-  /**\n-   * Set parameters for entry-point call (Main procedure defined by -main option)\n-   */\n-  void setCallParameters(HplsqlParser.Create_routine_paramsContext ctx) {\n-    int cnt = ctx.create_routine_param_item().size();\n-    for (int i = 0; i < cnt; i++) {\n-      HplsqlParser.Create_routine_param_itemContext p = ctx.create_routine_param_item(i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var value = exec.findVariable(name);\n-      Var var = setCallParameter(name, type, len, scale, value);\n-      if (trace) {\n-        trace(ctx, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      }      \n-    }\n-  }\n-  \n-  /**\n-   * Create a function or procedure parameter and set its value\n-   */\n-  Var setCallParameter(String name, String type, String len, String scale, Var value) {\n-    Var var = new Var(name, type, len, scale, null);\n-    var.cast(value);\n-    exec.addVariable(var);    \n-    return var;\n-  }\n-  \n-  /**\n-   * Get call parameter definition by name (if specified) or position\n-   */\n-  HplsqlParser.Create_routine_param_itemContext getCallParameter(HplsqlParser.Expr_func_paramsContext actual, \n-      HplsqlParser.Create_routine_paramsContext formal, int pos) {\n-    String named = null;\n-    int out_pos = pos;\n-    if (actual.func_param(pos).ident() != null) {\n-      named = actual.func_param(pos).ident().getText(); \n-      int cnt = formal.create_routine_param_item().size();\n-      for (int i = 0; i < cnt; i++) {\n-        if (named.equalsIgnoreCase(formal.create_routine_param_item(i).ident().getText())) {\n-          out_pos = i;\n-          break;\n-        }\n-      }\n-    }\n-    return formal.create_routine_param_item(out_pos);\n-  }  \n-  \n-  /**\n-   * Evaluate actual call parameters\n-   */\n-  public ArrayList<Var> getActualCallParameters(HplsqlParser.Expr_func_paramsContext actual) {\n-    if (actual == null || actual.func_param() == null) {\n-      return null;\n-    }\n-    int cnt = actual.func_param().size();\n-    ArrayList<Var> values = new ArrayList<Var>(cnt);\n-    for (int i = 0; i < cnt; i++) {\n-      values.add(evalPop(actual.func_param(i).expr()));\n-    }\n-    return values;\n-  }\n-  \n-  /**\n-   * Add a user-defined function\n-   */\n-  public void addUserFunction(HplsqlParser.Create_function_stmtContext ctx) {\n-    String name = ctx.ident().getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE FUNCTION \" + name);\n-    }\n-    userMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Add a user-defined procedure\n-   */\n-  public void addUserProcedure(HplsqlParser.Create_procedure_stmtContext ctx) {\n-    String name = ctx.ident(0).getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE PROCEDURE \" + name);\n-    }\n-    procMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Get the number of parameters in function call\n-   */\n-  public int getParamCount(HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (ctx == null) {\n-      return 0;\n-    }\n-    return ctx.func_param().size();\n-  }\n-    \n-  /**\n-   * Execute a special function\n-   */\n-  public void specExec(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else if(ctx.T_MAX_PART_STRING() != null) {\n-      execMaxPartString(ctx);\n-    } else if(ctx.T_MIN_PART_STRING() != null) {\n-      execMinPartString(ctx);\n-    } else if(ctx.T_MAX_PART_INT() != null) {\n-      execMaxPartInt(ctx);\n-    } else if(ctx.T_MIN_PART_INT() != null) {\n-      execMinPartInt(ctx);\n-    } else if(ctx.T_MAX_PART_DATE() != null) {\n-      execMaxPartDate(ctx);\n-    } else if(ctx.T_MIN_PART_DATE() != null) {\n-      execMinPartDate(ctx);\n-    } else if(ctx.T_PART_LOC() != null) {\n-      execPartLoc(ctx);\n-    } else {\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * Execute a special function in executable SQL statement\n-   */\n-  public void specExecSql(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specSqlMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else {\n-      exec.stackPush(exec.getFormattedText(ctx));\n-    }\n-  }\n-  \n-  /**\n-   * Get the current date\n-   */\n-  public void execCurrentDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"CURRENT_DATE\");\n-    }\n-    SimpleDateFormat f = new SimpleDateFormat(\"yyyy-MM-dd\");\n-    String s = f.format(Calendar.getInstance().getTime());\n-    exec.stackPush(new Var(Var.Type.DATE, Utils.toDate(s))); \n-  }\n-  \n-  /**\n-   * Execute MAX_PART_STRING function\n-   */\n-  public void execMaxPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_STRING function\n-   */\n-  public void execMinPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_INT function\n-   */\n-  public void execMaxPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_INT function\n-   */\n-  public void execMinPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_DATE function\n-   */\n-  public void execMaxPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_DATE function\n-   */\n-  public void execMinPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, false /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN or MAX partition function\n-   */\n-  public void execMinMaxPart(HplsqlParser.Expr_spec_funcContext ctx, Var.Type type, boolean max) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"SHOW PARTITIONS \" + tabname;    \n-    String colname = null;    \n-    int colnum = -1;\n-    int exprnum = ctx.expr().size();    \n-    // Column name \n-    if (ctx.expr(1) != null) {\n-      colname = evalPop(ctx.expr(1)).toString();\n-    } else {\n-      colnum = 0;\n-    }\n-    // Partition filter\n-    if (exprnum >= 4) {\n-      sql += \" PARTITION (\";\n-      int i = 2;\n-      while (i + 1 < exprnum) {\n-        String fcol = evalPop(ctx.expr(i)).toString();\n-        String fval = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += fcol + \"=\" + fval;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      String resultString = null;\n-      Long resultInt = null;\n-      Date resultDate = null;      \n-      while (rs.next()) {\n-        String[] parts = rs.getString(1).split(\"/\");\n-        // Find partition column by name\n-        if (colnum == -1) {\n-          for (int i = 0; i < parts.length; i++) {\n-            String[] name = parts[i].split(\"=\");\n-            if (name[0].equalsIgnoreCase(colname)) {\n-              colnum = i;\n-              break;\n-            }\n-          }\n-          // No partition column with the specified name exists\n-          if (colnum == -1) {\n-            evalNullClose(query, exec.conf.defaultConnection);\n-            return;\n-          }\n-        }\n-        String[] pair = parts[colnum].split(\"=\");\n-        if (type == Var.Type.STRING) {\n-          resultString = Utils.minMaxString(resultString, pair[1], max);          \n-        } \n-        else if (type == Var.Type.BIGINT) {\n-          resultInt = Utils.minMaxInt(resultInt, pair[1], max);          \n-        } \n-        else if (type == Var.Type.DATE) {\n-          resultDate = Utils.minMaxDate(resultDate, pair[1], max);\n-        }\n-      }\n-      if (resultString != null) {\n-        evalString(resultString);\n-      } \n-      else if (resultInt != null) {\n-        evalInt(resultInt);\n-      } \n-      else if (resultDate != null) {\n-        evalDate(resultDate);\n-      } \n-      else {\n-        evalNull();\n-      }\n-    } catch (SQLException e) {}  \n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Execute PART_LOC function\n-   */\n-  public void execPartLoc(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"DESCRIBE EXTENDED \" + tabname;    \n-    int exprnum = ctx.expr().size();   \n-    boolean hostname = false;\n-    // Partition filter\n-    if (exprnum > 1) {\n-      sql += \" PARTITION (\";\n-      int i = 1;\n-      while (i + 1 < exprnum) {\n-        String col = evalPop(ctx.expr(i)).toString();\n-        String val = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += col + \"=\" + val;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    // With host name\n-    if (exprnum % 2 == 0 && evalPop(ctx.expr(exprnum - 1)).intValue() == 1) {\n-      hostname = true;\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    String result = null;\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      while (rs.next()) {\n-        if (rs.getString(1).startsWith(\"Detailed Partition Information\")) {\n-          Matcher m = Pattern.compile(\".*, location:(.*?),.*\").matcher(rs.getString(2));\n-          if (m.find()) {\n-            result = m.group(1);\n-          }    \n-        }\n-      }\n-    } catch (SQLException e) {}  \n-    if (result != null) {\n-      // Remove the host name\n-      if (!hostname) {\n-        Matcher m = Pattern.compile(\".*://.*?(/.*)\").matcher(result); \n-        if (m.find()) {\n-          result = m.group(1);\n-        }\n-      }\n-      evalString(result);\n-    }    \n-    else {\n-      evalNull();\n-    }\n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Evaluate the expression and push the value to the stack\n-   */\n-  void eval(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-  }\n-\n-  /**\n-   * Evaluate the expression to the specified variable\n-   */\n-  void evalVar(Var var) {\n-    exec.stackPush(var); \n-  }\n-\n-  /**\n-   * Evaluate the expression to NULL\n-   */\n-  void evalNull() {\n-    exec.stackPush(Var.Null); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified String value\n-   */\n-  void evalString(String string) {\n-    exec.stackPush(new Var(string)); \n-  }\n-  \n-  void evalString(StringBuilder string) {\n-    evalString(string.toString()); \n-  }\n-\n-  /**\n-   * Evaluate the expression to specified Int value\n-   */\n-  void evalInt(Long i) {\n-    exec.stackPush(new Var(i)); \n-  }\n-  \n-  void evalInt(int i) {\n-    evalInt(Long.valueOf(i));\n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified Date value\n-   */\n-  void evalDate(Date date) {\n-    exec.stackPush(new Var(Var.Type.DATE, date)); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to NULL and close the query\n-   */\n-  void evalNullClose(Query query, String conn) {\n-    exec.stackPush(Var.Null); \n-    exec.closeQuery(query, conn);\n-    if(trace) {\n-      query.printStackTrace();\n-    }\n-  }\n-  \n-  /**\n-   * Evaluate the expression and pop value from the stack\n-   */\n-  Var evalPop(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-    return exec.stackPop();  \n-  }\n-  \n-  Var evalPop(ParserRuleContext ctx, int value) {\n-    if (ctx != null) {\n-      return evalPop(ctx);\n-    }\n-    return new Var(Long.valueOf(value));\n-  }\n-  \n-  /**\n-   * Execute rules\n-   */\n-  Integer visit(ParserRuleContext ctx) {\n-    return exec.visit(ctx);  \n-  } \n- \n-  /**\n-   * Execute children rules\n-   */\n-  Integer visitChildren(ParserRuleContext ctx) {\n-    return exec.visitChildren(ctx);  \n-  }  \n-  \n-  /**\n-   * Trace information\n-   */\n-  public void trace(ParserRuleContext ctx, String message) {\n-    if (trace) {\n-      exec.trace(ctx, message);\n-    }\n-  }\n-  \n-  public void trace(String message) {\n-    trace(null, message);\n-  }\n-  \n-  public void info(ParserRuleContext ctx, String message) {\n-    exec.info(ctx, message);\n-  }\n+public interface Function {\n+  boolean exec(String name, HplsqlParser.Expr_func_paramsContext ctx);\n+  void addUserFunction(HplsqlParser.Create_function_stmtContext ctx);\n+  void addUserProcedure(HplsqlParser.Create_procedure_stmtContext ctx);\n+  boolean exists(String name);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 800}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA1ODI5MQ==", "bodyText": "The terminology came from the existing code, it doesn't represent a particular function but it was already called this way (it was a class not an interface but worked the same way). It's more like a registry for functions but I didn't want to change it as part of this patch. Fixing this right now is out of scope.", "url": "https://github.com/apache/hive/pull/1542#discussion_r512058291", "createdAt": "2020-10-26T15:38:22Z", "author": {"login": "zeroflag"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java", "diffHunk": "@@ -1,780 +1,30 @@\n /*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n  *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n  */\n \n package org.apache.hive.hplsql.functions;\n \n-import java.sql.ResultSet;\n-import java.sql.Date;\n-import java.sql.SQLException;\n-import java.text.SimpleDateFormat;\n-import java.util.ArrayList;\n-import java.util.Calendar;\n-import java.util.HashMap;\n-import java.util.Map;\n-import java.util.TimeZone;\n-import java.util.regex.Matcher;\n-import java.util.regex.Pattern;\n+import org.apache.hive.hplsql.HplsqlParser;\n \n-import org.apache.commons.lang3.StringUtils;\n-import org.antlr.v4.runtime.ParserRuleContext;\n-import org.apache.hive.hplsql.*;\n-\n-interface FuncCommand {\n-  void run(HplsqlParser.Expr_func_paramsContext ctx);\n-}\n-\n-interface FuncSpecCommand {\n-  void run(HplsqlParser.Expr_spec_funcContext ctx);\n-}\n-\n-/**\n- * HPL/SQL functions\n- */\n-public class Function {\n-  Exec exec;\n-  HashMap<String, FuncCommand> map = new HashMap<String, FuncCommand>();  \n-  HashMap<String, FuncSpecCommand> specMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, FuncSpecCommand> specSqlMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, HplsqlParser.Create_function_stmtContext> userMap = new HashMap<String, HplsqlParser.Create_function_stmtContext>();\n-  HashMap<String, HplsqlParser.Create_procedure_stmtContext> procMap = new HashMap<String, HplsqlParser.Create_procedure_stmtContext>();\n-  boolean trace = false; \n-  \n-  public Function(Exec e) {\n-    exec = e;  \n-    trace = exec.getTrace();\n-  }\n-  \n-  /** \n-   * Register functions\n-   */\n-  public void register(Function f) {    \n-  }\n-  \n-  /**\n-   * Execute a function\n-   */\n-  public void exec(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUser(name, ctx)) {\n-      return;\n-    }\n-    else if (isProc(name) && execProc(name, ctx, null)) {\n-      return;\n-    }\n-    if (name.indexOf(\".\") != -1) {               // Name can be qualified and spaces are allowed between parts\n-      String[] parts = name.split(\"\\\\.\");\n-      StringBuilder str = new StringBuilder();\n-      for (int i = 0; i < parts.length; i++) {\n-        if (i > 0) {\n-          str.append(\".\");\n-        }\n-        str.append(parts[i].trim());        \n-      }\n-      name = str.toString();      \n-    } \n-    if (trace && ctx != null && ctx.parent != null && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncCommand func = map.get(name.toUpperCase());    \n-    if (func != null) {\n-      func.run(ctx);\n-    }    \n-    else {\n-      info(ctx, \"Function not found: \" + name);\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * User-defined function in a SQL query\n-   */\n-  public void execSql(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUserSql(ctx, name)) {\n-      return;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(name);\n-    sql.append(\"(\");\n-    if (ctx != null) {\n-      int cnt = ctx.func_param().size();\n-      for (int i = 0; i < cnt; i++) {\n-        sql.append(evalPop(ctx.func_param(i).expr()));\n-        if (i + 1 < cnt) {\n-          sql.append(\", \");\n-        }\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-  }\n-  \n-  /**\n-   * Aggregate or window function in a SQL query\n-   */\n-  public void execAggWindowSql(HplsqlParser.Expr_agg_window_funcContext ctx) {\n-    exec.stackPush(exec.getFormattedText(ctx));\n-  }\n-  \n-  /**\n-   * Execute a user-defined function\n-   */\n-  public boolean execUser(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    if (trace) {\n-      trace(ctx, \"EXEC FUNCTION \" + name);\n-    }\n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    setCallParameters(ctx, actualParams, userCtx.create_routine_params(), null);\n-    if (userCtx.declare_block_inplace() != null) {\n-      visit(userCtx.declare_block_inplace());\n-    }\n-    visit(userCtx.single_block_stmt());\n-    exec.leaveScope(); \n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a HPL/SQL user-defined function in a query \n-   */\n-  public boolean execUserSql(HplsqlParser.Expr_func_paramsContext ctx, String name) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(\"hplsql('\");\n-    sql.append(name);\n-    sql.append(\"(\");\n-    int cnt = ctx.func_param().size();\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(\":\" + (i + 1));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")'\");\n-    if (cnt > 0) {\n-      sql.append(\", \");\n-    }\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(evalPop(ctx.func_param(i).expr()));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-    exec.registerUdf();\n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure as the entry point of the script (defined by -main option)\n-   */\n-  public boolean execProc(String name) {\n-    if (trace) {\n-      trace(\"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(\"Procedure not found\");\n-      return false;\n-    }    \n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(procCtx.create_routine_params());\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    return true;\n-  }\n-  \n-  /**\n-   * Check if the stored procedure with the specified name is defined\n-   */\n-  public boolean isProc(String name) {\n-    if (procMap.get(name.toUpperCase()) != null) {\n-      return true;\n-    }\n-    return false;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure using CALL or EXEC statement passing parameters\n-   */\n-  public boolean execProc(String name, HplsqlParser.Expr_func_paramsContext ctx, ParserRuleContext callCtx) {\n-    if (trace) {\n-      trace(callCtx, \"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(callCtx, \"Procedure not found\");\n-      return false;\n-    }    \n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    HashMap<String, Var> out = new HashMap<String, Var>();\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.declare_block_inplace() != null) {\n-      visit(procCtx.declare_block_inplace());\n-    }\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(ctx, actualParams, procCtx.create_routine_params(), out);\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    for (Map.Entry<String, Var> i : out.entrySet()) {      // Set OUT parameters\n-      exec.setVariable(i.getKey(), i.getValue());\n-    }\n-    return true;\n-  }\n-  \n-  /**\n-   * Set parameters for user-defined function call\n-   */\n-  public void setCallParameters(HplsqlParser.Expr_func_paramsContext actual, ArrayList<Var> actualValues, \n-                         HplsqlParser.Create_routine_paramsContext formal,\n-                         HashMap<String, Var> out) {\n-    if (actual == null || actual.func_param() == null || actualValues == null) {\n-      return;\n-    }\n-    int actualCnt = actualValues.size();\n-    int formalCnt = formal.create_routine_param_item().size();\n-    for (int i = 0; i < actualCnt; i++) {\n-      if (i >= formalCnt) {\n-        break;\n-      }\n-      HplsqlParser.ExprContext a = actual.func_param(i).expr(); \n-      HplsqlParser.Create_routine_param_itemContext p = getCallParameter(actual, formal, i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var var = setCallParameter(name, type, len, scale, actualValues.get(i));\n-      if (trace) {\n-        trace(actual, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      } \n-      if (out != null && a.expr_atom() != null && a.expr_atom().ident() != null &&\n-          (p.T_OUT() != null || p.T_INOUT() != null)) {\n-        String actualName = a.expr_atom().ident().getText();\n-        if (actualName != null) {\n-          out.put(actualName, var);  \n-        }         \n-      }\n-    }\n-  }\n-  \n-  /**\n-   * Set parameters for entry-point call (Main procedure defined by -main option)\n-   */\n-  void setCallParameters(HplsqlParser.Create_routine_paramsContext ctx) {\n-    int cnt = ctx.create_routine_param_item().size();\n-    for (int i = 0; i < cnt; i++) {\n-      HplsqlParser.Create_routine_param_itemContext p = ctx.create_routine_param_item(i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var value = exec.findVariable(name);\n-      Var var = setCallParameter(name, type, len, scale, value);\n-      if (trace) {\n-        trace(ctx, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      }      \n-    }\n-  }\n-  \n-  /**\n-   * Create a function or procedure parameter and set its value\n-   */\n-  Var setCallParameter(String name, String type, String len, String scale, Var value) {\n-    Var var = new Var(name, type, len, scale, null);\n-    var.cast(value);\n-    exec.addVariable(var);    \n-    return var;\n-  }\n-  \n-  /**\n-   * Get call parameter definition by name (if specified) or position\n-   */\n-  HplsqlParser.Create_routine_param_itemContext getCallParameter(HplsqlParser.Expr_func_paramsContext actual, \n-      HplsqlParser.Create_routine_paramsContext formal, int pos) {\n-    String named = null;\n-    int out_pos = pos;\n-    if (actual.func_param(pos).ident() != null) {\n-      named = actual.func_param(pos).ident().getText(); \n-      int cnt = formal.create_routine_param_item().size();\n-      for (int i = 0; i < cnt; i++) {\n-        if (named.equalsIgnoreCase(formal.create_routine_param_item(i).ident().getText())) {\n-          out_pos = i;\n-          break;\n-        }\n-      }\n-    }\n-    return formal.create_routine_param_item(out_pos);\n-  }  \n-  \n-  /**\n-   * Evaluate actual call parameters\n-   */\n-  public ArrayList<Var> getActualCallParameters(HplsqlParser.Expr_func_paramsContext actual) {\n-    if (actual == null || actual.func_param() == null) {\n-      return null;\n-    }\n-    int cnt = actual.func_param().size();\n-    ArrayList<Var> values = new ArrayList<Var>(cnt);\n-    for (int i = 0; i < cnt; i++) {\n-      values.add(evalPop(actual.func_param(i).expr()));\n-    }\n-    return values;\n-  }\n-  \n-  /**\n-   * Add a user-defined function\n-   */\n-  public void addUserFunction(HplsqlParser.Create_function_stmtContext ctx) {\n-    String name = ctx.ident().getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE FUNCTION \" + name);\n-    }\n-    userMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Add a user-defined procedure\n-   */\n-  public void addUserProcedure(HplsqlParser.Create_procedure_stmtContext ctx) {\n-    String name = ctx.ident(0).getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE PROCEDURE \" + name);\n-    }\n-    procMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Get the number of parameters in function call\n-   */\n-  public int getParamCount(HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (ctx == null) {\n-      return 0;\n-    }\n-    return ctx.func_param().size();\n-  }\n-    \n-  /**\n-   * Execute a special function\n-   */\n-  public void specExec(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else if(ctx.T_MAX_PART_STRING() != null) {\n-      execMaxPartString(ctx);\n-    } else if(ctx.T_MIN_PART_STRING() != null) {\n-      execMinPartString(ctx);\n-    } else if(ctx.T_MAX_PART_INT() != null) {\n-      execMaxPartInt(ctx);\n-    } else if(ctx.T_MIN_PART_INT() != null) {\n-      execMinPartInt(ctx);\n-    } else if(ctx.T_MAX_PART_DATE() != null) {\n-      execMaxPartDate(ctx);\n-    } else if(ctx.T_MIN_PART_DATE() != null) {\n-      execMinPartDate(ctx);\n-    } else if(ctx.T_PART_LOC() != null) {\n-      execPartLoc(ctx);\n-    } else {\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * Execute a special function in executable SQL statement\n-   */\n-  public void specExecSql(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specSqlMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else {\n-      exec.stackPush(exec.getFormattedText(ctx));\n-    }\n-  }\n-  \n-  /**\n-   * Get the current date\n-   */\n-  public void execCurrentDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"CURRENT_DATE\");\n-    }\n-    SimpleDateFormat f = new SimpleDateFormat(\"yyyy-MM-dd\");\n-    String s = f.format(Calendar.getInstance().getTime());\n-    exec.stackPush(new Var(Var.Type.DATE, Utils.toDate(s))); \n-  }\n-  \n-  /**\n-   * Execute MAX_PART_STRING function\n-   */\n-  public void execMaxPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_STRING function\n-   */\n-  public void execMinPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_INT function\n-   */\n-  public void execMaxPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_INT function\n-   */\n-  public void execMinPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_DATE function\n-   */\n-  public void execMaxPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_DATE function\n-   */\n-  public void execMinPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, false /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN or MAX partition function\n-   */\n-  public void execMinMaxPart(HplsqlParser.Expr_spec_funcContext ctx, Var.Type type, boolean max) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"SHOW PARTITIONS \" + tabname;    \n-    String colname = null;    \n-    int colnum = -1;\n-    int exprnum = ctx.expr().size();    \n-    // Column name \n-    if (ctx.expr(1) != null) {\n-      colname = evalPop(ctx.expr(1)).toString();\n-    } else {\n-      colnum = 0;\n-    }\n-    // Partition filter\n-    if (exprnum >= 4) {\n-      sql += \" PARTITION (\";\n-      int i = 2;\n-      while (i + 1 < exprnum) {\n-        String fcol = evalPop(ctx.expr(i)).toString();\n-        String fval = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += fcol + \"=\" + fval;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      String resultString = null;\n-      Long resultInt = null;\n-      Date resultDate = null;      \n-      while (rs.next()) {\n-        String[] parts = rs.getString(1).split(\"/\");\n-        // Find partition column by name\n-        if (colnum == -1) {\n-          for (int i = 0; i < parts.length; i++) {\n-            String[] name = parts[i].split(\"=\");\n-            if (name[0].equalsIgnoreCase(colname)) {\n-              colnum = i;\n-              break;\n-            }\n-          }\n-          // No partition column with the specified name exists\n-          if (colnum == -1) {\n-            evalNullClose(query, exec.conf.defaultConnection);\n-            return;\n-          }\n-        }\n-        String[] pair = parts[colnum].split(\"=\");\n-        if (type == Var.Type.STRING) {\n-          resultString = Utils.minMaxString(resultString, pair[1], max);          \n-        } \n-        else if (type == Var.Type.BIGINT) {\n-          resultInt = Utils.minMaxInt(resultInt, pair[1], max);          \n-        } \n-        else if (type == Var.Type.DATE) {\n-          resultDate = Utils.minMaxDate(resultDate, pair[1], max);\n-        }\n-      }\n-      if (resultString != null) {\n-        evalString(resultString);\n-      } \n-      else if (resultInt != null) {\n-        evalInt(resultInt);\n-      } \n-      else if (resultDate != null) {\n-        evalDate(resultDate);\n-      } \n-      else {\n-        evalNull();\n-      }\n-    } catch (SQLException e) {}  \n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Execute PART_LOC function\n-   */\n-  public void execPartLoc(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"DESCRIBE EXTENDED \" + tabname;    \n-    int exprnum = ctx.expr().size();   \n-    boolean hostname = false;\n-    // Partition filter\n-    if (exprnum > 1) {\n-      sql += \" PARTITION (\";\n-      int i = 1;\n-      while (i + 1 < exprnum) {\n-        String col = evalPop(ctx.expr(i)).toString();\n-        String val = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += col + \"=\" + val;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    // With host name\n-    if (exprnum % 2 == 0 && evalPop(ctx.expr(exprnum - 1)).intValue() == 1) {\n-      hostname = true;\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    String result = null;\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      while (rs.next()) {\n-        if (rs.getString(1).startsWith(\"Detailed Partition Information\")) {\n-          Matcher m = Pattern.compile(\".*, location:(.*?),.*\").matcher(rs.getString(2));\n-          if (m.find()) {\n-            result = m.group(1);\n-          }    \n-        }\n-      }\n-    } catch (SQLException e) {}  \n-    if (result != null) {\n-      // Remove the host name\n-      if (!hostname) {\n-        Matcher m = Pattern.compile(\".*://.*?(/.*)\").matcher(result); \n-        if (m.find()) {\n-          result = m.group(1);\n-        }\n-      }\n-      evalString(result);\n-    }    \n-    else {\n-      evalNull();\n-    }\n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Evaluate the expression and push the value to the stack\n-   */\n-  void eval(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-  }\n-\n-  /**\n-   * Evaluate the expression to the specified variable\n-   */\n-  void evalVar(Var var) {\n-    exec.stackPush(var); \n-  }\n-\n-  /**\n-   * Evaluate the expression to NULL\n-   */\n-  void evalNull() {\n-    exec.stackPush(Var.Null); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified String value\n-   */\n-  void evalString(String string) {\n-    exec.stackPush(new Var(string)); \n-  }\n-  \n-  void evalString(StringBuilder string) {\n-    evalString(string.toString()); \n-  }\n-\n-  /**\n-   * Evaluate the expression to specified Int value\n-   */\n-  void evalInt(Long i) {\n-    exec.stackPush(new Var(i)); \n-  }\n-  \n-  void evalInt(int i) {\n-    evalInt(Long.valueOf(i));\n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified Date value\n-   */\n-  void evalDate(Date date) {\n-    exec.stackPush(new Var(Var.Type.DATE, date)); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to NULL and close the query\n-   */\n-  void evalNullClose(Query query, String conn) {\n-    exec.stackPush(Var.Null); \n-    exec.closeQuery(query, conn);\n-    if(trace) {\n-      query.printStackTrace();\n-    }\n-  }\n-  \n-  /**\n-   * Evaluate the expression and pop value from the stack\n-   */\n-  Var evalPop(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-    return exec.stackPop();  \n-  }\n-  \n-  Var evalPop(ParserRuleContext ctx, int value) {\n-    if (ctx != null) {\n-      return evalPop(ctx);\n-    }\n-    return new Var(Long.valueOf(value));\n-  }\n-  \n-  /**\n-   * Execute rules\n-   */\n-  Integer visit(ParserRuleContext ctx) {\n-    return exec.visit(ctx);  \n-  } \n- \n-  /**\n-   * Execute children rules\n-   */\n-  Integer visitChildren(ParserRuleContext ctx) {\n-    return exec.visitChildren(ctx);  \n-  }  \n-  \n-  /**\n-   * Trace information\n-   */\n-  public void trace(ParserRuleContext ctx, String message) {\n-    if (trace) {\n-      exec.trace(ctx, message);\n-    }\n-  }\n-  \n-  public void trace(String message) {\n-    trace(null, message);\n-  }\n-  \n-  public void info(ParserRuleContext ctx, String message) {\n-    exec.info(ctx, message);\n-  }\n+public interface Function {\n+  boolean exec(String name, HplsqlParser.Expr_func_paramsContext ctx);\n+  void addUserFunction(HplsqlParser.Create_function_stmtContext ctx);\n+  void addUserProcedure(HplsqlParser.Create_procedure_stmtContext ctx);\n+  boolean exists(String name);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5MDgxMQ=="}, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 800}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzgyMjIxOnYy", "diffSide": "RIGHT", "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/BuiltinFunctions.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDoxODowNVrOHoRpqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTozODo0MVrOHoVk0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5NDI4Mg==", "bodyText": "do we really need to define these function differently than others; I've taken a look at MIN_PART_STRING and it seenms like its an ordinary function...so it could probably use the registry way approach", "url": "https://github.com/apache/hive/pull/1542#discussion_r511994282", "createdAt": "2020-10-26T14:18:05Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/BuiltinFunctions.java", "diffHunk": "@@ -0,0 +1,435 @@\n+/*\n+ *\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hive.hplsql.functions;\n+\n+import java.sql.Date;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Calendar;\n+import java.util.HashMap;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import org.antlr.v4.runtime.ParserRuleContext;\n+import org.apache.hive.hplsql.Exec;\n+import org.apache.hive.hplsql.HplsqlParser;\n+import org.apache.hive.hplsql.Query;\n+import org.apache.hive.hplsql.Utils;\n+import org.apache.hive.hplsql.Var;\n+\n+public class BuiltinFunctions {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA1ODU3Ng==", "bodyText": "This was just extracted out from the existing code, it's a not a new thing, I think this is out of scope now.", "url": "https://github.com/apache/hive/pull/1542#discussion_r512058576", "createdAt": "2020-10-26T15:38:41Z", "author": {"login": "zeroflag"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/BuiltinFunctions.java", "diffHunk": "@@ -0,0 +1,435 @@\n+/*\n+ *\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hive.hplsql.functions;\n+\n+import java.sql.Date;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Calendar;\n+import java.util.HashMap;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import org.antlr.v4.runtime.ParserRuleContext;\n+import org.apache.hive.hplsql.Exec;\n+import org.apache.hive.hplsql.HplsqlParser;\n+import org.apache.hive.hplsql.Query;\n+import org.apache.hive.hplsql.Utils;\n+import org.apache.hive.hplsql.Var;\n+\n+public class BuiltinFunctions {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5NDI4Mg=="}, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzg0MTQxOnYy", "diffSide": "RIGHT", "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/FunctionDatetime.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDoyMjowNlrOHoR1mA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTozOTowMFrOHoVl3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5NzMzNg==", "bodyText": "I think this class should implement the Function interface and not extends a class which has a name which suggest that it's a \"container of functions\"", "url": "https://github.com/apache/hive/pull/1542#discussion_r511997336", "createdAt": "2020-10-26T14:22:06Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/FunctionDatetime.java", "diffHunk": "@@ -27,7 +27,7 @@\n import org.apache.commons.lang3.StringUtils;\n import org.apache.hive.hplsql.*;\n \n-public class FunctionDatetime extends Function {\n+public class FunctionDatetime extends BuiltinFunctions {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA1ODg0Ng==", "bodyText": "It used to extend from Function which was a class before. I didn't really change how it used to work, it's still using implementation inheritance, which I personally don't like but didn't want to change it as part of this patch. We might want to move builtin functions into the DB later on, making this class unnecessary in the future.", "url": "https://github.com/apache/hive/pull/1542#discussion_r512058846", "createdAt": "2020-10-26T15:39:00Z", "author": {"login": "zeroflag"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/FunctionDatetime.java", "diffHunk": "@@ -27,7 +27,7 @@\n import org.apache.commons.lang3.StringUtils;\n import org.apache.hive.hplsql.*;\n \n-public class FunctionDatetime extends Function {\n+public class FunctionDatetime extends BuiltinFunctions {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5NzMzNg=="}, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzg0MzcxOnYy", "diffSide": "RIGHT", "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/FunctionDatetime.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDoyMjozNVrOHoR28g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo0MDo1NFrOHoVr1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5NzY4Mg==", "bodyText": "note: registrator exposes it's internals", "url": "https://github.com/apache/hive/pull/1542#discussion_r511997682", "createdAt": "2020-10-26T14:22:35Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/FunctionDatetime.java", "diffHunk": "@@ -36,20 +36,20 @@ public FunctionDatetime(Exec e) {\n    * Register functions\n    */\n   @Override\n-  public void register(Function f) {\n-    f.map.put(\"DATE\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { date(ctx); }});\n-    f.map.put(\"FROM_UNIXTIME\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { fromUnixtime(ctx); }});\n-    f.map.put(\"NOW\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { now(ctx); }});\n-    f.map.put(\"TIMESTAMP_ISO\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { timestampIso(ctx); }});\n-    f.map.put(\"TO_TIMESTAMP\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { toTimestamp(ctx); }});\n-    f.map.put(\"UNIX_TIMESTAMP\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { unixTimestamp(ctx); }});\n+  public void register(BuiltinFunctions f) {\n+    f.map.put(\"DATE\", this::date);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2MDM3Mg==", "bodyText": "This is a general issue, many things are package private. We can refactor that little by little.\n.", "url": "https://github.com/apache/hive/pull/1542#discussion_r512060372", "createdAt": "2020-10-26T15:40:54Z", "author": {"login": "zeroflag"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/FunctionDatetime.java", "diffHunk": "@@ -36,20 +36,20 @@ public FunctionDatetime(Exec e) {\n    * Register functions\n    */\n   @Override\n-  public void register(Function f) {\n-    f.map.put(\"DATE\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { date(ctx); }});\n-    f.map.put(\"FROM_UNIXTIME\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { fromUnixtime(ctx); }});\n-    f.map.put(\"NOW\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { now(ctx); }});\n-    f.map.put(\"TIMESTAMP_ISO\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { timestampIso(ctx); }});\n-    f.map.put(\"TO_TIMESTAMP\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { toTimestamp(ctx); }});\n-    f.map.put(\"UNIX_TIMESTAMP\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { unixTimestamp(ctx); }});\n+  public void register(BuiltinFunctions f) {\n+    f.map.put(\"DATE\", this::date);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5NzY4Mg=="}, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzg4NDg5OnYy", "diffSide": "RIGHT", "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/HmsFunction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDozMDoyNFrOHoSPkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo0MTowNVrOHoVsfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAwMzk4NA==", "bodyText": "I think Function-s should be independent from where they are defined (builtin or hms) - and there should be some kind of registry which knows about all the functions.\nI feel that the Function interface is not really intuituve - its more like a cointainer of functions or something like that.\nRight now this class seem to contain some parts of a registry; which could also do chaining - plus  the registration and execution logic for functions defined in the metastore.\nIt seems to me that there is no real registry right now - if there would be one then I think this class could be some kind of \"factory of functions\" ?", "url": "https://github.com/apache/hive/pull/1542#discussion_r512003984", "createdAt": "2020-10-26T14:30:24Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/HmsFunction.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ *\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hive.hplsql.functions;\n+\n+import static org.apache.hive.hplsql.functions.InMemoryFunction.setCallParameters;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import org.antlr.v4.runtime.ANTLRInputStream;\n+import org.antlr.v4.runtime.CommonTokenStream;\n+import org.antlr.v4.runtime.ParserRuleContext;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Database;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.StoredProcedure;\n+import org.apache.hadoop.hive.metastore.api.StoredProcedureRequest;\n+import org.apache.hive.hplsql.Exec;\n+import org.apache.hive.hplsql.HplsqlBaseVisitor;\n+import org.apache.hive.hplsql.HplsqlLexer;\n+import org.apache.hive.hplsql.HplsqlParser;\n+import org.apache.hive.hplsql.Scope;\n+import org.apache.hive.hplsql.Var;\n+import org.apache.thrift.TException;\n+\n+public class HmsFunction implements Function {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2MDU0MQ==", "bodyText": "I agree, but this is how it used to work, and I didn't want to address these things as part of this patch.", "url": "https://github.com/apache/hive/pull/1542#discussion_r512060541", "createdAt": "2020-10-26T15:41:05Z", "author": {"login": "zeroflag"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/HmsFunction.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ *\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hive.hplsql.functions;\n+\n+import static org.apache.hive.hplsql.functions.InMemoryFunction.setCallParameters;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import org.antlr.v4.runtime.ANTLRInputStream;\n+import org.antlr.v4.runtime.CommonTokenStream;\n+import org.antlr.v4.runtime.ParserRuleContext;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Database;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.StoredProcedure;\n+import org.apache.hadoop.hive.metastore.api.StoredProcedureRequest;\n+import org.apache.hive.hplsql.Exec;\n+import org.apache.hive.hplsql.HplsqlBaseVisitor;\n+import org.apache.hive.hplsql.HplsqlLexer;\n+import org.apache.hive.hplsql.HplsqlParser;\n+import org.apache.hive.hplsql.Scope;\n+import org.apache.hive.hplsql.Var;\n+import org.apache.thrift.TException;\n+\n+public class HmsFunction implements Function {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAwMzk4NA=="}, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzkyNzU5OnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/model/MPosParam.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDozODozMVrOHoSpQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxMDo1MzoxNVrOHo2CMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxMDU2Mg==", "bodyText": "is this an unused class?", "url": "https://github.com/apache/hive/pull/1542#discussion_r512010562", "createdAt": "2020-10-26T14:38:31Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/model/MPosParam.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ *\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hadoop.hive.metastore.model;\n+\n+public class MPosParam {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjU5MDM4Ng==", "bodyText": "removed", "url": "https://github.com/apache/hive/pull/1542#discussion_r512590386", "createdAt": "2020-10-27T10:53:15Z", "author": {"login": "zeroflag"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/model/MPosParam.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ *\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hadoop.hive.metastore.model;\n+\n+public class MPosParam {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxMDU2Mg=="}, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzk3MDUyOnYy", "diffSide": "RIGHT", "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDo0Njo1NVrOHoTDZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo0MTozMFrOHoVtug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxNzI1Mg==", "bodyText": "right now I'm wondering how much this differs from the MFunction stuff; is it differ enough to have a separate table?", "url": "https://github.com/apache/hive/pull/1542#discussion_r512017252", "createdAt": "2020-10-26T14:46:55Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,31 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2MDg1OA==", "bodyText": "I feel like the similarity is accidental and in the future we can expect more diversion between the two. We would likely have columns which are only applicable to one case and not the other. There are already things like className, resourceUri, resourceType in MFunction.", "url": "https://github.com/apache/hive/pull/1542#discussion_r512060858", "createdAt": "2020-10-26T15:41:30Z", "author": {"login": "zeroflag"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,31 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxNzI1Mg=="}, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODAxMzkyOnYy", "diffSide": "RIGHT", "path": "hplsql/src/main/java/org/apache/hive/hplsql/Exec.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDo1NTowN1rOHoTdUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo0MTo0MFrOHoVuOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyMzg4OQ==", "bodyText": "will this escaping be enough in all cases?", "url": "https://github.com/apache/hive/pull/1542#discussion_r512023889", "createdAt": "2020-10-26T14:55:07Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/Exec.java", "diffHunk": "@@ -1659,13 +1665,70 @@ public Integer visitExpr_func(HplsqlParser.Expr_funcContext ctx) {\n     }\n     return 0;\n   }\n-  \n+\n+  /**\n+   * User-defined function in a SQL query\n+   */\n+  public void execSql(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n+    if (execUserSql(ctx, name)) {\n+      return;\n+    }\n+    StringBuilder sql = new StringBuilder();\n+    sql.append(name);\n+    sql.append(\"(\");\n+    if (ctx != null) {\n+      int cnt = ctx.func_param().size();\n+      for (int i = 0; i < cnt; i++) {\n+        sql.append(evalPop(ctx.func_param(i).expr()));\n+        if (i + 1 < cnt) {\n+          sql.append(\", \");\n+        }\n+      }\n+    }\n+    sql.append(\")\");\n+    exec.stackPush(sql);\n+  }\n+\n+  /**\n+   * Execute a HPL/SQL user-defined function in a query\n+   */\n+  private boolean execUserSql(HplsqlParser.Expr_func_paramsContext ctx, String name) {\n+    if (!function.exists(name.toUpperCase())) {\n+      return false;\n+    }\n+    StringBuilder sql = new StringBuilder();\n+    sql.append(\"hplsql('\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 352}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2MDk4Ng==", "bodyText": "I didn't check since this is not new code, and fixing every issue in the existing code was not the scope of this issue.", "url": "https://github.com/apache/hive/pull/1542#discussion_r512060986", "createdAt": "2020-10-26T15:41:40Z", "author": {"login": "zeroflag"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/Exec.java", "diffHunk": "@@ -1659,13 +1665,70 @@ public Integer visitExpr_func(HplsqlParser.Expr_funcContext ctx) {\n     }\n     return 0;\n   }\n-  \n+\n+  /**\n+   * User-defined function in a SQL query\n+   */\n+  public void execSql(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n+    if (execUserSql(ctx, name)) {\n+      return;\n+    }\n+    StringBuilder sql = new StringBuilder();\n+    sql.append(name);\n+    sql.append(\"(\");\n+    if (ctx != null) {\n+      int cnt = ctx.func_param().size();\n+      for (int i = 0; i < cnt; i++) {\n+        sql.append(evalPop(ctx.func_param(i).expr()));\n+        if (i + 1 < cnt) {\n+          sql.append(\", \");\n+        }\n+      }\n+    }\n+    sql.append(\")\");\n+    exec.stackPush(sql);\n+  }\n+\n+  /**\n+   * Execute a HPL/SQL user-defined function in a query\n+   */\n+  private boolean execUserSql(HplsqlParser.Expr_func_paramsContext ctx, String name) {\n+    if (!function.exists(name.toUpperCase())) {\n+      return false;\n+    }\n+    StringBuilder sql = new StringBuilder();\n+    sql.append(\"hplsql('\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyMzg4OQ=="}, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 352}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwODAzODQzOnYy", "diffSide": "RIGHT", "path": "hplsql/src/main/java/org/apache/hive/hplsql/Exec.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDo1OTozN1rOHoTsUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNTo0MTo1NlrOHoVvAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyNzcyOA==", "bodyText": "typo: hplsq\nnote: this getMsc() method could be placed closer to the Hms related stuff", "url": "https://github.com/apache/hive/pull/1542#discussion_r512027728", "createdAt": "2020-10-26T14:59:37Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/Exec.java", "diffHunk": "@@ -799,30 +801,35 @@ Integer init(String[] args) throws Exception {\n     select = new Select(this);\n     stmt = new Stmt(this);\n     converter = new Converter(this);\n-        \n-    function = new Function(this);\n-    new FunctionDatetime(this).register(function);\n-    new FunctionMisc(this).register(function);\n-    new FunctionString(this).register(function);\n-    new FunctionOra(this).register(function);\n+\n+    builtinFunctions = new BuiltinFunctions(this);\n+    new FunctionDatetime(this).register(builtinFunctions);\n+    new FunctionMisc(this).register(builtinFunctions);\n+    new FunctionString(this).register(builtinFunctions);\n+    new FunctionOra(this).register(builtinFunctions);\n+    if (\"hms\".equalsIgnoreCase(System.getProperty(\"hplsql.storage\"))) {\n+      function = new HmsFunction(this, getMsc(System.getProperty(\"hplsq.metastore.uris\", \"thrift://localhost:9083\")), builtinFunctions);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 214}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjA2MTE4NA==", "bodyText": "This part is removed from subsequent patch.", "url": "https://github.com/apache/hive/pull/1542#discussion_r512061184", "createdAt": "2020-10-26T15:41:56Z", "author": {"login": "zeroflag"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/Exec.java", "diffHunk": "@@ -799,30 +801,35 @@ Integer init(String[] args) throws Exception {\n     select = new Select(this);\n     stmt = new Stmt(this);\n     converter = new Converter(this);\n-        \n-    function = new Function(this);\n-    new FunctionDatetime(this).register(function);\n-    new FunctionMisc(this).register(function);\n-    new FunctionString(this).register(function);\n-    new FunctionOra(this).register(function);\n+\n+    builtinFunctions = new BuiltinFunctions(this);\n+    new FunctionDatetime(this).register(builtinFunctions);\n+    new FunctionMisc(this).register(builtinFunctions);\n+    new FunctionString(this).register(builtinFunctions);\n+    new FunctionOra(this).register(builtinFunctions);\n+    if (\"hms\".equalsIgnoreCase(System.getProperty(\"hplsql.storage\"))) {\n+      function = new HmsFunction(this, getMsc(System.getProperty(\"hplsq.metastore.uris\", \"thrift://localhost:9083\")), builtinFunctions);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyNzcyOA=="}, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 214}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 312, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}