{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM3MDc2Mjgz", "number": 1151, "reviewThreads": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzowMTowNFrOEHG6dA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNzoyMToyOVrOEIP3_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODg4NzU2OnYy", "diffSide": "RIGHT", "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzowMTowNFrOGmT_oA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzowMTowNFrOGmT_oA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjgyNjY1Ng==", "bodyText": "I think retryLock would be a proper name.", "url": "https://github.com/apache/hive/pull/1151#discussion_r442826656", "createdAt": "2020-06-19T13:01:04Z", "author": {"login": "deniskuzZ"}, "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "diffHunk": "@@ -4979,10 +4979,11 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n \n     HIVE_QUERY_REEXECUTION_ENABLED(\"hive.query.reexecution.enabled\", true,\n         \"Enable query reexecutions\"),\n-    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize\",\n+    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize,lockacquisition\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f703ff9139d9ce9f9fd4128e9b9663d71809840d"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODkwMDYxOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzowNToxMFrOGmUHug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMzowNToxMFrOGmUHug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjgyODczMA==", "bodyText": "wrap releaseLocksAndCommitOrRollback(false) with try and throw exception in finally, instead of rollback null", "url": "https://github.com/apache/hive/pull/1151#discussion_r442828730", "createdAt": "2020-06-19T13:05:10Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,14 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n           // txn list and retry\n-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n-          // Currently, we acquire a snapshot, we compile the query wrt that snapshot,\n-          // and then, we acquire locks. If snapshot is still valid, we continue as usual.\n-          // But if snapshot is not valid, we recompile the query.\n-          if (driverContext.isOutdatedTxn()) {\n-            driverContext.getTxnManager().rollbackTxn();\n-\n-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n-            lockAndRespond();\n-          }\n-          driverContext.setRetrial(true);\n-          driverContext.getBackupContext().addSubContext(context);\n-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n-          context = driverContext.getBackupContext();\n-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n-            driverContext.getTxnManager().getValidTxns().toString());\n-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n-            validTxnManager.recordValidWriteIds();\n-          }\n-\n-          if (!alreadyCompiled) {\n-            // compile internal will automatically reset the perf logger\n-            compileInternal(command, true);\n-          } else {\n-            // Since we're reusing the compiled plan, we need to update its start time for current run\n-            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n-          }\n-\n-          if (!validTxnManager.isValidTxnListState()) {\n-            // Throw exception\n-            throw handleHiveException(new HiveException(\"Operation could not be executed\"), 14);\n-          }\n-\n-          //Reset the PerfLogger\n-          perfLogger = SessionState.getPerfLogger(true);\n-\n-          // the reason that we set the txn manager for the cxt here is because each\n-          // query has its own ctx object. The txn mgr is shared across the\n-          // same instance of Driver, which can run multiple queries.\n-          context.setHiveTxnManager(driverContext.getTxnManager());\n+          rollback(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f703ff9139d9ce9f9fd4128e9b9663d71809840d"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2MTkyMzkyOnYy", "diffSide": "RIGHT", "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMVQyMzo1OToyMVrOGmvCmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMVQyMzo1OToyMVrOGmvCmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2OTc4NQ==", "bodyText": "Do we really want retrylock to be driven by a config? Shouldn't it just be enabled?", "url": "https://github.com/apache/hive/pull/1151#discussion_r443269785", "createdAt": "2020-06-21T23:59:21Z", "author": {"login": "jcamachor"}, "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "diffHunk": "@@ -4979,10 +4979,11 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n \n     HIVE_QUERY_REEXECUTION_ENABLED(\"hive.query.reexecution.enabled\", true,\n         \"Enable query reexecutions\"),\n-    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize\",\n+    HIVE_QUERY_REEXECUTION_STRATEGIES(\"hive.query.reexecution.strategies\", \"overlay,reoptimize,retrylock\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2MTkyNTI5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQwMDowMToxNlrOGmvDUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQwMDowMToxNlrOGmvDUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI2OTk3MA==", "bodyText": "equals ?", "url": "https://github.com/apache/hive/pull/1151#discussion_r443269970", "createdAt": "2020-06-22T00:01:16Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java", "diffHunk": "@@ -64,6 +65,9 @@ private static IReExecutionPlugin buildReExecPlugin(String name) throws RuntimeE\n     if (name.equals(\"reoptimize\")) {\n       return new ReOptimizePlugin();\n     }\n+    if (name.endsWith(\"retrylock\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2MTkzMDA2OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yMlQwMDowOTozOFrOGmvGIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNDowMzo1MlrOGnq1yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI3MDY4OA==", "bodyText": "I think this makes behavior wrt original logic slightly different. For instance, if another transaction obtains the locks in between the moment that this transaction releases them and is going to acquire them again, does this mean the transaction would fail for a second time? If that is the case, should we have a specific configuration for the number of retries in this case? It seems for the default re-execution the number of retries is 1, but in this case, we could retry several times before failing the query.", "url": "https://github.com/apache/hive/pull/1151#discussion_r443270688", "createdAt": "2020-06-22T00:09:38Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,18 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n-          // txn list and retry\n-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n-          // Currently, we acquire a snapshot, we compile the query wrt that snapshot,\n-          // and then, we acquire locks. If snapshot is still valid, we continue as usual.\n-          // But if snapshot is not valid, we recompile the query.\n-          if (driverContext.isOutdatedTxn()) {\n-            driverContext.getTxnManager().rollbackTxn();\n-\n-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n-            lockAndRespond();\n-          }\n-          driverContext.setRetrial(true);\n-          driverContext.getBackupContext().addSubContext(context);\n-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n-          context = driverContext.getBackupContext();\n-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n-            driverContext.getTxnManager().getValidTxns().toString());\n-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n-            validTxnManager.recordValidWriteIds();\n-          }\n-\n-          if (!alreadyCompiled) {\n-            // compile internal will automatically reset the perf logger\n-            compileInternal(command, true);\n-          } else {\n-            // Since we're reusing the compiled plan, we need to update its start time for current run\n-            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n-          }\n-\n-          if (!validTxnManager.isValidTxnListState()) {\n-            // Throw exception\n-            throw handleHiveException(new HiveException(\"Operation could not be executed\"), 14);\n+          // txn list and retry (see ReExecutionRetryLockPlugin)\n+          try {\n+            releaseLocksAndCommitOrRollback(false);\n+          } catch (LockException e) {\n+            handleHiveException(e, 12);\n           }\n-\n-          //Reset the PerfLogger\n-          perfLogger = SessionState.getPerfLogger(true);\n-\n-          // the reason that we set the txn manager for the cxt here is because each\n-          // query has its own ctx object. The txn mgr is shared across the\n-          // same instance of Driver, which can run multiple queries.\n-          context.setHiveTxnManager(driverContext.getTxnManager());\n+          throw handleHiveException(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzYzOTEyNg==", "bodyText": "This is an interesting question. In the original logic, if an other commit invalidated the snaphsot a second time, the query also failed with HiveExection. The main difference is, we do more work in this case (compile and acquire the locks again), so the chance is probably higher that the snapshot gets invalidated a second time, but I don't know if it is high enough that we should consider it. The ReexecDriver uses one global config for the number of retries, it would take some refactoring to make it independently configurable for the different plugins.", "url": "https://github.com/apache/hive/pull/1151#discussion_r443639126", "createdAt": "2020-06-22T15:22:07Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,18 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n-          // txn list and retry\n-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n-          // Currently, we acquire a snapshot, we compile the query wrt that snapshot,\n-          // and then, we acquire locks. If snapshot is still valid, we continue as usual.\n-          // But if snapshot is not valid, we recompile the query.\n-          if (driverContext.isOutdatedTxn()) {\n-            driverContext.getTxnManager().rollbackTxn();\n-\n-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n-            lockAndRespond();\n-          }\n-          driverContext.setRetrial(true);\n-          driverContext.getBackupContext().addSubContext(context);\n-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n-          context = driverContext.getBackupContext();\n-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n-            driverContext.getTxnManager().getValidTxns().toString());\n-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n-            validTxnManager.recordValidWriteIds();\n-          }\n-\n-          if (!alreadyCompiled) {\n-            // compile internal will automatically reset the perf logger\n-            compileInternal(command, true);\n-          } else {\n-            // Since we're reusing the compiled plan, we need to update its start time for current run\n-            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n-          }\n-\n-          if (!validTxnManager.isValidTxnListState()) {\n-            // Throw exception\n-            throw handleHiveException(new HiveException(\"Operation could not be executed\"), 14);\n+          // txn list and retry (see ReExecutionRetryLockPlugin)\n+          try {\n+            releaseLocksAndCommitOrRollback(false);\n+          } catch (LockException e) {\n+            handleHiveException(e, 12);\n           }\n-\n-          //Reset the PerfLogger\n-          perfLogger = SessionState.getPerfLogger(true);\n-\n-          // the reason that we set the txn manager for the cxt here is because each\n-          // query has its own ctx object. The txn mgr is shared across the\n-          // same instance of Driver, which can run multiple queries.\n-          context.setHiveTxnManager(driverContext.getTxnManager());\n+          throw handleHiveException(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI3MDY4OA=="}, "originalCommit": {"oid": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk2OTQyMA==", "bodyText": "In the original logic, if another commit invalidated the snaphsot a second time, the query also failed with HiveExection.\n\nIiuc that should not happen because we were holding the locks that we had already acquired; however, now we are releasing them. Hence, the logic is slightly different?\nIn any case, it is straightforward to add a config property such as HIVE_QUERY_MAX_REEXECUTION_COUNT for this specific retry, then retrieve it in shouldReExecute method in ReExecutionRetryLockPlugin: You have both the number of retries as well as the conf (getConf method) to retrieve the max number of retries for the config. The check on HIVE_QUERY_MAX_REEXECUTION_COUNT for the rest of the plugins will need to be moved into shouldReExecute method in those plugins too (currently it is done within the run method in the ReExecDriver itself).", "url": "https://github.com/apache/hive/pull/1151#discussion_r443969420", "createdAt": "2020-06-23T05:25:33Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,18 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n-          // txn list and retry\n-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n-          // Currently, we acquire a snapshot, we compile the query wrt that snapshot,\n-          // and then, we acquire locks. If snapshot is still valid, we continue as usual.\n-          // But if snapshot is not valid, we recompile the query.\n-          if (driverContext.isOutdatedTxn()) {\n-            driverContext.getTxnManager().rollbackTxn();\n-\n-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n-            lockAndRespond();\n-          }\n-          driverContext.setRetrial(true);\n-          driverContext.getBackupContext().addSubContext(context);\n-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n-          context = driverContext.getBackupContext();\n-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n-            driverContext.getTxnManager().getValidTxns().toString());\n-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n-            validTxnManager.recordValidWriteIds();\n-          }\n-\n-          if (!alreadyCompiled) {\n-            // compile internal will automatically reset the perf logger\n-            compileInternal(command, true);\n-          } else {\n-            // Since we're reusing the compiled plan, we need to update its start time for current run\n-            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n-          }\n-\n-          if (!validTxnManager.isValidTxnListState()) {\n-            // Throw exception\n-            throw handleHiveException(new HiveException(\"Operation could not be executed\"), 14);\n+          // txn list and retry (see ReExecutionRetryLockPlugin)\n+          try {\n+            releaseLocksAndCommitOrRollback(false);\n+          } catch (LockException e) {\n+            handleHiveException(e, 12);\n           }\n-\n-          //Reset the PerfLogger\n-          perfLogger = SessionState.getPerfLogger(true);\n-\n-          // the reason that we set the txn manager for the cxt here is because each\n-          // query has its own ctx object. The txn mgr is shared across the\n-          // same instance of Driver, which can run multiple queries.\n-          context.setHiveTxnManager(driverContext.getTxnManager());\n+          throw handleHiveException(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI3MDY4OA=="}, "originalCommit": {"oid": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI0OTU0Nw==", "bodyText": "Added the new config.", "url": "https://github.com/apache/hive/pull/1151#discussion_r444249547", "createdAt": "2020-06-23T14:03:52Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,18 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n-          // txn list and retry\n-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n-          // Currently, we acquire a snapshot, we compile the query wrt that snapshot,\n-          // and then, we acquire locks. If snapshot is still valid, we continue as usual.\n-          // But if snapshot is not valid, we recompile the query.\n-          if (driverContext.isOutdatedTxn()) {\n-            driverContext.getTxnManager().rollbackTxn();\n-\n-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n-            lockAndRespond();\n-          }\n-          driverContext.setRetrial(true);\n-          driverContext.getBackupContext().addSubContext(context);\n-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n-          context = driverContext.getBackupContext();\n-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n-            driverContext.getTxnManager().getValidTxns().toString());\n-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n-            validTxnManager.recordValidWriteIds();\n-          }\n-\n-          if (!alreadyCompiled) {\n-            // compile internal will automatically reset the perf logger\n-            compileInternal(command, true);\n-          } else {\n-            // Since we're reusing the compiled plan, we need to update its start time for current run\n-            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n-          }\n-\n-          if (!validTxnManager.isValidTxnListState()) {\n-            // Throw exception\n-            throw handleHiveException(new HiveException(\"Operation could not be executed\"), 14);\n+          // txn list and retry (see ReExecutionRetryLockPlugin)\n+          try {\n+            releaseLocksAndCommitOrRollback(false);\n+          } catch (LockException e) {\n+            handleHiveException(e, 12);\n           }\n-\n-          //Reset the PerfLogger\n-          perfLogger = SessionState.getPerfLogger(true);\n-\n-          // the reason that we set the txn manager for the cxt here is because each\n-          // query has its own ctx object. The txn mgr is shared across the\n-          // same instance of Driver, which can run multiple queries.\n-          context.setHiveTxnManager(driverContext.getTxnManager());\n+          throw handleHiveException(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI3MDY4OA=="}, "originalCommit": {"oid": "b9cf35abae2e62b8d7f50971e115d1b5a3eabc84"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODM0MzQ4OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNTowMDowNlrOGntezg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwODoyNjoxMFrOGoIGTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MjgxNA==", "bodyText": "Is this still needed in ReExecDriver? Shouldn't it be driven completely by the plugins implementation now, i.e., shouldReExecute will return false after the number of retries exceeds the max?", "url": "https://github.com/apache/hive/pull/1151#discussion_r444292814", "createdAt": "2020-06-23T15:00:06Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java", "diffHunk": "@@ -148,8 +148,7 @@ public void setOperationId(String operationId) {\n   @Override\n   public CommandProcessorResponse run() throws CommandProcessorException {\n     executionIndex = 0;\n-    int maxExecutuions = 1 + coreDriver.getConf().getIntVar(ConfVars.HIVE_QUERY_MAX_REEXECUTION_COUNT);\n-\n+    int maxExecutions = getMaxExecutions();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ba8c29266ee5ca8bd25b764f795f903505bda8a4"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5NTUxMg==", "bodyText": "I would like to avoid, that some new plugin forgets to check the max in its shouldReExecute and we go in an infinite loop.", "url": "https://github.com/apache/hive/pull/1151#discussion_r444295512", "createdAt": "2020-06-23T15:03:56Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java", "diffHunk": "@@ -148,8 +148,7 @@ public void setOperationId(String operationId) {\n   @Override\n   public CommandProcessorResponse run() throws CommandProcessorException {\n     executionIndex = 0;\n-    int maxExecutuions = 1 + coreDriver.getConf().getIntVar(ConfVars.HIVE_QUERY_MAX_REEXECUTION_COUNT);\n-\n+    int maxExecutions = getMaxExecutions();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MjgxNA=="}, "originalCommit": {"oid": "ba8c29266ee5ca8bd25b764f795f903505bda8a4"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5NzcwMA==", "bodyText": "Makes sense. Can we leave a comment mentioning that? Thanks", "url": "https://github.com/apache/hive/pull/1151#discussion_r444297700", "createdAt": "2020-06-23T15:06:59Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java", "diffHunk": "@@ -148,8 +148,7 @@ public void setOperationId(String operationId) {\n   @Override\n   public CommandProcessorResponse run() throws CommandProcessorException {\n     executionIndex = 0;\n-    int maxExecutuions = 1 + coreDriver.getConf().getIntVar(ConfVars.HIVE_QUERY_MAX_REEXECUTION_COUNT);\n-\n+    int maxExecutions = getMaxExecutions();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MjgxNA=="}, "originalCommit": {"oid": "ba8c29266ee5ca8bd25b764f795f903505bda8a4"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5ODk4OQ==", "bodyText": "I also do not like this approach as you are aggregating all the conditions from underlying plugins here (when adding new plugin you should incorporate it's config here as well). What you could do is to define default shouldReExecute method under IReExecutionPlugin, in this case new plugin would use that or has to override it.", "url": "https://github.com/apache/hive/pull/1151#discussion_r444698989", "createdAt": "2020-06-24T07:30:42Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java", "diffHunk": "@@ -148,8 +148,7 @@ public void setOperationId(String operationId) {\n   @Override\n   public CommandProcessorResponse run() throws CommandProcessorException {\n     executionIndex = 0;\n-    int maxExecutuions = 1 + coreDriver.getConf().getIntVar(ConfVars.HIVE_QUERY_MAX_REEXECUTION_COUNT);\n-\n+    int maxExecutions = getMaxExecutions();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MjgxNA=="}, "originalCommit": {"oid": "ba8c29266ee5ca8bd25b764f795f903505bda8a4"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDcwMjg0OA==", "bodyText": "That would not solve the problem, every plugin will override the shouldReExecute method, that is the main goal of a plugin. Still they can ignore the max execution property. This is here for a failsafe, to not have infinite loops.", "url": "https://github.com/apache/hive/pull/1151#discussion_r444702848", "createdAt": "2020-06-24T07:38:40Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java", "diffHunk": "@@ -148,8 +148,7 @@ public void setOperationId(String operationId) {\n   @Override\n   public CommandProcessorResponse run() throws CommandProcessorException {\n     executionIndex = 0;\n-    int maxExecutuions = 1 + coreDriver.getConf().getIntVar(ConfVars.HIVE_QUERY_MAX_REEXECUTION_COUNT);\n-\n+    int maxExecutions = getMaxExecutions();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MjgxNA=="}, "originalCommit": {"oid": "ba8c29266ee5ca8bd25b764f795f903505bda8a4"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDcyODkxMA==", "bodyText": "in getMaxReExecutions could we then just stick with some default hardcoded value?", "url": "https://github.com/apache/hive/pull/1151#discussion_r444728910", "createdAt": "2020-06-24T08:26:10Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecDriver.java", "diffHunk": "@@ -148,8 +148,7 @@ public void setOperationId(String operationId) {\n   @Override\n   public CommandProcessorResponse run() throws CommandProcessorException {\n     executionIndex = 0;\n-    int maxExecutuions = 1 + coreDriver.getConf().getIntVar(ConfVars.HIVE_QUERY_MAX_REEXECUTION_COUNT);\n-\n+    int maxExecutions = getMaxExecutions();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MjgxNA=="}, "originalCommit": {"oid": "ba8c29266ee5ca8bd25b764f795f903505bda8a4"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MDc3MDg0OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNjo1NjozNlrOGoFT6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNjo1NjozNlrOGoFT6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4MzI0MQ==", "bodyText": "should we use WARN here?", "url": "https://github.com/apache/hive/pull/1151#discussion_r444683241", "createdAt": "2020-06-24T06:56:36Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,18 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MDc3NDIxOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNjo1Nzo1MlrOGoFWBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNzoyNTowMVrOGoGGMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4Mzc4Mg==", "bodyText": "what is this magic number 12? do we have enum for error codes?", "url": "https://github.com/apache/hive/pull/1151#discussion_r444683782", "createdAt": "2020-06-24T06:57:52Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,18 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n-          // txn list and retry\n-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n-          // Currently, we acquire a snapshot, we compile the query wrt that snapshot,\n-          // and then, we acquire locks. If snapshot is still valid, we continue as usual.\n-          // But if snapshot is not valid, we recompile the query.\n-          if (driverContext.isOutdatedTxn()) {\n-            driverContext.getTxnManager().rollbackTxn();\n-\n-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n-            lockAndRespond();\n-          }\n-          driverContext.setRetrial(true);\n-          driverContext.getBackupContext().addSubContext(context);\n-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n-          context = driverContext.getBackupContext();\n-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n-            driverContext.getTxnManager().getValidTxns().toString());\n-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n-            validTxnManager.recordValidWriteIds();\n-          }\n-\n-          if (!alreadyCompiled) {\n-            // compile internal will automatically reset the perf logger\n-            compileInternal(command, true);\n-          } else {\n-            // Since we're reusing the compiled plan, we need to update its start time for current run\n-            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n-          }\n-\n-          if (!validTxnManager.isValidTxnListState()) {\n-            // Throw exception\n-            throw handleHiveException(new HiveException(\"Operation could not be executed\"), 14);\n+          // txn list and retry (see ReExecutionRetryLockPlugin)\n+          try {\n+            releaseLocksAndCommitOrRollback(false);\n+          } catch (LockException e) {\n+            handleHiveException(e, 12);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5NjExNQ==", "bodyText": "I do not really know where these response codes come from, it seems like to me there are different codes, for different error types. I can not see any enum or anything that would explain the different codes. 12 seems like the code for lockexception during rollback/commit.", "url": "https://github.com/apache/hive/pull/1151#discussion_r444696115", "createdAt": "2020-06-24T07:25:01Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -675,50 +678,18 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n \n       try {\n         if (!validTxnManager.isValidTxnListState()) {\n-          LOG.info(\"Compiling after acquiring locks\");\n+          LOG.info(\"Reexecuting after acquiring locks, since snapshot was outdated.\");\n           // Snapshot was outdated when locks were acquired, hence regenerate context,\n-          // txn list and retry\n-          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n-          // Currently, we acquire a snapshot, we compile the query wrt that snapshot,\n-          // and then, we acquire locks. If snapshot is still valid, we continue as usual.\n-          // But if snapshot is not valid, we recompile the query.\n-          if (driverContext.isOutdatedTxn()) {\n-            driverContext.getTxnManager().rollbackTxn();\n-\n-            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n-            lockAndRespond();\n-          }\n-          driverContext.setRetrial(true);\n-          driverContext.getBackupContext().addSubContext(context);\n-          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n-          context = driverContext.getBackupContext();\n-          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n-            driverContext.getTxnManager().getValidTxns().toString());\n-          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n-            validTxnManager.recordValidWriteIds();\n-          }\n-\n-          if (!alreadyCompiled) {\n-            // compile internal will automatically reset the perf logger\n-            compileInternal(command, true);\n-          } else {\n-            // Since we're reusing the compiled plan, we need to update its start time for current run\n-            driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n-          }\n-\n-          if (!validTxnManager.isValidTxnListState()) {\n-            // Throw exception\n-            throw handleHiveException(new HiveException(\"Operation could not be executed\"), 14);\n+          // txn list and retry (see ReExecutionRetryLockPlugin)\n+          try {\n+            releaseLocksAndCommitOrRollback(false);\n+          } catch (LockException e) {\n+            handleHiveException(e, 12);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4Mzc4Mg=="}, "originalCommit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MDgxMzkxOnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNzoxMjowM1rOGoFuQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNzoyMDo0M1rOGoF-LA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4OTk4NQ==", "bodyText": "why do we need this? there is clean up at the beginning. same in other tests", "url": "https://github.com/apache/hive/pull/1151#discussion_r444689985", "createdAt": "2020-06-24T07:12:03Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java", "diffHunk": "@@ -2329,6 +2338,142 @@ private void testConcurrentMergeInsertNoDuplicates(String query, boolean sharedW\n     List res = new ArrayList();\n     driver.getFetchTask().fetch(res);\n     Assert.assertEquals(\"Duplicate records found\", 4, res.size());\n+    dropTable(new String[]{\"target\", \"source\"});\n+  }\n+\n+  /**\n+   * ValidTxnManager.isValidTxnListState can invalidate a snapshot if a relevant write transaction was committed\n+   * between a query compilation and lock acquisition. When this happens we have to recompile the given query,\n+   * otherwise we can miss reading partitions created between. The following three cases test these scenarios.\n+   * @throws Exception ex\n+   */\n+  @Test\n+  public void testMergeInsertDynamicPartitioningSequential() throws Exception {\n+\n+    dropTable(new String[]{\"target\", \"source\"});\n+    conf.setBoolVar(HiveConf.ConfVars.TXN_WRITE_X_LOCK, false);\n+\n+    // Create partition c=1\n+    driver.run(\"create table target (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')\");\n+    driver.run(\"insert into target values (1,1,1), (2,2,1)\");\n+    //Create partition c=2\n+    driver.run(\"create table source (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')\");\n+    driver.run(\"insert into source values (3,3,2), (4,4,2)\");\n+\n+    // txn 1 inserts data to an old and a new partition\n+    driver.run(\"insert into source values (5,5,2), (6,6,3)\");\n+\n+    // txn 2 inserts into the target table into a new partition ( and a duplicate considering the source table)\n+    driver.run(\"insert into target values (3, 3, 2)\");\n+\n+    // txn3 merge\n+    driver.run(\"merge into target t using source s on t.a = s.a \" +\n+        \"when not matched then insert values (s.a, s.b, s.c)\");\n+    driver.run(\"select * from target\");\n+    List res = new ArrayList();\n+    driver.getFetchTask().fetch(res);\n+    // The merge should see all three partition and not create duplicates\n+    Assert.assertEquals(\"Duplicate records found\", 6, res.size());\n+    Assert.assertTrue(\"Partition 3 was skipped\", res.contains(\"6\\t6\\t3\"));\n+    dropTable(new String[]{\"target\", \"source\"});", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "originalPosition": 115}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5NDA2MA==", "bodyText": "This just makes the local reexecution easier. If the test finishes it leaves some data in the warehouse folder and even if the next run starts with drop table if exists, it won't clean the folder since the table is not existing the hms", "url": "https://github.com/apache/hive/pull/1151#discussion_r444694060", "createdAt": "2020-06-24T07:20:43Z", "author": {"login": "pvargacl"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/lockmgr/TestDbTxnManager2.java", "diffHunk": "@@ -2329,6 +2338,142 @@ private void testConcurrentMergeInsertNoDuplicates(String query, boolean sharedW\n     List res = new ArrayList();\n     driver.getFetchTask().fetch(res);\n     Assert.assertEquals(\"Duplicate records found\", 4, res.size());\n+    dropTable(new String[]{\"target\", \"source\"});\n+  }\n+\n+  /**\n+   * ValidTxnManager.isValidTxnListState can invalidate a snapshot if a relevant write transaction was committed\n+   * between a query compilation and lock acquisition. When this happens we have to recompile the given query,\n+   * otherwise we can miss reading partitions created between. The following three cases test these scenarios.\n+   * @throws Exception ex\n+   */\n+  @Test\n+  public void testMergeInsertDynamicPartitioningSequential() throws Exception {\n+\n+    dropTable(new String[]{\"target\", \"source\"});\n+    conf.setBoolVar(HiveConf.ConfVars.TXN_WRITE_X_LOCK, false);\n+\n+    // Create partition c=1\n+    driver.run(\"create table target (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')\");\n+    driver.run(\"insert into target values (1,1,1), (2,2,1)\");\n+    //Create partition c=2\n+    driver.run(\"create table source (a int, b int) partitioned by (c int) stored as orc TBLPROPERTIES ('transactional'='true')\");\n+    driver.run(\"insert into source values (3,3,2), (4,4,2)\");\n+\n+    // txn 1 inserts data to an old and a new partition\n+    driver.run(\"insert into source values (5,5,2), (6,6,3)\");\n+\n+    // txn 2 inserts into the target table into a new partition ( and a duplicate considering the source table)\n+    driver.run(\"insert into target values (3, 3, 2)\");\n+\n+    // txn3 merge\n+    driver.run(\"merge into target t using source s on t.a = s.a \" +\n+        \"when not matched then insert values (s.a, s.b, s.c)\");\n+    driver.run(\"select * from target\");\n+    List res = new ArrayList();\n+    driver.getFetchTask().fetch(res);\n+    // The merge should see all three partition and not create duplicates\n+    Assert.assertEquals(\"Duplicate records found\", 6, res.size());\n+    Assert.assertTrue(\"Partition 3 was skipped\", res.contains(\"6\\t6\\t3\"));\n+    dropTable(new String[]{\"target\", \"source\"});", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY4OTk4NQ=="}, "originalCommit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MDg0MTU4OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionRetryLockPlugin.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNzoyMToyOVrOGoF_gQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxMDo1ODoxNlrOGoNNzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5NDQwMQ==", "bodyText": "should we re-execute when here?", "url": "https://github.com/apache/hive/pull/1151#discussion_r444694401", "createdAt": "2020-06-24T07:21:29Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionRetryLockPlugin.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.reexec;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.Driver;\n+import org.apache.hadoop.hive.ql.plan.mapper.PlanMapper;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorException;\n+\n+public class ReExecutionRetryLockPlugin implements IReExecutionPlugin {\n+\n+  private Driver coreDriver;\n+  private int maxRetryLockExecutions = 1;\n+\n+  @Override\n+  public void initialize(Driver driver) {\n+    coreDriver = driver;\n+    maxRetryLockExecutions = 1 + coreDriver.getConf().getIntVar(HiveConf.ConfVars.HIVE_QUERY_MAX_REEXECUTION_RETRYLOCK_COUNT);\n+  }\n+\n+  @Override\n+  public void beforeExecute(int executionIndex, boolean explainReOptimization) {\n+  }\n+\n+  @Override\n+  public boolean shouldReExecute(int executionNum, CommandProcessorException ex) {\n+    return executionNum < maxRetryLockExecutions && ex != null &&\n+        ex.getMessage().contains(Driver.SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED);\n+  }\n+\n+  @Override\n+  public void prepareToReExecute() {\n+  }\n+\n+  @Override\n+  public boolean shouldReExecute(int executionNum, PlanMapper oldPlanMapper, PlanMapper newPlanMapper) {\n+    return executionNum < maxRetryLockExecutions;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5OTAxMw==", "bodyText": "The ReExecutePluginInterface is not really straightforward. This method is called after the query is recompiled from the reExecDriver shouldReExecuteAfterCompile . Basicly it asks after the recompilation do you want to continue the execution. It is only used by the reoptimize plugin, which tries to recompile the query with different statistics (if I understand correctly) and only reexecutes if the plan did change.", "url": "https://github.com/apache/hive/pull/1151#discussion_r444699013", "createdAt": "2020-06-24T07:30:44Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionRetryLockPlugin.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.reexec;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.Driver;\n+import org.apache.hadoop.hive.ql.plan.mapper.PlanMapper;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorException;\n+\n+public class ReExecutionRetryLockPlugin implements IReExecutionPlugin {\n+\n+  private Driver coreDriver;\n+  private int maxRetryLockExecutions = 1;\n+\n+  @Override\n+  public void initialize(Driver driver) {\n+    coreDriver = driver;\n+    maxRetryLockExecutions = 1 + coreDriver.getConf().getIntVar(HiveConf.ConfVars.HIVE_QUERY_MAX_REEXECUTION_RETRYLOCK_COUNT);\n+  }\n+\n+  @Override\n+  public void beforeExecute(int executionIndex, boolean explainReOptimization) {\n+  }\n+\n+  @Override\n+  public boolean shouldReExecute(int executionNum, CommandProcessorException ex) {\n+    return executionNum < maxRetryLockExecutions && ex != null &&\n+        ex.getMessage().contains(Driver.SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED);\n+  }\n+\n+  @Override\n+  public void prepareToReExecute() {\n+  }\n+\n+  @Override\n+  public boolean shouldReExecute(int executionNum, PlanMapper oldPlanMapper, PlanMapper newPlanMapper) {\n+    return executionNum < maxRetryLockExecutions;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5NDQwMQ=="}, "originalCommit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDcwOTAxNQ==", "bodyText": "if it's only used by reoptimize plugin shouldn't we throw UnsupportedOperationException?", "url": "https://github.com/apache/hive/pull/1151#discussion_r444709015", "createdAt": "2020-06-24T07:50:33Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionRetryLockPlugin.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.reexec;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.Driver;\n+import org.apache.hadoop.hive.ql.plan.mapper.PlanMapper;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorException;\n+\n+public class ReExecutionRetryLockPlugin implements IReExecutionPlugin {\n+\n+  private Driver coreDriver;\n+  private int maxRetryLockExecutions = 1;\n+\n+  @Override\n+  public void initialize(Driver driver) {\n+    coreDriver = driver;\n+    maxRetryLockExecutions = 1 + coreDriver.getConf().getIntVar(HiveConf.ConfVars.HIVE_QUERY_MAX_REEXECUTION_RETRYLOCK_COUNT);\n+  }\n+\n+  @Override\n+  public void beforeExecute(int executionIndex, boolean explainReOptimization) {\n+  }\n+\n+  @Override\n+  public boolean shouldReExecute(int executionNum, CommandProcessorException ex) {\n+    return executionNum < maxRetryLockExecutions && ex != null &&\n+        ex.getMessage().contains(Driver.SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED);\n+  }\n+\n+  @Override\n+  public void prepareToReExecute() {\n+  }\n+\n+  @Override\n+  public boolean shouldReExecute(int executionNum, PlanMapper oldPlanMapper, PlanMapper newPlanMapper) {\n+    return executionNum < maxRetryLockExecutions;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5NDQwMQ=="}, "originalCommit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDcxMDI3Mg==", "bodyText": "No, you must return true, saying you do not care with the result of the recompile, you want the query to reexecute.", "url": "https://github.com/apache/hive/pull/1151#discussion_r444710272", "createdAt": "2020-06-24T07:53:00Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionRetryLockPlugin.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.reexec;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.Driver;\n+import org.apache.hadoop.hive.ql.plan.mapper.PlanMapper;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorException;\n+\n+public class ReExecutionRetryLockPlugin implements IReExecutionPlugin {\n+\n+  private Driver coreDriver;\n+  private int maxRetryLockExecutions = 1;\n+\n+  @Override\n+  public void initialize(Driver driver) {\n+    coreDriver = driver;\n+    maxRetryLockExecutions = 1 + coreDriver.getConf().getIntVar(HiveConf.ConfVars.HIVE_QUERY_MAX_REEXECUTION_RETRYLOCK_COUNT);\n+  }\n+\n+  @Override\n+  public void beforeExecute(int executionIndex, boolean explainReOptimization) {\n+  }\n+\n+  @Override\n+  public boolean shouldReExecute(int executionNum, CommandProcessorException ex) {\n+    return executionNum < maxRetryLockExecutions && ex != null &&\n+        ex.getMessage().contains(Driver.SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED);\n+  }\n+\n+  @Override\n+  public void prepareToReExecute() {\n+  }\n+\n+  @Override\n+  public boolean shouldReExecute(int executionNum, PlanMapper oldPlanMapper, PlanMapper newPlanMapper) {\n+    return executionNum < maxRetryLockExecutions;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5NDQwMQ=="}, "originalCommit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDgxMjc1MA==", "bodyText": "ok :)", "url": "https://github.com/apache/hive/pull/1151#discussion_r444812750", "createdAt": "2020-06-24T10:58:16Z", "author": {"login": "deniskuzZ"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/reexec/ReExecutionRetryLockPlugin.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.reexec;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.Driver;\n+import org.apache.hadoop.hive.ql.plan.mapper.PlanMapper;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorException;\n+\n+public class ReExecutionRetryLockPlugin implements IReExecutionPlugin {\n+\n+  private Driver coreDriver;\n+  private int maxRetryLockExecutions = 1;\n+\n+  @Override\n+  public void initialize(Driver driver) {\n+    coreDriver = driver;\n+    maxRetryLockExecutions = 1 + coreDriver.getConf().getIntVar(HiveConf.ConfVars.HIVE_QUERY_MAX_REEXECUTION_RETRYLOCK_COUNT);\n+  }\n+\n+  @Override\n+  public void beforeExecute(int executionIndex, boolean explainReOptimization) {\n+  }\n+\n+  @Override\n+  public boolean shouldReExecute(int executionNum, CommandProcessorException ex) {\n+    return executionNum < maxRetryLockExecutions && ex != null &&\n+        ex.getMessage().contains(Driver.SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED);\n+  }\n+\n+  @Override\n+  public void prepareToReExecute() {\n+  }\n+\n+  @Override\n+  public boolean shouldReExecute(int executionNum, PlanMapper oldPlanMapper, PlanMapper newPlanMapper) {\n+    return executionNum < maxRetryLockExecutions;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY5NDQwMQ=="}, "originalCommit": {"oid": "4d421609193669b5a46c2f530f6c570de69ad4e7"}, "originalPosition": 53}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 663, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}