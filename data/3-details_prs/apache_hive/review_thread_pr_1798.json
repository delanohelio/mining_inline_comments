{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQzNjE1NjQ0", "number": 1798, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQxNzozOTo1NFrOFRipSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQxNzo1MDo1MFrOFRi7sA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzOTM3NzM4OnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestConvertAstToSearchArg.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQxNzozOTo1NFrOIYCNbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQxODoyNTozNlrOIYED8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3Mjk0Mg==", "bodyText": "nit: maybe move Sarg  Expr creation to separate method for consistency?", "url": "https://github.com/apache/hive/pull/1798#discussion_r562072942", "createdAt": "2021-01-21T17:39:54Z", "author": {"login": "pgaref"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestConvertAstToSearchArg.java", "diffHunk": "@@ -2855,16 +2836,26 @@ public void TestBigintSarg() throws Exception {\n   }\n \n   @Test\n-  public void TestBooleanSarg() throws Exception {\n-    String serialAst =\n-        \"AQEAamF2YS51dGlsLkFycmF5TGlz9AECAQFvcmcuYXBhY2hlLmhhZG9vcC5oaXZlLnFsLnBsYW4uRXh\" +\n-            \"wck5vZGVHZW5lcmljRnVuY0Rlc+MBAQABAgECb3JnLmFwYWNoZS5oYWRvb3AuaGl2ZS5xbC5wbGFuLk\" +\n-            \"V4cHJOb2RlQ29sdW1uRGVz4wEBYrEAAAFib29sb3LjAQNvcmcuYXBhY2hlLmhhZG9vcC5oaXZlLnNlc\" +\n-            \"mRlMi50eXBlaW5mby5QcmltaXRpdmVUeXBlSW5m7wEBYm9vbGVh7gEEb3JnLmFwYWNoZS5oYWRvb3Au\" +\n-            \"aGl2ZS5xbC5wbGFuLkV4cHJOb2RlQ29uc3RhbnREZXPjAQEDCQUBAQVvcmcuYXBhY2hlLmhhZG9vcC5\" +\n-            \"oaXZlLnFsLnVkZi5nZW5lcmljLkdlbmVyaWNVREZPUEVxdWHsAQAAAYI9AUVRVUHMAQZvcmcuYXBhY2\" +\n-            \"hlLmhhZG9vcC5pby5Cb29sZWFuV3JpdGFibOUBAAABAwkBAgEBYrIAAAgBAwkBB29yZy5hcGFjaGUua\" +\n-            \"GFkb29wLmhpdmUucWwudWRmLmdlbmVyaWMuR2VuZXJpY1VERk9QQW7kAQEGAQAAAQMJ\";\n+  public void testBooleanSarg() throws Exception {\n+    ExprNodeDesc column1 =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjEwMzI4Mw==", "bodyText": "the reason I haven't do that is this case differs from others in the class as we have two columns, and this is the only case...however, I can do a utility method", "url": "https://github.com/apache/hive/pull/1798#discussion_r562103283", "createdAt": "2021-01-21T18:25:36Z", "author": {"login": "abstractdog"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestConvertAstToSearchArg.java", "diffHunk": "@@ -2855,16 +2836,26 @@ public void TestBigintSarg() throws Exception {\n   }\n \n   @Test\n-  public void TestBooleanSarg() throws Exception {\n-    String serialAst =\n-        \"AQEAamF2YS51dGlsLkFycmF5TGlz9AECAQFvcmcuYXBhY2hlLmhhZG9vcC5oaXZlLnFsLnBsYW4uRXh\" +\n-            \"wck5vZGVHZW5lcmljRnVuY0Rlc+MBAQABAgECb3JnLmFwYWNoZS5oYWRvb3AuaGl2ZS5xbC5wbGFuLk\" +\n-            \"V4cHJOb2RlQ29sdW1uRGVz4wEBYrEAAAFib29sb3LjAQNvcmcuYXBhY2hlLmhhZG9vcC5oaXZlLnNlc\" +\n-            \"mRlMi50eXBlaW5mby5QcmltaXRpdmVUeXBlSW5m7wEBYm9vbGVh7gEEb3JnLmFwYWNoZS5oYWRvb3Au\" +\n-            \"aGl2ZS5xbC5wbGFuLkV4cHJOb2RlQ29uc3RhbnREZXPjAQEDCQUBAQVvcmcuYXBhY2hlLmhhZG9vcC5\" +\n-            \"oaXZlLnFsLnVkZi5nZW5lcmljLkdlbmVyaWNVREZPUEVxdWHsAQAAAYI9AUVRVUHMAQZvcmcuYXBhY2\" +\n-            \"hlLmhhZG9vcC5pby5Cb29sZWFuV3JpdGFibOUBAAABAwkBAgEBYrIAAAgBAwkBB29yZy5hcGFjaGUua\" +\n-            \"GFkb29wLmhpdmUucWwudWRmLmdlbmVyaWMuR2VuZXJpY1VERk9QQW7kAQEGAQAAAQMJ\";\n+  public void testBooleanSarg() throws Exception {\n+    ExprNodeDesc column1 =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3Mjk0Mg=="}, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 169}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzOTM4NTY5OnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestSparkInvalidFileFormat.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQxNzo0MTo0NVrOIYCSWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQxODoxNDozM1rOIYDneg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3NDIwMQ==", "bodyText": "Does this mean this is a breaking change for Spark?", "url": "https://github.com/apache/hive/pull/1798#discussion_r562074201", "createdAt": "2021-01-21T17:41:45Z", "author": {"login": "pgaref"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestSparkInvalidFileFormat.java", "diffHunk": "@@ -30,11 +30,13 @@\n import org.apache.hadoop.hive.ql.session.SessionState;\n \n import org.junit.Assert;\n+import org.junit.Ignore;\n import org.junit.Test;\n \n import java.io.File;\n import java.io.IOException;\n \n+@Ignore(\"HIVE-22944: Kryo 5 upgrade conflicts with Spark, which is not supported anymore\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA5NTk5NA==", "bodyText": "yes, it is\nmaybe I could hack with that, but it would be painful: using kryo5 for tez execution and kryo4 for spark execution", "url": "https://github.com/apache/hive/pull/1798#discussion_r562095994", "createdAt": "2021-01-21T18:14:33Z", "author": {"login": "abstractdog"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestSparkInvalidFileFormat.java", "diffHunk": "@@ -30,11 +30,13 @@\n import org.apache.hadoop.hive.ql.session.SessionState;\n \n import org.junit.Assert;\n+import org.junit.Ignore;\n import org.junit.Test;\n \n import java.io.File;\n import java.io.IOException;\n \n+@Ignore(\"HIVE-22944: Kryo 5 upgrade conflicts with Spark, which is not supported anymore\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3NDIwMQ=="}, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzOTM5MjA4OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/SerializationUtilities.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQxNzo0MzoyN1rOIYCWdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQxODoyMTozMVrOIYD56A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3NTI1NQ==", "bodyText": "Maybe rename the method itself here as well?  something like obtainKryo() ?", "url": "https://github.com/apache/hive/pull/1798#discussion_r562075255", "createdAt": "2021-01-21T17:43:27Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/SerializationUtilities.java", "diffHunk": "@@ -278,7 +284,7 @@ public Kryo create() {\n    * @return kryo instance\n    */\n   public static Kryo borrowKryo() {\n-    Kryo kryo = kryoPool.borrow();\n+    Kryo kryo = kryoPool.obtain();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjEwMDcxMg==", "bodyText": "it's very unfortunate that kryo changed this method name without any reasons (or just I don't understand that :) )...we might want a method name that reflects the behavior of \"getting a kryo instance from the pool\", I don't have a strong opinion about that, but I'm not sure if we need to change a public method name because kryo changed theirs", "url": "https://github.com/apache/hive/pull/1798#discussion_r562100712", "createdAt": "2021-01-21T18:21:31Z", "author": {"login": "abstractdog"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/SerializationUtilities.java", "diffHunk": "@@ -278,7 +284,7 @@ public Kryo create() {\n    * @return kryo instance\n    */\n   public static Kryo borrowKryo() {\n-    Kryo kryo = kryoPool.borrow();\n+    Kryo kryo = kryoPool.obtain();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3NTI1NQ=="}, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzOTQxODIzOnYy", "diffSide": "RIGHT", "path": "pom.xml", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQxNzo0OToyMlrOIYCmCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQxOTowNjowNVrOIYFmFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3OTI0MA==", "bodyText": "Add a comment linking to Spark removal JIRA as a TODO?", "url": "https://github.com/apache/hive/pull/1798#discussion_r562079240", "createdAt": "2021-01-21T17:49:22Z", "author": {"login": "pgaref"}, "path": "pom.xml", "diffHunk": "@@ -170,7 +170,9 @@\n     <junit.jupiter.version>5.6.2</junit.jupiter.version>\n     <junit.vintage.version>5.6.2</junit.vintage.version>\n     <kafka.version>2.5.0</kafka.version>\n-    <kryo.version>4.0.2</kryo.version>\n+    <kryo.version>5.0.3</kryo.version>\n+    <kryo4.version>4.0.2</kryo4.version> <!-- old kryo, for spark-client -->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA5ODgyMA==", "bodyText": "is there a spark removal jira already?", "url": "https://github.com/apache/hive/pull/1798#discussion_r562098820", "createdAt": "2021-01-21T18:18:44Z", "author": {"login": "abstractdog"}, "path": "pom.xml", "diffHunk": "@@ -170,7 +170,9 @@\n     <junit.jupiter.version>5.6.2</junit.jupiter.version>\n     <junit.vintage.version>5.6.2</junit.vintage.version>\n     <kafka.version>2.5.0</kafka.version>\n-    <kryo.version>4.0.2</kryo.version>\n+    <kryo.version>5.0.3</kryo.version>\n+    <kryo4.version>4.0.2</kryo4.version> <!-- old kryo, for spark-client -->", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3OTI0MA=="}, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjEyODQwNw==", "bodyText": "sure, that would be HIVE-23885", "url": "https://github.com/apache/hive/pull/1798#discussion_r562128407", "createdAt": "2021-01-21T19:06:05Z", "author": {"login": "pgaref"}, "path": "pom.xml", "diffHunk": "@@ -170,7 +170,9 @@\n     <junit.jupiter.version>5.6.2</junit.jupiter.version>\n     <junit.vintage.version>5.6.2</junit.vintage.version>\n     <kafka.version>2.5.0</kafka.version>\n-    <kryo.version>4.0.2</kryo.version>\n+    <kryo.version>5.0.3</kryo.version>\n+    <kryo4.version>4.0.2</kryo4.version> <!-- old kryo, for spark-client -->", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3OTI0MA=="}, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzOTQyMDQxOnYy", "diffSide": "RIGHT", "path": "itests/hive-jmh/src/main/java/org/apache/hive/benchmark/ql/exec/KryoBench.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQxNzo0OTo1MlrOIYCnVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yNVQxNTo1MTo0MFrOIZtfVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3OTU3NA==", "bodyText": "This is cool! Shall we add a more complex Map for bench here and a perf number for reference?\nMaybe previous kryo4 vs kryo5?", "url": "https://github.com/apache/hive/pull/1798#discussion_r562079574", "createdAt": "2021-01-21T17:49:52Z", "author": {"login": "pgaref"}, "path": "itests/hive-jmh/src/main/java/org/apache/hive/benchmark/ql/exec/KryoBench.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hive.benchmark.ql.exec;\n+\n+import java.util.ArrayList;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx;\n+import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;\n+import org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat;\n+import org.apache.hadoop.hive.ql.io.orc.OrcSerde;\n+import org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowInspector;\n+import org.apache.hadoop.hive.ql.plan.MapWork;\n+import org.apache.hadoop.hive.ql.plan.PartitionDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorPartitionDesc;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import com.google.common.io.ByteStreams;\n+\n+@State(Scope.Benchmark)\n+public class KryoBench {\n+\n+  @BenchmarkMode(Mode.AverageTime)\n+  @Fork(1)\n+  @State(Scope.Thread)\n+  @OutputTimeUnit(TimeUnit.MILLISECONDS)\n+  public static class BaseBench {\n+\n+    private MapWork mapWork;\n+\n+    @Setup\n+    public void setup() throws Exception {\n+      mapWork = KryoBench.mockMapWork(\"my_table\", 1000, new BigRowInspector());\n+    }\n+\n+    @Benchmark\n+    @Warmup(iterations = 2, time = 2, timeUnit = TimeUnit.SECONDS)\n+    @Measurement(iterations = 20, time = 2, timeUnit = TimeUnit.SECONDS)\n+    public void testSerializeMapWork() {\n+      SerializationUtilities.serializePlan(mapWork, ByteStreams.nullOutputStream());\n+    }\n+  }\n+\n+  public static void main(String[] args) throws RunnerException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA5NzY0Ng==", "bodyText": "there is a benchmark result kryo4 vs kryo5: https://issues.apache.org/jira/secure/attachment/13018768/kryo4_vs_5_benchmark.log\nnot an over-complicated MapWork, but contains 1000 partitions, which is the most weight of mapworks (+ a few columns)", "url": "https://github.com/apache/hive/pull/1798#discussion_r562097646", "createdAt": "2021-01-21T18:17:04Z", "author": {"login": "abstractdog"}, "path": "itests/hive-jmh/src/main/java/org/apache/hive/benchmark/ql/exec/KryoBench.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hive.benchmark.ql.exec;\n+\n+import java.util.ArrayList;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx;\n+import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;\n+import org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat;\n+import org.apache.hadoop.hive.ql.io.orc.OrcSerde;\n+import org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowInspector;\n+import org.apache.hadoop.hive.ql.plan.MapWork;\n+import org.apache.hadoop.hive.ql.plan.PartitionDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorPartitionDesc;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import com.google.common.io.ByteStreams;\n+\n+@State(Scope.Benchmark)\n+public class KryoBench {\n+\n+  @BenchmarkMode(Mode.AverageTime)\n+  @Fork(1)\n+  @State(Scope.Thread)\n+  @OutputTimeUnit(TimeUnit.MILLISECONDS)\n+  public static class BaseBench {\n+\n+    private MapWork mapWork;\n+\n+    @Setup\n+    public void setup() throws Exception {\n+      mapWork = KryoBench.mockMapWork(\"my_table\", 1000, new BigRowInspector());\n+    }\n+\n+    @Benchmark\n+    @Warmup(iterations = 2, time = 2, timeUnit = TimeUnit.SECONDS)\n+    @Measurement(iterations = 20, time = 2, timeUnit = TimeUnit.SECONDS)\n+    public void testSerializeMapWork() {\n+      SerializationUtilities.serializePlan(mapWork, ByteStreams.nullOutputStream());\n+    }\n+  }\n+\n+  public static void main(String[] args) throws RunnerException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3OTU3NA=="}, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjEzMDA5Ng==", "bodyText": "Nice, I missed that -- the numbers are only for 5 though right?", "url": "https://github.com/apache/hive/pull/1798#discussion_r562130096", "createdAt": "2021-01-21T19:08:58Z", "author": {"login": "pgaref"}, "path": "itests/hive-jmh/src/main/java/org/apache/hive/benchmark/ql/exec/KryoBench.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hive.benchmark.ql.exec;\n+\n+import java.util.ArrayList;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx;\n+import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;\n+import org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat;\n+import org.apache.hadoop.hive.ql.io.orc.OrcSerde;\n+import org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowInspector;\n+import org.apache.hadoop.hive.ql.plan.MapWork;\n+import org.apache.hadoop.hive.ql.plan.PartitionDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorPartitionDesc;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import com.google.common.io.ByteStreams;\n+\n+@State(Scope.Benchmark)\n+public class KryoBench {\n+\n+  @BenchmarkMode(Mode.AverageTime)\n+  @Fork(1)\n+  @State(Scope.Thread)\n+  @OutputTimeUnit(TimeUnit.MILLISECONDS)\n+  public static class BaseBench {\n+\n+    private MapWork mapWork;\n+\n+    @Setup\n+    public void setup() throws Exception {\n+      mapWork = KryoBench.mockMapWork(\"my_table\", 1000, new BigRowInspector());\n+    }\n+\n+    @Benchmark\n+    @Warmup(iterations = 2, time = 2, timeUnit = TimeUnit.SECONDS)\n+    @Measurement(iterations = 20, time = 2, timeUnit = TimeUnit.SECONDS)\n+    public void testSerializeMapWork() {\n+      SerializationUtilities.serializePlan(mapWork, ByteStreams.nullOutputStream());\n+    }\n+  }\n+\n+  public static void main(String[] args) throws RunnerException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3OTU3NA=="}, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MzgzMDYxNA==", "bodyText": "it's an edited file, scroll down in it for kryo4 results", "url": "https://github.com/apache/hive/pull/1798#discussion_r563830614", "createdAt": "2021-01-25T15:51:40Z", "author": {"login": "abstractdog"}, "path": "itests/hive-jmh/src/main/java/org/apache/hive/benchmark/ql/exec/KryoBench.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hive.benchmark.ql.exec;\n+\n+import java.util.ArrayList;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.exec.SerializationUtilities;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx;\n+import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;\n+import org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat;\n+import org.apache.hadoop.hive.ql.io.orc.OrcSerde;\n+import org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.BigRowInspector;\n+import org.apache.hadoop.hive.ql.plan.MapWork;\n+import org.apache.hadoop.hive.ql.plan.PartitionDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorPartitionDesc;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.OutputTimeUnit;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.runner.Runner;\n+import org.openjdk.jmh.runner.RunnerException;\n+import org.openjdk.jmh.runner.options.Options;\n+import org.openjdk.jmh.runner.options.OptionsBuilder;\n+\n+import com.google.common.io.ByteStreams;\n+\n+@State(Scope.Benchmark)\n+public class KryoBench {\n+\n+  @BenchmarkMode(Mode.AverageTime)\n+  @Fork(1)\n+  @State(Scope.Thread)\n+  @OutputTimeUnit(TimeUnit.MILLISECONDS)\n+  public static class BaseBench {\n+\n+    private MapWork mapWork;\n+\n+    @Setup\n+    public void setup() throws Exception {\n+      mapWork = KryoBench.mockMapWork(\"my_table\", 1000, new BigRowInspector());\n+    }\n+\n+    @Benchmark\n+    @Warmup(iterations = 2, time = 2, timeUnit = TimeUnit.SECONDS)\n+    @Measurement(iterations = 20, time = 2, timeUnit = TimeUnit.SECONDS)\n+    public void testSerializeMapWork() {\n+      SerializationUtilities.serializePlan(mapWork, ByteStreams.nullOutputStream());\n+    }\n+  }\n+\n+  public static void main(String[] args) throws RunnerException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA3OTU3NA=="}, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzOTQyNDQ4OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/SerializationUtilities.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQxNzo1MDo1MFrOIYCp2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMVQxOTowOToxOVrOIYFtfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA4MDIxOA==", "bodyText": "Maybe add a comment why the above 3 lines are needed?", "url": "https://github.com/apache/hive/pull/1798#discussion_r562080218", "createdAt": "2021-01-21T17:50:50Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/SerializationUtilities.java", "diffHunk": "@@ -224,51 +224,57 @@ public Registration readClass(Input input) {\n \n   private static final Object FAKE_REFERENCE = new Object();\n \n-  private static KryoFactory factory = new KryoFactory() {\n-    @Override\n-    public Kryo create() {\n-      KryoWithHooks kryo = new KryoWithHooks();\n-      kryo.register(java.sql.Date.class, new SqlDateSerializer());\n-      kryo.register(java.sql.Timestamp.class, new TimestampSerializer());\n-      kryo.register(TimestampTZ.class, new TimestampTZSerializer());\n-      kryo.register(Path.class, new PathSerializer());\n-      kryo.register(Arrays.asList(\"\").getClass(), new ArraysAsListSerializer());\n-      kryo.register(new java.util.ArrayList().subList(0,0).getClass(), new ArrayListSubListSerializer());\n-      kryo.register(CopyOnFirstWriteProperties.class, new CopyOnFirstWritePropertiesSerializer());\n-      kryo.register(PartitionDesc.class, new PartitionDescSerializer(kryo, PartitionDesc.class));\n-\n-      ((Kryo.DefaultInstantiatorStrategy) kryo.getInstantiatorStrategy())\n-          .setFallbackInstantiatorStrategy(\n-              new StdInstantiatorStrategy());\n-      removeField(kryo, AbstractOperatorDesc.class, \"colExprMap\");\n-      removeField(kryo, AbstractOperatorDesc.class, \"statistics\");\n-      kryo.register(ReduceWork.class);\n-      kryo.register(TableDesc.class);\n-      kryo.register(UnionOperator.class);\n-      kryo.register(FileSinkOperator.class);\n-      kryo.register(VectorFileSinkOperator.class);\n-      kryo.register(HiveIgnoreKeyTextOutputFormat.class);\n-      kryo.register(StandardConstantListObjectInspector.class);\n-      kryo.register(StandardConstantMapObjectInspector.class);\n-      kryo.register(StandardConstantStructObjectInspector.class);\n-      kryo.register(SequenceFileInputFormat.class);\n-      kryo.register(RCFileInputFormat.class);\n-      kryo.register(HiveSequenceFileOutputFormat.class);\n-      kryo.register(LlapOutputFormat.class);\n-      kryo.register(SparkEdgeProperty.class);\n-      kryo.register(SparkWork.class);\n-      kryo.register(Pair.class);\n-      kryo.register(MemoryMonitorInfo.class);\n-\n-      // This must be called after all the explicit register calls.\n-      return kryo.processHooks(kryoTypeHooks, globalHook);\n-    }\n-  };\n-\n   // Bounded queue could be specified here but that will lead to blocking.\n   // ConcurrentLinkedQueue is unbounded and will release soft referenced kryo instances under\n   // memory pressure.\n-  private static KryoPool kryoPool = new KryoPool.Builder(factory).softReferences().build();\n+  private static Pool<Kryo> kryoPool = new Pool<Kryo>(true, false, 8) {\n+    protected Kryo create() {\n+      return createNewKryo();\n+    }\n+  };\n+\n+  public static Kryo createNewKryo() {\n+    KryoWithHooks kryo = new KryoWithHooks();\n+\n+    kryo.setReferences(true);\n+    kryo.setCopyReferences(true);\n+    kryo.setRegistrationRequired(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjEwMTQzNg==", "bodyText": "to be honest, because it works this way :) worst case I'll comment about what issues can come without these options set", "url": "https://github.com/apache/hive/pull/1798#discussion_r562101436", "createdAt": "2021-01-21T18:22:39Z", "author": {"login": "abstractdog"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/SerializationUtilities.java", "diffHunk": "@@ -224,51 +224,57 @@ public Registration readClass(Input input) {\n \n   private static final Object FAKE_REFERENCE = new Object();\n \n-  private static KryoFactory factory = new KryoFactory() {\n-    @Override\n-    public Kryo create() {\n-      KryoWithHooks kryo = new KryoWithHooks();\n-      kryo.register(java.sql.Date.class, new SqlDateSerializer());\n-      kryo.register(java.sql.Timestamp.class, new TimestampSerializer());\n-      kryo.register(TimestampTZ.class, new TimestampTZSerializer());\n-      kryo.register(Path.class, new PathSerializer());\n-      kryo.register(Arrays.asList(\"\").getClass(), new ArraysAsListSerializer());\n-      kryo.register(new java.util.ArrayList().subList(0,0).getClass(), new ArrayListSubListSerializer());\n-      kryo.register(CopyOnFirstWriteProperties.class, new CopyOnFirstWritePropertiesSerializer());\n-      kryo.register(PartitionDesc.class, new PartitionDescSerializer(kryo, PartitionDesc.class));\n-\n-      ((Kryo.DefaultInstantiatorStrategy) kryo.getInstantiatorStrategy())\n-          .setFallbackInstantiatorStrategy(\n-              new StdInstantiatorStrategy());\n-      removeField(kryo, AbstractOperatorDesc.class, \"colExprMap\");\n-      removeField(kryo, AbstractOperatorDesc.class, \"statistics\");\n-      kryo.register(ReduceWork.class);\n-      kryo.register(TableDesc.class);\n-      kryo.register(UnionOperator.class);\n-      kryo.register(FileSinkOperator.class);\n-      kryo.register(VectorFileSinkOperator.class);\n-      kryo.register(HiveIgnoreKeyTextOutputFormat.class);\n-      kryo.register(StandardConstantListObjectInspector.class);\n-      kryo.register(StandardConstantMapObjectInspector.class);\n-      kryo.register(StandardConstantStructObjectInspector.class);\n-      kryo.register(SequenceFileInputFormat.class);\n-      kryo.register(RCFileInputFormat.class);\n-      kryo.register(HiveSequenceFileOutputFormat.class);\n-      kryo.register(LlapOutputFormat.class);\n-      kryo.register(SparkEdgeProperty.class);\n-      kryo.register(SparkWork.class);\n-      kryo.register(Pair.class);\n-      kryo.register(MemoryMonitorInfo.class);\n-\n-      // This must be called after all the explicit register calls.\n-      return kryo.processHooks(kryoTypeHooks, globalHook);\n-    }\n-  };\n-\n   // Bounded queue could be specified here but that will lead to blocking.\n   // ConcurrentLinkedQueue is unbounded and will release soft referenced kryo instances under\n   // memory pressure.\n-  private static KryoPool kryoPool = new KryoPool.Builder(factory).softReferences().build();\n+  private static Pool<Kryo> kryoPool = new Pool<Kryo>(true, false, 8) {\n+    protected Kryo create() {\n+      return createNewKryo();\n+    }\n+  };\n+\n+  public static Kryo createNewKryo() {\n+    KryoWithHooks kryo = new KryoWithHooks();\n+\n+    kryo.setReferences(true);\n+    kryo.setCopyReferences(true);\n+    kryo.setRegistrationRequired(false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA4MDIxOA=="}, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjEzMDMwMw==", "bodyText": "Sure, makes sense!", "url": "https://github.com/apache/hive/pull/1798#discussion_r562130303", "createdAt": "2021-01-21T19:09:19Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/SerializationUtilities.java", "diffHunk": "@@ -224,51 +224,57 @@ public Registration readClass(Input input) {\n \n   private static final Object FAKE_REFERENCE = new Object();\n \n-  private static KryoFactory factory = new KryoFactory() {\n-    @Override\n-    public Kryo create() {\n-      KryoWithHooks kryo = new KryoWithHooks();\n-      kryo.register(java.sql.Date.class, new SqlDateSerializer());\n-      kryo.register(java.sql.Timestamp.class, new TimestampSerializer());\n-      kryo.register(TimestampTZ.class, new TimestampTZSerializer());\n-      kryo.register(Path.class, new PathSerializer());\n-      kryo.register(Arrays.asList(\"\").getClass(), new ArraysAsListSerializer());\n-      kryo.register(new java.util.ArrayList().subList(0,0).getClass(), new ArrayListSubListSerializer());\n-      kryo.register(CopyOnFirstWriteProperties.class, new CopyOnFirstWritePropertiesSerializer());\n-      kryo.register(PartitionDesc.class, new PartitionDescSerializer(kryo, PartitionDesc.class));\n-\n-      ((Kryo.DefaultInstantiatorStrategy) kryo.getInstantiatorStrategy())\n-          .setFallbackInstantiatorStrategy(\n-              new StdInstantiatorStrategy());\n-      removeField(kryo, AbstractOperatorDesc.class, \"colExprMap\");\n-      removeField(kryo, AbstractOperatorDesc.class, \"statistics\");\n-      kryo.register(ReduceWork.class);\n-      kryo.register(TableDesc.class);\n-      kryo.register(UnionOperator.class);\n-      kryo.register(FileSinkOperator.class);\n-      kryo.register(VectorFileSinkOperator.class);\n-      kryo.register(HiveIgnoreKeyTextOutputFormat.class);\n-      kryo.register(StandardConstantListObjectInspector.class);\n-      kryo.register(StandardConstantMapObjectInspector.class);\n-      kryo.register(StandardConstantStructObjectInspector.class);\n-      kryo.register(SequenceFileInputFormat.class);\n-      kryo.register(RCFileInputFormat.class);\n-      kryo.register(HiveSequenceFileOutputFormat.class);\n-      kryo.register(LlapOutputFormat.class);\n-      kryo.register(SparkEdgeProperty.class);\n-      kryo.register(SparkWork.class);\n-      kryo.register(Pair.class);\n-      kryo.register(MemoryMonitorInfo.class);\n-\n-      // This must be called after all the explicit register calls.\n-      return kryo.processHooks(kryoTypeHooks, globalHook);\n-    }\n-  };\n-\n   // Bounded queue could be specified here but that will lead to blocking.\n   // ConcurrentLinkedQueue is unbounded and will release soft referenced kryo instances under\n   // memory pressure.\n-  private static KryoPool kryoPool = new KryoPool.Builder(factory).softReferences().build();\n+  private static Pool<Kryo> kryoPool = new Pool<Kryo>(true, false, 8) {\n+    protected Kryo create() {\n+      return createNewKryo();\n+    }\n+  };\n+\n+  public static Kryo createNewKryo() {\n+    KryoWithHooks kryo = new KryoWithHooks();\n+\n+    kryo.setReferences(true);\n+    kryo.setCopyReferences(true);\n+    kryo.setRegistrationRequired(false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjA4MDIxOA=="}, "originalCommit": {"oid": "a51fee9c4b5f8137ad83949d54858240dab221ec"}, "originalPosition": 74}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 188, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}