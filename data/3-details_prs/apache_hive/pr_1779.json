{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM5Nzk1OTEw", "number": 1779, "title": "HIVE-24535: Cleanup AcidUtils.Directory and remove unnecessary filesystem listing", "bodyText": "What changes were proposed in this pull request?\n\nAcidUtils.getAcidState is doing a recursive listing on S3 FileSystem, it already knows the content of each delta and base directory, this could be returned to OrcInputFormat, to avoid listing each delta directory again there.\nAcidUtils.getAcidstate submethods are collecting more and more infos about the state of the data directory. This could be done directly to the final Directory object to avoid 10+ parameters in methods.\nAcidUtils.Directory, OrcInputFormat.AcidDirInfo and AcidUtils.TxnBase can be merged to one class, to clean up duplications.\n\nWhy are the changes needed?\n\nperformance improvement\n\nDoes this PR introduce any user-facing change?\nNo\nHow was this patch tested?\nExisting unit tests", "createdAt": "2020-12-14T21:00:29Z", "url": "https://github.com/apache/hive/pull/1779", "merged": true, "mergeCommit": {"oid": "4846df0027a4df3f84f11828b0fea54ad7c235ed"}, "closed": true, "closedAt": "2021-01-04T10:34:30Z", "author": {"login": "pvargacl"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdmMJTngH2gAyNTM5Nzk1OTEwOjI4YzAyYzQ1NTQyMzMxZThlNzdhYTE1ZDcyNDhlMTExOWFkNzAzZWU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdoqq1GAFqTU1NzA4MzU2MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "28c02c45542331e8e77aa15d7248e1119ad703ee", "author": {"user": {"login": "pvargacl", "name": "Peter Varga"}}, "url": "https://github.com/apache/hive/commit/28c02c45542331e8e77aa15d7248e1119ad703ee", "committedDate": "2020-12-14T20:55:23Z", "message": "HIVE-24535: Cleanup AcidUtils.Directory and remove unnecessary filesystem listings"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ae152b6b2e0bddca885d1eb7f23215972c91e2e6", "author": {"user": {"login": "pvargacl", "name": "Peter Varga"}}, "url": "https://github.com/apache/hive/commit/ae152b6b2e0bddca885d1eb7f23215972c91e2e6", "committedDate": "2020-12-14T21:08:01Z", "message": "remove dummy test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b", "author": {"user": {"login": "pvargacl", "name": "Peter Varga"}}, "url": "https://github.com/apache/hive/commit/028112e650a8f22ddf1ae5d6e87a6484c147c81b", "committedDate": "2020-12-15T09:04:07Z", "message": "Fix original bucket filter"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU2NDE5NjUy", "url": "https://github.com/apache/hive/pull/1779#pullrequestreview-556419652", "createdAt": "2020-12-21T14:30:30Z", "commit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxNDozMDozMFrOIJaLrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxNjozNjoyNFrOIJeewQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjczNzA2OQ==", "bodyText": "Side note: Shouldn't HIVE_MM_ALLOW_ORIGINALS also be part of this condition?", "url": "https://github.com/apache/hive/pull/1779#discussion_r546737069", "createdAt": "2020-12-21T14:30:30Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "diffHunk": "@@ -1316,7 +1282,7 @@ private Directory getAcidState() throws IOException {\n     }\n \n \n-    private AcidDirInfo callInternal() throws IOException {\n+    private AcidDirectory callInternal() throws IOException {\n       if (context.acidOperationalProperties != null", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjczODA1OQ==", "bodyText": "Nit: Can just return getAcidState()", "url": "https://github.com/apache/hive/pull/1779#discussion_r546738059", "createdAt": "2020-12-21T14:32:29Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "diffHunk": "@@ -1326,106 +1292,16 @@ private AcidDirInfo callInternal() throws IOException {\n         // the originals could still be handled by AcidUtils like a regular non-txn table.\n         boolean isRecursive = context.conf.getBoolean(FileInputFormat.INPUT_DIR_RECURSIVE,\n             context.conf.getBoolean(\"mapred.input.dir.recursive\", false));\n-        List<HdfsFileStatusWithId> originals = new ArrayList<>();\n-        List<AcidBaseFileInfo> baseFiles = new ArrayList<>();\n-        AcidUtils.findOriginals(fs.get(), dir, originals, useFileIds, true, isRecursive);\n-        for (HdfsFileStatusWithId fileId : originals) {\n-          baseFiles.add(new AcidBaseFileInfo(fileId, AcidUtils.AcidBaseFileType.ORIGINAL_BASE));\n-        }\n-        return new AcidDirInfo(fs.get(), dir, new AcidUtils.DirectoryImpl(Lists.newArrayList(), Sets.newHashSet(), false, true, originals,\n-            Lists.newArrayList(), Lists.newArrayList(), null), baseFiles, new ArrayList<>());\n+\n+        List<HdfsFileStatusWithId> originals = AcidUtils.findOriginals(fs.get(), dir, useFileIds, true, isRecursive);\n+        AcidDirectory directory = new AcidDirectory(dir, fs.get(), useFileIds);\n+        directory.getOriginalFiles().addAll(originals);\n+        return directory;\n       }\n       //todo: shouldn't ignoreEmptyFiles be set based on ExecutionEngine?\n-      AcidUtils.Directory dirInfo  = getAcidState();\n-      // find the base files (original or new style)\n-      List<AcidBaseFileInfo> baseFiles = new ArrayList<>();\n-      if (dirInfo.getBaseDirectory() == null) {\n-        // For non-acid tables (or paths), all data files are in getOriginalFiles() list\n-        for (HdfsFileStatusWithId fileId : dirInfo.getOriginalFiles()) {\n-          baseFiles.add(new AcidBaseFileInfo(fileId, AcidUtils.AcidBaseFileType.ORIGINAL_BASE));\n-        }\n-      } else {\n-        List<HdfsFileStatusWithId> compactedBaseFiles = dirInfo.getBaseFiles();\n-        if (compactedBaseFiles == null) {\n-          compactedBaseFiles = AcidUtils.findBaseFiles(dirInfo.getBaseDirectory(), useFileIds, fs);\n-        }\n-        for (HdfsFileStatusWithId fileId : compactedBaseFiles) {\n-          baseFiles.add(new AcidBaseFileInfo(fileId, dirInfo.isBaseInRawFormat() ?\n-            AcidUtils.AcidBaseFileType.ORIGINAL_BASE : AcidUtils.AcidBaseFileType.ACID_SCHEMA));\n-        }\n-      }\n-\n-      // Find the parsed deltas- some of them containing only the insert delta events\n-      // may get treated as base if split-update is enabled for ACID. (See HIVE-14035 for details)\n-      List<ParsedDelta> parsedDeltas = new ArrayList<>();\n-      if (context.acidOperationalProperties != null &&\n-          context.acidOperationalProperties.isSplitUpdate()) {\n-        // If we have split-update turned on for this table, then the delta events have already been\n-        // split into two directories- delta_x_y/ and delete_delta_x_y/.\n-        // When you have split-update turned on, the insert events go to delta_x_y/ directory and all\n-        // the delete events go to delete_x_y/. An update event will generate two events-\n-        // a delete event for the old record that is put into delete_delta_x_y/,\n-        // followed by an insert event for the updated record put into the usual delta_x_y/.\n-        // Therefore, everything inside delta_x_y/ is an insert event and all the files in delta_x_y/\n-        // can be treated like base files. Hence, each of these are added to baseOrOriginalFiles list.\n-\n-        for (ParsedDelta parsedDelta : dirInfo.getCurrentDirectories()) {\n-          if (parsedDelta.isDeleteDelta()) {\n-            parsedDeltas.add(parsedDelta);\n-          } else {\n-            AcidUtils.AcidBaseFileType deltaType = parsedDelta.isRawFormat() ?\n-              AcidUtils.AcidBaseFileType.ORIGINAL_BASE : AcidUtils.AcidBaseFileType.ACID_SCHEMA;\n-            PathFilter bucketFilter = parsedDelta.isRawFormat() ?\n-              AcidUtils.originalBucketFilter : AcidUtils.bucketFileFilter;\n-            if (parsedDelta.isRawFormat() && parsedDelta.getMinWriteId() != parsedDelta.getMaxWriteId()) {\n-              //delta/ with files in raw format are a result of Load Data (as opposed to compaction\n-              //or streaming ingest so must have interval length == 1.\n-              throw new IllegalStateException(\"Delta in \" + AcidUtils.AcidBaseFileType.ORIGINAL_BASE\n-               + \" format but txnIds are out of range: \" + parsedDelta.getPath());\n-            }\n-            // This is a normal insert delta, which only has insert events and hence all the files\n-            // in this delta directory can be considered as a base.\n-            Boolean val = useFileIds.value;\n-            if (val == null || val) {\n-              try {\n-                List<HdfsFileStatusWithId> insertDeltaFiles =\n-                    SHIMS.listLocatedHdfsStatus(fs.get(), parsedDelta.getPath(), bucketFilter);\n-                for (HdfsFileStatusWithId fileId : insertDeltaFiles) {\n-                  baseFiles.add(new AcidBaseFileInfo(fileId, deltaType));\n-                }\n-                if (val == null) {\n-                  useFileIds.value = true; // The call succeeded, so presumably the API is there.\n-                }\n-                continue; // move on to process to the next parsedDelta.\n-              } catch (Throwable t) {\n-                LOG.error(\"Failed to get files with ID; using regular API: \" + t.getMessage());\n-                if (val == null && t instanceof UnsupportedOperationException) {\n-                  useFileIds.value = false;\n-                }\n-              }\n-            }\n-            // Fall back to regular API and create statuses without ID.\n-            List<FileStatus> children = HdfsUtils.listLocatedStatus(fs.get(),\n-                parsedDelta.getPath(), bucketFilter);\n-            for (FileStatus child : children) {\n-              HdfsFileStatusWithId fileId = AcidUtils.createOriginalObj(null, child);\n-              baseFiles.add(new AcidBaseFileInfo(fileId, deltaType));\n-            }\n-          }\n-        }\n+      AcidDirectory dirInfo  = getAcidState();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njc1NjYxMQ==", "bodyText": "This is kind of weird? What if this is an acid table that has never gone through major compaction?", "url": "https://github.com/apache/hive/pull/1779#discussion_r546756611", "createdAt": "2020-12-21T15:05:47Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidDirectory.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hive.shims.HadoopShims;\n+import org.apache.hive.common.util.Ref;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.hive.ql.io.AcidUtils.AcidBaseFileType.ORIGINAL_BASE;\n+\n+/**\n+ * AcidDirectory used to provide ACID directory layout information, which directories and files to read.\n+ * This representation only valid in a context of a ValidWriteIdList and ValidTxnList.\n+ */\n+public final class AcidDirectory {\n+\n+  private final Path path;\n+  private final FileSystem fs;\n+  private final Ref<Boolean> useFileId;\n+\n+  private AcidUtils.ParsedBase base;\n+  private AcidUtils.ParsedBaseLight oldestBase;\n+\n+  private final List<Path> abortedDirectories = new ArrayList<>();\n+  private final Set<Long> abortedWriteIds = new HashSet<>();\n+  private boolean unCompactedAborts;\n+  private final List<HadoopShims.HdfsFileStatusWithId> originalFiles = new ArrayList<>();\n+  private final List<Path> originalDirectories = new ArrayList<>();\n+  private final List<Path> obsolete = new ArrayList<>();\n+  private final List<AcidUtils.ParsedDelta> currentDirectories = new ArrayList<>();\n+\n+  public AcidDirectory(Path path, FileSystem fs, Ref<Boolean> useFileId) {\n+    this.path = path;\n+    this.fs = fs;\n+    this.useFileId = useFileId;\n+    if (!(this.fs instanceof DistributedFileSystem) && this.useFileId != null) {\n+      this.useFileId.value = false;\n+    }\n+  }\n+\n+  public Path getPath() {\n+    return path;\n+  }\n+\n+  /**\n+   * Get the base directory path.\n+   * @return the base directory to read\n+   */\n+  public Path getBaseDirectory() {\n+    return base == null ? null : base.getBaseDirPath();\n+  }\n+\n+  /**\n+   * Get the base directory.\n+   * @return the base directory to read\n+   */\n+  public AcidUtils.ParsedBase getBase() {\n+    return base;\n+  }\n+\n+  /**\n+   * Oldest base directory in the filesystem, may be shadowed by newer base\n+   */\n+  public AcidUtils.ParsedBaseLight getOldestBase() {\n+    return oldestBase;\n+  }\n+\n+  public void setBase(AcidUtils.ParsedBase base) {\n+    this.base = base;\n+  }\n+\n+  public void setOldestBase(AcidUtils.ParsedBaseLight oldestBase) {\n+    this.oldestBase = oldestBase;\n+  }\n+\n+  public void setUnCompactedAborts(boolean unCompactedAborts) {\n+    this.unCompactedAborts = unCompactedAborts;\n+  }\n+\n+  /**\n+   * Is Base directory in raw format or in Acid format\n+   */\n+  public boolean isBaseInRawFormat() {\n+    return base != null && base.isRawFormat();\n+  }\n+\n+  /**\n+   * Get the list of original files.  Not {@code null}.  Must be sorted.\n+   * @return the list of original files (eg. 000000_0)\n+   */\n+  public List<HadoopShims.HdfsFileStatusWithId> getOriginalFiles() {\n+    return originalFiles;\n+  }\n+\n+  /**\n+   * List of original directories containing files in not ACID format\n+   */\n+  public List<Path> getOriginalDirectories() {\n+    return originalDirectories;\n+  }\n+\n+  /**\n+   * Get the list of delta directories that are valid and not\n+   * obsolete.  Not {@code null}.  List must be sorted in a specific way.\n+   * See {@link org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight#compareTo(org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight)}\n+   * for details.\n+   * @return the minimal list of current directories\n+   */\n+  public List<AcidUtils.ParsedDelta> getCurrentDirectories() {\n+    return currentDirectories;\n+  }\n+\n+  /**\n+   * Get the list of obsolete directories. After filtering out bases and\n+   * deltas that are not selected by the valid transaction/write ids list, return the\n+   * list of original files, bases, and deltas that have been replaced by\n+   * more up to date ones.  Not {@code null}.\n+   */\n+  public List<Path> getObsolete() {\n+    return obsolete;\n+  }\n+\n+  /**\n+   * Get the list of directories that has nothing but aborted transactions.\n+   * @return the list of aborted directories\n+   */\n+  public List<Path> getAbortedDirectories() {\n+    return abortedDirectories;\n+  }\n+\n+  /**\n+   * Get the list of writeIds that belong to the aborted transactions.\n+   * @return the list of aborted writeIds\n+   */\n+  public Set<Long> getAbortedWriteIds() {\n+    return abortedWriteIds;\n+  }\n+\n+  /**\n+   * Does the directory contain writeIds that belong to aborted transactions,\n+   * but are mixed together with committed writes. These aborted writes can not be cleaned.\n+   * @return true if there are aborted writes that can can be cleaned\n+   */\n+  public boolean hasUncompactedAborts() {\n+    return unCompactedAborts;\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  /**\n+   * Delete deltas that should be read by this reader.\n+   */\n+  public List<AcidUtils.ParsedDelta> getDeleteDeltas() {\n+    return currentDirectories.stream().filter(AcidUtils.ParsedDeltaLight::isDeleteDelta).collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * All original, base and delta bucket files that should be read by this reader\n+   * @throws IOException ex\n+   */\n+  public List<AcidUtils.AcidBaseFileInfo> getBaseAndDeltaFiles() throws IOException {\n+    List<AcidUtils.AcidBaseFileInfo> baseAndDeltaFiles = new ArrayList<>();\n+    if (base == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 190}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njc1OTc2OA==", "bodyText": "It looks like this method is only used in the context of: acidDir.getBaseAndDeltaFiles().isEmpty()\nWhat if this method returned a boolean (something like isEmpty) instead?", "url": "https://github.com/apache/hive/pull/1779#discussion_r546759768", "createdAt": "2020-12-21T15:11:29Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidDirectory.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hive.shims.HadoopShims;\n+import org.apache.hive.common.util.Ref;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.hive.ql.io.AcidUtils.AcidBaseFileType.ORIGINAL_BASE;\n+\n+/**\n+ * AcidDirectory used to provide ACID directory layout information, which directories and files to read.\n+ * This representation only valid in a context of a ValidWriteIdList and ValidTxnList.\n+ */\n+public final class AcidDirectory {\n+\n+  private final Path path;\n+  private final FileSystem fs;\n+  private final Ref<Boolean> useFileId;\n+\n+  private AcidUtils.ParsedBase base;\n+  private AcidUtils.ParsedBaseLight oldestBase;\n+\n+  private final List<Path> abortedDirectories = new ArrayList<>();\n+  private final Set<Long> abortedWriteIds = new HashSet<>();\n+  private boolean unCompactedAborts;\n+  private final List<HadoopShims.HdfsFileStatusWithId> originalFiles = new ArrayList<>();\n+  private final List<Path> originalDirectories = new ArrayList<>();\n+  private final List<Path> obsolete = new ArrayList<>();\n+  private final List<AcidUtils.ParsedDelta> currentDirectories = new ArrayList<>();\n+\n+  public AcidDirectory(Path path, FileSystem fs, Ref<Boolean> useFileId) {\n+    this.path = path;\n+    this.fs = fs;\n+    this.useFileId = useFileId;\n+    if (!(this.fs instanceof DistributedFileSystem) && this.useFileId != null) {\n+      this.useFileId.value = false;\n+    }\n+  }\n+\n+  public Path getPath() {\n+    return path;\n+  }\n+\n+  /**\n+   * Get the base directory path.\n+   * @return the base directory to read\n+   */\n+  public Path getBaseDirectory() {\n+    return base == null ? null : base.getBaseDirPath();\n+  }\n+\n+  /**\n+   * Get the base directory.\n+   * @return the base directory to read\n+   */\n+  public AcidUtils.ParsedBase getBase() {\n+    return base;\n+  }\n+\n+  /**\n+   * Oldest base directory in the filesystem, may be shadowed by newer base\n+   */\n+  public AcidUtils.ParsedBaseLight getOldestBase() {\n+    return oldestBase;\n+  }\n+\n+  public void setBase(AcidUtils.ParsedBase base) {\n+    this.base = base;\n+  }\n+\n+  public void setOldestBase(AcidUtils.ParsedBaseLight oldestBase) {\n+    this.oldestBase = oldestBase;\n+  }\n+\n+  public void setUnCompactedAborts(boolean unCompactedAborts) {\n+    this.unCompactedAborts = unCompactedAborts;\n+  }\n+\n+  /**\n+   * Is Base directory in raw format or in Acid format\n+   */\n+  public boolean isBaseInRawFormat() {\n+    return base != null && base.isRawFormat();\n+  }\n+\n+  /**\n+   * Get the list of original files.  Not {@code null}.  Must be sorted.\n+   * @return the list of original files (eg. 000000_0)\n+   */\n+  public List<HadoopShims.HdfsFileStatusWithId> getOriginalFiles() {\n+    return originalFiles;\n+  }\n+\n+  /**\n+   * List of original directories containing files in not ACID format\n+   */\n+  public List<Path> getOriginalDirectories() {\n+    return originalDirectories;\n+  }\n+\n+  /**\n+   * Get the list of delta directories that are valid and not\n+   * obsolete.  Not {@code null}.  List must be sorted in a specific way.\n+   * See {@link org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight#compareTo(org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight)}\n+   * for details.\n+   * @return the minimal list of current directories\n+   */\n+  public List<AcidUtils.ParsedDelta> getCurrentDirectories() {\n+    return currentDirectories;\n+  }\n+\n+  /**\n+   * Get the list of obsolete directories. After filtering out bases and\n+   * deltas that are not selected by the valid transaction/write ids list, return the\n+   * list of original files, bases, and deltas that have been replaced by\n+   * more up to date ones.  Not {@code null}.\n+   */\n+  public List<Path> getObsolete() {\n+    return obsolete;\n+  }\n+\n+  /**\n+   * Get the list of directories that has nothing but aborted transactions.\n+   * @return the list of aborted directories\n+   */\n+  public List<Path> getAbortedDirectories() {\n+    return abortedDirectories;\n+  }\n+\n+  /**\n+   * Get the list of writeIds that belong to the aborted transactions.\n+   * @return the list of aborted writeIds\n+   */\n+  public Set<Long> getAbortedWriteIds() {\n+    return abortedWriteIds;\n+  }\n+\n+  /**\n+   * Does the directory contain writeIds that belong to aborted transactions,\n+   * but are mixed together with committed writes. These aborted writes can not be cleaned.\n+   * @return true if there are aborted writes that can can be cleaned\n+   */\n+  public boolean hasUncompactedAborts() {\n+    return unCompactedAborts;\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  /**\n+   * Delete deltas that should be read by this reader.\n+   */\n+  public List<AcidUtils.ParsedDelta> getDeleteDeltas() {\n+    return currentDirectories.stream().filter(AcidUtils.ParsedDeltaLight::isDeleteDelta).collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * All original, base and delta bucket files that should be read by this reader\n+   * @throws IOException ex\n+   */\n+  public List<AcidUtils.AcidBaseFileInfo> getBaseAndDeltaFiles() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 188}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njc3MDEwMg==", "bodyText": "consider including isBaseInRawFormat as before; maybe also abortedWriteIds.", "url": "https://github.com/apache/hive/pull/1779#discussion_r546770102", "createdAt": "2020-12-21T15:29:53Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidDirectory.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hive.shims.HadoopShims;\n+import org.apache.hive.common.util.Ref;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.hive.ql.io.AcidUtils.AcidBaseFileType.ORIGINAL_BASE;\n+\n+/**\n+ * AcidDirectory used to provide ACID directory layout information, which directories and files to read.\n+ * This representation only valid in a context of a ValidWriteIdList and ValidTxnList.\n+ */\n+public final class AcidDirectory {\n+\n+  private final Path path;\n+  private final FileSystem fs;\n+  private final Ref<Boolean> useFileId;\n+\n+  private AcidUtils.ParsedBase base;\n+  private AcidUtils.ParsedBaseLight oldestBase;\n+\n+  private final List<Path> abortedDirectories = new ArrayList<>();\n+  private final Set<Long> abortedWriteIds = new HashSet<>();\n+  private boolean unCompactedAborts;\n+  private final List<HadoopShims.HdfsFileStatusWithId> originalFiles = new ArrayList<>();\n+  private final List<Path> originalDirectories = new ArrayList<>();\n+  private final List<Path> obsolete = new ArrayList<>();\n+  private final List<AcidUtils.ParsedDelta> currentDirectories = new ArrayList<>();\n+\n+  public AcidDirectory(Path path, FileSystem fs, Ref<Boolean> useFileId) {\n+    this.path = path;\n+    this.fs = fs;\n+    this.useFileId = useFileId;\n+    if (!(this.fs instanceof DistributedFileSystem) && this.useFileId != null) {\n+      this.useFileId.value = false;\n+    }\n+  }\n+\n+  public Path getPath() {\n+    return path;\n+  }\n+\n+  /**\n+   * Get the base directory path.\n+   * @return the base directory to read\n+   */\n+  public Path getBaseDirectory() {\n+    return base == null ? null : base.getBaseDirPath();\n+  }\n+\n+  /**\n+   * Get the base directory.\n+   * @return the base directory to read\n+   */\n+  public AcidUtils.ParsedBase getBase() {\n+    return base;\n+  }\n+\n+  /**\n+   * Oldest base directory in the filesystem, may be shadowed by newer base\n+   */\n+  public AcidUtils.ParsedBaseLight getOldestBase() {\n+    return oldestBase;\n+  }\n+\n+  public void setBase(AcidUtils.ParsedBase base) {\n+    this.base = base;\n+  }\n+\n+  public void setOldestBase(AcidUtils.ParsedBaseLight oldestBase) {\n+    this.oldestBase = oldestBase;\n+  }\n+\n+  public void setUnCompactedAborts(boolean unCompactedAborts) {\n+    this.unCompactedAborts = unCompactedAborts;\n+  }\n+\n+  /**\n+   * Is Base directory in raw format or in Acid format\n+   */\n+  public boolean isBaseInRawFormat() {\n+    return base != null && base.isRawFormat();\n+  }\n+\n+  /**\n+   * Get the list of original files.  Not {@code null}.  Must be sorted.\n+   * @return the list of original files (eg. 000000_0)\n+   */\n+  public List<HadoopShims.HdfsFileStatusWithId> getOriginalFiles() {\n+    return originalFiles;\n+  }\n+\n+  /**\n+   * List of original directories containing files in not ACID format\n+   */\n+  public List<Path> getOriginalDirectories() {\n+    return originalDirectories;\n+  }\n+\n+  /**\n+   * Get the list of delta directories that are valid and not\n+   * obsolete.  Not {@code null}.  List must be sorted in a specific way.\n+   * See {@link org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight#compareTo(org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight)}\n+   * for details.\n+   * @return the minimal list of current directories\n+   */\n+  public List<AcidUtils.ParsedDelta> getCurrentDirectories() {\n+    return currentDirectories;\n+  }\n+\n+  /**\n+   * Get the list of obsolete directories. After filtering out bases and\n+   * deltas that are not selected by the valid transaction/write ids list, return the\n+   * list of original files, bases, and deltas that have been replaced by\n+   * more up to date ones.  Not {@code null}.\n+   */\n+  public List<Path> getObsolete() {\n+    return obsolete;\n+  }\n+\n+  /**\n+   * Get the list of directories that has nothing but aborted transactions.\n+   * @return the list of aborted directories\n+   */\n+  public List<Path> getAbortedDirectories() {\n+    return abortedDirectories;\n+  }\n+\n+  /**\n+   * Get the list of writeIds that belong to the aborted transactions.\n+   * @return the list of aborted writeIds\n+   */\n+  public Set<Long> getAbortedWriteIds() {\n+    return abortedWriteIds;\n+  }\n+\n+  /**\n+   * Does the directory contain writeIds that belong to aborted transactions,\n+   * but are mixed together with committed writes. These aborted writes can not be cleaned.\n+   * @return true if there are aborted writes that can can be cleaned\n+   */\n+  public boolean hasUncompactedAborts() {\n+    return unCompactedAborts;\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  /**\n+   * Delete deltas that should be read by this reader.\n+   */\n+  public List<AcidUtils.ParsedDelta> getDeleteDeltas() {\n+    return currentDirectories.stream().filter(AcidUtils.ParsedDeltaLight::isDeleteDelta).collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * All original, base and delta bucket files that should be read by this reader\n+   * @throws IOException ex\n+   */\n+  public List<AcidUtils.AcidBaseFileInfo> getBaseAndDeltaFiles() throws IOException {\n+    List<AcidUtils.AcidBaseFileInfo> baseAndDeltaFiles = new ArrayList<>();\n+    if (base == null) {\n+      // For non-acid tables (or paths), all data files are in getOriginalFiles() list\n+      for (HadoopShims.HdfsFileStatusWithId fileId : originalFiles) {\n+        baseAndDeltaFiles.add(new AcidUtils.AcidBaseFileInfo(fileId, ORIGINAL_BASE));\n+      }\n+    } else {\n+      // The base files are either listed in ParsedBase or will be populated now\n+      for (HadoopShims.HdfsFileStatusWithId fileId : base.getFiles(fs, useFileId)) {\n+        baseAndDeltaFiles.add(new AcidUtils.AcidBaseFileInfo(fileId,\n+            isBaseInRawFormat() ? ORIGINAL_BASE : AcidUtils.AcidBaseFileType.ACID_SCHEMA));\n+      }\n+    }\n+    for (AcidUtils.ParsedDelta parsedDelta : currentDirectories) {\n+      if (parsedDelta.isDeleteDelta()) {\n+        continue;\n+      }\n+      if (parsedDelta.isRawFormat() && parsedDelta.getMinWriteId() != parsedDelta.getMaxWriteId()) {\n+        // delta/ with files in raw format are a result of Load Data (as opposed to compaction\n+        // or streaming ingest so must have interval length == 1.\n+        throw new IllegalStateException(\n+            \"Delta in \" + ORIGINAL_BASE + \" format but txnIds are out of range: \" + parsedDelta.getPath());\n+      }\n+\n+      AcidUtils.AcidBaseFileType deltaType =\n+          parsedDelta.isRawFormat() ? ORIGINAL_BASE : AcidUtils.AcidBaseFileType.ACID_SCHEMA;\n+      // The bucket files are either listed in ParsedDelta or will be populated now\n+      for (HadoopShims.HdfsFileStatusWithId deltaFile : parsedDelta.getFiles(fs, useFileId)) {\n+        baseAndDeltaFiles.add(new AcidUtils.AcidBaseFileInfo(deltaFile, deltaType));\n+      }\n+    }\n+\n+    return baseAndDeltaFiles;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return \"Aborted Directories: \" + abortedDirectories + \"; original: \" + originalFiles + \"; obsolete: \" + obsolete\n+        + \"; currentDirectories: \" + currentDirectories + \"; base: \" + base;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 227}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njc5MzY3Mw==", "bodyText": "Seems like this might kind of duplicate AcidBaseFileInfo? If so we can get rid of AcidBaseFileInfo?", "url": "https://github.com/apache/hive/pull/1779#discussion_r546793673", "createdAt": "2020-12-21T16:11:08Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -994,18 +857,76 @@ public long getVisibilityTxnId() {\n     public Path getBaseDirPath() {\n       return baseDirPath;\n     }\n-    public static ParsedBase parseBase(Path path) {\n+\n+\n+\n+    public static ParsedBaseLight parseBase(Path path) {\n       String filename = path.getName();\n       if(!filename.startsWith(BASE_PREFIX)) {\n         throw new IllegalArgumentException(filename + \" does not start with \" + BASE_PREFIX);\n       }\n       int idxOfv = filename.indexOf(VISIBILITY_PREFIX);\n       if(idxOfv < 0) {\n-        return new ParsedBase(Long.parseLong(filename.substring(BASE_PREFIX.length())), path);\n+        return new ParsedBaseLight(Long.parseLong(filename.substring(BASE_PREFIX.length())), path);\n       }\n-      return new ParsedBase(Long.parseLong(filename.substring(BASE_PREFIX.length(), idxOfv)),\n+      return new ParsedBaseLight(Long.parseLong(filename.substring(BASE_PREFIX.length(), idxOfv)),\n           Long.parseLong(filename.substring(idxOfv + VISIBILITY_PREFIX.length())), path);\n     }\n+\n+    @Override\n+    public String toString() {\n+      return \"Path: \" + baseDirPath + \"; writeId: \"\n+          + writeId + \"; visibilityTxnId: \" + visibilityTxnId;\n+    }\n+  }\n+  /**\n+   * In addition to {@link ParsedBaseLight} this knows if the data is in raw format, i.e. doesn't\n+   * have acid metadata columns embedded in the files.  To determine this in some cases\n+   * requires looking at the footer of the data file which can be expensive so if this info is\n+   * not needed {@link ParsedBaseLight} should be used.\n+   */\n+  public static final class ParsedBase extends ParsedBaseLight {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 230}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjgwMDI2Ng==", "bodyText": "Why was this block (1260-1268) needed originally?", "url": "https://github.com/apache/hive/pull/1779#discussion_r546800266", "createdAt": "2020-12-21T16:23:21Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -1253,19 +1190,12 @@ public static ParsedDelta parsedDelta(Path deltaDir, String deltaPrefix, FileSys\n       ParsedDelta p = parsedDelta(deltaDir, isRawFormat);\n       List<HdfsFileStatusWithId> files = null;\n       if (dirSnapshot != null) {\n+        final PathFilter filter = isRawFormat ? AcidUtils.originalBucketFilter : AcidUtils.bucketFileFilter;\n+        // If we already know the files, store it for future use\n         files = dirSnapshot.getFiles().stream()\n-            .filter(fileStatus -> bucketFileFilter.accept(fileStatus.getPath()))\n+            .filter(fileStatus -> filter.accept(fileStatus.getPath()))\n             .map(HdfsFileStatusWithoutId::new)\n             .collect(Collectors.toList());\n-      } else if (isDeleteDelta) {\n-        // For delete deltas we need the files for AcidState\n-        try {\n-          files = SHIMS.listLocatedHdfsStatus(fs, deltaDir, bucketFileFilter);\n-        } catch (UnsupportedOperationException uoe) {\n-          files = Arrays.stream(fs.listStatus(deltaDir, bucketFileFilter))\n-              .map(HdfsFileStatusWithoutId::new)\n-              .collect(Collectors.toList());\n-        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 392}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjgwMTgyMQ==", "bodyText": "getChildState parameters have been changed", "url": "https://github.com/apache/hive/pull/1779#discussion_r546801821", "createdAt": "2020-12-21T16:26:06Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -1490,49 +1432,31 @@ else if (prev != null && next.maxWriteId == prev.maxWriteId\n         prev = next;\n       }\n       else {\n-        obsolete.add(next.path);\n+        directory.getObsolete().add(next.path);\n       }\n     }\n+    directory.getCurrentDirectories().clear();\n+    directory.getCurrentDirectories().addAll(deltas);\n+  }\n \n-    if(bestBase.oldestBase != null && bestBase.basePath == null &&\n-        isCompactedBase(ParsedBase.parseBase(bestBase.oldestBase), fs, dirSnapshots)) {\n+  private static ValidTxnList getValidTxnList(Configuration conf) {\n+    ValidTxnList validTxnList = null;\n+    String s = conf.get(ValidTxnList.VALID_TXNS_KEY);\n+    if(!Strings.isNullOrEmpty(s)) {\n       /*\n-       * If here, it means there was a base_x (> 1 perhaps) but none were suitable for given\n-       * {@link writeIdList}.  Note that 'original' files are logically a base_Long.MIN_VALUE and thus\n-       * cannot have any data for an open txn.  We could check {@link deltas} has files to cover\n-       * [1,n] w/o gaps but this would almost never happen...\n+       * getAcidState() is sometimes called on non-transactional tables, e.g.\n+       * OrcInputFileFormat.FileGenerator.callInternal().  e.g. orc_merge3.q In that case\n+       * writeIdList is bogus - doesn't even have a table name.\n+       * see https://issues.apache.org/jira/browse/HIVE-20856.\n        *\n-       * We only throw for base_x produced by Compactor since that base erases all history and\n-       * cannot be used for a client that has a snapshot in which something inside this base is\n-       * open.  (Nor can we ignore this base of course)  But base_x which is a result of IOW,\n-       * contains all history so we treat it just like delta wrt visibility.  Imagine, IOW which\n-       * aborts. It creates a base_x, which can and should just be ignored.*/\n-      long[] exceptions = writeIdList.getInvalidWriteIds();\n-      String minOpenWriteId = exceptions != null && exceptions.length > 0 ?\n-        Long.toString(exceptions[0]) : \"x\";\n-      throw new IOException(ErrorMsg.ACID_NOT_ENOUGH_HISTORY.format(\n-        Long.toString(writeIdList.getHighWatermark()),\n-              minOpenWriteId, bestBase.oldestBase.toString()));\n-    }\n-\n-    Path base = null;\n-    boolean isBaseInRawFormat = false;\n-    if (bestBase.basePath != null) {\n-      base = bestBase.basePath;\n-      isBaseInRawFormat = MetaDataFile.isRawFormat(base, fs, dirSnapshots != null ? dirSnapshots.get(base) : null);\n+       * For now, assert that ValidTxnList.VALID_TXNS_KEY is set only if this is really a read\n+       * of a transactional table.\n+       * see {@link #getChildState(FileStatus, HdfsFileStatusWithId, ValidWriteIdList, List, List, List, List, TxnBase, boolean, List, Map, FileSystem, ValidTxnList)}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 615}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjgwNzQ4OQ==", "bodyText": "Why was this here?", "url": "https://github.com/apache/hive/pull/1779#discussion_r546807489", "createdAt": "2020-12-21T16:36:24Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -3360,14 +3257,13 @@ public static Directory getAcidStateFromCache(Supplier<FileSystem> fileSystem,\n \n     // compute and add to cache\n     if (recompute || (value == null)) {\n-      Directory dirInfo = getAcidState(fileSystem.get(), candidateDirectory, conf,\n+      AcidDirectory dirInfo = getAcidState(fileSystem.get(), candidateDirectory, conf,\n           writeIdList, useFileIds, ignoreEmptyFiles);\n       value = new DirInfoValue(writeIdList.writeToString(), dirInfo);\n \n       if (value.dirInfo != null && value.dirInfo.getBaseDirectory() != null\n           && value.dirInfo.getCurrentDirectories().isEmpty()) {\n         if (dirCacheDuration > 0) {\n-          populateBaseFiles(dirInfo, useFileIds, fileSystem);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 988}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7f37790c505a64c89aa81754f44015b5497d9458", "author": {"user": {"login": "pvargacl", "name": "Peter Varga"}}, "url": "https://github.com/apache/hive/commit/7f37790c505a64c89aa81754f44015b5497d9458", "committedDate": "2020-12-22T09:43:39Z", "message": "Address review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5afbec44baf1dfa7ce352538a521275f79b11eeb", "author": {"user": {"login": "pvargacl", "name": "Peter Varga"}}, "url": "https://github.com/apache/hive/commit/5afbec44baf1dfa7ce352538a521275f79b11eeb", "committedDate": "2020-12-22T09:44:46Z", "message": "Merge remote-tracking branch 'origin/master' into aciddirectory\n\n# Conflicts:\n#\tql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "212849354de743c280b3d6944956d5ae53a380b2", "author": {"user": {"login": "pvargacl", "name": "Peter Varga"}}, "url": "https://github.com/apache/hive/commit/212849354de743c280b3d6944956d5ae53a380b2", "committedDate": "2020-12-22T11:12:51Z", "message": "revert MM_ALLOW_ORIGINALS"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3MDgzNTYx", "url": "https://github.com/apache/hive/pull/1779#pullrequestreview-557083561", "createdAt": "2020-12-22T13:37:00Z", "commit": {"oid": "212849354de743c280b3d6944956d5ae53a380b2"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2920, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}