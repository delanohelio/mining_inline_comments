{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU3MzA0MDYy", "number": 1325, "title": "HIVE-21196 HIVE-23940 Multi-column semijoin reducers and TPC-H datasets ", "bodyText": "Multi column semi-join reducers were tested using:\nmvn test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=dynamic_semijoin_reduction_multicol.q\nmvn test -Dtest=TestTezPerfCliDriver\nTPC-H datasets were tested using:\nmvn test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=tpch_sf0_001_datasets.q\nmvn test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=tpch_tiny_datasets.q", "createdAt": "2020-07-27T17:35:21Z", "url": "https://github.com/apache/hive/pull/1325", "merged": true, "mergeCommit": {"oid": "e6dd5546d8224d0afe4b76c3e939781db183a897"}, "closed": true, "closedAt": "2020-08-10T14:18:07Z", "author": {"login": "zabetak"}, "timelineItems": {"totalCount": 35, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc5ZccyABqjM1OTUzMTM3NTY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc9U_2HAH2gAyNDU3MzA0MDYyOjc0NjRjYjNkM2QyZGMxOWE0OThjYmVmNDFiZGUwOWM4NGJhN2NiNjU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8ae111c53bbe8856c81ad18e96134651d3b96d81", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/8ae111c53bbe8856c81ad18e96134651d3b96d81", "committedDate": "2020-07-27T17:20:42Z", "message": "HIVE-21196: Add tests for multicol semijoin reduction"}, "afterCommit": {"oid": "7ec73bb62d7fab120793e7a223c827db943bebd6", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/7ec73bb62d7fab120793e7a223c827db943bebd6", "committedDate": "2020-07-28T16:13:03Z", "message": "HIVE-21196: Update TestTezPerfConstraintsCliDriver query plans\n\nImportant changes only on query24, query50, query93 plans.\nSingle columns semi join reducers are merged to composite ones as per optimization."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7ec73bb62d7fab120793e7a223c827db943bebd6", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/7ec73bb62d7fab120793e7a223c827db943bebd6", "committedDate": "2020-07-28T16:13:03Z", "message": "HIVE-21196: Update TestTezPerfConstraintsCliDriver query plans\n\nImportant changes only on query24, query50, query93 plans.\nSingle columns semi join reducers are merged to composite ones as per optimization."}, "afterCommit": {"oid": "555ac23c2bc2b13e2b45529e78003d32a5b3ff58", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/555ac23c2bc2b13e2b45529e78003d32a5b3ff58", "committedDate": "2020-07-29T13:32:23Z", "message": "HIVE-21196: Update TestTezPerfConstraintsCliDriver query plans\n\nImportant changes only on query24, query50, query93 plans.\nSingle columns semi join reducers are merged to composite ones as per optimization."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "555ac23c2bc2b13e2b45529e78003d32a5b3ff58", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/555ac23c2bc2b13e2b45529e78003d32a5b3ff58", "committedDate": "2020-07-29T13:32:23Z", "message": "HIVE-21196: Update TestTezPerfConstraintsCliDriver query plans\n\nImportant changes only on query24, query50, query93 plans.\nSingle columns semi join reducers are merged to composite ones as per optimization."}, "afterCommit": {"oid": "792f7f2faece0017c524270c3dd0df674057d4c8", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/792f7f2faece0017c524270c3dd0df674057d4c8", "committedDate": "2020-07-30T12:55:14Z", "message": "HIVE-21196: Update TestTezPerfConstraintsCliDriver query plans\n\nImportant changes only on query24, query50, query93 plans.\nSingle columns semi join reducers are merged to composite ones as per optimization."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "25383ae9e530378c31bbfbf6b4124bb8e4cfd0bd", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/25383ae9e530378c31bbfbf6b4124bb8e4cfd0bd", "committedDate": "2020-08-03T13:05:14Z", "message": "HIVE-23940: Add TPCH tables (scale factor 0.001) as qt datasets\n\n1. Add new tables in a separate Hive database namely tpch_0_001 to distinguish\nfrom existing lineitem, part tables in default database.\n2. Compress existing part-tiny and lineitem data to gain a bit of space."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c47de66808b13d1c2d02bcb2b623f4b6b144146a", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/c47de66808b13d1c2d02bcb2b623f4b6b144146a", "committedDate": "2020-08-03T13:40:51Z", "message": "HIVE-21196: Add transformation merging multiple single-col SJ reducers to one multi-col SJ\n\n1. Put the merge transformation logic in self-contained SemiJoinReductionMerge class\n2. Extend SJ related code (outside of SemiJoinReductionMerge) to account for multiple columns\n3. Add zero as lower bound for in bloomfilter reduction factor estimation\n4. Execute semi-join reducers merge transformation just before stats annotation\n5. Remove duplicate case branch in SemanticAnalyzer#groupByDescModeToUDAFMode\n6. Add utility function for finding an ancestor operator provided a path\n7. Add configuration property for multi-column semi-join reduction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a4c6b5b889ae1477a8fe016c3e21d4f0ac11d0d3", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/a4c6b5b889ae1477a8fe016c3e21d4f0ac11d0d3", "committedDate": "2020-08-03T13:40:51Z", "message": "HIVE-21196: Add tests for multicol semijoin reduction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "089ffbe841c80eccce9827185991d0ec860c5e9e", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/089ffbe841c80eccce9827185991d0ec860c5e9e", "committedDate": "2020-08-03T13:40:51Z", "message": "HIVE-21196: Update plans in Perf cli drivers\n\nMulti-col semijoin reduction optimization correctly kicks in in queries:\nquery24, query50, query93\n\nThe remaining changes are not significant and consist only in different\nopperator ids."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/edffed725aa5cb794578688d353cd015bc54cb6d", "committedDate": "2020-08-03T13:40:51Z", "message": "HIVE-21196: Update plans in MiniLlap drivers"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "792f7f2faece0017c524270c3dd0df674057d4c8", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/792f7f2faece0017c524270c3dd0df674057d4c8", "committedDate": "2020-07-30T12:55:14Z", "message": "HIVE-21196: Update TestTezPerfConstraintsCliDriver query plans\n\nImportant changes only on query24, query50, query93 plans.\nSingle columns semi join reducers are merged to composite ones as per optimization."}, "afterCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/edffed725aa5cb794578688d353cd015bc54cb6d", "committedDate": "2020-08-03T13:40:51Z", "message": "HIVE-21196: Update plans in MiniLlap drivers"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYwNDU1ODQy", "url": "https://github.com/apache/hive/pull/1325#pullrequestreview-460455842", "createdAt": "2020-08-04T02:33:38Z", "commit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "state": "COMMENTED", "comments": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMjozMzozOFrOG7O5tw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNDoyMTozMlrOG7QjNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2MzMxOQ==", "bodyText": "Neat! Interesting method... Reminds me of our good old times with XPath \ud83d\ude04", "url": "https://github.com/apache/hive/pull/1325#discussion_r464763319", "createdAt": "2020-08-04T02:33:38Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java", "diffHunk": "@@ -53,6 +53,34 @@\n \n   private static final Logger LOG = LoggerFactory.getLogger(OperatorUtils.class);\n \n+  /**\n+   * Return the ancestor of the specified operator at the provided path or null if the path is invalid.\n+   *\n+   * The method is equivalent to following code:\n+   * <pre>{@code\n+   *     op.getParentOperators().get(path[0])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NDA1OQ==", "bodyText": "nit. We use { } even for single line statements.\nPlease, check below in other code changes in this PR too since I have seen the same.", "url": "https://github.com/apache/hive/pull/1325#discussion_r464764059", "createdAt": "2020-08-04T02:36:31Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java", "diffHunk": "@@ -53,6 +53,34 @@\n \n   private static final Logger LOG = LoggerFactory.getLogger(OperatorUtils.class);\n \n+  /**\n+   * Return the ancestor of the specified operator at the provided path or null if the path is invalid.\n+   *\n+   * The method is equivalent to following code:\n+   * <pre>{@code\n+   *     op.getParentOperators().get(path[0])\n+   *     .getParentOperators().get(path[1])\n+   *     ...\n+   *     .getParentOperators().get(path[n])\n+   * }</pre>\n+   * with additional checks about the validity of the provided path and the type of the ancestor.\n+   *\n+   * @param op the operator for which we\n+   * @param clazz the class of the ancestor operator\n+   * @param path the path leading to the desired ancestor\n+   * @param <T> the type of the ancestor\n+   * @return the ancestor of the specified operator at the provided path or null if the path is invalid.\n+   */\n+  public static <T> T ancestor(Operator<?> op, Class<T> clazz, int... path) {\n+    Operator<?> target = op;\n+    for (int i = 0; i < path.length; i++) {\n+      if (target.getParentOperators() == null || path[i] > target.getParentOperators().size())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NDI5OQ==", "bodyText": "Please add comment with general explanation of purpose to the class.\nIt would also be useful to add comments to the methods below.", "url": "https://github.com/apache/hive/pull/1325#discussion_r464764299", "createdAt": "2020-08-04T02:37:31Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NDg1OQ==", "bodyText": "Wondering whether we should make these checks Preconditions rather than assert.", "url": "https://github.com/apache/hive/pull/1325#discussion_r464764859", "createdAt": "2020-08-04T02:39:53Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NzUwOA==", "bodyText": "Maybe using a LinkedHashMap for rsToSemiJoinBranchInfo from the onset would eliminate the need for sorting and above (just a thought).", "url": "https://github.com/apache/hive/pull/1325#discussion_r464767508", "createdAt": "2020-08-04T02:49:55Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2NzY4MA==", "bodyText": "typo? sjBrances -> sjBranches", "url": "https://github.com/apache/hive/pull/1325#discussion_r464767680", "createdAt": "2020-08-04T02:50:37Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3ODE2NA==", "bodyText": "Can these SEL operators have multiple columns? I thought they would project a single column for the SJ?", "url": "https://github.com/apache/hive/pull/1325#discussion_r464778164", "createdAt": "2020-08-04T03:32:12Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);\n+\n+      GroupByOperator gbPartialOp = createGroupBy(selectOp, selectOp, GroupByDesc.Mode.HASH, hiveConf);\n+\n+      ReduceSinkOperator rsPartialOp = createReduceSink(gbPartialOp, NullOrdering.defaultNullOrder(hiveConf));\n+      rsPartialOp.getConf().setReducerTraits(EnumSet.of(ReduceSinkDesc.ReducerTraits.QUICKSTART));\n+\n+      GroupByOperator gbCompleteOp = createGroupBy(selectOp, rsPartialOp, GroupByDesc.Mode.FINAL, hiveConf);\n+\n+      ReduceSinkOperator rsCompleteOp = createReduceSink(gbCompleteOp, NullOrdering.defaultNullOrder(hiveConf));\n+\n+      final TableScanOperator sjTargetTable = sjMergeCandidate.getKey().target;\n+      SemiJoinBranchInfo sjInfo = new SemiJoinBranchInfo(sjTargetTable, false);\n+      parseContext.getRsToSemiJoinBranchInfo().put(rsCompleteOp, sjInfo);\n+\n+      // Save the info that is required at query time to resolve dynamic/runtime values.\n+      RuntimeValuesInfo valuesInfo = createRuntimeValuesInfo(rsCompleteOp, sjBrances, parseContext);\n+      parseContext.getRsToRuntimeValuesInfoMap().put(rsCompleteOp, valuesInfo);\n+\n+      ExprNodeGenericFuncDesc sjPredicate = createSemiJoinPredicate(sjBrances, valuesInfo, parseContext);\n+\n+      // Update filter operators with the new semi-join predicate\n+      for (Operator<?> op : sjTargetTable.getChildOperators()) {\n+        if (op instanceof FilterOperator) {\n+          FilterDesc filter = ((FilterOperator) op).getConf();\n+          filter.setPredicate(and(Arrays.asList(filter.getPredicate(), sjPredicate)));\n+        }\n+      }\n+      // Update tableScan with the new semi-join predicate\n+      sjTargetTable.getConf().setFilterExpr(and(Arrays.asList(sjTargetTable.getConf().getFilterExpr(), sjPredicate)));\n+\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        GenTezUtils.removeSemiJoinOperator(parseContext, rs, sjTargetTable);\n+        GenTezUtils.removeBranch(rs);\n+      }\n+\n+      // TODO How to associate multi-cols with gb ?\n+      // parseContext.getColExprToGBMap().put(key, gb);\n+    }\n+    return parseContext;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc createSemiJoinPredicate(List<ReduceSinkOperator> sjBranches,\n+      RuntimeValuesInfo sjValueInfo, ParseContext context) {\n+    // Performance note: To speed-up evaluation 'BETWEEN' predicates should come before the 'IN_BLOOM_FILTER'\n+    Deque<String> dynamicIds = new ArrayDeque<>(sjValueInfo.getDynamicValueIDs());\n+    List<ExprNodeDesc> sjPredicates = new ArrayList<>();\n+    List<ExprNodeDesc> hashArgs = new ArrayList<>();\n+    for (ReduceSinkOperator rs : sjBranches) {\n+      RuntimeValuesInfo info = context.getRsToRuntimeValuesInfoMap().get(rs);\n+      assert info.getTargetColumns().size() == 1;\n+      final ExprNodeDesc targetColumn = info.getTargetColumns().get(0);\n+      TypeInfo typeInfo = targetColumn.getTypeInfo();\n+      DynamicValue minDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+      DynamicValue maxDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+\n+      List<ExprNodeDesc> betweenArgs = Arrays.asList(\n+          // Use false to not invert between result\n+          new ExprNodeConstantDesc(Boolean.FALSE),\n+          targetColumn,\n+          new ExprNodeDynamicValueDesc(minDynamic),\n+          new ExprNodeDynamicValueDesc(maxDynamic));\n+      ExprNodeDesc betweenExp =\n+          new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFBetween(), \"between\", betweenArgs);\n+      sjPredicates.add(betweenExp);\n+      hashArgs.add(targetColumn);\n+    }\n+\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", hashArgs);\n+\n+    assert dynamicIds.size() == 1 : \"There should be one column left untreated the one with the bloom filter\";\n+    DynamicValue bloomDynamic = new DynamicValue(dynamicIds.poll(), TypeInfoFactory.binaryTypeInfo);\n+    sjPredicates.add(\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFInBloomFilter(), \"in_bloom_filter\",\n+            Arrays.asList(hashExp, new ExprNodeDynamicValueDesc(bloomDynamic))));\n+    return and(sjPredicates);\n+  }\n+\n+  private static RuntimeValuesInfo createRuntimeValuesInfo(ReduceSinkOperator rs, List<ReduceSinkOperator> sjBranches,\n+      ParseContext parseContext) {\n+    List<ExprNodeDesc> valueCols = rs.getConf().getValueCols();\n+    RuntimeValuesInfo info = new RuntimeValuesInfo();\n+    TableDesc rsFinalTableDesc =\n+        PlanUtils.getReduceValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols, \"_col\"));\n+    List<String> dynamicValueIDs = new ArrayList<>();\n+    for (ExprNodeDesc rsCol : valueCols) {\n+      dynamicValueIDs.add(rs.toString() + rsCol.getExprString());\n+    }\n+\n+    info.setTableDesc(rsFinalTableDesc);\n+    info.setDynamicValueIDs(dynamicValueIDs);\n+    info.setColExprs(valueCols);\n+    List<ExprNodeDesc> targetTableExpressions = new ArrayList<>();\n+    for (ReduceSinkOperator sjBranch : sjBranches) {\n+      RuntimeValuesInfo sjInfo = parseContext.getRsToRuntimeValuesInfoMap().get(sjBranch);\n+      assert sjInfo.getTargetColumns().size() == 1;\n+      targetTableExpressions.add(sjInfo.getTargetColumns().get(0));\n+    }\n+    info.setTargetColumns(targetTableExpressions);\n+    return info;\n+  }\n+\n+  private static SelectOperator mergeSelectOps(Operator<?> parent, List<SelectOperator> selectOperators) {\n+    List<String> colNames = new ArrayList<>();\n+    List<ExprNodeDesc> colDescs = new ArrayList<>();\n+    List<ColumnInfo> columnInfos = new ArrayList<>();\n+    Map<String, ExprNodeDesc> selectColumnExprMap = new HashMap<>();\n+    for (SelectOperator sel : selectOperators) {\n+      for (ExprNodeDesc col : sel.getConf().getColList()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 221}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3OTcxNQ==", "bodyText": "Add comment mentioning you need these for min/max", "url": "https://github.com/apache/hive/pull/1325#discussion_r464779715", "createdAt": "2020-08-04T03:38:24Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);\n+\n+      GroupByOperator gbPartialOp = createGroupBy(selectOp, selectOp, GroupByDesc.Mode.HASH, hiveConf);\n+\n+      ReduceSinkOperator rsPartialOp = createReduceSink(gbPartialOp, NullOrdering.defaultNullOrder(hiveConf));\n+      rsPartialOp.getConf().setReducerTraits(EnumSet.of(ReduceSinkDesc.ReducerTraits.QUICKSTART));\n+\n+      GroupByOperator gbCompleteOp = createGroupBy(selectOp, rsPartialOp, GroupByDesc.Mode.FINAL, hiveConf);\n+\n+      ReduceSinkOperator rsCompleteOp = createReduceSink(gbCompleteOp, NullOrdering.defaultNullOrder(hiveConf));\n+\n+      final TableScanOperator sjTargetTable = sjMergeCandidate.getKey().target;\n+      SemiJoinBranchInfo sjInfo = new SemiJoinBranchInfo(sjTargetTable, false);\n+      parseContext.getRsToSemiJoinBranchInfo().put(rsCompleteOp, sjInfo);\n+\n+      // Save the info that is required at query time to resolve dynamic/runtime values.\n+      RuntimeValuesInfo valuesInfo = createRuntimeValuesInfo(rsCompleteOp, sjBrances, parseContext);\n+      parseContext.getRsToRuntimeValuesInfoMap().put(rsCompleteOp, valuesInfo);\n+\n+      ExprNodeGenericFuncDesc sjPredicate = createSemiJoinPredicate(sjBrances, valuesInfo, parseContext);\n+\n+      // Update filter operators with the new semi-join predicate\n+      for (Operator<?> op : sjTargetTable.getChildOperators()) {\n+        if (op instanceof FilterOperator) {\n+          FilterDesc filter = ((FilterOperator) op).getConf();\n+          filter.setPredicate(and(Arrays.asList(filter.getPredicate(), sjPredicate)));\n+        }\n+      }\n+      // Update tableScan with the new semi-join predicate\n+      sjTargetTable.getConf().setFilterExpr(and(Arrays.asList(sjTargetTable.getConf().getFilterExpr(), sjPredicate)));\n+\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        GenTezUtils.removeSemiJoinOperator(parseContext, rs, sjTargetTable);\n+        GenTezUtils.removeBranch(rs);\n+      }\n+\n+      // TODO How to associate multi-cols with gb ?\n+      // parseContext.getColExprToGBMap().put(key, gb);\n+    }\n+    return parseContext;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc createSemiJoinPredicate(List<ReduceSinkOperator> sjBranches,\n+      RuntimeValuesInfo sjValueInfo, ParseContext context) {\n+    // Performance note: To speed-up evaluation 'BETWEEN' predicates should come before the 'IN_BLOOM_FILTER'\n+    Deque<String> dynamicIds = new ArrayDeque<>(sjValueInfo.getDynamicValueIDs());\n+    List<ExprNodeDesc> sjPredicates = new ArrayList<>();\n+    List<ExprNodeDesc> hashArgs = new ArrayList<>();\n+    for (ReduceSinkOperator rs : sjBranches) {\n+      RuntimeValuesInfo info = context.getRsToRuntimeValuesInfoMap().get(rs);\n+      assert info.getTargetColumns().size() == 1;\n+      final ExprNodeDesc targetColumn = info.getTargetColumns().get(0);\n+      TypeInfo typeInfo = targetColumn.getTypeInfo();\n+      DynamicValue minDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+      DynamicValue maxDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+\n+      List<ExprNodeDesc> betweenArgs = Arrays.asList(\n+          // Use false to not invert between result\n+          new ExprNodeConstantDesc(Boolean.FALSE),\n+          targetColumn,\n+          new ExprNodeDynamicValueDesc(minDynamic),\n+          new ExprNodeDynamicValueDesc(maxDynamic));\n+      ExprNodeDesc betweenExp =\n+          new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFBetween(), \"between\", betweenArgs);\n+      sjPredicates.add(betweenExp);\n+      hashArgs.add(targetColumn);\n+    }\n+\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", hashArgs);\n+\n+    assert dynamicIds.size() == 1 : \"There should be one column left untreated the one with the bloom filter\";\n+    DynamicValue bloomDynamic = new DynamicValue(dynamicIds.poll(), TypeInfoFactory.binaryTypeInfo);\n+    sjPredicates.add(\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFInBloomFilter(), \"in_bloom_filter\",\n+            Arrays.asList(hashExp, new ExprNodeDynamicValueDesc(bloomDynamic))));\n+    return and(sjPredicates);\n+  }\n+\n+  private static RuntimeValuesInfo createRuntimeValuesInfo(ReduceSinkOperator rs, List<ReduceSinkOperator> sjBranches,\n+      ParseContext parseContext) {\n+    List<ExprNodeDesc> valueCols = rs.getConf().getValueCols();\n+    RuntimeValuesInfo info = new RuntimeValuesInfo();\n+    TableDesc rsFinalTableDesc =\n+        PlanUtils.getReduceValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols, \"_col\"));\n+    List<String> dynamicValueIDs = new ArrayList<>();\n+    for (ExprNodeDesc rsCol : valueCols) {\n+      dynamicValueIDs.add(rs.toString() + rsCol.getExprString());\n+    }\n+\n+    info.setTableDesc(rsFinalTableDesc);\n+    info.setDynamicValueIDs(dynamicValueIDs);\n+    info.setColExprs(valueCols);\n+    List<ExprNodeDesc> targetTableExpressions = new ArrayList<>();\n+    for (ReduceSinkOperator sjBranch : sjBranches) {\n+      RuntimeValuesInfo sjInfo = parseContext.getRsToRuntimeValuesInfoMap().get(sjBranch);\n+      assert sjInfo.getTargetColumns().size() == 1;\n+      targetTableExpressions.add(sjInfo.getTargetColumns().get(0));\n+    }\n+    info.setTargetColumns(targetTableExpressions);\n+    return info;\n+  }\n+\n+  private static SelectOperator mergeSelectOps(Operator<?> parent, List<SelectOperator> selectOperators) {\n+    List<String> colNames = new ArrayList<>();\n+    List<ExprNodeDesc> colDescs = new ArrayList<>();\n+    List<ColumnInfo> columnInfos = new ArrayList<>();\n+    Map<String, ExprNodeDesc> selectColumnExprMap = new HashMap<>();\n+    for (SelectOperator sel : selectOperators) {\n+      for (ExprNodeDesc col : sel.getConf().getColList()) {\n+        String colName = HiveConf.getColumnInternalName(colDescs.size());\n+        colNames.add(colName);\n+        columnInfos.add(new ColumnInfo(colName, col.getTypeInfo(), \"\", false));\n+        colDescs.add(col);\n+        selectColumnExprMap.put(colName, col);\n+      }\n+    }\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", colDescs);\n+    String hashName = HiveConf.getColumnInternalName(colDescs.size() + 1);\n+    colNames.add(hashName);\n+    columnInfos.add(new ColumnInfo(hashName, hashExp.getTypeInfo(), \"\", false));\n+\n+    List<ExprNodeDesc> selDescs = new ArrayList<>(colDescs);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MDM5NA==", "bodyText": "It seems we are somehow duplicating part of the logic to create SJs below. We can create a follow-up JIRA to try to consolidate this logic with the creation path for single column SJs.", "url": "https://github.com/apache/hive/pull/1325#discussion_r464780394", "createdAt": "2020-08-04T03:41:14Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MDc4Mg==", "bodyText": "It would be interesting to flatten here in case any of the args is an AND.", "url": "https://github.com/apache/hive/pull/1325#discussion_r464780782", "createdAt": "2020-08-04T03:42:50Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);\n+\n+      GroupByOperator gbPartialOp = createGroupBy(selectOp, selectOp, GroupByDesc.Mode.HASH, hiveConf);\n+\n+      ReduceSinkOperator rsPartialOp = createReduceSink(gbPartialOp, NullOrdering.defaultNullOrder(hiveConf));\n+      rsPartialOp.getConf().setReducerTraits(EnumSet.of(ReduceSinkDesc.ReducerTraits.QUICKSTART));\n+\n+      GroupByOperator gbCompleteOp = createGroupBy(selectOp, rsPartialOp, GroupByDesc.Mode.FINAL, hiveConf);\n+\n+      ReduceSinkOperator rsCompleteOp = createReduceSink(gbCompleteOp, NullOrdering.defaultNullOrder(hiveConf));\n+\n+      final TableScanOperator sjTargetTable = sjMergeCandidate.getKey().target;\n+      SemiJoinBranchInfo sjInfo = new SemiJoinBranchInfo(sjTargetTable, false);\n+      parseContext.getRsToSemiJoinBranchInfo().put(rsCompleteOp, sjInfo);\n+\n+      // Save the info that is required at query time to resolve dynamic/runtime values.\n+      RuntimeValuesInfo valuesInfo = createRuntimeValuesInfo(rsCompleteOp, sjBrances, parseContext);\n+      parseContext.getRsToRuntimeValuesInfoMap().put(rsCompleteOp, valuesInfo);\n+\n+      ExprNodeGenericFuncDesc sjPredicate = createSemiJoinPredicate(sjBrances, valuesInfo, parseContext);\n+\n+      // Update filter operators with the new semi-join predicate\n+      for (Operator<?> op : sjTargetTable.getChildOperators()) {\n+        if (op instanceof FilterOperator) {\n+          FilterDesc filter = ((FilterOperator) op).getConf();\n+          filter.setPredicate(and(Arrays.asList(filter.getPredicate(), sjPredicate)));\n+        }\n+      }\n+      // Update tableScan with the new semi-join predicate\n+      sjTargetTable.getConf().setFilterExpr(and(Arrays.asList(sjTargetTable.getConf().getFilterExpr(), sjPredicate)));\n+\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        GenTezUtils.removeSemiJoinOperator(parseContext, rs, sjTargetTable);\n+        GenTezUtils.removeBranch(rs);\n+      }\n+\n+      // TODO How to associate multi-cols with gb ?\n+      // parseContext.getColExprToGBMap().put(key, gb);\n+    }\n+    return parseContext;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc createSemiJoinPredicate(List<ReduceSinkOperator> sjBranches,\n+      RuntimeValuesInfo sjValueInfo, ParseContext context) {\n+    // Performance note: To speed-up evaluation 'BETWEEN' predicates should come before the 'IN_BLOOM_FILTER'\n+    Deque<String> dynamicIds = new ArrayDeque<>(sjValueInfo.getDynamicValueIDs());\n+    List<ExprNodeDesc> sjPredicates = new ArrayList<>();\n+    List<ExprNodeDesc> hashArgs = new ArrayList<>();\n+    for (ReduceSinkOperator rs : sjBranches) {\n+      RuntimeValuesInfo info = context.getRsToRuntimeValuesInfoMap().get(rs);\n+      assert info.getTargetColumns().size() == 1;\n+      final ExprNodeDesc targetColumn = info.getTargetColumns().get(0);\n+      TypeInfo typeInfo = targetColumn.getTypeInfo();\n+      DynamicValue minDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+      DynamicValue maxDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+\n+      List<ExprNodeDesc> betweenArgs = Arrays.asList(\n+          // Use false to not invert between result\n+          new ExprNodeConstantDesc(Boolean.FALSE),\n+          targetColumn,\n+          new ExprNodeDynamicValueDesc(minDynamic),\n+          new ExprNodeDynamicValueDesc(maxDynamic));\n+      ExprNodeDesc betweenExp =\n+          new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFBetween(), \"between\", betweenArgs);\n+      sjPredicates.add(betweenExp);\n+      hashArgs.add(targetColumn);\n+    }\n+\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", hashArgs);\n+\n+    assert dynamicIds.size() == 1 : \"There should be one column left untreated the one with the bloom filter\";\n+    DynamicValue bloomDynamic = new DynamicValue(dynamicIds.poll(), TypeInfoFactory.binaryTypeInfo);\n+    sjPredicates.add(\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFInBloomFilter(), \"in_bloom_filter\",\n+            Arrays.asList(hashExp, new ExprNodeDynamicValueDesc(bloomDynamic))));\n+    return and(sjPredicates);\n+  }\n+\n+  private static RuntimeValuesInfo createRuntimeValuesInfo(ReduceSinkOperator rs, List<ReduceSinkOperator> sjBranches,\n+      ParseContext parseContext) {\n+    List<ExprNodeDesc> valueCols = rs.getConf().getValueCols();\n+    RuntimeValuesInfo info = new RuntimeValuesInfo();\n+    TableDesc rsFinalTableDesc =\n+        PlanUtils.getReduceValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols, \"_col\"));\n+    List<String> dynamicValueIDs = new ArrayList<>();\n+    for (ExprNodeDesc rsCol : valueCols) {\n+      dynamicValueIDs.add(rs.toString() + rsCol.getExprString());\n+    }\n+\n+    info.setTableDesc(rsFinalTableDesc);\n+    info.setDynamicValueIDs(dynamicValueIDs);\n+    info.setColExprs(valueCols);\n+    List<ExprNodeDesc> targetTableExpressions = new ArrayList<>();\n+    for (ReduceSinkOperator sjBranch : sjBranches) {\n+      RuntimeValuesInfo sjInfo = parseContext.getRsToRuntimeValuesInfoMap().get(sjBranch);\n+      assert sjInfo.getTargetColumns().size() == 1;\n+      targetTableExpressions.add(sjInfo.getTargetColumns().get(0));\n+    }\n+    info.setTargetColumns(targetTableExpressions);\n+    return info;\n+  }\n+\n+  private static SelectOperator mergeSelectOps(Operator<?> parent, List<SelectOperator> selectOperators) {\n+    List<String> colNames = new ArrayList<>();\n+    List<ExprNodeDesc> colDescs = new ArrayList<>();\n+    List<ColumnInfo> columnInfos = new ArrayList<>();\n+    Map<String, ExprNodeDesc> selectColumnExprMap = new HashMap<>();\n+    for (SelectOperator sel : selectOperators) {\n+      for (ExprNodeDesc col : sel.getConf().getColList()) {\n+        String colName = HiveConf.getColumnInternalName(colDescs.size());\n+        colNames.add(colName);\n+        columnInfos.add(new ColumnInfo(colName, col.getTypeInfo(), \"\", false));\n+        colDescs.add(col);\n+        selectColumnExprMap.put(colName, col);\n+      }\n+    }\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", colDescs);\n+    String hashName = HiveConf.getColumnInternalName(colDescs.size() + 1);\n+    colNames.add(hashName);\n+    columnInfos.add(new ColumnInfo(hashName, hashExp.getTypeInfo(), \"\", false));\n+\n+    List<ExprNodeDesc> selDescs = new ArrayList<>(colDescs);\n+    selDescs.add(hashExp);\n+\n+    SelectDesc select = new SelectDesc(selDescs, colNames);\n+    SelectOperator selectOp =\n+        (SelectOperator) OperatorFactory.getAndMakeChild(select, new RowSchema(columnInfos), parent);\n+    selectOp.setColumnExprMap(selectColumnExprMap);\n+    return selectOp;\n+  }\n+\n+  private static ReduceSinkOperator createReduceSink(Operator<?> parentOp, NullOrdering nullOrder)\n+      throws SemanticException {\n+    List<ExprNodeDesc> valueCols = new ArrayList<>();\n+    RowSchema parentSchema = parentOp.getSchema();\n+    List<String> outColNames = new ArrayList<>();\n+    for (int i = 0; i < parentSchema.getSignature().size(); i++) {\n+      ColumnInfo colInfo = parentSchema.getSignature().get(i);\n+      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName(), \"\", false);\n+      valueCols.add(colExpr);\n+      outColNames.add(SemanticAnalyzer.getColumnInternalName(i));\n+    }\n+\n+    ReduceSinkDesc rsDesc = PlanUtils\n+        .getReduceSinkDesc(Collections.emptyList(), valueCols, outColNames, false, -1, 0, 1,\n+            AcidUtils.Operation.NOT_ACID, nullOrder);\n+    rsDesc.setColumnExprMap(Collections.emptyMap());\n+    return (ReduceSinkOperator) OperatorFactory.getAndMakeChild(rsDesc, new RowSchema(parentSchema), parentOp);\n+  }\n+\n+  private static GroupByOperator createGroupBy(SelectOperator selectOp, Operator<?> parentOp, GroupByDesc.Mode gbMode,\n+      HiveConf hiveConf) {\n+\n+    final List<ExprNodeDesc> params;\n+    final GenericUDAFEvaluator.Mode udafMode = SemanticAnalyzer.groupByDescModeToUDAFMode(gbMode, false);\n+    switch (gbMode) {\n+    case FINAL:\n+      params = createGroupByAggregationParameters((ReduceSinkOperator) parentOp);\n+      break;\n+    case HASH:\n+      params = createGroupByAggregationParameters(selectOp);\n+      break;\n+    default:\n+      throw new AssertionError(gbMode.toString() + \" is not supported\");\n+    }\n+\n+    List<AggregationDesc> gbAggs = new ArrayList<>();\n+    Deque<ExprNodeDesc> paramsCopy = new ArrayDeque<>(params);\n+    while (paramsCopy.size() > 1) {\n+      gbAggs.add(minAggregation(udafMode, paramsCopy.poll()));\n+      gbAggs.add(maxAggregation(udafMode, paramsCopy.poll()));\n+    }\n+    gbAggs.add(bloomFilterAggregation(udafMode, paramsCopy.poll(), selectOp, hiveConf));\n+    assert paramsCopy.size() == 0;\n+\n+    List<String> gbOutputNames = new ArrayList<>(gbAggs.size());\n+    List<ColumnInfo> gbColInfos = new ArrayList<>(gbAggs.size());\n+    for (int i = 0; i < params.size(); i++) {\n+      String colName = HiveConf.getColumnInternalName(i);\n+      gbOutputNames.add(colName);\n+      final TypeInfo colType;\n+      if (i == params.size() - 1) {\n+        colType = TypeInfoFactory.binaryTypeInfo; // Bloom type\n+      } else {\n+        colType = params.get(i).getTypeInfo(); // Min/Max type\n+      }\n+      gbColInfos.add(new ColumnInfo(colName, colType, \"\", false));\n+    }\n+\n+    float groupByMemoryUsage = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n+    float memoryThreshold = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n+    float minReductionHashAggr = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMINREDUCTION);\n+    GroupByDesc groupBy =\n+        new GroupByDesc(gbMode, gbOutputNames, Collections.emptyList(), gbAggs, false, groupByMemoryUsage,\n+            memoryThreshold, minReductionHashAggr, null, false, -1, false);\n+    groupBy.setColumnExprMap(Collections.emptyMap());\n+    return (GroupByOperator) OperatorFactory.getAndMakeChild(groupBy, new RowSchema(gbColInfos), parentOp);\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(SelectOperator selectOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // The first n-1 cols are used as parameters for min & max so we need two expressions\n+    for (ColumnInfo c : selectOp.getSchema().getSignature()) {\n+      String name = c.getInternalName();\n+      ExprNodeColumnDesc p = new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false));\n+      params.add(p);\n+      params.add(p);\n+    }\n+    // The last col is used as parameter for bloom so we need only one expression\n+    params.remove(params.size() - 1);\n+    return params;\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(ReduceSinkOperator reduceOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // There is a 1-1 mapping between columns and parameters for the aggregation functions min, max, bloom\n+    for (ColumnInfo c : reduceOp.getSchema().getSignature()) {\n+      String name = Utilities.ReduceField.VALUE + \".\" + c.getInternalName();\n+      params.add(new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false)));\n+    }\n+    return params;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc and(List<ExprNodeDesc> args) {\n+    return new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFOPAnd(), \"and\", args);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 338}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MTE2Mw==", "bodyText": "Objects.hash(source, target);", "url": "https://github.com/apache/hive/pull/1325#discussion_r464781163", "createdAt": "2020-08-04T03:44:23Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);\n+\n+      GroupByOperator gbPartialOp = createGroupBy(selectOp, selectOp, GroupByDesc.Mode.HASH, hiveConf);\n+\n+      ReduceSinkOperator rsPartialOp = createReduceSink(gbPartialOp, NullOrdering.defaultNullOrder(hiveConf));\n+      rsPartialOp.getConf().setReducerTraits(EnumSet.of(ReduceSinkDesc.ReducerTraits.QUICKSTART));\n+\n+      GroupByOperator gbCompleteOp = createGroupBy(selectOp, rsPartialOp, GroupByDesc.Mode.FINAL, hiveConf);\n+\n+      ReduceSinkOperator rsCompleteOp = createReduceSink(gbCompleteOp, NullOrdering.defaultNullOrder(hiveConf));\n+\n+      final TableScanOperator sjTargetTable = sjMergeCandidate.getKey().target;\n+      SemiJoinBranchInfo sjInfo = new SemiJoinBranchInfo(sjTargetTable, false);\n+      parseContext.getRsToSemiJoinBranchInfo().put(rsCompleteOp, sjInfo);\n+\n+      // Save the info that is required at query time to resolve dynamic/runtime values.\n+      RuntimeValuesInfo valuesInfo = createRuntimeValuesInfo(rsCompleteOp, sjBrances, parseContext);\n+      parseContext.getRsToRuntimeValuesInfoMap().put(rsCompleteOp, valuesInfo);\n+\n+      ExprNodeGenericFuncDesc sjPredicate = createSemiJoinPredicate(sjBrances, valuesInfo, parseContext);\n+\n+      // Update filter operators with the new semi-join predicate\n+      for (Operator<?> op : sjTargetTable.getChildOperators()) {\n+        if (op instanceof FilterOperator) {\n+          FilterDesc filter = ((FilterOperator) op).getConf();\n+          filter.setPredicate(and(Arrays.asList(filter.getPredicate(), sjPredicate)));\n+        }\n+      }\n+      // Update tableScan with the new semi-join predicate\n+      sjTargetTable.getConf().setFilterExpr(and(Arrays.asList(sjTargetTable.getConf().getFilterExpr(), sjPredicate)));\n+\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        GenTezUtils.removeSemiJoinOperator(parseContext, rs, sjTargetTable);\n+        GenTezUtils.removeBranch(rs);\n+      }\n+\n+      // TODO How to associate multi-cols with gb ?\n+      // parseContext.getColExprToGBMap().put(key, gb);\n+    }\n+    return parseContext;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc createSemiJoinPredicate(List<ReduceSinkOperator> sjBranches,\n+      RuntimeValuesInfo sjValueInfo, ParseContext context) {\n+    // Performance note: To speed-up evaluation 'BETWEEN' predicates should come before the 'IN_BLOOM_FILTER'\n+    Deque<String> dynamicIds = new ArrayDeque<>(sjValueInfo.getDynamicValueIDs());\n+    List<ExprNodeDesc> sjPredicates = new ArrayList<>();\n+    List<ExprNodeDesc> hashArgs = new ArrayList<>();\n+    for (ReduceSinkOperator rs : sjBranches) {\n+      RuntimeValuesInfo info = context.getRsToRuntimeValuesInfoMap().get(rs);\n+      assert info.getTargetColumns().size() == 1;\n+      final ExprNodeDesc targetColumn = info.getTargetColumns().get(0);\n+      TypeInfo typeInfo = targetColumn.getTypeInfo();\n+      DynamicValue minDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+      DynamicValue maxDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+\n+      List<ExprNodeDesc> betweenArgs = Arrays.asList(\n+          // Use false to not invert between result\n+          new ExprNodeConstantDesc(Boolean.FALSE),\n+          targetColumn,\n+          new ExprNodeDynamicValueDesc(minDynamic),\n+          new ExprNodeDynamicValueDesc(maxDynamic));\n+      ExprNodeDesc betweenExp =\n+          new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFBetween(), \"between\", betweenArgs);\n+      sjPredicates.add(betweenExp);\n+      hashArgs.add(targetColumn);\n+    }\n+\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", hashArgs);\n+\n+    assert dynamicIds.size() == 1 : \"There should be one column left untreated the one with the bloom filter\";\n+    DynamicValue bloomDynamic = new DynamicValue(dynamicIds.poll(), TypeInfoFactory.binaryTypeInfo);\n+    sjPredicates.add(\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFInBloomFilter(), \"in_bloom_filter\",\n+            Arrays.asList(hashExp, new ExprNodeDynamicValueDesc(bloomDynamic))));\n+    return and(sjPredicates);\n+  }\n+\n+  private static RuntimeValuesInfo createRuntimeValuesInfo(ReduceSinkOperator rs, List<ReduceSinkOperator> sjBranches,\n+      ParseContext parseContext) {\n+    List<ExprNodeDesc> valueCols = rs.getConf().getValueCols();\n+    RuntimeValuesInfo info = new RuntimeValuesInfo();\n+    TableDesc rsFinalTableDesc =\n+        PlanUtils.getReduceValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols, \"_col\"));\n+    List<String> dynamicValueIDs = new ArrayList<>();\n+    for (ExprNodeDesc rsCol : valueCols) {\n+      dynamicValueIDs.add(rs.toString() + rsCol.getExprString());\n+    }\n+\n+    info.setTableDesc(rsFinalTableDesc);\n+    info.setDynamicValueIDs(dynamicValueIDs);\n+    info.setColExprs(valueCols);\n+    List<ExprNodeDesc> targetTableExpressions = new ArrayList<>();\n+    for (ReduceSinkOperator sjBranch : sjBranches) {\n+      RuntimeValuesInfo sjInfo = parseContext.getRsToRuntimeValuesInfoMap().get(sjBranch);\n+      assert sjInfo.getTargetColumns().size() == 1;\n+      targetTableExpressions.add(sjInfo.getTargetColumns().get(0));\n+    }\n+    info.setTargetColumns(targetTableExpressions);\n+    return info;\n+  }\n+\n+  private static SelectOperator mergeSelectOps(Operator<?> parent, List<SelectOperator> selectOperators) {\n+    List<String> colNames = new ArrayList<>();\n+    List<ExprNodeDesc> colDescs = new ArrayList<>();\n+    List<ColumnInfo> columnInfos = new ArrayList<>();\n+    Map<String, ExprNodeDesc> selectColumnExprMap = new HashMap<>();\n+    for (SelectOperator sel : selectOperators) {\n+      for (ExprNodeDesc col : sel.getConf().getColList()) {\n+        String colName = HiveConf.getColumnInternalName(colDescs.size());\n+        colNames.add(colName);\n+        columnInfos.add(new ColumnInfo(colName, col.getTypeInfo(), \"\", false));\n+        colDescs.add(col);\n+        selectColumnExprMap.put(colName, col);\n+      }\n+    }\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", colDescs);\n+    String hashName = HiveConf.getColumnInternalName(colDescs.size() + 1);\n+    colNames.add(hashName);\n+    columnInfos.add(new ColumnInfo(hashName, hashExp.getTypeInfo(), \"\", false));\n+\n+    List<ExprNodeDesc> selDescs = new ArrayList<>(colDescs);\n+    selDescs.add(hashExp);\n+\n+    SelectDesc select = new SelectDesc(selDescs, colNames);\n+    SelectOperator selectOp =\n+        (SelectOperator) OperatorFactory.getAndMakeChild(select, new RowSchema(columnInfos), parent);\n+    selectOp.setColumnExprMap(selectColumnExprMap);\n+    return selectOp;\n+  }\n+\n+  private static ReduceSinkOperator createReduceSink(Operator<?> parentOp, NullOrdering nullOrder)\n+      throws SemanticException {\n+    List<ExprNodeDesc> valueCols = new ArrayList<>();\n+    RowSchema parentSchema = parentOp.getSchema();\n+    List<String> outColNames = new ArrayList<>();\n+    for (int i = 0; i < parentSchema.getSignature().size(); i++) {\n+      ColumnInfo colInfo = parentSchema.getSignature().get(i);\n+      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName(), \"\", false);\n+      valueCols.add(colExpr);\n+      outColNames.add(SemanticAnalyzer.getColumnInternalName(i));\n+    }\n+\n+    ReduceSinkDesc rsDesc = PlanUtils\n+        .getReduceSinkDesc(Collections.emptyList(), valueCols, outColNames, false, -1, 0, 1,\n+            AcidUtils.Operation.NOT_ACID, nullOrder);\n+    rsDesc.setColumnExprMap(Collections.emptyMap());\n+    return (ReduceSinkOperator) OperatorFactory.getAndMakeChild(rsDesc, new RowSchema(parentSchema), parentOp);\n+  }\n+\n+  private static GroupByOperator createGroupBy(SelectOperator selectOp, Operator<?> parentOp, GroupByDesc.Mode gbMode,\n+      HiveConf hiveConf) {\n+\n+    final List<ExprNodeDesc> params;\n+    final GenericUDAFEvaluator.Mode udafMode = SemanticAnalyzer.groupByDescModeToUDAFMode(gbMode, false);\n+    switch (gbMode) {\n+    case FINAL:\n+      params = createGroupByAggregationParameters((ReduceSinkOperator) parentOp);\n+      break;\n+    case HASH:\n+      params = createGroupByAggregationParameters(selectOp);\n+      break;\n+    default:\n+      throw new AssertionError(gbMode.toString() + \" is not supported\");\n+    }\n+\n+    List<AggregationDesc> gbAggs = new ArrayList<>();\n+    Deque<ExprNodeDesc> paramsCopy = new ArrayDeque<>(params);\n+    while (paramsCopy.size() > 1) {\n+      gbAggs.add(minAggregation(udafMode, paramsCopy.poll()));\n+      gbAggs.add(maxAggregation(udafMode, paramsCopy.poll()));\n+    }\n+    gbAggs.add(bloomFilterAggregation(udafMode, paramsCopy.poll(), selectOp, hiveConf));\n+    assert paramsCopy.size() == 0;\n+\n+    List<String> gbOutputNames = new ArrayList<>(gbAggs.size());\n+    List<ColumnInfo> gbColInfos = new ArrayList<>(gbAggs.size());\n+    for (int i = 0; i < params.size(); i++) {\n+      String colName = HiveConf.getColumnInternalName(i);\n+      gbOutputNames.add(colName);\n+      final TypeInfo colType;\n+      if (i == params.size() - 1) {\n+        colType = TypeInfoFactory.binaryTypeInfo; // Bloom type\n+      } else {\n+        colType = params.get(i).getTypeInfo(); // Min/Max type\n+      }\n+      gbColInfos.add(new ColumnInfo(colName, colType, \"\", false));\n+    }\n+\n+    float groupByMemoryUsage = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n+    float memoryThreshold = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n+    float minReductionHashAggr = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMINREDUCTION);\n+    GroupByDesc groupBy =\n+        new GroupByDesc(gbMode, gbOutputNames, Collections.emptyList(), gbAggs, false, groupByMemoryUsage,\n+            memoryThreshold, minReductionHashAggr, null, false, -1, false);\n+    groupBy.setColumnExprMap(Collections.emptyMap());\n+    return (GroupByOperator) OperatorFactory.getAndMakeChild(groupBy, new RowSchema(gbColInfos), parentOp);\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(SelectOperator selectOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // The first n-1 cols are used as parameters for min & max so we need two expressions\n+    for (ColumnInfo c : selectOp.getSchema().getSignature()) {\n+      String name = c.getInternalName();\n+      ExprNodeColumnDesc p = new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false));\n+      params.add(p);\n+      params.add(p);\n+    }\n+    // The last col is used as parameter for bloom so we need only one expression\n+    params.remove(params.size() - 1);\n+    return params;\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(ReduceSinkOperator reduceOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // There is a 1-1 mapping between columns and parameters for the aggregation functions min, max, bloom\n+    for (ColumnInfo c : reduceOp.getSchema().getSignature()) {\n+      String name = Utilities.ReduceField.VALUE + \".\" + c.getInternalName();\n+      params.add(new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false)));\n+    }\n+    return params;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc and(List<ExprNodeDesc> args) {\n+    return new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFOPAnd(), \"and\", args);\n+  }\n+\n+  private static AggregationDesc minAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col) {\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    return new AggregationDesc(\"min\", new GenericUDAFMin.GenericUDAFMinEvaluator(), p, false, mode);\n+  }\n+\n+  private static AggregationDesc maxAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col) {\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    return new AggregationDesc(\"max\", new GenericUDAFMax.GenericUDAFMaxEvaluator(), p, false, mode);\n+  }\n+\n+  private static AggregationDesc bloomFilterAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col,\n+      SelectOperator source, HiveConf conf) {\n+    GenericUDAFBloomFilter.GenericUDAFBloomFilterEvaluator bloomFilterEval =\n+        new GenericUDAFBloomFilter.GenericUDAFBloomFilterEvaluator();\n+    bloomFilterEval.setSourceOperator(source);\n+    bloomFilterEval.setMaxEntries(conf.getLongVar(HiveConf.ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n+    bloomFilterEval.setMinEntries(conf.getLongVar(HiveConf.ConfVars.TEZ_MIN_BLOOM_FILTER_ENTRIES));\n+    bloomFilterEval.setFactor(conf.getFloatVar(HiveConf.ConfVars.TEZ_BLOOM_FILTER_FACTOR));\n+    // TODO Setup hints\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    AggregationDesc bloom = new AggregationDesc(\"bloom_filter\", bloomFilterEval, p, false, mode);\n+    // TODO Why do we need to set it explicitly?\n+    bloom.setGenericUDAFWritableEvaluator(bloomFilterEval);\n+    return bloom;\n+  }\n+\n+  private static final class SJSourceTarget {\n+    private final Operator<?> source;\n+    private final TableScanOperator target;\n+\n+    public SJSourceTarget(Operator<?> source, TableScanOperator target) {\n+      this.source = source;\n+      this.target = target;\n+    }\n+\n+    @Override public boolean equals(Object o) {\n+      if (this == o)\n+        return true;\n+      if (o == null || getClass() != o.getClass())\n+        return false;\n+\n+      SJSourceTarget that = (SJSourceTarget) o;\n+\n+      if (!source.equals(that.source))\n+        return false;\n+      return target.equals(that.target);\n+    }\n+\n+    @Override public int hashCode() {\n+      int result = source.hashCode();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 390}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MTcwMw==", "bodyText": "TODO can be removed?", "url": "https://github.com/apache/hive/pull/1325#discussion_r464781703", "createdAt": "2020-08-04T03:46:42Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);\n+\n+      GroupByOperator gbPartialOp = createGroupBy(selectOp, selectOp, GroupByDesc.Mode.HASH, hiveConf);\n+\n+      ReduceSinkOperator rsPartialOp = createReduceSink(gbPartialOp, NullOrdering.defaultNullOrder(hiveConf));\n+      rsPartialOp.getConf().setReducerTraits(EnumSet.of(ReduceSinkDesc.ReducerTraits.QUICKSTART));\n+\n+      GroupByOperator gbCompleteOp = createGroupBy(selectOp, rsPartialOp, GroupByDesc.Mode.FINAL, hiveConf);\n+\n+      ReduceSinkOperator rsCompleteOp = createReduceSink(gbCompleteOp, NullOrdering.defaultNullOrder(hiveConf));\n+\n+      final TableScanOperator sjTargetTable = sjMergeCandidate.getKey().target;\n+      SemiJoinBranchInfo sjInfo = new SemiJoinBranchInfo(sjTargetTable, false);\n+      parseContext.getRsToSemiJoinBranchInfo().put(rsCompleteOp, sjInfo);\n+\n+      // Save the info that is required at query time to resolve dynamic/runtime values.\n+      RuntimeValuesInfo valuesInfo = createRuntimeValuesInfo(rsCompleteOp, sjBrances, parseContext);\n+      parseContext.getRsToRuntimeValuesInfoMap().put(rsCompleteOp, valuesInfo);\n+\n+      ExprNodeGenericFuncDesc sjPredicate = createSemiJoinPredicate(sjBrances, valuesInfo, parseContext);\n+\n+      // Update filter operators with the new semi-join predicate\n+      for (Operator<?> op : sjTargetTable.getChildOperators()) {\n+        if (op instanceof FilterOperator) {\n+          FilterDesc filter = ((FilterOperator) op).getConf();\n+          filter.setPredicate(and(Arrays.asList(filter.getPredicate(), sjPredicate)));\n+        }\n+      }\n+      // Update tableScan with the new semi-join predicate\n+      sjTargetTable.getConf().setFilterExpr(and(Arrays.asList(sjTargetTable.getConf().getFilterExpr(), sjPredicate)));\n+\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        GenTezUtils.removeSemiJoinOperator(parseContext, rs, sjTargetTable);\n+        GenTezUtils.removeBranch(rs);\n+      }\n+\n+      // TODO How to associate multi-cols with gb ?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4Mjk3Nw==", "bodyText": "To the question, I am not sure... I would assume it would be initialized correctly.", "url": "https://github.com/apache/hive/pull/1325#discussion_r464782977", "createdAt": "2020-08-04T03:52:01Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);\n+\n+      GroupByOperator gbPartialOp = createGroupBy(selectOp, selectOp, GroupByDesc.Mode.HASH, hiveConf);\n+\n+      ReduceSinkOperator rsPartialOp = createReduceSink(gbPartialOp, NullOrdering.defaultNullOrder(hiveConf));\n+      rsPartialOp.getConf().setReducerTraits(EnumSet.of(ReduceSinkDesc.ReducerTraits.QUICKSTART));\n+\n+      GroupByOperator gbCompleteOp = createGroupBy(selectOp, rsPartialOp, GroupByDesc.Mode.FINAL, hiveConf);\n+\n+      ReduceSinkOperator rsCompleteOp = createReduceSink(gbCompleteOp, NullOrdering.defaultNullOrder(hiveConf));\n+\n+      final TableScanOperator sjTargetTable = sjMergeCandidate.getKey().target;\n+      SemiJoinBranchInfo sjInfo = new SemiJoinBranchInfo(sjTargetTable, false);\n+      parseContext.getRsToSemiJoinBranchInfo().put(rsCompleteOp, sjInfo);\n+\n+      // Save the info that is required at query time to resolve dynamic/runtime values.\n+      RuntimeValuesInfo valuesInfo = createRuntimeValuesInfo(rsCompleteOp, sjBrances, parseContext);\n+      parseContext.getRsToRuntimeValuesInfoMap().put(rsCompleteOp, valuesInfo);\n+\n+      ExprNodeGenericFuncDesc sjPredicate = createSemiJoinPredicate(sjBrances, valuesInfo, parseContext);\n+\n+      // Update filter operators with the new semi-join predicate\n+      for (Operator<?> op : sjTargetTable.getChildOperators()) {\n+        if (op instanceof FilterOperator) {\n+          FilterDesc filter = ((FilterOperator) op).getConf();\n+          filter.setPredicate(and(Arrays.asList(filter.getPredicate(), sjPredicate)));\n+        }\n+      }\n+      // Update tableScan with the new semi-join predicate\n+      sjTargetTable.getConf().setFilterExpr(and(Arrays.asList(sjTargetTable.getConf().getFilterExpr(), sjPredicate)));\n+\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        GenTezUtils.removeSemiJoinOperator(parseContext, rs, sjTargetTable);\n+        GenTezUtils.removeBranch(rs);\n+      }\n+\n+      // TODO How to associate multi-cols with gb ?\n+      // parseContext.getColExprToGBMap().put(key, gb);\n+    }\n+    return parseContext;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc createSemiJoinPredicate(List<ReduceSinkOperator> sjBranches,\n+      RuntimeValuesInfo sjValueInfo, ParseContext context) {\n+    // Performance note: To speed-up evaluation 'BETWEEN' predicates should come before the 'IN_BLOOM_FILTER'\n+    Deque<String> dynamicIds = new ArrayDeque<>(sjValueInfo.getDynamicValueIDs());\n+    List<ExprNodeDesc> sjPredicates = new ArrayList<>();\n+    List<ExprNodeDesc> hashArgs = new ArrayList<>();\n+    for (ReduceSinkOperator rs : sjBranches) {\n+      RuntimeValuesInfo info = context.getRsToRuntimeValuesInfoMap().get(rs);\n+      assert info.getTargetColumns().size() == 1;\n+      final ExprNodeDesc targetColumn = info.getTargetColumns().get(0);\n+      TypeInfo typeInfo = targetColumn.getTypeInfo();\n+      DynamicValue minDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+      DynamicValue maxDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+\n+      List<ExprNodeDesc> betweenArgs = Arrays.asList(\n+          // Use false to not invert between result\n+          new ExprNodeConstantDesc(Boolean.FALSE),\n+          targetColumn,\n+          new ExprNodeDynamicValueDesc(minDynamic),\n+          new ExprNodeDynamicValueDesc(maxDynamic));\n+      ExprNodeDesc betweenExp =\n+          new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFBetween(), \"between\", betweenArgs);\n+      sjPredicates.add(betweenExp);\n+      hashArgs.add(targetColumn);\n+    }\n+\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", hashArgs);\n+\n+    assert dynamicIds.size() == 1 : \"There should be one column left untreated the one with the bloom filter\";\n+    DynamicValue bloomDynamic = new DynamicValue(dynamicIds.poll(), TypeInfoFactory.binaryTypeInfo);\n+    sjPredicates.add(\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFInBloomFilter(), \"in_bloom_filter\",\n+            Arrays.asList(hashExp, new ExprNodeDynamicValueDesc(bloomDynamic))));\n+    return and(sjPredicates);\n+  }\n+\n+  private static RuntimeValuesInfo createRuntimeValuesInfo(ReduceSinkOperator rs, List<ReduceSinkOperator> sjBranches,\n+      ParseContext parseContext) {\n+    List<ExprNodeDesc> valueCols = rs.getConf().getValueCols();\n+    RuntimeValuesInfo info = new RuntimeValuesInfo();\n+    TableDesc rsFinalTableDesc =\n+        PlanUtils.getReduceValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols, \"_col\"));\n+    List<String> dynamicValueIDs = new ArrayList<>();\n+    for (ExprNodeDesc rsCol : valueCols) {\n+      dynamicValueIDs.add(rs.toString() + rsCol.getExprString());\n+    }\n+\n+    info.setTableDesc(rsFinalTableDesc);\n+    info.setDynamicValueIDs(dynamicValueIDs);\n+    info.setColExprs(valueCols);\n+    List<ExprNodeDesc> targetTableExpressions = new ArrayList<>();\n+    for (ReduceSinkOperator sjBranch : sjBranches) {\n+      RuntimeValuesInfo sjInfo = parseContext.getRsToRuntimeValuesInfoMap().get(sjBranch);\n+      assert sjInfo.getTargetColumns().size() == 1;\n+      targetTableExpressions.add(sjInfo.getTargetColumns().get(0));\n+    }\n+    info.setTargetColumns(targetTableExpressions);\n+    return info;\n+  }\n+\n+  private static SelectOperator mergeSelectOps(Operator<?> parent, List<SelectOperator> selectOperators) {\n+    List<String> colNames = new ArrayList<>();\n+    List<ExprNodeDesc> colDescs = new ArrayList<>();\n+    List<ColumnInfo> columnInfos = new ArrayList<>();\n+    Map<String, ExprNodeDesc> selectColumnExprMap = new HashMap<>();\n+    for (SelectOperator sel : selectOperators) {\n+      for (ExprNodeDesc col : sel.getConf().getColList()) {\n+        String colName = HiveConf.getColumnInternalName(colDescs.size());\n+        colNames.add(colName);\n+        columnInfos.add(new ColumnInfo(colName, col.getTypeInfo(), \"\", false));\n+        colDescs.add(col);\n+        selectColumnExprMap.put(colName, col);\n+      }\n+    }\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", colDescs);\n+    String hashName = HiveConf.getColumnInternalName(colDescs.size() + 1);\n+    colNames.add(hashName);\n+    columnInfos.add(new ColumnInfo(hashName, hashExp.getTypeInfo(), \"\", false));\n+\n+    List<ExprNodeDesc> selDescs = new ArrayList<>(colDescs);\n+    selDescs.add(hashExp);\n+\n+    SelectDesc select = new SelectDesc(selDescs, colNames);\n+    SelectOperator selectOp =\n+        (SelectOperator) OperatorFactory.getAndMakeChild(select, new RowSchema(columnInfos), parent);\n+    selectOp.setColumnExprMap(selectColumnExprMap);\n+    return selectOp;\n+  }\n+\n+  private static ReduceSinkOperator createReduceSink(Operator<?> parentOp, NullOrdering nullOrder)\n+      throws SemanticException {\n+    List<ExprNodeDesc> valueCols = new ArrayList<>();\n+    RowSchema parentSchema = parentOp.getSchema();\n+    List<String> outColNames = new ArrayList<>();\n+    for (int i = 0; i < parentSchema.getSignature().size(); i++) {\n+      ColumnInfo colInfo = parentSchema.getSignature().get(i);\n+      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName(), \"\", false);\n+      valueCols.add(colExpr);\n+      outColNames.add(SemanticAnalyzer.getColumnInternalName(i));\n+    }\n+\n+    ReduceSinkDesc rsDesc = PlanUtils\n+        .getReduceSinkDesc(Collections.emptyList(), valueCols, outColNames, false, -1, 0, 1,\n+            AcidUtils.Operation.NOT_ACID, nullOrder);\n+    rsDesc.setColumnExprMap(Collections.emptyMap());\n+    return (ReduceSinkOperator) OperatorFactory.getAndMakeChild(rsDesc, new RowSchema(parentSchema), parentOp);\n+  }\n+\n+  private static GroupByOperator createGroupBy(SelectOperator selectOp, Operator<?> parentOp, GroupByDesc.Mode gbMode,\n+      HiveConf hiveConf) {\n+\n+    final List<ExprNodeDesc> params;\n+    final GenericUDAFEvaluator.Mode udafMode = SemanticAnalyzer.groupByDescModeToUDAFMode(gbMode, false);\n+    switch (gbMode) {\n+    case FINAL:\n+      params = createGroupByAggregationParameters((ReduceSinkOperator) parentOp);\n+      break;\n+    case HASH:\n+      params = createGroupByAggregationParameters(selectOp);\n+      break;\n+    default:\n+      throw new AssertionError(gbMode.toString() + \" is not supported\");\n+    }\n+\n+    List<AggregationDesc> gbAggs = new ArrayList<>();\n+    Deque<ExprNodeDesc> paramsCopy = new ArrayDeque<>(params);\n+    while (paramsCopy.size() > 1) {\n+      gbAggs.add(minAggregation(udafMode, paramsCopy.poll()));\n+      gbAggs.add(maxAggregation(udafMode, paramsCopy.poll()));\n+    }\n+    gbAggs.add(bloomFilterAggregation(udafMode, paramsCopy.poll(), selectOp, hiveConf));\n+    assert paramsCopy.size() == 0;\n+\n+    List<String> gbOutputNames = new ArrayList<>(gbAggs.size());\n+    List<ColumnInfo> gbColInfos = new ArrayList<>(gbAggs.size());\n+    for (int i = 0; i < params.size(); i++) {\n+      String colName = HiveConf.getColumnInternalName(i);\n+      gbOutputNames.add(colName);\n+      final TypeInfo colType;\n+      if (i == params.size() - 1) {\n+        colType = TypeInfoFactory.binaryTypeInfo; // Bloom type\n+      } else {\n+        colType = params.get(i).getTypeInfo(); // Min/Max type\n+      }\n+      gbColInfos.add(new ColumnInfo(colName, colType, \"\", false));\n+    }\n+\n+    float groupByMemoryUsage = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n+    float memoryThreshold = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n+    float minReductionHashAggr = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMINREDUCTION);\n+    GroupByDesc groupBy =\n+        new GroupByDesc(gbMode, gbOutputNames, Collections.emptyList(), gbAggs, false, groupByMemoryUsage,\n+            memoryThreshold, minReductionHashAggr, null, false, -1, false);\n+    groupBy.setColumnExprMap(Collections.emptyMap());\n+    return (GroupByOperator) OperatorFactory.getAndMakeChild(groupBy, new RowSchema(gbColInfos), parentOp);\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(SelectOperator selectOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // The first n-1 cols are used as parameters for min & max so we need two expressions\n+    for (ColumnInfo c : selectOp.getSchema().getSignature()) {\n+      String name = c.getInternalName();\n+      ExprNodeColumnDesc p = new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false));\n+      params.add(p);\n+      params.add(p);\n+    }\n+    // The last col is used as parameter for bloom so we need only one expression\n+    params.remove(params.size() - 1);\n+    return params;\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(ReduceSinkOperator reduceOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // There is a 1-1 mapping between columns and parameters for the aggregation functions min, max, bloom\n+    for (ColumnInfo c : reduceOp.getSchema().getSignature()) {\n+      String name = Utilities.ReduceField.VALUE + \".\" + c.getInternalName();\n+      params.add(new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false)));\n+    }\n+    return params;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc and(List<ExprNodeDesc> args) {\n+    return new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFOPAnd(), \"and\", args);\n+  }\n+\n+  private static AggregationDesc minAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col) {\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    return new AggregationDesc(\"min\", new GenericUDAFMin.GenericUDAFMinEvaluator(), p, false, mode);\n+  }\n+\n+  private static AggregationDesc maxAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col) {\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    return new AggregationDesc(\"max\", new GenericUDAFMax.GenericUDAFMaxEvaluator(), p, false, mode);\n+  }\n+\n+  private static AggregationDesc bloomFilterAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col,\n+      SelectOperator source, HiveConf conf) {\n+    GenericUDAFBloomFilter.GenericUDAFBloomFilterEvaluator bloomFilterEval =\n+        new GenericUDAFBloomFilter.GenericUDAFBloomFilterEvaluator();\n+    bloomFilterEval.setSourceOperator(source);\n+    bloomFilterEval.setMaxEntries(conf.getLongVar(HiveConf.ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n+    bloomFilterEval.setMinEntries(conf.getLongVar(HiveConf.ConfVars.TEZ_MIN_BLOOM_FILTER_ENTRIES));\n+    bloomFilterEval.setFactor(conf.getFloatVar(HiveConf.ConfVars.TEZ_BLOOM_FILTER_FACTOR));\n+    // TODO Setup hints\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    AggregationDesc bloom = new AggregationDesc(\"bloom_filter\", bloomFilterEval, p, false, mode);\n+    // TODO Why do we need to set it explicitly?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 362}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MzE3OQ==", "bodyText": "What's the current behavior in the presence of hints? It seems we do not propagate them... Should we create a follow-up JIRA to keep track?", "url": "https://github.com/apache/hive/pull/1325#discussion_r464783179", "createdAt": "2020-08-04T03:53:04Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/SemiJoinReductionMerge.java", "diffHunk": "@@ -0,0 +1,399 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.ColumnInfo;\n+import org.apache.hadoop.hive.ql.exec.FilterOperator;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n+import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n+import org.apache.hadoop.hive.ql.exec.RowSchema;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.apache.hadoop.hive.ql.io.AcidUtils;\n+import org.apache.hadoop.hive.ql.parse.GenTezUtils;\n+import org.apache.hadoop.hive.ql.parse.ParseContext;\n+import org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo;\n+import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.parse.SemiJoinBranchInfo;\n+import org.apache.hadoop.hive.ql.plan.AggregationDesc;\n+import org.apache.hadoop.hive.ql.plan.DynamicValue;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDynamicValueDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc;\n+import org.apache.hadoop.hive.ql.plan.FilterDesc;\n+import org.apache.hadoop.hive.ql.plan.GroupByDesc;\n+import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.SelectDesc;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBetween;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFInBloomFilter;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFMurmurHash;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;\n+import org.apache.hadoop.hive.ql.util.NullOrdering;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+\n+public class SemiJoinReductionMerge extends Transform {\n+\n+  public ParseContext transform(ParseContext parseContext) throws SemanticException {\n+    Map<ReduceSinkOperator, SemiJoinBranchInfo> map = parseContext.getRsToSemiJoinBranchInfo();\n+    if (map.isEmpty()) {\n+      return parseContext;\n+    }\n+    HiveConf hiveConf = parseContext.getConf();\n+\n+    // Order does not really matter but it is necessary to keep plans stable\n+    SortedMap<SJSourceTarget, List<ReduceSinkOperator>> sameTableSJ =\n+        new TreeMap<>(Comparator.comparing(SJSourceTarget::toString));\n+    for (Map.Entry<ReduceSinkOperator, SemiJoinBranchInfo> smjEntry : map.entrySet()) {\n+      TableScanOperator ts = smjEntry.getValue().getTsOp();\n+      // Semijoin optimization branch should look like <Parent>-SEL-GB1-RS1-GB2-RS2\n+      SelectOperator selOp = OperatorUtils.ancestor(smjEntry.getKey(), SelectOperator.class, 0, 0, 0, 0);\n+      assert selOp != null;\n+      assert selOp.getParentOperators().size() == 1;\n+      Operator<?> source = selOp.getParentOperators().get(0);\n+      SJSourceTarget sjKey = new SJSourceTarget(source, ts);\n+      List<ReduceSinkOperator> ops = sameTableSJ.computeIfAbsent(sjKey, tableScanOperator -> new ArrayList<>());\n+      ops.add(smjEntry.getKey());\n+    }\n+    for (Map.Entry<SJSourceTarget, List<ReduceSinkOperator>> sjMergeCandidate : sameTableSJ.entrySet()) {\n+      final List<ReduceSinkOperator> sjBrances = sjMergeCandidate.getValue();\n+      if (sjBrances.size() < 2) {\n+        continue;\n+      }\n+      // Order does not really matter but it is necessary to keep plans stable\n+      sjBrances.sort(Comparator.comparing(Operator::getIdentifier));\n+\n+      List<SelectOperator> selOps = new ArrayList<>(sjBrances.size());\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        selOps.add(OperatorUtils.ancestor(rs, SelectOperator.class, 0, 0, 0, 0));\n+      }\n+      SelectOperator selectOp = mergeSelectOps(sjMergeCandidate.getKey().source, selOps);\n+\n+      GroupByOperator gbPartialOp = createGroupBy(selectOp, selectOp, GroupByDesc.Mode.HASH, hiveConf);\n+\n+      ReduceSinkOperator rsPartialOp = createReduceSink(gbPartialOp, NullOrdering.defaultNullOrder(hiveConf));\n+      rsPartialOp.getConf().setReducerTraits(EnumSet.of(ReduceSinkDesc.ReducerTraits.QUICKSTART));\n+\n+      GroupByOperator gbCompleteOp = createGroupBy(selectOp, rsPartialOp, GroupByDesc.Mode.FINAL, hiveConf);\n+\n+      ReduceSinkOperator rsCompleteOp = createReduceSink(gbCompleteOp, NullOrdering.defaultNullOrder(hiveConf));\n+\n+      final TableScanOperator sjTargetTable = sjMergeCandidate.getKey().target;\n+      SemiJoinBranchInfo sjInfo = new SemiJoinBranchInfo(sjTargetTable, false);\n+      parseContext.getRsToSemiJoinBranchInfo().put(rsCompleteOp, sjInfo);\n+\n+      // Save the info that is required at query time to resolve dynamic/runtime values.\n+      RuntimeValuesInfo valuesInfo = createRuntimeValuesInfo(rsCompleteOp, sjBrances, parseContext);\n+      parseContext.getRsToRuntimeValuesInfoMap().put(rsCompleteOp, valuesInfo);\n+\n+      ExprNodeGenericFuncDesc sjPredicate = createSemiJoinPredicate(sjBrances, valuesInfo, parseContext);\n+\n+      // Update filter operators with the new semi-join predicate\n+      for (Operator<?> op : sjTargetTable.getChildOperators()) {\n+        if (op instanceof FilterOperator) {\n+          FilterDesc filter = ((FilterOperator) op).getConf();\n+          filter.setPredicate(and(Arrays.asList(filter.getPredicate(), sjPredicate)));\n+        }\n+      }\n+      // Update tableScan with the new semi-join predicate\n+      sjTargetTable.getConf().setFilterExpr(and(Arrays.asList(sjTargetTable.getConf().getFilterExpr(), sjPredicate)));\n+\n+      for (ReduceSinkOperator rs : sjBrances) {\n+        GenTezUtils.removeSemiJoinOperator(parseContext, rs, sjTargetTable);\n+        GenTezUtils.removeBranch(rs);\n+      }\n+\n+      // TODO How to associate multi-cols with gb ?\n+      // parseContext.getColExprToGBMap().put(key, gb);\n+    }\n+    return parseContext;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc createSemiJoinPredicate(List<ReduceSinkOperator> sjBranches,\n+      RuntimeValuesInfo sjValueInfo, ParseContext context) {\n+    // Performance note: To speed-up evaluation 'BETWEEN' predicates should come before the 'IN_BLOOM_FILTER'\n+    Deque<String> dynamicIds = new ArrayDeque<>(sjValueInfo.getDynamicValueIDs());\n+    List<ExprNodeDesc> sjPredicates = new ArrayList<>();\n+    List<ExprNodeDesc> hashArgs = new ArrayList<>();\n+    for (ReduceSinkOperator rs : sjBranches) {\n+      RuntimeValuesInfo info = context.getRsToRuntimeValuesInfoMap().get(rs);\n+      assert info.getTargetColumns().size() == 1;\n+      final ExprNodeDesc targetColumn = info.getTargetColumns().get(0);\n+      TypeInfo typeInfo = targetColumn.getTypeInfo();\n+      DynamicValue minDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+      DynamicValue maxDynamic = new DynamicValue(dynamicIds.poll(), typeInfo);\n+\n+      List<ExprNodeDesc> betweenArgs = Arrays.asList(\n+          // Use false to not invert between result\n+          new ExprNodeConstantDesc(Boolean.FALSE),\n+          targetColumn,\n+          new ExprNodeDynamicValueDesc(minDynamic),\n+          new ExprNodeDynamicValueDesc(maxDynamic));\n+      ExprNodeDesc betweenExp =\n+          new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFBetween(), \"between\", betweenArgs);\n+      sjPredicates.add(betweenExp);\n+      hashArgs.add(targetColumn);\n+    }\n+\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", hashArgs);\n+\n+    assert dynamicIds.size() == 1 : \"There should be one column left untreated the one with the bloom filter\";\n+    DynamicValue bloomDynamic = new DynamicValue(dynamicIds.poll(), TypeInfoFactory.binaryTypeInfo);\n+    sjPredicates.add(\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFInBloomFilter(), \"in_bloom_filter\",\n+            Arrays.asList(hashExp, new ExprNodeDynamicValueDesc(bloomDynamic))));\n+    return and(sjPredicates);\n+  }\n+\n+  private static RuntimeValuesInfo createRuntimeValuesInfo(ReduceSinkOperator rs, List<ReduceSinkOperator> sjBranches,\n+      ParseContext parseContext) {\n+    List<ExprNodeDesc> valueCols = rs.getConf().getValueCols();\n+    RuntimeValuesInfo info = new RuntimeValuesInfo();\n+    TableDesc rsFinalTableDesc =\n+        PlanUtils.getReduceValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols, \"_col\"));\n+    List<String> dynamicValueIDs = new ArrayList<>();\n+    for (ExprNodeDesc rsCol : valueCols) {\n+      dynamicValueIDs.add(rs.toString() + rsCol.getExprString());\n+    }\n+\n+    info.setTableDesc(rsFinalTableDesc);\n+    info.setDynamicValueIDs(dynamicValueIDs);\n+    info.setColExprs(valueCols);\n+    List<ExprNodeDesc> targetTableExpressions = new ArrayList<>();\n+    for (ReduceSinkOperator sjBranch : sjBranches) {\n+      RuntimeValuesInfo sjInfo = parseContext.getRsToRuntimeValuesInfoMap().get(sjBranch);\n+      assert sjInfo.getTargetColumns().size() == 1;\n+      targetTableExpressions.add(sjInfo.getTargetColumns().get(0));\n+    }\n+    info.setTargetColumns(targetTableExpressions);\n+    return info;\n+  }\n+\n+  private static SelectOperator mergeSelectOps(Operator<?> parent, List<SelectOperator> selectOperators) {\n+    List<String> colNames = new ArrayList<>();\n+    List<ExprNodeDesc> colDescs = new ArrayList<>();\n+    List<ColumnInfo> columnInfos = new ArrayList<>();\n+    Map<String, ExprNodeDesc> selectColumnExprMap = new HashMap<>();\n+    for (SelectOperator sel : selectOperators) {\n+      for (ExprNodeDesc col : sel.getConf().getColList()) {\n+        String colName = HiveConf.getColumnInternalName(colDescs.size());\n+        colNames.add(colName);\n+        columnInfos.add(new ColumnInfo(colName, col.getTypeInfo(), \"\", false));\n+        colDescs.add(col);\n+        selectColumnExprMap.put(colName, col);\n+      }\n+    }\n+    ExprNodeDesc hashExp =\n+        new ExprNodeGenericFuncDesc(TypeInfoFactory.intTypeInfo, new GenericUDFMurmurHash(), \"hash\", colDescs);\n+    String hashName = HiveConf.getColumnInternalName(colDescs.size() + 1);\n+    colNames.add(hashName);\n+    columnInfos.add(new ColumnInfo(hashName, hashExp.getTypeInfo(), \"\", false));\n+\n+    List<ExprNodeDesc> selDescs = new ArrayList<>(colDescs);\n+    selDescs.add(hashExp);\n+\n+    SelectDesc select = new SelectDesc(selDescs, colNames);\n+    SelectOperator selectOp =\n+        (SelectOperator) OperatorFactory.getAndMakeChild(select, new RowSchema(columnInfos), parent);\n+    selectOp.setColumnExprMap(selectColumnExprMap);\n+    return selectOp;\n+  }\n+\n+  private static ReduceSinkOperator createReduceSink(Operator<?> parentOp, NullOrdering nullOrder)\n+      throws SemanticException {\n+    List<ExprNodeDesc> valueCols = new ArrayList<>();\n+    RowSchema parentSchema = parentOp.getSchema();\n+    List<String> outColNames = new ArrayList<>();\n+    for (int i = 0; i < parentSchema.getSignature().size(); i++) {\n+      ColumnInfo colInfo = parentSchema.getSignature().get(i);\n+      ExprNodeColumnDesc colExpr = new ExprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName(), \"\", false);\n+      valueCols.add(colExpr);\n+      outColNames.add(SemanticAnalyzer.getColumnInternalName(i));\n+    }\n+\n+    ReduceSinkDesc rsDesc = PlanUtils\n+        .getReduceSinkDesc(Collections.emptyList(), valueCols, outColNames, false, -1, 0, 1,\n+            AcidUtils.Operation.NOT_ACID, nullOrder);\n+    rsDesc.setColumnExprMap(Collections.emptyMap());\n+    return (ReduceSinkOperator) OperatorFactory.getAndMakeChild(rsDesc, new RowSchema(parentSchema), parentOp);\n+  }\n+\n+  private static GroupByOperator createGroupBy(SelectOperator selectOp, Operator<?> parentOp, GroupByDesc.Mode gbMode,\n+      HiveConf hiveConf) {\n+\n+    final List<ExprNodeDesc> params;\n+    final GenericUDAFEvaluator.Mode udafMode = SemanticAnalyzer.groupByDescModeToUDAFMode(gbMode, false);\n+    switch (gbMode) {\n+    case FINAL:\n+      params = createGroupByAggregationParameters((ReduceSinkOperator) parentOp);\n+      break;\n+    case HASH:\n+      params = createGroupByAggregationParameters(selectOp);\n+      break;\n+    default:\n+      throw new AssertionError(gbMode.toString() + \" is not supported\");\n+    }\n+\n+    List<AggregationDesc> gbAggs = new ArrayList<>();\n+    Deque<ExprNodeDesc> paramsCopy = new ArrayDeque<>(params);\n+    while (paramsCopy.size() > 1) {\n+      gbAggs.add(minAggregation(udafMode, paramsCopy.poll()));\n+      gbAggs.add(maxAggregation(udafMode, paramsCopy.poll()));\n+    }\n+    gbAggs.add(bloomFilterAggregation(udafMode, paramsCopy.poll(), selectOp, hiveConf));\n+    assert paramsCopy.size() == 0;\n+\n+    List<String> gbOutputNames = new ArrayList<>(gbAggs.size());\n+    List<ColumnInfo> gbColInfos = new ArrayList<>(gbAggs.size());\n+    for (int i = 0; i < params.size(); i++) {\n+      String colName = HiveConf.getColumnInternalName(i);\n+      gbOutputNames.add(colName);\n+      final TypeInfo colType;\n+      if (i == params.size() - 1) {\n+        colType = TypeInfoFactory.binaryTypeInfo; // Bloom type\n+      } else {\n+        colType = params.get(i).getTypeInfo(); // Min/Max type\n+      }\n+      gbColInfos.add(new ColumnInfo(colName, colType, \"\", false));\n+    }\n+\n+    float groupByMemoryUsage = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMEMORY);\n+    float memoryThreshold = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRMEMORYTHRESHOLD);\n+    float minReductionHashAggr = HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVEMAPAGGRHASHMINREDUCTION);\n+    GroupByDesc groupBy =\n+        new GroupByDesc(gbMode, gbOutputNames, Collections.emptyList(), gbAggs, false, groupByMemoryUsage,\n+            memoryThreshold, minReductionHashAggr, null, false, -1, false);\n+    groupBy.setColumnExprMap(Collections.emptyMap());\n+    return (GroupByOperator) OperatorFactory.getAndMakeChild(groupBy, new RowSchema(gbColInfos), parentOp);\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(SelectOperator selectOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // The first n-1 cols are used as parameters for min & max so we need two expressions\n+    for (ColumnInfo c : selectOp.getSchema().getSignature()) {\n+      String name = c.getInternalName();\n+      ExprNodeColumnDesc p = new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false));\n+      params.add(p);\n+      params.add(p);\n+    }\n+    // The last col is used as parameter for bloom so we need only one expression\n+    params.remove(params.size() - 1);\n+    return params;\n+  }\n+\n+  private static List<ExprNodeDesc> createGroupByAggregationParameters(ReduceSinkOperator reduceOp) {\n+    List<ExprNodeDesc> params = new ArrayList<>();\n+    // There is a 1-1 mapping between columns and parameters for the aggregation functions min, max, bloom\n+    for (ColumnInfo c : reduceOp.getSchema().getSignature()) {\n+      String name = Utilities.ReduceField.VALUE + \".\" + c.getInternalName();\n+      params.add(new ExprNodeColumnDesc(new ColumnInfo(name, c.getType(), \"\", false)));\n+    }\n+    return params;\n+  }\n+\n+  private static ExprNodeGenericFuncDesc and(List<ExprNodeDesc> args) {\n+    return new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo, new GenericUDFOPAnd(), \"and\", args);\n+  }\n+\n+  private static AggregationDesc minAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col) {\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    return new AggregationDesc(\"min\", new GenericUDAFMin.GenericUDAFMinEvaluator(), p, false, mode);\n+  }\n+\n+  private static AggregationDesc maxAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col) {\n+    List<ExprNodeDesc> p = Collections.singletonList(col);\n+    return new AggregationDesc(\"max\", new GenericUDAFMax.GenericUDAFMaxEvaluator(), p, false, mode);\n+  }\n+\n+  private static AggregationDesc bloomFilterAggregation(GenericUDAFEvaluator.Mode mode, ExprNodeDesc col,\n+      SelectOperator source, HiveConf conf) {\n+    GenericUDAFBloomFilter.GenericUDAFBloomFilterEvaluator bloomFilterEval =\n+        new GenericUDAFBloomFilter.GenericUDAFBloomFilterEvaluator();\n+    bloomFilterEval.setSourceOperator(source);\n+    bloomFilterEval.setMaxEntries(conf.getLongVar(HiveConf.ConfVars.TEZ_MAX_BLOOM_FILTER_ENTRIES));\n+    bloomFilterEval.setMinEntries(conf.getLongVar(HiveConf.ConfVars.TEZ_MIN_BLOOM_FILTER_ENTRIES));\n+    bloomFilterEval.setFactor(conf.getFloatVar(HiveConf.ConfVars.TEZ_BLOOM_FILTER_FACTOR));\n+    // TODO Setup hints", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 359}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4NzA4OQ==", "bodyText": "Shouldn't this be the min (selectivity initialized to 1)? The existing code was overly complex with the dichotomy between selectivity and benefit. However, I think we are trying to get the lowest selectivity so we can obtain the highest benefit? Or am I getting confused?", "url": "https://github.com/apache/hive/pull/1325#discussion_r464787089", "createdAt": "2020-08-04T04:09:00Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java", "diffHunk": "@@ -1737,35 +1737,46 @@ private static double getBloomFilterBenefit(\n       }\n     }\n \n-    // Selectivity: key cardinality of semijoin / domain cardinality\n-    // Benefit (rows filtered from ts): (1 - selectivity) * # ts rows\n-    double selectivity = selKeyCardinality / (double) keyDomainCardinality;\n-    selectivity = Math.min(selectivity, 1);\n-    benefit = tsRows * (1 - selectivity);\n-\n     if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"BloomFilter benefit for \" + selCol + \" to \" + tsCol\n-          + \", selKeyCardinality=\" + selKeyCardinality\n-          + \", tsKeyCardinality=\" + tsKeyCardinality\n-          + \", tsRows=\" + tsRows\n-          + \", keyDomainCardinality=\" + keyDomainCardinality);\n-      LOG.debug(\"SemiJoin key selectivity=\" + selectivity\n-          + \", benefit=\" + benefit);\n+      LOG.debug(\"BloomFilter selectivity for \" + selCol + \" to \" + tsCol + \", selKeyCardinality=\" + selKeyCardinality\n+          + \", tsKeyCardinality=\" + tsKeyCardinality + \", keyDomainCardinality=\" + keyDomainCardinality);\n     }\n+    // Selectivity: key cardinality of semijoin / domain cardinality\n+    return selKeyCardinality / (double) keyDomainCardinality;\n+  }\n \n-    return benefit;\n+  private static double getBloomFilterBenefit(\n+      SelectOperator sel, List<ExprNodeDesc> selExpr,\n+      Statistics filStats, List<ExprNodeDesc> tsExpr) {\n+    if (sel.getStatistics() == null || filStats == null) {\n+      LOG.debug(\"No stats available to compute BloomFilter benefit\");\n+      return -1;\n+    }\n+    double selectivity = 0.0;\n+    for (int i = 0; i < tsExpr.size(); i++) {\n+      selectivity = Math.max(selectivity, getBloomFilterSelectivity(sel, selExpr.get(i), filStats, tsExpr.get(i)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4OTA3MQ==", "bodyText": "TODO?", "url": "https://github.com/apache/hive/pull/1325#discussion_r464789071", "createdAt": "2020-08-04T04:16:35Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java", "diffHunk": "@@ -2054,7 +2067,8 @@ private void markSemiJoinForDPP(OptimizeTezProcContext procCtx)\n               // Lookup nDVs on TS side.\n               RuntimeValuesInfo rti = procCtx.parseContext\n                       .getRsToRuntimeValuesInfoMap().get(rs);\n-              ExprNodeDesc tsExpr = rti.getTsColExpr();\n+              // TODO Adapt for multi column semi-joins.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 256}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4OTI1Mg==", "bodyText": "This does not seem to distinguish between single column and multi column. Could we clarify in the comment?", "url": "https://github.com/apache/hive/pull/1325#discussion_r464789252", "createdAt": "2020-08-04T04:17:18Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java", "diffHunk": "@@ -1887,13 +1898,14 @@ private void removeSemijoinOptimizationByBenefit(OptimizeTezProcContext procCtx)\n         // Check the ndv/rows from the SEL vs the destination tablescan the semijoin opt is going to.\n         TableScanOperator ts = sjInfo.getTsOp();\n         RuntimeValuesInfo rti = procCtx.parseContext.getRsToRuntimeValuesInfoMap().get(rs);\n-        ExprNodeDesc tsExpr = rti.getTsColExpr();\n-        // In the SEL operator of the semijoin branch, there should be only one column in the operator\n-        ExprNodeDesc selExpr = sel.getConf().getColList().get(0);\n+        List<ExprNodeDesc> targetColumns = rti.getTargetColumns();\n+        // In multi column semijoin branches the last column of the SEL operator is hash(c1, c2, ..., cn)\n+        // so we shouldn't consider it.\n+        List<ExprNodeDesc> sourceColumns = sel.getConf().getColList().subList(0, targetColumns.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4OTM2Mg==", "bodyText": "Neat!", "url": "https://github.com/apache/hive/pull/1325#discussion_r464789362", "createdAt": "2020-08-04T04:17:49Z", "author": {"login": "jcamachor"}, "path": "ql/src/test/queries/clientpositive/dynamic_semijoin_reduction_multicol.q", "diffHunk": "@@ -0,0 +1,64 @@\n+--! qt:dataset:tpch_0_001.lineitem\n+--! qt:dataset:tpch_0_001.partsupp\n+use tpch_0_001;\n+-- The test is meant to verify the plan, results, and Tez execution stats/counters\n+-- of the same query in three cases:\n+--   case 1: no semi-join reducers\n+--   case 2: one single column semi-join reducer\n+--   case 3: one multi column semi-join reducer\n+set hive.mapred.mode=nonstrict;\n+set hive.explain.user=false;\n+set hive.tez.bigtable.minsize.semijoin.reduction=6000;\n+-- Use TezSummaryPrinter hook to verify the impact of bloom filters by", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc5MDMyNA==", "bodyText": "Interesting: New plan seems more compact (less stages), which is good. I am wondering if this has to do with Shared Work Optimizer finding further optimization opportunities? Or maybe something else? Have you checked?", "url": "https://github.com/apache/hive/pull/1325#discussion_r464790324", "createdAt": "2020-08-04T04:21:32Z", "author": {"login": "jcamachor"}, "path": "ql/src/test/results/clientpositive/llap/dynamic_semijoin_reduction_2.q.out", "diffHunk": "@@ -79,27 +79,25 @@ STAGE PLANS:\n     Tez\n #### A masked pattern was here ####\n       Edges:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "edffed725aa5cb794578688d353cd015bc54cb6d"}, "originalPosition": 3}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c36444b9c97507342376c9d34a864fdbf27975b", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/0c36444b9c97507342376c9d34a864fdbf27975b", "committedDate": "2020-08-04T12:57:55Z", "message": "HIVE-21196: Add missing braces in single line if statements"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "31e70a1c0e152d08267ae462b6134a583c40d817", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/31e70a1c0e152d08267ae462b6134a583c40d817", "committedDate": "2020-08-04T16:42:43Z", "message": "HIVE-21196: Add documentation in SemiJoinReductionMerge transformation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5421ee2b79e52946b1dd5d708f476ff23d85237d", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/5421ee2b79e52946b1dd5d708f476ff23d85237d", "committedDate": "2020-08-04T16:55:56Z", "message": "HIVE-21196: Fix typo (sjBrances -> sjBranches)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "522c1d53e2630b51f922776ebc02cbe2778adf25", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/522c1d53e2630b51f922776ebc02cbe2778adf25", "committedDate": "2020-08-04T17:04:12Z", "message": "HIVE-21196: Ensure select operators in mergeSelectOps have only one column"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "921e4e0c8e1f29e254877f3621a1d2b3f161b810", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/921e4e0c8e1f29e254877f3621a1d2b3f161b810", "committedDate": "2020-08-04T17:09:30Z", "message": "HIVE-21196: Extra doc about columns in the merged select operator"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "216d858e871cde9dbf0da4b96c13f3efc78ecfb5", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/216d858e871cde9dbf0da4b96c13f3efc78ecfb5", "committedDate": "2020-08-05T08:14:46Z", "message": "HIVE-21196: Remove unused import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b1043e5f5add779269b637bb79e0b865852b1375", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/b1043e5f5add779269b637bb79e0b865852b1375", "committedDate": "2020-08-05T11:09:59Z", "message": "HIVE-21196: Flatten conjunctions (AND) if possible"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c037d81bf2cd5d90f7f9bc91bd21c2d814b9adbe", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/c037d81bf2cd5d90f7f9bc91bd21c2d814b9adbe", "committedDate": "2020-08-06T09:34:08Z", "message": "HIVE-21196: Compute bloom benefit using min selectivity of columns"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c9f9112d0802906dce7442f3d4c01535a584af11", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/c9f9112d0802906dce7442f3d4c01535a584af11", "committedDate": "2020-08-06T09:40:13Z", "message": "HIVE-21196: Update plans in Perf cli drivers\n\nWith the new estimation in the benefit of the bloom the multi-col\nsemijoin reduction kicks in also in query5.\n\nInterestingly there are also some important changes in query24. Due to\nthe new estimation a multi-col semijoin reduction on the same columns is\nused in two scan operators (on the same table). This also triggers the\nshared work optimizer that is able to merge the semijoin branches and\nthe respective scan operators."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7b3798af1141ebcbae8502f0410269b640787533", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/7b3798af1141ebcbae8502f0410269b640787533", "committedDate": "2020-08-06T09:54:01Z", "message": "HIVE-21196: Update few plans in MiniLLap drivers due to AND flattening"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e23569c8d35573bd784d3e0cb14fdbcf3c341b40", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/e23569c8d35573bd784d3e0cb14fdbcf3c341b40", "committedDate": "2020-08-06T10:04:17Z", "message": "Merge branch 'master' into HIVE-21196"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "99f046db8ba95afac3677fa5c1abf7aa375ad973", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/99f046db8ba95afac3677fa5c1abf7aa375ad973", "committedDate": "2020-08-06T10:35:29Z", "message": "HIVE-21196: Update dynamic_semijoin_reduction_2.q in MiniLlap driver\n\nChanges only due to semijoin reduction merge transformation merging\ncombining three sj reducers to one."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f14c4b6197c8decb801c070bf9e395965662d586", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/f14c4b6197c8decb801c070bf9e395965662d586", "committedDate": "2020-08-06T10:55:50Z", "message": "HIVE-21196: Update plans in Perf cli drivers"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "403160445b331505ad7e81354026afaabb8ca202", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/403160445b331505ad7e81354026afaabb8ca202", "committedDate": "2020-08-06T12:06:42Z", "message": "HIVE-21196: Replace certain assertions in SemiJoinReductionMerge with exceptions\n\nAvoid the case where assertions are disabled in production"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "409f49ac29f23bf7bd75cb44332be843868d1d66", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/409f49ac29f23bf7bd75cb44332be843868d1d66", "committedDate": "2020-08-06T13:16:17Z", "message": "HIVE-21196: Update comment in removeSemijoinOptimizationByBenefit"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "47039f54ab4c772ba0419cd20293321720a36e41", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/47039f54ab4c772ba0419cd20293321720a36e41", "committedDate": "2020-08-06T13:16:17Z", "message": "HIVE-21196: Use OperatorUtils#ancestor to obtain SEL operator in semijoins"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a794cf9248338c1d6b65a7828238f8d64f5dab2a", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/a794cf9248338c1d6b65a7828238f8d64f5dab2a", "committedDate": "2020-08-06T13:16:17Z", "message": "HIVE-21196: Add HIVE-23934 reference in TODO inside markSemiJoinForDPP"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de66827208461ca01228d4bf97d5c5a217f4a636", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/de66827208461ca01228d4bf97d5c5a217f4a636", "committedDate": "2020-08-07T09:58:29Z", "message": "HIVE-21196: Exploit bloom entries hint from single-col semijoin reducers\n\n1. Add a FunctionUtils#extractEvaluators utility function to\nconveniently obtain a class of UDAFE evaluators from the provided\naggregations.\n2. Add a test in dynamic_semijoin_reduction_multicol.q to verify that\nbloom filter's entries hint is propagated correctly when multi-column\nsemijoin reducers are in use."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b4253d8c888179b4408156553dc606679e1f605", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/6b4253d8c888179b4408156553dc606679e1f605", "committedDate": "2020-08-07T12:55:05Z", "message": "HIVE-21196: Remove TODO for getColExprToGBMap() in SemiJoinReductionMerge\n\nThe map is used for finding sharing opportunities in the plan\nconstructing single column bloom filters. For multi column bloom filters\nit cannot be applied directly. We will explore sharing opportunities for\nmulti column bloom filter in HIVE-24016."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f387d78447d9d68e2df7b20fdaab7d0c1bac905e", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/f387d78447d9d68e2df7b20fdaab7d0c1bac905e", "committedDate": "2020-08-07T15:48:17Z", "message": "HIVE-21196: Use LinkedHashMap for keeping semijoin information in ParseContext"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "827be9ea067806e9c703f6c1c756ecd89465e104", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/827be9ea067806e9c703f6c1c756ecd89465e104", "committedDate": "2020-08-07T18:16:35Z", "message": "HIVE-21196: Extract creation of semijoin merge candidates to method\n\nOnly refactoring can be dropped if necessary."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bf5cad3aa0861eef811565214231e5dec87cd3e6", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/bf5cad3aa0861eef811565214231e5dec87cd3e6", "committedDate": "2020-08-07T22:28:29Z", "message": "HIVE-21196: Replace TODO about bloom filter evaluator with comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2394c4bab74e2364827ce47fe001f52067df7881", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/2394c4bab74e2364827ce47fe001f52067df7881", "committedDate": "2020-08-07T22:49:56Z", "message": "HIVE-21196: Update query plans in Perf and MiniLlap drivers"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cec37ba17a259aae6a6f525d5b790d52a0561a1f", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/cec37ba17a259aae6a6f525d5b790d52a0561a1f", "committedDate": "2020-08-08T06:47:00Z", "message": "Merge branch 'master' into HIVE-21196"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7464cb3d3d2dc19a498cbef41bde09c84ba7cb65", "author": {"user": {"login": "zabetak", "name": "Stamatis Zampetakis"}}, "url": "https://github.com/apache/hive/commit/7464cb3d3d2dc19a498cbef41bde09c84ba7cb65", "committedDate": "2020-08-09T22:03:18Z", "message": "Update query plans in MiniLlap and Perf drivers"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3358, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}