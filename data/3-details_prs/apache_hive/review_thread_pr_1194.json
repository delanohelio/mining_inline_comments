{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQyNTA3NTk2", "number": 1194, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwNzo1MTozNVrOEKZx6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwNzo1NDowNVrOEKZ1Xg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MzQzNTk1OnYy", "diffSide": "LEFT", "path": "ql/src/java/org/apache/hadoop/hive/ql/Compiler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwNzo1MTozNVrOGra1uQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwODowNzoyNVrOGrbXUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE4MTY4OQ==", "bodyText": "Are we sure about this?\nWe create a backup context so if we have to reexecute the query then we have a context at hand (for removing temporary files etc)", "url": "https://github.com/apache/hive/pull/1194#discussion_r448181689", "createdAt": "2020-07-01T07:51:35Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Compiler.java", "diffHunk": "@@ -188,7 +188,6 @@ private BaseSemanticAnalyzer analyze() throws Exception {\n     // because at that point we need access to the objects.\n     Hive.get().getMSC().flushCache();\n \n-    driverContext.setBackupContext(new Context(context));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a9f8c39251fa6acb00000f9eb72348003bf748e"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE5MDI5MA==", "bodyText": "The usage of backupContext was removed by Peter Varga recently (e2a02f1#diff-71a166c053d9c698f9cb64eaef832aff), I've asked him to confirm, and it was intentional. After this change we are only setting the backup context here, but it is never used.", "url": "https://github.com/apache/hive/pull/1194#discussion_r448190290", "createdAt": "2020-07-01T08:07:25Z", "author": {"login": "miklosgergely"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Compiler.java", "diffHunk": "@@ -188,7 +188,6 @@ private BaseSemanticAnalyzer analyze() throws Exception {\n     // because at that point we need access to the objects.\n     Hive.get().getMSC().flushCache();\n \n-    driverContext.setBackupContext(new Context(context));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE4MTY4OQ=="}, "originalCommit": {"oid": "0a9f8c39251fa6acb00000f9eb72348003bf748e"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MzQ0NDc4OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/DriverTxnHandler.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwNzo1NDowNVrOGra7FA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwODowODo0NlrOGrbaTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE4MzA2MA==", "bodyText": "I do not like storing lock related stuff in context. Shouldn't this be private to the DbTxnHandler?", "url": "https://github.com/apache/hive/pull/1194#discussion_r448183060", "createdAt": "2020-07-01T07:54:05Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/DriverTxnHandler.java", "diffHunk": "@@ -288,15 +313,231 @@ private void acquireLocksInternal() throws CommandProcessorException, LockExcept\n     }\n   }\n \n-  public void addHiveLocksFromContext() {\n+  /**\n+   *  Write the current set of valid write ids for the operated acid tables into the configuration so\n+   *  that it can be read by the input format.\n+   */\n+  private ValidTxnWriteIdList recordValidWriteIds() throws LockException {\n+    String txnString = driverContext.getConf().get(ValidTxnList.VALID_TXNS_KEY);\n+    if (Strings.isNullOrEmpty(txnString)) {\n+      throw new IllegalStateException(\"calling recordValidWritsIdss() without initializing ValidTxnList \" +\n+          JavaUtils.txnIdToString(driverContext.getTxnManager().getCurrentTxnId()));\n+    }\n+\n+    ValidTxnWriteIdList txnWriteIds = getTxnWriteIds(txnString);\n+    setValidWriteIds(txnWriteIds);\n+\n+    LOG.debug(\"Encoding valid txn write ids info {} txnid: {}\", txnWriteIds.toString(),\n+        driverContext.getTxnManager().getCurrentTxnId());\n+    return txnWriteIds;\n+  }\n+\n+  private ValidTxnWriteIdList getTxnWriteIds(String txnString) throws LockException {\n+    List<String> txnTables = getTransactionalTables(getTables(true, true));\n+    ValidTxnWriteIdList txnWriteIds = null;\n+    if (driverContext.getCompactionWriteIds() != null) {\n+      // This is kludgy: here we need to read with Compactor's snapshot/txn rather than the snapshot of the current\n+      // {@code txnMgr}, in effect simulating a \"flashback query\" but can't actually share compactor's txn since it\n+      // would run multiple statements.  See more comments in {@link org.apache.hadoop.hive.ql.txn.compactor.Worker}\n+      // where it start the compactor txn*/\n+      if (txnTables.size() != 1) {\n+        throw new LockException(\"Unexpected tables in compaction: \" + txnTables);\n+      }\n+      txnWriteIds = new ValidTxnWriteIdList(driverContext.getCompactorTxnId());\n+      txnWriteIds.addTableValidWriteIdList(driverContext.getCompactionWriteIds());\n+    } else {\n+      txnWriteIds = driverContext.getTxnManager().getValidWriteIds(txnTables, txnString);\n+    }\n+    if (driverContext.getTxnType() == TxnType.READ_ONLY && !getTables(false, true).isEmpty()) {\n+      throw new IllegalStateException(String.format(\n+          \"Inferred transaction type '%s' doesn't conform to the actual query string '%s'\",\n+          driverContext.getTxnType(), driverContext.getQueryState().getQueryString()));\n+    }\n+    return txnWriteIds;\n+  }\n+\n+  private void setValidWriteIds(ValidTxnWriteIdList txnWriteIds) {\n+    driverContext.getConf().set(ValidTxnWriteIdList.VALID_TABLES_WRITEIDS_KEY, txnWriteIds.toString());\n+    if (driverContext.getPlan().getFetchTask() != null) {\n+      // This is needed for {@link HiveConf.ConfVars.HIVEFETCHTASKCONVERSION} optimization which initializes JobConf\n+      // in FetchOperator before recordValidTxns() but this has to be done after locks are acquired to avoid race\n+      // conditions in ACID. This case is supported only for single source query.\n+      Operator<?> source = driverContext.getPlan().getFetchTask().getWork().getSource();\n+      if (source instanceof TableScanOperator) {\n+        TableScanOperator tsOp = (TableScanOperator)source;\n+        String fullTableName = AcidUtils.getFullTableName(tsOp.getConf().getDatabaseName(),\n+            tsOp.getConf().getTableName());\n+        ValidWriteIdList writeIdList = txnWriteIds.getTableValidWriteIdList(fullTableName);\n+        if (tsOp.getConf().isTranscationalTable() && (writeIdList == null)) {\n+          throw new IllegalStateException(String.format(\n+              \"ACID table: %s is missing from the ValidWriteIdList config: %s\", fullTableName, txnWriteIds.toString()));\n+        }\n+        if (writeIdList != null) {\n+          driverContext.getPlan().getFetchTask().setValidWriteIdList(writeIdList.toString());\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Checks whether txn list has been invalidated while planning the query.\n+   * This would happen if query requires exclusive/semi-shared lock, and there has been a committed transaction\n+   * on the table over which the lock is required.\n+   */\n+  boolean isValidTxnListState() throws LockException {\n+    // 1) Get valid txn list.\n+    String txnString = driverContext.getConf().get(ValidTxnList.VALID_TXNS_KEY);\n+    if (txnString == null) {\n+      return true; // Not a transactional op, nothing more to do\n+    }\n+\n+    // 2) Get locks that are relevant:\n+    // - Exclusive for INSERT OVERWRITE, when shared write is disabled (HiveConf.TXN_WRITE_X_LOCK=false).\n+    // - Excl-write for UPDATE/DELETE, when shared write is disabled, INSERT OVERWRITE - when enabled.\n+    Set<String> nonSharedLockedTables = getNonSharedLockedTables();\n+    if (nonSharedLockedTables.isEmpty()) {\n+      return true; // Nothing to check\n+    }\n+\n+    // 3) Get txn tables that are being written\n+    String txnWriteIdListString = driverContext.getConf().get(ValidTxnWriteIdList.VALID_TABLES_WRITEIDS_KEY);\n+    if (Strings.isNullOrEmpty(txnWriteIdListString)) {\n+      return true; // Nothing to check\n+    }\n+\n+    GetOpenTxnsResponse openTxns = driverContext.getTxnManager().getOpenTxns();\n+    ValidTxnList validTxnList = TxnCommonUtils.createValidReadTxnList(openTxns, 0);\n+    long txnId = driverContext.getTxnManager().getCurrentTxnId();\n+\n+    String currentTxnString;\n+    if (validTxnList.isTxnRangeValid(txnId + 1, openTxns.getTxn_high_water_mark()) != ValidTxnList.RangeResponse.NONE) {\n+      // If here, there was another txn opened & committed between current SNAPSHOT generation and locking.\n+      validTxnList.removeException(txnId);\n+      currentTxnString = validTxnList.toString();\n+    } else {\n+      currentTxnString = TxnCommonUtils.createValidReadTxnList(openTxns, txnId).toString();\n+    }\n+\n+    if (currentTxnString.equals(txnString)) {\n+      return true; // Still valid, nothing more to do\n+    }\n+    return checkWriteIds(currentTxnString, nonSharedLockedTables, txnWriteIdListString);\n+  }\n+\n+  private Set<String> getNonSharedLockedTables() {\n+    if (CollectionUtils.isEmpty(context.getHiveLocks())) {\n+      return Collections.emptySet(); // Nothing to check\n+    }\n+\n+    Set<String> nonSharedLockedTables = new HashSet<>();\n+    for (HiveLock lock : context.getHiveLocks()) {\n+      if (lock.mayContainComponents()) {\n+        // The lock may have multiple components, e.g., DbHiveLock, hence we need to check for each of them\n+        for (LockComponent lockComponent : lock.getHiveLockComponents()) {\n+          // We only consider tables for which we hold either an exclusive or a excl-write lock\n+          if ((lockComponent.getType() == LockType.EXCLUSIVE || lockComponent.getType() == LockType.EXCL_WRITE) &&\n+              lockComponent.getTablename() != null && !DbTxnManager.GLOBAL_LOCKS.equals(lockComponent.getDbname())) {\n+            nonSharedLockedTables.add(TableName.getDbTable(lockComponent.getDbname(), lockComponent.getTablename()));\n+          }\n+        }\n+      } else {\n+        // The lock has a single components, e.g., SimpleHiveLock or ZooKeeperHiveLock.\n+        // Pos 0 of lock paths array contains dbname, pos 1 contains tblname\n+        if ((lock.getHiveLockMode() == HiveLockMode.EXCLUSIVE || lock.getHiveLockMode() == HiveLockMode.SEMI_SHARED) &&\n+            lock.getHiveLockObject().getPaths().length == 2) {\n+          nonSharedLockedTables.add(\n+              TableName.getDbTable(lock.getHiveLockObject().getPaths()[0], lock.getHiveLockObject().getPaths()[1]));\n+        }\n+      }\n+    }\n+    return nonSharedLockedTables;\n+  }\n+\n+  private boolean checkWriteIds(String currentTxnString, Set<String> nonSharedLockedTables, String txnWriteIdListString)\n+      throws LockException {\n+    ValidTxnWriteIdList txnWriteIdList = new ValidTxnWriteIdList(txnWriteIdListString);\n+    Map<String, Table> writtenTables = getTables(false, true);\n+\n+    ValidTxnWriteIdList currentTxnWriteIds = driverContext.getTxnManager().getValidWriteIds(\n+        getTransactionalTables(writtenTables), currentTxnString);\n+\n+    for (Map.Entry<String, Table> tableInfo : writtenTables.entrySet()) {\n+      String fullQNameForLock = TableName.getDbTable(tableInfo.getValue().getDbName(),\n+          MetaStoreUtils.encodeTableName(tableInfo.getValue().getTableName()));\n+      if (nonSharedLockedTables.contains(fullQNameForLock)) {\n+        // Check if table is transactional\n+        if (AcidUtils.isTransactionalTable(tableInfo.getValue())) {\n+          ValidWriteIdList writeIdList = txnWriteIdList.getTableValidWriteIdList(tableInfo.getKey());\n+          ValidWriteIdList currentWriteIdList = currentTxnWriteIds.getTableValidWriteIdList(tableInfo.getKey());\n+          // Check if there was a conflicting write between current SNAPSHOT generation and locking.\n+          if (currentWriteIdList.isWriteIdRangeValid(writeIdList.getHighWatermark() + 1,\n+              currentWriteIdList.getHighWatermark()) != ValidWriteIdList.RangeResponse.NONE) {\n+            return false;\n+          }\n+          // Check that write id is still valid\n+          if (!TxnIdUtils.checkEquivalentWriteIds(writeIdList, currentWriteIdList)) {\n+            // Write id has changed, it is not valid anymore, we need to recompile\n+            return false;\n+          }\n+        }\n+        nonSharedLockedTables.remove(fullQNameForLock);\n+      }\n+    }\n+\n+    if (!nonSharedLockedTables.isEmpty()) {\n+      throw new LockException(\"Wrong state: non-shared locks contain information for tables that have not\" +\n+          \" been visited when trying to validate the locks from query tables.\\n\" +\n+          \"Tables: \" + writtenTables.keySet() + \"\\n\" +\n+          \"Remaining locks after check: \" + nonSharedLockedTables);\n+    }\n+\n+    return true; // It passes the test, it is valid\n+  }\n+\n+  private Map<String, Table> getTables(boolean inputNeeded, boolean outputNeeded) {\n+    Map<String, Table> tables = new HashMap<>();\n+    if (inputNeeded) {\n+      driverContext.getPlan().getInputs().forEach(input -> addTableFromEntity(input, tables));\n+    }\n+    if (outputNeeded) {\n+      driverContext.getPlan().getOutputs().forEach(output -> addTableFromEntity(output, tables));\n+    }\n+    return tables;\n+  }\n+\n+  private void addTableFromEntity(Entity entity, Map<String, Table> tables) {\n+    Table table;\n+    switch (entity.getType()) {\n+    case TABLE:\n+      table = entity.getTable();\n+      break;\n+    case PARTITION:\n+    case DUMMYPARTITION:\n+      table = entity.getPartition().getTable();\n+      break;\n+    default:\n+      return;\n+    }\n+    String fullTableName = AcidUtils.getFullTableName(table.getDbName(), table.getTableName());\n+    tables.put(fullTableName, table);\n+  }\n+\n+  private List<String> getTransactionalTables(Map<String, Table> tables) {\n+    return tables.entrySet().stream()\n+      .filter(entry -> AcidUtils.isTransactionalTable(entry.getValue()))\n+      .map(Map.Entry::getKey)\n+      .collect(Collectors.toList());\n+  }\n+\n+  void addHiveLocksFromContext() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a9f8c39251fa6acb00000f9eb72348003bf748e"}, "originalPosition": 341}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE5MTA1Mg==", "bodyText": "I agree, but in a separate jira. This one is for merging the two classes only, so we'll have a clean and easy to understand history.", "url": "https://github.com/apache/hive/pull/1194#discussion_r448191052", "createdAt": "2020-07-01T08:08:46Z", "author": {"login": "miklosgergely"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/DriverTxnHandler.java", "diffHunk": "@@ -288,15 +313,231 @@ private void acquireLocksInternal() throws CommandProcessorException, LockExcept\n     }\n   }\n \n-  public void addHiveLocksFromContext() {\n+  /**\n+   *  Write the current set of valid write ids for the operated acid tables into the configuration so\n+   *  that it can be read by the input format.\n+   */\n+  private ValidTxnWriteIdList recordValidWriteIds() throws LockException {\n+    String txnString = driverContext.getConf().get(ValidTxnList.VALID_TXNS_KEY);\n+    if (Strings.isNullOrEmpty(txnString)) {\n+      throw new IllegalStateException(\"calling recordValidWritsIdss() without initializing ValidTxnList \" +\n+          JavaUtils.txnIdToString(driverContext.getTxnManager().getCurrentTxnId()));\n+    }\n+\n+    ValidTxnWriteIdList txnWriteIds = getTxnWriteIds(txnString);\n+    setValidWriteIds(txnWriteIds);\n+\n+    LOG.debug(\"Encoding valid txn write ids info {} txnid: {}\", txnWriteIds.toString(),\n+        driverContext.getTxnManager().getCurrentTxnId());\n+    return txnWriteIds;\n+  }\n+\n+  private ValidTxnWriteIdList getTxnWriteIds(String txnString) throws LockException {\n+    List<String> txnTables = getTransactionalTables(getTables(true, true));\n+    ValidTxnWriteIdList txnWriteIds = null;\n+    if (driverContext.getCompactionWriteIds() != null) {\n+      // This is kludgy: here we need to read with Compactor's snapshot/txn rather than the snapshot of the current\n+      // {@code txnMgr}, in effect simulating a \"flashback query\" but can't actually share compactor's txn since it\n+      // would run multiple statements.  See more comments in {@link org.apache.hadoop.hive.ql.txn.compactor.Worker}\n+      // where it start the compactor txn*/\n+      if (txnTables.size() != 1) {\n+        throw new LockException(\"Unexpected tables in compaction: \" + txnTables);\n+      }\n+      txnWriteIds = new ValidTxnWriteIdList(driverContext.getCompactorTxnId());\n+      txnWriteIds.addTableValidWriteIdList(driverContext.getCompactionWriteIds());\n+    } else {\n+      txnWriteIds = driverContext.getTxnManager().getValidWriteIds(txnTables, txnString);\n+    }\n+    if (driverContext.getTxnType() == TxnType.READ_ONLY && !getTables(false, true).isEmpty()) {\n+      throw new IllegalStateException(String.format(\n+          \"Inferred transaction type '%s' doesn't conform to the actual query string '%s'\",\n+          driverContext.getTxnType(), driverContext.getQueryState().getQueryString()));\n+    }\n+    return txnWriteIds;\n+  }\n+\n+  private void setValidWriteIds(ValidTxnWriteIdList txnWriteIds) {\n+    driverContext.getConf().set(ValidTxnWriteIdList.VALID_TABLES_WRITEIDS_KEY, txnWriteIds.toString());\n+    if (driverContext.getPlan().getFetchTask() != null) {\n+      // This is needed for {@link HiveConf.ConfVars.HIVEFETCHTASKCONVERSION} optimization which initializes JobConf\n+      // in FetchOperator before recordValidTxns() but this has to be done after locks are acquired to avoid race\n+      // conditions in ACID. This case is supported only for single source query.\n+      Operator<?> source = driverContext.getPlan().getFetchTask().getWork().getSource();\n+      if (source instanceof TableScanOperator) {\n+        TableScanOperator tsOp = (TableScanOperator)source;\n+        String fullTableName = AcidUtils.getFullTableName(tsOp.getConf().getDatabaseName(),\n+            tsOp.getConf().getTableName());\n+        ValidWriteIdList writeIdList = txnWriteIds.getTableValidWriteIdList(fullTableName);\n+        if (tsOp.getConf().isTranscationalTable() && (writeIdList == null)) {\n+          throw new IllegalStateException(String.format(\n+              \"ACID table: %s is missing from the ValidWriteIdList config: %s\", fullTableName, txnWriteIds.toString()));\n+        }\n+        if (writeIdList != null) {\n+          driverContext.getPlan().getFetchTask().setValidWriteIdList(writeIdList.toString());\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Checks whether txn list has been invalidated while planning the query.\n+   * This would happen if query requires exclusive/semi-shared lock, and there has been a committed transaction\n+   * on the table over which the lock is required.\n+   */\n+  boolean isValidTxnListState() throws LockException {\n+    // 1) Get valid txn list.\n+    String txnString = driverContext.getConf().get(ValidTxnList.VALID_TXNS_KEY);\n+    if (txnString == null) {\n+      return true; // Not a transactional op, nothing more to do\n+    }\n+\n+    // 2) Get locks that are relevant:\n+    // - Exclusive for INSERT OVERWRITE, when shared write is disabled (HiveConf.TXN_WRITE_X_LOCK=false).\n+    // - Excl-write for UPDATE/DELETE, when shared write is disabled, INSERT OVERWRITE - when enabled.\n+    Set<String> nonSharedLockedTables = getNonSharedLockedTables();\n+    if (nonSharedLockedTables.isEmpty()) {\n+      return true; // Nothing to check\n+    }\n+\n+    // 3) Get txn tables that are being written\n+    String txnWriteIdListString = driverContext.getConf().get(ValidTxnWriteIdList.VALID_TABLES_WRITEIDS_KEY);\n+    if (Strings.isNullOrEmpty(txnWriteIdListString)) {\n+      return true; // Nothing to check\n+    }\n+\n+    GetOpenTxnsResponse openTxns = driverContext.getTxnManager().getOpenTxns();\n+    ValidTxnList validTxnList = TxnCommonUtils.createValidReadTxnList(openTxns, 0);\n+    long txnId = driverContext.getTxnManager().getCurrentTxnId();\n+\n+    String currentTxnString;\n+    if (validTxnList.isTxnRangeValid(txnId + 1, openTxns.getTxn_high_water_mark()) != ValidTxnList.RangeResponse.NONE) {\n+      // If here, there was another txn opened & committed between current SNAPSHOT generation and locking.\n+      validTxnList.removeException(txnId);\n+      currentTxnString = validTxnList.toString();\n+    } else {\n+      currentTxnString = TxnCommonUtils.createValidReadTxnList(openTxns, txnId).toString();\n+    }\n+\n+    if (currentTxnString.equals(txnString)) {\n+      return true; // Still valid, nothing more to do\n+    }\n+    return checkWriteIds(currentTxnString, nonSharedLockedTables, txnWriteIdListString);\n+  }\n+\n+  private Set<String> getNonSharedLockedTables() {\n+    if (CollectionUtils.isEmpty(context.getHiveLocks())) {\n+      return Collections.emptySet(); // Nothing to check\n+    }\n+\n+    Set<String> nonSharedLockedTables = new HashSet<>();\n+    for (HiveLock lock : context.getHiveLocks()) {\n+      if (lock.mayContainComponents()) {\n+        // The lock may have multiple components, e.g., DbHiveLock, hence we need to check for each of them\n+        for (LockComponent lockComponent : lock.getHiveLockComponents()) {\n+          // We only consider tables for which we hold either an exclusive or a excl-write lock\n+          if ((lockComponent.getType() == LockType.EXCLUSIVE || lockComponent.getType() == LockType.EXCL_WRITE) &&\n+              lockComponent.getTablename() != null && !DbTxnManager.GLOBAL_LOCKS.equals(lockComponent.getDbname())) {\n+            nonSharedLockedTables.add(TableName.getDbTable(lockComponent.getDbname(), lockComponent.getTablename()));\n+          }\n+        }\n+      } else {\n+        // The lock has a single components, e.g., SimpleHiveLock or ZooKeeperHiveLock.\n+        // Pos 0 of lock paths array contains dbname, pos 1 contains tblname\n+        if ((lock.getHiveLockMode() == HiveLockMode.EXCLUSIVE || lock.getHiveLockMode() == HiveLockMode.SEMI_SHARED) &&\n+            lock.getHiveLockObject().getPaths().length == 2) {\n+          nonSharedLockedTables.add(\n+              TableName.getDbTable(lock.getHiveLockObject().getPaths()[0], lock.getHiveLockObject().getPaths()[1]));\n+        }\n+      }\n+    }\n+    return nonSharedLockedTables;\n+  }\n+\n+  private boolean checkWriteIds(String currentTxnString, Set<String> nonSharedLockedTables, String txnWriteIdListString)\n+      throws LockException {\n+    ValidTxnWriteIdList txnWriteIdList = new ValidTxnWriteIdList(txnWriteIdListString);\n+    Map<String, Table> writtenTables = getTables(false, true);\n+\n+    ValidTxnWriteIdList currentTxnWriteIds = driverContext.getTxnManager().getValidWriteIds(\n+        getTransactionalTables(writtenTables), currentTxnString);\n+\n+    for (Map.Entry<String, Table> tableInfo : writtenTables.entrySet()) {\n+      String fullQNameForLock = TableName.getDbTable(tableInfo.getValue().getDbName(),\n+          MetaStoreUtils.encodeTableName(tableInfo.getValue().getTableName()));\n+      if (nonSharedLockedTables.contains(fullQNameForLock)) {\n+        // Check if table is transactional\n+        if (AcidUtils.isTransactionalTable(tableInfo.getValue())) {\n+          ValidWriteIdList writeIdList = txnWriteIdList.getTableValidWriteIdList(tableInfo.getKey());\n+          ValidWriteIdList currentWriteIdList = currentTxnWriteIds.getTableValidWriteIdList(tableInfo.getKey());\n+          // Check if there was a conflicting write between current SNAPSHOT generation and locking.\n+          if (currentWriteIdList.isWriteIdRangeValid(writeIdList.getHighWatermark() + 1,\n+              currentWriteIdList.getHighWatermark()) != ValidWriteIdList.RangeResponse.NONE) {\n+            return false;\n+          }\n+          // Check that write id is still valid\n+          if (!TxnIdUtils.checkEquivalentWriteIds(writeIdList, currentWriteIdList)) {\n+            // Write id has changed, it is not valid anymore, we need to recompile\n+            return false;\n+          }\n+        }\n+        nonSharedLockedTables.remove(fullQNameForLock);\n+      }\n+    }\n+\n+    if (!nonSharedLockedTables.isEmpty()) {\n+      throw new LockException(\"Wrong state: non-shared locks contain information for tables that have not\" +\n+          \" been visited when trying to validate the locks from query tables.\\n\" +\n+          \"Tables: \" + writtenTables.keySet() + \"\\n\" +\n+          \"Remaining locks after check: \" + nonSharedLockedTables);\n+    }\n+\n+    return true; // It passes the test, it is valid\n+  }\n+\n+  private Map<String, Table> getTables(boolean inputNeeded, boolean outputNeeded) {\n+    Map<String, Table> tables = new HashMap<>();\n+    if (inputNeeded) {\n+      driverContext.getPlan().getInputs().forEach(input -> addTableFromEntity(input, tables));\n+    }\n+    if (outputNeeded) {\n+      driverContext.getPlan().getOutputs().forEach(output -> addTableFromEntity(output, tables));\n+    }\n+    return tables;\n+  }\n+\n+  private void addTableFromEntity(Entity entity, Map<String, Table> tables) {\n+    Table table;\n+    switch (entity.getType()) {\n+    case TABLE:\n+      table = entity.getTable();\n+      break;\n+    case PARTITION:\n+    case DUMMYPARTITION:\n+      table = entity.getPartition().getTable();\n+      break;\n+    default:\n+      return;\n+    }\n+    String fullTableName = AcidUtils.getFullTableName(table.getDbName(), table.getTableName());\n+    tables.put(fullTableName, table);\n+  }\n+\n+  private List<String> getTransactionalTables(Map<String, Table> tables) {\n+    return tables.entrySet().stream()\n+      .filter(entry -> AcidUtils.isTransactionalTable(entry.getValue()))\n+      .map(Map.Entry::getKey)\n+      .collect(Collectors.toList());\n+  }\n+\n+  void addHiveLocksFromContext() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE4MzA2MA=="}, "originalCommit": {"oid": "0a9f8c39251fa6acb00000f9eb72348003bf748e"}, "originalPosition": 341}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 693, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}