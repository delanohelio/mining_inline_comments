{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM5Nzk1OTEw", "number": 1779, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxNDozMDozMFrOFHz8ug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxNjozNjoyNFrOFH2zsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNzM1NDgyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxNDozMDozMFrOIJaLrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQxMDozMjoxNlrOINsrcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjczNzA2OQ==", "bodyText": "Side note: Shouldn't HIVE_MM_ALLOW_ORIGINALS also be part of this condition?", "url": "https://github.com/apache/hive/pull/1779#discussion_r546737069", "createdAt": "2020-12-21T14:30:30Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "diffHunk": "@@ -1316,7 +1282,7 @@ private Directory getAcidState() throws IOException {\n     }\n \n \n-    private AcidDirInfo callInternal() throws IOException {\n+    private AcidDirectory callInternal() throws IOException {\n       if (context.acidOperationalProperties != null", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzMzOTM0NA==", "bodyText": "I played around this a bit and I think will do a follow up ticket for mm, because this again is very messy.\nFor mm tables HiveInputformat will take the partition/table directory, find the delta directories in it and validate them against the validwriteid list. This is neccessary to handle every other file format. But the consequence of this, that the OrcInputFormat#getSplits will be called with the delta directories directly, not with the partition/table dir as it is done normally. So this bypass actually will do a listing in the delta dir itself. And \"findOriginals\" not only finds the original for MM tables it finds all of the files in the table, and it is necessary otherwise we would not generate split for normal bucketfiles. So the sort answer is that these originals is not the same originals as the context of mm tables ...\nThe most disgusting part is, when you create an orc table that is not transactional, this method will be called, and we will create a full AcidDirectory just to list all the files in the table as originals. OrcInputformat is very much interweaved with acid code.", "url": "https://github.com/apache/hive/pull/1779#discussion_r547339344", "createdAt": "2020-12-22T15:25:06Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "diffHunk": "@@ -1316,7 +1282,7 @@ private Directory getAcidState() throws IOException {\n     }\n \n \n-    private AcidDirInfo callInternal() throws IOException {\n+    private AcidDirectory callInternal() throws IOException {\n       if (context.acidOperationalProperties != null", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjczNzA2OQ=="}, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTIzNDQxNg==", "bodyText": "Thanks for taking the time to research & explain this!", "url": "https://github.com/apache/hive/pull/1779#discussion_r551234416", "createdAt": "2021-01-04T10:32:16Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "diffHunk": "@@ -1316,7 +1282,7 @@ private Directory getAcidState() throws IOException {\n     }\n \n \n-    private AcidDirInfo callInternal() throws IOException {\n+    private AcidDirectory callInternal() throws IOException {\n       if (context.acidOperationalProperties != null", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjczNzA2OQ=="}, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 118}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNzM2MTI2OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxNDozMjoyOVrOIJaPiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxNDozMjoyOVrOIJaPiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjczODA1OQ==", "bodyText": "Nit: Can just return getAcidState()", "url": "https://github.com/apache/hive/pull/1779#discussion_r546738059", "createdAt": "2020-12-21T14:32:29Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "diffHunk": "@@ -1326,106 +1292,16 @@ private AcidDirInfo callInternal() throws IOException {\n         // the originals could still be handled by AcidUtils like a regular non-txn table.\n         boolean isRecursive = context.conf.getBoolean(FileInputFormat.INPUT_DIR_RECURSIVE,\n             context.conf.getBoolean(\"mapred.input.dir.recursive\", false));\n-        List<HdfsFileStatusWithId> originals = new ArrayList<>();\n-        List<AcidBaseFileInfo> baseFiles = new ArrayList<>();\n-        AcidUtils.findOriginals(fs.get(), dir, originals, useFileIds, true, isRecursive);\n-        for (HdfsFileStatusWithId fileId : originals) {\n-          baseFiles.add(new AcidBaseFileInfo(fileId, AcidUtils.AcidBaseFileType.ORIGINAL_BASE));\n-        }\n-        return new AcidDirInfo(fs.get(), dir, new AcidUtils.DirectoryImpl(Lists.newArrayList(), Sets.newHashSet(), false, true, originals,\n-            Lists.newArrayList(), Lists.newArrayList(), null), baseFiles, new ArrayList<>());\n+\n+        List<HdfsFileStatusWithId> originals = AcidUtils.findOriginals(fs.get(), dir, useFileIds, true, isRecursive);\n+        AcidDirectory directory = new AcidDirectory(dir, fs.get(), useFileIds);\n+        directory.getOriginalFiles().addAll(originals);\n+        return directory;\n       }\n       //todo: shouldn't ignoreEmptyFiles be set based on ExecutionEngine?\n-      AcidUtils.Directory dirInfo  = getAcidState();\n-      // find the base files (original or new style)\n-      List<AcidBaseFileInfo> baseFiles = new ArrayList<>();\n-      if (dirInfo.getBaseDirectory() == null) {\n-        // For non-acid tables (or paths), all data files are in getOriginalFiles() list\n-        for (HdfsFileStatusWithId fileId : dirInfo.getOriginalFiles()) {\n-          baseFiles.add(new AcidBaseFileInfo(fileId, AcidUtils.AcidBaseFileType.ORIGINAL_BASE));\n-        }\n-      } else {\n-        List<HdfsFileStatusWithId> compactedBaseFiles = dirInfo.getBaseFiles();\n-        if (compactedBaseFiles == null) {\n-          compactedBaseFiles = AcidUtils.findBaseFiles(dirInfo.getBaseDirectory(), useFileIds, fs);\n-        }\n-        for (HdfsFileStatusWithId fileId : compactedBaseFiles) {\n-          baseFiles.add(new AcidBaseFileInfo(fileId, dirInfo.isBaseInRawFormat() ?\n-            AcidUtils.AcidBaseFileType.ORIGINAL_BASE : AcidUtils.AcidBaseFileType.ACID_SCHEMA));\n-        }\n-      }\n-\n-      // Find the parsed deltas- some of them containing only the insert delta events\n-      // may get treated as base if split-update is enabled for ACID. (See HIVE-14035 for details)\n-      List<ParsedDelta> parsedDeltas = new ArrayList<>();\n-      if (context.acidOperationalProperties != null &&\n-          context.acidOperationalProperties.isSplitUpdate()) {\n-        // If we have split-update turned on for this table, then the delta events have already been\n-        // split into two directories- delta_x_y/ and delete_delta_x_y/.\n-        // When you have split-update turned on, the insert events go to delta_x_y/ directory and all\n-        // the delete events go to delete_x_y/. An update event will generate two events-\n-        // a delete event for the old record that is put into delete_delta_x_y/,\n-        // followed by an insert event for the updated record put into the usual delta_x_y/.\n-        // Therefore, everything inside delta_x_y/ is an insert event and all the files in delta_x_y/\n-        // can be treated like base files. Hence, each of these are added to baseOrOriginalFiles list.\n-\n-        for (ParsedDelta parsedDelta : dirInfo.getCurrentDirectories()) {\n-          if (parsedDelta.isDeleteDelta()) {\n-            parsedDeltas.add(parsedDelta);\n-          } else {\n-            AcidUtils.AcidBaseFileType deltaType = parsedDelta.isRawFormat() ?\n-              AcidUtils.AcidBaseFileType.ORIGINAL_BASE : AcidUtils.AcidBaseFileType.ACID_SCHEMA;\n-            PathFilter bucketFilter = parsedDelta.isRawFormat() ?\n-              AcidUtils.originalBucketFilter : AcidUtils.bucketFileFilter;\n-            if (parsedDelta.isRawFormat() && parsedDelta.getMinWriteId() != parsedDelta.getMaxWriteId()) {\n-              //delta/ with files in raw format are a result of Load Data (as opposed to compaction\n-              //or streaming ingest so must have interval length == 1.\n-              throw new IllegalStateException(\"Delta in \" + AcidUtils.AcidBaseFileType.ORIGINAL_BASE\n-               + \" format but txnIds are out of range: \" + parsedDelta.getPath());\n-            }\n-            // This is a normal insert delta, which only has insert events and hence all the files\n-            // in this delta directory can be considered as a base.\n-            Boolean val = useFileIds.value;\n-            if (val == null || val) {\n-              try {\n-                List<HdfsFileStatusWithId> insertDeltaFiles =\n-                    SHIMS.listLocatedHdfsStatus(fs.get(), parsedDelta.getPath(), bucketFilter);\n-                for (HdfsFileStatusWithId fileId : insertDeltaFiles) {\n-                  baseFiles.add(new AcidBaseFileInfo(fileId, deltaType));\n-                }\n-                if (val == null) {\n-                  useFileIds.value = true; // The call succeeded, so presumably the API is there.\n-                }\n-                continue; // move on to process to the next parsedDelta.\n-              } catch (Throwable t) {\n-                LOG.error(\"Failed to get files with ID; using regular API: \" + t.getMessage());\n-                if (val == null && t instanceof UnsupportedOperationException) {\n-                  useFileIds.value = false;\n-                }\n-              }\n-            }\n-            // Fall back to regular API and create statuses without ID.\n-            List<FileStatus> children = HdfsUtils.listLocatedStatus(fs.get(),\n-                parsedDelta.getPath(), bucketFilter);\n-            for (FileStatus child : children) {\n-              HdfsFileStatusWithId fileId = AcidUtils.createOriginalObj(null, child);\n-              baseFiles.add(new AcidBaseFileInfo(fileId, deltaType));\n-            }\n-          }\n-        }\n+      AcidDirectory dirInfo  = getAcidState();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 217}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNzQ4NDY4OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidDirectory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxNTowNTo0N1rOIJbYAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMDo0NjozOFrOIJlckQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njc1NjYxMQ==", "bodyText": "This is kind of weird? What if this is an acid table that has never gone through major compaction?", "url": "https://github.com/apache/hive/pull/1779#discussion_r546756611", "createdAt": "2020-12-21T15:05:47Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidDirectory.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hive.shims.HadoopShims;\n+import org.apache.hive.common.util.Ref;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.hive.ql.io.AcidUtils.AcidBaseFileType.ORIGINAL_BASE;\n+\n+/**\n+ * AcidDirectory used to provide ACID directory layout information, which directories and files to read.\n+ * This representation only valid in a context of a ValidWriteIdList and ValidTxnList.\n+ */\n+public final class AcidDirectory {\n+\n+  private final Path path;\n+  private final FileSystem fs;\n+  private final Ref<Boolean> useFileId;\n+\n+  private AcidUtils.ParsedBase base;\n+  private AcidUtils.ParsedBaseLight oldestBase;\n+\n+  private final List<Path> abortedDirectories = new ArrayList<>();\n+  private final Set<Long> abortedWriteIds = new HashSet<>();\n+  private boolean unCompactedAborts;\n+  private final List<HadoopShims.HdfsFileStatusWithId> originalFiles = new ArrayList<>();\n+  private final List<Path> originalDirectories = new ArrayList<>();\n+  private final List<Path> obsolete = new ArrayList<>();\n+  private final List<AcidUtils.ParsedDelta> currentDirectories = new ArrayList<>();\n+\n+  public AcidDirectory(Path path, FileSystem fs, Ref<Boolean> useFileId) {\n+    this.path = path;\n+    this.fs = fs;\n+    this.useFileId = useFileId;\n+    if (!(this.fs instanceof DistributedFileSystem) && this.useFileId != null) {\n+      this.useFileId.value = false;\n+    }\n+  }\n+\n+  public Path getPath() {\n+    return path;\n+  }\n+\n+  /**\n+   * Get the base directory path.\n+   * @return the base directory to read\n+   */\n+  public Path getBaseDirectory() {\n+    return base == null ? null : base.getBaseDirPath();\n+  }\n+\n+  /**\n+   * Get the base directory.\n+   * @return the base directory to read\n+   */\n+  public AcidUtils.ParsedBase getBase() {\n+    return base;\n+  }\n+\n+  /**\n+   * Oldest base directory in the filesystem, may be shadowed by newer base\n+   */\n+  public AcidUtils.ParsedBaseLight getOldestBase() {\n+    return oldestBase;\n+  }\n+\n+  public void setBase(AcidUtils.ParsedBase base) {\n+    this.base = base;\n+  }\n+\n+  public void setOldestBase(AcidUtils.ParsedBaseLight oldestBase) {\n+    this.oldestBase = oldestBase;\n+  }\n+\n+  public void setUnCompactedAborts(boolean unCompactedAborts) {\n+    this.unCompactedAborts = unCompactedAborts;\n+  }\n+\n+  /**\n+   * Is Base directory in raw format or in Acid format\n+   */\n+  public boolean isBaseInRawFormat() {\n+    return base != null && base.isRawFormat();\n+  }\n+\n+  /**\n+   * Get the list of original files.  Not {@code null}.  Must be sorted.\n+   * @return the list of original files (eg. 000000_0)\n+   */\n+  public List<HadoopShims.HdfsFileStatusWithId> getOriginalFiles() {\n+    return originalFiles;\n+  }\n+\n+  /**\n+   * List of original directories containing files in not ACID format\n+   */\n+  public List<Path> getOriginalDirectories() {\n+    return originalDirectories;\n+  }\n+\n+  /**\n+   * Get the list of delta directories that are valid and not\n+   * obsolete.  Not {@code null}.  List must be sorted in a specific way.\n+   * See {@link org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight#compareTo(org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight)}\n+   * for details.\n+   * @return the minimal list of current directories\n+   */\n+  public List<AcidUtils.ParsedDelta> getCurrentDirectories() {\n+    return currentDirectories;\n+  }\n+\n+  /**\n+   * Get the list of obsolete directories. After filtering out bases and\n+   * deltas that are not selected by the valid transaction/write ids list, return the\n+   * list of original files, bases, and deltas that have been replaced by\n+   * more up to date ones.  Not {@code null}.\n+   */\n+  public List<Path> getObsolete() {\n+    return obsolete;\n+  }\n+\n+  /**\n+   * Get the list of directories that has nothing but aborted transactions.\n+   * @return the list of aborted directories\n+   */\n+  public List<Path> getAbortedDirectories() {\n+    return abortedDirectories;\n+  }\n+\n+  /**\n+   * Get the list of writeIds that belong to the aborted transactions.\n+   * @return the list of aborted writeIds\n+   */\n+  public Set<Long> getAbortedWriteIds() {\n+    return abortedWriteIds;\n+  }\n+\n+  /**\n+   * Does the directory contain writeIds that belong to aborted transactions,\n+   * but are mixed together with committed writes. These aborted writes can not be cleaned.\n+   * @return true if there are aborted writes that can can be cleaned\n+   */\n+  public boolean hasUncompactedAborts() {\n+    return unCompactedAborts;\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  /**\n+   * Delete deltas that should be read by this reader.\n+   */\n+  public List<AcidUtils.ParsedDelta> getDeleteDeltas() {\n+    return currentDirectories.stream().filter(AcidUtils.ParsedDeltaLight::isDeleteDelta).collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * All original, base and delta bucket files that should be read by this reader\n+   * @throws IOException ex\n+   */\n+  public List<AcidUtils.AcidBaseFileInfo> getBaseAndDeltaFiles() throws IOException {\n+    List<AcidUtils.AcidBaseFileInfo> baseAndDeltaFiles = new ArrayList<>();\n+    if (base == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 190}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkyMTYxNw==", "bodyText": "If it is a normal acid table and contains only a few delta, this branch will do nothing, because there can not be original files in the table (the delta files will be added later from the currentDirectories). This path is here (if I understood it correctly) for tables that were converted to transactional, but not yet compacted.", "url": "https://github.com/apache/hive/pull/1779#discussion_r546921617", "createdAt": "2020-12-21T20:46:38Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidDirectory.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hive.shims.HadoopShims;\n+import org.apache.hive.common.util.Ref;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.hive.ql.io.AcidUtils.AcidBaseFileType.ORIGINAL_BASE;\n+\n+/**\n+ * AcidDirectory used to provide ACID directory layout information, which directories and files to read.\n+ * This representation only valid in a context of a ValidWriteIdList and ValidTxnList.\n+ */\n+public final class AcidDirectory {\n+\n+  private final Path path;\n+  private final FileSystem fs;\n+  private final Ref<Boolean> useFileId;\n+\n+  private AcidUtils.ParsedBase base;\n+  private AcidUtils.ParsedBaseLight oldestBase;\n+\n+  private final List<Path> abortedDirectories = new ArrayList<>();\n+  private final Set<Long> abortedWriteIds = new HashSet<>();\n+  private boolean unCompactedAborts;\n+  private final List<HadoopShims.HdfsFileStatusWithId> originalFiles = new ArrayList<>();\n+  private final List<Path> originalDirectories = new ArrayList<>();\n+  private final List<Path> obsolete = new ArrayList<>();\n+  private final List<AcidUtils.ParsedDelta> currentDirectories = new ArrayList<>();\n+\n+  public AcidDirectory(Path path, FileSystem fs, Ref<Boolean> useFileId) {\n+    this.path = path;\n+    this.fs = fs;\n+    this.useFileId = useFileId;\n+    if (!(this.fs instanceof DistributedFileSystem) && this.useFileId != null) {\n+      this.useFileId.value = false;\n+    }\n+  }\n+\n+  public Path getPath() {\n+    return path;\n+  }\n+\n+  /**\n+   * Get the base directory path.\n+   * @return the base directory to read\n+   */\n+  public Path getBaseDirectory() {\n+    return base == null ? null : base.getBaseDirPath();\n+  }\n+\n+  /**\n+   * Get the base directory.\n+   * @return the base directory to read\n+   */\n+  public AcidUtils.ParsedBase getBase() {\n+    return base;\n+  }\n+\n+  /**\n+   * Oldest base directory in the filesystem, may be shadowed by newer base\n+   */\n+  public AcidUtils.ParsedBaseLight getOldestBase() {\n+    return oldestBase;\n+  }\n+\n+  public void setBase(AcidUtils.ParsedBase base) {\n+    this.base = base;\n+  }\n+\n+  public void setOldestBase(AcidUtils.ParsedBaseLight oldestBase) {\n+    this.oldestBase = oldestBase;\n+  }\n+\n+  public void setUnCompactedAborts(boolean unCompactedAborts) {\n+    this.unCompactedAborts = unCompactedAborts;\n+  }\n+\n+  /**\n+   * Is Base directory in raw format or in Acid format\n+   */\n+  public boolean isBaseInRawFormat() {\n+    return base != null && base.isRawFormat();\n+  }\n+\n+  /**\n+   * Get the list of original files.  Not {@code null}.  Must be sorted.\n+   * @return the list of original files (eg. 000000_0)\n+   */\n+  public List<HadoopShims.HdfsFileStatusWithId> getOriginalFiles() {\n+    return originalFiles;\n+  }\n+\n+  /**\n+   * List of original directories containing files in not ACID format\n+   */\n+  public List<Path> getOriginalDirectories() {\n+    return originalDirectories;\n+  }\n+\n+  /**\n+   * Get the list of delta directories that are valid and not\n+   * obsolete.  Not {@code null}.  List must be sorted in a specific way.\n+   * See {@link org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight#compareTo(org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight)}\n+   * for details.\n+   * @return the minimal list of current directories\n+   */\n+  public List<AcidUtils.ParsedDelta> getCurrentDirectories() {\n+    return currentDirectories;\n+  }\n+\n+  /**\n+   * Get the list of obsolete directories. After filtering out bases and\n+   * deltas that are not selected by the valid transaction/write ids list, return the\n+   * list of original files, bases, and deltas that have been replaced by\n+   * more up to date ones.  Not {@code null}.\n+   */\n+  public List<Path> getObsolete() {\n+    return obsolete;\n+  }\n+\n+  /**\n+   * Get the list of directories that has nothing but aborted transactions.\n+   * @return the list of aborted directories\n+   */\n+  public List<Path> getAbortedDirectories() {\n+    return abortedDirectories;\n+  }\n+\n+  /**\n+   * Get the list of writeIds that belong to the aborted transactions.\n+   * @return the list of aborted writeIds\n+   */\n+  public Set<Long> getAbortedWriteIds() {\n+    return abortedWriteIds;\n+  }\n+\n+  /**\n+   * Does the directory contain writeIds that belong to aborted transactions,\n+   * but are mixed together with committed writes. These aborted writes can not be cleaned.\n+   * @return true if there are aborted writes that can can be cleaned\n+   */\n+  public boolean hasUncompactedAborts() {\n+    return unCompactedAborts;\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  /**\n+   * Delete deltas that should be read by this reader.\n+   */\n+  public List<AcidUtils.ParsedDelta> getDeleteDeltas() {\n+    return currentDirectories.stream().filter(AcidUtils.ParsedDeltaLight::isDeleteDelta).collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * All original, base and delta bucket files that should be read by this reader\n+   * @throws IOException ex\n+   */\n+  public List<AcidUtils.AcidBaseFileInfo> getBaseAndDeltaFiles() throws IOException {\n+    List<AcidUtils.AcidBaseFileInfo> baseAndDeltaFiles = new ArrayList<>();\n+    if (base == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njc1NjYxMQ=="}, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 190}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNzUwNTc4OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidDirectory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxNToxMToyOVrOIJbkWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMDo0MjowOFrOIJlV5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njc1OTc2OA==", "bodyText": "It looks like this method is only used in the context of: acidDir.getBaseAndDeltaFiles().isEmpty()\nWhat if this method returned a boolean (something like isEmpty) instead?", "url": "https://github.com/apache/hive/pull/1779#discussion_r546759768", "createdAt": "2020-12-21T15:11:29Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidDirectory.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hive.shims.HadoopShims;\n+import org.apache.hive.common.util.Ref;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.hive.ql.io.AcidUtils.AcidBaseFileType.ORIGINAL_BASE;\n+\n+/**\n+ * AcidDirectory used to provide ACID directory layout information, which directories and files to read.\n+ * This representation only valid in a context of a ValidWriteIdList and ValidTxnList.\n+ */\n+public final class AcidDirectory {\n+\n+  private final Path path;\n+  private final FileSystem fs;\n+  private final Ref<Boolean> useFileId;\n+\n+  private AcidUtils.ParsedBase base;\n+  private AcidUtils.ParsedBaseLight oldestBase;\n+\n+  private final List<Path> abortedDirectories = new ArrayList<>();\n+  private final Set<Long> abortedWriteIds = new HashSet<>();\n+  private boolean unCompactedAborts;\n+  private final List<HadoopShims.HdfsFileStatusWithId> originalFiles = new ArrayList<>();\n+  private final List<Path> originalDirectories = new ArrayList<>();\n+  private final List<Path> obsolete = new ArrayList<>();\n+  private final List<AcidUtils.ParsedDelta> currentDirectories = new ArrayList<>();\n+\n+  public AcidDirectory(Path path, FileSystem fs, Ref<Boolean> useFileId) {\n+    this.path = path;\n+    this.fs = fs;\n+    this.useFileId = useFileId;\n+    if (!(this.fs instanceof DistributedFileSystem) && this.useFileId != null) {\n+      this.useFileId.value = false;\n+    }\n+  }\n+\n+  public Path getPath() {\n+    return path;\n+  }\n+\n+  /**\n+   * Get the base directory path.\n+   * @return the base directory to read\n+   */\n+  public Path getBaseDirectory() {\n+    return base == null ? null : base.getBaseDirPath();\n+  }\n+\n+  /**\n+   * Get the base directory.\n+   * @return the base directory to read\n+   */\n+  public AcidUtils.ParsedBase getBase() {\n+    return base;\n+  }\n+\n+  /**\n+   * Oldest base directory in the filesystem, may be shadowed by newer base\n+   */\n+  public AcidUtils.ParsedBaseLight getOldestBase() {\n+    return oldestBase;\n+  }\n+\n+  public void setBase(AcidUtils.ParsedBase base) {\n+    this.base = base;\n+  }\n+\n+  public void setOldestBase(AcidUtils.ParsedBaseLight oldestBase) {\n+    this.oldestBase = oldestBase;\n+  }\n+\n+  public void setUnCompactedAborts(boolean unCompactedAborts) {\n+    this.unCompactedAborts = unCompactedAborts;\n+  }\n+\n+  /**\n+   * Is Base directory in raw format or in Acid format\n+   */\n+  public boolean isBaseInRawFormat() {\n+    return base != null && base.isRawFormat();\n+  }\n+\n+  /**\n+   * Get the list of original files.  Not {@code null}.  Must be sorted.\n+   * @return the list of original files (eg. 000000_0)\n+   */\n+  public List<HadoopShims.HdfsFileStatusWithId> getOriginalFiles() {\n+    return originalFiles;\n+  }\n+\n+  /**\n+   * List of original directories containing files in not ACID format\n+   */\n+  public List<Path> getOriginalDirectories() {\n+    return originalDirectories;\n+  }\n+\n+  /**\n+   * Get the list of delta directories that are valid and not\n+   * obsolete.  Not {@code null}.  List must be sorted in a specific way.\n+   * See {@link org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight#compareTo(org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight)}\n+   * for details.\n+   * @return the minimal list of current directories\n+   */\n+  public List<AcidUtils.ParsedDelta> getCurrentDirectories() {\n+    return currentDirectories;\n+  }\n+\n+  /**\n+   * Get the list of obsolete directories. After filtering out bases and\n+   * deltas that are not selected by the valid transaction/write ids list, return the\n+   * list of original files, bases, and deltas that have been replaced by\n+   * more up to date ones.  Not {@code null}.\n+   */\n+  public List<Path> getObsolete() {\n+    return obsolete;\n+  }\n+\n+  /**\n+   * Get the list of directories that has nothing but aborted transactions.\n+   * @return the list of aborted directories\n+   */\n+  public List<Path> getAbortedDirectories() {\n+    return abortedDirectories;\n+  }\n+\n+  /**\n+   * Get the list of writeIds that belong to the aborted transactions.\n+   * @return the list of aborted writeIds\n+   */\n+  public Set<Long> getAbortedWriteIds() {\n+    return abortedWriteIds;\n+  }\n+\n+  /**\n+   * Does the directory contain writeIds that belong to aborted transactions,\n+   * but are mixed together with committed writes. These aborted writes can not be cleaned.\n+   * @return true if there are aborted writes that can can be cleaned\n+   */\n+  public boolean hasUncompactedAborts() {\n+    return unCompactedAborts;\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  /**\n+   * Delete deltas that should be read by this reader.\n+   */\n+  public List<AcidUtils.ParsedDelta> getDeleteDeltas() {\n+    return currentDirectories.stream().filter(AcidUtils.ParsedDeltaLight::isDeleteDelta).collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * All original, base and delta bucket files that should be read by this reader\n+   * @throws IOException ex\n+   */\n+  public List<AcidUtils.AcidBaseFileInfo> getBaseAndDeltaFiles() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 188}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkxOTkwOQ==", "bodyText": "No, this is the main file list that is used for split generation. It is one of the input parameter of OrcInputFormat#determineSplitStrategies.", "url": "https://github.com/apache/hive/pull/1779#discussion_r546919909", "createdAt": "2020-12-21T20:42:08Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidDirectory.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hive.shims.HadoopShims;\n+import org.apache.hive.common.util.Ref;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.hive.ql.io.AcidUtils.AcidBaseFileType.ORIGINAL_BASE;\n+\n+/**\n+ * AcidDirectory used to provide ACID directory layout information, which directories and files to read.\n+ * This representation only valid in a context of a ValidWriteIdList and ValidTxnList.\n+ */\n+public final class AcidDirectory {\n+\n+  private final Path path;\n+  private final FileSystem fs;\n+  private final Ref<Boolean> useFileId;\n+\n+  private AcidUtils.ParsedBase base;\n+  private AcidUtils.ParsedBaseLight oldestBase;\n+\n+  private final List<Path> abortedDirectories = new ArrayList<>();\n+  private final Set<Long> abortedWriteIds = new HashSet<>();\n+  private boolean unCompactedAborts;\n+  private final List<HadoopShims.HdfsFileStatusWithId> originalFiles = new ArrayList<>();\n+  private final List<Path> originalDirectories = new ArrayList<>();\n+  private final List<Path> obsolete = new ArrayList<>();\n+  private final List<AcidUtils.ParsedDelta> currentDirectories = new ArrayList<>();\n+\n+  public AcidDirectory(Path path, FileSystem fs, Ref<Boolean> useFileId) {\n+    this.path = path;\n+    this.fs = fs;\n+    this.useFileId = useFileId;\n+    if (!(this.fs instanceof DistributedFileSystem) && this.useFileId != null) {\n+      this.useFileId.value = false;\n+    }\n+  }\n+\n+  public Path getPath() {\n+    return path;\n+  }\n+\n+  /**\n+   * Get the base directory path.\n+   * @return the base directory to read\n+   */\n+  public Path getBaseDirectory() {\n+    return base == null ? null : base.getBaseDirPath();\n+  }\n+\n+  /**\n+   * Get the base directory.\n+   * @return the base directory to read\n+   */\n+  public AcidUtils.ParsedBase getBase() {\n+    return base;\n+  }\n+\n+  /**\n+   * Oldest base directory in the filesystem, may be shadowed by newer base\n+   */\n+  public AcidUtils.ParsedBaseLight getOldestBase() {\n+    return oldestBase;\n+  }\n+\n+  public void setBase(AcidUtils.ParsedBase base) {\n+    this.base = base;\n+  }\n+\n+  public void setOldestBase(AcidUtils.ParsedBaseLight oldestBase) {\n+    this.oldestBase = oldestBase;\n+  }\n+\n+  public void setUnCompactedAborts(boolean unCompactedAborts) {\n+    this.unCompactedAborts = unCompactedAborts;\n+  }\n+\n+  /**\n+   * Is Base directory in raw format or in Acid format\n+   */\n+  public boolean isBaseInRawFormat() {\n+    return base != null && base.isRawFormat();\n+  }\n+\n+  /**\n+   * Get the list of original files.  Not {@code null}.  Must be sorted.\n+   * @return the list of original files (eg. 000000_0)\n+   */\n+  public List<HadoopShims.HdfsFileStatusWithId> getOriginalFiles() {\n+    return originalFiles;\n+  }\n+\n+  /**\n+   * List of original directories containing files in not ACID format\n+   */\n+  public List<Path> getOriginalDirectories() {\n+    return originalDirectories;\n+  }\n+\n+  /**\n+   * Get the list of delta directories that are valid and not\n+   * obsolete.  Not {@code null}.  List must be sorted in a specific way.\n+   * See {@link org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight#compareTo(org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight)}\n+   * for details.\n+   * @return the minimal list of current directories\n+   */\n+  public List<AcidUtils.ParsedDelta> getCurrentDirectories() {\n+    return currentDirectories;\n+  }\n+\n+  /**\n+   * Get the list of obsolete directories. After filtering out bases and\n+   * deltas that are not selected by the valid transaction/write ids list, return the\n+   * list of original files, bases, and deltas that have been replaced by\n+   * more up to date ones.  Not {@code null}.\n+   */\n+  public List<Path> getObsolete() {\n+    return obsolete;\n+  }\n+\n+  /**\n+   * Get the list of directories that has nothing but aborted transactions.\n+   * @return the list of aborted directories\n+   */\n+  public List<Path> getAbortedDirectories() {\n+    return abortedDirectories;\n+  }\n+\n+  /**\n+   * Get the list of writeIds that belong to the aborted transactions.\n+   * @return the list of aborted writeIds\n+   */\n+  public Set<Long> getAbortedWriteIds() {\n+    return abortedWriteIds;\n+  }\n+\n+  /**\n+   * Does the directory contain writeIds that belong to aborted transactions,\n+   * but are mixed together with committed writes. These aborted writes can not be cleaned.\n+   * @return true if there are aborted writes that can can be cleaned\n+   */\n+  public boolean hasUncompactedAborts() {\n+    return unCompactedAborts;\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  /**\n+   * Delete deltas that should be read by this reader.\n+   */\n+  public List<AcidUtils.ParsedDelta> getDeleteDeltas() {\n+    return currentDirectories.stream().filter(AcidUtils.ParsedDeltaLight::isDeleteDelta).collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * All original, base and delta bucket files that should be read by this reader\n+   * @throws IOException ex\n+   */\n+  public List<AcidUtils.AcidBaseFileInfo> getBaseAndDeltaFiles() throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njc1OTc2OA=="}, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 188}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNzU3NTI3OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidDirectory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxNToyOTo1M1rOIJcMtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwOToxNTozNlrOIJz-nQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njc3MDEwMg==", "bodyText": "consider including isBaseInRawFormat as before; maybe also abortedWriteIds.", "url": "https://github.com/apache/hive/pull/1779#discussion_r546770102", "createdAt": "2020-12-21T15:29:53Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidDirectory.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hive.shims.HadoopShims;\n+import org.apache.hive.common.util.Ref;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.hive.ql.io.AcidUtils.AcidBaseFileType.ORIGINAL_BASE;\n+\n+/**\n+ * AcidDirectory used to provide ACID directory layout information, which directories and files to read.\n+ * This representation only valid in a context of a ValidWriteIdList and ValidTxnList.\n+ */\n+public final class AcidDirectory {\n+\n+  private final Path path;\n+  private final FileSystem fs;\n+  private final Ref<Boolean> useFileId;\n+\n+  private AcidUtils.ParsedBase base;\n+  private AcidUtils.ParsedBaseLight oldestBase;\n+\n+  private final List<Path> abortedDirectories = new ArrayList<>();\n+  private final Set<Long> abortedWriteIds = new HashSet<>();\n+  private boolean unCompactedAborts;\n+  private final List<HadoopShims.HdfsFileStatusWithId> originalFiles = new ArrayList<>();\n+  private final List<Path> originalDirectories = new ArrayList<>();\n+  private final List<Path> obsolete = new ArrayList<>();\n+  private final List<AcidUtils.ParsedDelta> currentDirectories = new ArrayList<>();\n+\n+  public AcidDirectory(Path path, FileSystem fs, Ref<Boolean> useFileId) {\n+    this.path = path;\n+    this.fs = fs;\n+    this.useFileId = useFileId;\n+    if (!(this.fs instanceof DistributedFileSystem) && this.useFileId != null) {\n+      this.useFileId.value = false;\n+    }\n+  }\n+\n+  public Path getPath() {\n+    return path;\n+  }\n+\n+  /**\n+   * Get the base directory path.\n+   * @return the base directory to read\n+   */\n+  public Path getBaseDirectory() {\n+    return base == null ? null : base.getBaseDirPath();\n+  }\n+\n+  /**\n+   * Get the base directory.\n+   * @return the base directory to read\n+   */\n+  public AcidUtils.ParsedBase getBase() {\n+    return base;\n+  }\n+\n+  /**\n+   * Oldest base directory in the filesystem, may be shadowed by newer base\n+   */\n+  public AcidUtils.ParsedBaseLight getOldestBase() {\n+    return oldestBase;\n+  }\n+\n+  public void setBase(AcidUtils.ParsedBase base) {\n+    this.base = base;\n+  }\n+\n+  public void setOldestBase(AcidUtils.ParsedBaseLight oldestBase) {\n+    this.oldestBase = oldestBase;\n+  }\n+\n+  public void setUnCompactedAborts(boolean unCompactedAborts) {\n+    this.unCompactedAborts = unCompactedAborts;\n+  }\n+\n+  /**\n+   * Is Base directory in raw format or in Acid format\n+   */\n+  public boolean isBaseInRawFormat() {\n+    return base != null && base.isRawFormat();\n+  }\n+\n+  /**\n+   * Get the list of original files.  Not {@code null}.  Must be sorted.\n+   * @return the list of original files (eg. 000000_0)\n+   */\n+  public List<HadoopShims.HdfsFileStatusWithId> getOriginalFiles() {\n+    return originalFiles;\n+  }\n+\n+  /**\n+   * List of original directories containing files in not ACID format\n+   */\n+  public List<Path> getOriginalDirectories() {\n+    return originalDirectories;\n+  }\n+\n+  /**\n+   * Get the list of delta directories that are valid and not\n+   * obsolete.  Not {@code null}.  List must be sorted in a specific way.\n+   * See {@link org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight#compareTo(org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight)}\n+   * for details.\n+   * @return the minimal list of current directories\n+   */\n+  public List<AcidUtils.ParsedDelta> getCurrentDirectories() {\n+    return currentDirectories;\n+  }\n+\n+  /**\n+   * Get the list of obsolete directories. After filtering out bases and\n+   * deltas that are not selected by the valid transaction/write ids list, return the\n+   * list of original files, bases, and deltas that have been replaced by\n+   * more up to date ones.  Not {@code null}.\n+   */\n+  public List<Path> getObsolete() {\n+    return obsolete;\n+  }\n+\n+  /**\n+   * Get the list of directories that has nothing but aborted transactions.\n+   * @return the list of aborted directories\n+   */\n+  public List<Path> getAbortedDirectories() {\n+    return abortedDirectories;\n+  }\n+\n+  /**\n+   * Get the list of writeIds that belong to the aborted transactions.\n+   * @return the list of aborted writeIds\n+   */\n+  public Set<Long> getAbortedWriteIds() {\n+    return abortedWriteIds;\n+  }\n+\n+  /**\n+   * Does the directory contain writeIds that belong to aborted transactions,\n+   * but are mixed together with committed writes. These aborted writes can not be cleaned.\n+   * @return true if there are aborted writes that can can be cleaned\n+   */\n+  public boolean hasUncompactedAborts() {\n+    return unCompactedAborts;\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  /**\n+   * Delete deltas that should be read by this reader.\n+   */\n+  public List<AcidUtils.ParsedDelta> getDeleteDeltas() {\n+    return currentDirectories.stream().filter(AcidUtils.ParsedDeltaLight::isDeleteDelta).collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * All original, base and delta bucket files that should be read by this reader\n+   * @throws IOException ex\n+   */\n+  public List<AcidUtils.AcidBaseFileInfo> getBaseAndDeltaFiles() throws IOException {\n+    List<AcidUtils.AcidBaseFileInfo> baseAndDeltaFiles = new ArrayList<>();\n+    if (base == null) {\n+      // For non-acid tables (or paths), all data files are in getOriginalFiles() list\n+      for (HadoopShims.HdfsFileStatusWithId fileId : originalFiles) {\n+        baseAndDeltaFiles.add(new AcidUtils.AcidBaseFileInfo(fileId, ORIGINAL_BASE));\n+      }\n+    } else {\n+      // The base files are either listed in ParsedBase or will be populated now\n+      for (HadoopShims.HdfsFileStatusWithId fileId : base.getFiles(fs, useFileId)) {\n+        baseAndDeltaFiles.add(new AcidUtils.AcidBaseFileInfo(fileId,\n+            isBaseInRawFormat() ? ORIGINAL_BASE : AcidUtils.AcidBaseFileType.ACID_SCHEMA));\n+      }\n+    }\n+    for (AcidUtils.ParsedDelta parsedDelta : currentDirectories) {\n+      if (parsedDelta.isDeleteDelta()) {\n+        continue;\n+      }\n+      if (parsedDelta.isRawFormat() && parsedDelta.getMinWriteId() != parsedDelta.getMaxWriteId()) {\n+        // delta/ with files in raw format are a result of Load Data (as opposed to compaction\n+        // or streaming ingest so must have interval length == 1.\n+        throw new IllegalStateException(\n+            \"Delta in \" + ORIGINAL_BASE + \" format but txnIds are out of range: \" + parsedDelta.getPath());\n+      }\n+\n+      AcidUtils.AcidBaseFileType deltaType =\n+          parsedDelta.isRawFormat() ? ORIGINAL_BASE : AcidUtils.AcidBaseFileType.ACID_SCHEMA;\n+      // The bucket files are either listed in ParsedDelta or will be populated now\n+      for (HadoopShims.HdfsFileStatusWithId deltaFile : parsedDelta.getFiles(fs, useFileId)) {\n+        baseAndDeltaFiles.add(new AcidUtils.AcidBaseFileInfo(deltaFile, deltaType));\n+      }\n+    }\n+\n+    return baseAndDeltaFiles;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return \"Aborted Directories: \" + abortedDirectories + \"; original: \" + originalFiles + \"; obsolete: \" + obsolete\n+        + \"; currentDirectories: \" + currentDirectories + \"; base: \" + base;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 227}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE1OTcwOQ==", "bodyText": "Will do that, I will add the rawformat to ParsedBase#toString, this will print that too.", "url": "https://github.com/apache/hive/pull/1779#discussion_r547159709", "createdAt": "2020-12-22T09:15:36Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidDirectory.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.DistributedFileSystem;\n+import org.apache.hadoop.hive.shims.HadoopShims;\n+import org.apache.hive.common.util.Ref;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.hadoop.hive.ql.io.AcidUtils.AcidBaseFileType.ORIGINAL_BASE;\n+\n+/**\n+ * AcidDirectory used to provide ACID directory layout information, which directories and files to read.\n+ * This representation only valid in a context of a ValidWriteIdList and ValidTxnList.\n+ */\n+public final class AcidDirectory {\n+\n+  private final Path path;\n+  private final FileSystem fs;\n+  private final Ref<Boolean> useFileId;\n+\n+  private AcidUtils.ParsedBase base;\n+  private AcidUtils.ParsedBaseLight oldestBase;\n+\n+  private final List<Path> abortedDirectories = new ArrayList<>();\n+  private final Set<Long> abortedWriteIds = new HashSet<>();\n+  private boolean unCompactedAborts;\n+  private final List<HadoopShims.HdfsFileStatusWithId> originalFiles = new ArrayList<>();\n+  private final List<Path> originalDirectories = new ArrayList<>();\n+  private final List<Path> obsolete = new ArrayList<>();\n+  private final List<AcidUtils.ParsedDelta> currentDirectories = new ArrayList<>();\n+\n+  public AcidDirectory(Path path, FileSystem fs, Ref<Boolean> useFileId) {\n+    this.path = path;\n+    this.fs = fs;\n+    this.useFileId = useFileId;\n+    if (!(this.fs instanceof DistributedFileSystem) && this.useFileId != null) {\n+      this.useFileId.value = false;\n+    }\n+  }\n+\n+  public Path getPath() {\n+    return path;\n+  }\n+\n+  /**\n+   * Get the base directory path.\n+   * @return the base directory to read\n+   */\n+  public Path getBaseDirectory() {\n+    return base == null ? null : base.getBaseDirPath();\n+  }\n+\n+  /**\n+   * Get the base directory.\n+   * @return the base directory to read\n+   */\n+  public AcidUtils.ParsedBase getBase() {\n+    return base;\n+  }\n+\n+  /**\n+   * Oldest base directory in the filesystem, may be shadowed by newer base\n+   */\n+  public AcidUtils.ParsedBaseLight getOldestBase() {\n+    return oldestBase;\n+  }\n+\n+  public void setBase(AcidUtils.ParsedBase base) {\n+    this.base = base;\n+  }\n+\n+  public void setOldestBase(AcidUtils.ParsedBaseLight oldestBase) {\n+    this.oldestBase = oldestBase;\n+  }\n+\n+  public void setUnCompactedAborts(boolean unCompactedAborts) {\n+    this.unCompactedAborts = unCompactedAborts;\n+  }\n+\n+  /**\n+   * Is Base directory in raw format or in Acid format\n+   */\n+  public boolean isBaseInRawFormat() {\n+    return base != null && base.isRawFormat();\n+  }\n+\n+  /**\n+   * Get the list of original files.  Not {@code null}.  Must be sorted.\n+   * @return the list of original files (eg. 000000_0)\n+   */\n+  public List<HadoopShims.HdfsFileStatusWithId> getOriginalFiles() {\n+    return originalFiles;\n+  }\n+\n+  /**\n+   * List of original directories containing files in not ACID format\n+   */\n+  public List<Path> getOriginalDirectories() {\n+    return originalDirectories;\n+  }\n+\n+  /**\n+   * Get the list of delta directories that are valid and not\n+   * obsolete.  Not {@code null}.  List must be sorted in a specific way.\n+   * See {@link org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight#compareTo(org.apache.hadoop.hive.ql.io.AcidUtils.ParsedDeltaLight)}\n+   * for details.\n+   * @return the minimal list of current directories\n+   */\n+  public List<AcidUtils.ParsedDelta> getCurrentDirectories() {\n+    return currentDirectories;\n+  }\n+\n+  /**\n+   * Get the list of obsolete directories. After filtering out bases and\n+   * deltas that are not selected by the valid transaction/write ids list, return the\n+   * list of original files, bases, and deltas that have been replaced by\n+   * more up to date ones.  Not {@code null}.\n+   */\n+  public List<Path> getObsolete() {\n+    return obsolete;\n+  }\n+\n+  /**\n+   * Get the list of directories that has nothing but aborted transactions.\n+   * @return the list of aborted directories\n+   */\n+  public List<Path> getAbortedDirectories() {\n+    return abortedDirectories;\n+  }\n+\n+  /**\n+   * Get the list of writeIds that belong to the aborted transactions.\n+   * @return the list of aborted writeIds\n+   */\n+  public Set<Long> getAbortedWriteIds() {\n+    return abortedWriteIds;\n+  }\n+\n+  /**\n+   * Does the directory contain writeIds that belong to aborted transactions,\n+   * but are mixed together with committed writes. These aborted writes can not be cleaned.\n+   * @return true if there are aborted writes that can can be cleaned\n+   */\n+  public boolean hasUncompactedAborts() {\n+    return unCompactedAborts;\n+  }\n+\n+  public FileSystem getFs() {\n+    return fs;\n+  }\n+\n+  /**\n+   * Delete deltas that should be read by this reader.\n+   */\n+  public List<AcidUtils.ParsedDelta> getDeleteDeltas() {\n+    return currentDirectories.stream().filter(AcidUtils.ParsedDeltaLight::isDeleteDelta).collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * All original, base and delta bucket files that should be read by this reader\n+   * @throws IOException ex\n+   */\n+  public List<AcidUtils.AcidBaseFileInfo> getBaseAndDeltaFiles() throws IOException {\n+    List<AcidUtils.AcidBaseFileInfo> baseAndDeltaFiles = new ArrayList<>();\n+    if (base == null) {\n+      // For non-acid tables (or paths), all data files are in getOriginalFiles() list\n+      for (HadoopShims.HdfsFileStatusWithId fileId : originalFiles) {\n+        baseAndDeltaFiles.add(new AcidUtils.AcidBaseFileInfo(fileId, ORIGINAL_BASE));\n+      }\n+    } else {\n+      // The base files are either listed in ParsedBase or will be populated now\n+      for (HadoopShims.HdfsFileStatusWithId fileId : base.getFiles(fs, useFileId)) {\n+        baseAndDeltaFiles.add(new AcidUtils.AcidBaseFileInfo(fileId,\n+            isBaseInRawFormat() ? ORIGINAL_BASE : AcidUtils.AcidBaseFileType.ACID_SCHEMA));\n+      }\n+    }\n+    for (AcidUtils.ParsedDelta parsedDelta : currentDirectories) {\n+      if (parsedDelta.isDeleteDelta()) {\n+        continue;\n+      }\n+      if (parsedDelta.isRawFormat() && parsedDelta.getMinWriteId() != parsedDelta.getMaxWriteId()) {\n+        // delta/ with files in raw format are a result of Load Data (as opposed to compaction\n+        // or streaming ingest so must have interval length == 1.\n+        throw new IllegalStateException(\n+            \"Delta in \" + ORIGINAL_BASE + \" format but txnIds are out of range: \" + parsedDelta.getPath());\n+      }\n+\n+      AcidUtils.AcidBaseFileType deltaType =\n+          parsedDelta.isRawFormat() ? ORIGINAL_BASE : AcidUtils.AcidBaseFileType.ACID_SCHEMA;\n+      // The bucket files are either listed in ParsedDelta or will be populated now\n+      for (HadoopShims.HdfsFileStatusWithId deltaFile : parsedDelta.getFiles(fs, useFileId)) {\n+        baseAndDeltaFiles.add(new AcidUtils.AcidBaseFileInfo(deltaFile, deltaType));\n+      }\n+    }\n+\n+    return baseAndDeltaFiles;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return \"Aborted Directories: \" + abortedDirectories + \"; original: \" + originalFiles + \"; obsolete: \" + obsolete\n+        + \"; currentDirectories: \" + currentDirectories + \"; base: \" + base;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njc3MDEwMg=="}, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 227}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNzczMjg2OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxNjoxMTowOFrOIJdoyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMDo1MzoxNFrOIJlmig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njc5MzY3Mw==", "bodyText": "Seems like this might kind of duplicate AcidBaseFileInfo? If so we can get rid of AcidBaseFileInfo?", "url": "https://github.com/apache/hive/pull/1779#discussion_r546793673", "createdAt": "2020-12-21T16:11:08Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -994,18 +857,76 @@ public long getVisibilityTxnId() {\n     public Path getBaseDirPath() {\n       return baseDirPath;\n     }\n-    public static ParsedBase parseBase(Path path) {\n+\n+\n+\n+    public static ParsedBaseLight parseBase(Path path) {\n       String filename = path.getName();\n       if(!filename.startsWith(BASE_PREFIX)) {\n         throw new IllegalArgumentException(filename + \" does not start with \" + BASE_PREFIX);\n       }\n       int idxOfv = filename.indexOf(VISIBILITY_PREFIX);\n       if(idxOfv < 0) {\n-        return new ParsedBase(Long.parseLong(filename.substring(BASE_PREFIX.length())), path);\n+        return new ParsedBaseLight(Long.parseLong(filename.substring(BASE_PREFIX.length())), path);\n       }\n-      return new ParsedBase(Long.parseLong(filename.substring(BASE_PREFIX.length(), idxOfv)),\n+      return new ParsedBaseLight(Long.parseLong(filename.substring(BASE_PREFIX.length(), idxOfv)),\n           Long.parseLong(filename.substring(idxOfv + VISIBILITY_PREFIX.length())), path);\n     }\n+\n+    @Override\n+    public String toString() {\n+      return \"Path: \" + baseDirPath + \"; writeId: \"\n+          + writeId + \"; visibilityTxnId: \" + visibilityTxnId;\n+    }\n+  }\n+  /**\n+   * In addition to {@link ParsedBaseLight} this knows if the data is in raw format, i.e. doesn't\n+   * have acid metadata columns embedded in the files.  To determine this in some cases\n+   * requires looking at the footer of the data file which can be expensive so if this info is\n+   * not needed {@link ParsedBaseLight} should be used.\n+   */\n+  public static final class ParsedBase extends ParsedBaseLight {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 230}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkyNDE3MA==", "bodyText": "ParsedBase represents a base directory with many files. AcidBaseFileInfo name is rather confusing for me, but it represent any datafile that could be in an acid table (original, bucketfile in base, bucketfile in delta) These are the \"base\" files for orc splits.", "url": "https://github.com/apache/hive/pull/1779#discussion_r546924170", "createdAt": "2020-12-21T20:53:14Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -994,18 +857,76 @@ public long getVisibilityTxnId() {\n     public Path getBaseDirPath() {\n       return baseDirPath;\n     }\n-    public static ParsedBase parseBase(Path path) {\n+\n+\n+\n+    public static ParsedBaseLight parseBase(Path path) {\n       String filename = path.getName();\n       if(!filename.startsWith(BASE_PREFIX)) {\n         throw new IllegalArgumentException(filename + \" does not start with \" + BASE_PREFIX);\n       }\n       int idxOfv = filename.indexOf(VISIBILITY_PREFIX);\n       if(idxOfv < 0) {\n-        return new ParsedBase(Long.parseLong(filename.substring(BASE_PREFIX.length())), path);\n+        return new ParsedBaseLight(Long.parseLong(filename.substring(BASE_PREFIX.length())), path);\n       }\n-      return new ParsedBase(Long.parseLong(filename.substring(BASE_PREFIX.length(), idxOfv)),\n+      return new ParsedBaseLight(Long.parseLong(filename.substring(BASE_PREFIX.length(), idxOfv)),\n           Long.parseLong(filename.substring(idxOfv + VISIBILITY_PREFIX.length())), path);\n     }\n+\n+    @Override\n+    public String toString() {\n+      return \"Path: \" + baseDirPath + \"; writeId: \"\n+          + writeId + \"; visibilityTxnId: \" + visibilityTxnId;\n+    }\n+  }\n+  /**\n+   * In addition to {@link ParsedBaseLight} this knows if the data is in raw format, i.e. doesn't\n+   * have acid metadata columns embedded in the files.  To determine this in some cases\n+   * requires looking at the footer of the data file which can be expensive so if this info is\n+   * not needed {@link ParsedBaseLight} should be used.\n+   */\n+  public static final class ParsedBase extends ParsedBaseLight {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njc5MzY3Mw=="}, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 230}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNzc3NTM1OnYy", "diffSide": "LEFT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxNjoyMzoyMVrOIJeCig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMTowMjowNVrOIJl0FQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjgwMDI2Ng==", "bodyText": "Why was this block (1260-1268) needed originally?", "url": "https://github.com/apache/hive/pull/1779#discussion_r546800266", "createdAt": "2020-12-21T16:23:21Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -1253,19 +1190,12 @@ public static ParsedDelta parsedDelta(Path deltaDir, String deltaPrefix, FileSys\n       ParsedDelta p = parsedDelta(deltaDir, isRawFormat);\n       List<HdfsFileStatusWithId> files = null;\n       if (dirSnapshot != null) {\n+        final PathFilter filter = isRawFormat ? AcidUtils.originalBucketFilter : AcidUtils.bucketFileFilter;\n+        // If we already know the files, store it for future use\n         files = dirSnapshot.getFiles().stream()\n-            .filter(fileStatus -> bucketFileFilter.accept(fileStatus.getPath()))\n+            .filter(fileStatus -> filter.accept(fileStatus.getPath()))\n             .map(HdfsFileStatusWithoutId::new)\n             .collect(Collectors.toList());\n-      } else if (isDeleteDelta) {\n-        // For delete deltas we need the files for AcidState\n-        try {\n-          files = SHIMS.listLocatedHdfsStatus(fs, deltaDir, bucketFileFilter);\n-        } catch (UnsupportedOperationException uoe) {\n-          files = Arrays.stream(fs.listStatus(deltaDir, bucketFileFilter))\n-              .map(HdfsFileStatusWithoutId::new)\n-              .collect(Collectors.toList());\n-        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 392}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkyNzYzNw==", "bodyText": "For delete delta folders the metadata for all files will be send to the execution side, so the FileId could be used to retrieve the orctail - and with the new feature of Adam - the whole file from llap cache. This was here to ensure the file list is propagated even in HDFS where we don't have anything in directory snapshot. Now this functionality is hidden inside ParsedDelta#getFiles it will either return the list from cache or do the FileSystem call itself if it was not done before", "url": "https://github.com/apache/hive/pull/1779#discussion_r546927637", "createdAt": "2020-12-21T21:02:05Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -1253,19 +1190,12 @@ public static ParsedDelta parsedDelta(Path deltaDir, String deltaPrefix, FileSys\n       ParsedDelta p = parsedDelta(deltaDir, isRawFormat);\n       List<HdfsFileStatusWithId> files = null;\n       if (dirSnapshot != null) {\n+        final PathFilter filter = isRawFormat ? AcidUtils.originalBucketFilter : AcidUtils.bucketFileFilter;\n+        // If we already know the files, store it for future use\n         files = dirSnapshot.getFiles().stream()\n-            .filter(fileStatus -> bucketFileFilter.accept(fileStatus.getPath()))\n+            .filter(fileStatus -> filter.accept(fileStatus.getPath()))\n             .map(HdfsFileStatusWithoutId::new)\n             .collect(Collectors.toList());\n-      } else if (isDeleteDelta) {\n-        // For delete deltas we need the files for AcidState\n-        try {\n-          files = SHIMS.listLocatedHdfsStatus(fs, deltaDir, bucketFileFilter);\n-        } catch (UnsupportedOperationException uoe) {\n-          files = Arrays.stream(fs.listStatus(deltaDir, bucketFileFilter))\n-              .map(HdfsFileStatusWithoutId::new)\n-              .collect(Collectors.toList());\n-        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjgwMDI2Ng=="}, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 392}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNzc4NTg1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxNjoyNjowNlrOIJeInQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQwOTo0Mjo1OVrOIJ01bA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjgwMTgyMQ==", "bodyText": "getChildState parameters have been changed", "url": "https://github.com/apache/hive/pull/1779#discussion_r546801821", "createdAt": "2020-12-21T16:26:06Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -1490,49 +1432,31 @@ else if (prev != null && next.maxWriteId == prev.maxWriteId\n         prev = next;\n       }\n       else {\n-        obsolete.add(next.path);\n+        directory.getObsolete().add(next.path);\n       }\n     }\n+    directory.getCurrentDirectories().clear();\n+    directory.getCurrentDirectories().addAll(deltas);\n+  }\n \n-    if(bestBase.oldestBase != null && bestBase.basePath == null &&\n-        isCompactedBase(ParsedBase.parseBase(bestBase.oldestBase), fs, dirSnapshots)) {\n+  private static ValidTxnList getValidTxnList(Configuration conf) {\n+    ValidTxnList validTxnList = null;\n+    String s = conf.get(ValidTxnList.VALID_TXNS_KEY);\n+    if(!Strings.isNullOrEmpty(s)) {\n       /*\n-       * If here, it means there was a base_x (> 1 perhaps) but none were suitable for given\n-       * {@link writeIdList}.  Note that 'original' files are logically a base_Long.MIN_VALUE and thus\n-       * cannot have any data for an open txn.  We could check {@link deltas} has files to cover\n-       * [1,n] w/o gaps but this would almost never happen...\n+       * getAcidState() is sometimes called on non-transactional tables, e.g.\n+       * OrcInputFileFormat.FileGenerator.callInternal().  e.g. orc_merge3.q In that case\n+       * writeIdList is bogus - doesn't even have a table name.\n+       * see https://issues.apache.org/jira/browse/HIVE-20856.\n        *\n-       * We only throw for base_x produced by Compactor since that base erases all history and\n-       * cannot be used for a client that has a snapshot in which something inside this base is\n-       * open.  (Nor can we ignore this base of course)  But base_x which is a result of IOW,\n-       * contains all history so we treat it just like delta wrt visibility.  Imagine, IOW which\n-       * aborts. It creates a base_x, which can and should just be ignored.*/\n-      long[] exceptions = writeIdList.getInvalidWriteIds();\n-      String minOpenWriteId = exceptions != null && exceptions.length > 0 ?\n-        Long.toString(exceptions[0]) : \"x\";\n-      throw new IOException(ErrorMsg.ACID_NOT_ENOUGH_HISTORY.format(\n-        Long.toString(writeIdList.getHighWatermark()),\n-              minOpenWriteId, bestBase.oldestBase.toString()));\n-    }\n-\n-    Path base = null;\n-    boolean isBaseInRawFormat = false;\n-    if (bestBase.basePath != null) {\n-      base = bestBase.basePath;\n-      isBaseInRawFormat = MetaDataFile.isRawFormat(base, fs, dirSnapshots != null ? dirSnapshots.get(base) : null);\n+       * For now, assert that ValidTxnList.VALID_TXNS_KEY is set only if this is really a read\n+       * of a transactional table.\n+       * see {@link #getChildState(FileStatus, HdfsFileStatusWithId, ValidWriteIdList, List, List, List, List, TxnBase, boolean, List, Map, FileSystem, ValidTxnList)}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 615}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzE3Mzc0MA==", "bodyText": "Removed this link, because it is outdated, i don't see anything related to this in getChildState.", "url": "https://github.com/apache/hive/pull/1779#discussion_r547173740", "createdAt": "2020-12-22T09:42:59Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -1490,49 +1432,31 @@ else if (prev != null && next.maxWriteId == prev.maxWriteId\n         prev = next;\n       }\n       else {\n-        obsolete.add(next.path);\n+        directory.getObsolete().add(next.path);\n       }\n     }\n+    directory.getCurrentDirectories().clear();\n+    directory.getCurrentDirectories().addAll(deltas);\n+  }\n \n-    if(bestBase.oldestBase != null && bestBase.basePath == null &&\n-        isCompactedBase(ParsedBase.parseBase(bestBase.oldestBase), fs, dirSnapshots)) {\n+  private static ValidTxnList getValidTxnList(Configuration conf) {\n+    ValidTxnList validTxnList = null;\n+    String s = conf.get(ValidTxnList.VALID_TXNS_KEY);\n+    if(!Strings.isNullOrEmpty(s)) {\n       /*\n-       * If here, it means there was a base_x (> 1 perhaps) but none were suitable for given\n-       * {@link writeIdList}.  Note that 'original' files are logically a base_Long.MIN_VALUE and thus\n-       * cannot have any data for an open txn.  We could check {@link deltas} has files to cover\n-       * [1,n] w/o gaps but this would almost never happen...\n+       * getAcidState() is sometimes called on non-transactional tables, e.g.\n+       * OrcInputFileFormat.FileGenerator.callInternal().  e.g. orc_merge3.q In that case\n+       * writeIdList is bogus - doesn't even have a table name.\n+       * see https://issues.apache.org/jira/browse/HIVE-20856.\n        *\n-       * We only throw for base_x produced by Compactor since that base erases all history and\n-       * cannot be used for a client that has a snapshot in which something inside this base is\n-       * open.  (Nor can we ignore this base of course)  But base_x which is a result of IOW,\n-       * contains all history so we treat it just like delta wrt visibility.  Imagine, IOW which\n-       * aborts. It creates a base_x, which can and should just be ignored.*/\n-      long[] exceptions = writeIdList.getInvalidWriteIds();\n-      String minOpenWriteId = exceptions != null && exceptions.length > 0 ?\n-        Long.toString(exceptions[0]) : \"x\";\n-      throw new IOException(ErrorMsg.ACID_NOT_ENOUGH_HISTORY.format(\n-        Long.toString(writeIdList.getHighWatermark()),\n-              minOpenWriteId, bestBase.oldestBase.toString()));\n-    }\n-\n-    Path base = null;\n-    boolean isBaseInRawFormat = false;\n-    if (bestBase.basePath != null) {\n-      base = bestBase.basePath;\n-      isBaseInRawFormat = MetaDataFile.isRawFormat(base, fs, dirSnapshots != null ? dirSnapshots.get(base) : null);\n+       * For now, assert that ValidTxnList.VALID_TXNS_KEY is set only if this is really a read\n+       * of a transactional table.\n+       * see {@link #getChildState(FileStatus, HdfsFileStatusWithId, ValidWriteIdList, List, List, List, List, TxnBase, boolean, List, Map, FileSystem, ValidTxnList)}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjgwMTgyMQ=="}, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 615}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNzgyMzIwOnYy", "diffSide": "LEFT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxNjozNjoyNFrOIJeewQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQyMToxMTozNlrOIJmCnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjgwNzQ4OQ==", "bodyText": "Why was this here?", "url": "https://github.com/apache/hive/pull/1779#discussion_r546807489", "createdAt": "2020-12-21T16:36:24Z", "author": {"login": "klcopp"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -3360,14 +3257,13 @@ public static Directory getAcidStateFromCache(Supplier<FileSystem> fileSystem,\n \n     // compute and add to cache\n     if (recompute || (value == null)) {\n-      Directory dirInfo = getAcidState(fileSystem.get(), candidateDirectory, conf,\n+      AcidDirectory dirInfo = getAcidState(fileSystem.get(), candidateDirectory, conf,\n           writeIdList, useFileIds, ignoreEmptyFiles);\n       value = new DirInfoValue(writeIdList.writeToString(), dirInfo);\n \n       if (value.dirInfo != null && value.dirInfo.getBaseDirectory() != null\n           && value.dirInfo.getCurrentDirectories().isEmpty()) {\n         if (dirCacheDuration > 0) {\n-          populateBaseFiles(dirInfo, useFileIds, fileSystem);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 988}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjkzMTM1Nw==", "bodyText": "The basefiles were populated and stored in cache, so they could be later used in OrcInputFormat if no write happens to the table before the next query. This is not necessary now, because ParsedBase will handle this when getFiles is called, if the baseFiles are not there it will do the listing once and store it. What is even better, if we are on S3 the files will be there already from the first recursive listing, so the FS is not even called once.", "url": "https://github.com/apache/hive/pull/1779#discussion_r546931357", "createdAt": "2020-12-21T21:11:36Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -3360,14 +3257,13 @@ public static Directory getAcidStateFromCache(Supplier<FileSystem> fileSystem,\n \n     // compute and add to cache\n     if (recompute || (value == null)) {\n-      Directory dirInfo = getAcidState(fileSystem.get(), candidateDirectory, conf,\n+      AcidDirectory dirInfo = getAcidState(fileSystem.get(), candidateDirectory, conf,\n           writeIdList, useFileIds, ignoreEmptyFiles);\n       value = new DirInfoValue(writeIdList.writeToString(), dirInfo);\n \n       if (value.dirInfo != null && value.dirInfo.getBaseDirectory() != null\n           && value.dirInfo.getCurrentDirectories().isEmpty()) {\n         if (dirCacheDuration > 0) {\n-          populateBaseFiles(dirInfo, useFileIds, fileSystem);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjgwNzQ4OQ=="}, "originalCommit": {"oid": "028112e650a8f22ddf1ae5d6e87a6484c147c81b"}, "originalPosition": 988}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 157, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}