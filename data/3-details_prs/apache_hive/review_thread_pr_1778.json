{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM5Njg5MjY2", "number": 1778, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QwOTozNToxOFrOFxajJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMDowMjowMFrOFxbQ9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3MzU5NTI1OnYy", "diffSide": "RIGHT", "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/FadvisedFileRegion.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QwOTozNToxOFrOJH7M4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMDozNjowNVrOJH9nUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI4OTc2Mg==", "bodyText": "Shall we simplify this to:\ntransferred = true;\nif (this.shuffleTransferToAllowed) {\n      return super.transferTo(target, position);\n}\nreturn  customShuffleTransfer(target, position);", "url": "https://github.com/apache/hive/pull/1778#discussion_r612289762", "createdAt": "2021-04-13T09:35:18Z", "author": {"login": "pgaref"}, "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/FadvisedFileRegion.java", "diffHunk": "@@ -71,15 +72,39 @@ public long transferTo(WritableByteChannel target, long position)\n       throws IOException {\n     if (manageOsCache && readaheadPool != null) {\n       readaheadRequest = readaheadPool.readaheadStream(identifier, fd,\n-          getPosition() + position, readaheadLength,\n-          getPosition() + getCount(), readaheadRequest);\n+          position() + position, readaheadLength,\n+          position() + count(), readaheadRequest);\n     }\n-    \n+    long written = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78ad65f2dd2cea1ee3bb06b0b3d27a2a6cebecd7"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMyODI4MA==", "bodyText": "looks better, but I don't think it's correct: in case of an exception during the transfer, we should not have set transferred=true", "url": "https://github.com/apache/hive/pull/1778#discussion_r612328280", "createdAt": "2021-04-13T10:34:25Z", "author": {"login": "abstractdog"}, "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/FadvisedFileRegion.java", "diffHunk": "@@ -71,15 +72,39 @@ public long transferTo(WritableByteChannel target, long position)\n       throws IOException {\n     if (manageOsCache && readaheadPool != null) {\n       readaheadRequest = readaheadPool.readaheadStream(identifier, fd,\n-          getPosition() + position, readaheadLength,\n-          getPosition() + getCount(), readaheadRequest);\n+          position() + position, readaheadLength,\n+          position() + count(), readaheadRequest);\n     }\n-    \n+    long written = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI4OTc2Mg=="}, "originalCommit": {"oid": "78ad65f2dd2cea1ee3bb06b0b3d27a2a6cebecd7"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMyOTI5OA==", "bodyText": "Got it", "url": "https://github.com/apache/hive/pull/1778#discussion_r612329298", "createdAt": "2021-04-13T10:36:05Z", "author": {"login": "pgaref"}, "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/FadvisedFileRegion.java", "diffHunk": "@@ -71,15 +72,39 @@ public long transferTo(WritableByteChannel target, long position)\n       throws IOException {\n     if (manageOsCache && readaheadPool != null) {\n       readaheadRequest = readaheadPool.readaheadStream(identifier, fd,\n-          getPosition() + position, readaheadLength,\n-          getPosition() + getCount(), readaheadRequest);\n+          position() + position, readaheadLength,\n+          position() + count(), readaheadRequest);\n     }\n-    \n+    long written = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI4OTc2Mg=="}, "originalCommit": {"oid": "78ad65f2dd2cea1ee3bb06b0b3d27a2a6cebecd7"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3MzYxMzMxOnYy", "diffSide": "RIGHT", "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/FadvisedFileRegion.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QwOTozOToxNFrOJH7X5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMDozMToxMVrOJH9cLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI5MjU4Mw==", "bodyText": "Shall we keep the original log message for advancing the file descriptor as well here?\nWhy do we need the extra fd.valid check here? (maybe leave a comment?)", "url": "https://github.com/apache/hive/pull/1778#discussion_r612292583", "createdAt": "2021-04-13T09:39:14Z", "author": {"login": "pgaref"}, "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/FadvisedFileRegion.java", "diffHunk": "@@ -124,39 +149,33 @@ long customShuffleTransfer(WritableByteChannel target, long position)\n         position += trans; \n         trans = 0;\n       }\n-      \n+\n       //write data to the target\n       while(byteBuffer.hasRemaining()) {\n         target.write(byteBuffer);\n       }\n       \n       byteBuffer.clear();\n     }\n-    \n+\n     return actualCount - trans;\n   }\n \n-  \n-  @Override\n-  public void releaseExternalResources() {\n-    if (readaheadRequest != null) {\n-      readaheadRequest.cancel();\n-    }\n-    super.releaseExternalResources();\n-  }\n-  \n   /**\n    * Call when the transfer completes successfully so we can advise the OS that\n    * we don't need the region to be cached anymore.\n    */\n   public void transferSuccessful() {\n-    if (manageOsCache && getCount() > 0) {\n+    if (manageOsCache && count() > 0) {\n       try {\n         if (canEvictAfterTransfer) {\n-          LOG.debug(\"shuffleBufferSize: {}, path: {}\", shuffleBufferSize, identifier);\n-          NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(identifier,\n-              fd, getPosition(), getCount(),\n-              NativeIO.POSIX.POSIX_FADV_DONTNEED);\n+          if (fd.valid()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78ad65f2dd2cea1ee3bb06b0b3d27a2a6cebecd7"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMyNjQ0NQ==", "bodyText": "hm, thought this over again, fd.valid() change was needed while I haven't been handling deallocate() stuff properly, but now, at this point fd should be valid...initially I left this check here because I thought that an invalid fd is not a problem (which is true, we won't advise to OS cache, and that's it), but as we already have try/catch, we don't need this this check (we'll have the exception in the logs anyway)", "url": "https://github.com/apache/hive/pull/1778#discussion_r612326445", "createdAt": "2021-04-13T10:31:11Z", "author": {"login": "abstractdog"}, "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/FadvisedFileRegion.java", "diffHunk": "@@ -124,39 +149,33 @@ long customShuffleTransfer(WritableByteChannel target, long position)\n         position += trans; \n         trans = 0;\n       }\n-      \n+\n       //write data to the target\n       while(byteBuffer.hasRemaining()) {\n         target.write(byteBuffer);\n       }\n       \n       byteBuffer.clear();\n     }\n-    \n+\n     return actualCount - trans;\n   }\n \n-  \n-  @Override\n-  public void releaseExternalResources() {\n-    if (readaheadRequest != null) {\n-      readaheadRequest.cancel();\n-    }\n-    super.releaseExternalResources();\n-  }\n-  \n   /**\n    * Call when the transfer completes successfully so we can advise the OS that\n    * we don't need the region to be cached anymore.\n    */\n   public void transferSuccessful() {\n-    if (manageOsCache && getCount() > 0) {\n+    if (manageOsCache && count() > 0) {\n       try {\n         if (canEvictAfterTransfer) {\n-          LOG.debug(\"shuffleBufferSize: {}, path: {}\", shuffleBufferSize, identifier);\n-          NativeIO.POSIX.getCacheManipulator().posixFadviseIfPossible(identifier,\n-              fd, getPosition(), getCount(),\n-              NativeIO.POSIX.POSIX_FADV_DONTNEED);\n+          if (fd.valid()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjI5MjU4Mw=="}, "originalCommit": {"oid": "78ad65f2dd2cea1ee3bb06b0b3d27a2a6cebecd7"}, "originalPosition": 117}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3MzY2NzIzOnYy", "diffSide": "RIGHT", "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QwOTo1MToyNFrOJH74tA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMDoyNjo1N1rOJH9RVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMwMDk4MA==", "bodyText": "I know this is copy pasted from below but do we have a ticket for this?\nIs it still needed?", "url": "https://github.com/apache/hive/pull/1778#discussion_r612300980", "createdAt": "2021-04-13T09:51:24Z", "author": {"login": "pgaref"}, "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java", "diffHunk": "@@ -339,27 +350,60 @@ private ShuffleHandler(Configuration conf) {\n \n \n   public void start() throws Exception {\n-    ServerBootstrap bootstrap = new ServerBootstrap(selector);\n-    // Timer is shared across entire factory and must be released separately\n-    timer = new HashedWheelTimer();\n-    try {\n-      pipelineFact = new HttpPipelineFactory(conf, timer);\n-    } catch (Exception ex) {\n-      throw new RuntimeException(ex);\n-    }\n-    bootstrap.setPipelineFactory(pipelineFact);\n-    bootstrap.setOption(\"backlog\", NetUtil.SOMAXCONN);\n+    ServerBootstrap bootstrap = new ServerBootstrap()\n+        .channel(NioServerSocketChannel.class)\n+        .group(bossGroup, workerGroup)\n+        .localAddress(port)\n+        .option(ChannelOption.SO_BACKLOG, NetUtil.SOMAXCONN)\n+        .childOption(ChannelOption.SO_KEEPALIVE, true);\n+    initPipeline(bootstrap, conf);\n+\n     port = conf.getInt(SHUFFLE_PORT_CONFIG_KEY, DEFAULT_SHUFFLE_PORT);\n-    Channel ch = bootstrap.bind(new InetSocketAddress(port));\n+    Channel ch = bootstrap.bind().sync().channel();\n     accepted.add(ch);\n-    port = ((InetSocketAddress)ch.getLocalAddress()).getPort();\n+    port = ((InetSocketAddress)ch.localAddress()).getPort();\n     conf.set(SHUFFLE_PORT_CONFIG_KEY, Integer.toString(port));\n-    pipelineFact.SHUFFLE.setPort(port);\n+    SHUFFLE.setPort(port);\n     if (dirWatcher != null) {\n       dirWatcher.start();\n     }\n-    LOG.info(\"LlapShuffleHandler\" + \" listening on port \" + port + \" (SOMAXCONN: \" + bootstrap.getOption(\"backlog\")\n-      + \")\");\n+    LOG.info(\"LlapShuffleHandler listening on port {} (SOMAXCONN: {})\", port, NetUtil.SOMAXCONN);\n+  }\n+\n+  private void initPipeline(ServerBootstrap bootstrap, Configuration conf) throws Exception {\n+    SHUFFLE = getShuffle(conf);\n+    // TODO Setup SSL Shuffle", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78ad65f2dd2cea1ee3bb06b0b3d27a2a6cebecd7"}, "originalPosition": 259}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMyMzY2OQ==", "bodyText": "I think we don't support SSL shuffle for LLAP at the moment (+ the comment is quite old), e.g. Cloudera's data warehouse, ssl on shuffle is handled transparently by the environment\nI haven't touched this part in this patch, and not even sure what's the plan :) that's why I simply kept this as is", "url": "https://github.com/apache/hive/pull/1778#discussion_r612323669", "createdAt": "2021-04-13T10:26:57Z", "author": {"login": "abstractdog"}, "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java", "diffHunk": "@@ -339,27 +350,60 @@ private ShuffleHandler(Configuration conf) {\n \n \n   public void start() throws Exception {\n-    ServerBootstrap bootstrap = new ServerBootstrap(selector);\n-    // Timer is shared across entire factory and must be released separately\n-    timer = new HashedWheelTimer();\n-    try {\n-      pipelineFact = new HttpPipelineFactory(conf, timer);\n-    } catch (Exception ex) {\n-      throw new RuntimeException(ex);\n-    }\n-    bootstrap.setPipelineFactory(pipelineFact);\n-    bootstrap.setOption(\"backlog\", NetUtil.SOMAXCONN);\n+    ServerBootstrap bootstrap = new ServerBootstrap()\n+        .channel(NioServerSocketChannel.class)\n+        .group(bossGroup, workerGroup)\n+        .localAddress(port)\n+        .option(ChannelOption.SO_BACKLOG, NetUtil.SOMAXCONN)\n+        .childOption(ChannelOption.SO_KEEPALIVE, true);\n+    initPipeline(bootstrap, conf);\n+\n     port = conf.getInt(SHUFFLE_PORT_CONFIG_KEY, DEFAULT_SHUFFLE_PORT);\n-    Channel ch = bootstrap.bind(new InetSocketAddress(port));\n+    Channel ch = bootstrap.bind().sync().channel();\n     accepted.add(ch);\n-    port = ((InetSocketAddress)ch.getLocalAddress()).getPort();\n+    port = ((InetSocketAddress)ch.localAddress()).getPort();\n     conf.set(SHUFFLE_PORT_CONFIG_KEY, Integer.toString(port));\n-    pipelineFact.SHUFFLE.setPort(port);\n+    SHUFFLE.setPort(port);\n     if (dirWatcher != null) {\n       dirWatcher.start();\n     }\n-    LOG.info(\"LlapShuffleHandler\" + \" listening on port \" + port + \" (SOMAXCONN: \" + bootstrap.getOption(\"backlog\")\n-      + \")\");\n+    LOG.info(\"LlapShuffleHandler listening on port {} (SOMAXCONN: {})\", port, NetUtil.SOMAXCONN);\n+  }\n+\n+  private void initPipeline(ServerBootstrap bootstrap, Configuration conf) throws Exception {\n+    SHUFFLE = getShuffle(conf);\n+    // TODO Setup SSL Shuffle", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMwMDk4MA=="}, "originalCommit": {"oid": "78ad65f2dd2cea1ee3bb06b0b3d27a2a6cebecd7"}, "originalPosition": 259}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3MzY5NjkzOnYy", "diffSide": "RIGHT", "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QwOTo1ODoxNFrOJH8LFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMTowNjoxN1rOJH-tlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMwNTY4Ng==", "bodyText": "Why is this need now? How was connection kept alive before ?", "url": "https://github.com/apache/hive/pull/1778#discussion_r612305686", "createdAt": "2021-04-13T09:58:14Z", "author": {"login": "pgaref"}, "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java", "diffHunk": "@@ -797,16 +803,17 @@ public void messageReceived(ChannelHandlerContext ctx, MessageEvent evt)\n \n       Map<String, MapOutputInfo> mapOutputInfoMap =\n           new HashMap<String, MapOutputInfo>();\n-      Channel ch = evt.getChannel();\n-\n+      Channel ch = ctx.channel();\n       // In case of KeepAlive, ensure that timeout handler does not close connection until entire\n       // response is written (i.e, response headers + mapOutput).\n-      ChannelPipeline pipeline = ch.getPipeline();\n+      ChannelPipeline pipeline = ch.pipeline();\n       TimeoutHandler timeoutHandler = (TimeoutHandler)pipeline.get(TIMEOUT_HANDLER);\n       timeoutHandler.setEnabledTimeout(false);\n \n       String user = userRsrc.get(jobId);\n-\n+      if (keepAliveParam || connectionKeepAliveEnabled){", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78ad65f2dd2cea1ee3bb06b0b3d27a2a6cebecd7"}, "originalPosition": 494}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzMDU4MQ==", "bodyText": "good catch :) this is an epic workaround for a problem that I haven't been able to figure out 100%, here are some details:\nhttps://issues.apache.org/jira/browse/TEZ-4157?focusedCommentId=17100835&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17100835\n(btw: with netty3, we didn't need this)\nare you fine with a comment explaining this?", "url": "https://github.com/apache/hive/pull/1778#discussion_r612330581", "createdAt": "2021-04-13T10:38:05Z", "author": {"login": "abstractdog"}, "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java", "diffHunk": "@@ -797,16 +803,17 @@ public void messageReceived(ChannelHandlerContext ctx, MessageEvent evt)\n \n       Map<String, MapOutputInfo> mapOutputInfoMap =\n           new HashMap<String, MapOutputInfo>();\n-      Channel ch = evt.getChannel();\n-\n+      Channel ch = ctx.channel();\n       // In case of KeepAlive, ensure that timeout handler does not close connection until entire\n       // response is written (i.e, response headers + mapOutput).\n-      ChannelPipeline pipeline = ch.getPipeline();\n+      ChannelPipeline pipeline = ch.pipeline();\n       TimeoutHandler timeoutHandler = (TimeoutHandler)pipeline.get(TIMEOUT_HANDLER);\n       timeoutHandler.setEnabledTimeout(false);\n \n       String user = userRsrc.get(jobId);\n-\n+      if (keepAliveParam || connectionKeepAliveEnabled){", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMwNTY4Ng=="}, "originalCommit": {"oid": "78ad65f2dd2cea1ee3bb06b0b3d27a2a6cebecd7"}, "originalPosition": 494}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMzNDA3OA==", "bodyText": "Got it, this is helpful but lets make sure this is expected from nettys' side of things before committing -- this would be helpful for the Tez change as well :)", "url": "https://github.com/apache/hive/pull/1778#discussion_r612334078", "createdAt": "2021-04-13T10:43:56Z", "author": {"login": "pgaref"}, "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java", "diffHunk": "@@ -797,16 +803,17 @@ public void messageReceived(ChannelHandlerContext ctx, MessageEvent evt)\n \n       Map<String, MapOutputInfo> mapOutputInfoMap =\n           new HashMap<String, MapOutputInfo>();\n-      Channel ch = evt.getChannel();\n-\n+      Channel ch = ctx.channel();\n       // In case of KeepAlive, ensure that timeout handler does not close connection until entire\n       // response is written (i.e, response headers + mapOutput).\n-      ChannelPipeline pipeline = ch.getPipeline();\n+      ChannelPipeline pipeline = ch.pipeline();\n       TimeoutHandler timeoutHandler = (TimeoutHandler)pipeline.get(TIMEOUT_HANDLER);\n       timeoutHandler.setEnabledTimeout(false);\n \n       String user = userRsrc.get(jobId);\n-\n+      if (keepAliveParam || connectionKeepAliveEnabled){", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMwNTY4Ng=="}, "originalCommit": {"oid": "78ad65f2dd2cea1ee3bb06b0b3d27a2a6cebecd7"}, "originalPosition": 494}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM0MzU0Mw==", "bodyText": "okay, in this case I'll have to include some unit tests here (which are part of tez codebase already) + create a simple repro to share with netty community", "url": "https://github.com/apache/hive/pull/1778#discussion_r612343543", "createdAt": "2021-04-13T10:59:56Z", "author": {"login": "abstractdog"}, "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java", "diffHunk": "@@ -797,16 +803,17 @@ public void messageReceived(ChannelHandlerContext ctx, MessageEvent evt)\n \n       Map<String, MapOutputInfo> mapOutputInfoMap =\n           new HashMap<String, MapOutputInfo>();\n-      Channel ch = evt.getChannel();\n-\n+      Channel ch = ctx.channel();\n       // In case of KeepAlive, ensure that timeout handler does not close connection until entire\n       // response is written (i.e, response headers + mapOutput).\n-      ChannelPipeline pipeline = ch.getPipeline();\n+      ChannelPipeline pipeline = ch.pipeline();\n       TimeoutHandler timeoutHandler = (TimeoutHandler)pipeline.get(TIMEOUT_HANDLER);\n       timeoutHandler.setEnabledTimeout(false);\n \n       String user = userRsrc.get(jobId);\n-\n+      if (keepAliveParam || connectionKeepAliveEnabled){", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMwNTY4Ng=="}, "originalCommit": {"oid": "78ad65f2dd2cea1ee3bb06b0b3d27a2a6cebecd7"}, "originalPosition": 494}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjM0NzI4Ng==", "bodyText": "Thanks Laszlo! sounds like a plan!", "url": "https://github.com/apache/hive/pull/1778#discussion_r612347286", "createdAt": "2021-04-13T11:06:17Z", "author": {"login": "pgaref"}, "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java", "diffHunk": "@@ -797,16 +803,17 @@ public void messageReceived(ChannelHandlerContext ctx, MessageEvent evt)\n \n       Map<String, MapOutputInfo> mapOutputInfoMap =\n           new HashMap<String, MapOutputInfo>();\n-      Channel ch = evt.getChannel();\n-\n+      Channel ch = ctx.channel();\n       // In case of KeepAlive, ensure that timeout handler does not close connection until entire\n       // response is written (i.e, response headers + mapOutput).\n-      ChannelPipeline pipeline = ch.getPipeline();\n+      ChannelPipeline pipeline = ch.pipeline();\n       TimeoutHandler timeoutHandler = (TimeoutHandler)pipeline.get(TIMEOUT_HANDLER);\n       timeoutHandler.setEnabledTimeout(false);\n \n       String user = userRsrc.get(jobId);\n-\n+      if (keepAliveParam || connectionKeepAliveEnabled){", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMwNTY4Ng=="}, "originalCommit": {"oid": "78ad65f2dd2cea1ee3bb06b0b3d27a2a6cebecd7"}, "originalPosition": 494}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg3MzcxMjUzOnYy", "diffSide": "RIGHT", "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMDowMjowMFrOJH8UwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xM1QxMDowMjowMFrOJH8UwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxMjMwODE2MQ==", "bodyText": "This looks much cleaner with deallocate() call replacing completion Listeners", "url": "https://github.com/apache/hive/pull/1778#discussion_r612308161", "createdAt": "2021-04-13T10:02:00Z", "author": {"login": "pgaref"}, "path": "llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java", "diffHunk": "@@ -1031,25 +1038,14 @@ protected ChannelFuture sendMapOutput(ChannelHandlerContext ctx, Channel ch,\n             info.getStartOffset(), info.getPartLength(), manageOsCache, readaheadLength,\n             readaheadPool, spillfile.getAbsolutePath(), \n             shuffleBufferSize, shuffleTransferToAllowed, canEvictAfterTransfer);\n-        writeFuture = ch.write(partition);\n-        writeFuture.addListener(new ChannelFutureListener() {\n-            // TODO error handling; distinguish IO/connection failures,\n-            //      attribute to appropriate spill output\n-          @Override\n-          public void operationComplete(ChannelFuture future) {\n-            if (future.isSuccess()) {\n-              partition.transferSuccessful();\n-            }\n-            partition.releaseExternalResources();\n-          }\n-        });\n+        writeFuture = ch.writeAndFlush(partition);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "78ad65f2dd2cea1ee3bb06b0b3d27a2a6cebecd7"}, "originalPosition": 552}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 155, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}