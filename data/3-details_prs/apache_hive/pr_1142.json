{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM2MTE3NzAz", "number": 1142, "title": "HIVE-23718 Extract transaction handling from Driver (Miklos Gergely, reviewed by Peter Vary)", "bodyText": "", "createdAt": "2020-06-17T21:58:28Z", "url": "https://github.com/apache/hive/pull/1142", "merged": true, "mergeCommit": {"oid": "7a8aad1e62f29b53438946369b9ff14e7c2b4f64"}, "closed": true, "closedAt": "2020-07-01T05:30:56Z", "author": {"login": "miklosgergely"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcsamXLgBqjM0NTcxMDQ3NzU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcwb-3agBqjM0OTk1ODgyODY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bf4e867f3f2e2e7ae6da29d8a4c26681133506d1", "author": {"user": {"login": "miklosgergely", "name": "Miklos Gergely"}}, "url": "https://github.com/apache/hive/commit/bf4e867f3f2e2e7ae6da29d8a4c26681133506d1", "committedDate": "2020-06-17T21:58:00Z", "message": "HIVE-23718 Extract transaction handling from Driver (Miklos Gergely)"}, "afterCommit": {"oid": "f558e2488632ed4adfe7a6c193b3ae4570a58718", "author": {"user": {"login": "miklosgergely", "name": "Miklos Gergely"}}, "url": "https://github.com/apache/hive/commit/f558e2488632ed4adfe7a6c193b3ae4570a58718", "committedDate": "2020-06-18T08:57:53Z", "message": "HIVE-23718 Extract transaction handling from Driver (Miklos Gergely)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzMTEzMjY0", "url": "https://github.com/apache/hive/pull/1142#pullrequestreview-433113264", "createdAt": "2020-06-18T09:40:01Z", "commit": {"oid": "f558e2488632ed4adfe7a6c193b3ae4570a58718"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwOTo0MDowMVrOGlnsWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwOTo0MDowMVrOGlnsWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwMDgyNA==", "bodyText": "What about driverTxnHandler.acquireLocksIfNeeded, and pushing the logic to the TxnHandler?", "url": "https://github.com/apache/hive/pull/1142#discussion_r442100824", "createdAt": "2020-06-18T09:40:01Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -565,9 +368,9 @@ public void lockAndRespond() throws CommandProcessorException {\n           \"No previously compiled query for driver - queryId=\" + driverContext.getQueryState().getQueryId());\n     }\n \n-    if (requiresLock()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f558e2488632ed4adfe7a6c193b3ae4570a58718"}, "originalPosition": 304}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzMTE0MTk3", "url": "https://github.com/apache/hive/pull/1142#pullrequestreview-433114197", "createdAt": "2020-06-18T09:41:14Z", "commit": {"oid": "f558e2488632ed4adfe7a6c193b3ae4570a58718"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwOTo0MToxNFrOGlnvHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwOTo0MToxNFrOGlnvHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwMTUzNA==", "bodyText": "What about removing Driver.releaseLocksAndCommitOrRollback altogether, and call driverTxnHandler.releaseLocksAndCommitOrRollback(commit, txnManager) immediately?", "url": "https://github.com/apache/hive/pull/1142#discussion_r442101534", "createdAt": "2020-06-18T09:41:14Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -308,110 +255,6 @@ public FetchTask getFetchTask() {\n     return driverContext.getFetchTask();\n   }\n \n-  /**\n-   * Acquire read and write locks needed by the statement. The list of objects to be locked are\n-   * obtained from the inputs and outputs populated by the compiler.  Locking strategy depends on\n-   * HiveTxnManager and HiveLockManager configured\n-   *\n-   * This method also records the list of valid transactions.  This must be done after any\n-   * transactions have been opened.\n-   * @throws CommandProcessorException\n-   **/\n-  private void acquireLocks() throws CommandProcessorException {\n-    PerfLogger perfLogger = SessionState.getPerfLogger();\n-    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.ACQUIRE_READ_WRITE_LOCKS);\n-\n-    if(!driverContext.getTxnManager().isTxnOpen() && driverContext.getTxnManager().supportsAcid()) {\n-      /*non acid txn managers don't support txns but fwd lock requests to lock managers\n-        acid txn manager requires all locks to be associated with a txn so if we\n-        end up here w/o an open txn it's because we are processing something like \"use <database>\n-        which by definition needs no locks*/\n-      return;\n-    }\n-    try {\n-      String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n-\n-      // Set the table write id in all of the acid file sinks\n-      if (!driverContext.getPlan().getAcidSinks().isEmpty()) {\n-        List<FileSinkDesc> acidSinks = new ArrayList<>(driverContext.getPlan().getAcidSinks());\n-        //sorting makes tests easier to write since file names and ROW__IDs depend on statementId\n-        //so this makes (file name -> data) mapping stable\n-        acidSinks.sort((FileSinkDesc fsd1, FileSinkDesc fsd2) ->\n-          fsd1.getDirName().compareTo(fsd2.getDirName()));\n-        for (FileSinkDesc desc : acidSinks) {\n-          TableDesc tableInfo = desc.getTableInfo();\n-          final TableName tn = HiveTableName.ofNullable(tableInfo.getTableName());\n-          long writeId = driverContext.getTxnManager().getTableWriteId(tn.getDb(), tn.getTable());\n-          desc.setTableWriteId(writeId);\n-\n-          /**\n-           * it's possible to have > 1 FileSink writing to the same table/partition\n-           * e.g. Merge stmt, multi-insert stmt when mixing DP and SP writes\n-           * Insert ... Select ... Union All Select ... using\n-           * {@link org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator#UNION_SUDBIR_PREFIX}\n-           */\n-          desc.setStatementId(driverContext.getTxnManager().getStmtIdAndIncrement());\n-          String unionAllSubdir = \"/\" + AbstractFileMergeOperator.UNION_SUDBIR_PREFIX;\n-          if(desc.getInsertOverwrite() && desc.getDirName().toString().contains(unionAllSubdir) &&\n-              desc.isFullAcidTable()) {\n-            throw new UnsupportedOperationException(\"QueryId=\" + driverContext.getPlan().getQueryId() +\n-                \" is not supported due to OVERWRITE and UNION ALL.  Please use truncate + insert\");\n-          }\n-        }\n-      }\n-\n-      if (driverContext.getPlan().getAcidAnalyzeTable() != null) {\n-        // Allocate write ID for the table being analyzed.\n-        Table t = driverContext.getPlan().getAcidAnalyzeTable().getTable();\n-        driverContext.getTxnManager().getTableWriteId(t.getDbName(), t.getTableName());\n-      }\n-\n-\n-      DDLDescWithWriteId acidDdlDesc = driverContext.getPlan().getAcidDdlDesc();\n-      boolean hasAcidDdl = acidDdlDesc != null && acidDdlDesc.mayNeedWriteId();\n-      if (hasAcidDdl) {\n-        String fqTableName = acidDdlDesc.getFullTableName();\n-        final TableName tn = HiveTableName.ofNullableWithNoDefault(fqTableName);\n-        long writeId = driverContext.getTxnManager().getTableWriteId(tn.getDb(), tn.getTable());\n-        acidDdlDesc.setWriteId(writeId);\n-      }\n-\n-      /*It's imperative that {@code acquireLocks()} is called for all commands so that\n-      HiveTxnManager can transition its state machine correctly*/\n-      driverContext.getTxnManager().acquireLocks(driverContext.getPlan(), context, userFromUGI, driverState);\n-      final List<HiveLock> locks = context.getHiveLocks();\n-      LOG.info(\"Operation {} obtained {} locks\", driverContext.getPlan().getOperation(),\n-          ((locks == null) ? 0 : locks.size()));\n-      // This check is for controlling the correctness of the current state\n-      if (driverContext.getTxnManager().recordSnapshot(driverContext.getPlan()) &&\n-          !driverContext.isValidTxnListsGenerated()) {\n-        throw new IllegalStateException(\n-            \"Need to record valid WriteID list but there is no valid TxnID list (\" +\n-                JavaUtils.txnIdToString(driverContext.getTxnManager().getCurrentTxnId()) +\n-                \", queryId:\" + driverContext.getPlan().getQueryId() + \")\");\n-      }\n-\n-      if (driverContext.getPlan().hasAcidResourcesInQuery() || hasAcidDdl) {\n-        validTxnManager.recordValidWriteIds();\n-      }\n-\n-    } catch (Exception e) {\n-      String errorMessage;\n-      if (driverState.isDestroyed() || driverState.isAborted() || driverState.isClosed()) {\n-        errorMessage = String.format(\"Ignore lock acquisition related exception in terminal state (%s): %s\",\n-            driverState.toString(), e.getMessage());\n-        CONSOLE.printInfo(errorMessage);\n-      } else {\n-        errorMessage = String.format(\"FAILED: Error in acquiring locks: %s\", e.getMessage());\n-        CONSOLE.printError(errorMessage, \"\\n\" + StringUtils.stringifyException(e));\n-      }\n-      throw DriverUtils.createProcessorException(driverContext, 10, errorMessage, ErrorMsg.findSQLState(e.getMessage()),\n-          e);\n-    } finally {\n-      perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.ACQUIRE_READ_WRITE_LOCKS);\n-    }\n-  }\n-\n   public void releaseLocksAndCommitOrRollback(boolean commit) throws LockException {\n     releaseLocksAndCommitOrRollback(commit, driverContext.getTxnManager());\n   }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f558e2488632ed4adfe7a6c193b3ae4570a58718"}, "originalPosition": 250}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzMTE5MTk4", "url": "https://github.com/apache/hive/pull/1142#pullrequestreview-433119198", "createdAt": "2020-06-18T09:47:40Z", "commit": {"oid": "f558e2488632ed4adfe7a6c193b3ae4570a58718"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwOTo0Nzo0MFrOGln9BA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwOTo0Nzo0MFrOGln9BA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNTA5Mg==", "bodyText": "Do we need this magic? DriverTxnHandler already has a Context.\nIf I understand correctly this is just clearing the context at hand, should be the responsibility of the DriverTxnHandler.... or even better it can have a DriverTxnHandlerContext...", "url": "https://github.com/apache/hive/pull/1142#discussion_r442105092", "createdAt": "2020-06-18T09:47:40Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -988,7 +741,7 @@ private void releaseContext() {\n         }\n         context.clear(deleteResultDir);\n         if (context.getHiveLocks() != null) {\n-          hiveLocks.addAll(context.getHiveLocks());\n+          driverTxnHandler.addHiveLocksFromContext();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f558e2488632ed4adfe7a6c193b3ae4570a58718"}, "originalPosition": 374}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzMTE5ODU3", "url": "https://github.com/apache/hive/pull/1142#pullrequestreview-433119857", "createdAt": "2020-06-18T09:48:29Z", "commit": {"oid": "f558e2488632ed4adfe7a6c193b3ae4570a58718"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwOTo0ODoyOVrOGln-0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwOTo0ODoyOVrOGln-0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNTU1NA==", "bodyText": "Again, maybe just a driverTxnHandler.release?", "url": "https://github.com/apache/hive/pull/1142#discussion_r442105554", "createdAt": "2020-06-18T09:48:29Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -1058,15 +811,15 @@ private int closeInProcess(boolean destroyed) {\n     releaseResStream();\n     releaseContext();\n     if (destroyed) {\n-      if (!hiveLocks.isEmpty()) {\n+      if (!driverTxnHandler.hasHiveLock()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f558e2488632ed4adfe7a6c193b3ae4570a58718"}, "originalPosition": 383}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzMTIwMTgz", "url": "https://github.com/apache/hive/pull/1142#pullrequestreview-433120183", "createdAt": "2020-06-18T09:48:53Z", "commit": {"oid": "f558e2488632ed4adfe7a6c193b3ae4570a58718"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwOTo0ODo1M1rOGln_vw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwOTo0ODo1M1rOGln_vw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNTc5MQ==", "bodyText": "See comment above: DriverTxnHanlder.destroy?", "url": "https://github.com/apache/hive/pull/1142#discussion_r442105791", "createdAt": "2020-06-18T09:48:53Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -1112,15 +865,15 @@ public void destroy() {\n     boolean isTxnOpen = driverContext != null\n         && driverContext.getTxnManager() != null\n         && driverContext.getTxnManager().isTxnOpen();\n-    if (!hiveLocks.isEmpty() || isTxnOpen) {\n+    if (!driverTxnHandler.hasHiveLock() || isTxnOpen) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f558e2488632ed4adfe7a6c193b3ae4570a58718"}, "originalPosition": 401}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzMTIxMzM2", "url": "https://github.com/apache/hive/pull/1142#pullrequestreview-433121336", "createdAt": "2020-06-18T09:50:23Z", "commit": {"oid": "f558e2488632ed4adfe7a6c193b3ae4570a58718"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwOTo1MDoyM1rOGloC_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQwOTo1MDoyM1rOGloC_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjEwNjYyMA==", "bodyText": "I would prefer keeping the locks local to DriverTxnManager, and its' own context(?), and not storing in the global context?", "url": "https://github.com/apache/hive/pull/1142#discussion_r442106620", "createdAt": "2020-06-18T09:50:23Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/DriverTxnHandler.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql;\n+\n+import java.util.ArrayList;\n+import java.util.LinkedList;\n+import java.util.List;\n+import java.util.Queue;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.hive.common.JavaUtils;\n+import org.apache.hadoop.hive.common.TableName;\n+import org.apache.hadoop.hive.common.ValidTxnList;\n+import org.apache.hadoop.hive.common.ValidTxnWriteIdList;\n+import org.apache.hadoop.hive.conf.Constants;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.ql.ddl.DDLDesc.DDLDescWithWriteId;\n+import org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator;\n+import org.apache.hadoop.hive.ql.exec.ConditionalTask;\n+import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.lockmgr.HiveLock;\n+import org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager;\n+import org.apache.hadoop.hive.ql.lockmgr.LockException;\n+import org.apache.hadoop.hive.ql.log.PerfLogger;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.parse.HiveTableName;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.plan.FileSinkDesc;\n+import org.apache.hadoop.hive.ql.plan.HiveOperation;\n+import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorException;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;\n+import org.apache.hadoop.util.StringUtils;\n+import org.apache.hive.common.util.ShutdownHookManager;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * \n+ */\n+public class DriverTxnHandler {\n+  private static final String CLASS_NAME = Driver.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  private static final LogHelper CONSOLE = new LogHelper(LOG);\n+  private static final int SHUTDOWN_HOOK_PRIORITY = 0;\n+\n+  private final DriverContext driverContext;\n+  private final DriverState driverState;\n+  private final ValidTxnManager validTxnManager;\n+\n+  private final List<HiveLock> hiveLocks = new ArrayList<HiveLock>();\n+\n+  private Context context;\n+\n+  public DriverTxnHandler(DriverContext driverContext, DriverState driverState, ValidTxnManager validTxnManager) {\n+    this.driverContext = driverContext;\n+    this.driverState = driverState;\n+    this.validTxnManager = validTxnManager;\n+  }\n+\n+  public void createTxnManager() throws CommandProcessorException {\n+    try {\n+      // Initialize the transaction manager.  This must be done before analyze is called.\n+      HiveTxnManager queryTxnManager = (driverContext.getInitTxnManager() != null) ?\n+          driverContext.getInitTxnManager() : SessionState.get().initTxnMgr(driverContext.getConf());\n+\n+      if (queryTxnManager instanceof Configurable) {\n+        ((Configurable) queryTxnManager).setConf(driverContext.getConf());\n+      }\n+      driverContext.setTxnManager(queryTxnManager);\n+      driverContext.getQueryState().setTxnManager(queryTxnManager);\n+\n+      // In case when user Ctrl-C twice to kill Hive CLI JVM, we want to release locks\n+      // if compile is being called multiple times, clear the old shutdownhook\n+      ShutdownHookManager.removeShutdownHook(driverContext.getShutdownRunner());\n+      Runnable shutdownRunner = new Runnable() {\n+        @Override\n+        public void run() {\n+          try {\n+            releaseLocksAndCommitOrRollback(false, driverContext.getTxnManager());\n+          } catch (LockException e) {\n+            LOG.warn(\"Exception when releasing locks in ShutdownHook for Driver: \" +\n+                e.getMessage());\n+          }\n+        }\n+      };\n+      ShutdownHookManager.addShutdownHook(shutdownRunner, SHUTDOWN_HOOK_PRIORITY);\n+      driverContext.setShutdownRunner(shutdownRunner);\n+    } catch (LockException e) {\n+      ErrorMsg error = ErrorMsg.getErrorMsg(e.getMessage());\n+      String errorMessage = \"FAILED: \" + e.getClass().getSimpleName() + \" [Error \"  + error.getErrorCode()  + \"]:\";\n+\n+      CONSOLE.printError(errorMessage, \"\\n\" + StringUtils.stringifyException(e));\n+      throw DriverUtils.createProcessorException(driverContext, error.getErrorCode(), errorMessage, error.getSQLState(),\n+          e);\n+    }\n+  }\n+\n+  public void setContext(Context context) {\n+    this.context = context;\n+  }\n+\n+  public boolean requiresLock() {\n+    if (!DriverUtils.checkConcurrency(driverContext)) {\n+      LOG.info(\"Concurrency mode is disabled, not creating a lock manager\");\n+      return false;\n+    }\n+\n+    // Lock operations themselves don't require the lock.\n+    if (isExplicitLockOperation()) {\n+      return false;\n+    }\n+\n+    if (!HiveConf.getBoolVar(driverContext.getConf(), ConfVars.HIVE_LOCK_MAPRED_ONLY)) {\n+      return true;\n+    }\n+\n+    if (driverContext.getConf().get(Constants.HIVE_QUERY_EXCLUSIVE_LOCK) != null) {\n+      return true;\n+    }\n+\n+    Queue<Task<?>> tasks = new LinkedList<Task<?>>();\n+    tasks.addAll(driverContext.getPlan().getRootTasks());\n+    while (tasks.peek() != null) {\n+      Task<?> task = tasks.remove();\n+      if (task.requireLock()) {\n+        return true;\n+      }\n+\n+      if (task instanceof ConditionalTask) {\n+        tasks.addAll(((ConditionalTask)task).getListTasks());\n+      }\n+\n+      if (task.getChildTasks() != null) {\n+        tasks.addAll(task.getChildTasks());\n+      }\n+      // does not add back up task here, because back up task should be the same type of the original task.\n+    }\n+\n+    return false;\n+  }\n+\n+  private boolean isExplicitLockOperation() {\n+    HiveOperation currentOpt = driverContext.getPlan().getOperation();\n+    if (currentOpt != null) {\n+      switch (currentOpt) {\n+      case LOCKDB:\n+      case UNLOCKDB:\n+      case LOCKTABLE:\n+      case UNLOCKTABLE:\n+        return true;\n+      default:\n+        return false;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  /**\n+   * Acquire read and write locks needed by the statement. The list of objects to be locked are obtained from the inputs\n+   * and outputs populated by the compiler. Locking strategy depends on HiveTxnManager and HiveLockManager configured.\n+   *\n+   * This method also records the list of valid transactions. This must be done after any transactions have been opened.\n+   */\n+  public void acquireLocks() throws CommandProcessorException {\n+    PerfLogger perfLogger = SessionState.getPerfLogger();\n+    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.ACQUIRE_READ_WRITE_LOCKS);\n+\n+    if (!driverContext.getTxnManager().isTxnOpen() && driverContext.getTxnManager().supportsAcid()) {\n+      /* non acid txn managers don't support txns but fwd lock requests to lock managers\n+         acid txn manager requires all locks to be associated with a txn so if we end up here w/o an open txn\n+         it's because we are processing something like \"use <database> which by definition needs no locks */\n+      return;\n+    }\n+\n+    try {\n+      setWriteIdForAcidFileSinks();\n+      allocateWriteIdForAcidAnalyzeTable();\n+      boolean hasAcidDdl = setWriteIdForAcidDdl();\n+      acquireLocksInternal();\n+\n+      if (driverContext.getPlan().hasAcidResourcesInQuery() || hasAcidDdl) {\n+        validTxnManager.recordValidWriteIds();\n+      }\n+    } catch (Exception e) {\n+      String errorMessage;\n+      if (driverState.isDestroyed() || driverState.isAborted() || driverState.isClosed()) {\n+        errorMessage = String.format(\"Ignore lock acquisition related exception in terminal state (%s): %s\",\n+            driverState.toString(), e.getMessage());\n+        CONSOLE.printInfo(errorMessage);\n+      } else {\n+        errorMessage = String.format(\"FAILED: Error in acquiring locks: %s\", e.getMessage());\n+        CONSOLE.printError(errorMessage, \"\\n\" + StringUtils.stringifyException(e));\n+      }\n+      throw DriverUtils.createProcessorException(driverContext, 10, errorMessage, ErrorMsg.findSQLState(e.getMessage()),\n+          e);\n+    } finally {\n+      perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.ACQUIRE_READ_WRITE_LOCKS);\n+    }\n+  }\n+\n+  private void setWriteIdForAcidFileSinks() throws SemanticException, LockException {\n+    if (!driverContext.getPlan().getAcidSinks().isEmpty()) {\n+      List<FileSinkDesc> acidSinks = new ArrayList<>(driverContext.getPlan().getAcidSinks());\n+      //sorting makes tests easier to write since file names and ROW__IDs depend on statementId\n+      //so this makes (file name -> data) mapping stable\n+      acidSinks.sort((FileSinkDesc fsd1, FileSinkDesc fsd2) -> fsd1.getDirName().compareTo(fsd2.getDirName()));\n+      for (FileSinkDesc acidSink : acidSinks) {\n+        TableDesc tableInfo = acidSink.getTableInfo();\n+        TableName tableName = HiveTableName.of(tableInfo.getTableName());\n+        long writeId = driverContext.getTxnManager().getTableWriteId(tableName.getDb(), tableName.getTable());\n+        acidSink.setTableWriteId(writeId);\n+\n+        /**\n+         * it's possible to have > 1 FileSink writing to the same table/partition\n+         * e.g. Merge stmt, multi-insert stmt when mixing DP and SP writes\n+         * Insert ... Select ... Union All Select ... using\n+         * {@link org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator#UNION_SUDBIR_PREFIX}\n+         */\n+        acidSink.setStatementId(driverContext.getTxnManager().getStmtIdAndIncrement());\n+        String unionAllSubdir = \"/\" + AbstractFileMergeOperator.UNION_SUDBIR_PREFIX;\n+        if (acidSink.getInsertOverwrite() && acidSink.getDirName().toString().contains(unionAllSubdir) &&\n+            acidSink.isFullAcidTable()) {\n+          throw new UnsupportedOperationException(\"QueryId=\" + driverContext.getPlan().getQueryId() +\n+              \" is not supported due to OVERWRITE and UNION ALL.  Please use truncate + insert\");\n+        }\n+      }\n+    }\n+  }\n+\n+  private void allocateWriteIdForAcidAnalyzeTable() throws LockException {\n+    if (driverContext.getPlan().getAcidAnalyzeTable() != null) {\n+      Table table = driverContext.getPlan().getAcidAnalyzeTable().getTable();\n+      driverContext.getTxnManager().getTableWriteId(table.getDbName(), table.getTableName());\n+    }\n+  }\n+\n+  private boolean setWriteIdForAcidDdl() throws SemanticException, LockException {\n+    DDLDescWithWriteId acidDdlDesc = driverContext.getPlan().getAcidDdlDesc();\n+    boolean hasAcidDdl = acidDdlDesc != null && acidDdlDesc.mayNeedWriteId();\n+    if (hasAcidDdl) {\n+      String fqTableName = acidDdlDesc.getFullTableName();\n+      TableName tableName = HiveTableName.of(fqTableName);\n+      long writeId = driverContext.getTxnManager().getTableWriteId(tableName.getDb(), tableName.getTable());\n+      acidDdlDesc.setWriteId(writeId);\n+    }\n+    return hasAcidDdl;\n+  }\n+\n+  private void acquireLocksInternal() throws CommandProcessorException, LockException {\n+    /* It's imperative that {@code acquireLocks()} is called for all commands so that\n+       HiveTxnManager can transition its state machine correctly */\n+    String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n+    driverContext.getTxnManager().acquireLocks(driverContext.getPlan(), context, userFromUGI, driverState);\n+    List<HiveLock> locks = context.getHiveLocks();\n+    LOG.info(\"Operation {} obtained {} locks\", driverContext.getPlan().getOperation(),\n+        ((locks == null) ? 0 : locks.size()));\n+    // This check is for controlling the correctness of the current state\n+    if (driverContext.getTxnManager().recordSnapshot(driverContext.getPlan()) &&\n+        !driverContext.isValidTxnListsGenerated()) {\n+      throw new IllegalStateException(\"Need to record valid WriteID list but there is no valid TxnID list (\" +\n+          JavaUtils.txnIdToString(driverContext.getTxnManager().getCurrentTxnId()) +\n+          \", queryId: \" + driverContext.getPlan().getQueryId() + \")\");\n+    }\n+  }\n+\n+  public void addHiveLocksFromContext() {\n+    hiveLocks.addAll(context.getHiveLocks());\n+  }\n+\n+  public boolean hasHiveLock() {\n+    return hiveLocks.isEmpty();\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f558e2488632ed4adfe7a6c193b3ae4570a58718"}, "originalPosition": 291}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f558e2488632ed4adfe7a6c193b3ae4570a58718", "author": {"user": {"login": "miklosgergely", "name": "Miklos Gergely"}}, "url": "https://github.com/apache/hive/commit/f558e2488632ed4adfe7a6c193b3ae4570a58718", "committedDate": "2020-06-18T08:57:53Z", "message": "HIVE-23718 Extract transaction handling from Driver (Miklos Gergely)"}, "afterCommit": {"oid": "96f1e6dd82bb735f9b1cd8bfa5224b743e88bf2a", "author": {"user": {"login": "miklosgergely", "name": "Miklos Gergely"}}, "url": "https://github.com/apache/hive/commit/96f1e6dd82bb735f9b1cd8bfa5224b743e88bf2a", "committedDate": "2020-06-18T21:39:10Z", "message": "HIVE-23718 Extract transaction handling from Driver (Miklos Gergely)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzODkxMTM2", "url": "https://github.com/apache/hive/pull/1142#pullrequestreview-433891136", "createdAt": "2020-06-19T08:18:35Z", "commit": {"oid": "96f1e6dd82bb735f9b1cd8bfa5224b743e88bf2a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "75ecc1dfb1fbcad4b114d6947ce3e397bea2d0c4", "author": {"user": {"login": "miklosgergely", "name": "Miklos Gergely"}}, "url": "https://github.com/apache/hive/commit/75ecc1dfb1fbcad4b114d6947ce3e397bea2d0c4", "committedDate": "2020-06-30T20:50:14Z", "message": "HIVE-23718 Extract transaction handling from Driver (Miklos Gergely)"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "96f1e6dd82bb735f9b1cd8bfa5224b743e88bf2a", "author": {"user": {"login": "miklosgergely", "name": "Miklos Gergely"}}, "url": "https://github.com/apache/hive/commit/96f1e6dd82bb735f9b1cd8bfa5224b743e88bf2a", "committedDate": "2020-06-18T21:39:10Z", "message": "HIVE-23718 Extract transaction handling from Driver (Miklos Gergely)"}, "afterCommit": {"oid": "75ecc1dfb1fbcad4b114d6947ce3e397bea2d0c4", "author": {"user": {"login": "miklosgergely", "name": "Miklos Gergely"}}, "url": "https://github.com/apache/hive/commit/75ecc1dfb1fbcad4b114d6947ce3e397bea2d0c4", "committedDate": "2020-06-30T20:50:14Z", "message": "HIVE-23718 Extract transaction handling from Driver (Miklos Gergely)"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3659, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}