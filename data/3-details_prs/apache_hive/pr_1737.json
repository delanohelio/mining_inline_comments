{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMxOTE2MTc0", "number": 1737, "title": "HIVE-24482: Set advanced write Id during add constraint DDL tasks", "bodyText": "What changes were proposed in this pull request?\nSet the advanced write ID correctly during alter table add constraint DDL tasks.\nWhy are the changes needed?\nIn order to serve consistent data from HMS cache, we use Valid Write Id list and we also advance write ID for all DDL tasks. Although we were advancing the write ID for table constraints DDL, it was not set correctly.\nDoes this PR introduce any user-facing change?\nNo\nHow was this patch tested?\nAdded a new test", "createdAt": "2020-12-03T16:33:24Z", "url": "https://github.com/apache/hive/pull/1737", "merged": true, "mergeCommit": {"oid": "c70765c38357284a7a20e7cca010a357f7dd56bf"}, "closed": true, "closedAt": "2020-12-11T22:50:55Z", "author": {"login": "kishendas"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdilvZVgH2gAyNTMxOTE2MTc0OmY4Mzc2OTlhM2Q3OGM2NDM5Njk1MDhjYjI3YTA3NjY5MDM5NDY1OWI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdlPpuXgFqTU1MDYxODAyMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "f837699a3d78c643969508cb27a076690394659b", "author": {"user": null}, "url": "https://github.com/apache/hive/commit/f837699a3d78c643969508cb27a076690394659b", "committedDate": "2020-12-03T16:28:55Z", "message": "Set advanced write Id during add constraint DDL tasks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5c7c9a0b069384548bd2a3728498d95d84e34fc7", "author": {"user": null}, "url": "https://github.com/apache/hive/commit/5c7c9a0b069384548bd2a3728498d95d84e34fc7", "committedDate": "2020-12-04T09:24:38Z", "message": "Fix test cases"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "261a8f822ac2260970dd70a5394d8d087e70f37a", "author": {"user": null}, "url": "https://github.com/apache/hive/commit/261a8f822ac2260970dd70a5394d8d087e70f37a", "committedDate": "2020-12-08T09:30:11Z", "message": "Add unit tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4MDI3MzEz", "url": "https://github.com/apache/hive/pull/1737#pullrequestreview-548027313", "createdAt": "2020-12-09T10:11:17Z", "commit": {"oid": "261a8f822ac2260970dd70a5394d8d087e70f37a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxMDoxMToxOFrOICMtPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxMDoxMToxOFrOICMtPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTE3NjI1Mg==", "bodyText": "What is the reason for introducing this abstract class? Is there any plan for having multiple classes extending it? Why don't you just simply add the necessary changes to the AlterTableAddConstraintAnalyzer?", "url": "https://github.com/apache/hive/pull/1737#discussion_r539176252", "createdAt": "2020-12-09T10:11:18Z", "author": {"login": "miklosgergely"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/add/AbstractAddConstraintAnalyzer.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.constraint.add;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import org.apache.hadoop.hive.common.TableName;\n+import org.apache.hadoop.hive.metastore.api.SQLCheckConstraint;\n+import org.apache.hadoop.hive.metastore.api.SQLForeignKey;\n+import org.apache.hadoop.hive.metastore.api.SQLPrimaryKey;\n+import org.apache.hadoop.hive.metastore.api.SQLUniqueConstraint;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n+import org.apache.hadoop.hive.ql.QueryState;\n+import org.apache.hadoop.hive.ql.ddl.DDLDesc.DDLDescWithWriteId;\n+import org.apache.hadoop.hive.ql.ddl.DDLWork;\n+import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n+import org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableAnalyzer;\n+import org.apache.hadoop.hive.ql.ddl.table.constraint.Constraints;\n+import org.apache.hadoop.hive.ql.ddl.table.constraint.ConstraintsUtils;\n+import org.apache.hadoop.hive.ql.ddl.table.partition.add.AlterTableAddPartitionDesc;\n+import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.exec.TaskFactory;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.parse.ASTNode;\n+import org.apache.hadoop.hive.ql.parse.HiveParser;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+\n+/**\n+ * Analyzer for add constraint commands.\n+ */\n+public abstract class AbstractAddConstraintAnalyzer extends AbstractAlterTableAnalyzer {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "261a8f822ac2260970dd70a5394d8d087e70f37a"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4NDc5NTI3", "url": "https://github.com/apache/hive/pull/1737#pullrequestreview-548479527", "createdAt": "2020-12-09T18:29:20Z", "commit": {"oid": "261a8f822ac2260970dd70a5394d8d087e70f37a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxODoyOToyMFrOICjRrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxODoyOToyMFrOICjRrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU0NjAyOQ==", "bodyText": "Can we add the test for unique and check constraints as well for completeness?", "url": "https://github.com/apache/hive/pull/1737#discussion_r539546029", "createdAt": "2020-12-09T18:29:20Z", "author": {"login": "vihangk1"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java", "diffHunk": "@@ -378,6 +378,33 @@ private IMetaStoreClient prepareParallelTest(String tableName, int val)\n     return msClient;\n   }\n \n+  @Test\n+  public void testAddConstraintAdvancingWriteIds() throws Exception {\n+\n+    String tableName = \"constraints_table\";\n+    hiveConf.setBoolean(\"hive.stats.autogather\", true);\n+    hiveConf.setBoolean(\"hive.stats.column.autogather\", true);\n+    // Need to close the thread local Hive object so that configuration change is reflected to HMS.\n+    Hive.closeCurrent();\n+    runStatementOnDriver(\"drop table if exists \" + tableName);\n+    runStatementOnDriver(String.format(\"create table %s (a int, b string) stored as orc \" +\n+        \"TBLPROPERTIES ('transactional'='true', 'transactional_properties'='insert_only')\",\n+        tableName));\n+    runStatementOnDriver(String.format(\"insert into %s (a) values (0)\", tableName));\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(hiveConf);\n+    String validWriteIds = msClient.getValidWriteIds(\"default.\" + tableName).toString();\n+    LOG.info(\"ValidWriteIds before add constraint::\"+ validWriteIds);\n+    Assert.assertEquals(\"default.constraints_table:1:9223372036854775807::\", validWriteIds);\n+    runStatementOnDriver(String.format(\"alter table %s  ADD CONSTRAINT a_PK PRIMARY KEY (`a`) DISABLE NOVALIDATE\", tableName));\n+    validWriteIds  = msClient.getValidWriteIds(\"default.\" + tableName).toString();\n+    LOG.info(\"ValidWriteIds after add constraint primary key::\"+ validWriteIds);\n+    Assert.assertEquals(\"default.constraints_table:2:9223372036854775807::\", validWriteIds);\n+    runStatementOnDriver(String.format(\"alter table %s CHANGE COLUMN b b STRING NOT NULL\", tableName));\n+    validWriteIds  = msClient.getValidWriteIds(\"default.\" + tableName).toString();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "261a8f822ac2260970dd70a5394d8d087e70f37a"}, "originalPosition": 26}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "914b9d17ea24c345048f4e8473cbfbea4f97c671", "author": {"user": null}, "url": "https://github.com/apache/hive/commit/914b9d17ea24c345048f4e8473cbfbea4f97c671", "committedDate": "2020-12-09T23:23:58Z", "message": "Add more tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4NzUwMTI0", "url": "https://github.com/apache/hive/pull/1737#pullrequestreview-548750124", "createdAt": "2020-12-10T01:31:21Z", "commit": {"oid": "914b9d17ea24c345048f4e8473cbfbea4f97c671"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0dc3c33169befaf12e83265f203765b13f3e4cbd", "author": {"user": null}, "url": "https://github.com/apache/hive/commit/0dc3c33169befaf12e83265f203765b13f3e4cbd", "committedDate": "2020-12-10T02:36:06Z", "message": "Modify class name"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5MDgwODM2", "url": "https://github.com/apache/hive/pull/1737#pullrequestreview-549080836", "createdAt": "2020-12-10T10:59:36Z", "commit": {"oid": "0dc3c33169befaf12e83265f203765b13f3e4cbd"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMDo1OTozNlrOIDDhoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMTowMTozN1rOIDDmrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDA3NDQwMA==", "bodyText": "No need to modify the indentation of this annotation", "url": "https://github.com/apache/hive/pull/1737#discussion_r540074400", "createdAt": "2020-12-10T10:59:36Z", "author": {"login": "miklosgergely"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/add/AlterTableAddConstraintAnalyzer.java", "diffHunk": "@@ -43,12 +48,12 @@\n  * Analyzer for add constraint commands.\n  */\n @DDLType(types = HiveParser.TOK_ALTERTABLE_ADDCONSTRAINT)\n-public class AlterTableAddConstraintAnalyzer extends AbstractAlterTableAnalyzer {\n+public class AlterTableAddConstraintAnalyzer extends AbstractConstraintAnalyzer {\n   public AlterTableAddConstraintAnalyzer(QueryState queryState) throws SemanticException {\n     super(queryState);\n   }\n \n-  @Override\n+    @Override", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc3c33169befaf12e83265f203765b13f3e4cbd"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDA3NDc2OQ==", "bodyText": "According to the Hive checkstyle, the max line length is 120, please format the code according to that.", "url": "https://github.com/apache/hive/pull/1737#discussion_r540074769", "createdAt": "2020-12-10T11:00:14Z", "author": {"login": "miklosgergely"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/add/AlterTableAddConstraintAnalyzer.java", "diffHunk": "@@ -77,9 +82,23 @@ protected void analyzeCommand(TableName tableName, Map<String, String> partition\n       throw new SemanticException(ErrorMsg.NOT_RECOGNIZED_CONSTRAINT.getMsg(constraintNode.getToken().getText()));\n     }\n \n-    Constraints constraints = new Constraints(primaryKeys, foreignKeys, null, uniqueConstraints, null,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc3c33169befaf12e83265f203765b13f3e4cbd"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDA3NTY5Mg==", "bodyText": "As this class will be extended by both the add and drop constraint analyzer, it should be in the org.apache.hadoop.hive.ql.ddl.table.constraint package.", "url": "https://github.com/apache/hive/pull/1737#discussion_r540075692", "createdAt": "2020-12-10T11:01:37Z", "author": {"login": "miklosgergely"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/add/AbstractConstraintAnalyzer.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.constraint.add;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0dc3c33169befaf12e83265f203765b13f3e4cbd"}, "originalPosition": 19}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "22b5539518a8ad7c29ddd1bcf98ac72339f0dc6a", "author": {"user": null}, "url": "https://github.com/apache/hive/commit/22b5539518a8ad7c29ddd1bcf98ac72339f0dc6a", "committedDate": "2020-12-10T19:39:56Z", "message": "Address Miklos feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NTcyNDc3", "url": "https://github.com/apache/hive/pull/1737#pullrequestreview-549572477", "createdAt": "2020-12-10T20:24:20Z", "commit": {"oid": "22b5539518a8ad7c29ddd1bcf98ac72339f0dc6a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDoyNDoyMFrOIDbzjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDoyNDoyMFrOIDbzjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ3MjIwNg==", "bodyText": "Actually I don't see anything constraint specific in this class. I'd suggest to just add the ddlDescWithWriteId and it's getter and setter to the AbstractAlterTableAnalyzer.", "url": "https://github.com/apache/hive/pull/1737#discussion_r540472206", "createdAt": "2020-12-10T20:24:20Z", "author": {"login": "miklosgergely"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/ddl/table/constraint/AbstractConstraintAnalyzer.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.ddl.table.constraint;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import org.apache.hadoop.hive.common.TableName;\n+import org.apache.hadoop.hive.metastore.api.SQLCheckConstraint;\n+import org.apache.hadoop.hive.metastore.api.SQLForeignKey;\n+import org.apache.hadoop.hive.metastore.api.SQLPrimaryKey;\n+import org.apache.hadoop.hive.metastore.api.SQLUniqueConstraint;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n+import org.apache.hadoop.hive.ql.QueryState;\n+import org.apache.hadoop.hive.ql.ddl.DDLDesc.DDLDescWithWriteId;\n+import org.apache.hadoop.hive.ql.ddl.DDLWork;\n+import org.apache.hadoop.hive.ql.ddl.DDLSemanticAnalyzerFactory.DDLType;\n+import org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableAnalyzer;\n+import org.apache.hadoop.hive.ql.ddl.table.constraint.Constraints;\n+import org.apache.hadoop.hive.ql.ddl.table.constraint.ConstraintsUtils;\n+import org.apache.hadoop.hive.ql.ddl.table.partition.add.AlterTableAddPartitionDesc;\n+import org.apache.hadoop.hive.ql.exec.Task;\n+import org.apache.hadoop.hive.ql.exec.TaskFactory;\n+import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.parse.ASTNode;\n+import org.apache.hadoop.hive.ql.parse.HiveParser;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+\n+/**\n+ * Analyzer for add constraint commands.\n+ */\n+public abstract class AbstractConstraintAnalyzer extends AbstractAlterTableAnalyzer {\n+  public AbstractConstraintAnalyzer(QueryState queryState) throws SemanticException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22b5539518a8ad7c29ddd1bcf98ac72339f0dc6a"}, "originalPosition": 50}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "037f971687a7091d98a1b4792f80cdc451cb6357", "author": {"user": null}, "url": "https://github.com/apache/hive/commit/037f971687a7091d98a1b4792f80cdc451cb6357", "committedDate": "2020-12-10T22:43:05Z", "message": "Address Miklos feedback"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NzAzOTI2", "url": "https://github.com/apache/hive/pull/1737#pullrequestreview-549703926", "createdAt": "2020-12-10T23:53:38Z", "commit": {"oid": "037f971687a7091d98a1b4792f80cdc451cb6357"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUwNjE4MDIz", "url": "https://github.com/apache/hive/pull/1737#pullrequestreview-550618023", "createdAt": "2020-12-11T22:25:21Z", "commit": {"oid": "037f971687a7091d98a1b4792f80cdc451cb6357"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMjoyNToyMlrOIESHmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMjoyNToyMlrOIESHmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTM2MjA3NQ==", "bodyText": "Thanks. Looks good!", "url": "https://github.com/apache/hive/pull/1737#discussion_r541362075", "createdAt": "2020-12-11T22:25:22Z", "author": {"login": "vihangk1"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java", "diffHunk": "@@ -378,6 +378,33 @@ private IMetaStoreClient prepareParallelTest(String tableName, int val)\n     return msClient;\n   }\n \n+  @Test\n+  public void testAddConstraintAdvancingWriteIds() throws Exception {\n+\n+    String tableName = \"constraints_table\";\n+    hiveConf.setBoolean(\"hive.stats.autogather\", true);\n+    hiveConf.setBoolean(\"hive.stats.column.autogather\", true);\n+    // Need to close the thread local Hive object so that configuration change is reflected to HMS.\n+    Hive.closeCurrent();\n+    runStatementOnDriver(\"drop table if exists \" + tableName);\n+    runStatementOnDriver(String.format(\"create table %s (a int, b string) stored as orc \" +\n+        \"TBLPROPERTIES ('transactional'='true', 'transactional_properties'='insert_only')\",\n+        tableName));\n+    runStatementOnDriver(String.format(\"insert into %s (a) values (0)\", tableName));\n+    IMetaStoreClient msClient = new HiveMetaStoreClient(hiveConf);\n+    String validWriteIds = msClient.getValidWriteIds(\"default.\" + tableName).toString();\n+    LOG.info(\"ValidWriteIds before add constraint::\"+ validWriteIds);\n+    Assert.assertEquals(\"default.constraints_table:1:9223372036854775807::\", validWriteIds);\n+    runStatementOnDriver(String.format(\"alter table %s  ADD CONSTRAINT a_PK PRIMARY KEY (`a`) DISABLE NOVALIDATE\", tableName));\n+    validWriteIds  = msClient.getValidWriteIds(\"default.\" + tableName).toString();\n+    LOG.info(\"ValidWriteIds after add constraint primary key::\"+ validWriteIds);\n+    Assert.assertEquals(\"default.constraints_table:2:9223372036854775807::\", validWriteIds);\n+    runStatementOnDriver(String.format(\"alter table %s CHANGE COLUMN b b STRING NOT NULL\", tableName));\n+    validWriteIds  = msClient.getValidWriteIds(\"default.\" + tableName).toString();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU0NjAyOQ=="}, "originalCommit": {"oid": "261a8f822ac2260970dd70a5394d8d087e70f37a"}, "originalPosition": 26}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2865, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}