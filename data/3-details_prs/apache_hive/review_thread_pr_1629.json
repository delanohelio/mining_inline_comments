{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTEyNzEyNzU1", "number": 1629, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxNjoxNzowNlrOE3TV6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxNjo0ODo1MFrOE3UP3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NDI0MDQzOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxNjoxNzowNlrOHwkXEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxODoxNzo0OFrOHwpbOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY4OTQyNA==", "bodyText": "This is nice because it has been moved outside the lock scope.  I was double-checking if this variable can safely be examined outside the scope of the lock, and I noted that driverContext can never be null.  It is defined as final and set in the constructor.  Please remove this check.", "url": "https://github.com/apache/hive/pull/1629#discussion_r520689424", "createdAt": "2020-11-10T16:17:06Z", "author": {"login": "belugabehr"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -149,207 +149,64 @@ private CommandProcessorResponse run(String command, boolean alreadyCompiled) th\n       runInternal(command, alreadyCompiled);\n       return new CommandProcessorResponse(getSchema(), null);\n     } catch (CommandProcessorException cpe) {\n-      SessionState ss = SessionState.get();\n-      if (ss == null) {\n-        throw cpe;\n-      }\n-      MetaDataFormatter mdf = MetaDataFormatUtils.getFormatter(ss.getConf());\n-      if (!(mdf instanceof JsonMetaDataFormatter)) {\n-        throw cpe;\n-      }\n-      /*Here we want to encode the error in machine readable way (e.g. JSON)\n-       * Ideally, errorCode would always be set to a canonical error defined in ErrorMsg.\n-       * In practice that is rarely the case, so the messy logic below tries to tease\n-       * out canonical error code if it can.  Exclude stack trace from output when\n-       * the error is a specific/expected one.\n-       * It's written to stdout for backward compatibility (WebHCat consumes it).*/\n-      try {\n-        if (cpe.getCause() == null) {\n-          mdf.error(ss.out, cpe.getMessage(), cpe.getResponseCode(), cpe.getSqlState());\n-          throw cpe;\n-        }\n-        ErrorMsg canonicalErr = ErrorMsg.getErrorMsg(cpe.getResponseCode());\n-        if (canonicalErr != null && canonicalErr != ErrorMsg.GENERIC_ERROR) {\n-          /*Some HiveExceptions (e.g. SemanticException) don't set\n-            canonical ErrorMsg explicitly, but there is logic\n-            (e.g. #compile()) to find an appropriate canonical error and\n-            return its code as error code. In this case we want to\n-            preserve it for downstream code to interpret*/\n-          mdf.error(ss.out, cpe.getMessage(), cpe.getResponseCode(), cpe.getSqlState(), null);\n-          throw cpe;\n-        }\n-        if (cpe.getCause() instanceof HiveException) {\n-          HiveException rc = (HiveException)cpe.getCause();\n-          mdf.error(ss.out, cpe.getMessage(), rc.getCanonicalErrorMsg().getErrorCode(), cpe.getSqlState(),\n-              rc.getCanonicalErrorMsg() == ErrorMsg.GENERIC_ERROR ? StringUtils.stringifyException(rc) : null);\n-        } else {\n-          ErrorMsg canonicalMsg = ErrorMsg.getErrorMsg(cpe.getCause().getMessage());\n-          mdf.error(ss.out, cpe.getMessage(), canonicalMsg.getErrorCode(), cpe.getSqlState(),\n-              StringUtils.stringifyException(cpe.getCause()));\n-        }\n-      } catch (HiveException ex) {\n-        CONSOLE.printError(\"Unable to JSON-encode the error\", StringUtils.stringifyException(ex));\n-      }\n+      processRunException(cpe);\n       throw cpe;\n     }\n   }\n \n   private void runInternal(String command, boolean alreadyCompiled) throws CommandProcessorException {\n     DriverState.setDriverState(driverState);\n \n-    driverState.lock();\n-    try {\n-      if (driverContext != null && driverContext.getPlan() != null\n-          && driverContext.getPlan().isPrepareQuery()\n-          && !driverContext.getPlan().isExplain()) {\n-        LOG.info(\"Skip running tasks for prepare plan\");\n-        return;\n-      }\n-      if (alreadyCompiled) {\n-        if (driverState.isCompiled()) {\n-          driverState.executing();\n-        } else {\n-          String errorMessage = \"FAILED: Precompiled query has been cancelled or closed.\";\n-          CONSOLE.printError(errorMessage);\n-          throw DriverUtils.createProcessorException(driverContext, 12, errorMessage, null, null);\n-        }\n-      } else {\n-        driverState.compiling();\n-      }\n-    } finally {\n-      driverState.unlock();\n+    if (driverContext != null && driverContext.getPlan() != null &&", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7316cc96b7042a6227e6507e4cd265f97d91c93d"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY5MzQxOQ==", "bodyText": "I do see that the driverContext plan is set in a few different places.  I'm not sure if this can be checked outside of a lock.  At the very least, I would minimize the risk a bit:\nQueryPlan plan = driverContext.getPlan();\nif (plan != null\n          && plan.isPrepareQuery()\n          && !plan.isExplain()) {\n\nAt least you know that the value of plan cannot possibly change between calls.", "url": "https://github.com/apache/hive/pull/1629#discussion_r520693419", "createdAt": "2020-11-10T16:22:21Z", "author": {"login": "belugabehr"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -149,207 +149,64 @@ private CommandProcessorResponse run(String command, boolean alreadyCompiled) th\n       runInternal(command, alreadyCompiled);\n       return new CommandProcessorResponse(getSchema(), null);\n     } catch (CommandProcessorException cpe) {\n-      SessionState ss = SessionState.get();\n-      if (ss == null) {\n-        throw cpe;\n-      }\n-      MetaDataFormatter mdf = MetaDataFormatUtils.getFormatter(ss.getConf());\n-      if (!(mdf instanceof JsonMetaDataFormatter)) {\n-        throw cpe;\n-      }\n-      /*Here we want to encode the error in machine readable way (e.g. JSON)\n-       * Ideally, errorCode would always be set to a canonical error defined in ErrorMsg.\n-       * In practice that is rarely the case, so the messy logic below tries to tease\n-       * out canonical error code if it can.  Exclude stack trace from output when\n-       * the error is a specific/expected one.\n-       * It's written to stdout for backward compatibility (WebHCat consumes it).*/\n-      try {\n-        if (cpe.getCause() == null) {\n-          mdf.error(ss.out, cpe.getMessage(), cpe.getResponseCode(), cpe.getSqlState());\n-          throw cpe;\n-        }\n-        ErrorMsg canonicalErr = ErrorMsg.getErrorMsg(cpe.getResponseCode());\n-        if (canonicalErr != null && canonicalErr != ErrorMsg.GENERIC_ERROR) {\n-          /*Some HiveExceptions (e.g. SemanticException) don't set\n-            canonical ErrorMsg explicitly, but there is logic\n-            (e.g. #compile()) to find an appropriate canonical error and\n-            return its code as error code. In this case we want to\n-            preserve it for downstream code to interpret*/\n-          mdf.error(ss.out, cpe.getMessage(), cpe.getResponseCode(), cpe.getSqlState(), null);\n-          throw cpe;\n-        }\n-        if (cpe.getCause() instanceof HiveException) {\n-          HiveException rc = (HiveException)cpe.getCause();\n-          mdf.error(ss.out, cpe.getMessage(), rc.getCanonicalErrorMsg().getErrorCode(), cpe.getSqlState(),\n-              rc.getCanonicalErrorMsg() == ErrorMsg.GENERIC_ERROR ? StringUtils.stringifyException(rc) : null);\n-        } else {\n-          ErrorMsg canonicalMsg = ErrorMsg.getErrorMsg(cpe.getCause().getMessage());\n-          mdf.error(ss.out, cpe.getMessage(), canonicalMsg.getErrorCode(), cpe.getSqlState(),\n-              StringUtils.stringifyException(cpe.getCause()));\n-        }\n-      } catch (HiveException ex) {\n-        CONSOLE.printError(\"Unable to JSON-encode the error\", StringUtils.stringifyException(ex));\n-      }\n+      processRunException(cpe);\n       throw cpe;\n     }\n   }\n \n   private void runInternal(String command, boolean alreadyCompiled) throws CommandProcessorException {\n     DriverState.setDriverState(driverState);\n \n-    driverState.lock();\n-    try {\n-      if (driverContext != null && driverContext.getPlan() != null\n-          && driverContext.getPlan().isPrepareQuery()\n-          && !driverContext.getPlan().isExplain()) {\n-        LOG.info(\"Skip running tasks for prepare plan\");\n-        return;\n-      }\n-      if (alreadyCompiled) {\n-        if (driverState.isCompiled()) {\n-          driverState.executing();\n-        } else {\n-          String errorMessage = \"FAILED: Precompiled query has been cancelled or closed.\";\n-          CONSOLE.printError(errorMessage);\n-          throw DriverUtils.createProcessorException(driverContext, 12, errorMessage, null, null);\n-        }\n-      } else {\n-        driverState.compiling();\n-      }\n-    } finally {\n-      driverState.unlock();\n+    if (driverContext != null && driverContext.getPlan() != null &&", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY4OTQyNA=="}, "originalCommit": {"oid": "7316cc96b7042a6227e6507e4cd265f97d91c93d"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc3MjQxMA==", "bodyText": "Agree, nice catch, fixed.", "url": "https://github.com/apache/hive/pull/1629#discussion_r520772410", "createdAt": "2020-11-10T18:17:48Z", "author": {"login": "miklosgergely"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -149,207 +149,64 @@ private CommandProcessorResponse run(String command, boolean alreadyCompiled) th\n       runInternal(command, alreadyCompiled);\n       return new CommandProcessorResponse(getSchema(), null);\n     } catch (CommandProcessorException cpe) {\n-      SessionState ss = SessionState.get();\n-      if (ss == null) {\n-        throw cpe;\n-      }\n-      MetaDataFormatter mdf = MetaDataFormatUtils.getFormatter(ss.getConf());\n-      if (!(mdf instanceof JsonMetaDataFormatter)) {\n-        throw cpe;\n-      }\n-      /*Here we want to encode the error in machine readable way (e.g. JSON)\n-       * Ideally, errorCode would always be set to a canonical error defined in ErrorMsg.\n-       * In practice that is rarely the case, so the messy logic below tries to tease\n-       * out canonical error code if it can.  Exclude stack trace from output when\n-       * the error is a specific/expected one.\n-       * It's written to stdout for backward compatibility (WebHCat consumes it).*/\n-      try {\n-        if (cpe.getCause() == null) {\n-          mdf.error(ss.out, cpe.getMessage(), cpe.getResponseCode(), cpe.getSqlState());\n-          throw cpe;\n-        }\n-        ErrorMsg canonicalErr = ErrorMsg.getErrorMsg(cpe.getResponseCode());\n-        if (canonicalErr != null && canonicalErr != ErrorMsg.GENERIC_ERROR) {\n-          /*Some HiveExceptions (e.g. SemanticException) don't set\n-            canonical ErrorMsg explicitly, but there is logic\n-            (e.g. #compile()) to find an appropriate canonical error and\n-            return its code as error code. In this case we want to\n-            preserve it for downstream code to interpret*/\n-          mdf.error(ss.out, cpe.getMessage(), cpe.getResponseCode(), cpe.getSqlState(), null);\n-          throw cpe;\n-        }\n-        if (cpe.getCause() instanceof HiveException) {\n-          HiveException rc = (HiveException)cpe.getCause();\n-          mdf.error(ss.out, cpe.getMessage(), rc.getCanonicalErrorMsg().getErrorCode(), cpe.getSqlState(),\n-              rc.getCanonicalErrorMsg() == ErrorMsg.GENERIC_ERROR ? StringUtils.stringifyException(rc) : null);\n-        } else {\n-          ErrorMsg canonicalMsg = ErrorMsg.getErrorMsg(cpe.getCause().getMessage());\n-          mdf.error(ss.out, cpe.getMessage(), canonicalMsg.getErrorCode(), cpe.getSqlState(),\n-              StringUtils.stringifyException(cpe.getCause()));\n-        }\n-      } catch (HiveException ex) {\n-        CONSOLE.printError(\"Unable to JSON-encode the error\", StringUtils.stringifyException(ex));\n-      }\n+      processRunException(cpe);\n       throw cpe;\n     }\n   }\n \n   private void runInternal(String command, boolean alreadyCompiled) throws CommandProcessorException {\n     DriverState.setDriverState(driverState);\n \n-    driverState.lock();\n-    try {\n-      if (driverContext != null && driverContext.getPlan() != null\n-          && driverContext.getPlan().isPrepareQuery()\n-          && !driverContext.getPlan().isExplain()) {\n-        LOG.info(\"Skip running tasks for prepare plan\");\n-        return;\n-      }\n-      if (alreadyCompiled) {\n-        if (driverState.isCompiled()) {\n-          driverState.executing();\n-        } else {\n-          String errorMessage = \"FAILED: Precompiled query has been cancelled or closed.\";\n-          CONSOLE.printError(errorMessage);\n-          throw DriverUtils.createProcessorException(driverContext, 12, errorMessage, null, null);\n-        }\n-      } else {\n-        driverState.compiling();\n-      }\n-    } finally {\n-      driverState.unlock();\n+    if (driverContext != null && driverContext.getPlan() != null &&", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY4OTQyNA=="}, "originalCommit": {"oid": "7316cc96b7042a6227e6507e4cd265f97d91c93d"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NDMwNjA1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxNjozMDo0N1rOHwk_dQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxNjo0NDoxOFrOHwllnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY5OTc2NQ==", "bodyText": "Since you are slicing and dicing things, you may want to look at breaking this out a bit.\nIt is bad form to throw and exception, and then handle it, within the same method.\nI think this should be moved out of this try block.\n      if (retryShapshotCount > maxRetrySnapshotCount) {\n        // Throw exception\n        HiveException e = new HiveException(\n            \"Operation could not be executed, \" + SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED + \".\");\n        DriverUtils.handleHiveException(driverContext, e, 14, null);\n      }\n  if (retryShapshotCount != 0) {\n        // the reason that we set the txn manager for the cxt here is because each query has its own ctx object.\n        // The txn mgr is shared across the same instance of Driver, which can run multiple queries.\n        context.setHiveTxnManager(driverContext.getTxnManager());\n        return true;\n    }\n    return false;", "url": "https://github.com/apache/hive/pull/1629#discussion_r520699765", "createdAt": "2020-11-10T16:30:47Z", "author": {"login": "belugabehr"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -359,6 +216,101 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n     SessionState.getPerfLogger().cleanupPerfLogMetrics();\n   }\n \n+  /**\n+   * @return If the perfLogger should be reseted.\n+   */\n+  private boolean validateTxnList() throws CommandProcessorException {\n+    int retryShapshotCount = 0;\n+    int maxRetrySnapshotCount = HiveConf.getIntVar(driverContext.getConf(),\n+        HiveConf.ConfVars.HIVE_TXN_MAX_RETRYSNAPSHOT_COUNT);\n+\n+    try {\n+      do {\n+        driverContext.setOutdatedTxn(false);\n+        // Inserts will not invalidate the snapshot, that could cause duplicates.\n+        if (!driverTxnHandler.isValidTxnListState()) {\n+          LOG.info(\"Re-compiling after acquiring locks, attempt #\" + retryShapshotCount);\n+          // Snapshot was outdated when locks were acquired, hence regenerate context, txn list and retry.\n+          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n+          // Currently, we acquire a snapshot, compile the query with that snapshot, and then - acquire locks.\n+          // If snapshot is still valid, we continue as usual.\n+          // But if snapshot is not valid, we recompile the query.\n+          if (driverContext.isOutdatedTxn()) {\n+            // Later transaction invalidated the snapshot, a new transaction is required\n+            LOG.info(\"Snapshot is outdated, re-initiating transaction ...\");\n+            driverContext.getTxnManager().rollbackTxn();\n+\n+            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n+            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n+            lockAndRespond();\n+          }\n+          driverContext.setRetrial(true);\n+          driverContext.getBackupContext().addSubContext(context);\n+          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n+          context = driverContext.getBackupContext();\n+\n+          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n+              driverContext.getTxnManager().getValidTxns().toString());\n+\n+          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n+            compileInternal(context.getCmd(), true);\n+            driverTxnHandler.recordValidWriteIds();\n+            driverTxnHandler.setWriteIdForAcidFileSinks();\n+          }\n+          // Since we're reusing the compiled plan, we need to update its start time for current run\n+          driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n+        }\n+        // Re-check snapshot only in case we had to release locks and open a new transaction,\n+        // otherwise exclusive locks should protect output tables/partitions in snapshot from concurrent writes.\n+      } while (driverContext.isOutdatedTxn() && ++retryShapshotCount <= maxRetrySnapshotCount);\n+\n+      if (retryShapshotCount > maxRetrySnapshotCount) {\n+        // Throw exception\n+        HiveException e = new HiveException(\n+            \"Operation could not be executed, \" + SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED + \".\");\n+        DriverUtils.handleHiveException(driverContext, e, 14, null);\n+      } else if (retryShapshotCount != 0) {\n+        // the reason that we set the txn manager for the cxt here is because each query has its own ctx object.\n+        // The txn mgr is shared across the same instance of Driver, which can run multiple queries.\n+        context.setHiveTxnManager(driverContext.getTxnManager());\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7316cc96b7042a6227e6507e4cd265f97d91c93d"}, "originalPosition": 285}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDcwMTA5Nw==", "bodyText": "It is probably overwriting the error code of '14' with the outer-try-catch code of '13'.", "url": "https://github.com/apache/hive/pull/1629#discussion_r520701097", "createdAt": "2020-11-10T16:32:40Z", "author": {"login": "belugabehr"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -359,6 +216,101 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n     SessionState.getPerfLogger().cleanupPerfLogMetrics();\n   }\n \n+  /**\n+   * @return If the perfLogger should be reseted.\n+   */\n+  private boolean validateTxnList() throws CommandProcessorException {\n+    int retryShapshotCount = 0;\n+    int maxRetrySnapshotCount = HiveConf.getIntVar(driverContext.getConf(),\n+        HiveConf.ConfVars.HIVE_TXN_MAX_RETRYSNAPSHOT_COUNT);\n+\n+    try {\n+      do {\n+        driverContext.setOutdatedTxn(false);\n+        // Inserts will not invalidate the snapshot, that could cause duplicates.\n+        if (!driverTxnHandler.isValidTxnListState()) {\n+          LOG.info(\"Re-compiling after acquiring locks, attempt #\" + retryShapshotCount);\n+          // Snapshot was outdated when locks were acquired, hence regenerate context, txn list and retry.\n+          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n+          // Currently, we acquire a snapshot, compile the query with that snapshot, and then - acquire locks.\n+          // If snapshot is still valid, we continue as usual.\n+          // But if snapshot is not valid, we recompile the query.\n+          if (driverContext.isOutdatedTxn()) {\n+            // Later transaction invalidated the snapshot, a new transaction is required\n+            LOG.info(\"Snapshot is outdated, re-initiating transaction ...\");\n+            driverContext.getTxnManager().rollbackTxn();\n+\n+            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n+            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n+            lockAndRespond();\n+          }\n+          driverContext.setRetrial(true);\n+          driverContext.getBackupContext().addSubContext(context);\n+          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n+          context = driverContext.getBackupContext();\n+\n+          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n+              driverContext.getTxnManager().getValidTxns().toString());\n+\n+          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n+            compileInternal(context.getCmd(), true);\n+            driverTxnHandler.recordValidWriteIds();\n+            driverTxnHandler.setWriteIdForAcidFileSinks();\n+          }\n+          // Since we're reusing the compiled plan, we need to update its start time for current run\n+          driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n+        }\n+        // Re-check snapshot only in case we had to release locks and open a new transaction,\n+        // otherwise exclusive locks should protect output tables/partitions in snapshot from concurrent writes.\n+      } while (driverContext.isOutdatedTxn() && ++retryShapshotCount <= maxRetrySnapshotCount);\n+\n+      if (retryShapshotCount > maxRetrySnapshotCount) {\n+        // Throw exception\n+        HiveException e = new HiveException(\n+            \"Operation could not be executed, \" + SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED + \".\");\n+        DriverUtils.handleHiveException(driverContext, e, 14, null);\n+      } else if (retryShapshotCount != 0) {\n+        // the reason that we set the txn manager for the cxt here is because each query has its own ctx object.\n+        // The txn mgr is shared across the same instance of Driver, which can run multiple queries.\n+        context.setHiveTxnManager(driverContext.getTxnManager());\n+      }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY5OTc2NQ=="}, "originalCommit": {"oid": "7316cc96b7042a6227e6507e4cd265f97d91c93d"}, "originalPosition": 285}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDcwNzY0Nw==", "bodyText": "DriverUtils.handleHiveException throws a CommandProcessorException which is not caught, as it is neither a LockException nor a SemanticException. Still the block that you've mentioned can be moved to a separate method, but I don't see the meaning of the returned boolean. What would that be used for?", "url": "https://github.com/apache/hive/pull/1629#discussion_r520707647", "createdAt": "2020-11-10T16:41:46Z", "author": {"login": "miklosgergely"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -359,6 +216,101 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n     SessionState.getPerfLogger().cleanupPerfLogMetrics();\n   }\n \n+  /**\n+   * @return If the perfLogger should be reseted.\n+   */\n+  private boolean validateTxnList() throws CommandProcessorException {\n+    int retryShapshotCount = 0;\n+    int maxRetrySnapshotCount = HiveConf.getIntVar(driverContext.getConf(),\n+        HiveConf.ConfVars.HIVE_TXN_MAX_RETRYSNAPSHOT_COUNT);\n+\n+    try {\n+      do {\n+        driverContext.setOutdatedTxn(false);\n+        // Inserts will not invalidate the snapshot, that could cause duplicates.\n+        if (!driverTxnHandler.isValidTxnListState()) {\n+          LOG.info(\"Re-compiling after acquiring locks, attempt #\" + retryShapshotCount);\n+          // Snapshot was outdated when locks were acquired, hence regenerate context, txn list and retry.\n+          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n+          // Currently, we acquire a snapshot, compile the query with that snapshot, and then - acquire locks.\n+          // If snapshot is still valid, we continue as usual.\n+          // But if snapshot is not valid, we recompile the query.\n+          if (driverContext.isOutdatedTxn()) {\n+            // Later transaction invalidated the snapshot, a new transaction is required\n+            LOG.info(\"Snapshot is outdated, re-initiating transaction ...\");\n+            driverContext.getTxnManager().rollbackTxn();\n+\n+            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n+            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n+            lockAndRespond();\n+          }\n+          driverContext.setRetrial(true);\n+          driverContext.getBackupContext().addSubContext(context);\n+          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n+          context = driverContext.getBackupContext();\n+\n+          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n+              driverContext.getTxnManager().getValidTxns().toString());\n+\n+          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n+            compileInternal(context.getCmd(), true);\n+            driverTxnHandler.recordValidWriteIds();\n+            driverTxnHandler.setWriteIdForAcidFileSinks();\n+          }\n+          // Since we're reusing the compiled plan, we need to update its start time for current run\n+          driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n+        }\n+        // Re-check snapshot only in case we had to release locks and open a new transaction,\n+        // otherwise exclusive locks should protect output tables/partitions in snapshot from concurrent writes.\n+      } while (driverContext.isOutdatedTxn() && ++retryShapshotCount <= maxRetrySnapshotCount);\n+\n+      if (retryShapshotCount > maxRetrySnapshotCount) {\n+        // Throw exception\n+        HiveException e = new HiveException(\n+            \"Operation could not be executed, \" + SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED + \".\");\n+        DriverUtils.handleHiveException(driverContext, e, 14, null);\n+      } else if (retryShapshotCount != 0) {\n+        // the reason that we set the txn manager for the cxt here is because each query has its own ctx object.\n+        // The txn mgr is shared across the same instance of Driver, which can run multiple queries.\n+        context.setHiveTxnManager(driverContext.getTxnManager());\n+      }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY5OTc2NQ=="}, "originalCommit": {"oid": "7316cc96b7042a6227e6507e4cd265f97d91c93d"}, "originalPosition": 285}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDcwOTUzNQ==", "bodyText": "oh, I see the meaning of the return value, it is what needs to b returned from the original method. Sure, I can move this to a separate method, do you have a suggestion for the name?", "url": "https://github.com/apache/hive/pull/1629#discussion_r520709535", "createdAt": "2020-11-10T16:44:18Z", "author": {"login": "miklosgergely"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -359,6 +216,101 @@ private void runInternal(String command, boolean alreadyCompiled) throws Command\n     SessionState.getPerfLogger().cleanupPerfLogMetrics();\n   }\n \n+  /**\n+   * @return If the perfLogger should be reseted.\n+   */\n+  private boolean validateTxnList() throws CommandProcessorException {\n+    int retryShapshotCount = 0;\n+    int maxRetrySnapshotCount = HiveConf.getIntVar(driverContext.getConf(),\n+        HiveConf.ConfVars.HIVE_TXN_MAX_RETRYSNAPSHOT_COUNT);\n+\n+    try {\n+      do {\n+        driverContext.setOutdatedTxn(false);\n+        // Inserts will not invalidate the snapshot, that could cause duplicates.\n+        if (!driverTxnHandler.isValidTxnListState()) {\n+          LOG.info(\"Re-compiling after acquiring locks, attempt #\" + retryShapshotCount);\n+          // Snapshot was outdated when locks were acquired, hence regenerate context, txn list and retry.\n+          // TODO: Lock acquisition should be moved before analyze, this is a bit hackish.\n+          // Currently, we acquire a snapshot, compile the query with that snapshot, and then - acquire locks.\n+          // If snapshot is still valid, we continue as usual.\n+          // But if snapshot is not valid, we recompile the query.\n+          if (driverContext.isOutdatedTxn()) {\n+            // Later transaction invalidated the snapshot, a new transaction is required\n+            LOG.info(\"Snapshot is outdated, re-initiating transaction ...\");\n+            driverContext.getTxnManager().rollbackTxn();\n+\n+            String userFromUGI = DriverUtils.getUserFromUGI(driverContext);\n+            driverContext.getTxnManager().openTxn(context, userFromUGI, driverContext.getTxnType());\n+            lockAndRespond();\n+          }\n+          driverContext.setRetrial(true);\n+          driverContext.getBackupContext().addSubContext(context);\n+          driverContext.getBackupContext().setHiveLocks(context.getHiveLocks());\n+          context = driverContext.getBackupContext();\n+\n+          driverContext.getConf().set(ValidTxnList.VALID_TXNS_KEY,\n+              driverContext.getTxnManager().getValidTxns().toString());\n+\n+          if (driverContext.getPlan().hasAcidResourcesInQuery()) {\n+            compileInternal(context.getCmd(), true);\n+            driverTxnHandler.recordValidWriteIds();\n+            driverTxnHandler.setWriteIdForAcidFileSinks();\n+          }\n+          // Since we're reusing the compiled plan, we need to update its start time for current run\n+          driverContext.getPlan().setQueryStartTime(driverContext.getQueryDisplay().getQueryStartTime());\n+        }\n+        // Re-check snapshot only in case we had to release locks and open a new transaction,\n+        // otherwise exclusive locks should protect output tables/partitions in snapshot from concurrent writes.\n+      } while (driverContext.isOutdatedTxn() && ++retryShapshotCount <= maxRetrySnapshotCount);\n+\n+      if (retryShapshotCount > maxRetrySnapshotCount) {\n+        // Throw exception\n+        HiveException e = new HiveException(\n+            \"Operation could not be executed, \" + SNAPSHOT_WAS_OUTDATED_WHEN_LOCKS_WERE_ACQUIRED + \".\");\n+        DriverUtils.handleHiveException(driverContext, e, 14, null);\n+      } else if (retryShapshotCount != 0) {\n+        // the reason that we set the txn manager for the cxt here is because each query has its own ctx object.\n+        // The txn mgr is shared across the same instance of Driver, which can run multiple queries.\n+        context.setHiveTxnManager(driverContext.getTxnManager());\n+      }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY5OTc2NQ=="}, "originalCommit": {"oid": "7316cc96b7042a6227e6507e4cd265f97d91c93d"}, "originalPosition": 285}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NDM1ODE3OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxNjo0MjoxM1rOHwlfmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxODoxOToxMVrOHwpeRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDcwNzk5NQ==", "bodyText": "This is a little hard to follow.  Perhaps something like:\n    if (driverContext.getResStream() == null) {\n       // If the driver does not have a stream and neither does the context, return\n       Stream contextStream = context.getStream();\n       if (contextStream == null) {\n         return false;\n       }\n       driverContext.setResStream(contextStream);\n      }\n    }", "url": "https://github.com/apache/hive/pull/1629#discussion_r520707995", "createdAt": "2020-11-10T16:42:13Z", "author": {"login": "belugabehr"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -638,46 +654,28 @@ public boolean getResults(List results) throws IOException {\n     }\n \n     if (isFetchingTable()) {\n-      /**\n-       * If resultset serialization to thrift object is enabled, and if the destination table is\n-       * indeed written using ThriftJDBCBinarySerDe, read one row from the output sequence file,\n-       * since it is a blob of row batches.\n-       */\n-      if (driverContext.getFetchTask().getWork().isUsingThriftJDBCBinarySerDe()) {\n-        maxRows = 1;\n-      }\n-      driverContext.getFetchTask().setMaxRows(maxRows);\n-      return driverContext.getFetchTask().fetch(results);\n+      return getFetchingTableResults(results);\n     }\n \n     if (driverContext.getResStream() == null) {\n       driverContext.setResStream(context.getStream());\n-    }\n-    if (driverContext.getResStream() == null) {\n-      return false;\n+      if (driverContext.getResStream() == null) {\n+        return false;\n+      }\n     }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7316cc96b7042a6227e6507e4cd265f97d91c93d"}, "originalPosition": 433}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc3MzE5MQ==", "bodyText": "I agree, it's nicer like this, fixed.", "url": "https://github.com/apache/hive/pull/1629#discussion_r520773191", "createdAt": "2020-11-10T18:19:11Z", "author": {"login": "miklosgergely"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -638,46 +654,28 @@ public boolean getResults(List results) throws IOException {\n     }\n \n     if (isFetchingTable()) {\n-      /**\n-       * If resultset serialization to thrift object is enabled, and if the destination table is\n-       * indeed written using ThriftJDBCBinarySerDe, read one row from the output sequence file,\n-       * since it is a blob of row batches.\n-       */\n-      if (driverContext.getFetchTask().getWork().isUsingThriftJDBCBinarySerDe()) {\n-        maxRows = 1;\n-      }\n-      driverContext.getFetchTask().setMaxRows(maxRows);\n-      return driverContext.getFetchTask().fetch(results);\n+      return getFetchingTableResults(results);\n     }\n \n     if (driverContext.getResStream() == null) {\n       driverContext.setResStream(context.getStream());\n-    }\n-    if (driverContext.getResStream() == null) {\n-      return false;\n+      if (driverContext.getResStream() == null) {\n+        return false;\n+      }\n     }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDcwNzk5NQ=="}, "originalCommit": {"oid": "7316cc96b7042a6227e6507e4cd265f97d91c93d"}, "originalPosition": 433}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NDM3NTAyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxNjo0NTo1MFrOHwlp6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxODoxNzo1OVrOHwpblQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDcxMDYzMw==", "bodyText": "nit: Safer to make this final to ensure that there aren't multiple paths later if this code changes.", "url": "https://github.com/apache/hive/pull/1629#discussion_r520710633", "createdAt": "2020-11-10T16:45:50Z", "author": {"login": "belugabehr"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -687,13 +685,36 @@ public boolean getResults(List results) throws IOException {\n         return false;\n       }\n \n-      if (ss == Utilities.StreamStatus.EOF) {\n+      if (streamStatus == Utilities.StreamStatus.EOF) {\n         driverContext.setResStream(context.getStream());\n       }\n     }\n     return true;\n   }\n \n+  @SuppressWarnings(\"rawtypes\")\n+  private boolean getFetchingTableResults(List results) throws IOException {\n+    // If result set serialization to thrift object is enabled, and if the destination table is indeed written using\n+    // ThriftJDBCBinarySerDe, read one row from the output sequence file, since it is a blob of row batches.\n+    if (driverContext.getFetchTask().getWork().isUsingThriftJDBCBinarySerDe()) {\n+      maxRows = 1;\n+    }\n+    driverContext.getFetchTask().setMaxRows(maxRows);\n+    return driverContext.getFetchTask().fetch(results);\n+  }\n+\n+  private String getRow(ByteStream.Output bos, Utilities.StreamStatus streamStatus) {\n+    String row;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7316cc96b7042a6227e6507e4cd265f97d91c93d"}, "originalPosition": 486}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc3MjUwMQ==", "bodyText": "final added.", "url": "https://github.com/apache/hive/pull/1629#discussion_r520772501", "createdAt": "2020-11-10T18:17:59Z", "author": {"login": "miklosgergely"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "diffHunk": "@@ -687,13 +685,36 @@ public boolean getResults(List results) throws IOException {\n         return false;\n       }\n \n-      if (ss == Utilities.StreamStatus.EOF) {\n+      if (streamStatus == Utilities.StreamStatus.EOF) {\n         driverContext.setResStream(context.getStream());\n       }\n     }\n     return true;\n   }\n \n+  @SuppressWarnings(\"rawtypes\")\n+  private boolean getFetchingTableResults(List results) throws IOException {\n+    // If result set serialization to thrift object is enabled, and if the destination table is indeed written using\n+    // ThriftJDBCBinarySerDe, read one row from the output sequence file, since it is a blob of row batches.\n+    if (driverContext.getFetchTask().getWork().isUsingThriftJDBCBinarySerDe()) {\n+      maxRows = 1;\n+    }\n+    driverContext.getFetchTask().setMaxRows(maxRows);\n+    return driverContext.getFetchTask().fetch(results);\n+  }\n+\n+  private String getRow(ByteStream.Output bos, Utilities.StreamStatus streamStatus) {\n+    String row;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDcxMDYzMw=="}, "originalCommit": {"oid": "7316cc96b7042a6227e6507e4cd265f97d91c93d"}, "originalPosition": 486}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NDM4ODI1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxNjo0ODo0NFrOHwlyXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxODoxNzoxNVrOHwpZ8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDcxMjc5Nw==", "bodyText": "This is a good change, but please revert for this PR to keep it focused on the Driver classes.", "url": "https://github.com/apache/hive/pull/1629#discussion_r520712797", "createdAt": "2020-11-10T16:48:44Z", "author": {"login": "belugabehr"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java", "diffHunk": "@@ -73,7 +73,7 @@ private static IReExecutionPlugin buildReExecPlugin(String name) throws RuntimeE\n     if(\"reexecute_lost_am\".equals(name)) {\n       return new ReExecuteLostAMQueryPlugin();\n     }\n-    if (name.equals(\"dagsubmit\")) {\n+    if (\"dagsubmit\".equals(name)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7316cc96b7042a6227e6507e4cd265f97d91c93d"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc3MjA4Mg==", "bodyText": "OK, removed.", "url": "https://github.com/apache/hive/pull/1629#discussion_r520772082", "createdAt": "2020-11-10T18:17:15Z", "author": {"login": "miklosgergely"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/DriverFactory.java", "diffHunk": "@@ -73,7 +73,7 @@ private static IReExecutionPlugin buildReExecPlugin(String name) throws RuntimeE\n     if(\"reexecute_lost_am\".equals(name)) {\n       return new ReExecuteLostAMQueryPlugin();\n     }\n-    if (name.equals(\"dagsubmit\")) {\n+    if (\"dagsubmit\".equals(name)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDcxMjc5Nw=="}, "originalCommit": {"oid": "7316cc96b7042a6227e6507e4cd265f97d91c93d"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NDM4ODc2OnYy", "diffSide": "LEFT", "path": "ql/src/java/org/apache/hadoop/hive/ql/DriverTxnHandler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxNjo0ODo1MFrOHwlyqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxODoxNzoyN1rOHwpaYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDcxMjg3Mw==", "bodyText": "This is a good change, but please revert for this PR to keep it focused on the Driver classes.", "url": "https://github.com/apache/hive/pull/1629#discussion_r520712873", "createdAt": "2020-11-10T16:48:50Z", "author": {"login": "belugabehr"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/DriverTxnHandler.java", "diffHunk": "@@ -67,7 +67,6 @@\n import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;\n import org.apache.hadoop.util.StringUtils;\n import org.apache.hive.common.util.ShutdownHookManager;\n-import org.apache.hive.common.util.TxnIdUtils;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7316cc96b7042a6227e6507e4cd265f97d91c93d"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc3MjE5Mw==", "bodyText": "OK, removed.", "url": "https://github.com/apache/hive/pull/1629#discussion_r520772193", "createdAt": "2020-11-10T18:17:27Z", "author": {"login": "miklosgergely"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/DriverTxnHandler.java", "diffHunk": "@@ -67,7 +67,6 @@\n import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;\n import org.apache.hadoop.util.StringUtils;\n import org.apache.hive.common.util.ShutdownHookManager;\n-import org.apache.hive.common.util.TxnIdUtils;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDcxMjg3Mw=="}, "originalCommit": {"oid": "7316cc96b7042a6227e6507e4cd265f97d91c93d"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 207, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}