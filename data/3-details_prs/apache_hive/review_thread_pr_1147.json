{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM2NDkxODUx", "number": 1147, "reviewThreads": {"totalCount": 63, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo1MDo1NFrOEPn5Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMzowMTo0MFrOEUyPCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODE3NzY3OnYy", "diffSide": "RIGHT", "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo1MDo1NVrOGzb70Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwNToxOToxN1rOG9Mzdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4ODI0MQ==", "bodyText": "@maheshk114 Have you run all the tests with this feature set to true by default? This change touches existing logic/code and we should definitely run all the existing tests with this set to TRUE.", "url": "https://github.com/apache/hive/pull/1147#discussion_r456588241", "createdAt": "2020-07-17T17:50:55Z", "author": {"login": "vineetgarg02"}, "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "diffHunk": "@@ -2162,7 +2162,8 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n         \"Whether Hive enables the optimization about converting common join into mapjoin based on the input file size. \\n\" +\n         \"If this parameter is on, and the sum of size for n-1 of the tables/partitions for a n-way join is smaller than the\\n\" +\n         \"specified size, the join is directly converted to a mapjoin (there is no conditional task).\"),\n-\n+    HIVE_CONVERT_ANTI_JOIN(\"hive.auto.convert.anti.join\", false,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MzkwOA==", "bodyText": "Yes, i had triggered a ptest run with this config enabled to true by default. There were some 26 failures. I had analyzed those and some fixes were done to make sure that the result is same for both and difference in plan is as expected.", "url": "https://github.com/apache/hive/pull/1147#discussion_r456593908", "createdAt": "2020-07-17T18:01:08Z", "author": {"login": "maheshk114"}, "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "diffHunk": "@@ -2162,7 +2162,8 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n         \"Whether Hive enables the optimization about converting common join into mapjoin based on the input file size. \\n\" +\n         \"If this parameter is on, and the sum of size for n-1 of the tables/partitions for a n-way join is smaller than the\\n\" +\n         \"specified size, the join is directly converted to a mapjoin (there is no conditional task).\"),\n-\n+    HIVE_CONVERT_ANTI_JOIN(\"hive.auto.convert.anti.join\", false,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4ODI0MQ=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgyOTgyMA==", "bodyText": "Is there any reason why we should not enable this by default in master? It seems it is always beneficial to execute the antijoin since we already have a vectorized implementation too. That would increase the test coverage for the feature.", "url": "https://github.com/apache/hive/pull/1147#discussion_r457829820", "createdAt": "2020-07-21T04:32:08Z", "author": {"login": "jcamachor"}, "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "diffHunk": "@@ -2162,7 +2162,8 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n         \"Whether Hive enables the optimization about converting common join into mapjoin based on the input file size. \\n\" +\n         \"If this parameter is on, and the sum of size for n-1 of the tables/partitions for a n-way join is smaller than the\\n\" +\n         \"specified size, the join is directly converted to a mapjoin (there is no conditional task).\"),\n-\n+    HIVE_CONVERT_ANTI_JOIN(\"hive.auto.convert.anti.join\", false,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4ODI0MQ=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODA4MjI0Mw==", "bodyText": "Agree with the above, I believe we should enable anti-join by default as 1) this feature should aways improve runtime 2) can help us find possible issues and 3) further optimize existing implementation based on future scenarios", "url": "https://github.com/apache/hive/pull/1147#discussion_r458082243", "createdAt": "2020-07-21T13:09:18Z", "author": {"login": "pgaref"}, "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "diffHunk": "@@ -2162,7 +2162,8 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n         \"Whether Hive enables the optimization about converting common join into mapjoin based on the input file size. \\n\" +\n         \"If this parameter is on, and the sum of size for n-1 of the tables/partitions for a n-way join is smaller than the\\n\" +\n         \"specified size, the join is directly converted to a mapjoin (there is no conditional task).\"),\n-\n+    HIVE_CONVERT_ANTI_JOIN(\"hive.auto.convert.anti.join\", false,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4ODI0MQ=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgyNjEwMw==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r466826103", "createdAt": "2020-08-07T05:19:17Z", "author": {"login": "maheshk114"}, "path": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "diffHunk": "@@ -2162,7 +2162,8 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n         \"Whether Hive enables the optimization about converting common join into mapjoin based on the input file size. \\n\" +\n         \"If this parameter is on, and the sum of size for n-1 of the tables/partitions for a n-way join is smaller than the\\n\" +\n         \"specified size, the join is directly converted to a mapjoin (there is no conditional task).\"),\n-\n+    HIVE_CONVERT_ANTI_JOIN(\"hive.auto.convert.anti.join\", false,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4ODI0MQ=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODE4MTk5OnYy", "diffSide": "RIGHT", "path": "ql/src/test/results/clientpositive/llap/antijoin.q.out", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo1MjoxOVrOGzb-ew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwNToxMDowMVrOG9Mqhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4ODkyMw==", "bodyText": "How was the correctness of results verified?", "url": "https://github.com/apache/hive/pull/1147#discussion_r456588923", "createdAt": "2020-07-17T17:52:19Z", "author": {"login": "vineetgarg02"}, "path": "ql/src/test/results/clientpositive/llap/antijoin.q.out", "diffHunk": "@@ -0,0 +1,1007 @@\n+PREHOOK: query: create table t1_n55 as select cast(key as int) key, value from src where key <= 10\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@t1_n55\n+POSTHOOK: query: create table t1_n55 as select cast(key as int) key, value from src where key <= 10\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@t1_n55\n+POSTHOOK: Lineage: t1_n55.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: t1_n55.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: select * from t1_n55 sort by key\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@t1_n55\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from t1_n55 sort by key\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@t1_n55\n+#### A masked pattern was here ####\n+0\tval_0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NDYwOA==", "bodyText": "These all new test cases are added from the failure test cases of  a dry run with anti join enabled  true. Manually i have verified that the resultant records are same and plan difference is as per expected behavior.", "url": "https://github.com/apache/hive/pull/1147#discussion_r456594608", "createdAt": "2020-07-17T18:02:40Z", "author": {"login": "maheshk114"}, "path": "ql/src/test/results/clientpositive/llap/antijoin.q.out", "diffHunk": "@@ -0,0 +1,1007 @@\n+PREHOOK: query: create table t1_n55 as select cast(key as int) key, value from src where key <= 10\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@t1_n55\n+POSTHOOK: query: create table t1_n55 as select cast(key as int) key, value from src where key <= 10\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@t1_n55\n+POSTHOOK: Lineage: t1_n55.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: t1_n55.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: select * from t1_n55 sort by key\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@t1_n55\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from t1_n55 sort by key\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@t1_n55\n+#### A masked pattern was here ####\n+0\tval_0", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4ODkyMw=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgyMzgxNQ==", "bodyText": "now i have made anti join conversion to true by default", "url": "https://github.com/apache/hive/pull/1147#discussion_r466823815", "createdAt": "2020-08-07T05:10:01Z", "author": {"login": "maheshk114"}, "path": "ql/src/test/results/clientpositive/llap/antijoin.q.out", "diffHunk": "@@ -0,0 +1,1007 @@\n+PREHOOK: query: create table t1_n55 as select cast(key as int) key, value from src where key <= 10\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@t1_n55\n+POSTHOOK: query: create table t1_n55 as select cast(key as int) key, value from src where key <= 10\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@t1_n55\n+POSTHOOK: Lineage: t1_n55.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: t1_n55.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: select * from t1_n55 sort by key\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@t1_n55\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from t1_n55 sort by key\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@t1_n55\n+#### A masked pattern was here ####\n+0\tval_0", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4ODkyMw=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MTM4MDEzOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQwMzoxMzo1NlrOGz1nVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxNjozNzoxNFrOG0WVeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzAwODk4Mw==", "bodyText": "I think this can be moved down after all the condition checks below and return statements and within a isDebugEnabled check?", "url": "https://github.com/apache/hive/pull/1147#discussion_r457008983", "createdAt": "2020-07-20T03:13:56Z", "author": {"login": "ramesh0201"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveJoinWithFilterToAntiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveJoinWithFilterToAntiJoinRule.class);\n+  public static final HiveJoinWithFilterToAntiJoinRule INSTANCE = new HiveJoinWithFilterToAntiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveJoinWithFilterToAntiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Matched HiveAntiJoinRule\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzU0NTA4MQ==", "bodyText": "sure ..will do that", "url": "https://github.com/apache/hive/pull/1147#discussion_r457545081", "createdAt": "2020-07-20T16:37:14Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveJoinWithFilterToAntiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveJoinWithFilterToAntiJoinRule.class);\n+  public static final HiveJoinWithFilterToAntiJoinRule INSTANCE = new HiveJoinWithFilterToAntiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveJoinWithFilterToAntiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Matched HiveAntiJoinRule\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzAwODk4Mw=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1Njk0MDgxOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNDozMzoxNVrOG0nuzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjoyNzozOFrOG3L_qA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzMDA5Mw==", "bodyText": "nit. Fwd -> Forward", "url": "https://github.com/apache/hive/pull/1147#discussion_r457830093", "createdAt": "2020-07-21T04:33:15Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "diffHunk": "@@ -509,11 +513,17 @@ protected void addToAliasFilterTags(byte alias, List<Object> object, boolean isN\n     }\n   }\n \n+  private void createForwardJoinObjectForAntiJoin(boolean[] skip) throws HiveException {\n+    boolean forward = fillFwdCache(skip);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUyMTM4NA==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460521384", "createdAt": "2020-07-26T12:27:38Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "diffHunk": "@@ -509,11 +513,17 @@ protected void addToAliasFilterTags(byte alias, List<Object> object, boolean isN\n     }\n   }\n \n+  private void createForwardJoinObjectForAntiJoin(boolean[] skip) throws HiveException {\n+    boolean forward = fillFwdCache(skip);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzMDA5Mw=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1Njk0ODIwOnYy", "diffSide": "RIGHT", "path": "parser/src/java/org/apache/hadoop/hive/ql/parse/FromClauseParser.g", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNDozODoxMFrOG0nzJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjoyNjozMVrOG3L_Hg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzMTIwNw==", "bodyText": "Since we are exposing this and to prevent any ambiguity, should we use:\nKW_LEFT KW_ANTI KW_JOIN -> TOK_LEFTANTISEMIJOIN", "url": "https://github.com/apache/hive/pull/1147#discussion_r457831207", "createdAt": "2020-07-21T04:38:10Z", "author": {"login": "jcamachor"}, "path": "parser/src/java/org/apache/hadoop/hive/ql/parse/FromClauseParser.g", "diffHunk": "@@ -145,6 +145,7 @@ joinToken\n     | KW_RIGHT (KW_OUTER)? KW_JOIN -> TOK_RIGHTOUTERJOIN\n     | KW_FULL  (KW_OUTER)? KW_JOIN -> TOK_FULLOUTERJOIN\n     | KW_LEFT KW_SEMI KW_JOIN      -> TOK_LEFTSEMIJOIN\n+    | KW_ANTI KW_JOIN      -> TOK_ANTIJOIN", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUyMTI0Ng==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460521246", "createdAt": "2020-07-26T12:26:31Z", "author": {"login": "maheshk114"}, "path": "parser/src/java/org/apache/hadoop/hive/ql/parse/FromClauseParser.g", "diffHunk": "@@ -145,6 +145,7 @@ joinToken\n     | KW_RIGHT (KW_OUTER)? KW_JOIN -> TOK_RIGHTOUTERJOIN\n     | KW_FULL  (KW_OUTER)? KW_JOIN -> TOK_FULLOUTERJOIN\n     | KW_LEFT KW_SEMI KW_JOIN      -> TOK_LEFTSEMIJOIN\n+    | KW_ANTI KW_JOIN      -> TOK_ANTIJOIN", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzMTIwNw=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1Njk0OTQ4OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNDozOTowM1rOG0nz8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjoyNDo0M1rOG3L-YA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzMTQwOA==", "bodyText": "hasAntiJoin -> hasLeftAntiSemiJoin\nAccordingly in other places. This fits well with other logic where you use isSemiJoin boolean... Now it is less ambiguous that it applies to both.", "url": "https://github.com/apache/hive/pull/1147#discussion_r457831408", "createdAt": "2020-07-21T04:39:03Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "diffHunk": "@@ -153,6 +153,8 @@\n \n   transient boolean hasLeftSemiJoin = false;\n \n+  transient boolean hasAntiJoin = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUyMTA1Ng==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460521056", "createdAt": "2020-07-26T12:24:43Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "diffHunk": "@@ -153,6 +153,8 @@\n \n   transient boolean hasLeftSemiJoin = false;\n \n+  transient boolean hasAntiJoin = false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzMTQwOA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1Njk1NDA2OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinGenerateResultOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNDo0MTo1NlrOG0n2ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwNToyNToxMlrOG18m_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzMjA5NA==", "bodyText": "Can we create a JIRA for this and link it to HIVE-23716?", "url": "https://github.com/apache/hive/pull/1147#discussion_r457832094", "createdAt": "2020-07-21T04:41:56Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinGenerateResultOperator.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashSet;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashSetResult;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashTableResult;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+// TODO : This class is duplicate of semi join. Need to do a refactoring to merge it with semi join.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIyMDczNA==", "bodyText": "https://issues.apache.org/jira/browse/HIVE-23905", "url": "https://github.com/apache/hive/pull/1147#discussion_r459220734", "createdAt": "2020-07-23T05:25:12Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinGenerateResultOperator.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashSet;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashSetResult;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashTableResult;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+// TODO : This class is duplicate of semi join. Need to do a refactoring to merge it with semi join.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzMjA5NA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1Njk2MTE5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelFactories.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNDo0NTo1NFrOG0n6yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjoyMzoyOFrOG3L94g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzMzE2MA==", "bodyText": "Why is HiveAntiJoinFactoryImpl extending SemiJoinFactory? I think it is not used... Can we remove it?", "url": "https://github.com/apache/hive/pull/1147#discussion_r457833160", "createdAt": "2020-07-21T04:45:54Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelFactories.java", "diffHunk": "@@ -188,6 +193,20 @@ public RelNode createSemiJoin(RelNode left, RelNode right,\n     }\n   }\n \n+  /**\n+   * Implementation of {@link AntiJoinFactory} that returns\n+   * {@link org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAntiJoin}\n+   * .\n+   */\n+  private static class HiveAntiJoinFactoryImpl implements SemiJoinFactory {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUyMDkzMA==", "bodyText": "HiveAntiJoinFactoryImpl is removed", "url": "https://github.com/apache/hive/pull/1147#discussion_r460520930", "createdAt": "2020-07-26T12:23:28Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelFactories.java", "diffHunk": "@@ -188,6 +193,20 @@ public RelNode createSemiJoin(RelNode left, RelNode right,\n     }\n   }\n \n+  /**\n+   * Implementation of {@link AntiJoinFactory} that returns\n+   * {@link org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAntiJoin}\n+   * .\n+   */\n+  private static class HiveAntiJoinFactoryImpl implements SemiJoinFactory {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzMzE2MA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1Njk2MzM3OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelOptMaterializationValidator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNDo0NzoxN1rOG0n8Eg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjoyMzoxNlrOG3L9xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzMzQ5MA==", "bodyText": "Not currently part of the HiveRelNode interface? What does that mean? HiveAntiJoin is implementing HiveRelNode.", "url": "https://github.com/apache/hive/pull/1147#discussion_r457833490", "createdAt": "2020-07-21T04:47:17Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelOptMaterializationValidator.java", "diffHunk": "@@ -253,6 +256,14 @@ private RelNode visit(HiveSemiJoin semiJoin) {\n     return visitChildren(semiJoin);\n   }\n \n+  // Note: Not currently part of the HiveRelNode interface\n+  private RelNode visit(HiveAntiJoin antiJoin) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUyMDkwMw==", "bodyText": "Not sure ..copy pasted from semi join.", "url": "https://github.com/apache/hive/pull/1147#discussion_r460520903", "createdAt": "2020-07-26T12:23:16Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelOptMaterializationValidator.java", "diffHunk": "@@ -253,6 +256,14 @@ private RelNode visit(HiveSemiJoin semiJoin) {\n     return visitChildren(semiJoin);\n   }\n \n+  // Note: Not currently part of the HiveRelNode interface\n+  private RelNode visit(HiveAntiJoin antiJoin) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzMzQ5MA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1Njk2ODQyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelOptUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNDo1MDoxOVrOG0n_Mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwNzowOTozNVrOG1-mkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzNDI5MA==", "bodyText": "This is interesting. An antijoin of a PK-FK join returns no rows? Can we create a JIRA for such optimization based on integrity constraints?", "url": "https://github.com/apache/hive/pull/1147#discussion_r457834290", "createdAt": "2020-07-21T04:50:19Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelOptUtil.java", "diffHunk": "@@ -747,7 +747,7 @@ public static RewritablePKFKJoinInfo isRewritablePKFKJoin(Join join,\n     final RelNode nonFkInput = leftInputPotentialFK ? join.getRight() : join.getLeft();\n     final RewritablePKFKJoinInfo nonRewritable = RewritablePKFKJoinInfo.of(false, null);\n \n-    if (joinType != JoinRelType.INNER && !join.isSemiJoin()) {\n+    if (joinType != JoinRelType.INNER && !join.isSemiJoin() && joinType != JoinRelType.ANTI) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI1MzM5Mg==", "bodyText": "https://issues.apache.org/jira/browse/HIVE-23906", "url": "https://github.com/apache/hive/pull/1147#discussion_r459253392", "createdAt": "2020-07-23T07:09:35Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelOptUtil.java", "diffHunk": "@@ -747,7 +747,7 @@ public static RewritablePKFKJoinInfo isRewritablePKFKJoin(Join join,\n     final RelNode nonFkInput = leftInputPotentialFK ? join.getRight() : join.getLeft();\n     final RewritablePKFKJoinInfo nonRewritable = RewritablePKFKJoinInfo.of(false, null);\n \n-    if (joinType != JoinRelType.INNER && !join.isSemiJoin()) {\n+    if (joinType != JoinRelType.INNER && !join.isSemiJoin() && joinType != JoinRelType.ANTI) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzNDI5MA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1Njk3MDcxOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveSubQRemoveRelBuilder.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNDo1MTozMlrOG0oAkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjoyMDozNFrOG3L8xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzNDY0Mg==", "bodyText": "Should we add precondition for semiJoinType that is either SEMI or ANTI?", "url": "https://github.com/apache/hive/pull/1147#discussion_r457834642", "createdAt": "2020-07-21T04:51:32Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveSubQRemoveRelBuilder.java", "diffHunk": "@@ -1112,7 +1112,7 @@ public RexNode field(RexNode e, String name) {\n   }\n \n   public HiveSubQRemoveRelBuilder join(JoinRelType joinType, RexNode condition,\n-                                       Set<CorrelationId> variablesSet, boolean createSemiJoin) {\n+                                       Set<CorrelationId> variablesSet, JoinRelType semiJoinType) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUyMDY0Nw==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460520647", "createdAt": "2020-07-26T12:20:34Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveSubQRemoveRelBuilder.java", "diffHunk": "@@ -1112,7 +1112,7 @@ public RexNode field(RexNode e, String name) {\n   }\n \n   public HiveSubQRemoveRelBuilder join(JoinRelType joinType, RexNode condition,\n-                                       Set<CorrelationId> variablesSet, boolean createSemiJoin) {\n+                                       Set<CorrelationId> variablesSet, JoinRelType semiJoinType) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzNDY0Mg=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1Njk3MzE3OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveAntiJoin.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNDo1Mjo1MVrOG0oB-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwMjozNToyMlrOG2hjOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzNTAwMA==", "bodyText": "Can we add a comment explaining what joinFilter holds?\nIs this an aux data structure? Will condition in Join hold the full condition? I am asking because it is important that digest contains the full condition so Calcite does not think that two operators are equivalent when they are not.", "url": "https://github.com/apache/hive/pull/1147#discussion_r457835000", "createdAt": "2020-07-21T04:52:51Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveAntiJoin.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.reloperators;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Sets;\n+import org.apache.calcite.plan.RelOptCluster;\n+import org.apache.calcite.plan.RelTraitSet;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.CalciteSemanticException;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRulesRegistry;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HiveAntiJoin extends Join implements HiveRelNode {\n+\n+  private final RexNode joinFilter;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgyNTk3Nw==", "bodyText": "The joinjoinFilter holds the residual filter which is used during post processing. These are the join conditions that are not part of the join key. I think condition in Join hold the full condition.", "url": "https://github.com/apache/hive/pull/1147#discussion_r459825977", "createdAt": "2020-07-24T02:35:22Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveAntiJoin.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.reloperators;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Sets;\n+import org.apache.calcite.plan.RelOptCluster;\n+import org.apache.calcite.plan.RelTraitSet;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.CalciteSemanticException;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRulesRegistry;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HiveAntiJoin extends Join implements HiveRelNode {\n+\n+  private final RexNode joinFilter;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgzNTAwMA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1ODkxMDc3OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNDoyMjoyM1rOG06Zvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwMTozODo1NFrOG3RIOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODEzNTk5OQ==", "bodyText": "nit: if inner join found a match.", "url": "https://github.com/apache/hive/pull/1147#discussion_r458135999", "createdAt": "2020-07-21T14:22:23Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "diffHunk": "@@ -638,6 +657,12 @@ private void genObject(int aliasNum, boolean allLeftFirst, boolean allLeftNull)\n           // skipping the rest of the rows in the rhs table of the semijoin\n           done = !needsPostEvaluation;\n         }\n+      } else if (type == JoinDesc.ANTI_JOIN) {\n+        if (innerJoin(skip, left, right)) {\n+          // if anti join found a match then the condition is not matched for anti join, so we can skip rest of the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYwNTQ5OA==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460605498", "createdAt": "2020-07-27T01:38:54Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "diffHunk": "@@ -638,6 +657,12 @@ private void genObject(int aliasNum, boolean allLeftFirst, boolean allLeftNull)\n           // skipping the rest of the rows in the rhs table of the semijoin\n           done = !needsPostEvaluation;\n         }\n+      } else if (type == JoinDesc.ANTI_JOIN) {\n+        if (innerJoin(skip, left, right)) {\n+          // if anti join found a match then the condition is not matched for anti join, so we can skip rest of the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODEzNTk5OQ=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1ODk1NjY3OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNDozMTozNVrOG062kg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwMTo0MDowNFrOG3RJIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE0MzM3OA==", "bodyText": "Not sure I fully understand the comment here -- !forward (false) and antijoin (true) will still skip the object", "url": "https://github.com/apache/hive/pull/1147#discussion_r458143378", "createdAt": "2020-07-21T14:31:35Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "diffHunk": "@@ -523,11 +533,19 @@ private boolean createForwardJoinObject(boolean[] skip) throws HiveException {\n         forward = true;\n       }\n     }\n+    return forward;\n+  }\n+\n+  // returns whether a record was forwarded\n+  private boolean createForwardJoinObject(boolean[] skip, boolean antiJoin) throws HiveException {\n+    boolean forward = fillFwdCache(skip);\n     if (forward) {\n       if (needsPostEvaluation) {\n         forward = !JoinUtil.isFiltered(forwardCache, residualJoinFilters, residualJoinFiltersOIs);\n       }\n-      if (forward) {\n+\n+      // For anti join, check all right side and if nothing is matched then only forward.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE5ODcxOA==", "bodyText": "For anti join we don't emit the record here. It's done after all the records are checked and none of the record matches the condition. Here if forward is false we don't forward and as its a \"&\" we don't forward for anti join == true even if forward is true.", "url": "https://github.com/apache/hive/pull/1147#discussion_r459198718", "createdAt": "2020-07-23T03:39:23Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "diffHunk": "@@ -523,11 +533,19 @@ private boolean createForwardJoinObject(boolean[] skip) throws HiveException {\n         forward = true;\n       }\n     }\n+    return forward;\n+  }\n+\n+  // returns whether a record was forwarded\n+  private boolean createForwardJoinObject(boolean[] skip, boolean antiJoin) throws HiveException {\n+    boolean forward = fillFwdCache(skip);\n     if (forward) {\n       if (needsPostEvaluation) {\n         forward = !JoinUtil.isFiltered(forwardCache, residualJoinFilters, residualJoinFiltersOIs);\n       }\n-      if (forward) {\n+\n+      // For anti join, check all right side and if nothing is matched then only forward.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE0MzM3OA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTMwNzU0Nw==", "bodyText": "Ok makes sense now -- so maybe we should just mention that for anti-join we dont forward at this point", "url": "https://github.com/apache/hive/pull/1147#discussion_r459307547", "createdAt": "2020-07-23T08:57:35Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "diffHunk": "@@ -523,11 +533,19 @@ private boolean createForwardJoinObject(boolean[] skip) throws HiveException {\n         forward = true;\n       }\n     }\n+    return forward;\n+  }\n+\n+  // returns whether a record was forwarded\n+  private boolean createForwardJoinObject(boolean[] skip, boolean antiJoin) throws HiveException {\n+    boolean forward = fillFwdCache(skip);\n     if (forward) {\n       if (needsPostEvaluation) {\n         forward = !JoinUtil.isFiltered(forwardCache, residualJoinFilters, residualJoinFiltersOIs);\n       }\n-      if (forward) {\n+\n+      // For anti join, check all right side and if nothing is matched then only forward.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE0MzM3OA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYwNTczMA==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460605730", "createdAt": "2020-07-27T01:40:04Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java", "diffHunk": "@@ -523,11 +533,19 @@ private boolean createForwardJoinObject(boolean[] skip) throws HiveException {\n         forward = true;\n       }\n     }\n+    return forward;\n+  }\n+\n+  // returns whether a record was forwarded\n+  private boolean createForwardJoinObject(boolean[] skip, boolean antiJoin) throws HiveException {\n+    boolean forward = fillFwdCache(skip);\n     if (forward) {\n       if (needsPostEvaluation) {\n         forward = !JoinUtil.isFiltered(forwardCache, residualJoinFilters, residualJoinFiltersOIs);\n       }\n-      if (forward) {\n+\n+      // For anti join, check all right side and if nothing is matched then only forward.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE0MzM3OA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1ODk4NDQwOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinGenerateResultOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNDozNzozM1rOG07ICQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwMTo0MDozNVrOG3RJjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE0Nzg0OQ==", "bodyText": "nit:  The result is modified during", "url": "https://github.com/apache/hive/pull/1147#discussion_r458147849", "createdAt": "2020-07-21T14:37:33Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinGenerateResultOperator.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashSet;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashSetResult;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashTableResult;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+// TODO : This class is duplicate of semi join. Need to do a refactoring to merge it with semi join.\n+/**\n+ * This class has methods for generating vectorized join results for Anti joins.\n+ * The big difference between inner joins and anti joins is existence testing.\n+ * Inner joins use a hash map to lookup the 1 or more small table values.\n+ * Anti joins are a specialized join for outputting big table rows whose key exists\n+ * in the small table.\n+ *\n+ * No small table values are needed for anti since they would be empty.  So,\n+ * we use a hash set as the hash table.  Hash sets just report whether a key exists.  This\n+ * is a big performance optimization.\n+ */\n+public abstract class VectorMapJoinAntiJoinGenerateResultOperator\n+        extends VectorMapJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(VectorMapJoinAntiJoinGenerateResultOperator.class.getName());\n+\n+  // Anti join specific members.\n+\n+  // An array of hash set results so we can do lookups on the whole batch before output result\n+  // generation.\n+  protected transient VectorMapJoinHashSetResult hashSetResults[];\n+\n+  // Pre-allocated member for storing the (physical) batch index of matching row (single- or\n+  // multi-small-table-valued) indexes during a process call.\n+  protected transient int[] allMatchs;\n+\n+  // Pre-allocated member for storing the (physical) batch index of rows that need to be spilled.\n+  protected transient int[] spills;\n+\n+  // Pre-allocated member for storing index into the hashSetResults for each spilled row.\n+  protected transient int[] spillHashMapResultIndices;\n+\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinGenerateResultOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinGenerateResultOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinGenerateResultOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                                     VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  /*\n+   * Setup our anti join specific members.\n+   */\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Anti join specific.\n+    VectorMapJoinHashSet baseHashSet = (VectorMapJoinHashSet) vectorMapJoinHashTable;\n+\n+    hashSetResults = new VectorMapJoinHashSetResult[VectorizedRowBatch.DEFAULT_SIZE];\n+    for (int i = 0; i < hashSetResults.length; i++) {\n+      hashSetResults[i] = baseHashSet.createHashSetResult();\n+    }\n+\n+    allMatchs = new int[VectorizedRowBatch.DEFAULT_SIZE];\n+\n+    spills = new int[VectorizedRowBatch.DEFAULT_SIZE];\n+    spillHashMapResultIndices = new int[VectorizedRowBatch.DEFAULT_SIZE];\n+  }\n+\n+  //-----------------------------------------------------------------------------------------------\n+\n+  /*\n+   * Anti join (hash set).\n+   */\n+\n+  /**\n+   * Generate the anti join output results for one vectorized row batch. The result is modified in the during hash", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYwNTgzNg==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460605836", "createdAt": "2020-07-27T01:40:35Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinGenerateResultOperator.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashSet;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashSetResult;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashTableResult;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+// TODO : This class is duplicate of semi join. Need to do a refactoring to merge it with semi join.\n+/**\n+ * This class has methods for generating vectorized join results for Anti joins.\n+ * The big difference between inner joins and anti joins is existence testing.\n+ * Inner joins use a hash map to lookup the 1 or more small table values.\n+ * Anti joins are a specialized join for outputting big table rows whose key exists\n+ * in the small table.\n+ *\n+ * No small table values are needed for anti since they would be empty.  So,\n+ * we use a hash set as the hash table.  Hash sets just report whether a key exists.  This\n+ * is a big performance optimization.\n+ */\n+public abstract class VectorMapJoinAntiJoinGenerateResultOperator\n+        extends VectorMapJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(VectorMapJoinAntiJoinGenerateResultOperator.class.getName());\n+\n+  // Anti join specific members.\n+\n+  // An array of hash set results so we can do lookups on the whole batch before output result\n+  // generation.\n+  protected transient VectorMapJoinHashSetResult hashSetResults[];\n+\n+  // Pre-allocated member for storing the (physical) batch index of matching row (single- or\n+  // multi-small-table-valued) indexes during a process call.\n+  protected transient int[] allMatchs;\n+\n+  // Pre-allocated member for storing the (physical) batch index of rows that need to be spilled.\n+  protected transient int[] spills;\n+\n+  // Pre-allocated member for storing index into the hashSetResults for each spilled row.\n+  protected transient int[] spillHashMapResultIndices;\n+\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinGenerateResultOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinGenerateResultOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinGenerateResultOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                                     VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  /*\n+   * Setup our anti join specific members.\n+   */\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Anti join specific.\n+    VectorMapJoinHashSet baseHashSet = (VectorMapJoinHashSet) vectorMapJoinHashTable;\n+\n+    hashSetResults = new VectorMapJoinHashSetResult[VectorizedRowBatch.DEFAULT_SIZE];\n+    for (int i = 0; i < hashSetResults.length; i++) {\n+      hashSetResults[i] = baseHashSet.createHashSetResult();\n+    }\n+\n+    allMatchs = new int[VectorizedRowBatch.DEFAULT_SIZE];\n+\n+    spills = new int[VectorizedRowBatch.DEFAULT_SIZE];\n+    spillHashMapResultIndices = new int[VectorizedRowBatch.DEFAULT_SIZE];\n+  }\n+\n+  //-----------------------------------------------------------------------------------------------\n+\n+  /*\n+   * Anti join (hash set).\n+   */\n+\n+  /**\n+   * Generate the anti join output results for one vectorized row batch. The result is modified in the during hash", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE0Nzg0OQ=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTAwOTMzOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinGenerateResultOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNDo0MjoyN1rOG07XlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwMTo0MTozMFrOG3RKQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE1MTgyOQ==", "bodyText": "nit: whose key DOES NOT exist", "url": "https://github.com/apache/hive/pull/1147#discussion_r458151829", "createdAt": "2020-07-21T14:42:27Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinGenerateResultOperator.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashSet;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashSetResult;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashTableResult;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+// TODO : This class is duplicate of semi join. Need to do a refactoring to merge it with semi join.\n+/**\n+ * This class has methods for generating vectorized join results for Anti joins.\n+ * The big difference between inner joins and anti joins is existence testing.\n+ * Inner joins use a hash map to lookup the 1 or more small table values.\n+ * Anti joins are a specialized join for outputting big table rows whose key exists", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDYwNjAxNg==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460606016", "createdAt": "2020-07-27T01:41:30Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinGenerateResultOperator.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashSet;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashSetResult;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinHashTableResult;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+\n+// TODO : This class is duplicate of semi join. Need to do a refactoring to merge it with semi join.\n+/**\n+ * This class has methods for generating vectorized join results for Anti joins.\n+ * The big difference between inner joins and anti joins is existence testing.\n+ * Inner joins use a hash map to lookup the 1 or more small table values.\n+ * Anti joins are a specialized join for outputting big table rows whose key exists", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE1MTgyOQ=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTA0MjU1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNDo0OToyNFrOG07soA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwNDoyNjozMFrOG171gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE1NzIxNg==", "bodyText": "leftover?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458157216", "createdAt": "2020-07-21T14:49:24Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIwODA2Nw==", "bodyText": "The pre batch processing done only for joins which emits the right table records. For semi join and anti join, it's not required.", "url": "https://github.com/apache/hive/pull/1147#discussion_r459208067", "createdAt": "2020-07-23T04:26:30Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE1NzIxNg=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTA1MjUxOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNDo1MToyMFrOG07y7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjozODo0MlrOG3MEQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE1ODgyOA==", "bodyText": "would it make sense to move the Result inversion to a utility function?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458158828", "createdAt": "2020-07-21T14:51:20Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      // The one join column for this specialized class.\n+      LongColumnVector joinColVector = (LongColumnVector) batch.cols[singleJoinColumn];\n+      long[] vector = joinColVector.vector;\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          // For anti join, if the right side is null then its a match.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          long key = vector[0];\n+          if (useMinMax && (key < min || key > max)) {\n+            // Out of range for whole batch. Its a match for anti join. We can emit the row.\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else {\n+            joinResult = hashSet.contains(key, hashSetResults[0]);\n+            // reverse the join result for anti join.\n+            if (joinResult == JoinUtil.JoinResult.NOMATCH) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUyMjU2Mg==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460522562", "createdAt": "2020-07-26T12:38:42Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      // The one join column for this specialized class.\n+      LongColumnVector joinColVector = (LongColumnVector) batch.cols[singleJoinColumn];\n+      long[] vector = joinColVector.vector;\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          // For anti join, if the right side is null then its a match.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          long key = vector[0];\n+          if (useMinMax && (key < min || key > max)) {\n+            // Out of range for whole batch. Its a match for anti join. We can emit the row.\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else {\n+            joinResult = hashSet.contains(key, hashSetResults[0]);\n+            // reverse the join result for anti join.\n+            if (joinResult == JoinUtil.JoinResult.NOMATCH) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE1ODgyOA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 151}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTA3NjEyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNDo1NjoxMVrOG08Bfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwNDo0NjozNlrOG9MUkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE2MjU1OQ==", "bodyText": "Maybe rename to haveExistingKey? or HaveCurrentKey?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458162559", "createdAt": "2020-07-21T14:56:11Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      // The one join column for this specialized class.\n+      LongColumnVector joinColVector = (LongColumnVector) batch.cols[singleJoinColumn];\n+      long[] vector = joinColVector.vector;\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          // For anti join, if the right side is null then its a match.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          long key = vector[0];\n+          if (useMinMax && (key < min || key > max)) {\n+            // Out of range for whole batch. Its a match for anti join. We can emit the row.\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else {\n+            joinResult = hashSet.contains(key, hashSetResults[0]);\n+            // reverse the join result for anti join.\n+            if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+              joinResult = JoinUtil.JoinResult.MATCH;\n+            } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+              joinResult = JoinUtil.JoinResult.NOMATCH;\n+            }\n+          }\n+        }\n+\n+        // Common repeated join result processing.\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+        // NOT Repeating.\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matches / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+        long saveKey = 0;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgxODE5NA==", "bodyText": "I have kept the code same as Semi join. may be we can take up this refactoring as part of merging the code.", "url": "https://github.com/apache/hive/pull/1147#discussion_r466818194", "createdAt": "2020-08-07T04:46:36Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      // The one join column for this specialized class.\n+      LongColumnVector joinColVector = (LongColumnVector) batch.cols[singleJoinColumn];\n+      long[] vector = joinColVector.vector;\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          // For anti join, if the right side is null then its a match.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          long key = vector[0];\n+          if (useMinMax && (key < min || key > max)) {\n+            // Out of range for whole batch. Its a match for anti join. We can emit the row.\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else {\n+            joinResult = hashSet.contains(key, hashSetResults[0]);\n+            // reverse the join result for anti join.\n+            if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+              joinResult = JoinUtil.JoinResult.MATCH;\n+            } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+              joinResult = JoinUtil.JoinResult.NOMATCH;\n+            }\n+          }\n+        }\n+\n+        // Common repeated join result processing.\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+        // NOT Repeating.\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matches / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+        long saveKey = 0;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE2MjU1OQ=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 183}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTA4ODg2OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNDo1ODo0NFrOG08JcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwODo1ODoxNlrOG2B7kQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE2NDU5Mg==", "bodyText": "It seems that this could be simplified (not sure haveSaveKey variable is needed)", "url": "https://github.com/apache/hive/pull/1147#discussion_r458164592", "createdAt": "2020-07-21T14:58:44Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      // The one join column for this specialized class.\n+      LongColumnVector joinColVector = (LongColumnVector) batch.cols[singleJoinColumn];\n+      long[] vector = joinColVector.vector;\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          // For anti join, if the right side is null then its a match.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          long key = vector[0];\n+          if (useMinMax && (key < min || key > max)) {\n+            // Out of range for whole batch. Its a match for anti join. We can emit the row.\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else {\n+            joinResult = hashSet.contains(key, hashSetResults[0]);\n+            // reverse the join result for anti join.\n+            if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+              joinResult = JoinUtil.JoinResult.MATCH;\n+            } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+              joinResult = JoinUtil.JoinResult.NOMATCH;\n+            }\n+          }\n+        }\n+\n+        // Common repeated join result processing.\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+        // NOT Repeating.\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matches / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+        long saveKey = 0;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;\n+        JoinUtil.JoinResult saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+\n+        // Logical loop over the rows in the batch since the batch may have selected in use.\n+        for (int logical = 0; logical < inputLogicalSize; logical++) {\n+          int batchIndex = (selectedInUse ? selected[logical] : logical);\n+\n+          // Single-Column Long get key.\n+          long currentKey;\n+          boolean isNull;\n+          if (!joinColVector.noNulls && joinColVector.isNull[batchIndex]) {\n+            currentKey = 0;\n+            isNull = true;\n+          } else {\n+            currentKey = vector[batchIndex];\n+            isNull = false;\n+          }\n+\n+          // Equal key series checking.\n+          if (isNull || !haveSaveKey || currentKey != saveKey) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxMjM2OQ==", "bodyText": "I have kept the code same as Semi join. may be we can take up this refactoring as part of merging the code.", "url": "https://github.com/apache/hive/pull/1147#discussion_r459212369", "createdAt": "2020-07-23T04:47:36Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      // The one join column for this specialized class.\n+      LongColumnVector joinColVector = (LongColumnVector) batch.cols[singleJoinColumn];\n+      long[] vector = joinColVector.vector;\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          // For anti join, if the right side is null then its a match.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          long key = vector[0];\n+          if (useMinMax && (key < min || key > max)) {\n+            // Out of range for whole batch. Its a match for anti join. We can emit the row.\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else {\n+            joinResult = hashSet.contains(key, hashSetResults[0]);\n+            // reverse the join result for anti join.\n+            if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+              joinResult = JoinUtil.JoinResult.MATCH;\n+            } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+              joinResult = JoinUtil.JoinResult.NOMATCH;\n+            }\n+          }\n+        }\n+\n+        // Common repeated join result processing.\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+        // NOT Repeating.\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matches / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+        long saveKey = 0;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;\n+        JoinUtil.JoinResult saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+\n+        // Logical loop over the rows in the batch since the batch may have selected in use.\n+        for (int logical = 0; logical < inputLogicalSize; logical++) {\n+          int batchIndex = (selectedInUse ? selected[logical] : logical);\n+\n+          // Single-Column Long get key.\n+          long currentKey;\n+          boolean isNull;\n+          if (!joinColVector.noNulls && joinColVector.isNull[batchIndex]) {\n+            currentKey = 0;\n+            isNull = true;\n+          } else {\n+            currentKey = vector[batchIndex];\n+            isNull = false;\n+          }\n+\n+          // Equal key series checking.\n+          if (isNull || !haveSaveKey || currentKey != saveKey) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE2NDU5Mg=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTMwNzkyMQ==", "bodyText": "ack", "url": "https://github.com/apache/hive/pull/1147#discussion_r459307921", "createdAt": "2020-07-23T08:58:16Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      // The one join column for this specialized class.\n+      LongColumnVector joinColVector = (LongColumnVector) batch.cols[singleJoinColumn];\n+      long[] vector = joinColVector.vector;\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          // For anti join, if the right side is null then its a match.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          long key = vector[0];\n+          if (useMinMax && (key < min || key > max)) {\n+            // Out of range for whole batch. Its a match for anti join. We can emit the row.\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else {\n+            joinResult = hashSet.contains(key, hashSetResults[0]);\n+            // reverse the join result for anti join.\n+            if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+              joinResult = JoinUtil.JoinResult.MATCH;\n+            } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+              joinResult = JoinUtil.JoinResult.NOMATCH;\n+            }\n+          }\n+        }\n+\n+        // Common repeated join result processing.\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+        // NOT Repeating.\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matches / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+        long saveKey = 0;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;\n+        JoinUtil.JoinResult saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+\n+        // Logical loop over the rows in the batch since the batch may have selected in use.\n+        for (int logical = 0; logical < inputLogicalSize; logical++) {\n+          int batchIndex = (selectedInUse ? selected[logical] : logical);\n+\n+          // Single-Column Long get key.\n+          long currentKey;\n+          boolean isNull;\n+          if (!joinColVector.noNulls && joinColVector.isNull[batchIndex]) {\n+            currentKey = 0;\n+            isNull = true;\n+          } else {\n+            currentKey = vector[batchIndex];\n+            isNull = false;\n+          }\n+\n+          // Equal key series checking.\n+          if (isNull || !haveSaveKey || currentKey != saveKey) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE2NDU5Mg=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 202}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTA5NjUyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNTowMDoxOFrOG08OVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwNDo0NzoxM1rOG9MVNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE2NTg0NA==", "bodyText": "seems that only SPILL case is useful here?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458165844", "createdAt": "2020-07-21T15:00:18Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      // The one join column for this specialized class.\n+      LongColumnVector joinColVector = (LongColumnVector) batch.cols[singleJoinColumn];\n+      long[] vector = joinColVector.vector;\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          // For anti join, if the right side is null then its a match.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          long key = vector[0];\n+          if (useMinMax && (key < min || key > max)) {\n+            // Out of range for whole batch. Its a match for anti join. We can emit the row.\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else {\n+            joinResult = hashSet.contains(key, hashSetResults[0]);\n+            // reverse the join result for anti join.\n+            if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+              joinResult = JoinUtil.JoinResult.MATCH;\n+            } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+              joinResult = JoinUtil.JoinResult.NOMATCH;\n+            }\n+          }\n+        }\n+\n+        // Common repeated join result processing.\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+        // NOT Repeating.\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matches / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+        long saveKey = 0;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;\n+        JoinUtil.JoinResult saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+\n+        // Logical loop over the rows in the batch since the batch may have selected in use.\n+        for (int logical = 0; logical < inputLogicalSize; logical++) {\n+          int batchIndex = (selectedInUse ? selected[logical] : logical);\n+\n+          // Single-Column Long get key.\n+          long currentKey;\n+          boolean isNull;\n+          if (!joinColVector.noNulls && joinColVector.isNull[batchIndex]) {\n+            currentKey = 0;\n+            isNull = true;\n+          } else {\n+            currentKey = vector[batchIndex];\n+            isNull = false;\n+          }\n+\n+          // Equal key series checking.\n+          if (isNull || !haveSaveKey || currentKey != saveKey) {\n+            // New key.\n+            if (haveSaveKey) {\n+              // Move on with our counts.\n+              switch (saveJoinResult) {\n+              case MATCH:\n+                // We have extracted the existence from the hash set result, so we don't keep it.\n+                break;\n+              case SPILL:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 210}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgxODM1OA==", "bodyText": "I have kept the code same as Semi join. may be we can take up this refactoring as part of merging the code.", "url": "https://github.com/apache/hive/pull/1147#discussion_r466818358", "createdAt": "2020-08-07T04:47:13Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      // The one join column for this specialized class.\n+      LongColumnVector joinColVector = (LongColumnVector) batch.cols[singleJoinColumn];\n+      long[] vector = joinColVector.vector;\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          // For anti join, if the right side is null then its a match.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          long key = vector[0];\n+          if (useMinMax && (key < min || key > max)) {\n+            // Out of range for whole batch. Its a match for anti join. We can emit the row.\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else {\n+            joinResult = hashSet.contains(key, hashSetResults[0]);\n+            // reverse the join result for anti join.\n+            if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+              joinResult = JoinUtil.JoinResult.MATCH;\n+            } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+              joinResult = JoinUtil.JoinResult.NOMATCH;\n+            }\n+          }\n+        }\n+\n+        // Common repeated join result processing.\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+        // NOT Repeating.\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matches / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+        long saveKey = 0;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;\n+        JoinUtil.JoinResult saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+\n+        // Logical loop over the rows in the batch since the batch may have selected in use.\n+        for (int logical = 0; logical < inputLogicalSize; logical++) {\n+          int batchIndex = (selectedInUse ? selected[logical] : logical);\n+\n+          // Single-Column Long get key.\n+          long currentKey;\n+          boolean isNull;\n+          if (!joinColVector.noNulls && joinColVector.isNull[batchIndex]) {\n+            currentKey = 0;\n+            isNull = true;\n+          } else {\n+            currentKey = vector[batchIndex];\n+            isNull = false;\n+          }\n+\n+          // Equal key series checking.\n+          if (isNull || !haveSaveKey || currentKey != saveKey) {\n+            // New key.\n+            if (haveSaveKey) {\n+              // Move on with our counts.\n+              switch (saveJoinResult) {\n+              case MATCH:\n+                // We have extracted the existence from the hash set result, so we don't keep it.\n+                break;\n+              case SPILL:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE2NTg0NA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 210}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTEwNDEwOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNTowMTo1NVrOG08TQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjozNzozMVrOG3MD1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE2NzEwNA==", "bodyText": "Inversion is actual done below", "url": "https://github.com/apache/hive/pull/1147#discussion_r458167104", "createdAt": "2020-07-21T15:01:55Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      // The one join column for this specialized class.\n+      LongColumnVector joinColVector = (LongColumnVector) batch.cols[singleJoinColumn];\n+      long[] vector = joinColVector.vector;\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          // For anti join, if the right side is null then its a match.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          long key = vector[0];\n+          if (useMinMax && (key < min || key > max)) {\n+            // Out of range for whole batch. Its a match for anti join. We can emit the row.\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else {\n+            joinResult = hashSet.contains(key, hashSetResults[0]);\n+            // reverse the join result for anti join.\n+            if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+              joinResult = JoinUtil.JoinResult.MATCH;\n+            } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+              joinResult = JoinUtil.JoinResult.NOMATCH;\n+            }\n+          }\n+        }\n+\n+        // Common repeated join result processing.\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+        // NOT Repeating.\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matches / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+        long saveKey = 0;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;\n+        JoinUtil.JoinResult saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+\n+        // Logical loop over the rows in the batch since the batch may have selected in use.\n+        for (int logical = 0; logical < inputLogicalSize; logical++) {\n+          int batchIndex = (selectedInUse ? selected[logical] : logical);\n+\n+          // Single-Column Long get key.\n+          long currentKey;\n+          boolean isNull;\n+          if (!joinColVector.noNulls && joinColVector.isNull[batchIndex]) {\n+            currentKey = 0;\n+            isNull = true;\n+          } else {\n+            currentKey = vector[batchIndex];\n+            isNull = false;\n+          }\n+\n+          // Equal key series checking.\n+          if (isNull || !haveSaveKey || currentKey != saveKey) {\n+            // New key.\n+            if (haveSaveKey) {\n+              // Move on with our counts.\n+              switch (saveJoinResult) {\n+              case MATCH:\n+                // We have extracted the existence from the hash set result, so we don't keep it.\n+                break;\n+              case SPILL:\n+                // We keep the hash set result for its spill information.\n+                hashSetResultCount++;\n+                break;\n+              case NOMATCH:\n+                break;\n+              }\n+            }\n+\n+            if (isNull) {\n+              saveJoinResult = JoinUtil.JoinResult.MATCH;\n+              haveSaveKey = false;\n+            } else {\n+              // Regardless of our matching result, we keep that information to make multiple use\n+              // of it for a possible series of equal keys.\n+              haveSaveKey = true;\n+              saveKey = currentKey;\n+              if (useMinMax && (currentKey < min || currentKey > max)) {\n+                // Key out of range for whole hash table, is a valid match for anti join.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 228}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUyMjQ1NA==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460522454", "createdAt": "2020-07-26T12:37:31Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      // The one join column for this specialized class.\n+      LongColumnVector joinColVector = (LongColumnVector) batch.cols[singleJoinColumn];\n+      long[] vector = joinColVector.vector;\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          // For anti join, if the right side is null then its a match.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          long key = vector[0];\n+          if (useMinMax && (key < min || key > max)) {\n+            // Out of range for whole batch. Its a match for anti join. We can emit the row.\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else {\n+            joinResult = hashSet.contains(key, hashSetResults[0]);\n+            // reverse the join result for anti join.\n+            if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+              joinResult = JoinUtil.JoinResult.MATCH;\n+            } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+              joinResult = JoinUtil.JoinResult.NOMATCH;\n+            }\n+          }\n+        }\n+\n+        // Common repeated join result processing.\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+        // NOT Repeating.\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matches / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+        long saveKey = 0;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;\n+        JoinUtil.JoinResult saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+\n+        // Logical loop over the rows in the batch since the batch may have selected in use.\n+        for (int logical = 0; logical < inputLogicalSize; logical++) {\n+          int batchIndex = (selectedInUse ? selected[logical] : logical);\n+\n+          // Single-Column Long get key.\n+          long currentKey;\n+          boolean isNull;\n+          if (!joinColVector.noNulls && joinColVector.isNull[batchIndex]) {\n+            currentKey = 0;\n+            isNull = true;\n+          } else {\n+            currentKey = vector[batchIndex];\n+            isNull = false;\n+          }\n+\n+          // Equal key series checking.\n+          if (isNull || !haveSaveKey || currentKey != saveKey) {\n+            // New key.\n+            if (haveSaveKey) {\n+              // Move on with our counts.\n+              switch (saveJoinResult) {\n+              case MATCH:\n+                // We have extracted the existence from the hash set result, so we don't keep it.\n+                break;\n+              case SPILL:\n+                // We keep the hash set result for its spill information.\n+                hashSetResultCount++;\n+                break;\n+              case NOMATCH:\n+                break;\n+              }\n+            }\n+\n+            if (isNull) {\n+              saveJoinResult = JoinUtil.JoinResult.MATCH;\n+              haveSaveKey = false;\n+            } else {\n+              // Regardless of our matching result, we keep that information to make multiple use\n+              // of it for a possible series of equal keys.\n+              haveSaveKey = true;\n+              saveKey = currentKey;\n+              if (useMinMax && (currentKey < min || currentKey > max)) {\n+                // Key out of range for whole hash table, is a valid match for anti join.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE2NzEwNA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 228}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTEwNjUyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNTowMjoyN1rOG08U2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjozNjoyNlrOG3MDWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE2NzUxNQ==", "bodyText": "Again this could be replaced with an Inversion utility function", "url": "https://github.com/apache/hive/pull/1147#discussion_r458167515", "createdAt": "2020-07-21T15:02:27Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      // The one join column for this specialized class.\n+      LongColumnVector joinColVector = (LongColumnVector) batch.cols[singleJoinColumn];\n+      long[] vector = joinColVector.vector;\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          // For anti join, if the right side is null then its a match.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          long key = vector[0];\n+          if (useMinMax && (key < min || key > max)) {\n+            // Out of range for whole batch. Its a match for anti join. We can emit the row.\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else {\n+            joinResult = hashSet.contains(key, hashSetResults[0]);\n+            // reverse the join result for anti join.\n+            if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+              joinResult = JoinUtil.JoinResult.MATCH;\n+            } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+              joinResult = JoinUtil.JoinResult.NOMATCH;\n+            }\n+          }\n+        }\n+\n+        // Common repeated join result processing.\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+        // NOT Repeating.\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matches / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+        long saveKey = 0;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;\n+        JoinUtil.JoinResult saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+\n+        // Logical loop over the rows in the batch since the batch may have selected in use.\n+        for (int logical = 0; logical < inputLogicalSize; logical++) {\n+          int batchIndex = (selectedInUse ? selected[logical] : logical);\n+\n+          // Single-Column Long get key.\n+          long currentKey;\n+          boolean isNull;\n+          if (!joinColVector.noNulls && joinColVector.isNull[batchIndex]) {\n+            currentKey = 0;\n+            isNull = true;\n+          } else {\n+            currentKey = vector[batchIndex];\n+            isNull = false;\n+          }\n+\n+          // Equal key series checking.\n+          if (isNull || !haveSaveKey || currentKey != saveKey) {\n+            // New key.\n+            if (haveSaveKey) {\n+              // Move on with our counts.\n+              switch (saveJoinResult) {\n+              case MATCH:\n+                // We have extracted the existence from the hash set result, so we don't keep it.\n+                break;\n+              case SPILL:\n+                // We keep the hash set result for its spill information.\n+                hashSetResultCount++;\n+                break;\n+              case NOMATCH:\n+                break;\n+              }\n+            }\n+\n+            if (isNull) {\n+              saveJoinResult = JoinUtil.JoinResult.MATCH;\n+              haveSaveKey = false;\n+            } else {\n+              // Regardless of our matching result, we keep that information to make multiple use\n+              // of it for a possible series of equal keys.\n+              haveSaveKey = true;\n+              saveKey = currentKey;\n+              if (useMinMax && (currentKey < min || currentKey > max)) {\n+                // Key out of range for whole hash table, is a valid match for anti join.\n+                saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+              } else {\n+                saveJoinResult = hashSet.contains(currentKey, hashSetResults[hashSetResultCount]);\n+              }\n+\n+              // Reverse the match result for anti join.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 234}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUyMjMyOA==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460522328", "createdAt": "2020-07-26T12:36:26Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      // The one join column for this specialized class.\n+      LongColumnVector joinColVector = (LongColumnVector) batch.cols[singleJoinColumn];\n+      long[] vector = joinColVector.vector;\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          // For anti join, if the right side is null then its a match.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          long key = vector[0];\n+          if (useMinMax && (key < min || key > max)) {\n+            // Out of range for whole batch. Its a match for anti join. We can emit the row.\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else {\n+            joinResult = hashSet.contains(key, hashSetResults[0]);\n+            // reverse the join result for anti join.\n+            if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+              joinResult = JoinUtil.JoinResult.MATCH;\n+            } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+              joinResult = JoinUtil.JoinResult.NOMATCH;\n+            }\n+          }\n+        }\n+\n+        // Common repeated join result processing.\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+        // NOT Repeating.\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matches / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+        long saveKey = 0;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;\n+        JoinUtil.JoinResult saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+\n+        // Logical loop over the rows in the batch since the batch may have selected in use.\n+        for (int logical = 0; logical < inputLogicalSize; logical++) {\n+          int batchIndex = (selectedInUse ? selected[logical] : logical);\n+\n+          // Single-Column Long get key.\n+          long currentKey;\n+          boolean isNull;\n+          if (!joinColVector.noNulls && joinColVector.isNull[batchIndex]) {\n+            currentKey = 0;\n+            isNull = true;\n+          } else {\n+            currentKey = vector[batchIndex];\n+            isNull = false;\n+          }\n+\n+          // Equal key series checking.\n+          if (isNull || !haveSaveKey || currentKey != saveKey) {\n+            // New key.\n+            if (haveSaveKey) {\n+              // Move on with our counts.\n+              switch (saveJoinResult) {\n+              case MATCH:\n+                // We have extracted the existence from the hash set result, so we don't keep it.\n+                break;\n+              case SPILL:\n+                // We keep the hash set result for its spill information.\n+                hashSetResultCount++;\n+                break;\n+              case NOMATCH:\n+                break;\n+              }\n+            }\n+\n+            if (isNull) {\n+              saveJoinResult = JoinUtil.JoinResult.MATCH;\n+              haveSaveKey = false;\n+            } else {\n+              // Regardless of our matching result, we keep that information to make multiple use\n+              // of it for a possible series of equal keys.\n+              haveSaveKey = true;\n+              saveKey = currentKey;\n+              if (useMinMax && (currentKey < min || currentKey > max)) {\n+                // Key out of range for whole hash table, is a valid match for anti join.\n+                saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+              } else {\n+                saveJoinResult = hashSet.contains(currentKey, hashSetResults[hashSetResultCount]);\n+              }\n+\n+              // Reverse the match result for anti join.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE2NzUxNQ=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 234}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTExODQ1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNTowNToxMFrOG08ccQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjozNjoxOFrOG3MDSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE2OTQ1Nw==", "bodyText": "Repeating Common anti join result processing code -- move to function?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458169457", "createdAt": "2020-07-21T15:05:10Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      // The one join column for this specialized class.\n+      LongColumnVector joinColVector = (LongColumnVector) batch.cols[singleJoinColumn];\n+      long[] vector = joinColVector.vector;\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          // For anti join, if the right side is null then its a match.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          long key = vector[0];\n+          if (useMinMax && (key < min || key > max)) {\n+            // Out of range for whole batch. Its a match for anti join. We can emit the row.\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else {\n+            joinResult = hashSet.contains(key, hashSetResults[0]);\n+            // reverse the join result for anti join.\n+            if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+              joinResult = JoinUtil.JoinResult.MATCH;\n+            } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+              joinResult = JoinUtil.JoinResult.NOMATCH;\n+            }\n+          }\n+        }\n+\n+        // Common repeated join result processing.\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+        // NOT Repeating.\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matches / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+        long saveKey = 0;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;\n+        JoinUtil.JoinResult saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+\n+        // Logical loop over the rows in the batch since the batch may have selected in use.\n+        for (int logical = 0; logical < inputLogicalSize; logical++) {\n+          int batchIndex = (selectedInUse ? selected[logical] : logical);\n+\n+          // Single-Column Long get key.\n+          long currentKey;\n+          boolean isNull;\n+          if (!joinColVector.noNulls && joinColVector.isNull[batchIndex]) {\n+            currentKey = 0;\n+            isNull = true;\n+          } else {\n+            currentKey = vector[batchIndex];\n+            isNull = false;\n+          }\n+\n+          // Equal key series checking.\n+          if (isNull || !haveSaveKey || currentKey != saveKey) {\n+            // New key.\n+            if (haveSaveKey) {\n+              // Move on with our counts.\n+              switch (saveJoinResult) {\n+              case MATCH:\n+                // We have extracted the existence from the hash set result, so we don't keep it.\n+                break;\n+              case SPILL:\n+                // We keep the hash set result for its spill information.\n+                hashSetResultCount++;\n+                break;\n+              case NOMATCH:\n+                break;\n+              }\n+            }\n+\n+            if (isNull) {\n+              saveJoinResult = JoinUtil.JoinResult.MATCH;\n+              haveSaveKey = false;\n+            } else {\n+              // Regardless of our matching result, we keep that information to make multiple use\n+              // of it for a possible series of equal keys.\n+              haveSaveKey = true;\n+              saveKey = currentKey;\n+              if (useMinMax && (currentKey < min || currentKey > max)) {\n+                // Key out of range for whole hash table, is a valid match for anti join.\n+                saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+              } else {\n+                saveJoinResult = hashSet.contains(currentKey, hashSetResults[hashSetResultCount]);\n+              }\n+\n+              // Reverse the match result for anti join.\n+              if (saveJoinResult == JoinUtil.JoinResult.NOMATCH) {\n+                saveJoinResult = JoinUtil.JoinResult.MATCH;\n+              } else if (saveJoinResult == JoinUtil.JoinResult.MATCH) {\n+                saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+              }\n+            }\n+\n+            // Common anti join result processing.\n+            switch (saveJoinResult) {\n+            case MATCH:\n+              allMatchs[allMatchCount++] = batchIndex;\n+              // VectorizedBatchUtil.debugDisplayOneRow(batch, batchIndex, CLASS_NAME + \" MATCH isSingleValue \" + equalKeySeriesIsSingleValue[equalKeySeriesCount] + \" currentKey \" + currentKey);\n+              break;\n+\n+            case SPILL:\n+              spills[spillCount] = batchIndex;\n+              spillHashMapResultIndices[spillCount] = hashSetResultCount;\n+              spillCount++;\n+              break;\n+\n+            case NOMATCH:\n+              // VectorizedBatchUtil.debugDisplayOneRow(batch, batchIndex, CLASS_NAME + \" NOMATCH\" + \" currentKey \" + currentKey);\n+              break;\n+            }\n+          } else {\n+            // Series of equal keys.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 260}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUyMjMxMg==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460522312", "createdAt": "2020-07-26T12:36:18Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinLongOperator.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinLongHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+// Single-Column Long hash table import.\n+// Single-Column Long specific imports.\n+\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column Long\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinLongOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinLongOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinLongHashSet hashSet;\n+\n+  // Single-Column Long specific members.\n+  // For integers, we have optional min/max filtering.\n+  private transient boolean useMinMax;\n+  private transient long min;\n+  private transient long max;\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  // Pass-thru constructors.\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinLongOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinLongOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                           VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  // Process Single-Column Long Anti Join on a vectorized row batch.\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    // Initialize Single-Column Long members for this specialized class.\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    // Get our Single-Column Long hash set information for this specialized class.\n+    hashSet = (VectorMapJoinLongHashSet) vectorMapJoinHashTable;\n+    useMinMax = hashSet.useMinMax();\n+    if (useMinMax) {\n+      min = hashSet.min();\n+      max = hashSet.max();\n+    }\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      // The one join column for this specialized class.\n+      LongColumnVector joinColVector = (LongColumnVector) batch.cols[singleJoinColumn];\n+      long[] vector = joinColVector.vector;\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          // For anti join, if the right side is null then its a match.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          long key = vector[0];\n+          if (useMinMax && (key < min || key > max)) {\n+            // Out of range for whole batch. Its a match for anti join. We can emit the row.\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else {\n+            joinResult = hashSet.contains(key, hashSetResults[0]);\n+            // reverse the join result for anti join.\n+            if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+              joinResult = JoinUtil.JoinResult.MATCH;\n+            } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+              joinResult = JoinUtil.JoinResult.NOMATCH;\n+            }\n+          }\n+        }\n+\n+        // Common repeated join result processing.\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+        // NOT Repeating.\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matches / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+        long saveKey = 0;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;\n+        JoinUtil.JoinResult saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+\n+        // Logical loop over the rows in the batch since the batch may have selected in use.\n+        for (int logical = 0; logical < inputLogicalSize; logical++) {\n+          int batchIndex = (selectedInUse ? selected[logical] : logical);\n+\n+          // Single-Column Long get key.\n+          long currentKey;\n+          boolean isNull;\n+          if (!joinColVector.noNulls && joinColVector.isNull[batchIndex]) {\n+            currentKey = 0;\n+            isNull = true;\n+          } else {\n+            currentKey = vector[batchIndex];\n+            isNull = false;\n+          }\n+\n+          // Equal key series checking.\n+          if (isNull || !haveSaveKey || currentKey != saveKey) {\n+            // New key.\n+            if (haveSaveKey) {\n+              // Move on with our counts.\n+              switch (saveJoinResult) {\n+              case MATCH:\n+                // We have extracted the existence from the hash set result, so we don't keep it.\n+                break;\n+              case SPILL:\n+                // We keep the hash set result for its spill information.\n+                hashSetResultCount++;\n+                break;\n+              case NOMATCH:\n+                break;\n+              }\n+            }\n+\n+            if (isNull) {\n+              saveJoinResult = JoinUtil.JoinResult.MATCH;\n+              haveSaveKey = false;\n+            } else {\n+              // Regardless of our matching result, we keep that information to make multiple use\n+              // of it for a possible series of equal keys.\n+              haveSaveKey = true;\n+              saveKey = currentKey;\n+              if (useMinMax && (currentKey < min || currentKey > max)) {\n+                // Key out of range for whole hash table, is a valid match for anti join.\n+                saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+              } else {\n+                saveJoinResult = hashSet.contains(currentKey, hashSetResults[hashSetResultCount]);\n+              }\n+\n+              // Reverse the match result for anti join.\n+              if (saveJoinResult == JoinUtil.JoinResult.NOMATCH) {\n+                saveJoinResult = JoinUtil.JoinResult.MATCH;\n+              } else if (saveJoinResult == JoinUtil.JoinResult.MATCH) {\n+                saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+              }\n+            }\n+\n+            // Common anti join result processing.\n+            switch (saveJoinResult) {\n+            case MATCH:\n+              allMatchs[allMatchCount++] = batchIndex;\n+              // VectorizedBatchUtil.debugDisplayOneRow(batch, batchIndex, CLASS_NAME + \" MATCH isSingleValue \" + equalKeySeriesIsSingleValue[equalKeySeriesCount] + \" currentKey \" + currentKey);\n+              break;\n+\n+            case SPILL:\n+              spills[spillCount] = batchIndex;\n+              spillHashMapResultIndices[spillCount] = hashSetResultCount;\n+              spillCount++;\n+              break;\n+\n+            case NOMATCH:\n+              // VectorizedBatchUtil.debugDisplayOneRow(batch, batchIndex, CLASS_NAME + \" NOMATCH\" + \" currentKey \" + currentKey);\n+              break;\n+            }\n+          } else {\n+            // Series of equal keys.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE2OTQ1Nw=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 260}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTEyMDE1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinMultiKeyOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNTowNTozMFrOG08dew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwNDo0Nzo0NVrOG9MVvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE2OTcyMw==", "bodyText": "leftover?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458169723", "createdAt": "2020-07-21T15:05:30Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinMultiKeyOperator.java", "diffHunk": "@@ -0,0 +1,400 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorSerializeRow;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinBytesHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.apache.hadoop.hive.serde2.ByteStream.Output;\n+import org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableSerializeWrite;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// Multi-Key hash table import.\n+// Multi-Key specific imports.\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on Multi-Key\n+ * using hash set.\n+ */\n+public class VectorMapJoinAntiJoinMultiKeyOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  //------------------------------------------------------------------------------------------------\n+\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinMultiKeyOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  //------------------------------------------------------------------------------------------------\n+\n+  // (none)\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+  //---------------------------------------------------------------------------\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinBytesHashSet hashSet;\n+\n+  //---------------------------------------------------------------------------\n+  // Multi-Key specific members.\n+  //\n+\n+  // Object that can take a set of columns in row in a vectorized row batch and serialized it.\n+  // Known to not have any nulls.\n+  private transient VectorSerializeRow keyVectorSerializeWrite;\n+\n+  // The BinarySortable serialization of the current key.\n+  private transient Output currentKeyOutput;\n+\n+  // The BinarySortable serialization of the saved key for a possible series of equal keys.\n+  private transient Output saveKeyOutput;\n+\n+  //---------------------------------------------------------------------------\n+  // Pass-thru constructors.\n+  //\n+\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinMultiKeyOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinMultiKeyOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinMultiKeyOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                               VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  //---------------------------------------------------------------------------\n+  // Process Multi-Key Anti Join on a vectorized row batch.\n+  //\n+\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    /*\n+     * Initialize Multi-Key members for this specialized class.\n+     */\n+\n+    keyVectorSerializeWrite = new VectorSerializeRow(BinarySortableSerializeWrite.with(\n+            this.getConf().getKeyTblDesc().getProperties(), bigTableKeyColumnMap.length));\n+    keyVectorSerializeWrite.init(bigTableKeyTypeInfos, bigTableKeyColumnMap);\n+\n+    currentKeyOutput = new Output();\n+    saveKeyOutput = new Output();\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    /*\n+     * Get our Multi-Key hash set information for this specialized class.\n+     */\n+\n+    hashSet = (VectorMapJoinBytesHashSet) vectorMapJoinHashTable;\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+\n+      // Do the per-batch setup for an anti join.\n+\n+      // (Currently none)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 140}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgxODQ5Mg==", "bodyText": "I have kept the code same as Semi join. may be we can take up this refactoring as part of merging the code.", "url": "https://github.com/apache/hive/pull/1147#discussion_r466818492", "createdAt": "2020-08-07T04:47:45Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinMultiKeyOperator.java", "diffHunk": "@@ -0,0 +1,400 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorSerializeRow;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinBytesHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.apache.hadoop.hive.serde2.ByteStream.Output;\n+import org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableSerializeWrite;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// Multi-Key hash table import.\n+// Multi-Key specific imports.\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on Multi-Key\n+ * using hash set.\n+ */\n+public class VectorMapJoinAntiJoinMultiKeyOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  //------------------------------------------------------------------------------------------------\n+\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinMultiKeyOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  //------------------------------------------------------------------------------------------------\n+\n+  // (none)\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+  //---------------------------------------------------------------------------\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinBytesHashSet hashSet;\n+\n+  //---------------------------------------------------------------------------\n+  // Multi-Key specific members.\n+  //\n+\n+  // Object that can take a set of columns in row in a vectorized row batch and serialized it.\n+  // Known to not have any nulls.\n+  private transient VectorSerializeRow keyVectorSerializeWrite;\n+\n+  // The BinarySortable serialization of the current key.\n+  private transient Output currentKeyOutput;\n+\n+  // The BinarySortable serialization of the saved key for a possible series of equal keys.\n+  private transient Output saveKeyOutput;\n+\n+  //---------------------------------------------------------------------------\n+  // Pass-thru constructors.\n+  //\n+\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinMultiKeyOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinMultiKeyOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinMultiKeyOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                               VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  //---------------------------------------------------------------------------\n+  // Process Multi-Key Anti Join on a vectorized row batch.\n+  //\n+\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    /*\n+     * Initialize Multi-Key members for this specialized class.\n+     */\n+\n+    keyVectorSerializeWrite = new VectorSerializeRow(BinarySortableSerializeWrite.with(\n+            this.getConf().getKeyTblDesc().getProperties(), bigTableKeyColumnMap.length));\n+    keyVectorSerializeWrite.init(bigTableKeyTypeInfos, bigTableKeyColumnMap);\n+\n+    currentKeyOutput = new Output();\n+    saveKeyOutput = new Output();\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    /*\n+     * Get our Multi-Key hash set information for this specialized class.\n+     */\n+\n+    hashSet = (VectorMapJoinBytesHashSet) vectorMapJoinHashTable;\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+\n+      // Do the per-batch setup for an anti join.\n+\n+      // (Currently none)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE2OTcyMw=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 140}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTE3NTczOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinMultiKeyOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNToxNzo0M1rOG09A4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwNDo0ODowMFrOG9MV8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE3ODc4Ng==", "bodyText": "simplification?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458178786", "createdAt": "2020-07-21T15:17:43Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinMultiKeyOperator.java", "diffHunk": "@@ -0,0 +1,400 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorSerializeRow;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinBytesHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.apache.hadoop.hive.serde2.ByteStream.Output;\n+import org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableSerializeWrite;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// Multi-Key hash table import.\n+// Multi-Key specific imports.\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on Multi-Key\n+ * using hash set.\n+ */\n+public class VectorMapJoinAntiJoinMultiKeyOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  //------------------------------------------------------------------------------------------------\n+\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinMultiKeyOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  //------------------------------------------------------------------------------------------------\n+\n+  // (none)\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+  //---------------------------------------------------------------------------\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinBytesHashSet hashSet;\n+\n+  //---------------------------------------------------------------------------\n+  // Multi-Key specific members.\n+  //\n+\n+  // Object that can take a set of columns in row in a vectorized row batch and serialized it.\n+  // Known to not have any nulls.\n+  private transient VectorSerializeRow keyVectorSerializeWrite;\n+\n+  // The BinarySortable serialization of the current key.\n+  private transient Output currentKeyOutput;\n+\n+  // The BinarySortable serialization of the saved key for a possible series of equal keys.\n+  private transient Output saveKeyOutput;\n+\n+  //---------------------------------------------------------------------------\n+  // Pass-thru constructors.\n+  //\n+\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinMultiKeyOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinMultiKeyOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinMultiKeyOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                               VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  //---------------------------------------------------------------------------\n+  // Process Multi-Key Anti Join on a vectorized row batch.\n+  //\n+\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    /*\n+     * Initialize Multi-Key members for this specialized class.\n+     */\n+\n+    keyVectorSerializeWrite = new VectorSerializeRow(BinarySortableSerializeWrite.with(\n+            this.getConf().getKeyTblDesc().getProperties(), bigTableKeyColumnMap.length));\n+    keyVectorSerializeWrite.init(bigTableKeyTypeInfos, bigTableKeyColumnMap);\n+\n+    currentKeyOutput = new Output();\n+    saveKeyOutput = new Output();\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    /*\n+     * Get our Multi-Key hash set information for this specialized class.\n+     */\n+\n+    hashSet = (VectorMapJoinBytesHashSet) vectorMapJoinHashTable;\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+\n+      // Do the per-batch setup for an anti join.\n+\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      /*\n+       * Multi-Key specific declarations.\n+       */\n+\n+      // None.\n+\n+      /*\n+       * Multi-Key Long check for repeating.\n+       */\n+\n+      // If all BigTable input columns to key expressions are isRepeating, then\n+      // calculate key once; lookup once.\n+      boolean allKeyInputColumnsRepeating;\n+      if (bigTableKeyColumnMap.length == 0) {\n+       allKeyInputColumnsRepeating = false;\n+      } else {\n+        allKeyInputColumnsRepeating = true;\n+        for (int i = 0; i < bigTableKeyColumnMap.length; i++) {\n+          if (!batch.cols[bigTableKeyColumnMap[i]].isRepeating) {\n+            allKeyInputColumnsRepeating =  false;\n+            break;\n+          }\n+        }\n+      }\n+\n+      if (allKeyInputColumnsRepeating) {\n+\n+        /*\n+         * Repeating.\n+         */\n+\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+\n+        /*\n+         * Multi-Key specific repeated lookup.\n+         */\n+\n+        keyVectorSerializeWrite.setOutput(currentKeyOutput);\n+        keyVectorSerializeWrite.serializeWrite(batch, 0);\n+        JoinUtil.JoinResult joinResult;\n+        if (keyVectorSerializeWrite.getHasAnyNulls()) {\n+          // If right side is null, its a match for anti join.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          byte[] keyBytes = currentKeyOutput.getData();\n+          int keyLength = currentKeyOutput.getLength();\n+          // LOG.debug(CLASS_NAME + \" processOp all \" + displayBytes(keyBytes, 0, keyLength));\n+          joinResult = hashSet.contains(keyBytes, 0, keyLength, hashSetResults[0]);\n+          // reverse the join result from hash table for anti join.\n+          if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+            joinResult = JoinUtil.JoinResult.NOMATCH;\n+          }\n+        }\n+\n+        /*\n+         * Common repeated join result processing.\n+         */\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+\n+        /*\n+         * NOT Repeating.\n+         */\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matches / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+\n+        /*\n+         * Multi-Key specific variables.\n+         */\n+\n+        Output temp;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;\n+        JoinUtil.JoinResult saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+\n+        // Logical loop over the rows in the batch since the batch may have selected in use.\n+        for (int logical = 0; logical < inputLogicalSize; logical++) {\n+          int batchIndex = (selectedInUse ? selected[logical] : logical);\n+\n+          /*\n+           * Multi-Key get key.\n+           */\n+\n+          // Generate binary sortable key for current row in vectorized row batch.\n+          keyVectorSerializeWrite.setOutput(currentKeyOutput);\n+          keyVectorSerializeWrite.serializeWrite(batch, batchIndex);\n+          boolean isAnyNull = keyVectorSerializeWrite.getHasAnyNulls();\n+\n+          // LOG.debug(CLASS_NAME + \" currentKey \" +\n+          //      VectorizedBatchUtil.displayBytes(currentKeyOutput.getData(), 0, currentKeyOutput.getLength()));\n+\n+          /*\n+           * Equal key series checking.\n+           */\n+\n+          if (isAnyNull || !haveSaveKey || !saveKeyOutput.arraysEquals(currentKeyOutput)) {\n+\n+            // New key.\n+\n+            if (haveSaveKey) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 279}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgxODU0NA==", "bodyText": "I have kept the code same as Semi join. may be we can take up this refactoring as part of merging the code.", "url": "https://github.com/apache/hive/pull/1147#discussion_r466818544", "createdAt": "2020-08-07T04:48:00Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinMultiKeyOperator.java", "diffHunk": "@@ -0,0 +1,400 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorSerializeRow;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinBytesHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.apache.hadoop.hive.serde2.ByteStream.Output;\n+import org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableSerializeWrite;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// Multi-Key hash table import.\n+// Multi-Key specific imports.\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on Multi-Key\n+ * using hash set.\n+ */\n+public class VectorMapJoinAntiJoinMultiKeyOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  //------------------------------------------------------------------------------------------------\n+\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinMultiKeyOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  //------------------------------------------------------------------------------------------------\n+\n+  // (none)\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+  //---------------------------------------------------------------------------\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinBytesHashSet hashSet;\n+\n+  //---------------------------------------------------------------------------\n+  // Multi-Key specific members.\n+  //\n+\n+  // Object that can take a set of columns in row in a vectorized row batch and serialized it.\n+  // Known to not have any nulls.\n+  private transient VectorSerializeRow keyVectorSerializeWrite;\n+\n+  // The BinarySortable serialization of the current key.\n+  private transient Output currentKeyOutput;\n+\n+  // The BinarySortable serialization of the saved key for a possible series of equal keys.\n+  private transient Output saveKeyOutput;\n+\n+  //---------------------------------------------------------------------------\n+  // Pass-thru constructors.\n+  //\n+\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinMultiKeyOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinMultiKeyOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinMultiKeyOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                               VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  //---------------------------------------------------------------------------\n+  // Process Multi-Key Anti Join on a vectorized row batch.\n+  //\n+\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    /*\n+     * Initialize Multi-Key members for this specialized class.\n+     */\n+\n+    keyVectorSerializeWrite = new VectorSerializeRow(BinarySortableSerializeWrite.with(\n+            this.getConf().getKeyTblDesc().getProperties(), bigTableKeyColumnMap.length));\n+    keyVectorSerializeWrite.init(bigTableKeyTypeInfos, bigTableKeyColumnMap);\n+\n+    currentKeyOutput = new Output();\n+    saveKeyOutput = new Output();\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    /*\n+     * Get our Multi-Key hash set information for this specialized class.\n+     */\n+\n+    hashSet = (VectorMapJoinBytesHashSet) vectorMapJoinHashTable;\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+\n+      // Do the per-batch setup for an anti join.\n+\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      /*\n+       * Multi-Key specific declarations.\n+       */\n+\n+      // None.\n+\n+      /*\n+       * Multi-Key Long check for repeating.\n+       */\n+\n+      // If all BigTable input columns to key expressions are isRepeating, then\n+      // calculate key once; lookup once.\n+      boolean allKeyInputColumnsRepeating;\n+      if (bigTableKeyColumnMap.length == 0) {\n+       allKeyInputColumnsRepeating = false;\n+      } else {\n+        allKeyInputColumnsRepeating = true;\n+        for (int i = 0; i < bigTableKeyColumnMap.length; i++) {\n+          if (!batch.cols[bigTableKeyColumnMap[i]].isRepeating) {\n+            allKeyInputColumnsRepeating =  false;\n+            break;\n+          }\n+        }\n+      }\n+\n+      if (allKeyInputColumnsRepeating) {\n+\n+        /*\n+         * Repeating.\n+         */\n+\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+\n+        /*\n+         * Multi-Key specific repeated lookup.\n+         */\n+\n+        keyVectorSerializeWrite.setOutput(currentKeyOutput);\n+        keyVectorSerializeWrite.serializeWrite(batch, 0);\n+        JoinUtil.JoinResult joinResult;\n+        if (keyVectorSerializeWrite.getHasAnyNulls()) {\n+          // If right side is null, its a match for anti join.\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          byte[] keyBytes = currentKeyOutput.getData();\n+          int keyLength = currentKeyOutput.getLength();\n+          // LOG.debug(CLASS_NAME + \" processOp all \" + displayBytes(keyBytes, 0, keyLength));\n+          joinResult = hashSet.contains(keyBytes, 0, keyLength, hashSetResults[0]);\n+          // reverse the join result from hash table for anti join.\n+          if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+            joinResult = JoinUtil.JoinResult.NOMATCH;\n+          }\n+        }\n+\n+        /*\n+         * Common repeated join result processing.\n+         */\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+\n+        /*\n+         * NOT Repeating.\n+         */\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matches / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+\n+        /*\n+         * Multi-Key specific variables.\n+         */\n+\n+        Output temp;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;\n+        JoinUtil.JoinResult saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+\n+        // Logical loop over the rows in the batch since the batch may have selected in use.\n+        for (int logical = 0; logical < inputLogicalSize; logical++) {\n+          int batchIndex = (selectedInUse ? selected[logical] : logical);\n+\n+          /*\n+           * Multi-Key get key.\n+           */\n+\n+          // Generate binary sortable key for current row in vectorized row batch.\n+          keyVectorSerializeWrite.setOutput(currentKeyOutput);\n+          keyVectorSerializeWrite.serializeWrite(batch, batchIndex);\n+          boolean isAnyNull = keyVectorSerializeWrite.getHasAnyNulls();\n+\n+          // LOG.debug(CLASS_NAME + \" currentKey \" +\n+          //      VectorizedBatchUtil.displayBytes(currentKeyOutput.getData(), 0, currentKeyOutput.getLength()));\n+\n+          /*\n+           * Equal key series checking.\n+           */\n+\n+          if (isAnyNull || !haveSaveKey || !saveKeyOutput.arraysEquals(currentKeyOutput)) {\n+\n+            // New key.\n+\n+            if (haveSaveKey) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE3ODc4Ng=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 279}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTE4NTE5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinStringOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNToxOToyN1rOG09GqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjozNTo1NVrOG3MDFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE4MDI2NA==", "bodyText": "Inversion func", "url": "https://github.com/apache/hive/pull/1147#discussion_r458180264", "createdAt": "2020-07-21T15:19:27Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinStringOperator.java", "diffHunk": "@@ -0,0 +1,371 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinBytesHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// Single-Column String hash table import.\n+// Single-Column String specific imports.\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column String\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinStringOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  //------------------------------------------------------------------------------------------------\n+\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinStringOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  //------------------------------------------------------------------------------------------------\n+\n+  // (none)\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+  //---------------------------------------------------------------------------\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinBytesHashSet hashSet;\n+\n+  //---------------------------------------------------------------------------\n+  // Single-Column String specific members.\n+  //\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  //---------------------------------------------------------------------------\n+  // Pass-thru constructors.\n+  //\n+\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinStringOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinStringOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinStringOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                             VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  //---------------------------------------------------------------------------\n+  // Process Single-Column String anti Join on a vectorized row batch.\n+  //\n+\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    /*\n+     * Initialize Single-Column String members for this specialized class.\n+     */\n+\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    /*\n+     * Get our Single-Column String hash set information for this specialized class.\n+     */\n+\n+    hashSet = (VectorMapJoinBytesHashSet) vectorMapJoinHashTable;\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+\n+      // Do the per-batch setup for an anti join.\n+\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      /*\n+       * Single-Column String specific declarations.\n+       */\n+\n+      // The one join column for this specialized class.\n+      BytesColumnVector joinColVector = (BytesColumnVector) batch.cols[singleJoinColumn];\n+      byte[][] vector = joinColVector.vector;\n+      int[] start = joinColVector.start;\n+      int[] length = joinColVector.length;\n+\n+      /*\n+       * Single-Column Long check for repeating.\n+       */\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+\n+        /*\n+         * Repeating.\n+         */\n+\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+\n+        /*\n+         * Single-Column String specific repeated lookup.\n+         */\n+\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          byte[] keyBytes = vector[0];\n+          int keyStart = start[0];\n+          int keyLength = length[0];\n+          joinResult = hashSet.contains(keyBytes, keyStart, keyLength, hashSetResults[0]);\n+          if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+            joinResult = JoinUtil.JoinResult.NOMATCH;\n+          }\n+        }\n+\n+        /*\n+         * Common repeated join result processing.\n+         */\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+\n+        /*\n+         * NOT Repeating.\n+         */\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matchs / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+\n+        /*\n+         * Single-Column String specific variables.\n+         */\n+\n+        int saveKeyBatchIndex = -1;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;\n+        JoinUtil.JoinResult saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+\n+        // Logical loop over the rows in the batch since the batch may have selected in use.\n+        for (int logical = 0; logical < inputLogicalSize; logical++) {\n+          int batchIndex = (selectedInUse ? selected[logical] : logical);\n+\n+          /*\n+           * Single-Column String get key.\n+           */\n+\n+          // Implicit -- use batchIndex.\n+          boolean isNull = !joinColVector.noNulls && joinColVector.isNull[batchIndex];\n+\n+          /*\n+           * Equal key series checking.\n+           */\n+\n+          if (isNull || !haveSaveKey ||\n+              !StringExpr.equal(vector[saveKeyBatchIndex], start[saveKeyBatchIndex], length[saveKeyBatchIndex],\n+                      vector[batchIndex], start[batchIndex], length[batchIndex])) {\n+\n+            // New key.\n+\n+            if (haveSaveKey) {\n+              // Move on with our counts.\n+              switch (saveJoinResult) {\n+              case MATCH:\n+                // We have extracted the existence from the hash set result, so we don't keep it.\n+                break;\n+              case SPILL:\n+                // We keep the hash set result for its spill information.\n+                hashSetResultCount++;\n+                break;\n+              case NOMATCH:\n+                break;\n+              }\n+            }\n+\n+            if (isNull) {\n+              saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+              haveSaveKey = false;\n+            } else {\n+              // Regardless of our matching result, we keep that information to make multiple use\n+              // of it for a possible series of equal keys.\n+              haveSaveKey = true;\n+  \n+              /*\n+               * Single-Column String specific save key and lookup.\n+               */\n+  \n+              saveKeyBatchIndex = batchIndex;\n+  \n+              /*\n+               * Single-Column String specific lookup key.\n+               */\n+  \n+              byte[] keyBytes = vector[batchIndex];\n+              int keyStart = start[batchIndex];\n+              int keyLength = length[batchIndex];\n+              saveJoinResult = hashSet.contains(keyBytes, keyStart, keyLength, hashSetResults[hashSetResultCount]);\n+              if (saveJoinResult == JoinUtil.JoinResult.NOMATCH) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 288}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUyMjI2MQ==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460522261", "createdAt": "2020-07-26T12:35:55Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinAntiJoinStringOperator.java", "diffHunk": "@@ -0,0 +1,371 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.mapjoin;\n+\n+import org.apache.hadoop.hive.ql.CompilationOpContext;\n+import org.apache.hadoop.hive.ql.exec.JoinUtil;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizationContext;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable.VectorMapJoinBytesHashSet;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.VectorDesc;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.Arrays;\n+\n+// Single-Column String hash table import.\n+// Single-Column String specific imports.\n+\n+// TODO : Duplicate codes need to merge with semi join.\n+/*\n+ * Specialized class for doing a vectorized map join that is an anti join on a Single-Column String\n+ * using a hash set.\n+ */\n+public class VectorMapJoinAntiJoinStringOperator extends VectorMapJoinAntiJoinGenerateResultOperator {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  //------------------------------------------------------------------------------------------------\n+\n+  private static final String CLASS_NAME = VectorMapJoinAntiJoinStringOperator.class.getName();\n+  private static final Logger LOG = LoggerFactory.getLogger(CLASS_NAME);\n+\n+  protected String getLoggingPrefix() {\n+    return super.getLoggingPrefix(CLASS_NAME);\n+  }\n+\n+  //------------------------------------------------------------------------------------------------\n+\n+  // (none)\n+\n+  // The above members are initialized by the constructor and must not be\n+  // transient.\n+  //---------------------------------------------------------------------------\n+\n+  // The hash map for this specialized class.\n+  private transient VectorMapJoinBytesHashSet hashSet;\n+\n+  //---------------------------------------------------------------------------\n+  // Single-Column String specific members.\n+  //\n+\n+  // The column number for this one column join specialization.\n+  private transient int singleJoinColumn;\n+\n+  //---------------------------------------------------------------------------\n+  // Pass-thru constructors.\n+  //\n+\n+  /** Kryo ctor. */\n+  protected VectorMapJoinAntiJoinStringOperator() {\n+    super();\n+  }\n+\n+  public VectorMapJoinAntiJoinStringOperator(CompilationOpContext ctx) {\n+    super(ctx);\n+  }\n+\n+  public VectorMapJoinAntiJoinStringOperator(CompilationOpContext ctx, OperatorDesc conf,\n+                                             VectorizationContext vContext, VectorDesc vectorDesc) throws HiveException {\n+    super(ctx, conf, vContext, vectorDesc);\n+  }\n+\n+  //---------------------------------------------------------------------------\n+  // Process Single-Column String anti Join on a vectorized row batch.\n+  //\n+\n+  @Override\n+  protected void commonSetup() throws HiveException {\n+    super.commonSetup();\n+\n+    /*\n+     * Initialize Single-Column String members for this specialized class.\n+     */\n+\n+    singleJoinColumn = bigTableKeyColumnMap[0];\n+  }\n+\n+  @Override\n+  public void hashTableSetup() throws HiveException {\n+    super.hashTableSetup();\n+\n+    /*\n+     * Get our Single-Column String hash set information for this specialized class.\n+     */\n+\n+    hashSet = (VectorMapJoinBytesHashSet) vectorMapJoinHashTable;\n+  }\n+\n+  @Override\n+  public void processBatch(VectorizedRowBatch batch) throws HiveException {\n+\n+    try {\n+\n+      // Do the per-batch setup for an anti join.\n+\n+      // (Currently none)\n+      // antiPerBatchSetup(batch);\n+\n+      // For anti joins, we may apply the filter(s) now.\n+      for(VectorExpression ve : bigTableFilterExpressions) {\n+        ve.evaluate(batch);\n+      }\n+\n+      final int inputLogicalSize = batch.size;\n+      if (inputLogicalSize == 0) {\n+        return;\n+      }\n+\n+      // Perform any key expressions.  Results will go into scratch columns.\n+      if (bigTableKeyExpressions != null) {\n+        for (VectorExpression ve : bigTableKeyExpressions) {\n+          ve.evaluate(batch);\n+        }\n+      }\n+\n+      /*\n+       * Single-Column String specific declarations.\n+       */\n+\n+      // The one join column for this specialized class.\n+      BytesColumnVector joinColVector = (BytesColumnVector) batch.cols[singleJoinColumn];\n+      byte[][] vector = joinColVector.vector;\n+      int[] start = joinColVector.start;\n+      int[] length = joinColVector.length;\n+\n+      /*\n+       * Single-Column Long check for repeating.\n+       */\n+\n+      // Check single column for repeating.\n+      boolean allKeyInputColumnsRepeating = joinColVector.isRepeating;\n+\n+      if (allKeyInputColumnsRepeating) {\n+\n+        /*\n+         * Repeating.\n+         */\n+\n+        // All key input columns are repeating.  Generate key once.  Lookup once.\n+        // Since the key is repeated, we must use entry 0 regardless of selectedInUse.\n+\n+        /*\n+         * Single-Column String specific repeated lookup.\n+         */\n+\n+        JoinUtil.JoinResult joinResult;\n+        if (!joinColVector.noNulls && joinColVector.isNull[0]) {\n+          joinResult = JoinUtil.JoinResult.MATCH;\n+        } else {\n+          byte[] keyBytes = vector[0];\n+          int keyStart = start[0];\n+          int keyLength = length[0];\n+          joinResult = hashSet.contains(keyBytes, keyStart, keyLength, hashSetResults[0]);\n+          if (joinResult == JoinUtil.JoinResult.NOMATCH) {\n+            joinResult = JoinUtil.JoinResult.MATCH;\n+          } else if (joinResult == JoinUtil.JoinResult.MATCH) {\n+            joinResult = JoinUtil.JoinResult.NOMATCH;\n+          }\n+        }\n+\n+        /*\n+         * Common repeated join result processing.\n+         */\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" repeated joinResult \" + joinResult.name());\n+        }\n+        finishAntiRepeated(batch, joinResult, hashSetResults[0]);\n+      } else {\n+\n+        /*\n+         * NOT Repeating.\n+         */\n+\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(CLASS_NAME + \" batch #\" + batchCounter + \" non-repeated\");\n+        }\n+\n+        // We remember any matching rows in matchs / matchSize.  At the end of the loop,\n+        // selected / batch.size will represent both matching and non-matching rows for outer join.\n+        // Only deferred rows will have been removed from selected.\n+        int selected[] = batch.selected;\n+        boolean selectedInUse = batch.selectedInUse;\n+\n+        int hashSetResultCount = 0;\n+        int allMatchCount = 0;\n+        int spillCount = 0;\n+\n+        /*\n+         * Single-Column String specific variables.\n+         */\n+\n+        int saveKeyBatchIndex = -1;\n+\n+        // We optimize performance by only looking up the first key in a series of equal keys.\n+        boolean haveSaveKey = false;\n+        JoinUtil.JoinResult saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+\n+        // Logical loop over the rows in the batch since the batch may have selected in use.\n+        for (int logical = 0; logical < inputLogicalSize; logical++) {\n+          int batchIndex = (selectedInUse ? selected[logical] : logical);\n+\n+          /*\n+           * Single-Column String get key.\n+           */\n+\n+          // Implicit -- use batchIndex.\n+          boolean isNull = !joinColVector.noNulls && joinColVector.isNull[batchIndex];\n+\n+          /*\n+           * Equal key series checking.\n+           */\n+\n+          if (isNull || !haveSaveKey ||\n+              !StringExpr.equal(vector[saveKeyBatchIndex], start[saveKeyBatchIndex], length[saveKeyBatchIndex],\n+                      vector[batchIndex], start[batchIndex], length[batchIndex])) {\n+\n+            // New key.\n+\n+            if (haveSaveKey) {\n+              // Move on with our counts.\n+              switch (saveJoinResult) {\n+              case MATCH:\n+                // We have extracted the existence from the hash set result, so we don't keep it.\n+                break;\n+              case SPILL:\n+                // We keep the hash set result for its spill information.\n+                hashSetResultCount++;\n+                break;\n+              case NOMATCH:\n+                break;\n+              }\n+            }\n+\n+            if (isNull) {\n+              saveJoinResult = JoinUtil.JoinResult.NOMATCH;\n+              haveSaveKey = false;\n+            } else {\n+              // Regardless of our matching result, we keep that information to make multiple use\n+              // of it for a possible series of equal keys.\n+              haveSaveKey = true;\n+  \n+              /*\n+               * Single-Column String specific save key and lookup.\n+               */\n+  \n+              saveKeyBatchIndex = batchIndex;\n+  \n+              /*\n+               * Single-Column String specific lookup key.\n+               */\n+  \n+              byte[] keyBytes = vector[batchIndex];\n+              int keyStart = start[batchIndex];\n+              int keyLength = length[batchIndex];\n+              saveJoinResult = hashSet.contains(keyBytes, keyStart, keyLength, hashSetResults[hashSetResultCount]);\n+              if (saveJoinResult == JoinUtil.JoinResult.NOMATCH) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE4MDI2NA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 288}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTI0ODc1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNTozMzoxMlrOG09vMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwOTowMjo0OVrOG2CFJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE5MDY0Mg==", "bodyText": "Not sure I understand the issue here -- is the problem the fact that ANTI-join matches with NULL rows on the right side?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458190642", "createdAt": "2020-07-21T15:33:12Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java", "diffHunk": "@@ -74,7 +78,14 @@ public HiveJoinAddNotNullRule(Class<? extends Join> clazz,\n   @Override\n   public void onMatch(RelOptRuleCall call) {\n     Join join = call.rel(0);\n-    if (join.getJoinType() == JoinRelType.FULL || join.getCondition().isAlwaysTrue()) {\n+\n+    // For anti join case add the not null on right side if the condition is", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNTM1Mg==", "bodyText": "For the case when we have. join condition which gets evaluated, it will return false while comparing with a null on the right side. But for always true join condition, we will not do a match for right side assuming it's always true.  So for anti join, the left side records will not be emitted. To avoid this we put a null check on right side table and for all null entry, no records will be projected from right side and thus all records from left side will be emitted. So the comment is not very accurate. It's like even if the condition is always true, we add a null check on right side for anti join. I will update it.", "url": "https://github.com/apache/hive/pull/1147#discussion_r459215352", "createdAt": "2020-07-23T05:01:33Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java", "diffHunk": "@@ -74,7 +78,14 @@ public HiveJoinAddNotNullRule(Class<? extends Join> clazz,\n   @Override\n   public void onMatch(RelOptRuleCall call) {\n     Join join = call.rel(0);\n-    if (join.getJoinType() == JoinRelType.FULL || join.getCondition().isAlwaysTrue()) {\n+\n+    // For anti join case add the not null on right side if the condition is", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE5MDY0Mg=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTMxMDM3Mg==", "bodyText": "Thanks! Makes sense now", "url": "https://github.com/apache/hive/pull/1147#discussion_r459310372", "createdAt": "2020-07-23T09:02:49Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java", "diffHunk": "@@ -74,7 +78,14 @@ public HiveJoinAddNotNullRule(Class<? extends Join> clazz,\n   @Override\n   public void onMatch(RelOptRuleCall call) {\n     Join join = call.rel(0);\n-    if (join.getJoinType() == JoinRelType.FULL || join.getCondition().isAlwaysTrue()) {\n+\n+    // For anti join case add the not null on right side if the condition is", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODE5MDY0Mg=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTM0MDQzOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/ppd/SyntheticJoinPredicate.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNTo1Mjo1NVrOG0-pEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwOTowMDoxN1rOG2B_ug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIwNTQ1Ng==", "bodyText": "Makes sense, for this particular purpose in the future we could something like The opossite bloom filter to support such cases\nhttps://github.com/jmhodges/opposite_of_a_bloom_filter/", "url": "https://github.com/apache/hive/pull/1147#discussion_r458205456", "createdAt": "2020-07-21T15:52:55Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/ppd/SyntheticJoinPredicate.java", "diffHunk": "@@ -339,6 +339,12 @@ String getFuncText(String funcText, final int srcPos) {\n           vector.add(right, left);\n           break;\n         case JoinDesc.LEFT_OUTER_JOIN:\n+        case JoinDesc.ANTI_JOIN:\n+        //TODO : In case of anti join, bloom filter can be created on left side also (\"IN (keylist right table)\").\n+        // But the filter should be \"not-in\" (\"NOT IN (keylist right table)\") as we want to select the records from\n+        // left side which are not present in the right side. But it may cause wrong result as\n+        // bloom filter may have false positive and thus simply adding not is not correct,\n+        // special handling is required for \"NOT IN\".", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ5ODgwNw==", "bodyText": "Could we create a follow-up JIRA to explore this?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458498807", "createdAt": "2020-07-22T02:35:53Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/ppd/SyntheticJoinPredicate.java", "diffHunk": "@@ -339,6 +339,12 @@ String getFuncText(String funcText, final int srcPos) {\n           vector.add(right, left);\n           break;\n         case JoinDesc.LEFT_OUTER_JOIN:\n+        case JoinDesc.ANTI_JOIN:\n+        //TODO : In case of anti join, bloom filter can be created on left side also (\"IN (keylist right table)\").\n+        // But the filter should be \"not-in\" (\"NOT IN (keylist right table)\") as we want to select the records from\n+        // left side which are not present in the right side. But it may cause wrong result as\n+        // bloom filter may have false positive and thus simply adding not is not correct,\n+        // special handling is required for \"NOT IN\".", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIwNTQ1Ng=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxNjg3OA==", "bodyText": "created a Jira ..https://issues.apache.org/jira/browse/HIVE-23903", "url": "https://github.com/apache/hive/pull/1147#discussion_r459216878", "createdAt": "2020-07-23T05:08:19Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/ppd/SyntheticJoinPredicate.java", "diffHunk": "@@ -339,6 +339,12 @@ String getFuncText(String funcText, final int srcPos) {\n           vector.add(right, left);\n           break;\n         case JoinDesc.LEFT_OUTER_JOIN:\n+        case JoinDesc.ANTI_JOIN:\n+        //TODO : In case of anti join, bloom filter can be created on left side also (\"IN (keylist right table)\").\n+        // But the filter should be \"not-in\" (\"NOT IN (keylist right table)\") as we want to select the records from\n+        // left side which are not present in the right side. But it may cause wrong result as\n+        // bloom filter may have false positive and thus simply adding not is not correct,\n+        // special handling is required for \"NOT IN\".", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIwNTQ1Ng=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTMwODk4Ng==", "bodyText": "Thank Mahesh! Had this in the back of my head for a while -- this will be useful for a bunch of cases including anti-joins", "url": "https://github.com/apache/hive/pull/1147#discussion_r459308986", "createdAt": "2020-07-23T09:00:17Z", "author": {"login": "pgaref"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/ppd/SyntheticJoinPredicate.java", "diffHunk": "@@ -339,6 +339,12 @@ String getFuncText(String funcText, final int srcPos) {\n           vector.add(right, left);\n           break;\n         case JoinDesc.LEFT_OUTER_JOIN:\n+        case JoinDesc.ANTI_JOIN:\n+        //TODO : In case of anti join, bloom filter can be created on left side also (\"IN (keylist right table)\").\n+        // But the filter should be \"not-in\" (\"NOT IN (keylist right table)\") as we want to select the records from\n+        // left side which are not present in the right side. But it may cause wrong result as\n+        // bloom filter may have false positive and thus simply adding not is not correct,\n+        // special handling is required for \"NOT IN\".", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIwNTQ1Ng=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTM0MzM2OnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/TestMapJoinOperator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNTo1MzozNFrOG0-rBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwNToyMjowOVrOG18j_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIwNTk1OA==", "bodyText": "Shall we open a ticket to track this? What is the main challenge here?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458205958", "createdAt": "2020-07-21T15:53:34Z", "author": {"login": "pgaref"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/TestMapJoinOperator.java", "diffHunk": "@@ -1792,6 +1794,8 @@ private void executeTest(MapJoinTestDescription testDesc, MapJoinTestData testDa\n     case FULL_OUTER:\n       executeTestFullOuter(testDesc, testData, title);\n       break;\n+    case ANTI: //TODO", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIxOTk2Ng==", "bodyText": "https://issues.apache.org/jira/browse/HIVE-23904", "url": "https://github.com/apache/hive/pull/1147#discussion_r459219966", "createdAt": "2020-07-23T05:22:09Z", "author": {"login": "maheshk114"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/exec/vector/mapjoin/TestMapJoinOperator.java", "diffHunk": "@@ -1792,6 +1794,8 @@ private void executeTest(MapJoinTestDescription testDesc, MapJoinTestData testDa\n     case FULL_OUTER:\n       executeTestFullOuter(testDesc, testData, title);\n       break;\n+    case ANTI: //TODO", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIwNTk1OA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MDE5MjYyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveAntiJoin.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxOTo0MTowOVrOG1HCbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwMjozOTowOFrOG2hlzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0MzAyMQ==", "bodyText": "I wonder whether we really need all these operator variants at this stage. In Calcite, it all seems to be based on a single Join class in newer releases. Can we create a follow-up JIRA to explore whether we could merge HiveJoin, HiveSemiJoin, and HiveAntiSemiJoin?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458343021", "createdAt": "2020-07-21T19:41:09Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveAntiJoin.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.reloperators;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Sets;\n+import org.apache.calcite.plan.RelOptCluster;\n+import org.apache.calcite.plan.RelTraitSet;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.CalciteSemanticException;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRulesRegistry;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HiveAntiJoin extends Join implements HiveRelNode {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgyNjYzNg==", "bodyText": "https://issues.apache.org/jira/browse/HIVE-23919", "url": "https://github.com/apache/hive/pull/1147#discussion_r459826636", "createdAt": "2020-07-24T02:39:08Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveAntiJoin.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.reloperators;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Sets;\n+import org.apache.calcite.plan.RelOptCluster;\n+import org.apache.calcite.plan.RelTraitSet;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.CalciteSemanticException;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRulesRegistry;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HiveAntiJoin extends Join implements HiveRelNode {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0MzAyMQ=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MDE5MzQzOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxOTo0MToyN1rOG1HC9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjowOTo0OFrOG3L4Yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0MzE1OQ==", "bodyText": "nit. spacing", "url": "https://github.com/apache/hive/pull/1147#discussion_r458343159", "createdAt": "2020-07-21T19:41:27Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java", "diffHunk": "@@ -56,6 +57,9 @@\n   public static final HiveJoinAddNotNullRule INSTANCE_SEMIJOIN =\n       new HiveJoinAddNotNullRule(HiveSemiJoin.class, HiveRelFactories.HIVE_FILTER_FACTORY);\n \n+  public static final HiveJoinAddNotNullRule INSTANCE_ANTIJOIN =\n+          new HiveJoinAddNotNullRule(HiveAntiJoin.class, HiveRelFactories.HIVE_FILTER_FACTORY);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUxOTUyMw==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460519523", "createdAt": "2020-07-26T12:09:48Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java", "diffHunk": "@@ -56,6 +57,9 @@\n   public static final HiveJoinAddNotNullRule INSTANCE_SEMIJOIN =\n       new HiveJoinAddNotNullRule(HiveSemiJoin.class, HiveRelFactories.HIVE_FILTER_FACTORY);\n \n+  public static final HiveJoinAddNotNullRule INSTANCE_ANTIJOIN =\n+          new HiveJoinAddNotNullRule(HiveAntiJoin.class, HiveRelFactories.HIVE_FILTER_FACTORY);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0MzE1OQ=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MDIyMzI2OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxOTo0OTo1NlrOG1HVDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwMjo0MTowM1rOG2hnOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0Nzc4OA==", "bodyText": "This condition does not seem correct (I think the example you provided in the comment is different case).\n\nWhen join.getCondition().isAlwaysTrue(), we always bail out because we cannot introduce is not null condition on any key column (how can you know on which keys you would be filtering?).\nFor full outer join, we bail out because even non-matching rows from any of the inputs should still produce output rows.\nFor left anti join, as you did below, we introduce is not null filter on the right side. This only happens if condition is not always true.", "url": "https://github.com/apache/hive/pull/1147#discussion_r458347788", "createdAt": "2020-07-21T19:49:56Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java", "diffHunk": "@@ -74,7 +78,14 @@ public HiveJoinAddNotNullRule(Class<? extends Join> clazz,\n   @Override\n   public void onMatch(RelOptRuleCall call) {\n     Join join = call.rel(0);\n-    if (join.getJoinType() == JoinRelType.FULL || join.getCondition().isAlwaysTrue()) {\n+\n+    // For anti join case add the not null on right side if the condition is\n+    // always true. This is done because during execution, anti join expect the right side to\n+    // be empty and if we dont put null check on right, for null only right side table and condition\n+    // always true, execution will produce 0 records.\n+    // eg  select * from left_tbl where (select 1 from all_null_right limit 1) is null\n+    if (join.getJoinType() == JoinRelType.FULL ||\n+            (join.getJoinType() != JoinRelType.ANTI && join.getCondition().isAlwaysTrue())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgyNzAwMg==", "bodyText": "Yes, the comment is not proper. It's like we will add a not null condition for anti join even if the condition is always true.", "url": "https://github.com/apache/hive/pull/1147#discussion_r459827002", "createdAt": "2020-07-24T02:41:03Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java", "diffHunk": "@@ -74,7 +78,14 @@ public HiveJoinAddNotNullRule(Class<? extends Join> clazz,\n   @Override\n   public void onMatch(RelOptRuleCall call) {\n     Join join = call.rel(0);\n-    if (join.getJoinType() == JoinRelType.FULL || join.getCondition().isAlwaysTrue()) {\n+\n+    // For anti join case add the not null on right side if the condition is\n+    // always true. This is done because during execution, anti join expect the right side to\n+    // be empty and if we dont put null check on right, for null only right side table and condition\n+    // always true, execution will produce 0 records.\n+    // eg  select * from left_tbl where (select 1 from all_null_right limit 1) is null\n+    if (join.getJoinType() == JoinRelType.FULL ||\n+            (join.getJoinType() != JoinRelType.ANTI && join.getCondition().isAlwaysTrue())) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0Nzc4OA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MDIyNzg1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/plan/VectorMapJoinDesc.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxOTo1MToxNFrOG1HX1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjowNTo0NFrOG3L2xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0ODUwMg==", "bodyText": "LEFT_ANTI_SEMI ?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458348502", "createdAt": "2020-07-21T19:51:14Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/plan/VectorMapJoinDesc.java", "diffHunk": "@@ -89,7 +89,8 @@ public PrimitiveTypeInfo getPrimitiveTypeInfo() {\n     INNER_BIG_ONLY,\n     LEFT_SEMI,\n     OUTER,\n-    FULL_OUTER\n+    FULL_OUTER,\n+    ANTI", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUxOTExMQ==", "bodyText": "LEFT_ANTI", "url": "https://github.com/apache/hive/pull/1147#discussion_r460519111", "createdAt": "2020-07-26T12:05:44Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/plan/VectorMapJoinDesc.java", "diffHunk": "@@ -89,7 +89,8 @@ public PrimitiveTypeInfo getPrimitiveTypeInfo() {\n     INNER_BIG_ONLY,\n     LEFT_SEMI,\n     OUTER,\n-    FULL_OUTER\n+    FULL_OUTER,\n+    ANTI", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM0ODUwMg=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MDU0NjYzOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinConstraintsRule.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMToyOTozNFrOG1KekQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwMzo1MjoyNFrOG2iaiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM5OTM3Nw==", "bodyText": "This is not correct and needs further thinking. If we have a PK-FK join that is only appending columns to the FK side, it basically means it is not filtering anything (everything is matching). If that is the case, then ANTIJOIN result would be empty? We could detect this at planning time and trigger the rewriting.\nCould we bail out from the rule if it is an ANTIJOIN and create a follow-up JIRA to tackle this and introduce further tests?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458399377", "createdAt": "2020-07-21T21:29:34Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinConstraintsRule.java", "diffHunk": "@@ -100,7 +100,8 @@ public void onMatch(RelOptRuleCall call) {\n     // These boolean values represent corresponding left, right input which is potential FK\n     boolean leftInputPotentialFK = topRefs.intersects(leftBits);\n     boolean rightInputPotentialFK = topRefs.intersects(rightBits);\n-    if (leftInputPotentialFK && rightInputPotentialFK && (joinType == JoinRelType.INNER || joinType == JoinRelType.SEMI)) {\n+    if (leftInputPotentialFK && rightInputPotentialFK &&\n+            (joinType == JoinRelType.INNER || joinType == JoinRelType.SEMI || joinType == JoinRelType.ANTI)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg0MDEzOQ==", "bodyText": "https://issues.apache.org/jira/browse/HIVE-23920", "url": "https://github.com/apache/hive/pull/1147#discussion_r459840139", "createdAt": "2020-07-24T03:52:24Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinConstraintsRule.java", "diffHunk": "@@ -100,7 +100,8 @@ public void onMatch(RelOptRuleCall call) {\n     // These boolean values represent corresponding left, right input which is potential FK\n     boolean leftInputPotentialFK = topRefs.intersects(leftBits);\n     boolean rightInputPotentialFK = topRefs.intersects(rightBits);\n-    if (leftInputPotentialFK && rightInputPotentialFK && (joinType == JoinRelType.INNER || joinType == JoinRelType.SEMI)) {\n+    if (leftInputPotentialFK && rightInputPotentialFK &&\n+            (joinType == JoinRelType.INNER || joinType == JoinRelType.SEMI || joinType == JoinRelType.ANTI)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODM5OTM3Nw=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MDYwNDcwOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinProjectTransposeRule.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMTo0ODowMFrOG1LA6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwMzo1Nzo0NFrOG2iePw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQwODE2OA==", "bodyText": "Why is this rule skipped if it is an ANTI join? It seems this rule could be straightforward. Should we create a follow-up JIRA?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458408168", "createdAt": "2020-07-21T21:48:00Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinProjectTransposeRule.java", "diffHunk": "@@ -133,6 +135,10 @@ private HiveJoinProjectTransposeRuleBase(\n \n     public void onMatch(RelOptRuleCall call) {\n       //TODO: this can be removed once CALCITE-3824 is released\n+      Join joinRel = call.rel(0);\n+      if (joinRel.getJoinType() == JoinRelType.ANTI) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg0MTA4Nw==", "bodyText": "This was causing some issue with having clause. https://issues.apache.org/jira/browse/HIVE-23921", "url": "https://github.com/apache/hive/pull/1147#discussion_r459841087", "createdAt": "2020-07-24T03:57:44Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinProjectTransposeRule.java", "diffHunk": "@@ -133,6 +135,10 @@ private HiveJoinProjectTransposeRuleBase(\n \n     public void onMatch(RelOptRuleCall call) {\n       //TODO: this can be removed once CALCITE-3824 is released\n+      Join joinRel = call.rel(0);\n+      if (joinRel.getJoinType() == JoinRelType.ANTI) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQwODE2OA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MDYzNTU3OnYy", "diffSide": "RIGHT", "path": "ql/src/test/results/clientpositive/perf/tez/cbo_query16_anti_join.q.out", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMTo1ODoxNlrOG1LTJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwNToyMToyNFrOG9M1sw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQxMjgzOA==", "bodyText": "No need to add this perf tests if enabled by default.", "url": "https://github.com/apache/hive/pull/1147#discussion_r458412838", "createdAt": "2020-07-21T21:58:16Z", "author": {"login": "jcamachor"}, "path": "ql/src/test/results/clientpositive/perf/tez/cbo_query16_anti_join.q.out", "diffHunk": "@@ -0,0 +1,99 @@\n+PREHOOK: query: explain cbo", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgyNjY3NQ==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r466826675", "createdAt": "2020-08-07T05:21:24Z", "author": {"login": "maheshk114"}, "path": "ql/src/test/results/clientpositive/perf/tez/cbo_query16_anti_join.q.out", "diffHunk": "@@ -0,0 +1,99 @@\n+PREHOOK: query: explain cbo", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQxMjgzOA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MDg2MjE5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMzozMDo0OFrOG1NZBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjowNDozMlrOG3L2Pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ0NzEwOQ==", "bodyText": "Rename HiveJoinWithFilterToAntiJoinRule to HiveAntiSemiJoinRule to follow naming convention for other rules, e.g., HiveSemiJoinRule.", "url": "https://github.com/apache/hive/pull/1147#discussion_r458447109", "createdAt": "2020-07-21T23:30:48Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveJoinWithFilterToAntiJoinRule extends RelOptRule {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUxODk3NA==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460518974", "createdAt": "2020-07-26T12:04:32Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveJoinWithFilterToAntiJoinRule extends RelOptRule {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ0NzEwOQ=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTAwMTA5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMDo0MDo0NFrOG1OpKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjowMzowMlrOG3L1oQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2NzYyNA==", "bodyText": "projectsRight() is always true for LEFT join, this condition can be simplified.", "url": "https://github.com/apache/hive/pull/1147#discussion_r458467624", "createdAt": "2020-07-22T00:40:44Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveJoinWithFilterToAntiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveJoinWithFilterToAntiJoinRule.class);\n+  public static final HiveJoinWithFilterToAntiJoinRule INSTANCE = new HiveJoinWithFilterToAntiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveJoinWithFilterToAntiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Matched HiveAntiJoinRule\");\n+\n+    if (join.getCondition().isAlwaysTrue()) {\n+      return;\n+    }\n+\n+    //We support conversion from left outer join only.\n+    if (join.getJoinType() != JoinRelType.LEFT) {\n+      return;\n+    }\n+\n+    assert (filter != null);\n+\n+    List<RexNode> aboveFilters = RelOptUtil.conjunctions(filter.getCondition());\n+    boolean hasIsNull = false;\n+\n+    // Get all filter condition and check if any of them is a \"is null\" kind.\n+    for (RexNode filterNode : aboveFilters) {\n+      if (filterNode.getKind() == SqlKind.IS_NULL &&\n+              isFilterFromRightSide(join, filterNode, join.getJoinType())) {\n+        hasIsNull = true;\n+        break;\n+      }\n+    }\n+\n+    // Is null should be on a key from right side of the join.\n+    if (!hasIsNull) {\n+      return;\n+    }\n+\n+    // Build anti join with same left, right child and condition as original left outer join.\n+    Join anti = join.copy(join.getTraitSet(), join.getCondition(),\n+            join.getLeft(), join.getRight(), JoinRelType.ANTI, false);\n+\n+    //TODO : Do we really need it\n+    call.getPlanner().onCopy(join, anti);\n+\n+    RelNode newProject = getNewProjectNode(project, anti);\n+    if (newProject != null) {\n+      call.getPlanner().onCopy(project, newProject);\n+      call.transformTo(newProject);\n+    }\n+  }\n+\n+  protected RelNode getNewProjectNode(Project oldProject, Join newJoin) {\n+    List<RelDataTypeField> newJoinFiledList = newJoin.getRowType().getFieldList();\n+    List<RexNode> newProjectExpr = new ArrayList<>();\n+    for (RexNode field : oldProject.getProjects()) {\n+      if (!(field instanceof  RexInputRef)) {\n+        return null;\n+      }\n+      int idx = ((RexInputRef)field).getIndex();\n+      if (idx > newJoinFiledList.size()) {\n+        LOG.debug(\" Project filed \" + ((RexInputRef) field).getName() +\n+                \" is from right side of join. Can not convert to anti join.\");\n+        return null;\n+      }\n+\n+      final RexInputRef ref = newJoin.getCluster().getRexBuilder()\n+              .makeInputRef(field.getType(), idx);\n+      newProjectExpr.add(ref);\n+    }\n+    return oldProject.copy(oldProject.getTraitSet(), newJoin, newProjectExpr, oldProject.getRowType());\n+  }\n+\n+  private boolean isFilterFromRightSide(RelNode joinRel, RexNode filter, JoinRelType joinType) {\n+    List<RelDataTypeField> joinFields = joinRel.getRowType().getFieldList();\n+    int nTotalFields = joinFields.size();\n+\n+    List<RelDataTypeField> leftFields = (joinRel.getInputs().get(0)).getRowType().getFieldList();\n+    int nFieldsLeft = leftFields.size();\n+    List<RelDataTypeField> rightFields = (joinRel.getInputs().get(1)).getRowType().getFieldList();\n+    int nFieldsRight = rightFields.size();\n+    assert nTotalFields == (!joinType.projectsRight() ? nFieldsLeft : nFieldsLeft + nFieldsRight);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUxODgxNw==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460518817", "createdAt": "2020-07-26T12:03:02Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveJoinWithFilterToAntiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveJoinWithFilterToAntiJoinRule.class);\n+  public static final HiveJoinWithFilterToAntiJoinRule INSTANCE = new HiveJoinWithFilterToAntiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveJoinWithFilterToAntiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Matched HiveAntiJoinRule\");\n+\n+    if (join.getCondition().isAlwaysTrue()) {\n+      return;\n+    }\n+\n+    //We support conversion from left outer join only.\n+    if (join.getJoinType() != JoinRelType.LEFT) {\n+      return;\n+    }\n+\n+    assert (filter != null);\n+\n+    List<RexNode> aboveFilters = RelOptUtil.conjunctions(filter.getCondition());\n+    boolean hasIsNull = false;\n+\n+    // Get all filter condition and check if any of them is a \"is null\" kind.\n+    for (RexNode filterNode : aboveFilters) {\n+      if (filterNode.getKind() == SqlKind.IS_NULL &&\n+              isFilterFromRightSide(join, filterNode, join.getJoinType())) {\n+        hasIsNull = true;\n+        break;\n+      }\n+    }\n+\n+    // Is null should be on a key from right side of the join.\n+    if (!hasIsNull) {\n+      return;\n+    }\n+\n+    // Build anti join with same left, right child and condition as original left outer join.\n+    Join anti = join.copy(join.getTraitSet(), join.getCondition(),\n+            join.getLeft(), join.getRight(), JoinRelType.ANTI, false);\n+\n+    //TODO : Do we really need it\n+    call.getPlanner().onCopy(join, anti);\n+\n+    RelNode newProject = getNewProjectNode(project, anti);\n+    if (newProject != null) {\n+      call.getPlanner().onCopy(project, newProject);\n+      call.transformTo(newProject);\n+    }\n+  }\n+\n+  protected RelNode getNewProjectNode(Project oldProject, Join newJoin) {\n+    List<RelDataTypeField> newJoinFiledList = newJoin.getRowType().getFieldList();\n+    List<RexNode> newProjectExpr = new ArrayList<>();\n+    for (RexNode field : oldProject.getProjects()) {\n+      if (!(field instanceof  RexInputRef)) {\n+        return null;\n+      }\n+      int idx = ((RexInputRef)field).getIndex();\n+      if (idx > newJoinFiledList.size()) {\n+        LOG.debug(\" Project filed \" + ((RexInputRef) field).getName() +\n+                \" is from right side of join. Can not convert to anti join.\");\n+        return null;\n+      }\n+\n+      final RexInputRef ref = newJoin.getCluster().getRexBuilder()\n+              .makeInputRef(field.getType(), idx);\n+      newProjectExpr.add(ref);\n+    }\n+    return oldProject.copy(oldProject.getTraitSet(), newJoin, newProjectExpr, oldProject.getRowType());\n+  }\n+\n+  private boolean isFilterFromRightSide(RelNode joinRel, RexNode filter, JoinRelType joinType) {\n+    List<RelDataTypeField> joinFields = joinRel.getRowType().getFieldList();\n+    int nTotalFields = joinFields.size();\n+\n+    List<RelDataTypeField> leftFields = (joinRel.getInputs().get(0)).getRowType().getFieldList();\n+    int nFieldsLeft = leftFields.size();\n+    List<RelDataTypeField> rightFields = (joinRel.getInputs().get(1)).getRowType().getFieldList();\n+    int nFieldsRight = rightFields.size();\n+    assert nTotalFields == (!joinType.projectsRight() ? nFieldsLeft : nFieldsLeft + nFieldsRight);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2NzYyNA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 142}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTAwMzc1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMDo0MjoxNFrOG1OqvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjowMjo1NFrOG3L1mA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2ODAyOA==", "bodyText": "No need for this line indeed.", "url": "https://github.com/apache/hive/pull/1147#discussion_r458468028", "createdAt": "2020-07-22T00:42:14Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveJoinWithFilterToAntiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveJoinWithFilterToAntiJoinRule.class);\n+  public static final HiveJoinWithFilterToAntiJoinRule INSTANCE = new HiveJoinWithFilterToAntiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveJoinWithFilterToAntiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Matched HiveAntiJoinRule\");\n+\n+    if (join.getCondition().isAlwaysTrue()) {\n+      return;\n+    }\n+\n+    //We support conversion from left outer join only.\n+    if (join.getJoinType() != JoinRelType.LEFT) {\n+      return;\n+    }\n+\n+    assert (filter != null);\n+\n+    List<RexNode> aboveFilters = RelOptUtil.conjunctions(filter.getCondition());\n+    boolean hasIsNull = false;\n+\n+    // Get all filter condition and check if any of them is a \"is null\" kind.\n+    for (RexNode filterNode : aboveFilters) {\n+      if (filterNode.getKind() == SqlKind.IS_NULL &&\n+              isFilterFromRightSide(join, filterNode, join.getJoinType())) {\n+        hasIsNull = true;\n+        break;\n+      }\n+    }\n+\n+    // Is null should be on a key from right side of the join.\n+    if (!hasIsNull) {\n+      return;\n+    }\n+\n+    // Build anti join with same left, right child and condition as original left outer join.\n+    Join anti = join.copy(join.getTraitSet(), join.getCondition(),\n+            join.getLeft(), join.getRight(), JoinRelType.ANTI, false);\n+\n+    //TODO : Do we really need it\n+    call.getPlanner().onCopy(join, anti);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUxODgwOA==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460518808", "createdAt": "2020-07-26T12:02:54Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveJoinWithFilterToAntiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveJoinWithFilterToAntiJoinRule.class);\n+  public static final HiveJoinWithFilterToAntiJoinRule INSTANCE = new HiveJoinWithFilterToAntiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveJoinWithFilterToAntiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Matched HiveAntiJoinRule\");\n+\n+    if (join.getCondition().isAlwaysTrue()) {\n+      return;\n+    }\n+\n+    //We support conversion from left outer join only.\n+    if (join.getJoinType() != JoinRelType.LEFT) {\n+      return;\n+    }\n+\n+    assert (filter != null);\n+\n+    List<RexNode> aboveFilters = RelOptUtil.conjunctions(filter.getCondition());\n+    boolean hasIsNull = false;\n+\n+    // Get all filter condition and check if any of them is a \"is null\" kind.\n+    for (RexNode filterNode : aboveFilters) {\n+      if (filterNode.getKind() == SqlKind.IS_NULL &&\n+              isFilterFromRightSide(join, filterNode, join.getJoinType())) {\n+        hasIsNull = true;\n+        break;\n+      }\n+    }\n+\n+    // Is null should be on a key from right side of the join.\n+    if (!hasIsNull) {\n+      return;\n+    }\n+\n+    // Build anti join with same left, right child and condition as original left outer join.\n+    Join anti = join.copy(join.getTraitSet(), join.getCondition(),\n+            join.getLeft(), join.getRight(), JoinRelType.ANTI, false);\n+\n+    //TODO : Do we really need it\n+    call.getPlanner().onCopy(join, anti);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2ODAyOA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTAwNDAyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMDo0MjoyOVrOG1Oq8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMjowMjo0N1rOG3L1jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2ODA4MQ==", "bodyText": "This call is not necessary either.", "url": "https://github.com/apache/hive/pull/1147#discussion_r458468081", "createdAt": "2020-07-22T00:42:29Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveJoinWithFilterToAntiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveJoinWithFilterToAntiJoinRule.class);\n+  public static final HiveJoinWithFilterToAntiJoinRule INSTANCE = new HiveJoinWithFilterToAntiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveJoinWithFilterToAntiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Matched HiveAntiJoinRule\");\n+\n+    if (join.getCondition().isAlwaysTrue()) {\n+      return;\n+    }\n+\n+    //We support conversion from left outer join only.\n+    if (join.getJoinType() != JoinRelType.LEFT) {\n+      return;\n+    }\n+\n+    assert (filter != null);\n+\n+    List<RexNode> aboveFilters = RelOptUtil.conjunctions(filter.getCondition());\n+    boolean hasIsNull = false;\n+\n+    // Get all filter condition and check if any of them is a \"is null\" kind.\n+    for (RexNode filterNode : aboveFilters) {\n+      if (filterNode.getKind() == SqlKind.IS_NULL &&\n+              isFilterFromRightSide(join, filterNode, join.getJoinType())) {\n+        hasIsNull = true;\n+        break;\n+      }\n+    }\n+\n+    // Is null should be on a key from right side of the join.\n+    if (!hasIsNull) {\n+      return;\n+    }\n+\n+    // Build anti join with same left, right child and condition as original left outer join.\n+    Join anti = join.copy(join.getTraitSet(), join.getCondition(),\n+            join.getLeft(), join.getRight(), JoinRelType.ANTI, false);\n+\n+    //TODO : Do we really need it\n+    call.getPlanner().onCopy(join, anti);\n+\n+    RelNode newProject = getNewProjectNode(project, anti);\n+    if (newProject != null) {\n+      call.getPlanner().onCopy(project, newProject);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUxODc5OQ==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460518799", "createdAt": "2020-07-26T12:02:47Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveJoinWithFilterToAntiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveJoinWithFilterToAntiJoinRule.class);\n+  public static final HiveJoinWithFilterToAntiJoinRule INSTANCE = new HiveJoinWithFilterToAntiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveJoinWithFilterToAntiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Matched HiveAntiJoinRule\");\n+\n+    if (join.getCondition().isAlwaysTrue()) {\n+      return;\n+    }\n+\n+    //We support conversion from left outer join only.\n+    if (join.getJoinType() != JoinRelType.LEFT) {\n+      return;\n+    }\n+\n+    assert (filter != null);\n+\n+    List<RexNode> aboveFilters = RelOptUtil.conjunctions(filter.getCondition());\n+    boolean hasIsNull = false;\n+\n+    // Get all filter condition and check if any of them is a \"is null\" kind.\n+    for (RexNode filterNode : aboveFilters) {\n+      if (filterNode.getKind() == SqlKind.IS_NULL &&\n+              isFilterFromRightSide(join, filterNode, join.getJoinType())) {\n+        hasIsNull = true;\n+        break;\n+      }\n+    }\n+\n+    // Is null should be on a key from right side of the join.\n+    if (!hasIsNull) {\n+      return;\n+    }\n+\n+    // Build anti join with same left, right child and condition as original left outer join.\n+    Join anti = join.copy(join.getTraitSet(), join.getCondition(),\n+            join.getLeft(), join.getRight(), JoinRelType.ANTI, false);\n+\n+    //TODO : Do we really need it\n+    call.getPlanner().onCopy(join, anti);\n+\n+    RelNode newProject = getNewProjectNode(project, anti);\n+    if (newProject != null) {\n+      call.getPlanner().onCopy(project, newProject);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ2ODA4MQ=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTAzMDY5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMDo1NzowNlrOG1O6ug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQxMDo1NTo0MFrOG2rNKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3MjEyMg==", "bodyText": "We should simply use project.copy and remove this method (it seems we are also limiting it to RexInputRef... why is that done?).\nYou can verify that all fields are coming from left input using isFilterFromRightSide; in fact, you can rename it to isExprFromRightSide and pass a list of RexNode as second parameter. If they do not, just bail out similarly to what you do with the filter. Please, check whether such method already exists (e.g., in HiveCalciteUtil or RexUtil) so we do not re-implement it if it does.", "url": "https://github.com/apache/hive/pull/1147#discussion_r458472122", "createdAt": "2020-07-22T00:57:06Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveJoinWithFilterToAntiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveJoinWithFilterToAntiJoinRule.class);\n+  public static final HiveJoinWithFilterToAntiJoinRule INSTANCE = new HiveJoinWithFilterToAntiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveJoinWithFilterToAntiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Matched HiveAntiJoinRule\");\n+\n+    if (join.getCondition().isAlwaysTrue()) {\n+      return;\n+    }\n+\n+    //We support conversion from left outer join only.\n+    if (join.getJoinType() != JoinRelType.LEFT) {\n+      return;\n+    }\n+\n+    assert (filter != null);\n+\n+    List<RexNode> aboveFilters = RelOptUtil.conjunctions(filter.getCondition());\n+    boolean hasIsNull = false;\n+\n+    // Get all filter condition and check if any of them is a \"is null\" kind.\n+    for (RexNode filterNode : aboveFilters) {\n+      if (filterNode.getKind() == SqlKind.IS_NULL &&\n+              isFilterFromRightSide(join, filterNode, join.getJoinType())) {\n+        hasIsNull = true;\n+        break;\n+      }\n+    }\n+\n+    // Is null should be on a key from right side of the join.\n+    if (!hasIsNull) {\n+      return;\n+    }\n+\n+    // Build anti join with same left, right child and condition as original left outer join.\n+    Join anti = join.copy(join.getTraitSet(), join.getCondition(),\n+            join.getLeft(), join.getRight(), JoinRelType.ANTI, false);\n+\n+    //TODO : Do we really need it\n+    call.getPlanner().onCopy(join, anti);\n+\n+    RelNode newProject = getNewProjectNode(project, anti);\n+    if (newProject != null) {\n+      call.getPlanner().onCopy(project, newProject);\n+      call.transformTo(newProject);\n+    }\n+  }\n+\n+  protected RelNode getNewProjectNode(Project oldProject, Join newJoin) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk4NDE3MQ==", "bodyText": "I didn't find any such utility method, so added this into HiveCalciteUtil and used.", "url": "https://github.com/apache/hive/pull/1147#discussion_r459984171", "createdAt": "2020-07-24T10:55:40Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveJoinWithFilterToAntiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveJoinWithFilterToAntiJoinRule.class);\n+  public static final HiveJoinWithFilterToAntiJoinRule INSTANCE = new HiveJoinWithFilterToAntiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveJoinWithFilterToAntiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Matched HiveAntiJoinRule\");\n+\n+    if (join.getCondition().isAlwaysTrue()) {\n+      return;\n+    }\n+\n+    //We support conversion from left outer join only.\n+    if (join.getJoinType() != JoinRelType.LEFT) {\n+      return;\n+    }\n+\n+    assert (filter != null);\n+\n+    List<RexNode> aboveFilters = RelOptUtil.conjunctions(filter.getCondition());\n+    boolean hasIsNull = false;\n+\n+    // Get all filter condition and check if any of them is a \"is null\" kind.\n+    for (RexNode filterNode : aboveFilters) {\n+      if (filterNode.getKind() == SqlKind.IS_NULL &&\n+              isFilterFromRightSide(join, filterNode, join.getJoinType())) {\n+        hasIsNull = true;\n+        break;\n+      }\n+    }\n+\n+    // Is null should be on a key from right side of the join.\n+    if (!hasIsNull) {\n+      return;\n+    }\n+\n+    // Build anti join with same left, right child and condition as original left outer join.\n+    Join anti = join.copy(join.getTraitSet(), join.getCondition(),\n+            join.getLeft(), join.getRight(), JoinRelType.ANTI, false);\n+\n+    //TODO : Do we really need it\n+    call.getPlanner().onCopy(join, anti);\n+\n+    RelNode newProject = getNewProjectNode(project, anti);\n+    if (newProject != null) {\n+      call.getPlanner().onCopy(project, newProject);\n+      call.transformTo(newProject);\n+    }\n+  }\n+\n+  protected RelNode getNewProjectNode(Project oldProject, Join newJoin) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3MjEyMg=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTA0MDE2OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveRemoveGBYSemiJoinRule.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMTowMjoxNFrOG1PADg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMToyOTozN1rOG3LnuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3MzQ4Ng==", "bodyText": "nit. space before =", "url": "https://github.com/apache/hive/pull/1147#discussion_r458473486", "createdAt": "2020-07-22T01:02:14Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveRemoveGBYSemiJoinRule.java", "diffHunk": "@@ -41,17 +41,19 @@\n \n   public HiveRemoveGBYSemiJoinRule() {\n     super(\n-        operand(HiveSemiJoin.class,\n+        operand(Join.class,\n             some(\n                 operand(RelNode.class, any()),\n                 operand(Aggregate.class, any()))),\n         HiveRelFactories.HIVE_BUILDER, \"HiveRemoveGBYSemiJoinRule\");\n   }\n \n   @Override public void onMatch(RelOptRuleCall call) {\n-    final HiveSemiJoin semijoin= call.rel(0);\n+    final Join join= call.rel(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUxNTI1Nw==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460515257", "createdAt": "2020-07-26T11:29:37Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveRemoveGBYSemiJoinRule.java", "diffHunk": "@@ -41,17 +41,19 @@\n \n   public HiveRemoveGBYSemiJoinRule() {\n     super(\n-        operand(HiveSemiJoin.class,\n+        operand(Join.class,\n             some(\n                 operand(RelNode.class, any()),\n                 operand(Aggregate.class, any()))),\n         HiveRelFactories.HIVE_BUILDER, \"HiveRemoveGBYSemiJoinRule\");\n   }\n \n   @Override public void onMatch(RelOptRuleCall call) {\n-    final HiveSemiJoin semijoin= call.rel(0);\n+    final Join join= call.rel(0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3MzQ4Ng=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTA0OTczOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSubQueryRemoveRule.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMTowNzo1MVrOG1PFig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQxMTowMDo1OFrOG2rVmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3NDg5MA==", "bodyText": "This does not seem to be the case anymore? Should we remove? Otherwise, please create a follow-up JIRA.", "url": "https://github.com/apache/hive/pull/1147#discussion_r458474890", "createdAt": "2020-07-22T01:07:51Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSubQueryRemoveRule.java", "diffHunk": "@@ -414,6 +416,13 @@ private RexNode rewriteInExists(RexSubQuery e, Set<CorrelationId> variablesSet,\n       // null keys we do not need to generate count(*), count(c)\n       if (e.getKind() == SqlKind.EXISTS) {\n         logic = RelOptUtil.Logic.TRUE_FALSE;\n+        if (conf.getBoolVar(HiveConf.ConfVars.HIVE_CONVERT_ANTI_JOIN)) {\n+          //TODO : As of now anti join is first converted to left outer join", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk4NjMyOQ==", "bodyText": "Now also the conversion is not done. The code is present but actual conversion is not done and logic is still TRUE_FALSE. For the code to be effective , the logic should be changed to FALSE. I have not done it yet, as it was causing some change in plan which i could not judge to be expected or not. Anyways i have created a JIRA to track this.\nhttps://issues.apache.org/jira/browse/HIVE-23928", "url": "https://github.com/apache/hive/pull/1147#discussion_r459986329", "createdAt": "2020-07-24T11:00:58Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSubQueryRemoveRule.java", "diffHunk": "@@ -414,6 +416,13 @@ private RexNode rewriteInExists(RexSubQuery e, Set<CorrelationId> variablesSet,\n       // null keys we do not need to generate count(*), count(c)\n       if (e.getKind() == SqlKind.EXISTS) {\n         logic = RelOptUtil.Logic.TRUE_FALSE;\n+        if (conf.getBoolVar(HiveConf.ConfVars.HIVE_CONVERT_ANTI_JOIN)) {\n+          //TODO : As of now anti join is first converted to left outer join", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3NDg5MA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTA4NDUwOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdDistinctRowCount.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMToyNjoxNlrOG1PZFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMTo1OToyOVrOG3L0Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3OTg5NA==", "bodyText": "Did you verify whether super method handles ANTI join? Not a blocker but we may need to create a follow-up JIRA if it does not.", "url": "https://github.com/apache/hive/pull/1147#discussion_r458479894", "createdAt": "2020-07-22T01:26:16Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdDistinctRowCount.java", "diffHunk": "@@ -79,6 +80,11 @@ public Double getDistinctRowCount(HiveSemiJoin rel, RelMetadataQuery mq, Immutab\n     return super.getDistinctRowCount(rel, mq, groupKey, predicate);\n   }\n \n+  public Double getDistinctRowCount(HiveAntiJoin rel, RelMetadataQuery mq, ImmutableBitSet groupKey,\n+                                    RexNode predicate) {\n+    return super.getDistinctRowCount(rel, mq, groupKey, predicate);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk4ODIxNA==", "bodyText": "calcite 21 does not support distinct calculation for Anti join.\nif (join.isSemiJoin()) {\nreturn getSemiJoinDistinctRowCount(join, mq, groupKey, predicate);\n} else {\nBuilder leftMask = ImmutableBitSet.builder();\nI think these rules will not get triggered as of now for Anti join as i am not converting the not-exists to anti join. As of now all these rules will be applied on left outer and then we convert the left outer to anti join.\nI", "url": "https://github.com/apache/hive/pull/1147#discussion_r459988214", "createdAt": "2020-07-24T11:05:50Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdDistinctRowCount.java", "diffHunk": "@@ -79,6 +80,11 @@ public Double getDistinctRowCount(HiveSemiJoin rel, RelMetadataQuery mq, Immutab\n     return super.getDistinctRowCount(rel, mq, groupKey, predicate);\n   }\n \n+  public Double getDistinctRowCount(HiveAntiJoin rel, RelMetadataQuery mq, ImmutableBitSet groupKey,\n+                                    RexNode predicate) {\n+    return super.getDistinctRowCount(rel, mq, groupKey, predicate);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3OTg5NA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUxODQ1NA==", "bodyText": "https://issues.apache.org/jira/browse/HIVE-23933", "url": "https://github.com/apache/hive/pull/1147#discussion_r460518454", "createdAt": "2020-07-26T11:59:29Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdDistinctRowCount.java", "diffHunk": "@@ -79,6 +80,11 @@ public Double getDistinctRowCount(HiveSemiJoin rel, RelMetadataQuery mq, Immutab\n     return super.getDistinctRowCount(rel, mq, groupKey, predicate);\n   }\n \n+  public Double getDistinctRowCount(HiveAntiJoin rel, RelMetadataQuery mq, ImmutableBitSet groupKey,\n+                                    RexNode predicate) {\n+    return super.getDistinctRowCount(rel, mq, groupKey, predicate);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ3OTg5NA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTEwNTI2OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdRowCount.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMTozNzozMlrOG1PlEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNlQxMTo1OToxMlrOG3L0Cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ4Mjk2Mg==", "bodyText": "Logic for antijoin would be slightly different?\nreturn pkfk.fkInfo.rowCount * (1-selectivity);\n\nYou had similar change below. Does that make sense?\nIn addition, does super.getRowCount(rel, mq) handle antijoin correctly?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458482962", "createdAt": "2020-07-22T01:37:32Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdRowCount.java", "diffHunk": "@@ -118,6 +119,15 @@ public Double getRowCount(HiveJoin join, RelMetadataQuery mq) {\n   }\n \n   public Double getRowCount(HiveSemiJoin rel, RelMetadataQuery mq) {\n+    return getRowCountInt(rel, mq);\n+  }\n+\n+  public Double getRowCount(HiveAntiJoin rel, RelMetadataQuery mq) {\n+    return getRowCountInt(rel, mq);\n+  }\n+\n+  private Double getRowCountInt(Join rel, RelMetadataQuery mq) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUxNTY5NQ==", "bodyText": "Yes done.", "url": "https://github.com/apache/hive/pull/1147#discussion_r460515695", "createdAt": "2020-07-26T11:34:07Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdRowCount.java", "diffHunk": "@@ -118,6 +119,15 @@ public Double getRowCount(HiveJoin join, RelMetadataQuery mq) {\n   }\n \n   public Double getRowCount(HiveSemiJoin rel, RelMetadataQuery mq) {\n+    return getRowCountInt(rel, mq);\n+  }\n+\n+  public Double getRowCount(HiveAntiJoin rel, RelMetadataQuery mq) {\n+    return getRowCountInt(rel, mq);\n+  }\n+\n+  private Double getRowCountInt(Join rel, RelMetadataQuery mq) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ4Mjk2Mg=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDUxODQxMQ==", "bodyText": "super.getRowCount(rel, mq) does not support Anti join. I think we need to handle it.\nhttps://issues.apache.org/jira/browse/HIVE-23933", "url": "https://github.com/apache/hive/pull/1147#discussion_r460518411", "createdAt": "2020-07-26T11:59:12Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdRowCount.java", "diffHunk": "@@ -118,6 +119,15 @@ public Double getRowCount(HiveJoin join, RelMetadataQuery mq) {\n   }\n \n   public Double getRowCount(HiveSemiJoin rel, RelMetadataQuery mq) {\n+    return getRowCountInt(rel, mq);\n+  }\n+\n+  public Double getRowCount(HiveAntiJoin rel, RelMetadataQuery mq) {\n+    return getRowCountInt(rel, mq);\n+  }\n+\n+  private Double getRowCountInt(Join rel, RelMetadataQuery mq) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ4Mjk2Mg=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTE2ODYzOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdSelectivity.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMjoxMjoyOVrOG1QKNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQxMTo0MjowMlrOG2sT1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ5MjQ3MA==", "bodyText": "We can probably remove j instanceof HiveJoin?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458492470", "createdAt": "2020-07-22T02:12:29Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdSelectivity.java", "diffHunk": "@@ -142,7 +146,7 @@ private Double computeInnerJoinSelectivity(Join j, RelMetadataQuery mq, RexNode\n         ndvEstimate = exponentialBackoff(peLst, colStatMap);\n       }\n \n-      if (j.isSemiJoin()) {\n+      if (j.isSemiJoin() || (j instanceof HiveJoin && j.getJoinType().equals(JoinRelType.ANTI))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDAwMjI2MQ==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460002261", "createdAt": "2020-07-24T11:42:02Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdSelectivity.java", "diffHunk": "@@ -142,7 +146,7 @@ private Double computeInnerJoinSelectivity(Join j, RelMetadataQuery mq, RexNode\n         ndvEstimate = exponentialBackoff(peLst, colStatMap);\n       }\n \n-      if (j.isSemiJoin()) {\n+      if (j.isSemiJoin() || (j instanceof HiveJoin && j.getJoinType().equals(JoinRelType.ANTI))) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ5MjQ3MA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTE4NDU1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMjoyMTozMlrOG1QTtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQxMTo0MToxNFrOG2sShQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ5NDkwMQ==", "bodyText": "Shouldn't we use result=0?\nIf we estimated that the result of the inner join is going to be larger than the left input, it seems we should assume that nothing is going to come out of the anti join?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458494901", "createdAt": "2020-07-22T02:21:32Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java", "diffHunk": "@@ -2606,6 +2607,17 @@ private long computeFinalRowCount(List<Long> rowCountParents, long interimRowCou\n           // max # of rows = rows from left side\n           result = Math.min(rowCountParents.get(joinCond.getLeft()), result);\n           break;\n+        case JoinDesc.ANTI_JOIN:\n+          long leftRowCount = rowCountParents.get(joinCond.getLeft());\n+          if (leftRowCount < result) {\n+            // Ideally the inner join count should be less than the left row count. but if its not calculated\n+            // properly then we can assume whole of left table will be selected.\n+            result = leftRowCount;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDAwMTkyNQ==", "bodyText": "This case will come if the stats are not proper. So to be on safer side, i assume that all rows from the left side will be projected. That is the max value. If set it to 0, it should not trigger some re-write, assuming the join result is empty.", "url": "https://github.com/apache/hive/pull/1147#discussion_r460001925", "createdAt": "2020-07-24T11:41:14Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java", "diffHunk": "@@ -2606,6 +2607,17 @@ private long computeFinalRowCount(List<Long> rowCountParents, long interimRowCou\n           // max # of rows = rows from left side\n           result = Math.min(rowCountParents.get(joinCond.getLeft()), result);\n           break;\n+        case JoinDesc.ANTI_JOIN:\n+          long leftRowCount = rowCountParents.get(joinCond.getLeft());\n+          if (leftRowCount < result) {\n+            // Ideally the inner join count should be less than the left row count. but if its not calculated\n+            // properly then we can assume whole of left table will be selected.\n+            result = leftRowCount;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ5NDkwMQ=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTE5MTM0OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMjoyNToxNFrOG1QXsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQxMTozOToyNVrOG2sP1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ5NTkyMQ==", "bodyText": "Can we make this block part of the applyPostJoinOrderingTransform, in particular before/after steps 3 and 4 in that method (those steps relate to semijoin conversion)?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458495921", "createdAt": "2020-07-22T02:25:14Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java", "diffHunk": "@@ -1901,6 +1905,11 @@ public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlu\n       calcitePreCboPlan = applyPreJoinOrderingTransforms(calciteGenPlan,\n           mdProvider.getMetadataProvider(), executorProvider);\n \n+      if (conf.getBoolVar(ConfVars.HIVE_CONVERT_ANTI_JOIN)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDAwMTIzNg==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r460001236", "createdAt": "2020-07-24T11:39:25Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java", "diffHunk": "@@ -1901,6 +1905,11 @@ public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlu\n       calcitePreCboPlan = applyPreJoinOrderingTransforms(calciteGenPlan,\n           mdProvider.getMetadataProvider(), executorProvider);\n \n+      if (conf.getBoolVar(ConfVars.HIVE_CONVERT_ANTI_JOIN)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ5NTkyMQ=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTI0ODcxOnYy", "diffSide": "RIGHT", "path": "ql/src/test/results/clientpositive/perf/tez/cbo_query16_anti_join.q.out", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMjo1Nzo1NFrOG1Q5WQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOFQwMToxOToxNFrOG9sZMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUwNDUzNw==", "bodyText": "Is this correct? Note that the condition in the SemiJoin below and the Antijoin have a common conjunct =($4, $14). The semijoin implies that =($4, $14) is true for all rows that passed through it. That means that the condition in the antijoin always evaluates to true, which means that the query would not produce any output rows?\nIf it is not correct, we need to fix it. Nevertheless, we should create a follow-up JIRA to implement a rule that rewrites this cases so we do not execute the subplan rooted at the antijoin.", "url": "https://github.com/apache/hive/pull/1147#discussion_r458504537", "createdAt": "2020-07-22T02:57:54Z", "author": {"login": "jcamachor"}, "path": "ql/src/test/results/clientpositive/perf/tez/cbo_query16_anti_join.q.out", "diffHunk": "@@ -0,0 +1,99 @@\n+PREHOOK: query: explain cbo\n+select\n+   count(distinct cs_order_number) as `order count`\n+  ,sum(cs_ext_ship_cost) as `total shipping cost`\n+  ,sum(cs_net_profit) as `total net profit`\n+from\n+   catalog_sales cs1\n+  ,date_dim\n+  ,customer_address\n+  ,call_center\n+where\n+    d_date between '2001-4-01' and\n+           (cast('2001-4-01' as date) + 60 days)\n+and cs1.cs_ship_date_sk = d_date_sk\n+and cs1.cs_ship_addr_sk = ca_address_sk\n+and ca_state = 'NY'\n+and cs1.cs_call_center_sk = cc_call_center_sk\n+and cc_county in ('Ziebach County','Levy County','Huron County','Franklin Parish',\n+                  'Daviess County'\n+)\n+and exists (select *\n+            from catalog_sales cs2\n+            where cs1.cs_order_number = cs2.cs_order_number\n+              and cs1.cs_warehouse_sk <> cs2.cs_warehouse_sk)\n+and not exists(select *\n+               from catalog_returns cr1\n+               where cs1.cs_order_number = cr1.cr_order_number)\n+order by count(distinct cs_order_number)\n+limit 100\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@call_center\n+PREHOOK: Input: default@catalog_returns\n+PREHOOK: Input: default@catalog_sales\n+PREHOOK: Input: default@customer_address\n+PREHOOK: Input: default@date_dim\n+PREHOOK: Output: hdfs://### HDFS PATH ###\n+POSTHOOK: query: explain cbo\n+select\n+   count(distinct cs_order_number) as `order count`\n+  ,sum(cs_ext_ship_cost) as `total shipping cost`\n+  ,sum(cs_net_profit) as `total net profit`\n+from\n+   catalog_sales cs1\n+  ,date_dim\n+  ,customer_address\n+  ,call_center\n+where\n+    d_date between '2001-4-01' and\n+           (cast('2001-4-01' as date) + 60 days)\n+and cs1.cs_ship_date_sk = d_date_sk\n+and cs1.cs_ship_addr_sk = ca_address_sk\n+and ca_state = 'NY'\n+and cs1.cs_call_center_sk = cc_call_center_sk\n+and cc_county in ('Ziebach County','Levy County','Huron County','Franklin Parish',\n+                  'Daviess County'\n+)\n+and exists (select *\n+            from catalog_sales cs2\n+            where cs1.cs_order_number = cs2.cs_order_number\n+              and cs1.cs_warehouse_sk <> cs2.cs_warehouse_sk)\n+and not exists(select *\n+               from catalog_returns cr1\n+               where cs1.cs_order_number = cr1.cr_order_number)\n+order by count(distinct cs_order_number)\n+limit 100\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@call_center\n+POSTHOOK: Input: default@catalog_returns\n+POSTHOOK: Input: default@catalog_sales\n+POSTHOOK: Input: default@customer_address\n+POSTHOOK: Input: default@date_dim\n+POSTHOOK: Output: hdfs://### HDFS PATH ###\n+CBO PLAN:\n+HiveAggregate(group=[{}], agg#0=[count(DISTINCT $4)], agg#1=[sum($5)], agg#2=[sum($6)])\n+  HiveJoin(condition=[=($4, $14)], joinType=[anti], algorithm=[none], cost=[not available])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk5MDQ5NQ==", "bodyText": "I think, it's not a problem. The filed index are for different input. So even though the number is same the condition is different. Even without anti join, the condition is same.\nHiveFilter(condition=[IS NULL($13)])\nHiveJoin(condition=[=($4, $14)], joinType=[left], algorithm=[none], cost=[not available])\nHiveSemiJoin(condition=[AND(<>($3, $13), =($4, $14))], joinType=[semi])", "url": "https://github.com/apache/hive/pull/1147#discussion_r459990495", "createdAt": "2020-07-24T11:11:51Z", "author": {"login": "maheshk114"}, "path": "ql/src/test/results/clientpositive/perf/tez/cbo_query16_anti_join.q.out", "diffHunk": "@@ -0,0 +1,99 @@\n+PREHOOK: query: explain cbo\n+select\n+   count(distinct cs_order_number) as `order count`\n+  ,sum(cs_ext_ship_cost) as `total shipping cost`\n+  ,sum(cs_net_profit) as `total net profit`\n+from\n+   catalog_sales cs1\n+  ,date_dim\n+  ,customer_address\n+  ,call_center\n+where\n+    d_date between '2001-4-01' and\n+           (cast('2001-4-01' as date) + 60 days)\n+and cs1.cs_ship_date_sk = d_date_sk\n+and cs1.cs_ship_addr_sk = ca_address_sk\n+and ca_state = 'NY'\n+and cs1.cs_call_center_sk = cc_call_center_sk\n+and cc_county in ('Ziebach County','Levy County','Huron County','Franklin Parish',\n+                  'Daviess County'\n+)\n+and exists (select *\n+            from catalog_sales cs2\n+            where cs1.cs_order_number = cs2.cs_order_number\n+              and cs1.cs_warehouse_sk <> cs2.cs_warehouse_sk)\n+and not exists(select *\n+               from catalog_returns cr1\n+               where cs1.cs_order_number = cr1.cr_order_number)\n+order by count(distinct cs_order_number)\n+limit 100\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@call_center\n+PREHOOK: Input: default@catalog_returns\n+PREHOOK: Input: default@catalog_sales\n+PREHOOK: Input: default@customer_address\n+PREHOOK: Input: default@date_dim\n+PREHOOK: Output: hdfs://### HDFS PATH ###\n+POSTHOOK: query: explain cbo\n+select\n+   count(distinct cs_order_number) as `order count`\n+  ,sum(cs_ext_ship_cost) as `total shipping cost`\n+  ,sum(cs_net_profit) as `total net profit`\n+from\n+   catalog_sales cs1\n+  ,date_dim\n+  ,customer_address\n+  ,call_center\n+where\n+    d_date between '2001-4-01' and\n+           (cast('2001-4-01' as date) + 60 days)\n+and cs1.cs_ship_date_sk = d_date_sk\n+and cs1.cs_ship_addr_sk = ca_address_sk\n+and ca_state = 'NY'\n+and cs1.cs_call_center_sk = cc_call_center_sk\n+and cc_county in ('Ziebach County','Levy County','Huron County','Franklin Parish',\n+                  'Daviess County'\n+)\n+and exists (select *\n+            from catalog_sales cs2\n+            where cs1.cs_order_number = cs2.cs_order_number\n+              and cs1.cs_warehouse_sk <> cs2.cs_warehouse_sk)\n+and not exists(select *\n+               from catalog_returns cr1\n+               where cs1.cs_order_number = cr1.cr_order_number)\n+order by count(distinct cs_order_number)\n+limit 100\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@call_center\n+POSTHOOK: Input: default@catalog_returns\n+POSTHOOK: Input: default@catalog_sales\n+POSTHOOK: Input: default@customer_address\n+POSTHOOK: Input: default@date_dim\n+POSTHOOK: Output: hdfs://### HDFS PATH ###\n+CBO PLAN:\n+HiveAggregate(group=[{}], agg#0=[count(DISTINCT $4)], agg#1=[sum($5)], agg#2=[sum($6)])\n+  HiveJoin(condition=[=($4, $14)], joinType=[anti], algorithm=[none], cost=[not available])", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUwNDUzNw=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM0MzY2NA==", "bodyText": "Do we have a JIRA to explore this optimization?", "url": "https://github.com/apache/hive/pull/1147#discussion_r467343664", "createdAt": "2020-08-08T01:19:14Z", "author": {"login": "jcamachor"}, "path": "ql/src/test/results/clientpositive/perf/tez/cbo_query16_anti_join.q.out", "diffHunk": "@@ -0,0 +1,99 @@\n+PREHOOK: query: explain cbo\n+select\n+   count(distinct cs_order_number) as `order count`\n+  ,sum(cs_ext_ship_cost) as `total shipping cost`\n+  ,sum(cs_net_profit) as `total net profit`\n+from\n+   catalog_sales cs1\n+  ,date_dim\n+  ,customer_address\n+  ,call_center\n+where\n+    d_date between '2001-4-01' and\n+           (cast('2001-4-01' as date) + 60 days)\n+and cs1.cs_ship_date_sk = d_date_sk\n+and cs1.cs_ship_addr_sk = ca_address_sk\n+and ca_state = 'NY'\n+and cs1.cs_call_center_sk = cc_call_center_sk\n+and cc_county in ('Ziebach County','Levy County','Huron County','Franklin Parish',\n+                  'Daviess County'\n+)\n+and exists (select *\n+            from catalog_sales cs2\n+            where cs1.cs_order_number = cs2.cs_order_number\n+              and cs1.cs_warehouse_sk <> cs2.cs_warehouse_sk)\n+and not exists(select *\n+               from catalog_returns cr1\n+               where cs1.cs_order_number = cr1.cr_order_number)\n+order by count(distinct cs_order_number)\n+limit 100\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@call_center\n+PREHOOK: Input: default@catalog_returns\n+PREHOOK: Input: default@catalog_sales\n+PREHOOK: Input: default@customer_address\n+PREHOOK: Input: default@date_dim\n+PREHOOK: Output: hdfs://### HDFS PATH ###\n+POSTHOOK: query: explain cbo\n+select\n+   count(distinct cs_order_number) as `order count`\n+  ,sum(cs_ext_ship_cost) as `total shipping cost`\n+  ,sum(cs_net_profit) as `total net profit`\n+from\n+   catalog_sales cs1\n+  ,date_dim\n+  ,customer_address\n+  ,call_center\n+where\n+    d_date between '2001-4-01' and\n+           (cast('2001-4-01' as date) + 60 days)\n+and cs1.cs_ship_date_sk = d_date_sk\n+and cs1.cs_ship_addr_sk = ca_address_sk\n+and ca_state = 'NY'\n+and cs1.cs_call_center_sk = cc_call_center_sk\n+and cc_county in ('Ziebach County','Levy County','Huron County','Franklin Parish',\n+                  'Daviess County'\n+)\n+and exists (select *\n+            from catalog_sales cs2\n+            where cs1.cs_order_number = cs2.cs_order_number\n+              and cs1.cs_warehouse_sk <> cs2.cs_warehouse_sk)\n+and not exists(select *\n+               from catalog_returns cr1\n+               where cs1.cs_order_number = cr1.cr_order_number)\n+order by count(distinct cs_order_number)\n+limit 100\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@call_center\n+POSTHOOK: Input: default@catalog_returns\n+POSTHOOK: Input: default@catalog_sales\n+POSTHOOK: Input: default@customer_address\n+POSTHOOK: Input: default@date_dim\n+POSTHOOK: Output: hdfs://### HDFS PATH ###\n+CBO PLAN:\n+HiveAggregate(group=[{}], agg#0=[count(DISTINCT $4)], agg#1=[sum($5)], agg#2=[sum($6)])\n+  HiveJoin(condition=[=($4, $14)], joinType=[anti], algorithm=[none], cost=[not available])", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUwNDUzNw=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTI2MjYxOnYy", "diffSide": "RIGHT", "path": "ql/src/test/results/clientpositive/perf/tez/constraints/cbo_query94_anti_join.q.out", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMzowNTo1NlrOG1RBYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwNToyMjoyNFrOG9M2yQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUwNjU5NA==", "bodyText": "Same as q16.", "url": "https://github.com/apache/hive/pull/1147#discussion_r458506594", "createdAt": "2020-07-22T03:05:56Z", "author": {"login": "jcamachor"}, "path": "ql/src/test/results/clientpositive/perf/tez/constraints/cbo_query94_anti_join.q.out", "diffHunk": "@@ -0,0 +1,94 @@\n+PREHOOK: query: explain cbo\n+select  \n+   count(distinct ws_order_number) as `order count`\n+  ,sum(ws_ext_ship_cost) as `total shipping cost`\n+  ,sum(ws_net_profit) as `total net profit`\n+from\n+   web_sales ws1\n+  ,date_dim\n+  ,customer_address\n+  ,web_site\n+where\n+    d_date between '1999-5-01' and \n+           (cast('1999-5-01' as date) + 60 days)\n+and ws1.ws_ship_date_sk = d_date_sk\n+and ws1.ws_ship_addr_sk = ca_address_sk\n+and ca_state = 'TX'\n+and ws1.ws_web_site_sk = web_site_sk\n+and web_company_name = 'pri'\n+and exists (select *\n+            from web_sales ws2\n+            where ws1.ws_order_number = ws2.ws_order_number\n+              and ws1.ws_warehouse_sk <> ws2.ws_warehouse_sk)\n+and not exists(select *\n+               from web_returns wr1\n+               where ws1.ws_order_number = wr1.wr_order_number)\n+order by count(distinct ws_order_number)\n+limit 100\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@customer_address\n+PREHOOK: Input: default@date_dim\n+PREHOOK: Input: default@web_returns\n+PREHOOK: Input: default@web_sales\n+PREHOOK: Input: default@web_site\n+PREHOOK: Output: hdfs://### HDFS PATH ###\n+POSTHOOK: query: explain cbo\n+select  \n+   count(distinct ws_order_number) as `order count`\n+  ,sum(ws_ext_ship_cost) as `total shipping cost`\n+  ,sum(ws_net_profit) as `total net profit`\n+from\n+   web_sales ws1\n+  ,date_dim\n+  ,customer_address\n+  ,web_site\n+where\n+    d_date between '1999-5-01' and \n+           (cast('1999-5-01' as date) + 60 days)\n+and ws1.ws_ship_date_sk = d_date_sk\n+and ws1.ws_ship_addr_sk = ca_address_sk\n+and ca_state = 'TX'\n+and ws1.ws_web_site_sk = web_site_sk\n+and web_company_name = 'pri'\n+and exists (select *\n+            from web_sales ws2\n+            where ws1.ws_order_number = ws2.ws_order_number\n+              and ws1.ws_warehouse_sk <> ws2.ws_warehouse_sk)\n+and not exists(select *\n+               from web_returns wr1\n+               where ws1.ws_order_number = wr1.wr_order_number)\n+order by count(distinct ws_order_number)\n+limit 100\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@customer_address\n+POSTHOOK: Input: default@date_dim\n+POSTHOOK: Input: default@web_returns\n+POSTHOOK: Input: default@web_sales\n+POSTHOOK: Input: default@web_site\n+POSTHOOK: Output: hdfs://### HDFS PATH ###\n+CBO PLAN:\n+HiveAggregate(group=[{}], agg#0=[count(DISTINCT $4)], agg#1=[sum($5)], agg#2=[sum($6)])\n+  HiveJoin(condition=[=($4, $14)], joinType=[anti], algorithm=[none], cost=[not available])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgyNjk1Mw==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r466826953", "createdAt": "2020-08-07T05:22:24Z", "author": {"login": "maheshk114"}, "path": "ql/src/test/results/clientpositive/perf/tez/constraints/cbo_query94_anti_join.q.out", "diffHunk": "@@ -0,0 +1,94 @@\n+PREHOOK: query: explain cbo\n+select  \n+   count(distinct ws_order_number) as `order count`\n+  ,sum(ws_ext_ship_cost) as `total shipping cost`\n+  ,sum(ws_net_profit) as `total net profit`\n+from\n+   web_sales ws1\n+  ,date_dim\n+  ,customer_address\n+  ,web_site\n+where\n+    d_date between '1999-5-01' and \n+           (cast('1999-5-01' as date) + 60 days)\n+and ws1.ws_ship_date_sk = d_date_sk\n+and ws1.ws_ship_addr_sk = ca_address_sk\n+and ca_state = 'TX'\n+and ws1.ws_web_site_sk = web_site_sk\n+and web_company_name = 'pri'\n+and exists (select *\n+            from web_sales ws2\n+            where ws1.ws_order_number = ws2.ws_order_number\n+              and ws1.ws_warehouse_sk <> ws2.ws_warehouse_sk)\n+and not exists(select *\n+               from web_returns wr1\n+               where ws1.ws_order_number = wr1.wr_order_number)\n+order by count(distinct ws_order_number)\n+limit 100\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@customer_address\n+PREHOOK: Input: default@date_dim\n+PREHOOK: Input: default@web_returns\n+PREHOOK: Input: default@web_sales\n+PREHOOK: Input: default@web_site\n+PREHOOK: Output: hdfs://### HDFS PATH ###\n+POSTHOOK: query: explain cbo\n+select  \n+   count(distinct ws_order_number) as `order count`\n+  ,sum(ws_ext_ship_cost) as `total shipping cost`\n+  ,sum(ws_net_profit) as `total net profit`\n+from\n+   web_sales ws1\n+  ,date_dim\n+  ,customer_address\n+  ,web_site\n+where\n+    d_date between '1999-5-01' and \n+           (cast('1999-5-01' as date) + 60 days)\n+and ws1.ws_ship_date_sk = d_date_sk\n+and ws1.ws_ship_addr_sk = ca_address_sk\n+and ca_state = 'TX'\n+and ws1.ws_web_site_sk = web_site_sk\n+and web_company_name = 'pri'\n+and exists (select *\n+            from web_sales ws2\n+            where ws1.ws_order_number = ws2.ws_order_number\n+              and ws1.ws_warehouse_sk <> ws2.ws_warehouse_sk)\n+and not exists(select *\n+               from web_returns wr1\n+               where ws1.ws_order_number = wr1.wr_order_number)\n+order by count(distinct ws_order_number)\n+limit 100\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@customer_address\n+POSTHOOK: Input: default@date_dim\n+POSTHOOK: Input: default@web_returns\n+POSTHOOK: Input: default@web_sales\n+POSTHOOK: Input: default@web_site\n+POSTHOOK: Output: hdfs://### HDFS PATH ###\n+CBO PLAN:\n+HiveAggregate(group=[{}], agg#0=[count(DISTINCT $4)], agg#1=[sum($5)], agg#2=[sum($6)])\n+  HiveJoin(condition=[=($4, $14)], joinType=[anti], algorithm=[none], cost=[not available])", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUwNjU5NA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTI3OTI5OnYy", "diffSide": "RIGHT", "path": "ql/src/test/results/clientpositive/perf/tez/cbo_query16_anti_join.q.out", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMzoxNjowMlrOG1RLIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQxMTozMzoyOVrOG2sHCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUwOTA5MA==", "bodyText": "Confused by something here, though it may be OK. It seems we have a HiveSemiJoin operator that is used when joinType is SEMI. You also created a HiveAntiJoin operator but it is not used for joinType is ANTI (at least in this plan). What is the reason for that? Are we creating the operator correctly?", "url": "https://github.com/apache/hive/pull/1147#discussion_r458509090", "createdAt": "2020-07-22T03:16:02Z", "author": {"login": "jcamachor"}, "path": "ql/src/test/results/clientpositive/perf/tez/cbo_query16_anti_join.q.out", "diffHunk": "@@ -0,0 +1,99 @@\n+PREHOOK: query: explain cbo\n+select\n+   count(distinct cs_order_number) as `order count`\n+  ,sum(cs_ext_ship_cost) as `total shipping cost`\n+  ,sum(cs_net_profit) as `total net profit`\n+from\n+   catalog_sales cs1\n+  ,date_dim\n+  ,customer_address\n+  ,call_center\n+where\n+    d_date between '2001-4-01' and\n+           (cast('2001-4-01' as date) + 60 days)\n+and cs1.cs_ship_date_sk = d_date_sk\n+and cs1.cs_ship_addr_sk = ca_address_sk\n+and ca_state = 'NY'\n+and cs1.cs_call_center_sk = cc_call_center_sk\n+and cc_county in ('Ziebach County','Levy County','Huron County','Franklin Parish',\n+                  'Daviess County'\n+)\n+and exists (select *\n+            from catalog_sales cs2\n+            where cs1.cs_order_number = cs2.cs_order_number\n+              and cs1.cs_warehouse_sk <> cs2.cs_warehouse_sk)\n+and not exists(select *\n+               from catalog_returns cr1\n+               where cs1.cs_order_number = cr1.cr_order_number)\n+order by count(distinct cs_order_number)\n+limit 100\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@call_center\n+PREHOOK: Input: default@catalog_returns\n+PREHOOK: Input: default@catalog_sales\n+PREHOOK: Input: default@customer_address\n+PREHOOK: Input: default@date_dim\n+PREHOOK: Output: hdfs://### HDFS PATH ###\n+POSTHOOK: query: explain cbo\n+select\n+   count(distinct cs_order_number) as `order count`\n+  ,sum(cs_ext_ship_cost) as `total shipping cost`\n+  ,sum(cs_net_profit) as `total net profit`\n+from\n+   catalog_sales cs1\n+  ,date_dim\n+  ,customer_address\n+  ,call_center\n+where\n+    d_date between '2001-4-01' and\n+           (cast('2001-4-01' as date) + 60 days)\n+and cs1.cs_ship_date_sk = d_date_sk\n+and cs1.cs_ship_addr_sk = ca_address_sk\n+and ca_state = 'NY'\n+and cs1.cs_call_center_sk = cc_call_center_sk\n+and cc_county in ('Ziebach County','Levy County','Huron County','Franklin Parish',\n+                  'Daviess County'\n+)\n+and exists (select *\n+            from catalog_sales cs2\n+            where cs1.cs_order_number = cs2.cs_order_number\n+              and cs1.cs_warehouse_sk <> cs2.cs_warehouse_sk)\n+and not exists(select *\n+               from catalog_returns cr1\n+               where cs1.cs_order_number = cr1.cr_order_number)\n+order by count(distinct cs_order_number)\n+limit 100\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@call_center\n+POSTHOOK: Input: default@catalog_returns\n+POSTHOOK: Input: default@catalog_sales\n+POSTHOOK: Input: default@customer_address\n+POSTHOOK: Input: default@date_dim\n+POSTHOOK: Output: hdfs://### HDFS PATH ###\n+CBO PLAN:\n+HiveAggregate(group=[{}], agg#0=[count(DISTINCT $4)], agg#1=[sum($5)], agg#2=[sum($6)])\n+  HiveJoin(condition=[=($4, $14)], joinType=[anti], algorithm=[none], cost=[not available])\n+    HiveSemiJoin(condition=[AND(<>($3, $13), =($4, $14))], joinType=[semi])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk5ODk4Ng==", "bodyText": "done ..creating the HiveAntiJoin operator directly", "url": "https://github.com/apache/hive/pull/1147#discussion_r459998986", "createdAt": "2020-07-24T11:33:29Z", "author": {"login": "maheshk114"}, "path": "ql/src/test/results/clientpositive/perf/tez/cbo_query16_anti_join.q.out", "diffHunk": "@@ -0,0 +1,99 @@\n+PREHOOK: query: explain cbo\n+select\n+   count(distinct cs_order_number) as `order count`\n+  ,sum(cs_ext_ship_cost) as `total shipping cost`\n+  ,sum(cs_net_profit) as `total net profit`\n+from\n+   catalog_sales cs1\n+  ,date_dim\n+  ,customer_address\n+  ,call_center\n+where\n+    d_date between '2001-4-01' and\n+           (cast('2001-4-01' as date) + 60 days)\n+and cs1.cs_ship_date_sk = d_date_sk\n+and cs1.cs_ship_addr_sk = ca_address_sk\n+and ca_state = 'NY'\n+and cs1.cs_call_center_sk = cc_call_center_sk\n+and cc_county in ('Ziebach County','Levy County','Huron County','Franklin Parish',\n+                  'Daviess County'\n+)\n+and exists (select *\n+            from catalog_sales cs2\n+            where cs1.cs_order_number = cs2.cs_order_number\n+              and cs1.cs_warehouse_sk <> cs2.cs_warehouse_sk)\n+and not exists(select *\n+               from catalog_returns cr1\n+               where cs1.cs_order_number = cr1.cr_order_number)\n+order by count(distinct cs_order_number)\n+limit 100\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@call_center\n+PREHOOK: Input: default@catalog_returns\n+PREHOOK: Input: default@catalog_sales\n+PREHOOK: Input: default@customer_address\n+PREHOOK: Input: default@date_dim\n+PREHOOK: Output: hdfs://### HDFS PATH ###\n+POSTHOOK: query: explain cbo\n+select\n+   count(distinct cs_order_number) as `order count`\n+  ,sum(cs_ext_ship_cost) as `total shipping cost`\n+  ,sum(cs_net_profit) as `total net profit`\n+from\n+   catalog_sales cs1\n+  ,date_dim\n+  ,customer_address\n+  ,call_center\n+where\n+    d_date between '2001-4-01' and\n+           (cast('2001-4-01' as date) + 60 days)\n+and cs1.cs_ship_date_sk = d_date_sk\n+and cs1.cs_ship_addr_sk = ca_address_sk\n+and ca_state = 'NY'\n+and cs1.cs_call_center_sk = cc_call_center_sk\n+and cc_county in ('Ziebach County','Levy County','Huron County','Franklin Parish',\n+                  'Daviess County'\n+)\n+and exists (select *\n+            from catalog_sales cs2\n+            where cs1.cs_order_number = cs2.cs_order_number\n+              and cs1.cs_warehouse_sk <> cs2.cs_warehouse_sk)\n+and not exists(select *\n+               from catalog_returns cr1\n+               where cs1.cs_order_number = cr1.cr_order_number)\n+order by count(distinct cs_order_number)\n+limit 100\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@call_center\n+POSTHOOK: Input: default@catalog_returns\n+POSTHOOK: Input: default@catalog_sales\n+POSTHOOK: Input: default@customer_address\n+POSTHOOK: Input: default@date_dim\n+POSTHOOK: Output: hdfs://### HDFS PATH ###\n+CBO PLAN:\n+HiveAggregate(group=[{}], agg#0=[count(DISTINCT $4)], agg#1=[sum($5)], agg#2=[sum($6)])\n+  HiveJoin(condition=[=($4, $14)], joinType=[anti], algorithm=[none], cost=[not available])\n+    HiveSemiJoin(condition=[AND(<>($3, $13), =($4, $14))], joinType=[semi])", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUwOTA5MA=="}, "originalCommit": {"oid": "ee4390223caf1816ba6c07c1245876dc3c99d1e9"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2MTI5MzczOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQwMzoyNDo0NlrOG1RTdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQxMTozMzoxMFrOG2sGfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUxMTIyMA==", "bodyText": "Probably it is here where we do not create the antijoin operator explicitly and why we end up with normal joins in Calcite plan. Since we are creating SemiJoin and AntiJoin as different operators, I think we should follow that pattern here and create an antijoin explicitly or using the builder (you can look at HiveSemiJoinRule). Nevertheless, we could possibly get rid of HiveAntiJoin and HiveSemiJoin all together as I mentioned in another comment, but that can be part of another JIRA.", "url": "https://github.com/apache/hive/pull/1147#discussion_r458511220", "createdAt": "2020-07-22T03:24:46Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveJoinWithFilterToAntiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveJoinWithFilterToAntiJoinRule.class);\n+  public static final HiveJoinWithFilterToAntiJoinRule INSTANCE = new HiveJoinWithFilterToAntiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveJoinWithFilterToAntiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Matched HiveAntiJoinRule\");\n+\n+    assert (filter != null);\n+\n+    //We support conversion from left outer join only.\n+    if (join.getJoinType() != JoinRelType.LEFT) {\n+      return;\n+    }\n+\n+    List<RexNode> aboveFilters = RelOptUtil.conjunctions(filter.getCondition());\n+    boolean hasIsNull = false;\n+\n+    // Get all filter condition and check if any of them is a \"is null\" kind.\n+    for (RexNode filterNode : aboveFilters) {\n+      if (filterNode.getKind() == SqlKind.IS_NULL &&\n+              isFilterFromRightSide(join, filterNode, join.getJoinType())) {\n+        hasIsNull = true;\n+        break;\n+      }\n+    }\n+\n+    // Is null should be on a key from right side of the join.\n+    if (!hasIsNull) {\n+      return;\n+    }\n+\n+    // Build anti join with same left, right child and condition as original left outer join.\n+    Join anti = join.copy(join.getTraitSet(), join.getCondition(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3631f8c2fcca7895e0ccec324ce55de99c2a4cb"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTk5ODg0NQ==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r459998845", "createdAt": "2020-07-24T11:33:10Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinWithFilterToAntiJoinRule.java", "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rel.type.RelDataTypeField;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.util.ImmutableBitSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveJoinWithFilterToAntiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveJoinWithFilterToAntiJoinRule.class);\n+  public static final HiveJoinWithFilterToAntiJoinRule INSTANCE = new HiveJoinWithFilterToAntiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveJoinWithFilterToAntiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Matched HiveAntiJoinRule\");\n+\n+    assert (filter != null);\n+\n+    //We support conversion from left outer join only.\n+    if (join.getJoinType() != JoinRelType.LEFT) {\n+      return;\n+    }\n+\n+    List<RexNode> aboveFilters = RelOptUtil.conjunctions(filter.getCondition());\n+    boolean hasIsNull = false;\n+\n+    // Get all filter condition and check if any of them is a \"is null\" kind.\n+    for (RexNode filterNode : aboveFilters) {\n+      if (filterNode.getKind() == SqlKind.IS_NULL &&\n+              isFilterFromRightSide(join, filterNode, join.getJoinType())) {\n+        hasIsNull = true;\n+        break;\n+      }\n+    }\n+\n+    // Is null should be on a key from right side of the join.\n+    if (!hasIsNull) {\n+      return;\n+    }\n+\n+    // Build anti join with same left, right child and condition as original left outer join.\n+    Join anti = join.copy(join.getTraitSet(), join.getCondition(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUxMTIyMA=="}, "originalCommit": {"oid": "b3631f8c2fcca7895e0ccec324ce55de99c2a4cb"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMTgxMDM5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveCalciteUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxOTo1NzoyNFrOG7Gykg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMDowMTo1OVrOG7vhsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDYzMDQxOA==", "bodyText": "nit. Change comment to javadoc", "url": "https://github.com/apache/hive/pull/1147#discussion_r464630418", "createdAt": "2020-08-03T19:57:24Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveCalciteUtil.java", "diffHunk": "@@ -1233,4 +1233,21 @@ public FixNullabilityShuttle(RexBuilder rexBuilder,\n     }\n   }\n \n+  // Checks if any of the expression given as list expressions are from right side of the join.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI5Nzg0MA==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r465297840", "createdAt": "2020-08-04T20:01:59Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveCalciteUtil.java", "diffHunk": "@@ -1233,4 +1233,21 @@ public FixNullabilityShuttle(RexBuilder rexBuilder,\n     }\n   }\n \n+  // Checks if any of the expression given as list expressions are from right side of the join.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDYzMDQxOA=="}, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjA4ODQyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelOptUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTozMjoxOVrOG7Ja3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMDowMTo1NVrOG7vhkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3MzUwMg==", "bodyText": "Thanks for creating HIVE-23906. Can we simply return nonRewritable if it is an anti-join for the time being, rather than proceeding? This certainly requires a bit of extra thinking and specific tests to make sure it is working as expected (for which we already have HIVE-23906).", "url": "https://github.com/apache/hive/pull/1147#discussion_r464673502", "createdAt": "2020-08-03T21:32:19Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelOptUtil.java", "diffHunk": "@@ -747,6 +747,8 @@ public static RewritablePKFKJoinInfo isRewritablePKFKJoin(Join join,\n     final RelNode nonFkInput = leftInputPotentialFK ? join.getRight() : join.getLeft();\n     final RewritablePKFKJoinInfo nonRewritable = RewritablePKFKJoinInfo.of(false, null);\n \n+    // TODO : Need to handle Anti join.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI5NzgwOQ==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r465297809", "createdAt": "2020-08-04T20:01:55Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelOptUtil.java", "diffHunk": "@@ -747,6 +747,8 @@ public static RewritablePKFKJoinInfo isRewritablePKFKJoin(Join join,\n     final RelNode nonFkInput = leftInputPotentialFK ? join.getRight() : join.getLeft();\n     final RewritablePKFKJoinInfo nonRewritable = RewritablePKFKJoinInfo.of(false, null);\n \n+    // TODO : Need to handle Anti join.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3MzUwMg=="}, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjExNTE1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelOptUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTo0Mjo0NFrOG7Jraw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMDowMTo1MVrOG7vhbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3NzczOQ==", "bodyText": "Can we remove this change? We will address this in HIVE-23906.", "url": "https://github.com/apache/hive/pull/1147#discussion_r464677739", "createdAt": "2020-08-03T21:42:44Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelOptUtil.java", "diffHunk": "@@ -854,7 +856,7 @@ public static RewritablePKFKJoinInfo isRewritablePKFKJoin(Join join,\n             if (ecT.getEquivalenceClassesMap().containsKey(uniqueKeyColumnRef) &&\n                 ecT.getEquivalenceClassesMap().get(uniqueKeyColumnRef).contains(foreignKeyColumnRef)) {\n               if (foreignKeyColumnType.isNullable()) {\n-                if (joinType == JoinRelType.INNER || join.isSemiJoin()) {\n+                if (joinType == JoinRelType.INNER || join.isSemiJoin() || joinType == JoinRelType.ANTI) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI5Nzc3Mg==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r465297772", "createdAt": "2020-08-04T20:01:51Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelOptUtil.java", "diffHunk": "@@ -854,7 +856,7 @@ public static RewritablePKFKJoinInfo isRewritablePKFKJoin(Join join,\n             if (ecT.getEquivalenceClassesMap().containsKey(uniqueKeyColumnRef) &&\n                 ecT.getEquivalenceClassesMap().get(uniqueKeyColumnRef).contains(foreignKeyColumnRef)) {\n               if (foreignKeyColumnType.isNullable()) {\n-                if (joinType == JoinRelType.INNER || join.isSemiJoin()) {\n+                if (joinType == JoinRelType.INNER || join.isSemiJoin() || joinType == JoinRelType.ANTI) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY3NzczOQ=="}, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjE3NTY5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAntiSemiJoinRule.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMjowNjoxMVrOG7KPXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwNDo1MDoyOVrOG9MYTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4Njk0Mw==", "bodyText": "It seems aboveFilters are ignored but we should not. For instance, condition is AND(func(x,y), z IS NULL), where x comes from the left input, and y and z from the right input. You would still need to apply first conjunct for correctness? Could we try to add such test too?", "url": "https://github.com/apache/hive/pull/1147#discussion_r464686943", "createdAt": "2020-08-03T22:06:11Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAntiSemiJoinRule.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAntiJoin;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveAntiSemiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveAntiSemiJoinRule.class);\n+  public static final HiveAntiSemiJoinRule INSTANCE = new HiveAntiSemiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveAntiSemiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Start Matching HiveAntiJoinRule\");\n+\n+    //TODO : Need to support this scenario.\n+    if (join.getCondition().isAlwaysTrue()) {\n+      return;\n+    }\n+\n+    //We support conversion from left outer join only.\n+    if (join.getJoinType() != JoinRelType.LEFT) {\n+      return;\n+    }\n+\n+    assert (filter != null);\n+\n+    // If null filter is not present from right side then we can not convert to anti join.\n+    List<RexNode> aboveFilters = RelOptUtil.conjunctions(filter.getCondition());\n+    Stream<RexNode> nullFilters = aboveFilters.stream().filter(filterNode -> filterNode.getKind() == SqlKind.IS_NULL);\n+    boolean hasNullFilter = HiveCalciteUtil.hasAnyExpressionFromRightSide(join, nullFilters.collect(Collectors.toList()));\n+    if (!hasNullFilter) {\n+      return;\n+    }\n+\n+    // If any projection is there from right side, then we can not convert to anti join.\n+    boolean hasProjection = HiveCalciteUtil.hasAnyExpressionFromRightSide(join, project.getProjects());\n+    if (hasProjection) {\n+      return;\n+    }\n+\n+    LOG.debug(\"Matched HiveAntiJoinRule\");\n+\n+    // Build anti join with same left, right child and condition as original left outer join.\n+    Join anti = HiveAntiJoin.getAntiJoin(join.getLeft().getCluster(), join.getLeft().getTraitSet(),\n+            join.getLeft(), join.getRight(), join.getCondition());\n+    RelNode newProject = project.copy(project.getTraitSet(), anti, project.getProjects(), project.getRowType());\n+    call.transformTo(newProject);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI5NzcxOA==", "bodyText": "added some new test cases like select t1.fld from tbl t1 left join tbl1 t2 on t1.fld = t2.fld where t2.fld is null and rand(t2.fld) > 100;\nselect t1.fld from tbl t1 left join tbl1 t2 on t1.fld = t2.fld where t2.fld is null and rand(t1.fld) > 100;\nselect t1.fld from tbl t1 left join tbl1 t2 on t1.fld = t2.fld where t2.fld is null and rand(t1.fld) > 100 and rand(t1.fld) > 2100;\nselect t1.fld from tbl t1 left join tbl2 t2 on t1.fld = t2.fld where t2.fld is null or t2.fld1 is null;\nselect t1.fld from tbl t1 left join tbl2 t2 on t1.fld = t2.fld where t2.fld is null or t2.fld1 = 1;", "url": "https://github.com/apache/hive/pull/1147#discussion_r465297718", "createdAt": "2020-08-04T20:01:46Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAntiSemiJoinRule.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAntiJoin;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveAntiSemiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveAntiSemiJoinRule.class);\n+  public static final HiveAntiSemiJoinRule INSTANCE = new HiveAntiSemiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveAntiSemiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Start Matching HiveAntiJoinRule\");\n+\n+    //TODO : Need to support this scenario.\n+    if (join.getCondition().isAlwaysTrue()) {\n+      return;\n+    }\n+\n+    //We support conversion from left outer join only.\n+    if (join.getJoinType() != JoinRelType.LEFT) {\n+      return;\n+    }\n+\n+    assert (filter != null);\n+\n+    // If null filter is not present from right side then we can not convert to anti join.\n+    List<RexNode> aboveFilters = RelOptUtil.conjunctions(filter.getCondition());\n+    Stream<RexNode> nullFilters = aboveFilters.stream().filter(filterNode -> filterNode.getKind() == SqlKind.IS_NULL);\n+    boolean hasNullFilter = HiveCalciteUtil.hasAnyExpressionFromRightSide(join, nullFilters.collect(Collectors.toList()));\n+    if (!hasNullFilter) {\n+      return;\n+    }\n+\n+    // If any projection is there from right side, then we can not convert to anti join.\n+    boolean hasProjection = HiveCalciteUtil.hasAnyExpressionFromRightSide(join, project.getProjects());\n+    if (hasProjection) {\n+      return;\n+    }\n+\n+    LOG.debug(\"Matched HiveAntiJoinRule\");\n+\n+    // Build anti join with same left, right child and condition as original left outer join.\n+    Join anti = HiveAntiJoin.getAntiJoin(join.getLeft().getCluster(), join.getLeft().getTraitSet(),\n+            join.getLeft(), join.getRight(), join.getCondition());\n+    RelNode newProject = project.copy(project.getTraitSet(), anti, project.getProjects(), project.getRowType());\n+    call.transformTo(newProject);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4Njk0Mw=="}, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ0NjI5OA==", "bodyText": "Is rand pushed down?\nI was referring specifically about conditions that remain on top of the join (that's why I used a function on columns from multiple inputs). The logic above seems to remove those conditions and they get ignored... but they should not.", "url": "https://github.com/apache/hive/pull/1147#discussion_r465446298", "createdAt": "2020-08-05T03:12:46Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAntiSemiJoinRule.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAntiJoin;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveAntiSemiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveAntiSemiJoinRule.class);\n+  public static final HiveAntiSemiJoinRule INSTANCE = new HiveAntiSemiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveAntiSemiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Start Matching HiveAntiJoinRule\");\n+\n+    //TODO : Need to support this scenario.\n+    if (join.getCondition().isAlwaysTrue()) {\n+      return;\n+    }\n+\n+    //We support conversion from left outer join only.\n+    if (join.getJoinType() != JoinRelType.LEFT) {\n+      return;\n+    }\n+\n+    assert (filter != null);\n+\n+    // If null filter is not present from right side then we can not convert to anti join.\n+    List<RexNode> aboveFilters = RelOptUtil.conjunctions(filter.getCondition());\n+    Stream<RexNode> nullFilters = aboveFilters.stream().filter(filterNode -> filterNode.getKind() == SqlKind.IS_NULL);\n+    boolean hasNullFilter = HiveCalciteUtil.hasAnyExpressionFromRightSide(join, nullFilters.collect(Collectors.toList()));\n+    if (!hasNullFilter) {\n+      return;\n+    }\n+\n+    // If any projection is there from right side, then we can not convert to anti join.\n+    boolean hasProjection = HiveCalciteUtil.hasAnyExpressionFromRightSide(join, project.getProjects());\n+    if (hasProjection) {\n+      return;\n+    }\n+\n+    LOG.debug(\"Matched HiveAntiJoinRule\");\n+\n+    // Build anti join with same left, right child and condition as original left outer join.\n+    Join anti = HiveAntiJoin.getAntiJoin(join.getLeft().getCluster(), join.getLeft().getTraitSet(),\n+            join.getLeft(), join.getRight(), join.getCondition());\n+    RelNode newProject = project.copy(project.getTraitSet(), anti, project.getProjects(), project.getRowType());\n+    call.transformTo(newProject);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4Njk0Mw=="}, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgxOTE0OQ==", "bodyText": "for normal filter, its being pushed down. here we get filters which can not be pushed down. I have modified the code to handle those filters. And added these extra tests to verify.  rand  is not pushed down.", "url": "https://github.com/apache/hive/pull/1147#discussion_r466819149", "createdAt": "2020-08-07T04:50:29Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveAntiSemiJoinRule.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.optimizer.calcite.rules;\n+\n+import org.apache.calcite.plan.RelOptRule;\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.plan.RelOptUtil;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Filter;\n+import org.apache.calcite.rel.core.Join;\n+import org.apache.calcite.rel.core.JoinRelType;\n+import org.apache.calcite.rel.core.Project;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil;\n+import org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAntiJoin;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Planner rule that converts a join plus filter to anti join.\n+ */\n+public class HiveAntiSemiJoinRule extends RelOptRule {\n+  protected static final Logger LOG = LoggerFactory.getLogger(HiveAntiSemiJoinRule.class);\n+  public static final HiveAntiSemiJoinRule INSTANCE = new HiveAntiSemiJoinRule();\n+\n+  //    HiveProject(fld=[$0])\n+  //      HiveFilter(condition=[IS NULL($1)])\n+  //        HiveJoin(condition=[=($0, $1)], joinType=[left], algorithm=[none], cost=[not available])\n+  //\n+  // TO\n+  //\n+  //    HiveProject(fld_tbl=[$0])\n+  //      HiveAntiJoin(condition=[=($0, $1)], joinType=[anti])\n+  //\n+  public HiveAntiSemiJoinRule() {\n+    super(operand(Project.class, operand(Filter.class, operand(Join.class, RelOptRule.any()))),\n+            \"HiveJoinWithFilterToAntiJoinRule:filter\");\n+  }\n+\n+  // is null filter over a left join.\n+  public void onMatch(final RelOptRuleCall call) {\n+    final Project project = call.rel(0);\n+    final Filter filter = call.rel(1);\n+    final Join join = call.rel(2);\n+    perform(call, project, filter, join);\n+  }\n+\n+  protected void perform(RelOptRuleCall call, Project project, Filter filter, Join join) {\n+    LOG.debug(\"Start Matching HiveAntiJoinRule\");\n+\n+    //TODO : Need to support this scenario.\n+    if (join.getCondition().isAlwaysTrue()) {\n+      return;\n+    }\n+\n+    //We support conversion from left outer join only.\n+    if (join.getJoinType() != JoinRelType.LEFT) {\n+      return;\n+    }\n+\n+    assert (filter != null);\n+\n+    // If null filter is not present from right side then we can not convert to anti join.\n+    List<RexNode> aboveFilters = RelOptUtil.conjunctions(filter.getCondition());\n+    Stream<RexNode> nullFilters = aboveFilters.stream().filter(filterNode -> filterNode.getKind() == SqlKind.IS_NULL);\n+    boolean hasNullFilter = HiveCalciteUtil.hasAnyExpressionFromRightSide(join, nullFilters.collect(Collectors.toList()));\n+    if (!hasNullFilter) {\n+      return;\n+    }\n+\n+    // If any projection is there from right side, then we can not convert to anti join.\n+    boolean hasProjection = HiveCalciteUtil.hasAnyExpressionFromRightSide(join, project.getProjects());\n+    if (hasProjection) {\n+      return;\n+    }\n+\n+    LOG.debug(\"Matched HiveAntiJoinRule\");\n+\n+    // Build anti join with same left, right child and condition as original left outer join.\n+    Join anti = HiveAntiJoin.getAntiJoin(join.getLeft().getCluster(), join.getLeft().getTraitSet(),\n+            join.getLeft(), join.getRight(), join.getCondition());\n+    RelNode newProject = project.copy(project.getTraitSet(), anti, project.getProjects(), project.getRowType());\n+    call.transformTo(newProject);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4Njk0Mw=="}, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjIxMzEzOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMjoyMTo0MVrOG7Klgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwNDo1MjoxMlrOG9MZ9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY5MjYxMA==", "bodyText": "I did not check the physical implementation in much detail: Just wanted to confirm that you are considering there the case for empty (0 rows) right input, correct? It came to mind as the introduced filter could filter all rows.", "url": "https://github.com/apache/hive/pull/1147#discussion_r464692610", "createdAt": "2020-08-03T22:21:41Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java", "diffHunk": "@@ -92,7 +104,7 @@ public void onMatch(RelOptRuleCall call) {\n     Set<String> rightPushedPredicates = Sets.newHashSet(registry.getPushedPredicates(join, 1));\n \n     boolean genPredOnLeft = join.getJoinType() == JoinRelType.RIGHT || join.getJoinType() == JoinRelType.INNER || join.isSemiJoin();\n-    boolean genPredOnRight = join.getJoinType() == JoinRelType.LEFT || join.getJoinType() == JoinRelType.INNER || join.isSemiJoin();\n+    boolean genPredOnRight = join.getJoinType() == JoinRelType.LEFT || join.getJoinType() == JoinRelType.INNER || join.isSemiJoin()|| join.getJoinType() == JoinRelType.ANTI;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI5NzY2Mw==", "bodyText": "Yes ..if right side is null then it emits all the right side records", "url": "https://github.com/apache/hive/pull/1147#discussion_r465297663", "createdAt": "2020-08-04T20:01:40Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java", "diffHunk": "@@ -92,7 +104,7 @@ public void onMatch(RelOptRuleCall call) {\n     Set<String> rightPushedPredicates = Sets.newHashSet(registry.getPushedPredicates(join, 1));\n \n     boolean genPredOnLeft = join.getJoinType() == JoinRelType.RIGHT || join.getJoinType() == JoinRelType.INNER || join.isSemiJoin();\n-    boolean genPredOnRight = join.getJoinType() == JoinRelType.LEFT || join.getJoinType() == JoinRelType.INNER || join.isSemiJoin();\n+    boolean genPredOnRight = join.getJoinType() == JoinRelType.LEFT || join.getJoinType() == JoinRelType.INNER || join.isSemiJoin()|| join.getJoinType() == JoinRelType.ANTI;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY5MjYxMA=="}, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ0NjQ5NQ==", "bodyText": "I was referring to empty input (no rows) rather than null.", "url": "https://github.com/apache/hive/pull/1147#discussion_r465446495", "createdAt": "2020-08-05T03:13:41Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java", "diffHunk": "@@ -92,7 +104,7 @@ public void onMatch(RelOptRuleCall call) {\n     Set<String> rightPushedPredicates = Sets.newHashSet(registry.getPushedPredicates(join, 1));\n \n     boolean genPredOnLeft = join.getJoinType() == JoinRelType.RIGHT || join.getJoinType() == JoinRelType.INNER || join.isSemiJoin();\n-    boolean genPredOnRight = join.getJoinType() == JoinRelType.LEFT || join.getJoinType() == JoinRelType.INNER || join.isSemiJoin();\n+    boolean genPredOnRight = join.getJoinType() == JoinRelType.LEFT || join.getJoinType() == JoinRelType.INNER || join.isSemiJoin()|| join.getJoinType() == JoinRelType.ANTI;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY5MjYxMA=="}, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgxOTU3Mg==", "bodyText": "yes ..that is taken care of.\n// For anti join, we should proceed to emit records if the right side is empty or not matching.\nif (type == JoinDesc.ANTI_JOIN && !producedRow) {", "url": "https://github.com/apache/hive/pull/1147#discussion_r466819572", "createdAt": "2020-08-07T04:52:12Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinAddNotNullRule.java", "diffHunk": "@@ -92,7 +104,7 @@ public void onMatch(RelOptRuleCall call) {\n     Set<String> rightPushedPredicates = Sets.newHashSet(registry.getPushedPredicates(join, 1));\n \n     boolean genPredOnLeft = join.getJoinType() == JoinRelType.RIGHT || join.getJoinType() == JoinRelType.INNER || join.isSemiJoin();\n-    boolean genPredOnRight = join.getJoinType() == JoinRelType.LEFT || join.getJoinType() == JoinRelType.INNER || join.isSemiJoin();\n+    boolean genPredOnRight = join.getJoinType() == JoinRelType.LEFT || join.getJoinType() == JoinRelType.INNER || join.isSemiJoin()|| join.getJoinType() == JoinRelType.ANTI;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY5MjYxMA=="}, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjIzNzE5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinConstraintsRule.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMjozMjoxM1rOG7Kz2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMDowMTozNVrOG7vg2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY5NjI4Mg==", "bodyText": "This should be removed to avoid confusion, since we bail out above.", "url": "https://github.com/apache/hive/pull/1147#discussion_r464696282", "createdAt": "2020-08-03T22:32:13Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinConstraintsRule.java", "diffHunk": "@@ -183,6 +189,7 @@ public void onMatch(RelOptRuleCall call) {\n     switch (joinType) {\n     case SEMI:\n     case INNER:\n+    case ANTI:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI5NzYyNQ==", "bodyText": "done", "url": "https://github.com/apache/hive/pull/1147#discussion_r465297625", "createdAt": "2020-08-04T20:01:35Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveJoinConstraintsRule.java", "diffHunk": "@@ -183,6 +189,7 @@ public void onMatch(RelOptRuleCall call) {\n     switch (joinType) {\n     case SEMI:\n     case INNER:\n+    case ANTI:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY5NjI4Mg=="}, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjI1NDczOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/ppd/PredicateTransitivePropagate.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMjo0MDowNFrOG7K-IQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMDowMTozMlrOG7vgxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY5ODkxMw==", "bodyText": "test?", "url": "https://github.com/apache/hive/pull/1147#discussion_r464698913", "createdAt": "2020-08-03T22:40:04Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/ppd/PredicateTransitivePropagate.java", "diffHunk": "@@ -203,6 +203,7 @@ private boolean filterExists(ReduceSinkOperator target, ExprNodeDesc replaced) {\n           vector.add(right, left);\n           break;\n         case JoinDesc.LEFT_OUTER_JOIN:\n+        case JoinDesc.ANTI_JOIN: //TODO : need to test", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI5NzYwNA==", "bodyText": "removed the comment.", "url": "https://github.com/apache/hive/pull/1147#discussion_r465297604", "createdAt": "2020-08-04T20:01:32Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/ppd/PredicateTransitivePropagate.java", "diffHunk": "@@ -203,6 +203,7 @@ private boolean filterExists(ReduceSinkOperator target, ExprNodeDesc replaced) {\n           vector.add(right, left);\n           break;\n         case JoinDesc.LEFT_OUTER_JOIN:\n+        case JoinDesc.ANTI_JOIN: //TODO : need to test", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY5ODkxMw=="}, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjI1ODYyOnYy", "diffSide": "RIGHT", "path": "ql/src/test/queries/clientpositive/subquery_in_having.q", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMjo0MjowNFrOG7LAcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMDowMToyOFrOG7vgmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY5OTUwNw==", "bodyText": "Should we execute this query with conversion=true?", "url": "https://github.com/apache/hive/pull/1147#discussion_r464699507", "createdAt": "2020-08-03T22:42:04Z", "author": {"login": "jcamachor"}, "path": "ql/src/test/queries/clientpositive/subquery_in_having.q", "diffHunk": "@@ -140,6 +140,22 @@ CREATE TABLE src_null_n4 (key STRING COMMENT 'default', value STRING COMMENT 'de\n LOAD DATA LOCAL INPATH \"../../data/files/kv1.txt\" INTO TABLE src_null_n4;\n INSERT INTO src_null_n4 values('5444', null);\n \n+explain\n+select key, value, count(*)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTI5NzU2Mw==", "bodyText": "By default anti join conversion is set to true. I have added few test cases with anti join set to false.", "url": "https://github.com/apache/hive/pull/1147#discussion_r465297563", "createdAt": "2020-08-04T20:01:28Z", "author": {"login": "maheshk114"}, "path": "ql/src/test/queries/clientpositive/subquery_in_having.q", "diffHunk": "@@ -140,6 +140,22 @@ CREATE TABLE src_null_n4 (key STRING COMMENT 'default', value STRING COMMENT 'de\n LOAD DATA LOCAL INPATH \"../../data/files/kv1.txt\" INTO TABLE src_null_n4;\n INSERT INTO src_null_n4 values('5444', null);\n \n+explain\n+select key, value, count(*)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY5OTUwNw=="}, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjI2MjQ2OnYy", "diffSide": "RIGHT", "path": "ql/src/test/results/clientpositive/llap/subquery_notexists_having.q.out", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMjo0NDowOFrOG7LCwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwNToxNzoxNFrOG9MxkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwMDA5OA==", "bodyText": "This is resulting in an additional task. Is this expected?", "url": "https://github.com/apache/hive/pull/1147#discussion_r464700098", "createdAt": "2020-08-03T22:44:08Z", "author": {"login": "jcamachor"}, "path": "ql/src/test/results/clientpositive/llap/subquery_notexists_having.q.out", "diffHunk": "@@ -31,7 +31,8 @@ STAGE PLANS:\n     Tez\n #### A masked pattern was here ####\n       Edges:\n-        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 3 (SIMPLE_EDGE)\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgyNTYxNw==", "bodyText": "yes ..the join is getting converted to SMB join ..and so no reducer is required. In case of anti join its not getting converted. That is because left outer is adding an extra group by which is making the RS node on left and right side  equal, the pre-condition for converting to SMB join.", "url": "https://github.com/apache/hive/pull/1147#discussion_r466825617", "createdAt": "2020-08-07T05:17:14Z", "author": {"login": "maheshk114"}, "path": "ql/src/test/results/clientpositive/llap/subquery_notexists_having.q.out", "diffHunk": "@@ -31,7 +31,8 @@ STAGE PLANS:\n     Tez\n #### A masked pattern was here ####\n       Edges:\n-        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 3 (SIMPLE_EDGE)\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwMDA5OA=="}, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjMwMDI1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMzowMTo0MFrOG7LYmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwNTowNDozOFrOG9MldA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwNTY5MA==", "bodyText": "This is concerning. In fact, I believe we should rather apply this rule after join reordering, since there is no support built-in for ANTIJOIN in the reordering algorithm, while there is support for outer join... which could lead to worse join order.\nIf the problem is that the inputs are being swapped, we can add a variant of HiveJoinCommuteRule / JoinCommuteRule to this program that is applied on RIGHT OUTER joins.", "url": "https://github.com/apache/hive/pull/1147#discussion_r464705690", "createdAt": "2020-08-03T23:01:40Z", "author": {"login": "jcamachor"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java", "diffHunk": "@@ -2129,6 +2133,16 @@ private RelNode applyPreJoinOrderingTransforms(RelNode basePlan, RelMetadataProv\n             HiveRemoveSqCountCheck.INSTANCE);\n       }\n \n+      // 10. Convert left outer join + null filter on right side table column to anti join. Add this\n+      // rule after all the optimization for which calcite support for anti join is missing.\n+      // Needs to be done before ProjectRemoveRule as it expect a project over filter.\n+      // This is done before join re-ordering as join re-ordering is converting the left outer", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgyMjUxNg==", "bodyText": "As discussed, i have created a Jira https://issues.apache.org/jira/browse/HIVE-24013", "url": "https://github.com/apache/hive/pull/1147#discussion_r466822516", "createdAt": "2020-08-07T05:04:38Z", "author": {"login": "maheshk114"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java", "diffHunk": "@@ -2129,6 +2133,16 @@ private RelNode applyPreJoinOrderingTransforms(RelNode basePlan, RelMetadataProv\n             HiveRemoveSqCountCheck.INSTANCE);\n       }\n \n+      // 10. Convert left outer join + null filter on right side table column to anti join. Add this\n+      // rule after all the optimization for which calcite support for anti join is missing.\n+      // Needs to be done before ProjectRemoveRule as it expect a project over filter.\n+      // This is done before join re-ordering as join re-ordering is converting the left outer", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwNTY5MA=="}, "originalCommit": {"oid": "50507cf6ed4a744c8789fe2da101024911d4336b"}, "originalPosition": 50}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 657, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}