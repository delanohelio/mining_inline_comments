{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk1NjM5MjM1", "number": 1542, "title": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)", "bodyText": "This patch adds HMS storage backend for HPL/SQL stored procedures. There are 3 main areas which were effected by the patch:\n\nNew MetaStore thrift API methods for creating, deleting and listing stored procedures\nDDLs for the new tables which store the procedure code and metadata\nHPL/SQL interpreter changes to make use of the new thrift API\n\nThe existing file based backend was kept in HPL/SQL (most of the functionality of Function.java was moved to InMemoryFunction.java and additionally a HmsFunction.java).", "createdAt": "2020-09-30T16:38:01Z", "url": "https://github.com/apache/hive/pull/1542", "merged": true, "mergeCommit": {"oid": "e2d637ea99f0ee906881b2cf8dee3fb308064627"}, "closed": true, "closedAt": "2020-10-29T16:44:47Z", "author": {"login": "zeroflag"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdONkkYgBqjM4MjgzNDY2Nzg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdW8bpKAFqTUxODU5MTMzNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "96dc6f7ee6d60d2c0316d92fff7068ca4ef6a680", "author": {"user": {"login": "zeroflag", "name": "Attila Magyar"}}, "url": "https://github.com/apache/hive/commit/96dc6f7ee6d60d2c0316d92fff7068ca4ef6a680", "committedDate": "2020-09-30T16:36:46Z", "message": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)"}, "afterCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c", "author": {"user": {"login": "zeroflag", "name": "Attila Magyar"}}, "url": "https://github.com/apache/hive/commit/adf8d6509cb9003bf08671ac95d5913e12afb80c", "committedDate": "2020-10-01T09:00:05Z", "message": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAxMTQ3OTI3", "url": "https://github.com/apache/hive/pull/1542#pullrequestreview-501147927", "createdAt": "2020-10-02T13:44:38Z", "commit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxMzo0NDozOVrOHbuM4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMlQxNDo0MToyMFrOHbwRgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODgzMDU2Mw==", "bodyText": "could you please follow the convention of other methods and define a struct for the requests arguments", "url": "https://github.com/apache/hive/pull/1542#discussion_r498830563", "createdAt": "2020-10-02T13:44:39Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift", "diffHunk": "@@ -2830,6 +2848,11 @@ PartitionsResponse get_partitions_req(1:PartitionsRequest req)\n   void add_replication_metrics(1: ReplicationMetricList replicationMetricList) throws(1:MetaException o1)\n   ReplicationMetricList get_replication_metrics(1: GetReplicationMetricsRequest rqst) throws(1:MetaException o1)\n   GetOpenTxnsResponse get_open_txns_req(1: GetOpenTxnsRequest getOpenTxnsRequest)\n+\n+  void create_stored_procedure(1: string catName, 2: StoredProcedure proc) throws(1:NoSuchObjectException o1, 2:MetaException o2)\n+  StoredProcedure get_stored_procedure(1: string catName, 2: string db, 3: string name) throws (1:MetaException o1, 2:NoSuchObjectException o2)\n+  void drop_stored_procedure(1: string catName, 2: string dbName, 3: string funcName) throws (1:MetaException o1, 2:NoSuchObjectException o2)\n+  list<StoredProcedure> get_all_stored_procedures(1: string catName) throws (1:MetaException o1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg1OTYzNA==", "bodyText": "I think instead of storing the return_type/argument types and such in the metastore - as they would never participate in a query or anything \"usefull\"; they will just travel as payload in the messages.\nGiven the fact that they are effectively implicit data which can be figured out from the function defintion - I think we may leave it to the execution engine; it should be able to figure it out (since it should be able to use it) .\noptionally; to give ourselfs(and users) some type of clarity we could add a \"signature\" string to the table - which could provide a human readable signature", "url": "https://github.com/apache/hive/pull/1542#discussion_r498859634", "createdAt": "2020-10-02T14:32:55Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,83 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">\n+      <datastore-identity>\n+        <column name=\"SP_ID\"/>\n+      </datastore-identity>\n+      <field name=\"createTime\">\n+        <column name=\"CREATE_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"lastAccessTime\">\n+        <column name=\"LAST_ACCESS_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"database\">\n+        <column name=\"DB_ID\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"name\">\n+        <column name=\"NAME\" length=\"256\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"owner\">\n+        <column name=\"OWNER_NAME\" length=\"128\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"source\">\n+        <column name=\"SOURCE\" jdbc-type=\"MEDIUMTEXT\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"language\">\n+        <column name=\"LANG\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"returnType\">\n+        <column name=\"RET_TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"true\"/>\n+      </field>\n+      <field name=\"parameters\" table=\"SP_POS_ARGS\" >\n+        <collection element-type=\"MPosParam\"/>\n+        <join>\n+            <column name=\"SP_ID\"/>\n+        </join>\n+        <element>\n+          <embedded>\n+            <field name=\"name\">\n+              <column name=\"NAME\" jdbc-type=\"varchar\" length=\"256\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"type\">\n+              <column name=\"TYPE\" jdbc-type=\"varchar\" length=\"128\" allows-null=\"false\"/>\n+            </field>\n+            <field name=\"length\">\n+              <column name=\"length\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"scale\">\n+              <column name=\"scale\" jdbc-type=\"integer\" allows-null=\"true\"/>\n+            </field>\n+            <field name=\"isOut\">\n+              <column name=\"OUT\" allows-null=\"false\"/>\n+            </field>\n+          </embedded>\n+        </element>\n+      </field>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg2MTM1OA==", "bodyText": "I think we should only add fields which are actually usefull and in use - because right now the accesstime would not be updated at all I don't think we should add it.", "url": "https://github.com/apache/hive/pull/1542#discussion_r498861358", "createdAt": "2020-10-02T14:35:57Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-server/src/main/sql/derby/hive-schema-4.0.0.derby.sql", "diffHunk": "@@ -786,6 +786,35 @@ CREATE TABLE \"APP\".\"REPLICATION_METRICS\" (\n CREATE INDEX \"POLICY_IDX\" ON \"APP\".\"REPLICATION_METRICS\" (\"RM_POLICY\");\n CREATE INDEX \"DUMP_IDX\" ON \"APP\".\"REPLICATION_METRICS\" (\"RM_DUMP_EXECUTION_ID\");\n \n+-- Create stored procedure tables\n+CREATE TABLE \"APP\".\"STORED_PROCS\" (\n+  \"SP_ID\" BIGINT NOT NULL,\n+  \"CREATE_TIME\" INTEGER NOT NULL,\n+  \"LAST_ACCESS_TIME\" INTEGER NOT NULL,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODg2NDUxNQ==", "bodyText": "this is the first occurence of MEDIUMTEXT in package.jdo - I don't know how well that will work\nwe had quite a few problems with \"long\" tableproperty values - and PARAM_VALUE was updated to use CLOB in oracle/etc\nthe most important would be to make sure that we can store the procedure in all supported metastore databases - if possible this should also be tested in some way (at least by hand)", "url": "https://github.com/apache/hive/pull/1542#discussion_r498864515", "createdAt": "2020-10-02T14:41:20Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,83 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">\n+      <datastore-identity>\n+        <column name=\"SP_ID\"/>\n+      </datastore-identity>\n+      <field name=\"createTime\">\n+        <column name=\"CREATE_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"lastAccessTime\">\n+        <column name=\"LAST_ACCESS_TIME\" jdbc-type=\"integer\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"database\">\n+        <column name=\"DB_ID\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"name\">\n+        <column name=\"NAME\" length=\"256\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"owner\">\n+        <column name=\"OWNER_NAME\" length=\"128\" jdbc-type=\"VARCHAR\" allows-null=\"false\"/>\n+      </field>\n+      <field name=\"source\">\n+        <column name=\"SOURCE\" jdbc-type=\"MEDIUMTEXT\" allows-null=\"false\"/>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "adf8d6509cb9003bf08671ac95d5913e12afb80c"}, "originalPosition": 25}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4d8ad5e8637f9789e8ae3d5bb97884f92a2bbade", "author": {"user": {"login": "zeroflag", "name": "Attila Magyar"}}, "url": "https://github.com/apache/hive/commit/4d8ad5e8637f9789e8ae3d5bb97884f92a2bbade", "committedDate": "2020-10-05T14:28:13Z", "message": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)"}, "afterCommit": {"oid": "5addba4f2167c91874b835595a3aebcbe567b5f9", "author": {"user": {"login": "zeroflag", "name": "Attila Magyar"}}, "url": "https://github.com/apache/hive/commit/5addba4f2167c91874b835595a3aebcbe567b5f9", "committedDate": "2020-10-14T12:34:09Z", "message": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1508f30aa7dc42372df67066a4dd464930cfe6b5", "author": {"user": {"login": "zeroflag", "name": "Attila Magyar"}}, "url": "https://github.com/apache/hive/commit/1508f30aa7dc42372df67066a4dd464930cfe6b5", "committedDate": "2020-10-26T08:18:33Z", "message": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b019108bc9efcc32c4a6d97eacca539af8eb22e1", "author": {"user": {"login": "zeroflag", "name": "Attila Magyar"}}, "url": "https://github.com/apache/hive/commit/b019108bc9efcc32c4a6d97eacca539af8eb22e1", "committedDate": "2020-10-26T08:18:56Z", "message": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0587963150a06530cd292e3556e1b3047ebdf3c3", "author": {"user": {"login": "zeroflag", "name": "Attila Magyar"}}, "url": "https://github.com/apache/hive/commit/0587963150a06530cd292e3556e1b3047ebdf3c3", "committedDate": "2020-10-26T08:19:29Z", "message": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "author": {"user": {"login": "zeroflag", "name": "Attila Magyar"}}, "url": "https://github.com/apache/hive/commit/11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "committedDate": "2020-10-26T08:30:35Z", "message": " HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3b4fdb61e11ba0533aae4fbb838b35ba86a248d1", "author": {"user": {"login": "zeroflag", "name": "Attila Magyar"}}, "url": "https://github.com/apache/hive/commit/3b4fdb61e11ba0533aae4fbb838b35ba86a248d1", "committedDate": "2020-10-22T09:37:25Z", "message": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)"}, "afterCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "author": {"user": {"login": "zeroflag", "name": "Attila Magyar"}}, "url": "https://github.com/apache/hive/commit/11d255130a596dc0e8c81c5fe4bdd7b5618f1f89", "committedDate": "2020-10-26T08:30:35Z", "message": " HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2ODA5NTMw", "url": "https://github.com/apache/hive/pull/1542#pullrequestreview-516809530", "createdAt": "2020-10-26T14:04:53Z", "commit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDowNDo1NFrOHoRDhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxNDo1OTozN1rOHoTsUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk4NDUxOA==", "bodyText": "what will happen when the db is dropped? wouldn't this FK will restrict the DB from being dropped?", "url": "https://github.com/apache/hive/pull/1542#discussion_r511984518", "createdAt": "2020-10-26T14:04:54Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.2.0-to-4.0.0.mysql.sql", "diffHunk": "@@ -109,6 +109,20 @@ CREATE TABLE IF NOT EXISTS REPLICATION_METRICS (\n CREATE INDEX POLICY_IDX ON REPLICATION_METRICS (RM_POLICY);\n CREATE INDEX DUMP_IDX ON REPLICATION_METRICS (RM_DUMP_EXECUTION_ID);\n \n+-- Create stored procedure tables\n+CREATE TABLE STORED_PROCS (\n+  `SP_ID` BIGINT(20) NOT NULL,\n+  `CREATE_TIME` INT(11) NOT NULL,\n+  `DB_ID` BIGINT(20) NOT NULL,\n+  `NAME` VARCHAR(256) NOT NULL,\n+  `OWNER_NAME` VARCHAR(128) NOT NULL,\n+  `SOURCE` LONGTEXT NOT NULL,\n+  PRIMARY KEY (`SP_ID`)\n+);\n+\n+CREATE UNIQUE INDEX UNIQUESTOREDPROC ON STORED_PROCS (NAME, DB_ID);\n+ALTER TABLE `STORED_PROCS` ADD CONSTRAINT `STOREDPROC_FK1` FOREIGN KEY (`DB_ID`) REFERENCES DBS (`DB_ID`);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk4OTM0OQ==", "bodyText": "these lines start with * *", "url": "https://github.com/apache/hive/pull/1542#discussion_r511989349", "createdAt": "2020-10-26T14:11:17Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java", "diffHunk": "@@ -1,780 +1,30 @@\n /*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  * Licensed to the Apache Software Foundation (ASF) under one", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5MDA1NA==", "bodyText": "what does this returned boolean mean?", "url": "https://github.com/apache/hive/pull/1542#discussion_r511990054", "createdAt": "2020-10-26T14:12:16Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java", "diffHunk": "@@ -1,780 +1,30 @@\n /*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n  *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n  */\n \n package org.apache.hive.hplsql.functions;\n \n-import java.sql.ResultSet;\n-import java.sql.Date;\n-import java.sql.SQLException;\n-import java.text.SimpleDateFormat;\n-import java.util.ArrayList;\n-import java.util.Calendar;\n-import java.util.HashMap;\n-import java.util.Map;\n-import java.util.TimeZone;\n-import java.util.regex.Matcher;\n-import java.util.regex.Pattern;\n+import org.apache.hive.hplsql.HplsqlParser;\n \n-import org.apache.commons.lang3.StringUtils;\n-import org.antlr.v4.runtime.ParserRuleContext;\n-import org.apache.hive.hplsql.*;\n-\n-interface FuncCommand {\n-  void run(HplsqlParser.Expr_func_paramsContext ctx);\n-}\n-\n-interface FuncSpecCommand {\n-  void run(HplsqlParser.Expr_spec_funcContext ctx);\n-}\n-\n-/**\n- * HPL/SQL functions\n- */\n-public class Function {\n-  Exec exec;\n-  HashMap<String, FuncCommand> map = new HashMap<String, FuncCommand>();  \n-  HashMap<String, FuncSpecCommand> specMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, FuncSpecCommand> specSqlMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, HplsqlParser.Create_function_stmtContext> userMap = new HashMap<String, HplsqlParser.Create_function_stmtContext>();\n-  HashMap<String, HplsqlParser.Create_procedure_stmtContext> procMap = new HashMap<String, HplsqlParser.Create_procedure_stmtContext>();\n-  boolean trace = false; \n-  \n-  public Function(Exec e) {\n-    exec = e;  \n-    trace = exec.getTrace();\n-  }\n-  \n-  /** \n-   * Register functions\n-   */\n-  public void register(Function f) {    \n-  }\n-  \n-  /**\n-   * Execute a function\n-   */\n-  public void exec(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUser(name, ctx)) {\n-      return;\n-    }\n-    else if (isProc(name) && execProc(name, ctx, null)) {\n-      return;\n-    }\n-    if (name.indexOf(\".\") != -1) {               // Name can be qualified and spaces are allowed between parts\n-      String[] parts = name.split(\"\\\\.\");\n-      StringBuilder str = new StringBuilder();\n-      for (int i = 0; i < parts.length; i++) {\n-        if (i > 0) {\n-          str.append(\".\");\n-        }\n-        str.append(parts[i].trim());        \n-      }\n-      name = str.toString();      \n-    } \n-    if (trace && ctx != null && ctx.parent != null && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncCommand func = map.get(name.toUpperCase());    \n-    if (func != null) {\n-      func.run(ctx);\n-    }    \n-    else {\n-      info(ctx, \"Function not found: \" + name);\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * User-defined function in a SQL query\n-   */\n-  public void execSql(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUserSql(ctx, name)) {\n-      return;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(name);\n-    sql.append(\"(\");\n-    if (ctx != null) {\n-      int cnt = ctx.func_param().size();\n-      for (int i = 0; i < cnt; i++) {\n-        sql.append(evalPop(ctx.func_param(i).expr()));\n-        if (i + 1 < cnt) {\n-          sql.append(\", \");\n-        }\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-  }\n-  \n-  /**\n-   * Aggregate or window function in a SQL query\n-   */\n-  public void execAggWindowSql(HplsqlParser.Expr_agg_window_funcContext ctx) {\n-    exec.stackPush(exec.getFormattedText(ctx));\n-  }\n-  \n-  /**\n-   * Execute a user-defined function\n-   */\n-  public boolean execUser(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    if (trace) {\n-      trace(ctx, \"EXEC FUNCTION \" + name);\n-    }\n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    setCallParameters(ctx, actualParams, userCtx.create_routine_params(), null);\n-    if (userCtx.declare_block_inplace() != null) {\n-      visit(userCtx.declare_block_inplace());\n-    }\n-    visit(userCtx.single_block_stmt());\n-    exec.leaveScope(); \n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a HPL/SQL user-defined function in a query \n-   */\n-  public boolean execUserSql(HplsqlParser.Expr_func_paramsContext ctx, String name) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(\"hplsql('\");\n-    sql.append(name);\n-    sql.append(\"(\");\n-    int cnt = ctx.func_param().size();\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(\":\" + (i + 1));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")'\");\n-    if (cnt > 0) {\n-      sql.append(\", \");\n-    }\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(evalPop(ctx.func_param(i).expr()));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-    exec.registerUdf();\n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure as the entry point of the script (defined by -main option)\n-   */\n-  public boolean execProc(String name) {\n-    if (trace) {\n-      trace(\"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(\"Procedure not found\");\n-      return false;\n-    }    \n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(procCtx.create_routine_params());\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    return true;\n-  }\n-  \n-  /**\n-   * Check if the stored procedure with the specified name is defined\n-   */\n-  public boolean isProc(String name) {\n-    if (procMap.get(name.toUpperCase()) != null) {\n-      return true;\n-    }\n-    return false;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure using CALL or EXEC statement passing parameters\n-   */\n-  public boolean execProc(String name, HplsqlParser.Expr_func_paramsContext ctx, ParserRuleContext callCtx) {\n-    if (trace) {\n-      trace(callCtx, \"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(callCtx, \"Procedure not found\");\n-      return false;\n-    }    \n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    HashMap<String, Var> out = new HashMap<String, Var>();\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.declare_block_inplace() != null) {\n-      visit(procCtx.declare_block_inplace());\n-    }\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(ctx, actualParams, procCtx.create_routine_params(), out);\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    for (Map.Entry<String, Var> i : out.entrySet()) {      // Set OUT parameters\n-      exec.setVariable(i.getKey(), i.getValue());\n-    }\n-    return true;\n-  }\n-  \n-  /**\n-   * Set parameters for user-defined function call\n-   */\n-  public void setCallParameters(HplsqlParser.Expr_func_paramsContext actual, ArrayList<Var> actualValues, \n-                         HplsqlParser.Create_routine_paramsContext formal,\n-                         HashMap<String, Var> out) {\n-    if (actual == null || actual.func_param() == null || actualValues == null) {\n-      return;\n-    }\n-    int actualCnt = actualValues.size();\n-    int formalCnt = formal.create_routine_param_item().size();\n-    for (int i = 0; i < actualCnt; i++) {\n-      if (i >= formalCnt) {\n-        break;\n-      }\n-      HplsqlParser.ExprContext a = actual.func_param(i).expr(); \n-      HplsqlParser.Create_routine_param_itemContext p = getCallParameter(actual, formal, i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var var = setCallParameter(name, type, len, scale, actualValues.get(i));\n-      if (trace) {\n-        trace(actual, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      } \n-      if (out != null && a.expr_atom() != null && a.expr_atom().ident() != null &&\n-          (p.T_OUT() != null || p.T_INOUT() != null)) {\n-        String actualName = a.expr_atom().ident().getText();\n-        if (actualName != null) {\n-          out.put(actualName, var);  \n-        }         \n-      }\n-    }\n-  }\n-  \n-  /**\n-   * Set parameters for entry-point call (Main procedure defined by -main option)\n-   */\n-  void setCallParameters(HplsqlParser.Create_routine_paramsContext ctx) {\n-    int cnt = ctx.create_routine_param_item().size();\n-    for (int i = 0; i < cnt; i++) {\n-      HplsqlParser.Create_routine_param_itemContext p = ctx.create_routine_param_item(i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var value = exec.findVariable(name);\n-      Var var = setCallParameter(name, type, len, scale, value);\n-      if (trace) {\n-        trace(ctx, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      }      \n-    }\n-  }\n-  \n-  /**\n-   * Create a function or procedure parameter and set its value\n-   */\n-  Var setCallParameter(String name, String type, String len, String scale, Var value) {\n-    Var var = new Var(name, type, len, scale, null);\n-    var.cast(value);\n-    exec.addVariable(var);    \n-    return var;\n-  }\n-  \n-  /**\n-   * Get call parameter definition by name (if specified) or position\n-   */\n-  HplsqlParser.Create_routine_param_itemContext getCallParameter(HplsqlParser.Expr_func_paramsContext actual, \n-      HplsqlParser.Create_routine_paramsContext formal, int pos) {\n-    String named = null;\n-    int out_pos = pos;\n-    if (actual.func_param(pos).ident() != null) {\n-      named = actual.func_param(pos).ident().getText(); \n-      int cnt = formal.create_routine_param_item().size();\n-      for (int i = 0; i < cnt; i++) {\n-        if (named.equalsIgnoreCase(formal.create_routine_param_item(i).ident().getText())) {\n-          out_pos = i;\n-          break;\n-        }\n-      }\n-    }\n-    return formal.create_routine_param_item(out_pos);\n-  }  \n-  \n-  /**\n-   * Evaluate actual call parameters\n-   */\n-  public ArrayList<Var> getActualCallParameters(HplsqlParser.Expr_func_paramsContext actual) {\n-    if (actual == null || actual.func_param() == null) {\n-      return null;\n-    }\n-    int cnt = actual.func_param().size();\n-    ArrayList<Var> values = new ArrayList<Var>(cnt);\n-    for (int i = 0; i < cnt; i++) {\n-      values.add(evalPop(actual.func_param(i).expr()));\n-    }\n-    return values;\n-  }\n-  \n-  /**\n-   * Add a user-defined function\n-   */\n-  public void addUserFunction(HplsqlParser.Create_function_stmtContext ctx) {\n-    String name = ctx.ident().getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE FUNCTION \" + name);\n-    }\n-    userMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Add a user-defined procedure\n-   */\n-  public void addUserProcedure(HplsqlParser.Create_procedure_stmtContext ctx) {\n-    String name = ctx.ident(0).getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE PROCEDURE \" + name);\n-    }\n-    procMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Get the number of parameters in function call\n-   */\n-  public int getParamCount(HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (ctx == null) {\n-      return 0;\n-    }\n-    return ctx.func_param().size();\n-  }\n-    \n-  /**\n-   * Execute a special function\n-   */\n-  public void specExec(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else if(ctx.T_MAX_PART_STRING() != null) {\n-      execMaxPartString(ctx);\n-    } else if(ctx.T_MIN_PART_STRING() != null) {\n-      execMinPartString(ctx);\n-    } else if(ctx.T_MAX_PART_INT() != null) {\n-      execMaxPartInt(ctx);\n-    } else if(ctx.T_MIN_PART_INT() != null) {\n-      execMinPartInt(ctx);\n-    } else if(ctx.T_MAX_PART_DATE() != null) {\n-      execMaxPartDate(ctx);\n-    } else if(ctx.T_MIN_PART_DATE() != null) {\n-      execMinPartDate(ctx);\n-    } else if(ctx.T_PART_LOC() != null) {\n-      execPartLoc(ctx);\n-    } else {\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * Execute a special function in executable SQL statement\n-   */\n-  public void specExecSql(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specSqlMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else {\n-      exec.stackPush(exec.getFormattedText(ctx));\n-    }\n-  }\n-  \n-  /**\n-   * Get the current date\n-   */\n-  public void execCurrentDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"CURRENT_DATE\");\n-    }\n-    SimpleDateFormat f = new SimpleDateFormat(\"yyyy-MM-dd\");\n-    String s = f.format(Calendar.getInstance().getTime());\n-    exec.stackPush(new Var(Var.Type.DATE, Utils.toDate(s))); \n-  }\n-  \n-  /**\n-   * Execute MAX_PART_STRING function\n-   */\n-  public void execMaxPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_STRING function\n-   */\n-  public void execMinPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_INT function\n-   */\n-  public void execMaxPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_INT function\n-   */\n-  public void execMinPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_DATE function\n-   */\n-  public void execMaxPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_DATE function\n-   */\n-  public void execMinPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, false /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN or MAX partition function\n-   */\n-  public void execMinMaxPart(HplsqlParser.Expr_spec_funcContext ctx, Var.Type type, boolean max) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"SHOW PARTITIONS \" + tabname;    \n-    String colname = null;    \n-    int colnum = -1;\n-    int exprnum = ctx.expr().size();    \n-    // Column name \n-    if (ctx.expr(1) != null) {\n-      colname = evalPop(ctx.expr(1)).toString();\n-    } else {\n-      colnum = 0;\n-    }\n-    // Partition filter\n-    if (exprnum >= 4) {\n-      sql += \" PARTITION (\";\n-      int i = 2;\n-      while (i + 1 < exprnum) {\n-        String fcol = evalPop(ctx.expr(i)).toString();\n-        String fval = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += fcol + \"=\" + fval;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      String resultString = null;\n-      Long resultInt = null;\n-      Date resultDate = null;      \n-      while (rs.next()) {\n-        String[] parts = rs.getString(1).split(\"/\");\n-        // Find partition column by name\n-        if (colnum == -1) {\n-          for (int i = 0; i < parts.length; i++) {\n-            String[] name = parts[i].split(\"=\");\n-            if (name[0].equalsIgnoreCase(colname)) {\n-              colnum = i;\n-              break;\n-            }\n-          }\n-          // No partition column with the specified name exists\n-          if (colnum == -1) {\n-            evalNullClose(query, exec.conf.defaultConnection);\n-            return;\n-          }\n-        }\n-        String[] pair = parts[colnum].split(\"=\");\n-        if (type == Var.Type.STRING) {\n-          resultString = Utils.minMaxString(resultString, pair[1], max);          \n-        } \n-        else if (type == Var.Type.BIGINT) {\n-          resultInt = Utils.minMaxInt(resultInt, pair[1], max);          \n-        } \n-        else if (type == Var.Type.DATE) {\n-          resultDate = Utils.minMaxDate(resultDate, pair[1], max);\n-        }\n-      }\n-      if (resultString != null) {\n-        evalString(resultString);\n-      } \n-      else if (resultInt != null) {\n-        evalInt(resultInt);\n-      } \n-      else if (resultDate != null) {\n-        evalDate(resultDate);\n-      } \n-      else {\n-        evalNull();\n-      }\n-    } catch (SQLException e) {}  \n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Execute PART_LOC function\n-   */\n-  public void execPartLoc(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"DESCRIBE EXTENDED \" + tabname;    \n-    int exprnum = ctx.expr().size();   \n-    boolean hostname = false;\n-    // Partition filter\n-    if (exprnum > 1) {\n-      sql += \" PARTITION (\";\n-      int i = 1;\n-      while (i + 1 < exprnum) {\n-        String col = evalPop(ctx.expr(i)).toString();\n-        String val = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += col + \"=\" + val;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    // With host name\n-    if (exprnum % 2 == 0 && evalPop(ctx.expr(exprnum - 1)).intValue() == 1) {\n-      hostname = true;\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    String result = null;\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      while (rs.next()) {\n-        if (rs.getString(1).startsWith(\"Detailed Partition Information\")) {\n-          Matcher m = Pattern.compile(\".*, location:(.*?),.*\").matcher(rs.getString(2));\n-          if (m.find()) {\n-            result = m.group(1);\n-          }    \n-        }\n-      }\n-    } catch (SQLException e) {}  \n-    if (result != null) {\n-      // Remove the host name\n-      if (!hostname) {\n-        Matcher m = Pattern.compile(\".*://.*?(/.*)\").matcher(result); \n-        if (m.find()) {\n-          result = m.group(1);\n-        }\n-      }\n-      evalString(result);\n-    }    \n-    else {\n-      evalNull();\n-    }\n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Evaluate the expression and push the value to the stack\n-   */\n-  void eval(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-  }\n-\n-  /**\n-   * Evaluate the expression to the specified variable\n-   */\n-  void evalVar(Var var) {\n-    exec.stackPush(var); \n-  }\n-\n-  /**\n-   * Evaluate the expression to NULL\n-   */\n-  void evalNull() {\n-    exec.stackPush(Var.Null); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified String value\n-   */\n-  void evalString(String string) {\n-    exec.stackPush(new Var(string)); \n-  }\n-  \n-  void evalString(StringBuilder string) {\n-    evalString(string.toString()); \n-  }\n-\n-  /**\n-   * Evaluate the expression to specified Int value\n-   */\n-  void evalInt(Long i) {\n-    exec.stackPush(new Var(i)); \n-  }\n-  \n-  void evalInt(int i) {\n-    evalInt(Long.valueOf(i));\n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified Date value\n-   */\n-  void evalDate(Date date) {\n-    exec.stackPush(new Var(Var.Type.DATE, date)); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to NULL and close the query\n-   */\n-  void evalNullClose(Query query, String conn) {\n-    exec.stackPush(Var.Null); \n-    exec.closeQuery(query, conn);\n-    if(trace) {\n-      query.printStackTrace();\n-    }\n-  }\n-  \n-  /**\n-   * Evaluate the expression and pop value from the stack\n-   */\n-  Var evalPop(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-    return exec.stackPop();  \n-  }\n-  \n-  Var evalPop(ParserRuleContext ctx, int value) {\n-    if (ctx != null) {\n-      return evalPop(ctx);\n-    }\n-    return new Var(Long.valueOf(value));\n-  }\n-  \n-  /**\n-   * Execute rules\n-   */\n-  Integer visit(ParserRuleContext ctx) {\n-    return exec.visit(ctx);  \n-  } \n- \n-  /**\n-   * Execute children rules\n-   */\n-  Integer visitChildren(ParserRuleContext ctx) {\n-    return exec.visitChildren(ctx);  \n-  }  \n-  \n-  /**\n-   * Trace information\n-   */\n-  public void trace(ParserRuleContext ctx, String message) {\n-    if (trace) {\n-      exec.trace(ctx, message);\n-    }\n-  }\n-  \n-  public void trace(String message) {\n-    trace(null, message);\n-  }\n-  \n-  public void info(ParserRuleContext ctx, String message) {\n-    exec.info(ctx, message);\n-  }\n+public interface Function {\n+  boolean exec(String name, HplsqlParser.Expr_func_paramsContext ctx);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 797}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5MDgxMQ==", "bodyText": "what does exists mean? an implementation of this interface is a Function - which should have a name ; so a getName would probably fit better", "url": "https://github.com/apache/hive/pull/1542#discussion_r511990811", "createdAt": "2020-10-26T14:13:21Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/Function.java", "diffHunk": "@@ -1,780 +1,30 @@\n /*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n  *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n  */\n \n package org.apache.hive.hplsql.functions;\n \n-import java.sql.ResultSet;\n-import java.sql.Date;\n-import java.sql.SQLException;\n-import java.text.SimpleDateFormat;\n-import java.util.ArrayList;\n-import java.util.Calendar;\n-import java.util.HashMap;\n-import java.util.Map;\n-import java.util.TimeZone;\n-import java.util.regex.Matcher;\n-import java.util.regex.Pattern;\n+import org.apache.hive.hplsql.HplsqlParser;\n \n-import org.apache.commons.lang3.StringUtils;\n-import org.antlr.v4.runtime.ParserRuleContext;\n-import org.apache.hive.hplsql.*;\n-\n-interface FuncCommand {\n-  void run(HplsqlParser.Expr_func_paramsContext ctx);\n-}\n-\n-interface FuncSpecCommand {\n-  void run(HplsqlParser.Expr_spec_funcContext ctx);\n-}\n-\n-/**\n- * HPL/SQL functions\n- */\n-public class Function {\n-  Exec exec;\n-  HashMap<String, FuncCommand> map = new HashMap<String, FuncCommand>();  \n-  HashMap<String, FuncSpecCommand> specMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, FuncSpecCommand> specSqlMap = new HashMap<String, FuncSpecCommand>();\n-  HashMap<String, HplsqlParser.Create_function_stmtContext> userMap = new HashMap<String, HplsqlParser.Create_function_stmtContext>();\n-  HashMap<String, HplsqlParser.Create_procedure_stmtContext> procMap = new HashMap<String, HplsqlParser.Create_procedure_stmtContext>();\n-  boolean trace = false; \n-  \n-  public Function(Exec e) {\n-    exec = e;  \n-    trace = exec.getTrace();\n-  }\n-  \n-  /** \n-   * Register functions\n-   */\n-  public void register(Function f) {    \n-  }\n-  \n-  /**\n-   * Execute a function\n-   */\n-  public void exec(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUser(name, ctx)) {\n-      return;\n-    }\n-    else if (isProc(name) && execProc(name, ctx, null)) {\n-      return;\n-    }\n-    if (name.indexOf(\".\") != -1) {               // Name can be qualified and spaces are allowed between parts\n-      String[] parts = name.split(\"\\\\.\");\n-      StringBuilder str = new StringBuilder();\n-      for (int i = 0; i < parts.length; i++) {\n-        if (i > 0) {\n-          str.append(\".\");\n-        }\n-        str.append(parts[i].trim());        \n-      }\n-      name = str.toString();      \n-    } \n-    if (trace && ctx != null && ctx.parent != null && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncCommand func = map.get(name.toUpperCase());    \n-    if (func != null) {\n-      func.run(ctx);\n-    }    \n-    else {\n-      info(ctx, \"Function not found: \" + name);\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * User-defined function in a SQL query\n-   */\n-  public void execSql(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (execUserSql(ctx, name)) {\n-      return;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(name);\n-    sql.append(\"(\");\n-    if (ctx != null) {\n-      int cnt = ctx.func_param().size();\n-      for (int i = 0; i < cnt; i++) {\n-        sql.append(evalPop(ctx.func_param(i).expr()));\n-        if (i + 1 < cnt) {\n-          sql.append(\", \");\n-        }\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-  }\n-  \n-  /**\n-   * Aggregate or window function in a SQL query\n-   */\n-  public void execAggWindowSql(HplsqlParser.Expr_agg_window_funcContext ctx) {\n-    exec.stackPush(exec.getFormattedText(ctx));\n-  }\n-  \n-  /**\n-   * Execute a user-defined function\n-   */\n-  public boolean execUser(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    if (trace) {\n-      trace(ctx, \"EXEC FUNCTION \" + name);\n-    }\n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    setCallParameters(ctx, actualParams, userCtx.create_routine_params(), null);\n-    if (userCtx.declare_block_inplace() != null) {\n-      visit(userCtx.declare_block_inplace());\n-    }\n-    visit(userCtx.single_block_stmt());\n-    exec.leaveScope(); \n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a HPL/SQL user-defined function in a query \n-   */\n-  public boolean execUserSql(HplsqlParser.Expr_func_paramsContext ctx, String name) {\n-    HplsqlParser.Create_function_stmtContext userCtx = userMap.get(name.toUpperCase());\n-    if (userCtx == null) {\n-      return false;\n-    }\n-    StringBuilder sql = new StringBuilder();\n-    sql.append(\"hplsql('\");\n-    sql.append(name);\n-    sql.append(\"(\");\n-    int cnt = ctx.func_param().size();\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(\":\" + (i + 1));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")'\");\n-    if (cnt > 0) {\n-      sql.append(\", \");\n-    }\n-    for (int i = 0; i < cnt; i++) {\n-      sql.append(evalPop(ctx.func_param(i).expr()));\n-      if (i + 1 < cnt) {\n-        sql.append(\", \");\n-      }\n-    }\n-    sql.append(\")\");\n-    exec.stackPush(sql);\n-    exec.registerUdf();\n-    return true;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure as the entry point of the script (defined by -main option)\n-   */\n-  public boolean execProc(String name) {\n-    if (trace) {\n-      trace(\"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(\"Procedure not found\");\n-      return false;\n-    }    \n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(procCtx.create_routine_params());\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    return true;\n-  }\n-  \n-  /**\n-   * Check if the stored procedure with the specified name is defined\n-   */\n-  public boolean isProc(String name) {\n-    if (procMap.get(name.toUpperCase()) != null) {\n-      return true;\n-    }\n-    return false;\n-  }\n-  \n-  /**\n-   * Execute a stored procedure using CALL or EXEC statement passing parameters\n-   */\n-  public boolean execProc(String name, HplsqlParser.Expr_func_paramsContext ctx, ParserRuleContext callCtx) {\n-    if (trace) {\n-      trace(callCtx, \"EXEC PROCEDURE \" + name);\n-    }\n-    HplsqlParser.Create_procedure_stmtContext procCtx = procMap.get(name.toUpperCase());    \n-    if (procCtx == null) {\n-      trace(callCtx, \"Procedure not found\");\n-      return false;\n-    }    \n-    ArrayList<Var> actualParams = getActualCallParameters(ctx);\n-    HashMap<String, Var> out = new HashMap<String, Var>();\n-    exec.enterScope(Scope.Type.ROUTINE);\n-    exec.callStackPush(name);\n-    if (procCtx.declare_block_inplace() != null) {\n-      visit(procCtx.declare_block_inplace());\n-    }\n-    if (procCtx.create_routine_params() != null) {\n-      setCallParameters(ctx, actualParams, procCtx.create_routine_params(), out);\n-    }\n-    visit(procCtx.proc_block());\n-    exec.callStackPop();\n-    exec.leaveScope();       \n-    for (Map.Entry<String, Var> i : out.entrySet()) {      // Set OUT parameters\n-      exec.setVariable(i.getKey(), i.getValue());\n-    }\n-    return true;\n-  }\n-  \n-  /**\n-   * Set parameters for user-defined function call\n-   */\n-  public void setCallParameters(HplsqlParser.Expr_func_paramsContext actual, ArrayList<Var> actualValues, \n-                         HplsqlParser.Create_routine_paramsContext formal,\n-                         HashMap<String, Var> out) {\n-    if (actual == null || actual.func_param() == null || actualValues == null) {\n-      return;\n-    }\n-    int actualCnt = actualValues.size();\n-    int formalCnt = formal.create_routine_param_item().size();\n-    for (int i = 0; i < actualCnt; i++) {\n-      if (i >= formalCnt) {\n-        break;\n-      }\n-      HplsqlParser.ExprContext a = actual.func_param(i).expr(); \n-      HplsqlParser.Create_routine_param_itemContext p = getCallParameter(actual, formal, i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var var = setCallParameter(name, type, len, scale, actualValues.get(i));\n-      if (trace) {\n-        trace(actual, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      } \n-      if (out != null && a.expr_atom() != null && a.expr_atom().ident() != null &&\n-          (p.T_OUT() != null || p.T_INOUT() != null)) {\n-        String actualName = a.expr_atom().ident().getText();\n-        if (actualName != null) {\n-          out.put(actualName, var);  \n-        }         \n-      }\n-    }\n-  }\n-  \n-  /**\n-   * Set parameters for entry-point call (Main procedure defined by -main option)\n-   */\n-  void setCallParameters(HplsqlParser.Create_routine_paramsContext ctx) {\n-    int cnt = ctx.create_routine_param_item().size();\n-    for (int i = 0; i < cnt; i++) {\n-      HplsqlParser.Create_routine_param_itemContext p = ctx.create_routine_param_item(i);\n-      String name = p.ident().getText();\n-      String type = p.dtype().getText();\n-      String len = null;\n-      String scale = null;   \n-      if (p.dtype_len() != null) {\n-        len = p.dtype_len().L_INT(0).getText();\n-        if (p.dtype_len().L_INT(1) != null) {\n-          scale = p.dtype_len().L_INT(1).getText();\n-        }\n-      }\n-      Var value = exec.findVariable(name);\n-      Var var = setCallParameter(name, type, len, scale, value);\n-      if (trace) {\n-        trace(ctx, \"SET PARAM \" + name + \" = \" + var.toString());      \n-      }      \n-    }\n-  }\n-  \n-  /**\n-   * Create a function or procedure parameter and set its value\n-   */\n-  Var setCallParameter(String name, String type, String len, String scale, Var value) {\n-    Var var = new Var(name, type, len, scale, null);\n-    var.cast(value);\n-    exec.addVariable(var);    \n-    return var;\n-  }\n-  \n-  /**\n-   * Get call parameter definition by name (if specified) or position\n-   */\n-  HplsqlParser.Create_routine_param_itemContext getCallParameter(HplsqlParser.Expr_func_paramsContext actual, \n-      HplsqlParser.Create_routine_paramsContext formal, int pos) {\n-    String named = null;\n-    int out_pos = pos;\n-    if (actual.func_param(pos).ident() != null) {\n-      named = actual.func_param(pos).ident().getText(); \n-      int cnt = formal.create_routine_param_item().size();\n-      for (int i = 0; i < cnt; i++) {\n-        if (named.equalsIgnoreCase(formal.create_routine_param_item(i).ident().getText())) {\n-          out_pos = i;\n-          break;\n-        }\n-      }\n-    }\n-    return formal.create_routine_param_item(out_pos);\n-  }  \n-  \n-  /**\n-   * Evaluate actual call parameters\n-   */\n-  public ArrayList<Var> getActualCallParameters(HplsqlParser.Expr_func_paramsContext actual) {\n-    if (actual == null || actual.func_param() == null) {\n-      return null;\n-    }\n-    int cnt = actual.func_param().size();\n-    ArrayList<Var> values = new ArrayList<Var>(cnt);\n-    for (int i = 0; i < cnt; i++) {\n-      values.add(evalPop(actual.func_param(i).expr()));\n-    }\n-    return values;\n-  }\n-  \n-  /**\n-   * Add a user-defined function\n-   */\n-  public void addUserFunction(HplsqlParser.Create_function_stmtContext ctx) {\n-    String name = ctx.ident().getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE FUNCTION \" + name);\n-    }\n-    userMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Add a user-defined procedure\n-   */\n-  public void addUserProcedure(HplsqlParser.Create_procedure_stmtContext ctx) {\n-    String name = ctx.ident(0).getText();\n-    if (trace) {\n-      trace(ctx, \"CREATE PROCEDURE \" + name);\n-    }\n-    procMap.put(name.toUpperCase(), ctx);\n-  }\n-  \n-  /**\n-   * Get the number of parameters in function call\n-   */\n-  public int getParamCount(HplsqlParser.Expr_func_paramsContext ctx) {\n-    if (ctx == null) {\n-      return 0;\n-    }\n-    return ctx.func_param().size();\n-  }\n-    \n-  /**\n-   * Execute a special function\n-   */\n-  public void specExec(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else if(ctx.T_MAX_PART_STRING() != null) {\n-      execMaxPartString(ctx);\n-    } else if(ctx.T_MIN_PART_STRING() != null) {\n-      execMinPartString(ctx);\n-    } else if(ctx.T_MAX_PART_INT() != null) {\n-      execMaxPartInt(ctx);\n-    } else if(ctx.T_MIN_PART_INT() != null) {\n-      execMinPartInt(ctx);\n-    } else if(ctx.T_MAX_PART_DATE() != null) {\n-      execMaxPartDate(ctx);\n-    } else if(ctx.T_MIN_PART_DATE() != null) {\n-      execMinPartDate(ctx);\n-    } else if(ctx.T_PART_LOC() != null) {\n-      execPartLoc(ctx);\n-    } else {\n-      evalNull();\n-    }\n-  }\n-  \n-  /**\n-   * Execute a special function in executable SQL statement\n-   */\n-  public void specExecSql(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String name = ctx.start.getText().toUpperCase();\n-    if (trace && ctx.parent.parent instanceof HplsqlParser.Expr_stmtContext) {\n-      trace(ctx, \"FUNC \" + name);      \n-    }\n-    FuncSpecCommand func = specSqlMap.get(name);    \n-    if (func != null) {\n-      func.run(ctx);\n-    }\n-    else {\n-      exec.stackPush(exec.getFormattedText(ctx));\n-    }\n-  }\n-  \n-  /**\n-   * Get the current date\n-   */\n-  public void execCurrentDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"CURRENT_DATE\");\n-    }\n-    SimpleDateFormat f = new SimpleDateFormat(\"yyyy-MM-dd\");\n-    String s = f.format(Calendar.getInstance().getTime());\n-    exec.stackPush(new Var(Var.Type.DATE, Utils.toDate(s))); \n-  }\n-  \n-  /**\n-   * Execute MAX_PART_STRING function\n-   */\n-  public void execMaxPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_STRING function\n-   */\n-  public void execMinPartString(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_STRING\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.STRING, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_INT function\n-   */\n-  public void execMaxPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_INT function\n-   */\n-  public void execMinPartInt(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_INT\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.BIGINT, false /*max*/);\n-  }\n-\n-  /**\n-   * Execute MAX_PART_DATE function\n-   */\n-  public void execMaxPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MAX_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, true /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN_PART_DATE function\n-   */\n-  public void execMinPartDate(HplsqlParser.Expr_spec_funcContext ctx) {\n-    if(trace) {\n-      trace(ctx, \"MIN_PART_DATE\");\n-    }\n-    execMinMaxPart(ctx, Var.Type.DATE, false /*max*/);\n-  }\n-  \n-  /**\n-   * Execute MIN or MAX partition function\n-   */\n-  public void execMinMaxPart(HplsqlParser.Expr_spec_funcContext ctx, Var.Type type, boolean max) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"SHOW PARTITIONS \" + tabname;    \n-    String colname = null;    \n-    int colnum = -1;\n-    int exprnum = ctx.expr().size();    \n-    // Column name \n-    if (ctx.expr(1) != null) {\n-      colname = evalPop(ctx.expr(1)).toString();\n-    } else {\n-      colnum = 0;\n-    }\n-    // Partition filter\n-    if (exprnum >= 4) {\n-      sql += \" PARTITION (\";\n-      int i = 2;\n-      while (i + 1 < exprnum) {\n-        String fcol = evalPop(ctx.expr(i)).toString();\n-        String fval = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += fcol + \"=\" + fval;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      String resultString = null;\n-      Long resultInt = null;\n-      Date resultDate = null;      \n-      while (rs.next()) {\n-        String[] parts = rs.getString(1).split(\"/\");\n-        // Find partition column by name\n-        if (colnum == -1) {\n-          for (int i = 0; i < parts.length; i++) {\n-            String[] name = parts[i].split(\"=\");\n-            if (name[0].equalsIgnoreCase(colname)) {\n-              colnum = i;\n-              break;\n-            }\n-          }\n-          // No partition column with the specified name exists\n-          if (colnum == -1) {\n-            evalNullClose(query, exec.conf.defaultConnection);\n-            return;\n-          }\n-        }\n-        String[] pair = parts[colnum].split(\"=\");\n-        if (type == Var.Type.STRING) {\n-          resultString = Utils.minMaxString(resultString, pair[1], max);          \n-        } \n-        else if (type == Var.Type.BIGINT) {\n-          resultInt = Utils.minMaxInt(resultInt, pair[1], max);          \n-        } \n-        else if (type == Var.Type.DATE) {\n-          resultDate = Utils.minMaxDate(resultDate, pair[1], max);\n-        }\n-      }\n-      if (resultString != null) {\n-        evalString(resultString);\n-      } \n-      else if (resultInt != null) {\n-        evalInt(resultInt);\n-      } \n-      else if (resultDate != null) {\n-        evalDate(resultDate);\n-      } \n-      else {\n-        evalNull();\n-      }\n-    } catch (SQLException e) {}  \n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Execute PART_LOC function\n-   */\n-  public void execPartLoc(HplsqlParser.Expr_spec_funcContext ctx) {\n-    String tabname = evalPop(ctx.expr(0)).toString();\n-    String sql = \"DESCRIBE EXTENDED \" + tabname;    \n-    int exprnum = ctx.expr().size();   \n-    boolean hostname = false;\n-    // Partition filter\n-    if (exprnum > 1) {\n-      sql += \" PARTITION (\";\n-      int i = 1;\n-      while (i + 1 < exprnum) {\n-        String col = evalPop(ctx.expr(i)).toString();\n-        String val = evalPop(ctx.expr(i+1)).toSqlString();\n-        if (i > 2) {\n-          sql += \", \";\n-        }\n-        sql += col + \"=\" + val;        \n-        i += 2;\n-      }\n-      sql += \")\";\n-    }\n-    // With host name\n-    if (exprnum % 2 == 0 && evalPop(ctx.expr(exprnum - 1)).intValue() == 1) {\n-      hostname = true;\n-    }\n-    if (trace) {\n-      trace(ctx, \"Query: \" + sql);\n-    }\n-    if (exec.getOffline()) {\n-      evalNull();\n-      return;\n-    }\n-    Query query = exec.executeQuery(ctx, sql, exec.conf.defaultConnection);\n-    if (query.error()) {\n-      evalNullClose(query, exec.conf.defaultConnection);\n-      return;\n-    }\n-    String result = null;\n-    ResultSet rs = query.getResultSet();\n-    try {\n-      while (rs.next()) {\n-        if (rs.getString(1).startsWith(\"Detailed Partition Information\")) {\n-          Matcher m = Pattern.compile(\".*, location:(.*?),.*\").matcher(rs.getString(2));\n-          if (m.find()) {\n-            result = m.group(1);\n-          }    \n-        }\n-      }\n-    } catch (SQLException e) {}  \n-    if (result != null) {\n-      // Remove the host name\n-      if (!hostname) {\n-        Matcher m = Pattern.compile(\".*://.*?(/.*)\").matcher(result); \n-        if (m.find()) {\n-          result = m.group(1);\n-        }\n-      }\n-      evalString(result);\n-    }    \n-    else {\n-      evalNull();\n-    }\n-    exec.closeQuery(query, exec.conf.defaultConnection);\n-  }\n-  \n-  /**\n-   * Evaluate the expression and push the value to the stack\n-   */\n-  void eval(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-  }\n-\n-  /**\n-   * Evaluate the expression to the specified variable\n-   */\n-  void evalVar(Var var) {\n-    exec.stackPush(var); \n-  }\n-\n-  /**\n-   * Evaluate the expression to NULL\n-   */\n-  void evalNull() {\n-    exec.stackPush(Var.Null); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified String value\n-   */\n-  void evalString(String string) {\n-    exec.stackPush(new Var(string)); \n-  }\n-  \n-  void evalString(StringBuilder string) {\n-    evalString(string.toString()); \n-  }\n-\n-  /**\n-   * Evaluate the expression to specified Int value\n-   */\n-  void evalInt(Long i) {\n-    exec.stackPush(new Var(i)); \n-  }\n-  \n-  void evalInt(int i) {\n-    evalInt(Long.valueOf(i));\n-  }\n-  \n-  /**\n-   * Evaluate the expression to specified Date value\n-   */\n-  void evalDate(Date date) {\n-    exec.stackPush(new Var(Var.Type.DATE, date)); \n-  }\n-  \n-  /**\n-   * Evaluate the expression to NULL and close the query\n-   */\n-  void evalNullClose(Query query, String conn) {\n-    exec.stackPush(Var.Null); \n-    exec.closeQuery(query, conn);\n-    if(trace) {\n-      query.printStackTrace();\n-    }\n-  }\n-  \n-  /**\n-   * Evaluate the expression and pop value from the stack\n-   */\n-  Var evalPop(ParserRuleContext ctx) {\n-    exec.visit(ctx);\n-    return exec.stackPop();  \n-  }\n-  \n-  Var evalPop(ParserRuleContext ctx, int value) {\n-    if (ctx != null) {\n-      return evalPop(ctx);\n-    }\n-    return new Var(Long.valueOf(value));\n-  }\n-  \n-  /**\n-   * Execute rules\n-   */\n-  Integer visit(ParserRuleContext ctx) {\n-    return exec.visit(ctx);  \n-  } \n- \n-  /**\n-   * Execute children rules\n-   */\n-  Integer visitChildren(ParserRuleContext ctx) {\n-    return exec.visitChildren(ctx);  \n-  }  \n-  \n-  /**\n-   * Trace information\n-   */\n-  public void trace(ParserRuleContext ctx, String message) {\n-    if (trace) {\n-      exec.trace(ctx, message);\n-    }\n-  }\n-  \n-  public void trace(String message) {\n-    trace(null, message);\n-  }\n-  \n-  public void info(ParserRuleContext ctx, String message) {\n-    exec.info(ctx, message);\n-  }\n+public interface Function {\n+  boolean exec(String name, HplsqlParser.Expr_func_paramsContext ctx);\n+  void addUserFunction(HplsqlParser.Create_function_stmtContext ctx);\n+  void addUserProcedure(HplsqlParser.Create_procedure_stmtContext ctx);\n+  boolean exists(String name);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 800}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5NDI4Mg==", "bodyText": "do we really need to define these function differently than others; I've taken a look at MIN_PART_STRING and it seenms like its an ordinary function...so it could probably use the registry way approach", "url": "https://github.com/apache/hive/pull/1542#discussion_r511994282", "createdAt": "2020-10-26T14:18:05Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/BuiltinFunctions.java", "diffHunk": "@@ -0,0 +1,435 @@\n+/*\n+ *\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hive.hplsql.functions;\n+\n+import java.sql.Date;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Calendar;\n+import java.util.HashMap;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import org.antlr.v4.runtime.ParserRuleContext;\n+import org.apache.hive.hplsql.Exec;\n+import org.apache.hive.hplsql.HplsqlParser;\n+import org.apache.hive.hplsql.Query;\n+import org.apache.hive.hplsql.Utils;\n+import org.apache.hive.hplsql.Var;\n+\n+public class BuiltinFunctions {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5NzMzNg==", "bodyText": "I think this class should implement the Function interface and not extends a class which has a name which suggest that it's a \"container of functions\"", "url": "https://github.com/apache/hive/pull/1542#discussion_r511997336", "createdAt": "2020-10-26T14:22:06Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/FunctionDatetime.java", "diffHunk": "@@ -27,7 +27,7 @@\n import org.apache.commons.lang3.StringUtils;\n import org.apache.hive.hplsql.*;\n \n-public class FunctionDatetime extends Function {\n+public class FunctionDatetime extends BuiltinFunctions {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTk5NzY4Mg==", "bodyText": "note: registrator exposes it's internals", "url": "https://github.com/apache/hive/pull/1542#discussion_r511997682", "createdAt": "2020-10-26T14:22:35Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/FunctionDatetime.java", "diffHunk": "@@ -36,20 +36,20 @@ public FunctionDatetime(Exec e) {\n    * Register functions\n    */\n   @Override\n-  public void register(Function f) {\n-    f.map.put(\"DATE\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { date(ctx); }});\n-    f.map.put(\"FROM_UNIXTIME\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { fromUnixtime(ctx); }});\n-    f.map.put(\"NOW\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { now(ctx); }});\n-    f.map.put(\"TIMESTAMP_ISO\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { timestampIso(ctx); }});\n-    f.map.put(\"TO_TIMESTAMP\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { toTimestamp(ctx); }});\n-    f.map.put(\"UNIX_TIMESTAMP\", new FuncCommand() { public void run(HplsqlParser.Expr_func_paramsContext ctx) { unixTimestamp(ctx); }});\n+  public void register(BuiltinFunctions f) {\n+    f.map.put(\"DATE\", this::date);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAwMzk4NA==", "bodyText": "I think Function-s should be independent from where they are defined (builtin or hms) - and there should be some kind of registry which knows about all the functions.\nI feel that the Function interface is not really intuituve - its more like a cointainer of functions or something like that.\nRight now this class seem to contain some parts of a registry; which could also do chaining - plus  the registration and execution logic for functions defined in the metastore.\nIt seems to me that there is no real registry right now - if there would be one then I think this class could be some kind of \"factory of functions\" ?", "url": "https://github.com/apache/hive/pull/1542#discussion_r512003984", "createdAt": "2020-10-26T14:30:24Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/functions/HmsFunction.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ *\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hive.hplsql.functions;\n+\n+import static org.apache.hive.hplsql.functions.InMemoryFunction.setCallParameters;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+\n+import org.antlr.v4.runtime.ANTLRInputStream;\n+import org.antlr.v4.runtime.CommonTokenStream;\n+import org.antlr.v4.runtime.ParserRuleContext;\n+import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n+import org.apache.hadoop.hive.metastore.api.Database;\n+import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n+import org.apache.hadoop.hive.metastore.api.StoredProcedure;\n+import org.apache.hadoop.hive.metastore.api.StoredProcedureRequest;\n+import org.apache.hive.hplsql.Exec;\n+import org.apache.hive.hplsql.HplsqlBaseVisitor;\n+import org.apache.hive.hplsql.HplsqlLexer;\n+import org.apache.hive.hplsql.HplsqlParser;\n+import org.apache.hive.hplsql.Scope;\n+import org.apache.hive.hplsql.Var;\n+import org.apache.thrift.TException;\n+\n+public class HmsFunction implements Function {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxMDU2Mg==", "bodyText": "is this an unused class?", "url": "https://github.com/apache/hive/pull/1542#discussion_r512010562", "createdAt": "2020-10-26T14:38:31Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/model/MPosParam.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/*\n+ *\n+ *  * Licensed to the Apache Software Foundation (ASF) under one\n+ *  * or more contributor license agreements.  See the NOTICE file\n+ *  * distributed with this work for additional information\n+ *  * regarding copyright ownership.  The ASF licenses this file\n+ *  * to you under the Apache License, Version 2.0 (the\n+ *  * \"License\"); you may not use this file except in compliance\n+ *  * with the License.  You may obtain a copy of the License at\n+ *  *\n+ *  *     http://www.apache.org/licenses/LICENSE-2.0\n+ *  *\n+ *  * Unless required by applicable law or agreed to in writing, software\n+ *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  * See the License for the specific language governing permissions and\n+ *  * limitations under the License.\n+ *\n+ */\n+\n+package org.apache.hadoop.hive.metastore.model;\n+\n+public class MPosParam {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAxNzI1Mg==", "bodyText": "right now I'm wondering how much this differs from the MFunction stuff; is it differ enough to have a separate table?", "url": "https://github.com/apache/hive/pull/1542#discussion_r512017252", "createdAt": "2020-10-26T14:46:55Z", "author": {"login": "kgyrtkirk"}, "path": "standalone-metastore/metastore-server/src/main/resources/package.jdo", "diffHunk": "@@ -1549,6 +1549,31 @@\n         <column name=\"RM_DUMP_EXECUTION_ID\"/>\n       </index>\n     </class>\n+\n+    <class name=\"MStoredProc\" table=\"STORED_PROCS\" identity-type=\"datastore\" detachable=\"true\">", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyMzg4OQ==", "bodyText": "will this escaping be enough in all cases?", "url": "https://github.com/apache/hive/pull/1542#discussion_r512023889", "createdAt": "2020-10-26T14:55:07Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/Exec.java", "diffHunk": "@@ -1659,13 +1665,70 @@ public Integer visitExpr_func(HplsqlParser.Expr_funcContext ctx) {\n     }\n     return 0;\n   }\n-  \n+\n+  /**\n+   * User-defined function in a SQL query\n+   */\n+  public void execSql(String name, HplsqlParser.Expr_func_paramsContext ctx) {\n+    if (execUserSql(ctx, name)) {\n+      return;\n+    }\n+    StringBuilder sql = new StringBuilder();\n+    sql.append(name);\n+    sql.append(\"(\");\n+    if (ctx != null) {\n+      int cnt = ctx.func_param().size();\n+      for (int i = 0; i < cnt; i++) {\n+        sql.append(evalPop(ctx.func_param(i).expr()));\n+        if (i + 1 < cnt) {\n+          sql.append(\", \");\n+        }\n+      }\n+    }\n+    sql.append(\")\");\n+    exec.stackPush(sql);\n+  }\n+\n+  /**\n+   * Execute a HPL/SQL user-defined function in a query\n+   */\n+  private boolean execUserSql(HplsqlParser.Expr_func_paramsContext ctx, String name) {\n+    if (!function.exists(name.toUpperCase())) {\n+      return false;\n+    }\n+    StringBuilder sql = new StringBuilder();\n+    sql.append(\"hplsql('\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 352}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjAyNzcyOA==", "bodyText": "typo: hplsq\nnote: this getMsc() method could be placed closer to the Hms related stuff", "url": "https://github.com/apache/hive/pull/1542#discussion_r512027728", "createdAt": "2020-10-26T14:59:37Z", "author": {"login": "kgyrtkirk"}, "path": "hplsql/src/main/java/org/apache/hive/hplsql/Exec.java", "diffHunk": "@@ -799,30 +801,35 @@ Integer init(String[] args) throws Exception {\n     select = new Select(this);\n     stmt = new Stmt(this);\n     converter = new Converter(this);\n-        \n-    function = new Function(this);\n-    new FunctionDatetime(this).register(function);\n-    new FunctionMisc(this).register(function);\n-    new FunctionString(this).register(function);\n-    new FunctionOra(this).register(function);\n+\n+    builtinFunctions = new BuiltinFunctions(this);\n+    new FunctionDatetime(this).register(builtinFunctions);\n+    new FunctionMisc(this).register(builtinFunctions);\n+    new FunctionString(this).register(builtinFunctions);\n+    new FunctionOra(this).register(builtinFunctions);\n+    if (\"hms\".equalsIgnoreCase(System.getProperty(\"hplsql.storage\"))) {\n+      function = new HmsFunction(this, getMsc(System.getProperty(\"hplsq.metastore.uris\", \"thrift://localhost:9083\")), builtinFunctions);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11d255130a596dc0e8c81c5fe4bdd7b5618f1f89"}, "originalPosition": 214}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e8719c0ab3431625706dd9e902f11d87a26a7f0a", "author": {"user": {"login": "zeroflag", "name": "Attila Magyar"}}, "url": "https://github.com/apache/hive/commit/e8719c0ab3431625706dd9e902f11d87a26a7f0a", "committedDate": "2020-10-27T10:52:29Z", "message": "HIVE-24217. HMS storage backend for HPL/SQL stored procedures (amagyar)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE4NTkxMzM0", "url": "https://github.com/apache/hive/pull/1542#pullrequestreview-518591334", "createdAt": "2020-10-28T12:08:04Z", "commit": {"oid": "e8719c0ab3431625706dd9e902f11d87a26a7f0a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3126, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}