{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU5MjU4NTc4", "number": 1339, "reviewThreads": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzoxNzozOVrOETxajg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTo1NDowMFrOEUiu0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTY4MDE0OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzoxNzozOVrOG5seCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzoxNzozOVrOG5seCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE1MDYwMg==", "bodyText": "Nit: maybe a space after the for keyword?", "url": "https://github.com/apache/hive/pull/1339#discussion_r463150602", "createdAt": "2020-07-30T17:17:39Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +123,183 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFileStatuses bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<HadoopShims.HdfsFileStatusWithId> deltaFileStatuses) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = new ArrayList<>();\n+      for(HadoopShims.HdfsFileStatusWithId fileStatus : deltaFileStatuses) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTY4Mzg4OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzoxODo0OFrOG5sggQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzoxODo0OFrOG5sggQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE1MTIzMw==", "bodyText": "Nit: newline", "url": "https://github.com/apache/hive/pull/1339#discussion_r463151233", "createdAt": "2020-07-30T17:18:48Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +123,183 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFileStatuses bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<HadoopShims.HdfsFileStatusWithId> deltaFileStatuses) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = new ArrayList<>();\n+      for(HadoopShims.HdfsFileStatusWithId fileStatus : deltaFileStatuses) {\n+        deltaFiles.add(new DeltaFileMetaData(fileStatus));\n+      }\n     }\n+\n     long getMinWriteId() {\n       return minWriteId;\n     }\n+\n     long getMaxWriteId() {\n       return maxWriteId;\n     }\n+\n     List<Integer> getStmtIds() {\n       return stmtIds;\n     }\n+\n     long getVisibilityTxnId() {\n       return visibilityTxnId;\n     }\n+\n+    public List<DeltaFileMetaData> getDeltaFiles() {\n+      return deltaFiles;\n+    }\n+\n     @Override\n     public void write(DataOutput out) throws IOException {\n       out.writeLong(minWriteId);\n       out.writeLong(maxWriteId);\n       out.writeInt(stmtIds.size());\n-      for(Integer id : stmtIds) {\n+      for (Integer id : stmtIds) {\n         out.writeInt(id);\n       }\n       out.writeLong(visibilityTxnId);\n+      out.writeInt(deltaFiles.size());\n+      for (DeltaFileMetaData fileMeta : deltaFiles) {\n+        fileMeta.write(out);\n+      }\n     }\n+\n     @Override\n     public void readFields(DataInput in) throws IOException {\n       minWriteId = in.readLong();\n       maxWriteId = in.readLong();\n       stmtIds.clear();\n       int numStatements = in.readInt();\n-      for(int i = 0; i < numStatements; i++) {\n+      for (int i = 0; i < numStatements; i++) {\n         stmtIds.add(in.readInt());\n       }\n       visibilityTxnId = in.readLong();\n+\n+      deltaFiles.clear();\n+      int numFiles = in.readInt();\n+      for(int i = 0; i< numFiles; i++) {\n+        DeltaFileMetaData file = new DeltaFileMetaData();\n+        file.readFields(in);\n+        deltaFiles.add(file);\n+      }\n     }\n-    String getName() {\n+    private String getName() {\n       assert stmtIds.isEmpty() : \"use getName(int)\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n     }\n-    String getName(int stmtId) {\n+    private String getName(int stmtId) {\n       assert !stmtIds.isEmpty() : \"use getName()\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId, stmtId), visibilityTxnId);\n     }\n+\n+    public List<Path> getPaths(Path root) {\n+      if (stmtIds.isEmpty()) {\n+        return Collections.singletonList(new Path(root, getName()));\n+      } else {\n+        // To support multistatement transactions we may have multiple directories corresponding to one DeltaMetaData\n+        return getStmtIds().stream().map(stmtId -> new Path(root, getName(stmtId))).collect(Collectors.toList());\n+      }\n+    }\n     @Override", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d"}, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTcxNTkwOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzoyNzozMVrOG5s0Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzoyNzozMVrOG5s0Zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE1NjMyNw==", "bodyText": "Nit: newline", "url": "https://github.com/apache/hive/pull/1339#discussion_r463156327", "createdAt": "2020-07-30T17:27:31Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +123,183 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n     /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTcxODE5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzoyODoxNlrOG5s1_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzoyODoxNlrOG5s1_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE1NjczNQ==", "bodyText": "Nit: newline", "url": "https://github.com/apache/hive/pull/1339#discussion_r463156735", "createdAt": "2020-07-30T17:28:16Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +123,183 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFileStatuses bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<HadoopShims.HdfsFileStatusWithId> deltaFileStatuses) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = new ArrayList<>();\n+      for(HadoopShims.HdfsFileStatusWithId fileStatus : deltaFileStatuses) {\n+        deltaFiles.add(new DeltaFileMetaData(fileStatus));\n+      }\n     }\n+\n     long getMinWriteId() {\n       return minWriteId;\n     }\n+\n     long getMaxWriteId() {\n       return maxWriteId;\n     }\n+\n     List<Integer> getStmtIds() {\n       return stmtIds;\n     }\n+\n     long getVisibilityTxnId() {\n       return visibilityTxnId;\n     }\n+\n+    public List<DeltaFileMetaData> getDeltaFiles() {\n+      return deltaFiles;\n+    }\n+\n     @Override\n     public void write(DataOutput out) throws IOException {\n       out.writeLong(minWriteId);\n       out.writeLong(maxWriteId);\n       out.writeInt(stmtIds.size());\n-      for(Integer id : stmtIds) {\n+      for (Integer id : stmtIds) {\n         out.writeInt(id);\n       }\n       out.writeLong(visibilityTxnId);\n+      out.writeInt(deltaFiles.size());\n+      for (DeltaFileMetaData fileMeta : deltaFiles) {\n+        fileMeta.write(out);\n+      }\n     }\n+\n     @Override\n     public void readFields(DataInput in) throws IOException {\n       minWriteId = in.readLong();\n       maxWriteId = in.readLong();\n       stmtIds.clear();\n       int numStatements = in.readInt();\n-      for(int i = 0; i < numStatements; i++) {\n+      for (int i = 0; i < numStatements; i++) {\n         stmtIds.add(in.readInt());\n       }\n       visibilityTxnId = in.readLong();\n+\n+      deltaFiles.clear();\n+      int numFiles = in.readInt();\n+      for(int i = 0; i< numFiles; i++) {\n+        DeltaFileMetaData file = new DeltaFileMetaData();\n+        file.readFields(in);\n+        deltaFiles.add(file);\n+      }\n     }\n-    String getName() {\n+    private String getName() {\n       assert stmtIds.isEmpty() : \"use getName(int)\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n     }\n-    String getName(int stmtId) {\n+    private String getName(int stmtId) {\n       assert !stmtIds.isEmpty() : \"use getName()\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId, stmtId), visibilityTxnId);\n     }\n+\n+    public List<Path> getPaths(Path root) {\n+      if (stmtIds.isEmpty()) {\n+        return Collections.singletonList(new Path(root, getName()));\n+      } else {\n+        // To support multistatement transactions we may have multiple directories corresponding to one DeltaMetaData\n+        return getStmtIds().stream().map(stmtId -> new Path(root, getName(stmtId))).collect(Collectors.toList());\n+      }\n+    }\n     @Override\n     public String toString() {\n       return \"Delta(?,\" + minWriteId + \",\" + maxWriteId + \",\" + stmtIds + \",\" + visibilityTxnId + \")\";\n     }\n   }\n+  final class DeltaFileMetaData implements Writable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d"}, "originalPosition": 153}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTczNzU5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzozNDowNFrOG5tCaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzozNDowNFrOG5tCaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE1OTkxMg==", "bodyText": "Nit: newline", "url": "https://github.com/apache/hive/pull/1339#discussion_r463159912", "createdAt": "2020-07-30T17:34:04Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +123,183 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFileStatuses bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<HadoopShims.HdfsFileStatusWithId> deltaFileStatuses) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = new ArrayList<>();\n+      for(HadoopShims.HdfsFileStatusWithId fileStatus : deltaFileStatuses) {\n+        deltaFiles.add(new DeltaFileMetaData(fileStatus));\n+      }\n     }\n+\n     long getMinWriteId() {\n       return minWriteId;\n     }\n+\n     long getMaxWriteId() {\n       return maxWriteId;\n     }\n+\n     List<Integer> getStmtIds() {\n       return stmtIds;\n     }\n+\n     long getVisibilityTxnId() {\n       return visibilityTxnId;\n     }\n+\n+    public List<DeltaFileMetaData> getDeltaFiles() {\n+      return deltaFiles;\n+    }\n+\n     @Override\n     public void write(DataOutput out) throws IOException {\n       out.writeLong(minWriteId);\n       out.writeLong(maxWriteId);\n       out.writeInt(stmtIds.size());\n-      for(Integer id : stmtIds) {\n+      for (Integer id : stmtIds) {\n         out.writeInt(id);\n       }\n       out.writeLong(visibilityTxnId);\n+      out.writeInt(deltaFiles.size());\n+      for (DeltaFileMetaData fileMeta : deltaFiles) {\n+        fileMeta.write(out);\n+      }\n     }\n+\n     @Override\n     public void readFields(DataInput in) throws IOException {\n       minWriteId = in.readLong();\n       maxWriteId = in.readLong();\n       stmtIds.clear();\n       int numStatements = in.readInt();\n-      for(int i = 0; i < numStatements; i++) {\n+      for (int i = 0; i < numStatements; i++) {\n         stmtIds.add(in.readInt());\n       }\n       visibilityTxnId = in.readLong();\n+\n+      deltaFiles.clear();\n+      int numFiles = in.readInt();\n+      for(int i = 0; i< numFiles; i++) {\n+        DeltaFileMetaData file = new DeltaFileMetaData();\n+        file.readFields(in);\n+        deltaFiles.add(file);\n+      }\n     }\n-    String getName() {\n+    private String getName() {\n       assert stmtIds.isEmpty() : \"use getName(int)\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n     }\n-    String getName(int stmtId) {\n+    private String getName(int stmtId) {\n       assert !stmtIds.isEmpty() : \"use getName()\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId, stmtId), visibilityTxnId);\n     }\n+\n+    public List<Path> getPaths(Path root) {\n+      if (stmtIds.isEmpty()) {\n+        return Collections.singletonList(new Path(root, getName()));\n+      } else {\n+        // To support multistatement transactions we may have multiple directories corresponding to one DeltaMetaData\n+        return getStmtIds().stream().map(stmtId -> new Path(root, getName(stmtId))).collect(Collectors.toList());\n+      }\n+    }\n     @Override\n     public String toString() {\n       return \"Delta(?,\" + minWriteId + \",\" + maxWriteId + \",\" + stmtIds + \",\" + visibilityTxnId + \")\";\n     }\n   }\n+  final class DeltaFileMetaData implements Writable {\n+    private static final int HAS_LONG_FILEID_FLAG = 1;\n+    private static final int HAS_ATTEMPTID_FLAG = 2;\n+\n+    private long modTime;\n+    private long length;\n+    // Optional\n+    private Integer attemptId;\n+    // Optional\n+    private Long fileId;\n+\n+    public DeltaFileMetaData() {\n+    }\n+\n+    public DeltaFileMetaData(HadoopShims.HdfsFileStatusWithId fileStatus) {\n+      modTime = fileStatus.getFileStatus().getModificationTime();\n+      length = fileStatus.getFileStatus().getLen();\n+      String attempt = AcidUtils.parseAttemptId(fileStatus.getFileStatus().getPath());\n+      attemptId = StringUtils.isEmpty(attempt) ? null : Integer.parseInt(attempt);\n+      fileId = fileStatus.getFileId();\n+    }\n+\n+    public DeltaFileMetaData(long modTime, long length, @Nullable Integer attemptId, @Nullable Long fileId) {\n+      this.modTime = modTime;\n+      this.length = length;\n+      this.attemptId = attemptId;\n+      this.fileId = fileId;\n+    }\n+\n+\n+    @Override\n+    public void write(DataOutput out) throws IOException {\n+      int flags = (fileId != null ? HAS_LONG_FILEID_FLAG : 0) |\n+          (attemptId != null ? HAS_ATTEMPTID_FLAG : 0);\n+      out.writeByte(flags);\n+      out.writeLong(modTime);\n+      out.writeLong(length);\n+      if (attemptId != null) {\n+        out.writeInt(attemptId);\n+      }\n+      if (fileId != null) {\n+        out.writeLong(fileId);\n+      }\n+    }\n+\n+    @Override\n+    public void readFields(DataInput in) throws IOException {\n+      byte flags = in.readByte();\n+      boolean hasLongFileId = (HAS_LONG_FILEID_FLAG & flags) != 0,\n+          hasAttemptId = (HAS_ATTEMPTID_FLAG & flags) != 0;\n+      modTime = in.readLong();\n+      length = in.readLong();\n+      if (hasAttemptId) {\n+        attemptId = in.readInt();\n+      }\n+      if (hasLongFileId) {\n+        fileId = in.readLong();\n+      }\n+    }\n+\n+    public Object getFileId(Path deltaDirectory, int bucketId) {\n+      if (fileId != null) {\n+        return fileId;\n+      }\n+      // Calculate the synthetic fileid\n+      Path realPath = getPath(deltaDirectory, bucketId);\n+      return new SyntheticFileId(realPath, length, modTime);\n+    }\n+    public Path getPath(Path deltaDirectory, int bucketId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d"}, "originalPosition": 221}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTc1ODA1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzo0MDowM1rOG5tPGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzo0MDowM1rOG5tPGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2MzE2MA==", "bodyText": "Nit: space before for", "url": "https://github.com/apache/hive/pull/1339#discussion_r463163160", "createdAt": "2020-07-30T17:40:03Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -1113,10 +1119,13 @@ else if(statementId != parsedDelta.statementId) {\n               && (last.getMinWriteId() == parsedDelta.getMinWriteId())\n               && (last.getMaxWriteId() == parsedDelta.getMaxWriteId())) {\n         last.getStmtIds().add(parsedDelta.getStatementId());\n+        for(HadoopShims.HdfsFileStatusWithId fileStatus : parsedDelta.getFiles()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTc2MjU4OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzo0MToyOVrOG5tSIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzo0MToyOVrOG5tSIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2MzkzNg==", "bodyText": "Nit: maybe do it in java8 way?", "url": "https://github.com/apache/hive/pull/1339#discussion_r463163936", "createdAt": "2020-07-30T17:41:29Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -1165,8 +1168,14 @@ private static ParsedDelta parseDelta(Path path, String deltaPrefix, FileSystem\n     throws IOException {\n     ParsedDelta p = parsedDelta(path, deltaPrefix, fs, dirSnapshot);\n     boolean isDeleteDelta = deltaPrefix.equals(DELETE_DELTA_PREFIX);\n+    List<HdfsFileStatusWithId> files = new ArrayList<>();\n+    for (FileStatus fileStatus : dirSnapshot.getFiles()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTc3ODM3OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzo0NjoxNVrOG5tcMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTozMzoxNlrOG6yuzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2NjUxNA==", "bodyText": "Maybe 2 different getOrcTail method on the LLAP IO interface? @szlta?", "url": "https://github.com/apache/hive/pull/1339#discussion_r463166514", "createdAt": "2020-07-30T17:46:15Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -680,14 +681,15 @@ public void setBaseAndInnerReader(\n    * @param path The Orc file path we want to get the OrcTail for\n    * @param conf The Configuration to access LLAP\n    * @param cacheTag The cacheTag needed to get OrcTail from LLAP IO cache\n+   * @param fileKey fileId of the Orc file (either the Long fileId of HDFS or the SyntheticFileId)\n    * @return ReaderData object where the orcTail is not null. Reader can be null, but if we had to create\n    * one we return that as well for further reuse.\n    */\n-  private static ReaderData getOrcTail(Path path, Configuration conf, CacheTag cacheTag) throws IOException {\n+  private static ReaderData getOrcTail(Path path, Configuration conf, CacheTag cacheTag, Object fileKey) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzIzNzIxMQ==", "bodyText": "I am not sure, we will ever want to use it. without fileId.", "url": "https://github.com/apache/hive/pull/1339#discussion_r463237211", "createdAt": "2020-07-30T19:58:16Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -680,14 +681,15 @@ public void setBaseAndInnerReader(\n    * @param path The Orc file path we want to get the OrcTail for\n    * @param conf The Configuration to access LLAP\n    * @param cacheTag The cacheTag needed to get OrcTail from LLAP IO cache\n+   * @param fileKey fileId of the Orc file (either the Long fileId of HDFS or the SyntheticFileId)\n    * @return ReaderData object where the orcTail is not null. Reader can be null, but if we had to create\n    * one we return that as well for further reuse.\n    */\n-  private static ReaderData getOrcTail(Path path, Configuration conf, CacheTag cacheTag) throws IOException {\n+  private static ReaderData getOrcTail(Path path, Configuration conf, CacheTag cacheTag, Object fileKey) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2NjUxNA=="}, "originalCommit": {"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwMTc3NA==", "bodyText": "I'm fine with leaving just this one endpoint, and fileKey being optionally null.\nPlease do emphasize the optionality of fileKey arg in the javadoc part: aka it will be generated if not given, etc..", "url": "https://github.com/apache/hive/pull/1339#discussion_r464301774", "createdAt": "2020-08-03T09:33:16Z", "author": {"login": "szlta"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -680,14 +681,15 @@ public void setBaseAndInnerReader(\n    * @param path The Orc file path we want to get the OrcTail for\n    * @param conf The Configuration to access LLAP\n    * @param cacheTag The cacheTag needed to get OrcTail from LLAP IO cache\n+   * @param fileKey fileId of the Orc file (either the Long fileId of HDFS or the SyntheticFileId)\n    * @return ReaderData object where the orcTail is not null. Reader can be null, but if we had to create\n    * one we return that as well for further reuse.\n    */\n-  private static ReaderData getOrcTail(Path path, Configuration conf, CacheTag cacheTag) throws IOException {\n+  private static ReaderData getOrcTail(Path path, Configuration conf, CacheTag cacheTag, Object fileKey) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2NjUxNA=="}, "originalCommit": {"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTc4ODkyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzo0OTowNFrOG5ti9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzo0OTowNFrOG5ti9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2ODI0Ng==", "bodyText": "isQualifiedDeleteDelta basically reparses the delta dir. Can we prevent this?", "url": "https://github.com/apache/hive/pull/1339#discussion_r463168246", "createdAt": "2020-07-30T17:49:04Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1574,45 +1576,46 @@ public int compareTo(CompressedOwid other) {\n       this.orcSplit = orcSplit;\n \n       try {\n-        final Path[] deleteDeltaDirs = getDeleteDeltaDirsFromSplit(orcSplit);\n-        if (deleteDeltaDirs.length > 0) {\n+        if (orcSplit.getDeltas().size() > 0) {\n           AcidOutputFormat.Options orcSplitMinMaxWriteIds =\n               AcidUtils.parseBaseOrDeltaBucketFilename(orcSplit.getPath(), conf);\n           int totalDeleteEventCount = 0;\n-          for (Path deleteDeltaDir : deleteDeltaDirs) {\n-            if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n-              continue;\n-            }\n-            Path[] deleteDeltaFiles = OrcRawRecordMerger.getDeltaFiles(deleteDeltaDir, bucket,\n-                new OrcRawRecordMerger.Options().isCompacting(false), null);\n-            for (Path deleteDeltaFile : deleteDeltaFiles) {\n-              try {\n-                ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag);\n-                OrcTail orcTail = readerData.orcTail;\n-                if (orcTail.getFooter().getNumberOfRows() <= 0) {\n-                  continue; // just a safe check to ensure that we are not reading empty delete files.\n-                }\n-                OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n-                if (!deleteKeyInterval.isIntersects(keyInterval)) {\n-                  // If there is no intersection between data and delete delta, do not read delete file\n-                  continue;\n-                }\n-                // Reader can be reused if it was created before for getting orcTail: mostly for non-LLAP cache cases.\n-                // For LLAP cases we need to create it here.\n-                Reader deleteDeltaReader = readerData.reader != null ? readerData.reader :\n-                    OrcFile.createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n-                totalDeleteEventCount += deleteDeltaReader.getNumberOfRows();\n-                DeleteReaderValue deleteReaderValue = new DeleteReaderValue(deleteDeltaReader,\n-                    deleteDeltaFile, readerOptions, bucket, validWriteIdList, isBucketedTable, conf,\n-                    keyInterval, orcSplit);\n-                DeleteRecordKey deleteRecordKey = new DeleteRecordKey();\n-                if (deleteReaderValue.next(deleteRecordKey)) {\n-                  sortMerger.put(deleteRecordKey, deleteReaderValue);\n-                } else {\n-                  deleteReaderValue.close();\n+          for (AcidInputFormat.DeltaMetaData deltaMetaData : orcSplit.getDeltas()) {\n+            for (Path deleteDeltaDir : deltaMetaData.getPaths(orcSplit.getRootDir())) {\n+              if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTc5MzYxOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzo1MDoyNVrOG5tmKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwNTowNDozNVrOG58F9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2OTA2NQ==", "bodyText": "Do we still need this?", "url": "https://github.com/apache/hive/pull/1339#discussion_r463169065", "createdAt": "2020-07-30T17:50:25Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1574,45 +1576,46 @@ public int compareTo(CompressedOwid other) {\n       this.orcSplit = orcSplit;\n \n       try {\n-        final Path[] deleteDeltaDirs = getDeleteDeltaDirsFromSplit(orcSplit);\n-        if (deleteDeltaDirs.length > 0) {\n+        if (orcSplit.getDeltas().size() > 0) {\n           AcidOutputFormat.Options orcSplitMinMaxWriteIds =\n               AcidUtils.parseBaseOrDeltaBucketFilename(orcSplit.getPath(), conf);\n           int totalDeleteEventCount = 0;\n-          for (Path deleteDeltaDir : deleteDeltaDirs) {\n-            if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n-              continue;\n-            }\n-            Path[] deleteDeltaFiles = OrcRawRecordMerger.getDeltaFiles(deleteDeltaDir, bucket,\n-                new OrcRawRecordMerger.Options().isCompacting(false), null);\n-            for (Path deleteDeltaFile : deleteDeltaFiles) {\n-              try {\n-                ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag);\n-                OrcTail orcTail = readerData.orcTail;\n-                if (orcTail.getFooter().getNumberOfRows() <= 0) {\n-                  continue; // just a safe check to ensure that we are not reading empty delete files.\n-                }\n-                OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n-                if (!deleteKeyInterval.isIntersects(keyInterval)) {\n-                  // If there is no intersection between data and delete delta, do not read delete file\n-                  continue;\n-                }\n-                // Reader can be reused if it was created before for getting orcTail: mostly for non-LLAP cache cases.\n-                // For LLAP cases we need to create it here.\n-                Reader deleteDeltaReader = readerData.reader != null ? readerData.reader :\n-                    OrcFile.createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n-                totalDeleteEventCount += deleteDeltaReader.getNumberOfRows();\n-                DeleteReaderValue deleteReaderValue = new DeleteReaderValue(deleteDeltaReader,\n-                    deleteDeltaFile, readerOptions, bucket, validWriteIdList, isBucketedTable, conf,\n-                    keyInterval, orcSplit);\n-                DeleteRecordKey deleteRecordKey = new DeleteRecordKey();\n-                if (deleteReaderValue.next(deleteRecordKey)) {\n-                  sortMerger.put(deleteRecordKey, deleteReaderValue);\n-                } else {\n-                  deleteReaderValue.close();\n+          for (AcidInputFormat.DeltaMetaData deltaMetaData : orcSplit.getDeltas()) {\n+            for (Path deleteDeltaDir : deltaMetaData.getPaths(orcSplit.getRootDir())) {\n+              if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n+                LOG.debug(\"Skipping delete delta dir {}\", deleteDeltaDir);\n+                continue;\n+              }\n+              for (AcidInputFormat.DeltaFileMetaData fileMetaData : deltaMetaData.getDeltaFiles()) {\n+                Path deleteDeltaFile = fileMetaData.getPath(deleteDeltaDir, bucket);\n+                try {\n+                  ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag, fileMetaData.getFileId(deleteDeltaDir, bucket));\n+                  OrcTail orcTail = readerData.orcTail;\n+                  if (orcTail.getFooter().getNumberOfRows() <= 0) {\n+                    continue; // just a safe check to ensure that we are not reading empty delete files.\n+                  }\n+                  OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n+                  if (!deleteKeyInterval.isIntersects(keyInterval)) {\n+                    // If there is no intersection between data and delete delta, do not read delete file\n+                    continue;\n+                  }\n+                  // Reader can be reused if it was created before for getting orcTail: mostly for non-LLAP cache cases.\n+                  // For LLAP cases we need to create it here.\n+                  Reader deleteDeltaReader = readerData.reader != null ? readerData.reader : OrcFile\n+                      .createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n+                  totalDeleteEventCount += deleteDeltaReader.getNumberOfRows();\n+                  DeleteReaderValue deleteReaderValue =\n+                      new DeleteReaderValue(deleteDeltaReader, deleteDeltaFile, readerOptions, bucket, validWriteIdList,\n+                          isBucketedTable, conf, keyInterval, orcSplit);\n+                  DeleteRecordKey deleteRecordKey = new DeleteRecordKey();\n+                  if (deleteReaderValue.next(deleteRecordKey)) {\n+                    sortMerger.put(deleteRecordKey, deleteReaderValue);\n+                  } else {\n+                    deleteReaderValue.close();\n+                  }\n+                } catch (FileNotFoundException fnf) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzI0OTkzMg==", "bodyText": "Technically we need this, because of the multistatement case is not handled too well. There is one DeltaMetaData for one writeId and the statementIds are collected there. I did not want to disturb this structure, but this way I have one merged fileList for the different folders and it will try each file for each folder. This is far from ideal, but I don't think it is worth the effort to change this, before the multistatement feature is developed. But I will change the comment to reflect that.", "url": "https://github.com/apache/hive/pull/1339#discussion_r463249932", "createdAt": "2020-07-30T20:23:51Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1574,45 +1576,46 @@ public int compareTo(CompressedOwid other) {\n       this.orcSplit = orcSplit;\n \n       try {\n-        final Path[] deleteDeltaDirs = getDeleteDeltaDirsFromSplit(orcSplit);\n-        if (deleteDeltaDirs.length > 0) {\n+        if (orcSplit.getDeltas().size() > 0) {\n           AcidOutputFormat.Options orcSplitMinMaxWriteIds =\n               AcidUtils.parseBaseOrDeltaBucketFilename(orcSplit.getPath(), conf);\n           int totalDeleteEventCount = 0;\n-          for (Path deleteDeltaDir : deleteDeltaDirs) {\n-            if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n-              continue;\n-            }\n-            Path[] deleteDeltaFiles = OrcRawRecordMerger.getDeltaFiles(deleteDeltaDir, bucket,\n-                new OrcRawRecordMerger.Options().isCompacting(false), null);\n-            for (Path deleteDeltaFile : deleteDeltaFiles) {\n-              try {\n-                ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag);\n-                OrcTail orcTail = readerData.orcTail;\n-                if (orcTail.getFooter().getNumberOfRows() <= 0) {\n-                  continue; // just a safe check to ensure that we are not reading empty delete files.\n-                }\n-                OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n-                if (!deleteKeyInterval.isIntersects(keyInterval)) {\n-                  // If there is no intersection between data and delete delta, do not read delete file\n-                  continue;\n-                }\n-                // Reader can be reused if it was created before for getting orcTail: mostly for non-LLAP cache cases.\n-                // For LLAP cases we need to create it here.\n-                Reader deleteDeltaReader = readerData.reader != null ? readerData.reader :\n-                    OrcFile.createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n-                totalDeleteEventCount += deleteDeltaReader.getNumberOfRows();\n-                DeleteReaderValue deleteReaderValue = new DeleteReaderValue(deleteDeltaReader,\n-                    deleteDeltaFile, readerOptions, bucket, validWriteIdList, isBucketedTable, conf,\n-                    keyInterval, orcSplit);\n-                DeleteRecordKey deleteRecordKey = new DeleteRecordKey();\n-                if (deleteReaderValue.next(deleteRecordKey)) {\n-                  sortMerger.put(deleteRecordKey, deleteReaderValue);\n-                } else {\n-                  deleteReaderValue.close();\n+          for (AcidInputFormat.DeltaMetaData deltaMetaData : orcSplit.getDeltas()) {\n+            for (Path deleteDeltaDir : deltaMetaData.getPaths(orcSplit.getRootDir())) {\n+              if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n+                LOG.debug(\"Skipping delete delta dir {}\", deleteDeltaDir);\n+                continue;\n+              }\n+              for (AcidInputFormat.DeltaFileMetaData fileMetaData : deltaMetaData.getDeltaFiles()) {\n+                Path deleteDeltaFile = fileMetaData.getPath(deleteDeltaDir, bucket);\n+                try {\n+                  ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag, fileMetaData.getFileId(deleteDeltaDir, bucket));\n+                  OrcTail orcTail = readerData.orcTail;\n+                  if (orcTail.getFooter().getNumberOfRows() <= 0) {\n+                    continue; // just a safe check to ensure that we are not reading empty delete files.\n+                  }\n+                  OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n+                  if (!deleteKeyInterval.isIntersects(keyInterval)) {\n+                    // If there is no intersection between data and delete delta, do not read delete file\n+                    continue;\n+                  }\n+                  // Reader can be reused if it was created before for getting orcTail: mostly for non-LLAP cache cases.\n+                  // For LLAP cases we need to create it here.\n+                  Reader deleteDeltaReader = readerData.reader != null ? readerData.reader : OrcFile\n+                      .createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n+                  totalDeleteEventCount += deleteDeltaReader.getNumberOfRows();\n+                  DeleteReaderValue deleteReaderValue =\n+                      new DeleteReaderValue(deleteDeltaReader, deleteDeltaFile, readerOptions, bucket, validWriteIdList,\n+                          isBucketedTable, conf, keyInterval, orcSplit);\n+                  DeleteRecordKey deleteRecordKey = new DeleteRecordKey();\n+                  if (deleteReaderValue.next(deleteRecordKey)) {\n+                    sortMerger.put(deleteRecordKey, deleteReaderValue);\n+                  } else {\n+                    deleteReaderValue.close();\n+                  }\n+                } catch (FileNotFoundException fnf) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2OTA2NQ=="}, "originalCommit": {"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQwNjU4MA==", "bodyText": "Multitable insersts also uses stmtId when inserting data. Not sure if we can insert twice in the same table with a single query....", "url": "https://github.com/apache/hive/pull/1339#discussion_r463406580", "createdAt": "2020-07-31T05:04:35Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1574,45 +1576,46 @@ public int compareTo(CompressedOwid other) {\n       this.orcSplit = orcSplit;\n \n       try {\n-        final Path[] deleteDeltaDirs = getDeleteDeltaDirsFromSplit(orcSplit);\n-        if (deleteDeltaDirs.length > 0) {\n+        if (orcSplit.getDeltas().size() > 0) {\n           AcidOutputFormat.Options orcSplitMinMaxWriteIds =\n               AcidUtils.parseBaseOrDeltaBucketFilename(orcSplit.getPath(), conf);\n           int totalDeleteEventCount = 0;\n-          for (Path deleteDeltaDir : deleteDeltaDirs) {\n-            if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n-              continue;\n-            }\n-            Path[] deleteDeltaFiles = OrcRawRecordMerger.getDeltaFiles(deleteDeltaDir, bucket,\n-                new OrcRawRecordMerger.Options().isCompacting(false), null);\n-            for (Path deleteDeltaFile : deleteDeltaFiles) {\n-              try {\n-                ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag);\n-                OrcTail orcTail = readerData.orcTail;\n-                if (orcTail.getFooter().getNumberOfRows() <= 0) {\n-                  continue; // just a safe check to ensure that we are not reading empty delete files.\n-                }\n-                OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n-                if (!deleteKeyInterval.isIntersects(keyInterval)) {\n-                  // If there is no intersection between data and delete delta, do not read delete file\n-                  continue;\n-                }\n-                // Reader can be reused if it was created before for getting orcTail: mostly for non-LLAP cache cases.\n-                // For LLAP cases we need to create it here.\n-                Reader deleteDeltaReader = readerData.reader != null ? readerData.reader :\n-                    OrcFile.createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n-                totalDeleteEventCount += deleteDeltaReader.getNumberOfRows();\n-                DeleteReaderValue deleteReaderValue = new DeleteReaderValue(deleteDeltaReader,\n-                    deleteDeltaFile, readerOptions, bucket, validWriteIdList, isBucketedTable, conf,\n-                    keyInterval, orcSplit);\n-                DeleteRecordKey deleteRecordKey = new DeleteRecordKey();\n-                if (deleteReaderValue.next(deleteRecordKey)) {\n-                  sortMerger.put(deleteRecordKey, deleteReaderValue);\n-                } else {\n-                  deleteReaderValue.close();\n+          for (AcidInputFormat.DeltaMetaData deltaMetaData : orcSplit.getDeltas()) {\n+            for (Path deleteDeltaDir : deltaMetaData.getPaths(orcSplit.getRootDir())) {\n+              if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n+                LOG.debug(\"Skipping delete delta dir {}\", deleteDeltaDir);\n+                continue;\n+              }\n+              for (AcidInputFormat.DeltaFileMetaData fileMetaData : deltaMetaData.getDeltaFiles()) {\n+                Path deleteDeltaFile = fileMetaData.getPath(deleteDeltaDir, bucket);\n+                try {\n+                  ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag, fileMetaData.getFileId(deleteDeltaDir, bucket));\n+                  OrcTail orcTail = readerData.orcTail;\n+                  if (orcTail.getFooter().getNumberOfRows() <= 0) {\n+                    continue; // just a safe check to ensure that we are not reading empty delete files.\n+                  }\n+                  OrcRawRecordMerger.KeyInterval deleteKeyInterval = findDeleteMinMaxKeys(orcTail, deleteDeltaFile);\n+                  if (!deleteKeyInterval.isIntersects(keyInterval)) {\n+                    // If there is no intersection between data and delete delta, do not read delete file\n+                    continue;\n+                  }\n+                  // Reader can be reused if it was created before for getting orcTail: mostly for non-LLAP cache cases.\n+                  // For LLAP cases we need to create it here.\n+                  Reader deleteDeltaReader = readerData.reader != null ? readerData.reader : OrcFile\n+                      .createReader(deleteDeltaFile, OrcFile.readerOptions(conf));\n+                  totalDeleteEventCount += deleteDeltaReader.getNumberOfRows();\n+                  DeleteReaderValue deleteReaderValue =\n+                      new DeleteReaderValue(deleteDeltaReader, deleteDeltaFile, readerOptions, bucket, validWriteIdList,\n+                          isBucketedTable, conf, keyInterval, orcSplit);\n+                  DeleteRecordKey deleteRecordKey = new DeleteRecordKey();\n+                  if (deleteReaderValue.next(deleteRecordKey)) {\n+                    sortMerger.put(deleteRecordKey, deleteReaderValue);\n+                  } else {\n+                    deleteReaderValue.close();\n+                  }\n+                } catch (FileNotFoundException fnf) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE2OTA2NQ=="}, "originalCommit": {"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5MTgwNjg4OnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzo1Mzo1MFrOG5tuVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxNzo1Mzo1MFrOG5tuVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzE3MTE1OQ==", "bodyText": "Test for all of the different serialization options", "url": "https://github.com/apache/hive/pull/1339#discussion_r463171159", "createdAt": "2020-07-30T17:53:50Z", "author": {"login": "pvary"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java", "diffHunk": "@@ -83,6 +94,34 @@ public void testDeltaMetaConstructWithState() throws Exception {\n     assertThat(deltaMetaData.getStmtIds().get(2), is(99));\n   }\n \n+  @Test\n+  public void testDeltaMetaWithFile() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "65c0398c5dd39e8ecd1bd317cf0ed3241e03c45d"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTY5OTM2OnYy", "diffSide": "RIGHT", "path": "llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTozNjoxMlrOG6y1Bg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTozNjoxMlrOG6y1Bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwMzM2Ng==", "bodyText": "you can add a fail call here, as it should always jump from line 308 to catch clause.", "url": "https://github.com/apache/hive/pull/1339#discussion_r464303366", "createdAt": "2020-08-03T09:36:12Z", "author": {"login": "szlta"}, "path": "llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java", "diffHunk": "@@ -250,18 +255,71 @@ public void testGetOrcTailForPath() throws Exception {\n     Configuration jobConf = new Configuration();\n     Configuration daemonConf = new Configuration();\n     CacheTag tag = CacheTag.build(\"test-table\");\n-    OrcTail uncached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache);\n+    OrcTail uncached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, null);\n     jobConf.set(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname, \"true\");\n-    OrcTail cached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache);\n+    OrcTail cached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, null);\n     assertEquals(uncached.getSerializedTail(), cached.getSerializedTail());\n     assertEquals(uncached.getFileTail(), cached.getFileTail());\n   }\n \n+  @Test\n+  public void testGetOrcTailForPathWithFileId() throws Exception {\n+    DummyMemoryManager mm = new DummyMemoryManager();\n+    DummyCachePolicy cp = new DummyCachePolicy();\n+    final int MAX_ALLOC = 64;\n+    LlapDaemonCacheMetrics metrics = LlapDaemonCacheMetrics.create(\"\", \"\");\n+    BuddyAllocator alloc = new BuddyAllocator(\n+        false, false, 8, MAX_ALLOC, 1, 4096, 0, null, mm, metrics, null, true);\n+    MetadataCache cache = new MetadataCache(alloc, mm, cp, true, metrics);\n+\n+    Path path = new Path(\"../data/files/alltypesorc\");\n+    Configuration jobConf = new Configuration();\n+    Configuration daemonConf = new Configuration();\n+    CacheTag tag = CacheTag.build(\"test-table\");\n+    FileSystem fs = FileSystem.get(daemonConf);\n+    FileStatus fileStatus = fs.getFileStatus(path);\n+    OrcTail uncached = OrcEncodedDataReader.getOrcTailForPath(fileStatus.getPath(), jobConf, tag, daemonConf, cache, new SyntheticFileId(fileStatus));\n+    jobConf.set(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname, \"true\");\n+    // this should work from the cache, by recalculating the same fileId\n+    OrcTail cached = OrcEncodedDataReader.getOrcTailForPath(fileStatus.getPath(), jobConf, tag, daemonConf, cache, null);\n+    assertEquals(uncached.getSerializedTail(), cached.getSerializedTail());\n+    assertEquals(uncached.getFileTail(), cached.getFileTail());\n+  }\n+\n+  @Test\n+  public void testGetOrcTailForPathWithFileIdChange() throws Exception {\n+    DummyMemoryManager mm = new DummyMemoryManager();\n+    DummyCachePolicy cp = new DummyCachePolicy();\n+    final int MAX_ALLOC = 64;\n+    LlapDaemonCacheMetrics metrics = LlapDaemonCacheMetrics.create(\"\", \"\");\n+    BuddyAllocator alloc = new BuddyAllocator(\n+        false, false, 8, MAX_ALLOC, 1, 4096, 0, null, mm, metrics, null, true);\n+    MetadataCache cache = new MetadataCache(alloc, mm, cp, true, metrics);\n+\n+    Path path = new Path(\"../data/files/alltypesorc\");\n+    Configuration jobConf = new Configuration();\n+    Configuration daemonConf = new Configuration();\n+    CacheTag tag = CacheTag.build(\"test-table\");\n+    OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, new SyntheticFileId(path, 100, 100));\n+    jobConf.set(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname, \"true\");\n+    Exception ex = null;\n+    try {\n+      // this should miss the cache, since the fileKey changed\n+      OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, new SyntheticFileId(path, 100, 101));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTcwNzc1OnYy", "diffSide": "RIGHT", "path": "llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTozODozN1rOG6y6Bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTozODozN1rOG6y6Bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwNDY0Nw==", "bodyText": "nit: too many newline. If we need any fix, please remove them", "url": "https://github.com/apache/hive/pull/1339#discussion_r464304647", "createdAt": "2020-08-03T09:38:37Z", "author": {"login": "pvary"}, "path": "llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestOrcMetadataCache.java", "diffHunk": "@@ -250,18 +255,71 @@ public void testGetOrcTailForPath() throws Exception {\n     Configuration jobConf = new Configuration();\n     Configuration daemonConf = new Configuration();\n     CacheTag tag = CacheTag.build(\"test-table\");\n-    OrcTail uncached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache);\n+    OrcTail uncached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, null);\n     jobConf.set(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname, \"true\");\n-    OrcTail cached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache);\n+    OrcTail cached = OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, null);\n     assertEquals(uncached.getSerializedTail(), cached.getSerializedTail());\n     assertEquals(uncached.getFileTail(), cached.getFileTail());\n   }\n \n+  @Test\n+  public void testGetOrcTailForPathWithFileId() throws Exception {\n+    DummyMemoryManager mm = new DummyMemoryManager();\n+    DummyCachePolicy cp = new DummyCachePolicy();\n+    final int MAX_ALLOC = 64;\n+    LlapDaemonCacheMetrics metrics = LlapDaemonCacheMetrics.create(\"\", \"\");\n+    BuddyAllocator alloc = new BuddyAllocator(\n+        false, false, 8, MAX_ALLOC, 1, 4096, 0, null, mm, metrics, null, true);\n+    MetadataCache cache = new MetadataCache(alloc, mm, cp, true, metrics);\n+\n+    Path path = new Path(\"../data/files/alltypesorc\");\n+    Configuration jobConf = new Configuration();\n+    Configuration daemonConf = new Configuration();\n+    CacheTag tag = CacheTag.build(\"test-table\");\n+    FileSystem fs = FileSystem.get(daemonConf);\n+    FileStatus fileStatus = fs.getFileStatus(path);\n+    OrcTail uncached = OrcEncodedDataReader.getOrcTailForPath(fileStatus.getPath(), jobConf, tag, daemonConf, cache, new SyntheticFileId(fileStatus));\n+    jobConf.set(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname, \"true\");\n+    // this should work from the cache, by recalculating the same fileId\n+    OrcTail cached = OrcEncodedDataReader.getOrcTailForPath(fileStatus.getPath(), jobConf, tag, daemonConf, cache, null);\n+    assertEquals(uncached.getSerializedTail(), cached.getSerializedTail());\n+    assertEquals(uncached.getFileTail(), cached.getFileTail());\n+  }\n+\n+  @Test\n+  public void testGetOrcTailForPathWithFileIdChange() throws Exception {\n+    DummyMemoryManager mm = new DummyMemoryManager();\n+    DummyCachePolicy cp = new DummyCachePolicy();\n+    final int MAX_ALLOC = 64;\n+    LlapDaemonCacheMetrics metrics = LlapDaemonCacheMetrics.create(\"\", \"\");\n+    BuddyAllocator alloc = new BuddyAllocator(\n+        false, false, 8, MAX_ALLOC, 1, 4096, 0, null, mm, metrics, null, true);\n+    MetadataCache cache = new MetadataCache(alloc, mm, cp, true, metrics);\n+\n+    Path path = new Path(\"../data/files/alltypesorc\");\n+    Configuration jobConf = new Configuration();\n+    Configuration daemonConf = new Configuration();\n+    CacheTag tag = CacheTag.build(\"test-table\");\n+    OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, new SyntheticFileId(path, 100, 100));\n+    jobConf.set(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname, \"true\");\n+    Exception ex = null;\n+    try {\n+      // this should miss the cache, since the fileKey changed\n+      OrcEncodedDataReader.getOrcTailForPath(path, jobConf, tag, daemonConf, cache, new SyntheticFileId(path, 100, 101));\n+    } catch (IOException e) {\n+      ex = e;\n+    }\n+    Assert.assertNotNull(ex);\n+    Assert.assertTrue(ex.getMessage().contains(HiveConf.ConfVars.LLAP_IO_CACHE_ONLY.varname));\n+  }\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTcxMTQ1OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTozOTozOFrOG6y8JQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNTowOToyOVrOG69Ysg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwNTE4OQ==", "bodyText": "The generation of fileId may be more complicated based on what configuration is given. Please refer to https://github.com/apache/hive/blob/master/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java#L496-L517", "url": "https://github.com/apache/hive/pull/1339#discussion_r464305189", "createdAt": "2020-08-03T09:39:38Z", "author": {"login": "szlta"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +126,217 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n+\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFiles bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    public DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<DeltaFileMetaData> deltaFiles) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = ObjectUtils.defaultIfNull(deltaFiles, new ArrayList<>());\n     }\n-    long getMinWriteId() {\n+\n+    public long getMinWriteId() {\n       return minWriteId;\n     }\n-    long getMaxWriteId() {\n+\n+    public long getMaxWriteId() {\n       return maxWriteId;\n     }\n-    List<Integer> getStmtIds() {\n+\n+    public List<Integer> getStmtIds() {\n       return stmtIds;\n     }\n-    long getVisibilityTxnId() {\n+\n+    public long getVisibilityTxnId() {\n       return visibilityTxnId;\n     }\n+\n+    public List<DeltaFileMetaData> getDeltaFiles() {\n+      return deltaFiles;\n+    }\n+\n+    public List<DeltaFileMetaData> getDeltaFilesForStmtId(final Integer stmtId) {\n+      if (stmtIds.size() <= 1 || stmtId == null) {\n+        // If it is not a multistatement delta, we do not store the stmtId in the file list\n+        return deltaFiles;\n+      } else {\n+        return deltaFiles.stream().filter(df -> stmtId.equals(df.getStmtId())).collect(Collectors.toList());\n+      }\n+    }\n+\n     @Override\n     public void write(DataOutput out) throws IOException {\n       out.writeLong(minWriteId);\n       out.writeLong(maxWriteId);\n       out.writeInt(stmtIds.size());\n-      for(Integer id : stmtIds) {\n+      for (Integer id : stmtIds) {\n         out.writeInt(id);\n       }\n       out.writeLong(visibilityTxnId);\n+      out.writeInt(deltaFiles.size());\n+      for (DeltaFileMetaData fileMeta : deltaFiles) {\n+        fileMeta.write(out);\n+      }\n     }\n+\n     @Override\n     public void readFields(DataInput in) throws IOException {\n       minWriteId = in.readLong();\n       maxWriteId = in.readLong();\n       stmtIds.clear();\n       int numStatements = in.readInt();\n-      for(int i = 0; i < numStatements; i++) {\n+      for (int i = 0; i < numStatements; i++) {\n         stmtIds.add(in.readInt());\n       }\n       visibilityTxnId = in.readLong();\n+\n+      deltaFiles.clear();\n+      int numFiles = in.readInt();\n+      for (int i = 0; i < numFiles; i++) {\n+        DeltaFileMetaData file = new DeltaFileMetaData();\n+        file.readFields(in);\n+        deltaFiles.add(file);\n+      }\n     }\n-    String getName() {\n+\n+    private String getName() {\n       assert stmtIds.isEmpty() : \"use getName(int)\";\n-      return AcidUtils.addVisibilitySuffix(AcidUtils\n-          .deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n+      return AcidUtils.addVisibilitySuffix(AcidUtils.deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n     }\n-    String getName(int stmtId) {\n+\n+    private String getName(int stmtId) {\n       assert !stmtIds.isEmpty() : \"use getName()\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId, stmtId), visibilityTxnId);\n     }\n+\n+    public List<Pair<Path, Integer>> getPaths(Path root) {\n+      if (stmtIds.isEmpty()) {\n+        return Collections.singletonList(new ImmutablePair<>(new Path(root, getName()), null));\n+      } else {\n+        // To support multistatement transactions we may have multiple directories corresponding to one DeltaMetaData\n+        return getStmtIds().stream()\n+            .map(stmtId -> new ImmutablePair<>(new Path(root, getName(stmtId)), stmtId)).collect(Collectors.toList());\n+      }\n+    }\n+\n     @Override\n     public String toString() {\n       return \"Delta(?,\" + minWriteId + \",\" + maxWriteId + \",\" + stmtIds + \",\" + visibilityTxnId + \")\";\n     }\n   }\n+\n+  final class DeltaFileMetaData implements Writable {\n+    private static final int HAS_LONG_FILEID_FLAG = 1;\n+    private static final int HAS_ATTEMPTID_FLAG = 2;\n+    private static final int HAS_STMTID_FLAG = 4;\n+\n+    private long modTime;\n+    private long length;\n+    // Optional\n+    private Integer attemptId;\n+    // Optional\n+    private Long fileId;\n+    // Optional, if the deltaMeta contains multiple stmtIds, it will contain this files parent's stmtId\n+    private Integer stmtId;\n+\n+    public DeltaFileMetaData() {\n+    }\n+\n+    public DeltaFileMetaData(HadoopShims.HdfsFileStatusWithId fileStatus, Integer stmtId) {\n+      modTime = fileStatus.getFileStatus().getModificationTime();\n+      length = fileStatus.getFileStatus().getLen();\n+      String attempt = AcidUtils.parseAttemptId(fileStatus.getFileStatus().getPath());\n+      attemptId = StringUtils.isEmpty(attempt) ? null : Integer.parseInt(attempt);\n+      fileId = fileStatus.getFileId();\n+      this.stmtId = stmtId;\n+    }\n+\n+    public DeltaFileMetaData(long modTime, long length, @Nullable Integer attemptId, @Nullable Long fileId,\n+        @Nullable Integer stmtId) {\n+      this.modTime = modTime;\n+      this.length = length;\n+      this.attemptId = attemptId;\n+      this.fileId = fileId;\n+      this.stmtId = stmtId;\n+    }\n+\n+    public void clearStmtId() {\n+      stmtId = null;\n+    }\n+\n+    public Integer getStmtId() {\n+      return stmtId;\n+    }\n+\n+    @Override\n+    public void write(DataOutput out) throws IOException {\n+      int flags = (fileId != null ? HAS_LONG_FILEID_FLAG : 0) |\n+          (attemptId != null ? HAS_ATTEMPTID_FLAG : 0) |\n+          (stmtId != null ? HAS_STMTID_FLAG : 0);\n+      out.writeByte(flags);\n+      out.writeLong(modTime);\n+      out.writeLong(length);\n+      if (attemptId != null) {\n+        out.writeInt(attemptId);\n+      }\n+      if (fileId != null) {\n+        out.writeLong(fileId);\n+      }\n+      if (stmtId != null) {\n+        out.writeInt(stmtId);\n+      }\n+    }\n+\n+    @Override\n+    public void readFields(DataInput in) throws IOException {\n+      byte flags = in.readByte();\n+      boolean hasLongFileId = (HAS_LONG_FILEID_FLAG & flags) != 0,\n+          hasAttemptId = (HAS_ATTEMPTID_FLAG & flags) != 0,\n+          hasStmtId = (HAS_STMTID_FLAG & flags) != 0;\n+      modTime = in.readLong();\n+      length = in.readLong();\n+      if (hasAttemptId) {\n+        attemptId = in.readInt();\n+      }\n+      if (hasLongFileId) {\n+        fileId = in.readLong();\n+      }\n+      if (hasStmtId) {\n+        stmtId = in.readInt();\n+      }\n+    }\n+\n+    public Object getFileId(Path deltaDirectory, int bucketId) {\n+      if (fileId != null) {\n+        return fileId;\n+      }\n+      // Calculate the synthetic fileid\n+      Path realPath = getPath(deltaDirectory, bucketId);\n+      return new SyntheticFileId(realPath, length, modTime);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b"}, "originalPosition": 268}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ3NjMzOA==", "bodyText": "I will add the forceSynthetic parameter, it has valid use-casee (https://issues.apache.org/jira/browse/HIVE-20338) but I have a problem with this: boolean allowSynthetic = HiveConf.getBoolVar(daemonConf, ConfVars.LLAP_CACHE_ALLOW_SYNTHETIC_FILEID);\nIf someone disables this, it will render the llap cache useless, even more, your orctailcache will just throw an IllegalCacheConfigurationException", "url": "https://github.com/apache/hive/pull/1339#discussion_r464476338", "createdAt": "2020-08-03T15:09:29Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +126,217 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n+\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFiles bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    public DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<DeltaFileMetaData> deltaFiles) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = ObjectUtils.defaultIfNull(deltaFiles, new ArrayList<>());\n     }\n-    long getMinWriteId() {\n+\n+    public long getMinWriteId() {\n       return minWriteId;\n     }\n-    long getMaxWriteId() {\n+\n+    public long getMaxWriteId() {\n       return maxWriteId;\n     }\n-    List<Integer> getStmtIds() {\n+\n+    public List<Integer> getStmtIds() {\n       return stmtIds;\n     }\n-    long getVisibilityTxnId() {\n+\n+    public long getVisibilityTxnId() {\n       return visibilityTxnId;\n     }\n+\n+    public List<DeltaFileMetaData> getDeltaFiles() {\n+      return deltaFiles;\n+    }\n+\n+    public List<DeltaFileMetaData> getDeltaFilesForStmtId(final Integer stmtId) {\n+      if (stmtIds.size() <= 1 || stmtId == null) {\n+        // If it is not a multistatement delta, we do not store the stmtId in the file list\n+        return deltaFiles;\n+      } else {\n+        return deltaFiles.stream().filter(df -> stmtId.equals(df.getStmtId())).collect(Collectors.toList());\n+      }\n+    }\n+\n     @Override\n     public void write(DataOutput out) throws IOException {\n       out.writeLong(minWriteId);\n       out.writeLong(maxWriteId);\n       out.writeInt(stmtIds.size());\n-      for(Integer id : stmtIds) {\n+      for (Integer id : stmtIds) {\n         out.writeInt(id);\n       }\n       out.writeLong(visibilityTxnId);\n+      out.writeInt(deltaFiles.size());\n+      for (DeltaFileMetaData fileMeta : deltaFiles) {\n+        fileMeta.write(out);\n+      }\n     }\n+\n     @Override\n     public void readFields(DataInput in) throws IOException {\n       minWriteId = in.readLong();\n       maxWriteId = in.readLong();\n       stmtIds.clear();\n       int numStatements = in.readInt();\n-      for(int i = 0; i < numStatements; i++) {\n+      for (int i = 0; i < numStatements; i++) {\n         stmtIds.add(in.readInt());\n       }\n       visibilityTxnId = in.readLong();\n+\n+      deltaFiles.clear();\n+      int numFiles = in.readInt();\n+      for (int i = 0; i < numFiles; i++) {\n+        DeltaFileMetaData file = new DeltaFileMetaData();\n+        file.readFields(in);\n+        deltaFiles.add(file);\n+      }\n     }\n-    String getName() {\n+\n+    private String getName() {\n       assert stmtIds.isEmpty() : \"use getName(int)\";\n-      return AcidUtils.addVisibilitySuffix(AcidUtils\n-          .deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n+      return AcidUtils.addVisibilitySuffix(AcidUtils.deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n     }\n-    String getName(int stmtId) {\n+\n+    private String getName(int stmtId) {\n       assert !stmtIds.isEmpty() : \"use getName()\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId, stmtId), visibilityTxnId);\n     }\n+\n+    public List<Pair<Path, Integer>> getPaths(Path root) {\n+      if (stmtIds.isEmpty()) {\n+        return Collections.singletonList(new ImmutablePair<>(new Path(root, getName()), null));\n+      } else {\n+        // To support multistatement transactions we may have multiple directories corresponding to one DeltaMetaData\n+        return getStmtIds().stream()\n+            .map(stmtId -> new ImmutablePair<>(new Path(root, getName(stmtId)), stmtId)).collect(Collectors.toList());\n+      }\n+    }\n+\n     @Override\n     public String toString() {\n       return \"Delta(?,\" + minWriteId + \",\" + maxWriteId + \",\" + stmtIds + \",\" + visibilityTxnId + \")\";\n     }\n   }\n+\n+  final class DeltaFileMetaData implements Writable {\n+    private static final int HAS_LONG_FILEID_FLAG = 1;\n+    private static final int HAS_ATTEMPTID_FLAG = 2;\n+    private static final int HAS_STMTID_FLAG = 4;\n+\n+    private long modTime;\n+    private long length;\n+    // Optional\n+    private Integer attemptId;\n+    // Optional\n+    private Long fileId;\n+    // Optional, if the deltaMeta contains multiple stmtIds, it will contain this files parent's stmtId\n+    private Integer stmtId;\n+\n+    public DeltaFileMetaData() {\n+    }\n+\n+    public DeltaFileMetaData(HadoopShims.HdfsFileStatusWithId fileStatus, Integer stmtId) {\n+      modTime = fileStatus.getFileStatus().getModificationTime();\n+      length = fileStatus.getFileStatus().getLen();\n+      String attempt = AcidUtils.parseAttemptId(fileStatus.getFileStatus().getPath());\n+      attemptId = StringUtils.isEmpty(attempt) ? null : Integer.parseInt(attempt);\n+      fileId = fileStatus.getFileId();\n+      this.stmtId = stmtId;\n+    }\n+\n+    public DeltaFileMetaData(long modTime, long length, @Nullable Integer attemptId, @Nullable Long fileId,\n+        @Nullable Integer stmtId) {\n+      this.modTime = modTime;\n+      this.length = length;\n+      this.attemptId = attemptId;\n+      this.fileId = fileId;\n+      this.stmtId = stmtId;\n+    }\n+\n+    public void clearStmtId() {\n+      stmtId = null;\n+    }\n+\n+    public Integer getStmtId() {\n+      return stmtId;\n+    }\n+\n+    @Override\n+    public void write(DataOutput out) throws IOException {\n+      int flags = (fileId != null ? HAS_LONG_FILEID_FLAG : 0) |\n+          (attemptId != null ? HAS_ATTEMPTID_FLAG : 0) |\n+          (stmtId != null ? HAS_STMTID_FLAG : 0);\n+      out.writeByte(flags);\n+      out.writeLong(modTime);\n+      out.writeLong(length);\n+      if (attemptId != null) {\n+        out.writeInt(attemptId);\n+      }\n+      if (fileId != null) {\n+        out.writeLong(fileId);\n+      }\n+      if (stmtId != null) {\n+        out.writeInt(stmtId);\n+      }\n+    }\n+\n+    @Override\n+    public void readFields(DataInput in) throws IOException {\n+      byte flags = in.readByte();\n+      boolean hasLongFileId = (HAS_LONG_FILEID_FLAG & flags) != 0,\n+          hasAttemptId = (HAS_ATTEMPTID_FLAG & flags) != 0,\n+          hasStmtId = (HAS_STMTID_FLAG & flags) != 0;\n+      modTime = in.readLong();\n+      length = in.readLong();\n+      if (hasAttemptId) {\n+        attemptId = in.readInt();\n+      }\n+      if (hasLongFileId) {\n+        fileId = in.readLong();\n+      }\n+      if (hasStmtId) {\n+        stmtId = in.readInt();\n+      }\n+    }\n+\n+    public Object getFileId(Path deltaDirectory, int bucketId) {\n+      if (fileId != null) {\n+        return fileId;\n+      }\n+      // Calculate the synthetic fileid\n+      Path realPath = getPath(deltaDirectory, bucketId);\n+      return new SyntheticFileId(realPath, length, modTime);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwNTE4OQ=="}, "originalCommit": {"oid": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b"}, "originalPosition": 268}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTcxOTIyOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTo0MTo0M1rOG6zApQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNToyMToxMVrOG690iQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwNjM0MQ==", "bodyText": "Question: How often do we call this? Is it ok to calculate this every time, or it would be better to store in a way that is already filtered, like a map?", "url": "https://github.com/apache/hive/pull/1339#discussion_r464306341", "createdAt": "2020-08-03T09:41:43Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +126,217 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n+\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFiles bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    public DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<DeltaFileMetaData> deltaFiles) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = ObjectUtils.defaultIfNull(deltaFiles, new ArrayList<>());\n     }\n-    long getMinWriteId() {\n+\n+    public long getMinWriteId() {\n       return minWriteId;\n     }\n-    long getMaxWriteId() {\n+\n+    public long getMaxWriteId() {\n       return maxWriteId;\n     }\n-    List<Integer> getStmtIds() {\n+\n+    public List<Integer> getStmtIds() {\n       return stmtIds;\n     }\n-    long getVisibilityTxnId() {\n+\n+    public long getVisibilityTxnId() {\n       return visibilityTxnId;\n     }\n+\n+    public List<DeltaFileMetaData> getDeltaFiles() {\n+      return deltaFiles;\n+    }\n+\n+    public List<DeltaFileMetaData> getDeltaFilesForStmtId(final Integer stmtId) {\n+      if (stmtIds.size() <= 1 || stmtId == null) {\n+        // If it is not a multistatement delta, we do not store the stmtId in the file list\n+        return deltaFiles;\n+      } else {\n+        return deltaFiles.stream().filter(df -> stmtId.equals(df.getStmtId())).collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ4MzQ2NQ==", "bodyText": "I will be a very small list, I don't think it matters.", "url": "https://github.com/apache/hive/pull/1339#discussion_r464483465", "createdAt": "2020-08-03T15:21:11Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +126,217 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n+\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFiles bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    public DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<DeltaFileMetaData> deltaFiles) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = ObjectUtils.defaultIfNull(deltaFiles, new ArrayList<>());\n     }\n-    long getMinWriteId() {\n+\n+    public long getMinWriteId() {\n       return minWriteId;\n     }\n-    long getMaxWriteId() {\n+\n+    public long getMaxWriteId() {\n       return maxWriteId;\n     }\n-    List<Integer> getStmtIds() {\n+\n+    public List<Integer> getStmtIds() {\n       return stmtIds;\n     }\n-    long getVisibilityTxnId() {\n+\n+    public long getVisibilityTxnId() {\n       return visibilityTxnId;\n     }\n+\n+    public List<DeltaFileMetaData> getDeltaFiles() {\n+      return deltaFiles;\n+    }\n+\n+    public List<DeltaFileMetaData> getDeltaFilesForStmtId(final Integer stmtId) {\n+      if (stmtIds.size() <= 1 || stmtId == null) {\n+        // If it is not a multistatement delta, we do not store the stmtId in the file list\n+        return deltaFiles;\n+      } else {\n+        return deltaFiles.stream().filter(df -> stmtId.equals(df.getStmtId())).collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwNjM0MQ=="}, "originalCommit": {"oid": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTczNDAzOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTo0NTo0OFrOG6zJMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTo0NTo0OFrOG6zJMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwODUzMA==", "bodyText": "nit: extra space", "url": "https://github.com/apache/hive/pull/1339#discussion_r464308530", "createdAt": "2020-08-03T09:45:48Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java", "diffHunk": "@@ -2493,7 +2514,7 @@ private static Path chooseFile(Path baseOrDeltaDir, FileSystem fs) throws IOExce\n       }\n       FileStatus[] dataFiles;\n       try {\n-        dataFiles = fs.listStatus(new Path[]{baseOrDeltaDir}, originalBucketFilter);\n+        dataFiles = fs.listStatus(baseOrDeltaDir , originalBucketFilter);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b"}, "originalPosition": 337}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTczODc5OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTo0NzoxOFrOG6zMEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTo0NzoxOFrOG6zMEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMwOTI2NQ==", "bodyText": "nit: space <Path, Integer>", "url": "https://github.com/apache/hive/pull/1339#discussion_r464309265", "createdAt": "2020-08-03T09:47:18Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1574,20 +1577,23 @@ public int compareTo(CompressedOwid other) {\n       this.orcSplit = orcSplit;\n \n       try {\n-        final Path[] deleteDeltaDirs = getDeleteDeltaDirsFromSplit(orcSplit);\n-        if (deleteDeltaDirs.length > 0) {\n+        if (orcSplit.getDeltas().size() > 0) {\n           AcidOutputFormat.Options orcSplitMinMaxWriteIds =\n               AcidUtils.parseBaseOrDeltaBucketFilename(orcSplit.getPath(), conf);\n           int totalDeleteEventCount = 0;\n-          for (Path deleteDeltaDir : deleteDeltaDirs) {\n-            if (!isQualifiedDeleteDeltaForSplit(orcSplitMinMaxWriteIds, deleteDeltaDir)) {\n-              continue;\n-            }\n-            Path[] deleteDeltaFiles = OrcRawRecordMerger.getDeltaFiles(deleteDeltaDir, bucket,\n-                new OrcRawRecordMerger.Options().isCompacting(false), null);\n-            for (Path deleteDeltaFile : deleteDeltaFiles) {\n-              try {\n-                ReaderData readerData = getOrcTail(deleteDeltaFile, conf, cacheTag);\n+          for (AcidInputFormat.DeltaMetaData deltaMetaData : orcSplit.getDeltas()) {\n+            // We got one path for each statement in a multiStmt transaction\n+            for (Pair<Path,Integer> deleteDeltaDir : deltaMetaData.getPaths(orcSplit.getRootDir())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTc0NDkwOnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTo0OToxM1rOG6zP1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNTowMjo1MVrOG69HxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxMDIyOA==", "bodyText": "Do we need the order? Why not map?", "url": "https://github.com/apache/hive/pull/1339#discussion_r464310228", "createdAt": "2020-08-03T09:49:13Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +126,217 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n+\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFiles bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    public DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<DeltaFileMetaData> deltaFiles) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = ObjectUtils.defaultIfNull(deltaFiles, new ArrayList<>());\n     }\n-    long getMinWriteId() {\n+\n+    public long getMinWriteId() {\n       return minWriteId;\n     }\n-    long getMaxWriteId() {\n+\n+    public long getMaxWriteId() {\n       return maxWriteId;\n     }\n-    List<Integer> getStmtIds() {\n+\n+    public List<Integer> getStmtIds() {\n       return stmtIds;\n     }\n-    long getVisibilityTxnId() {\n+\n+    public long getVisibilityTxnId() {\n       return visibilityTxnId;\n     }\n+\n+    public List<DeltaFileMetaData> getDeltaFiles() {\n+      return deltaFiles;\n+    }\n+\n+    public List<DeltaFileMetaData> getDeltaFilesForStmtId(final Integer stmtId) {\n+      if (stmtIds.size() <= 1 || stmtId == null) {\n+        // If it is not a multistatement delta, we do not store the stmtId in the file list\n+        return deltaFiles;\n+      } else {\n+        return deltaFiles.stream().filter(df -> stmtId.equals(df.getStmtId())).collect(Collectors.toList());\n+      }\n+    }\n+\n     @Override\n     public void write(DataOutput out) throws IOException {\n       out.writeLong(minWriteId);\n       out.writeLong(maxWriteId);\n       out.writeInt(stmtIds.size());\n-      for(Integer id : stmtIds) {\n+      for (Integer id : stmtIds) {\n         out.writeInt(id);\n       }\n       out.writeLong(visibilityTxnId);\n+      out.writeInt(deltaFiles.size());\n+      for (DeltaFileMetaData fileMeta : deltaFiles) {\n+        fileMeta.write(out);\n+      }\n     }\n+\n     @Override\n     public void readFields(DataInput in) throws IOException {\n       minWriteId = in.readLong();\n       maxWriteId = in.readLong();\n       stmtIds.clear();\n       int numStatements = in.readInt();\n-      for(int i = 0; i < numStatements; i++) {\n+      for (int i = 0; i < numStatements; i++) {\n         stmtIds.add(in.readInt());\n       }\n       visibilityTxnId = in.readLong();\n+\n+      deltaFiles.clear();\n+      int numFiles = in.readInt();\n+      for (int i = 0; i < numFiles; i++) {\n+        DeltaFileMetaData file = new DeltaFileMetaData();\n+        file.readFields(in);\n+        deltaFiles.add(file);\n+      }\n     }\n-    String getName() {\n+\n+    private String getName() {\n       assert stmtIds.isEmpty() : \"use getName(int)\";\n-      return AcidUtils.addVisibilitySuffix(AcidUtils\n-          .deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n+      return AcidUtils.addVisibilitySuffix(AcidUtils.deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n     }\n-    String getName(int stmtId) {\n+\n+    private String getName(int stmtId) {\n       assert !stmtIds.isEmpty() : \"use getName()\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId, stmtId), visibilityTxnId);\n     }\n+\n+    public List<Pair<Path, Integer>> getPaths(Path root) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ3MjAwNQ==", "bodyText": "I think the List is much more straightforward, it will keep the stmid order.", "url": "https://github.com/apache/hive/pull/1339#discussion_r464472005", "createdAt": "2020-08-03T15:02:51Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/AcidInputFormat.java", "diffHunk": "@@ -118,70 +126,217 @@\n      */\n     private long visibilityTxnId;\n \n+    private List<DeltaFileMetaData> deltaFiles;\n+\n     public DeltaMetaData() {\n-      this(0,0,new ArrayList<Integer>(), 0);\n+      this(0, 0, new ArrayList<>(), 0, new ArrayList<>());\n     }\n+\n     /**\n+     * @param minWriteId min writeId of the delta directory\n+     * @param maxWriteId max writeId of the delta directory\n      * @param stmtIds delta dir suffixes when a single txn writes > 1 delta in the same partition\n      * @param visibilityTxnId maybe 0, if the dir name didn't have it.  txnid:0 is always visible\n+     * @param deltaFiles bucketFiles in the directory\n      */\n-    DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId) {\n+    public DeltaMetaData(long minWriteId, long maxWriteId, List<Integer> stmtIds, long visibilityTxnId,\n+        List<DeltaFileMetaData> deltaFiles) {\n       this.minWriteId = minWriteId;\n       this.maxWriteId = maxWriteId;\n       if (stmtIds == null) {\n         throw new IllegalArgumentException(\"stmtIds == null\");\n       }\n       this.stmtIds = stmtIds;\n       this.visibilityTxnId = visibilityTxnId;\n+      this.deltaFiles = ObjectUtils.defaultIfNull(deltaFiles, new ArrayList<>());\n     }\n-    long getMinWriteId() {\n+\n+    public long getMinWriteId() {\n       return minWriteId;\n     }\n-    long getMaxWriteId() {\n+\n+    public long getMaxWriteId() {\n       return maxWriteId;\n     }\n-    List<Integer> getStmtIds() {\n+\n+    public List<Integer> getStmtIds() {\n       return stmtIds;\n     }\n-    long getVisibilityTxnId() {\n+\n+    public long getVisibilityTxnId() {\n       return visibilityTxnId;\n     }\n+\n+    public List<DeltaFileMetaData> getDeltaFiles() {\n+      return deltaFiles;\n+    }\n+\n+    public List<DeltaFileMetaData> getDeltaFilesForStmtId(final Integer stmtId) {\n+      if (stmtIds.size() <= 1 || stmtId == null) {\n+        // If it is not a multistatement delta, we do not store the stmtId in the file list\n+        return deltaFiles;\n+      } else {\n+        return deltaFiles.stream().filter(df -> stmtId.equals(df.getStmtId())).collect(Collectors.toList());\n+      }\n+    }\n+\n     @Override\n     public void write(DataOutput out) throws IOException {\n       out.writeLong(minWriteId);\n       out.writeLong(maxWriteId);\n       out.writeInt(stmtIds.size());\n-      for(Integer id : stmtIds) {\n+      for (Integer id : stmtIds) {\n         out.writeInt(id);\n       }\n       out.writeLong(visibilityTxnId);\n+      out.writeInt(deltaFiles.size());\n+      for (DeltaFileMetaData fileMeta : deltaFiles) {\n+        fileMeta.write(out);\n+      }\n     }\n+\n     @Override\n     public void readFields(DataInput in) throws IOException {\n       minWriteId = in.readLong();\n       maxWriteId = in.readLong();\n       stmtIds.clear();\n       int numStatements = in.readInt();\n-      for(int i = 0; i < numStatements; i++) {\n+      for (int i = 0; i < numStatements; i++) {\n         stmtIds.add(in.readInt());\n       }\n       visibilityTxnId = in.readLong();\n+\n+      deltaFiles.clear();\n+      int numFiles = in.readInt();\n+      for (int i = 0; i < numFiles; i++) {\n+        DeltaFileMetaData file = new DeltaFileMetaData();\n+        file.readFields(in);\n+        deltaFiles.add(file);\n+      }\n     }\n-    String getName() {\n+\n+    private String getName() {\n       assert stmtIds.isEmpty() : \"use getName(int)\";\n-      return AcidUtils.addVisibilitySuffix(AcidUtils\n-          .deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n+      return AcidUtils.addVisibilitySuffix(AcidUtils.deleteDeltaSubdir(minWriteId, maxWriteId), visibilityTxnId);\n     }\n-    String getName(int stmtId) {\n+\n+    private String getName(int stmtId) {\n       assert !stmtIds.isEmpty() : \"use getName()\";\n       return AcidUtils.addVisibilitySuffix(AcidUtils\n           .deleteDeltaSubdir(minWriteId, maxWriteId, stmtId), visibilityTxnId);\n     }\n+\n+    public List<Pair<Path, Integer>> getPaths(Path root) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxMDIyOA=="}, "originalCommit": {"oid": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b"}, "originalPosition": 164}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTc1Mjc4OnYy", "diffSide": "RIGHT", "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTo1MToyMVrOG6zUVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNzo0MDoyMVrOG7Uy-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxMTM4Mg==", "bodyText": "nit: extra spaces?", "url": "https://github.com/apache/hive/pull/1339#discussion_r464311382", "createdAt": "2020-08-03T09:51:21Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1641,28 +1645,26 @@ public int compareTo(CompressedOwid other) {\n      * Check if the delete delta folder needs to be scanned for a given split's min/max write ids.\n      *\n      * @param orcSplitMinMaxWriteIds\n-     * @param deleteDeltaDir\n+     * @param deleteDelta\n+     * @param stmtId statementId of the deleteDelta if present\n      * @return true when  delete delta dir has to be scanned.\n      */\n     @VisibleForTesting\n     protected static boolean isQualifiedDeleteDeltaForSplit(AcidOutputFormat.Options orcSplitMinMaxWriteIds,\n-        Path deleteDeltaDir)\n-    {\n-      AcidUtils.ParsedDelta deleteDelta = AcidUtils.parsedDelta(deleteDeltaDir, false);\n+        AcidInputFormat.DeltaMetaData deleteDelta, Integer stmtId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ3MzEyNQ==", "bodyText": "it is the second line of parameters, no extra space here", "url": "https://github.com/apache/hive/pull/1339#discussion_r464473125", "createdAt": "2020-08-03T15:04:32Z", "author": {"login": "pvargacl"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1641,28 +1645,26 @@ public int compareTo(CompressedOwid other) {\n      * Check if the delete delta folder needs to be scanned for a given split's min/max write ids.\n      *\n      * @param orcSplitMinMaxWriteIds\n-     * @param deleteDeltaDir\n+     * @param deleteDelta\n+     * @param stmtId statementId of the deleteDelta if present\n      * @return true when  delete delta dir has to be scanned.\n      */\n     @VisibleForTesting\n     protected static boolean isQualifiedDeleteDeltaForSplit(AcidOutputFormat.Options orcSplitMinMaxWriteIds,\n-        Path deleteDeltaDir)\n-    {\n-      AcidUtils.ParsedDelta deleteDelta = AcidUtils.parsedDelta(deleteDeltaDir, false);\n+        AcidInputFormat.DeltaMetaData deleteDelta, Integer stmtId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxMTM4Mg=="}, "originalCommit": {"oid": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg1OTg5OQ==", "bodyText": ":)", "url": "https://github.com/apache/hive/pull/1339#discussion_r464859899", "createdAt": "2020-08-04T07:40:21Z", "author": {"login": "pvary"}, "path": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcAcidRowBatchReader.java", "diffHunk": "@@ -1641,28 +1645,26 @@ public int compareTo(CompressedOwid other) {\n      * Check if the delete delta folder needs to be scanned for a given split's min/max write ids.\n      *\n      * @param orcSplitMinMaxWriteIds\n-     * @param deleteDeltaDir\n+     * @param deleteDelta\n+     * @param stmtId statementId of the deleteDelta if present\n      * @return true when  delete delta dir has to be scanned.\n      */\n     @VisibleForTesting\n     protected static boolean isQualifiedDeleteDeltaForSplit(AcidOutputFormat.Options orcSplitMinMaxWriteIds,\n-        Path deleteDeltaDir)\n-    {\n-      AcidUtils.ParsedDelta deleteDelta = AcidUtils.parsedDelta(deleteDeltaDir, false);\n+        AcidInputFormat.DeltaMetaData deleteDelta, Integer stmtId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxMTM4Mg=="}, "originalCommit": {"oid": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b"}, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5OTc2MDE5OnYy", "diffSide": "RIGHT", "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTo1NDowMFrOG6zZJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QwOTo1NDowMFrOG6zZJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMxMjYxNQ==", "bodyText": "This is a valid test, but I think the testMultipleInserts test for inserts, and this is test for deletes. Maybe create its' own test method named testDeleteOfInserts like testUpdateOfInserts?", "url": "https://github.com/apache/hive/pull/1339#discussion_r464312615", "createdAt": "2020-08-03T09:54:00Z", "author": {"login": "pvary"}, "path": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java", "diffHunk": "@@ -618,7 +618,13 @@ public void testMultipleInserts() throws Exception {\n     dumpTableData(Table.ACIDTBL, 1, 1);\n     List<String> rs1 = runStatementOnDriver(\"select a,b from \" + Table.ACIDTBL + \" order by a,b\");\n     Assert.assertEquals(\"Content didn't match after commit rs1\", allData, rs1);\n+    runStatementOnDriver(\"delete from \" + Table.ACIDTBL + \" where b = 2\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f400cb2c757f82ff3387b7e3bd74ca5d29de572b"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 474, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}