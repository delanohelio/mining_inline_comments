{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMwNTU2ODI0", "number": 3895, "title": "Zero Downtime Deployment Design", "bodyText": "", "createdAt": "2020-12-01T20:29:56Z", "url": "https://github.com/telstra/open-kilda/pull/3895", "merged": true, "mergeCommit": {"oid": "fdd73756f451502aaeb58700a4ccc0106256a7e7"}, "closed": true, "closedAt": "2020-12-31T09:13:24Z", "author": {"login": "timofei-durakov"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdiMnSZAFqTU0MjYyMDc5Mw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdrgR_6ABqjQxNTkyODY0NDk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyNjIwNzkz", "url": "https://github.com/telstra/open-kilda/pull/3895#pullrequestreview-542620793", "createdAt": "2020-12-02T09:02:04Z", "commit": {"oid": "553bbc5b1e4b072a8522a6046b66351fc9e53a25"}, "state": "APPROVED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwOTowMjowNFrOH9QtPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxMToxMTowN1rOH9V-3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzk5ODkxMQ==", "bodyText": "described", "url": "https://github.com/telstra/open-kilda/pull/3895#discussion_r533998911", "createdAt": "2020-12-02T09:02:04Z", "author": {"login": "rtretyak"}, "path": "docs/design/zero-downtime/README.md", "diffHunk": "@@ -0,0 +1,62 @@\n+# Zero Downtime Upgrades for Open Kilda\n+\n+## Rationale\n+Current deployment process requires time slot with no reactions from contol plane for any network events as long\n+as no flow/switch ops are available as well. To improve this new deployment procedure is proposed.\n+It's details are descrivbed in this document.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "553bbc5b1e4b072a8522a6046b66351fc9e53a25"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDAwMDI3Mg==", "bodyText": "provides", "url": "https://github.com/telstra/open-kilda/pull/3895#discussion_r534000272", "createdAt": "2020-12-02T09:04:08Z", "author": {"login": "rtretyak"}, "path": "docs/design/zero-downtime/README.md", "diffHunk": "@@ -0,0 +1,62 @@\n+# Zero Downtime Upgrades for Open Kilda\n+\n+## Rationale\n+Current deployment process requires time slot with no reactions from contol plane for any network events as long\n+as no flow/switch ops are available as well. To improve this new deployment procedure is proposed.\n+It's details are descrivbed in this document.\n+\n+## Solution\n+\n+Provide ability to deploy kilda in so called blue/green mode. Which will allow to quickly switch\n+system between different release versions. However, due to event-driven nature of kilda and a bunch of\n+limitations from Apache Storm there are changes made in original Blue/Green approach.\n+\n+### Shared Transport\n+\n+Kilda uses Kafka as a Message broker and messages in it to communicate between components of the system.\n+To provide backward compatibility between green and new versions of kilda within the same kafka topic\n+we need to split messages by some value between new and old ones. For this purpose will be used kafka message\n+header with run-id encoded in it. Run id should be unique within a single deployment.\n+Each component will emit messages with run-id header that is valid by the deployment. All receiver parts\n+should validate message header first and verify that run id matches with its configuration in that case component\n+will handle the message. Kafka provide API to write custom interceptors for both consumer and producer. This", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "553bbc5b1e4b072a8522a6046b66351fc9e53a25"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDAwMjU2MA==", "bodyText": "hab->hub", "url": "https://github.com/telstra/open-kilda/pull/3895#discussion_r534002560", "createdAt": "2020-12-02T09:07:39Z", "author": {"login": "rtretyak"}, "path": "docs/design/zero-downtime/README.md", "diffHunk": "@@ -0,0 +1,62 @@\n+# Zero Downtime Upgrades for Open Kilda\n+\n+## Rationale\n+Current deployment process requires time slot with no reactions from contol plane for any network events as long\n+as no flow/switch ops are available as well. To improve this new deployment procedure is proposed.\n+It's details are descrivbed in this document.\n+\n+## Solution\n+\n+Provide ability to deploy kilda in so called blue/green mode. Which will allow to quickly switch\n+system between different release versions. However, due to event-driven nature of kilda and a bunch of\n+limitations from Apache Storm there are changes made in original Blue/Green approach.\n+\n+### Shared Transport\n+\n+Kilda uses Kafka as a Message broker and messages in it to communicate between components of the system.\n+To provide backward compatibility between green and new versions of kilda within the same kafka topic\n+we need to split messages by some value between new and old ones. For this purpose will be used kafka message\n+header with run-id encoded in it. Run id should be unique within a single deployment.\n+Each component will emit messages with run-id header that is valid by the deployment. All receiver parts\n+should validate message header first and verify that run id matches with its configuration in that case component\n+will handle the message. Kafka provide API to write custom interceptors for both consumer and producer. This\n+interceptor will be responsible for verifying deployment id.\n+\n+### Zookeeper to store the state\n+\n+Since storm has limitations on lifecycle of its topologies, new mechanism is required to deal with topologies states. \n+Also it should be responsible to handle graceful shutdown procedure for topologies. For this role Apache Zookeeper\n+looks like a good fit.\n+\n+#### Node structure for Zookeeper\n+\n+`/kilda/{component_type}/{env_id}` - root for every component process, where:\n+`component_type` - topology or service name, e.g. `floodlight`, `network`, `nb_worker`\n+`env_id` - flag of blue or green env\n+\n+#### Signal, States and Build-Version\n+\n+Each component root node will have 3 children zNodes:\n+- signal - input field, can be `START` or `SHUTDOWN`, the way to make component start emittin processing new events\n+- state - int, number of active subcomponents of a component, when `SHUTDOWN` is emitted, should be `0`, positive otherwise.\n+- build-version - string field with run id, could be changed on fly, see #Shared transport for details\n+For the long running task such as hub in hab and spoke topologies, there should be a way to stop receiving new", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "553bbc5b1e4b072a8522a6046b66351fc9e53a25"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDAwMjg5MA==", "bodyText": "floodlifghts >floodlights\neven are odd > even are green", "url": "https://github.com/telstra/open-kilda/pull/3895#discussion_r534002890", "createdAt": "2020-12-02T09:08:08Z", "author": {"login": "rtretyak"}, "path": "docs/design/zero-downtime/README.md", "diffHunk": "@@ -0,0 +1,62 @@\n+# Zero Downtime Upgrades for Open Kilda\n+\n+## Rationale\n+Current deployment process requires time slot with no reactions from contol plane for any network events as long\n+as no flow/switch ops are available as well. To improve this new deployment procedure is proposed.\n+It's details are descrivbed in this document.\n+\n+## Solution\n+\n+Provide ability to deploy kilda in so called blue/green mode. Which will allow to quickly switch\n+system between different release versions. However, due to event-driven nature of kilda and a bunch of\n+limitations from Apache Storm there are changes made in original Blue/Green approach.\n+\n+### Shared Transport\n+\n+Kilda uses Kafka as a Message broker and messages in it to communicate between components of the system.\n+To provide backward compatibility between green and new versions of kilda within the same kafka topic\n+we need to split messages by some value between new and old ones. For this purpose will be used kafka message\n+header with run-id encoded in it. Run id should be unique within a single deployment.\n+Each component will emit messages with run-id header that is valid by the deployment. All receiver parts\n+should validate message header first and verify that run id matches with its configuration in that case component\n+will handle the message. Kafka provide API to write custom interceptors for both consumer and producer. This\n+interceptor will be responsible for verifying deployment id.\n+\n+### Zookeeper to store the state\n+\n+Since storm has limitations on lifecycle of its topologies, new mechanism is required to deal with topologies states. \n+Also it should be responsible to handle graceful shutdown procedure for topologies. For this role Apache Zookeeper\n+looks like a good fit.\n+\n+#### Node structure for Zookeeper\n+\n+`/kilda/{component_type}/{env_id}` - root for every component process, where:\n+`component_type` - topology or service name, e.g. `floodlight`, `network`, `nb_worker`\n+`env_id` - flag of blue or green env\n+\n+#### Signal, States and Build-Version\n+\n+Each component root node will have 3 children zNodes:\n+- signal - input field, can be `START` or `SHUTDOWN`, the way to make component start emittin processing new events\n+- state - int, number of active subcomponents of a component, when `SHUTDOWN` is emitted, should be `0`, positive otherwise.\n+- build-version - string field with run id, could be changed on fly, see #Shared transport for details\n+For the long running task such as hub in hab and spoke topologies, there should be a way to stop receiving new\n+requests, finishing up existing, and after that decrementing counter by 1 in a state field.\n+\n+#### Basic Deployment Scenario:\n+\n+Since floodlifghts could be destinguished by region let's assume, that all odd regions are blue and even are odd", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "553bbc5b1e4b072a8522a6046b66351fc9e53a25"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDAwMzUwNw==", "bodyText": "should look like or should be like", "url": "https://github.com/telstra/open-kilda/pull/3895#discussion_r534003507", "createdAt": "2020-12-02T09:09:08Z", "author": {"login": "rtretyak"}, "path": "docs/design/zero-downtime/README.md", "diffHunk": "@@ -0,0 +1,62 @@\n+# Zero Downtime Upgrades for Open Kilda\n+\n+## Rationale\n+Current deployment process requires time slot with no reactions from contol plane for any network events as long\n+as no flow/switch ops are available as well. To improve this new deployment procedure is proposed.\n+It's details are descrivbed in this document.\n+\n+## Solution\n+\n+Provide ability to deploy kilda in so called blue/green mode. Which will allow to quickly switch\n+system between different release versions. However, due to event-driven nature of kilda and a bunch of\n+limitations from Apache Storm there are changes made in original Blue/Green approach.\n+\n+### Shared Transport\n+\n+Kilda uses Kafka as a Message broker and messages in it to communicate between components of the system.\n+To provide backward compatibility between green and new versions of kilda within the same kafka topic\n+we need to split messages by some value between new and old ones. For this purpose will be used kafka message\n+header with run-id encoded in it. Run id should be unique within a single deployment.\n+Each component will emit messages with run-id header that is valid by the deployment. All receiver parts\n+should validate message header first and verify that run id matches with its configuration in that case component\n+will handle the message. Kafka provide API to write custom interceptors for both consumer and producer. This\n+interceptor will be responsible for verifying deployment id.\n+\n+### Zookeeper to store the state\n+\n+Since storm has limitations on lifecycle of its topologies, new mechanism is required to deal with topologies states. \n+Also it should be responsible to handle graceful shutdown procedure for topologies. For this role Apache Zookeeper\n+looks like a good fit.\n+\n+#### Node structure for Zookeeper\n+\n+`/kilda/{component_type}/{env_id}` - root for every component process, where:\n+`component_type` - topology or service name, e.g. `floodlight`, `network`, `nb_worker`\n+`env_id` - flag of blue or green env\n+\n+#### Signal, States and Build-Version\n+\n+Each component root node will have 3 children zNodes:\n+- signal - input field, can be `START` or `SHUTDOWN`, the way to make component start emittin processing new events\n+- state - int, number of active subcomponents of a component, when `SHUTDOWN` is emitted, should be `0`, positive otherwise.\n+- build-version - string field with run id, could be changed on fly, see #Shared transport for details\n+For the long running task such as hub in hab and spoke topologies, there should be a way to stop receiving new\n+requests, finishing up existing, and after that decrementing counter by 1 in a state field.\n+\n+#### Basic Deployment Scenario:\n+\n+Since floodlifghts could be destinguished by region let's assume, that all odd regions are blue and even are odd\n+Based on that deployment process should be look like:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "553bbc5b1e4b072a8522a6046b66351fc9e53a25"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDAxMDI2OQ==", "bodyText": "worth mentioning that we should change their build-versions to match green floodlight before sending START signal", "url": "https://github.com/telstra/open-kilda/pull/3895#discussion_r534010269", "createdAt": "2020-12-02T09:18:57Z", "author": {"login": "rtretyak"}, "path": "docs/design/zero-downtime/README.md", "diffHunk": "@@ -0,0 +1,62 @@\n+# Zero Downtime Upgrades for Open Kilda\n+\n+## Rationale\n+Current deployment process requires time slot with no reactions from contol plane for any network events as long\n+as no flow/switch ops are available as well. To improve this new deployment procedure is proposed.\n+It's details are descrivbed in this document.\n+\n+## Solution\n+\n+Provide ability to deploy kilda in so called blue/green mode. Which will allow to quickly switch\n+system between different release versions. However, due to event-driven nature of kilda and a bunch of\n+limitations from Apache Storm there are changes made in original Blue/Green approach.\n+\n+### Shared Transport\n+\n+Kilda uses Kafka as a Message broker and messages in it to communicate between components of the system.\n+To provide backward compatibility between green and new versions of kilda within the same kafka topic\n+we need to split messages by some value between new and old ones. For this purpose will be used kafka message\n+header with run-id encoded in it. Run id should be unique within a single deployment.\n+Each component will emit messages with run-id header that is valid by the deployment. All receiver parts\n+should validate message header first and verify that run id matches with its configuration in that case component\n+will handle the message. Kafka provide API to write custom interceptors for both consumer and producer. This\n+interceptor will be responsible for verifying deployment id.\n+\n+### Zookeeper to store the state\n+\n+Since storm has limitations on lifecycle of its topologies, new mechanism is required to deal with topologies states. \n+Also it should be responsible to handle graceful shutdown procedure for topologies. For this role Apache Zookeeper\n+looks like a good fit.\n+\n+#### Node structure for Zookeeper\n+\n+`/kilda/{component_type}/{env_id}` - root for every component process, where:\n+`component_type` - topology or service name, e.g. `floodlight`, `network`, `nb_worker`\n+`env_id` - flag of blue or green env\n+\n+#### Signal, States and Build-Version\n+\n+Each component root node will have 3 children zNodes:\n+- signal - input field, can be `START` or `SHUTDOWN`, the way to make component start emittin processing new events\n+- state - int, number of active subcomponents of a component, when `SHUTDOWN` is emitted, should be `0`, positive otherwise.\n+- build-version - string field with run id, could be changed on fly, see #Shared transport for details\n+For the long running task such as hub in hab and spoke topologies, there should be a way to stop receiving new\n+requests, finishing up existing, and after that decrementing counter by 1 in a state field.\n+\n+#### Basic Deployment Scenario:\n+\n+Since floodlifghts could be destinguished by region let's assume, that all odd regions are blue and even are odd\n+Based on that deployment process should be look like:\n+- Set `SHUTDOWN` signal for odd floodlights and change it's build-version for the new one\n+- Redeploy floodlight containers of green color\n+- Ensure that all green components has a signal `SHUTDOWN` and state of each has 0 active workers.\n+- Deploy all new topologies for green\n+- Emit `START` for green floodlight\n+- Emit `START` for `router`, `flow_hs`, `reroute`, `switch_manager`, `nb_worker` of green", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "553bbc5b1e4b072a8522a6046b66351fc9e53a25"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDAxMzU2NQ==", "bodyText": "it's -> their", "url": "https://github.com/telstra/open-kilda/pull/3895#discussion_r534013565", "createdAt": "2020-12-02T09:23:47Z", "author": {"login": "rtretyak"}, "path": "docs/design/zero-downtime/README.md", "diffHunk": "@@ -0,0 +1,62 @@\n+# Zero Downtime Upgrades for Open Kilda\n+\n+## Rationale\n+Current deployment process requires time slot with no reactions from contol plane for any network events as long\n+as no flow/switch ops are available as well. To improve this new deployment procedure is proposed.\n+It's details are descrivbed in this document.\n+\n+## Solution\n+\n+Provide ability to deploy kilda in so called blue/green mode. Which will allow to quickly switch\n+system between different release versions. However, due to event-driven nature of kilda and a bunch of\n+limitations from Apache Storm there are changes made in original Blue/Green approach.\n+\n+### Shared Transport\n+\n+Kilda uses Kafka as a Message broker and messages in it to communicate between components of the system.\n+To provide backward compatibility between green and new versions of kilda within the same kafka topic\n+we need to split messages by some value between new and old ones. For this purpose will be used kafka message\n+header with run-id encoded in it. Run id should be unique within a single deployment.\n+Each component will emit messages with run-id header that is valid by the deployment. All receiver parts\n+should validate message header first and verify that run id matches with its configuration in that case component\n+will handle the message. Kafka provide API to write custom interceptors for both consumer and producer. This\n+interceptor will be responsible for verifying deployment id.\n+\n+### Zookeeper to store the state\n+\n+Since storm has limitations on lifecycle of its topologies, new mechanism is required to deal with topologies states. \n+Also it should be responsible to handle graceful shutdown procedure for topologies. For this role Apache Zookeeper\n+looks like a good fit.\n+\n+#### Node structure for Zookeeper\n+\n+`/kilda/{component_type}/{env_id}` - root for every component process, where:\n+`component_type` - topology or service name, e.g. `floodlight`, `network`, `nb_worker`\n+`env_id` - flag of blue or green env\n+\n+#### Signal, States and Build-Version\n+\n+Each component root node will have 3 children zNodes:\n+- signal - input field, can be `START` or `SHUTDOWN`, the way to make component start emittin processing new events\n+- state - int, number of active subcomponents of a component, when `SHUTDOWN` is emitted, should be `0`, positive otherwise.\n+- build-version - string field with run id, could be changed on fly, see #Shared transport for details\n+For the long running task such as hub in hab and spoke topologies, there should be a way to stop receiving new\n+requests, finishing up existing, and after that decrementing counter by 1 in a state field.\n+\n+#### Basic Deployment Scenario:\n+\n+Since floodlifghts could be destinguished by region let's assume, that all odd regions are blue and even are odd\n+Based on that deployment process should be look like:\n+- Set `SHUTDOWN` signal for odd floodlights and change it's build-version for the new one", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "553bbc5b1e4b072a8522a6046b66351fc9e53a25"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDAxNTA4Mw==", "bodyText": "components have", "url": "https://github.com/telstra/open-kilda/pull/3895#discussion_r534015083", "createdAt": "2020-12-02T09:25:47Z", "author": {"login": "rtretyak"}, "path": "docs/design/zero-downtime/README.md", "diffHunk": "@@ -0,0 +1,62 @@\n+# Zero Downtime Upgrades for Open Kilda\n+\n+## Rationale\n+Current deployment process requires time slot with no reactions from contol plane for any network events as long\n+as no flow/switch ops are available as well. To improve this new deployment procedure is proposed.\n+It's details are descrivbed in this document.\n+\n+## Solution\n+\n+Provide ability to deploy kilda in so called blue/green mode. Which will allow to quickly switch\n+system between different release versions. However, due to event-driven nature of kilda and a bunch of\n+limitations from Apache Storm there are changes made in original Blue/Green approach.\n+\n+### Shared Transport\n+\n+Kilda uses Kafka as a Message broker and messages in it to communicate between components of the system.\n+To provide backward compatibility between green and new versions of kilda within the same kafka topic\n+we need to split messages by some value between new and old ones. For this purpose will be used kafka message\n+header with run-id encoded in it. Run id should be unique within a single deployment.\n+Each component will emit messages with run-id header that is valid by the deployment. All receiver parts\n+should validate message header first and verify that run id matches with its configuration in that case component\n+will handle the message. Kafka provide API to write custom interceptors for both consumer and producer. This\n+interceptor will be responsible for verifying deployment id.\n+\n+### Zookeeper to store the state\n+\n+Since storm has limitations on lifecycle of its topologies, new mechanism is required to deal with topologies states. \n+Also it should be responsible to handle graceful shutdown procedure for topologies. For this role Apache Zookeeper\n+looks like a good fit.\n+\n+#### Node structure for Zookeeper\n+\n+`/kilda/{component_type}/{env_id}` - root for every component process, where:\n+`component_type` - topology or service name, e.g. `floodlight`, `network`, `nb_worker`\n+`env_id` - flag of blue or green env\n+\n+#### Signal, States and Build-Version\n+\n+Each component root node will have 3 children zNodes:\n+- signal - input field, can be `START` or `SHUTDOWN`, the way to make component start emittin processing new events\n+- state - int, number of active subcomponents of a component, when `SHUTDOWN` is emitted, should be `0`, positive otherwise.\n+- build-version - string field with run id, could be changed on fly, see #Shared transport for details\n+For the long running task such as hub in hab and spoke topologies, there should be a way to stop receiving new\n+requests, finishing up existing, and after that decrementing counter by 1 in a state field.\n+\n+#### Basic Deployment Scenario:\n+\n+Since floodlifghts could be destinguished by region let's assume, that all odd regions are blue and even are odd\n+Based on that deployment process should be look like:\n+- Set `SHUTDOWN` signal for odd floodlights and change it's build-version for the new one\n+- Redeploy floodlight containers of green color\n+- Ensure that all green components has a signal `SHUTDOWN` and state of each has 0 active workers.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "553bbc5b1e4b072a8522a6046b66351fc9e53a25"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDAxOTk0Nw==", "bodyText": "is it only 'network' topology that can conflict between blue/green versions? Is there any event that we can receive pass the network topology?", "url": "https://github.com/telstra/open-kilda/pull/3895#discussion_r534019947", "createdAt": "2020-12-02T09:32:45Z", "author": {"login": "rtretyak"}, "path": "docs/design/zero-downtime/README.md", "diffHunk": "@@ -0,0 +1,62 @@\n+# Zero Downtime Upgrades for Open Kilda\n+\n+## Rationale\n+Current deployment process requires time slot with no reactions from contol plane for any network events as long\n+as no flow/switch ops are available as well. To improve this new deployment procedure is proposed.\n+It's details are descrivbed in this document.\n+\n+## Solution\n+\n+Provide ability to deploy kilda in so called blue/green mode. Which will allow to quickly switch\n+system between different release versions. However, due to event-driven nature of kilda and a bunch of\n+limitations from Apache Storm there are changes made in original Blue/Green approach.\n+\n+### Shared Transport\n+\n+Kilda uses Kafka as a Message broker and messages in it to communicate between components of the system.\n+To provide backward compatibility between green and new versions of kilda within the same kafka topic\n+we need to split messages by some value between new and old ones. For this purpose will be used kafka message\n+header with run-id encoded in it. Run id should be unique within a single deployment.\n+Each component will emit messages with run-id header that is valid by the deployment. All receiver parts\n+should validate message header first and verify that run id matches with its configuration in that case component\n+will handle the message. Kafka provide API to write custom interceptors for both consumer and producer. This\n+interceptor will be responsible for verifying deployment id.\n+\n+### Zookeeper to store the state\n+\n+Since storm has limitations on lifecycle of its topologies, new mechanism is required to deal with topologies states. \n+Also it should be responsible to handle graceful shutdown procedure for topologies. For this role Apache Zookeeper\n+looks like a good fit.\n+\n+#### Node structure for Zookeeper\n+\n+`/kilda/{component_type}/{env_id}` - root for every component process, where:\n+`component_type` - topology or service name, e.g. `floodlight`, `network`, `nb_worker`\n+`env_id` - flag of blue or green env\n+\n+#### Signal, States and Build-Version\n+\n+Each component root node will have 3 children zNodes:\n+- signal - input field, can be `START` or `SHUTDOWN`, the way to make component start emittin processing new events\n+- state - int, number of active subcomponents of a component, when `SHUTDOWN` is emitted, should be `0`, positive otherwise.\n+- build-version - string field with run id, could be changed on fly, see #Shared transport for details\n+For the long running task such as hub in hab and spoke topologies, there should be a way to stop receiving new\n+requests, finishing up existing, and after that decrementing counter by 1 in a state field.\n+\n+#### Basic Deployment Scenario:\n+\n+Since floodlifghts could be destinguished by region let's assume, that all odd regions are blue and even are odd\n+Based on that deployment process should be look like:\n+- Set `SHUTDOWN` signal for odd floodlights and change it's build-version for the new one\n+- Redeploy floodlight containers of green color\n+- Ensure that all green components has a signal `SHUTDOWN` and state of each has 0 active workers.\n+- Deploy all new topologies for green\n+- Emit `START` for green floodlight\n+- Emit `START` for `router`, `flow_hs`, `reroute`, `switch_manager`, `nb_worker` of green\n+- Verify state is changed, and each has positive number value\n+- Emit `SHUTDOWN` for the network blue\n+- Emit `START` for the network green\n+- Emit `SHUTDOWN` for `router`, `flow_hs`, `reroute`, `switch_manager`, `nb_worker` of blue", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "553bbc5b1e4b072a8522a6046b66351fc9e53a25"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDA4MzUwOQ==", "bodyText": "Since both 'routers' and both floodlights are UP theoretically a switch can be marked as 'active' in both green and blue floodlights (in both regions). Is that correct? In this case we may process certain event twice", "url": "https://github.com/telstra/open-kilda/pull/3895#discussion_r534083509", "createdAt": "2020-12-02T11:08:11Z", "author": {"login": "rtretyak"}, "path": "docs/design/zero-downtime/README.md", "diffHunk": "@@ -0,0 +1,62 @@\n+# Zero Downtime Upgrades for Open Kilda\n+\n+## Rationale\n+Current deployment process requires time slot with no reactions from contol plane for any network events as long\n+as no flow/switch ops are available as well. To improve this new deployment procedure is proposed.\n+It's details are descrivbed in this document.\n+\n+## Solution\n+\n+Provide ability to deploy kilda in so called blue/green mode. Which will allow to quickly switch\n+system between different release versions. However, due to event-driven nature of kilda and a bunch of\n+limitations from Apache Storm there are changes made in original Blue/Green approach.\n+\n+### Shared Transport\n+\n+Kilda uses Kafka as a Message broker and messages in it to communicate between components of the system.\n+To provide backward compatibility between green and new versions of kilda within the same kafka topic\n+we need to split messages by some value between new and old ones. For this purpose will be used kafka message\n+header with run-id encoded in it. Run id should be unique within a single deployment.\n+Each component will emit messages with run-id header that is valid by the deployment. All receiver parts\n+should validate message header first and verify that run id matches with its configuration in that case component\n+will handle the message. Kafka provide API to write custom interceptors for both consumer and producer. This\n+interceptor will be responsible for verifying deployment id.\n+\n+### Zookeeper to store the state\n+\n+Since storm has limitations on lifecycle of its topologies, new mechanism is required to deal with topologies states. \n+Also it should be responsible to handle graceful shutdown procedure for topologies. For this role Apache Zookeeper\n+looks like a good fit.\n+\n+#### Node structure for Zookeeper\n+\n+`/kilda/{component_type}/{env_id}` - root for every component process, where:\n+`component_type` - topology or service name, e.g. `floodlight`, `network`, `nb_worker`\n+`env_id` - flag of blue or green env\n+\n+#### Signal, States and Build-Version\n+\n+Each component root node will have 3 children zNodes:\n+- signal - input field, can be `START` or `SHUTDOWN`, the way to make component start emittin processing new events\n+- state - int, number of active subcomponents of a component, when `SHUTDOWN` is emitted, should be `0`, positive otherwise.\n+- build-version - string field with run id, could be changed on fly, see #Shared transport for details\n+For the long running task such as hub in hab and spoke topologies, there should be a way to stop receiving new\n+requests, finishing up existing, and after that decrementing counter by 1 in a state field.\n+\n+#### Basic Deployment Scenario:\n+\n+Since floodlifghts could be destinguished by region let's assume, that all odd regions are blue and even are odd\n+Based on that deployment process should be look like:\n+- Set `SHUTDOWN` signal for odd floodlights and change it's build-version for the new one\n+- Redeploy floodlight containers of green color\n+- Ensure that all green components has a signal `SHUTDOWN` and state of each has 0 active workers.\n+- Deploy all new topologies for green\n+- Emit `START` for green floodlight\n+- Emit `START` for `router`, `flow_hs`, `reroute`, `switch_manager`, `nb_worker` of green\n+- Verify state is changed, and each has positive number value\n+- Emit `SHUTDOWN` for the network blue\n+- Emit `START` for the network green\n+- Emit `SHUTDOWN` for `router`, `flow_hs`, `reroute`, `switch_manager`, `nb_worker` of blue", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "553bbc5b1e4b072a8522a6046b66351fc9e53a25"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDA4NTM0MA==", "bodyText": "Please add a note that it is required for switch to be connected to at least 2 regions/equal controllers for this scenario", "url": "https://github.com/telstra/open-kilda/pull/3895#discussion_r534085340", "createdAt": "2020-12-02T11:11:07Z", "author": {"login": "rtretyak"}, "path": "docs/design/zero-downtime/README.md", "diffHunk": "@@ -0,0 +1,62 @@\n+# Zero Downtime Upgrades for Open Kilda\n+\n+## Rationale\n+Current deployment process requires time slot with no reactions from contol plane for any network events as long\n+as no flow/switch ops are available as well. To improve this new deployment procedure is proposed.\n+It's details are descrivbed in this document.\n+\n+## Solution\n+\n+Provide ability to deploy kilda in so called blue/green mode. Which will allow to quickly switch\n+system between different release versions. However, due to event-driven nature of kilda and a bunch of\n+limitations from Apache Storm there are changes made in original Blue/Green approach.\n+\n+### Shared Transport\n+\n+Kilda uses Kafka as a Message broker and messages in it to communicate between components of the system.\n+To provide backward compatibility between green and new versions of kilda within the same kafka topic\n+we need to split messages by some value between new and old ones. For this purpose will be used kafka message\n+header with run-id encoded in it. Run id should be unique within a single deployment.\n+Each component will emit messages with run-id header that is valid by the deployment. All receiver parts\n+should validate message header first and verify that run id matches with its configuration in that case component\n+will handle the message. Kafka provide API to write custom interceptors for both consumer and producer. This\n+interceptor will be responsible for verifying deployment id.\n+\n+### Zookeeper to store the state\n+\n+Since storm has limitations on lifecycle of its topologies, new mechanism is required to deal with topologies states. \n+Also it should be responsible to handle graceful shutdown procedure for topologies. For this role Apache Zookeeper\n+looks like a good fit.\n+\n+#### Node structure for Zookeeper\n+\n+`/kilda/{component_type}/{env_id}` - root for every component process, where:\n+`component_type` - topology or service name, e.g. `floodlight`, `network`, `nb_worker`\n+`env_id` - flag of blue or green env\n+\n+#### Signal, States and Build-Version\n+\n+Each component root node will have 3 children zNodes:\n+- signal - input field, can be `START` or `SHUTDOWN`, the way to make component start emittin processing new events\n+- state - int, number of active subcomponents of a component, when `SHUTDOWN` is emitted, should be `0`, positive otherwise.\n+- build-version - string field with run id, could be changed on fly, see #Shared transport for details\n+For the long running task such as hub in hab and spoke topologies, there should be a way to stop receiving new\n+requests, finishing up existing, and after that decrementing counter by 1 in a state field.\n+\n+#### Basic Deployment Scenario:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "553bbc5b1e4b072a8522a6046b66351fc9e53a25"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQzNjc0ODU0", "url": "https://github.com/telstra/open-kilda/pull/3895#pullrequestreview-543674854", "createdAt": "2020-12-03T07:38:18Z", "commit": {"oid": "553bbc5b1e4b072a8522a6046b66351fc9e53a25"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "553bbc5b1e4b072a8522a6046b66351fc9e53a25", "author": {"user": {"login": "timofei-durakov", "name": "Timofey Durakov"}}, "url": "https://github.com/telstra/open-kilda/commit/553bbc5b1e4b072a8522a6046b66351fc9e53a25", "committedDate": "2020-12-01T20:28:15Z", "message": "Zero Downtime Deployment Design"}, "afterCommit": {"oid": "9bcc510b03bb23ca1ad6466db222bf83eca7c99b", "author": {"user": {"login": "timofei-durakov", "name": "Timofey Durakov"}}, "url": "https://github.com/telstra/open-kilda/commit/9bcc510b03bb23ca1ad6466db222bf83eca7c99b", "committedDate": "2020-12-03T09:18:24Z", "message": "Zero Downtime Deployment Design"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ1ODUxMDUw", "url": "https://github.com/telstra/open-kilda/pull/3895#pullrequestreview-545851050", "createdAt": "2020-12-07T06:03:42Z", "commit": {"oid": "9bcc510b03bb23ca1ad6466db222bf83eca7c99b"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9bcc510b03bb23ca1ad6466db222bf83eca7c99b", "author": {"user": {"login": "timofei-durakov", "name": "Timofey Durakov"}}, "url": "https://github.com/telstra/open-kilda/commit/9bcc510b03bb23ca1ad6466db222bf83eca7c99b", "committedDate": "2020-12-03T09:18:24Z", "message": "Zero Downtime Deployment Design"}, "afterCommit": {"oid": "ace96b3057255a2cb7af3ae11620593bdee69394", "author": {"user": {"login": "timofei-durakov", "name": "Timofey Durakov"}}, "url": "https://github.com/telstra/open-kilda/commit/ace96b3057255a2cb7af3ae11620593bdee69394", "committedDate": "2020-12-29T08:50:54Z", "message": "Zero Downtime Deployment Design"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ace96b3057255a2cb7af3ae11620593bdee69394", "author": {"user": {"login": "timofei-durakov", "name": "Timofey Durakov"}}, "url": "https://github.com/telstra/open-kilda/commit/ace96b3057255a2cb7af3ae11620593bdee69394", "committedDate": "2020-12-29T08:50:54Z", "message": "Zero Downtime Deployment Design"}, "afterCommit": {"oid": "dbfa24f5f305d3fa7124b7ede971e5de8deba87b", "author": {"user": {"login": "timofei-durakov", "name": "Timofey Durakov"}}, "url": "https://github.com/telstra/open-kilda/commit/dbfa24f5f305d3fa7124b7ede971e5de8deba87b", "committedDate": "2020-12-29T08:51:44Z", "message": "Zero Downtime Deployment Design"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5NDM5Mzky", "url": "https://github.com/telstra/open-kilda/pull/3895#pullrequestreview-559439392", "createdAt": "2020-12-29T09:05:34Z", "commit": {"oid": "dbfa24f5f305d3fa7124b7ede971e5de8deba87b"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5NDM4Njcy", "url": "https://github.com/telstra/open-kilda/pull/3895#pullrequestreview-559438672", "createdAt": "2020-12-29T09:03:38Z", "commit": {"oid": "dbfa24f5f305d3fa7124b7ede971e5de8deba87b"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwOTowMzozOFrOIMKeWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwOTowNzoxOFrOIMKjSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTYyNTQzMg==", "bodyText": "each except northbound and GRPC. Maybe we should mention it", "url": "https://github.com/telstra/open-kilda/pull/3895#discussion_r549625432", "createdAt": "2020-12-29T09:03:38Z", "author": {"login": "niksv"}, "path": "docs/design/zero-downtime/README.md", "diffHunk": "@@ -0,0 +1,92 @@\n+# Zero Downtime Upgrades for Open Kilda\n+\n+## Rationale\n+Current deployment process requires time slot with no reactions from contol plane for any network events as long\n+as no flow/switch ops are available as well. To improve this new deployment procedure is proposed.\n+It's details are described in this document.\n+\n+## Solution\n+\n+Provide ability to deploy kilda in so called blue/green mode. Which will allow to quickly switch\n+system between different release versions. However, due to event-driven nature of kilda and a bunch of\n+limitations from Apache Storm there are changes made in original Blue/Green approach.\n+\n+### Shared Transport\n+\n+Kilda uses Kafka as a Message broker and messages in it to communicate between components of the system.\n+To provide backward compatibility between green and new versions of kilda within the same kafka topic\n+we need to split messages by some value between new and old ones. For this purpose will be used kafka message\n+header with run-id encoded in it. Run id should be unique within a single deployment.\n+Each component will emit messages with run-id header that is valid by the deployment. All receiver parts\n+should validate message header first and verify that run id matches with its configuration in that case component\n+will handle the message. Kafka provides API to write custom interceptors for both consumer and producer. This\n+interceptor will be responsible for verifying deployment id.\n+\n+### Zookeeper to store the state\n+\n+Since storm has limitations on lifecycle of its topologies, new mechanism is required to deal with topologies states. \n+Also it should be responsible to handle graceful shutdown procedure for topologies. For this role Apache Zookeeper\n+looks like a good fit.\n+\n+#### Node structure for Zookeeper\n+\n+`/kilda/{component_type}/{env_id}` - root for every component process, where:\n+`component_type` - topology or service name, e.g. `floodlight`, `network`, `nb_worker`\n+`env_id` - flag of blue or green env\n+\n+#### Signal, States and Build-Version\n+\n+Each component root node will have 3 children zNodes:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dbfa24f5f305d3fa7124b7ede971e5de8deba87b"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTYyNjY5Ng==", "bodyText": "why here? why not with northbound? We have stats requests from stats topology to GRPC service. at this point stats topology is blue and GRPC is green so they can't communicate", "url": "https://github.com/telstra/open-kilda/pull/3895#discussion_r549626696", "createdAt": "2020-12-29T09:07:18Z", "author": {"login": "niksv"}, "path": "docs/design/zero-downtime/README.md", "diffHunk": "@@ -0,0 +1,92 @@\n+# Zero Downtime Upgrades for Open Kilda\n+\n+## Rationale\n+Current deployment process requires time slot with no reactions from contol plane for any network events as long\n+as no flow/switch ops are available as well. To improve this new deployment procedure is proposed.\n+It's details are described in this document.\n+\n+## Solution\n+\n+Provide ability to deploy kilda in so called blue/green mode. Which will allow to quickly switch\n+system between different release versions. However, due to event-driven nature of kilda and a bunch of\n+limitations from Apache Storm there are changes made in original Blue/Green approach.\n+\n+### Shared Transport\n+\n+Kilda uses Kafka as a Message broker and messages in it to communicate between components of the system.\n+To provide backward compatibility between green and new versions of kilda within the same kafka topic\n+we need to split messages by some value between new and old ones. For this purpose will be used kafka message\n+header with run-id encoded in it. Run id should be unique within a single deployment.\n+Each component will emit messages with run-id header that is valid by the deployment. All receiver parts\n+should validate message header first and verify that run id matches with its configuration in that case component\n+will handle the message. Kafka provides API to write custom interceptors for both consumer and producer. This\n+interceptor will be responsible for verifying deployment id.\n+\n+### Zookeeper to store the state\n+\n+Since storm has limitations on lifecycle of its topologies, new mechanism is required to deal with topologies states. \n+Also it should be responsible to handle graceful shutdown procedure for topologies. For this role Apache Zookeeper\n+looks like a good fit.\n+\n+#### Node structure for Zookeeper\n+\n+`/kilda/{component_type}/{env_id}` - root for every component process, where:\n+`component_type` - topology or service name, e.g. `floodlight`, `network`, `nb_worker`\n+`env_id` - flag of blue or green env\n+\n+#### Signal, States and Build-Version\n+\n+Each component root node will have 3 children zNodes:\n+- signal - input field, can be `START` or `SHUTDOWN`, the way to make component start emittin processing new events\n+- state - int, number of active subcomponents of a component, when `SHUTDOWN` is emitted, should be `0`, positive otherwise.\n+- build-version - string field with run id, could be changed on fly, see #Shared transport for details\n+For the long running task such as hub in hub and spoke topologies, there should be a way to stop receiving new\n+requests, finishing up existing, and after that decrementing counter by 1 in a state field.\n+\n+#### Basic Deployment Scenario:\n+\n+*Note*\n+For this upgrade procedure each switch should be connected to 2 floodlights simultaneously.\n+ \n+Since floodlights could be distinguished by region let's assume, that all odd regions are green and even are blue\n+For the scenario, below suppose that `blue` is a current env and `green` is the one to be deployed.\n+Based on that the process should look like:\n+\n+- Ensure that `signal` for the `green` components is set to `SHUTDOWN` and the `build-version` is updated  \n+- Deploy green topologies  \n+- Set `SHUTDOWN` signal for `green` floodlights \n+- Redeploy floodlight containers of `green` color\n+- Redeploy grpc containers of `green` color", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dbfa24f5f305d3fa7124b7ede971e5de8deba87b"}, "originalPosition": 59}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8148d7411d7b600ea79b1fe192a059ea8dc22688", "author": {"user": {"login": "timofei-durakov", "name": "Timofey Durakov"}}, "url": "https://github.com/telstra/open-kilda/commit/8148d7411d7b600ea79b1fe192a059ea8dc22688", "committedDate": "2020-12-31T09:12:12Z", "message": "Zero Downtime Deployment Design"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "dbfa24f5f305d3fa7124b7ede971e5de8deba87b", "author": {"user": {"login": "timofei-durakov", "name": "Timofey Durakov"}}, "url": "https://github.com/telstra/open-kilda/commit/dbfa24f5f305d3fa7124b7ede971e5de8deba87b", "committedDate": "2020-12-29T08:51:44Z", "message": "Zero Downtime Deployment Design"}, "afterCommit": {"oid": "8148d7411d7b600ea79b1fe192a059ea8dc22688", "author": {"user": {"login": "timofei-durakov", "name": "Timofey Durakov"}}, "url": "https://github.com/telstra/open-kilda/commit/8148d7411d7b600ea79b1fe192a059ea8dc22688", "committedDate": "2020-12-31T09:12:12Z", "message": "Zero Downtime Deployment Design"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3784, "cost": 1, "resetAt": "2021-11-02T12:20:56Z"}}}