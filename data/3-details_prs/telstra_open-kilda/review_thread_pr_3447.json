{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE0MjEwNjQx", "number": 3447, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwOToyOTozM1rOD7kZMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwOToyOTozM1rOD7kZMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzNzg4ODUwOnYy", "diffSide": "RIGHT", "path": "src-java/testing/functional-tests/src/test/groovy/org/openkilda/functionaltests/spec/resilience/FloodlightKafkaConnectionSpec.groovy", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwOToyOTozM1rOGT-LMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwOToyOTozM1rOGT-LMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzU5NDgwMw==", "bodyText": "topology.islsForActiveSwitches.size() -> topology.islsForActiveSwitches.size() * 2", "url": "https://github.com/telstra/open-kilda/pull/3447#discussion_r423594803", "createdAt": "2020-05-12T09:29:33Z", "author": {"login": "andriidovhan"}, "path": "src-java/testing/functional-tests/src/test/groovy/org/openkilda/functionaltests/spec/resilience/FloodlightKafkaConnectionSpec.groovy", "diffHunk": "@@ -23,61 +24,87 @@ class FloodlightKafkaConnectionSpec extends HealthCheckSpecification {\n     @Value('${antiflap.cooldown}')\n     int antiflapCooldown\n \n-    @Ignore(\"Since now we have 2 regions, the ISL between regions will inevitably fail(L49). Need to refactor the test\")\n     def \"System survives temporary connection outage between Floodlight and Kafka\"() {\n-        when: \"Controller loses connection to Kafka\"\n-        sleep(3000) //Not respecting this 'sleep' may lead to subsequent tests instability\n-        def flOut = false\n-        regions.each { lockKeeper.knockoutFloodlight(it) }\n-        flOut = true\n+        setup: \"Pick a region to break, find which isls are between regions\"\n+        assumeTrue(\"This test requires at least 2 floodlight regions\", mgmtFlManager.regions.size() > 1)\n+        def regionToBreak = mgmtFlManager.regions.first()\n+        def islsBetweenRegions = topology.islsForActiveSwitches.findAll {\n+            [it.srcSwitch, it.dstSwitch].any { it.region == regionToBreak } && it.srcSwitch.region != it.dstSwitch.region\n+        }\n+\n+        when: \"Region 1 controller loses connection to Kafka\"\n+        lockKeeper.knockoutFloodlight(regionToBreak)\n+        def flOut = true\n+\n+        then: \"Non-rtl links between failed region and alive regions fail due to discovery timeout\"\n+        def nonRtlTransitIsls = islsBetweenRegions.findAll { isl ->\n+            [isl.srcSwitch, isl.dstSwitch].any { !it.features.contains(SwitchFeature.NOVIFLOW_COPY_FIELD) }\n+        }\n+        def asyncWait = task {\n+            wait(WAIT_OFFSET + discoveryTimeout) {\n+                nonRtlTransitIsls.each { assert northbound.getLink(it).state == IslChangeType.FAILED }\n+            }\n+        }\n \n-        then: \"Right before controller alive timeout switches are still active and links are discovered\"\n+        and: \"Right before controller alive timeout: switches are still active\"\n+        and: \"links inside regions are discovered\"\n+        and: \"rtl links between regions are discovered\"\n         double interval = floodlightAliveTimeout * 0.4\n-        Wrappers.timedLoop(floodlightAliveTimeout - interval) {\n+        def linksToRemainAlive = topology.islsForActiveSwitches.findAll { !nonRtlTransitIsls.contains(it) }\n+        timedLoop(floodlightAliveTimeout - interval) {\n             assert northbound.activeSwitches.size() == topology.activeSwitches.size()\n-            assert northbound.getActiveLinks().size() == topology.islsForActiveSwitches.size() * 2\n+            def isls = northbound.getAllLinks()\n+            linksToRemainAlive.each { assert islUtils.getIslInfo(isls, it).get().state == IslChangeType.DISCOVERED }\n             sleep(500)\n         }\n \n-        and: \"After controller alive timeout switches become inactive but links are still discovered\"\n-        Wrappers.wait(interval + WAIT_OFFSET) { assert northbound.activeSwitches.size() == 0 }\n-        northbound.getActiveLinks().size() == topology.islsForActiveSwitches.size() * 2\n+        and: \"After controller alive timeout switches in broken region become inactive but links are still discovered\"\n+        wait(interval + WAIT_OFFSET) {\n+            assert northbound.activeSwitches.size() == topology.activeSwitches.findAll { it.region != regionToBreak }.size()\n+        }\n+        linksToRemainAlive.each { assert northbound.getLink(it).state == IslChangeType.DISCOVERED }\n \n         when: \"System remains in this state for discovery timeout for ISLs\"\n         TimeUnit.SECONDS.sleep(discoveryTimeout + 1)\n+        asyncWait.get()\n \n-        then: \"All links are still discovered\"\n-        northbound.getActiveLinks().size() == topology.islsForActiveSwitches.size() * 2\n+        then: \"All links except for non-rtl transit ones are still discovered\"\n+        linksToRemainAlive.each { assert northbound.getLink(it).state == IslChangeType.DISCOVERED }\n \n         when: \"Controller restores connection to Kafka\"\n-        regions.each { lockKeeper.reviveFloodlight(it) }\n+        lockKeeper.reviveFloodlight(regionToBreak)\n         flOut = false\n \n         then: \"All links are discovered and switches become active\"\n-        northbound.getActiveLinks().size() == topology.islsForActiveSwitches.size() * 2\n-        Wrappers.wait(PERIODIC_SYNC_TIME) {\n+        wait(PERIODIC_SYNC_TIME) {\n+            assert northbound.getActiveLinks().size() == topology.islsForActiveSwitches.size() * 2\n             assert northbound.activeSwitches.size() == topology.activeSwitches.size()\n         }\n \n-        and: \"System is able to successfully create a valid flow\"\n-        def flow = flowHelper.randomFlow(topology.activeSwitches[0], topology.activeSwitches[1])\n-        northbound.addFlow(flow)\n-        Wrappers.wait(WAIT_OFFSET * 3) { //it takes longer than usual in these conditions. why?\n-            assert northbound.getFlowStatus(flow.id).status == FlowState.UP\n-            northbound.validateFlow(flow.id).each { assert it.asExpected }\n+        and: \"System is able to successfully create a valid flow between regions\"\n+        def swPair = topologyHelper.switchPairs.find { pair ->\n+            [pair.src, pair.dst].any { it.region == regionToBreak }  && pair.src.region != pair.dst.region\n         }\n-\n-        and: \"Cleanup: remove the flow\"\n-        flowHelperV2.deleteFlow(flow.id)\n+        def flow = flowHelperV2.randomFlow(swPair)\n+        flowHelperV2.addFlow(flow)\n+        northbound.validateFlow(flow.flowId).each { assert it.asExpected }\n \n         cleanup:\n-        flOut && regions.each { lockKeeper.reviveFloodlight(it) }\n+        asyncWait?.join()\n+        flow && flowHelperV2.deleteFlow(flow.flowId)\n+        if(flOut) {\n+            lockKeeper.reviveFloodlight(regionToBreak)\n+            wait(PERIODIC_SYNC_TIME + WAIT_OFFSET) {\n+                assert northbound.activeSwitches.size() == topology.activeSwitches.size()\n+                assert northbound.getAllLinks().size() == topology.islsForActiveSwitches.size()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4e951298acf472a359d2083484ad3e830658424"}, "originalPosition": 124}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1965, "cost": 1, "resetAt": "2021-11-13T14:23:39Z"}}}