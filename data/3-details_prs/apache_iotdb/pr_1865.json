{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA5OTY2Nzc1", "number": 1865, "title": "[IOTDB-915]Add raft log persist mechanism and use persist log to catch up", "bodyText": "", "createdAt": "2020-10-26T11:34:21Z", "url": "https://github.com/apache/iotdb/pull/1865", "merged": true, "mergeCommit": {"oid": "4715ad5d511df1e824afddc377ac3e2822507e45"}, "closed": true, "closedAt": "2020-11-03T11:30:42Z", "author": {"login": "neuyilan"}, "timelineItems": {"totalCount": 33, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdWoGGVAH2gAyNTA5OTY2Nzc1OjMzODJkNmI5NDNjNGM3N2RjNDliYWJiNjhkNzI0YjViMThmOWJiMzY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdYzH9OAH2gAyNTA5OTY2Nzc1OmM3NDUwOTk2YmI3OWQxYTViNThhYWQ1MGY0ZDQwNjUxMmE5OWMxOTA=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "3382d6b943c4c77dc49babb68d724b5b18f9bb36", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/3382d6b943c4c77dc49babb68d724b5b18f9bb36", "committedDate": "2020-10-27T12:26:26Z", "message": "add raft log mechanism and use persist log to catch up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b7cf81a002bafb87ca22471b8b04075ccff3b0cf", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/b7cf81a002bafb87ca22471b8b04075ccff3b0cf", "committedDate": "2020-10-27T12:26:26Z", "message": "add test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7caca25941610ebd3dd5b40ca4c5c5aff1e9ecbb", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/7caca25941610ebd3dd5b40ca4c5c5aff1e9ecbb", "committedDate": "2020-10-27T12:26:26Z", "message": "add tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f075e52212a34a604a448a930847a3dbe5678cfd", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/f075e52212a34a604a448a930847a3dbe5678cfd", "committedDate": "2020-10-27T12:26:36Z", "message": "Merge branch 'apache_cluster_new_1023_raft_log_catch_up' of github.com:neuyilan/incubator-iotdb into apache_cluster_new_1023_raft_log_catch_up"}, "afterCommit": {"oid": "7caca25941610ebd3dd5b40ca4c5c5aff1e9ecbb", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/7caca25941610ebd3dd5b40ca4c5c5aff1e9ecbb", "committedDate": "2020-10-27T12:26:26Z", "message": "add tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "851ef636efbdece6da8e610f718f95bb3f18f577", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/851ef636efbdece6da8e610f718f95bb3f18f577", "committedDate": "2020-10-27T12:58:52Z", "message": "add use persist log to snapshot"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "40b08e18fe45d78eaf750e119464698bfaa8b4bb", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/40b08e18fe45d78eaf750e119464698bfaa8b4bb", "committedDate": "2020-10-28T01:41:49Z", "message": "remove useless lines"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7949b1d5c7e8d2e14d64935858b8d90f78b2c847", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/7949b1d5c7e8d2e14d64935858b8d90f78b2c847", "committedDate": "2020-10-28T03:50:44Z", "message": "fix fristLogIndex bug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b83b80af4b0621269e5f4ec0fbd48806836b57b", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/6b83b80af4b0621269e5f4ec0fbd48806836b57b", "committedDate": "2020-10-28T04:01:39Z", "message": "add switch of whether use persist logs on disk to catchup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "106afb2d03a113ff516811a802aa6a4395b69ee2", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/106afb2d03a113ff516811a802aa6a4395b69ee2", "committedDate": "2020-10-28T05:18:26Z", "message": "fix log manager print messags"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b66f25dc47edcde947157629408c845ae7678151", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/b66f25dc47edcde947157629408c845ae7678151", "committedDate": "2020-10-28T11:21:28Z", "message": "add getLogs UT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8779750209db47695e176bae9f2e7bae210c4759", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/8779750209db47695e176bae9f2e7bae210c4759", "committedDate": "2020-10-28T11:55:24Z", "message": "change forceRaftLogPeriodInMS to 1000"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "482e36bab7b37fadb0350433262ae186c7578daa", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/482e36bab7b37fadb0350433262ae186c7578daa", "committedDate": "2020-10-28T13:01:15Z", "message": "use persist log when getTerm not found in memory"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "29de7ef7ce1b2e924fa278274083c8aeeef3d804", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/29de7ef7ce1b2e924fa278274083c8aeeef3d804", "committedDate": "2020-10-29T05:36:06Z", "message": "add recovery log data file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb5ba169a5262fecff40cd79cd26d1bb5b2599ab", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/bb5ba169a5262fecff40cd79cd26d1bb5b2599ab", "committedDate": "2020-10-29T08:24:47Z", "message": "revert the test log level to warn"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2463c422b62d9e6f728cbaef67214845bd792c95", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/2463c422b62d9e6f728cbaef67214845bd792c95", "committedDate": "2020-10-29T12:20:08Z", "message": "fix get log term"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/30542c1d5231ec31a73b0a7728cd0b43a7eb9015", "committedDate": "2020-10-30T06:27:00Z", "message": "fix ut on windows"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9eca1bc083d49ada8ed0cbaf877ece50ad10ff90", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/9eca1bc083d49ada8ed0cbaf877ece50ad10ff90", "committedDate": "2020-10-30T09:03:13Z", "message": "fix ut"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIwMzM4NjY0", "url": "https://github.com/apache/iotdb/pull/1865#pullrequestreview-520338664", "createdAt": "2020-10-30T02:30:57Z", "commit": {"oid": "2463c422b62d9e6f728cbaef67214845bd792c95"}, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQwMjozMDo1N1rOHq4SaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQwOTo1Mjo1MlrOHrH_2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDcyNDQ1Nw==", "bodyText": "It's better to add a default comment.such as 'default: 1G'", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514724457", "createdAt": "2020-10-30T02:30:57Z", "author": {"login": "LebronAl"}, "path": "cluster/src/assembly/resources/conf/iotdb-cluster.properties", "diffHunk": "@@ -129,7 +129,26 @@ flush_raft_log_threshold=10000\n # The cycle when raft log is periodically forced to be written to disk(in milliseconds)\n # If force_raft_log_period_in_ms = 0 it means force insert raft log to be written to disk after\n # each refreshment. Set this parameter to 0 may slow down the ingestion on slow disk.\n-force_raft_log_period_in_ms=10\n+force_raft_log_period_in_ms=1000\n \n # Size of log buffer in each RaftMember's LogManager(in byte).\n-raft_log_buffer_size=16777216\n\\ No newline at end of file\n+raft_log_buffer_size=16777216\n+\n+# The maximum value of the raft log index stored in the memory per raft group,\n+# These indexes are used to index the location of the log on the disk\n+max_raft_log_index_size_in_memory=10000\n+\n+# The maximum value of the raft log persisted on disk per file(in byte) per raft group", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2463c422b62d9e6f728cbaef67214845bd792c95"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDcyNDkyOA==", "bodyText": "Please fix typo", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514724928", "createdAt": "2020-10-30T02:31:28Z", "author": {"login": "LebronAl"}, "path": "cluster/src/assembly/resources/conf/iotdb-cluster.properties", "diffHunk": "@@ -129,7 +129,26 @@ flush_raft_log_threshold=10000\n # The cycle when raft log is periodically forced to be written to disk(in milliseconds)\n # If force_raft_log_period_in_ms = 0 it means force insert raft log to be written to disk after\n # each refreshment. Set this parameter to 0 may slow down the ingestion on slow disk.\n-force_raft_log_period_in_ms=10\n+force_raft_log_period_in_ms=1000\n \n # Size of log buffer in each RaftMember's LogManager(in byte).\n-raft_log_buffer_size=16777216\n\\ No newline at end of file\n+raft_log_buffer_size=16777216\n+\n+# The maximum value of the raft log index stored in the memory per raft group,\n+# These indexes are used to index the location of the log on the disk\n+max_raft_log_index_size_in_memory=10000\n+\n+# The maximum value of the raft log persisted on disk per file(in byte) per raft group\n+max_raft_log_persist_data_size_per_file=1073741824\n+\n+# The maximum number of persistent raft log files on disk per raft group, So each raft group's", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2463c422b62d9e6f728cbaef67214845bd792c95"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDcyOTU2NA==", "bodyText": "same as above", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514729564", "createdAt": "2020-10-30T02:36:26Z", "author": {"login": "LebronAl"}, "path": "cluster/src/main/java/org/apache/iotdb/cluster/config/ClusterConfig.java", "diffHunk": "@@ -127,6 +127,32 @@\n \n   private int pullSnapshotRetryIntervalMs = 5 * 1000;\n \n+  /**\n+   * The maximum value of the raft log index stored in the memory per raft group, These indexes are\n+   * used to index the location of the log on the disk\n+   */\n+  private int maxRaftLogIndexSizeInMemory = 10000;\n+\n+  /**\n+   * The maximum value of the raft log persisted on disk per file(in byte) per raft group", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2463c422b62d9e6f728cbaef67214845bd792c95"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDc3Mzc4Mg==", "bodyText": "I thought these code should be in the function maybeTerm in committedEntryManager, which should manager all persisted entries, it will throw a EntryCompactedException or get log from disk if isEnableRaftLogPersistence  is enabled.\nBTW, I'm a little confused about whether we should get log from disk in function getTerm,If so, then maybe we should change isEnableUsePersistLogOnDiskToCatchUp's name", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514773782", "createdAt": "2020-10-30T03:24:11Z", "author": {"login": "LebronAl"}, "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/RaftLogManager.java", "diffHunk": "@@ -292,15 +292,29 @@ public long getLastLogIndex() {\n   public long getTerm(long index) throws EntryCompactedException {\n     long dummyIndex = getFirstIndex() - 1;\n     if (index < dummyIndex) {\n+      // search in disk", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2463c422b62d9e6f728cbaef67214845bd792c95"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkyOTgxNA==", "bodyText": "change the logSizeDeque", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514929814", "createdAt": "2020-10-30T08:11:14Z", "author": {"login": "LebronAl"}, "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -50,82 +53,140 @@\n import org.apache.iotdb.db.engine.version.SimpleFileVersionController;\n import org.apache.iotdb.db.engine.version.VersionController;\n import org.apache.iotdb.db.utils.TestOnly;\n+import org.apache.iotdb.tsfile.utils.BytesUtils;\n+import org.apache.iotdb.tsfile.utils.Pair;\n import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n public class SyncLogDequeSerializer implements StableEntryManager {\n \n   private static final Logger logger = LoggerFactory.getLogger(SyncLogDequeSerializer.class);\n-  private static final String LOG_FILE_PREFIX = \".data\";\n+  private static final String LOG_DATA_FILE_SUFFIX = \"data\";\n+  private static final String LOG_INDEX_FILE_SUFFIX = \"idx\";\n+\n+  /**\n+   * the log data files\n+   */\n+  private List<File> logDataFileList;\n+\n+  /**\n+   * the log index files\n+   */\n+  private List<File> logIndexFileList;\n \n-  List<File> logFileList;\n   private LogParser parser = LogParser.getINSTANCE();\n   private File metaFile;\n-  private FileOutputStream currentLogOutputStream;\n-  private Deque<Integer> logSizeDeque = new ArrayDeque<>();\n+  private FileOutputStream currentLogDataOutputStream;\n+  private FileOutputStream currentLogIndexOutputStream;\n   private LogManagerMeta meta;\n   private HardState state;\n-  // mark first log position\n-  private long firstLogPosition = 0;\n-  // removed log size\n-  private long removedLogSize = 0;\n-  // when the removedLogSize larger than this, we actually delete logs\n-  private long maxRemovedLogSize = ClusterDescriptor.getInstance().getConfig()\n-      .getMaxUnsnapshotLogSize();\n-  // min version of available log\n+\n+  /**\n+   * min version of available log\n+   */\n   private long minAvailableVersion = 0;\n-  // max version of available log\n+\n+  /**\n+   * max version of available log\n+   */\n   private long maxAvailableVersion = Long.MAX_VALUE;\n-  // log dir\n+\n   private String logDir;\n-  // version controller\n+\n   private VersionController versionController;\n \n-  private ByteBuffer logBuffer = ByteBuffer\n+  private ByteBuffer logDataBuffer = ByteBuffer\n       .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+  private ByteBuffer logIndexBuffer = ByteBuffer\n+      .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+\n+  private long offsetOfTheCurrentLogDataOutputStream = 0;\n+\n+  /**\n+   * file name pattern:\n+   * <p>\n+   * for log data file: ${startTime}-${Long.MAX_VALUE}-{version}-data\n+   * <p>\n+   * for log index file: ${startTime}-${Long.MAX_VALUE}-{version}-idx\n+   */\n+  private static final int FILE_NAME_PART_LENGTH = 4;\n \n-  private final int flushRaftLogThreshold = ClusterDescriptor.getInstance().getConfig()\n-      .getFlushRaftLogThreshold();\n+  private int maxRaftLogIndexSizeInMemory = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxRaftLogIndexSizeInMemory();\n \n-  private int bufferedLogNum = 0;\n+  private int maxRaftLogPersistDataSizePerFile = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxRaftLogPersistDataSizePerFile();\n \n+  private int maxNumberOfPersistRaftLogFiles = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxNumberOfPersistRaftLogFiles();\n+\n+  private int maxPersistRaftLogNumberOnDisk = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxPersistRaftLogNumberOnDisk();\n+\n+  private ScheduledExecutorService persistLogDeleteExecutorService;\n+  private ScheduledFuture<?> persistLogDeleteLogFuture;\n+\n+  /**\n+   * indicate the first raft log's index of {@link SyncLogDequeSerializer#logIndexOffsetList}, for\n+   * example, if firstLogIndex=1000, then the offset of the log index 1000 equals\n+   * logIndexOffsetList[0], the offset of the log index 1001 equals logIndexOffsetList[1], and so\n+   * on.\n+   */\n+  private long firstLogIndex = 0;\n+\n+  /**\n+   * the offset of the log's index, for example, the first value is the offset of index\n+   * ${firstLogIndex}, the second value is the offset of index ${firstLogIndex+1}\n+   */\n+  private List<Long> logIndexOffsetList;\n+\n+  private static final int logDeleteCheckIntervalSecond = 1;\n \n   /**\n    * the lock uses when change the logSizeDeque", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkzNzIyNg==", "bodyText": "It seems that this function will return empty list when maxHaveAppliedCommitIndex == commitLogIndex ,So how can we handle redo log when restart as we have not merged wal and raft logs?", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514937226", "createdAt": "2020-10-30T08:27:21Z", "author": {"login": "LebronAl"}, "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -160,31 +216,28 @@ public LogManagerMeta getMeta() {\n    * Recover all the logs in disk. This function will be called once this instance is created.\n    */\n   @Override\n-  public List<Log> getAllEntries() {\n-    List<Log> logs = recoverLog();\n-    int size = logs.size();\n-    if (size != 0 && meta.getLastLogIndex() <= logs.get(size - 1).getCurrLogIndex()) {\n-      meta.setLastLogTerm(logs.get(size - 1).getCurrLogTerm());\n-      meta.setLastLogIndex(logs.get(size - 1).getCurrLogIndex());\n-      meta.setCommitLogTerm(logs.get(size - 1).getCurrLogTerm());\n-      meta.setCommitLogIndex(logs.get(size - 1).getCurrLogIndex());\n+  public List<Log> getAllEntriesBeforeAppliedIndex() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015"}, "originalPosition": 235}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk1MzM0Nw==", "bodyText": "It seems all the callers(close (),checkCloseCurrentFile())  to this function's has got the writelock?", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514953347", "createdAt": "2020-10-30T08:59:28Z", "author": {"login": "LebronAl"}, "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -203,47 +256,113 @@ public void append(List<Log> entries) throws IOException {\n    */\n   private void putLogs(List<Log> entries) {\n     for (Log log : entries) {\n-      logBuffer.mark();\n+      logDataBuffer.mark();\n+      logIndexBuffer.mark();\n       ByteBuffer logData = log.serialize();\n       int size = logData.capacity() + Integer.BYTES;\n       try {\n-        logBuffer.putInt(logData.capacity());\n-        logBuffer.put(logData);\n-        logSizeDeque.addLast(size);\n-        bufferedLogNum++;\n+        logDataBuffer.putInt(logData.capacity());\n+        logDataBuffer.put(logData);\n+        logIndexBuffer.putLong(offsetOfTheCurrentLogDataOutputStream);\n+        logIndexOffsetList.add(offsetOfTheCurrentLogDataOutputStream);\n+        offsetOfTheCurrentLogDataOutputStream += size;\n       } catch (BufferOverflowException e) {\n         logger.info(\"Raft log buffer overflow!\");\n-        logBuffer.reset();\n+        logDataBuffer.reset();\n+        logIndexBuffer.reset();\n         flushLogBuffer();\n-        logBuffer.putInt(logData.capacity());\n-        logBuffer.put(logData);\n-        logSizeDeque.addLast(size);\n-        bufferedLogNum++;\n+        checkCloseCurrentFile(log.getCurrLogIndex() - 1);\n+        logDataBuffer.putInt(logData.capacity());\n+        logDataBuffer.put(logData);\n+        logIndexBuffer.putLong(offsetOfTheCurrentLogDataOutputStream);\n+        logIndexOffsetList.add(offsetOfTheCurrentLogDataOutputStream);\n+        offsetOfTheCurrentLogDataOutputStream += size;\n+      }\n+    }\n+  }\n+\n+  private void checkCloseCurrentFile(long commitIndex) {\n+    if (offsetOfTheCurrentLogDataOutputStream > maxRaftLogPersistDataSizePerFile) {\n+      try {\n+        closeCurrentFile(commitIndex);\n+        serializeMeta(meta);\n+        createNewLogFile(logDir, commitIndex + 1);\n+      } catch (IOException e) {\n+        logger.error(\"check close current file failed\", e);\n       }\n     }\n   }\n \n+  private void closeCurrentFile(long commitIndex) throws IOException {\n+    lock.writeLock().lock();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015"}, "originalPosition": 317}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk2MDkyNw==", "bodyText": "I doubt if this parameter is necessary, given that you have already prevented storage abuse by taking two parameters: max_number_of_persist_raft_log_files and max_raft_log_persist_data_size_per_file", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514960927", "createdAt": "2020-10-30T09:14:43Z", "author": {"login": "LebronAl"}, "path": "cluster/src/assembly/resources/conf/iotdb-cluster.properties", "diffHunk": "@@ -129,7 +129,26 @@ flush_raft_log_threshold=10000\n # The cycle when raft log is periodically forced to be written to disk(in milliseconds)\n # If force_raft_log_period_in_ms = 0 it means force insert raft log to be written to disk after\n # each refreshment. Set this parameter to 0 may slow down the ingestion on slow disk.\n-force_raft_log_period_in_ms=10\n+force_raft_log_period_in_ms=1000\n \n # Size of log buffer in each RaftMember's LogManager(in byte).\n-raft_log_buffer_size=16777216\n\\ No newline at end of file\n+raft_log_buffer_size=16777216\n+\n+# The maximum value of the raft log index stored in the memory per raft group,\n+# These indexes are used to index the location of the log on the disk\n+max_raft_log_index_size_in_memory=10000\n+\n+# The maximum value of the raft log persisted on disk per file(in byte) per raft group\n+max_raft_log_persist_data_size_per_file=1073741824\n+\n+# The maximum number of persistent raft log files on disk per raft group, So each raft group's\n+# So each raft group's log takes up disk space approximately equals\n+# max_raft_log_persist_data_size_per_file*max_number_of_persist_raft_log_files\n+max_number_of_persist_raft_log_files=5\n+\n+# The maximum number of logs saved on the disk\n+max_persist_raft_log_number_on_disk=1000000", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9eca1bc083d49ada8ed0cbaf877ece50ad10ff90"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk2NjY2OA==", "bodyText": "It seems that the max_unsnapshoted_log_size can be deleted now", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514966668", "createdAt": "2020-10-30T09:25:52Z", "author": {"login": "LebronAl"}, "path": "cluster/src/assembly/resources/conf/iotdb-cluster.properties", "diffHunk": "@@ -129,7 +129,26 @@ flush_raft_log_threshold=10000\n # The cycle when raft log is periodically forced to be written to disk(in milliseconds)\n # If force_raft_log_period_in_ms = 0 it means force insert raft log to be written to disk after\n # each refreshment. Set this parameter to 0 may slow down the ingestion on slow disk.\n-force_raft_log_period_in_ms=10\n+force_raft_log_period_in_ms=1000\n ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9eca1bc083d49ada8ed0cbaf877ece50ad10ff90"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3MDUzMQ==", "bodyText": "As the function checkDeletePersistRaftLog  will get the write lock, I doubt whether 1 is too small?", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514970531", "createdAt": "2020-10-30T09:32:41Z", "author": {"login": "LebronAl"}, "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -50,82 +53,140 @@\n import org.apache.iotdb.db.engine.version.SimpleFileVersionController;\n import org.apache.iotdb.db.engine.version.VersionController;\n import org.apache.iotdb.db.utils.TestOnly;\n+import org.apache.iotdb.tsfile.utils.BytesUtils;\n+import org.apache.iotdb.tsfile.utils.Pair;\n import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n public class SyncLogDequeSerializer implements StableEntryManager {\n \n   private static final Logger logger = LoggerFactory.getLogger(SyncLogDequeSerializer.class);\n-  private static final String LOG_FILE_PREFIX = \".data\";\n+  private static final String LOG_DATA_FILE_SUFFIX = \"data\";\n+  private static final String LOG_INDEX_FILE_SUFFIX = \"idx\";\n+\n+  /**\n+   * the log data files\n+   */\n+  private List<File> logDataFileList;\n+\n+  /**\n+   * the log index files\n+   */\n+  private List<File> logIndexFileList;\n \n-  List<File> logFileList;\n   private LogParser parser = LogParser.getINSTANCE();\n   private File metaFile;\n-  private FileOutputStream currentLogOutputStream;\n-  private Deque<Integer> logSizeDeque = new ArrayDeque<>();\n+  private FileOutputStream currentLogDataOutputStream;\n+  private FileOutputStream currentLogIndexOutputStream;\n   private LogManagerMeta meta;\n   private HardState state;\n-  // mark first log position\n-  private long firstLogPosition = 0;\n-  // removed log size\n-  private long removedLogSize = 0;\n-  // when the removedLogSize larger than this, we actually delete logs\n-  private long maxRemovedLogSize = ClusterDescriptor.getInstance().getConfig()\n-      .getMaxUnsnapshotLogSize();\n-  // min version of available log\n+\n+  /**\n+   * min version of available log\n+   */\n   private long minAvailableVersion = 0;\n-  // max version of available log\n+\n+  /**\n+   * max version of available log\n+   */\n   private long maxAvailableVersion = Long.MAX_VALUE;\n-  // log dir\n+\n   private String logDir;\n-  // version controller\n+\n   private VersionController versionController;\n \n-  private ByteBuffer logBuffer = ByteBuffer\n+  private ByteBuffer logDataBuffer = ByteBuffer\n       .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+  private ByteBuffer logIndexBuffer = ByteBuffer\n+      .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+\n+  private long offsetOfTheCurrentLogDataOutputStream = 0;\n+\n+  /**\n+   * file name pattern:\n+   * <p>\n+   * for log data file: ${startTime}-${Long.MAX_VALUE}-{version}-data\n+   * <p>\n+   * for log index file: ${startTime}-${Long.MAX_VALUE}-{version}-idx\n+   */\n+  private static final int FILE_NAME_PART_LENGTH = 4;\n \n-  private final int flushRaftLogThreshold = ClusterDescriptor.getInstance().getConfig()\n-      .getFlushRaftLogThreshold();\n+  private int maxRaftLogIndexSizeInMemory = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxRaftLogIndexSizeInMemory();\n \n-  private int bufferedLogNum = 0;\n+  private int maxRaftLogPersistDataSizePerFile = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxRaftLogPersistDataSizePerFile();\n \n+  private int maxNumberOfPersistRaftLogFiles = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxNumberOfPersistRaftLogFiles();\n+\n+  private int maxPersistRaftLogNumberOnDisk = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxPersistRaftLogNumberOnDisk();\n+\n+  private ScheduledExecutorService persistLogDeleteExecutorService;\n+  private ScheduledFuture<?> persistLogDeleteLogFuture;\n+\n+  /**\n+   * indicate the first raft log's index of {@link SyncLogDequeSerializer#logIndexOffsetList}, for\n+   * example, if firstLogIndex=1000, then the offset of the log index 1000 equals\n+   * logIndexOffsetList[0], the offset of the log index 1001 equals logIndexOffsetList[1], and so\n+   * on.\n+   */\n+  private long firstLogIndex = 0;\n+\n+  /**\n+   * the offset of the log's index, for example, the first value is the offset of index\n+   * ${firstLogIndex}, the second value is the offset of index ${firstLogIndex+1}\n+   */\n+  private List<Long> logIndexOffsetList;\n+\n+  private static final int logDeleteCheckIntervalSecond = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9eca1bc083d49ada8ed0cbaf877ece50ad10ff90"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3NDg2Mg==", "bodyText": "As you have used logIndexOffsetList to record offset in putLog, I doubt whether it's necessary to maintain this buffer in putLog.Maybe You can generate index buffer in flushLogBuffer .", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514974862", "createdAt": "2020-10-30T09:40:25Z", "author": {"login": "LebronAl"}, "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -50,82 +53,140 @@\n import org.apache.iotdb.db.engine.version.SimpleFileVersionController;\n import org.apache.iotdb.db.engine.version.VersionController;\n import org.apache.iotdb.db.utils.TestOnly;\n+import org.apache.iotdb.tsfile.utils.BytesUtils;\n+import org.apache.iotdb.tsfile.utils.Pair;\n import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n public class SyncLogDequeSerializer implements StableEntryManager {\n \n   private static final Logger logger = LoggerFactory.getLogger(SyncLogDequeSerializer.class);\n-  private static final String LOG_FILE_PREFIX = \".data\";\n+  private static final String LOG_DATA_FILE_SUFFIX = \"data\";\n+  private static final String LOG_INDEX_FILE_SUFFIX = \"idx\";\n+\n+  /**\n+   * the log data files\n+   */\n+  private List<File> logDataFileList;\n+\n+  /**\n+   * the log index files\n+   */\n+  private List<File> logIndexFileList;\n \n-  List<File> logFileList;\n   private LogParser parser = LogParser.getINSTANCE();\n   private File metaFile;\n-  private FileOutputStream currentLogOutputStream;\n-  private Deque<Integer> logSizeDeque = new ArrayDeque<>();\n+  private FileOutputStream currentLogDataOutputStream;\n+  private FileOutputStream currentLogIndexOutputStream;\n   private LogManagerMeta meta;\n   private HardState state;\n-  // mark first log position\n-  private long firstLogPosition = 0;\n-  // removed log size\n-  private long removedLogSize = 0;\n-  // when the removedLogSize larger than this, we actually delete logs\n-  private long maxRemovedLogSize = ClusterDescriptor.getInstance().getConfig()\n-      .getMaxUnsnapshotLogSize();\n-  // min version of available log\n+\n+  /**\n+   * min version of available log\n+   */\n   private long minAvailableVersion = 0;\n-  // max version of available log\n+\n+  /**\n+   * max version of available log\n+   */\n   private long maxAvailableVersion = Long.MAX_VALUE;\n-  // log dir\n+\n   private String logDir;\n-  // version controller\n+\n   private VersionController versionController;\n \n-  private ByteBuffer logBuffer = ByteBuffer\n+  private ByteBuffer logDataBuffer = ByteBuffer\n       .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+  private ByteBuffer logIndexBuffer = ByteBuffer", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9eca1bc083d49ada8ed0cbaf877ece50ad10ff90"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk4MTg1MA==", "bodyText": "As currently the  default maxSize of each data file is 1G,it's seems a Integer is able to record the offset.", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514981850", "createdAt": "2020-10-30T09:52:52Z", "author": {"login": "LebronAl"}, "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -50,82 +53,140 @@\n import org.apache.iotdb.db.engine.version.SimpleFileVersionController;\n import org.apache.iotdb.db.engine.version.VersionController;\n import org.apache.iotdb.db.utils.TestOnly;\n+import org.apache.iotdb.tsfile.utils.BytesUtils;\n+import org.apache.iotdb.tsfile.utils.Pair;\n import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n public class SyncLogDequeSerializer implements StableEntryManager {\n \n   private static final Logger logger = LoggerFactory.getLogger(SyncLogDequeSerializer.class);\n-  private static final String LOG_FILE_PREFIX = \".data\";\n+  private static final String LOG_DATA_FILE_SUFFIX = \"data\";\n+  private static final String LOG_INDEX_FILE_SUFFIX = \"idx\";\n+\n+  /**\n+   * the log data files\n+   */\n+  private List<File> logDataFileList;\n+\n+  /**\n+   * the log index files\n+   */\n+  private List<File> logIndexFileList;\n \n-  List<File> logFileList;\n   private LogParser parser = LogParser.getINSTANCE();\n   private File metaFile;\n-  private FileOutputStream currentLogOutputStream;\n-  private Deque<Integer> logSizeDeque = new ArrayDeque<>();\n+  private FileOutputStream currentLogDataOutputStream;\n+  private FileOutputStream currentLogIndexOutputStream;\n   private LogManagerMeta meta;\n   private HardState state;\n-  // mark first log position\n-  private long firstLogPosition = 0;\n-  // removed log size\n-  private long removedLogSize = 0;\n-  // when the removedLogSize larger than this, we actually delete logs\n-  private long maxRemovedLogSize = ClusterDescriptor.getInstance().getConfig()\n-      .getMaxUnsnapshotLogSize();\n-  // min version of available log\n+\n+  /**\n+   * min version of available log\n+   */\n   private long minAvailableVersion = 0;\n-  // max version of available log\n+\n+  /**\n+   * max version of available log\n+   */\n   private long maxAvailableVersion = Long.MAX_VALUE;\n-  // log dir\n+\n   private String logDir;\n-  // version controller\n+\n   private VersionController versionController;\n \n-  private ByteBuffer logBuffer = ByteBuffer\n+  private ByteBuffer logDataBuffer = ByteBuffer\n       .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+  private ByteBuffer logIndexBuffer = ByteBuffer\n+      .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+\n+  private long offsetOfTheCurrentLogDataOutputStream = 0;\n+\n+  /**\n+   * file name pattern:\n+   * <p>\n+   * for log data file: ${startTime}-${Long.MAX_VALUE}-{version}-data\n+   * <p>\n+   * for log index file: ${startTime}-${Long.MAX_VALUE}-{version}-idx\n+   */\n+  private static final int FILE_NAME_PART_LENGTH = 4;\n \n-  private final int flushRaftLogThreshold = ClusterDescriptor.getInstance().getConfig()\n-      .getFlushRaftLogThreshold();\n+  private int maxRaftLogIndexSizeInMemory = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxRaftLogIndexSizeInMemory();\n \n-  private int bufferedLogNum = 0;\n+  private int maxRaftLogPersistDataSizePerFile = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxRaftLogPersistDataSizePerFile();\n \n+  private int maxNumberOfPersistRaftLogFiles = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxNumberOfPersistRaftLogFiles();\n+\n+  private int maxPersistRaftLogNumberOnDisk = ClusterDescriptor.getInstance().getConfig()\n+      .getMaxPersistRaftLogNumberOnDisk();\n+\n+  private ScheduledExecutorService persistLogDeleteExecutorService;\n+  private ScheduledFuture<?> persistLogDeleteLogFuture;\n+\n+  /**\n+   * indicate the first raft log's index of {@link SyncLogDequeSerializer#logIndexOffsetList}, for\n+   * example, if firstLogIndex=1000, then the offset of the log index 1000 equals\n+   * logIndexOffsetList[0], the offset of the log index 1001 equals logIndexOffsetList[1], and so\n+   * on.\n+   */\n+  private long firstLogIndex = 0;\n+\n+  /**\n+   * the offset of the log's index, for example, the first value is the offset of index\n+   * ${firstLogIndex}, the second value is the offset of index ${firstLogIndex+1}\n+   */\n+  private List<Long> logIndexOffsetList;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9eca1bc083d49ada8ed0cbaf877ece50ad10ff90"}, "originalPosition": 145}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIwNDcyOTEy", "url": "https://github.com/apache/iotdb/pull/1865#pullrequestreview-520472912", "createdAt": "2020-10-30T08:52:43Z", "commit": {"oid": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQwODo1Mjo0M1rOHrGCWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQwOTo0Mjo0NlrOHrHpqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk0OTcyMQ==", "bodyText": "Replace request.toString() with simply request.", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514949721", "createdAt": "2020-10-30T08:52:43Z", "author": {"login": "jt2594838"}, "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/catchup/LogCatchUpTask.java", "diffHunk": "@@ -191,12 +191,11 @@ private AppendEntriesRequest prepareRequest(List<ByteBuffer> logList, int startP\n         logger.error(\"getTerm failed for newly append entries\", e);\n       }\n     }\n+    logger.debug(\"{}, node={} catchup request={}\", raftMember.getName(), node, request.toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk1Mjk4OA==", "bodyText": "It seems more like getAllEntriesAfterAppliedIndex instead of  getAllEntriesBeforeAppliedIndex.", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514952988", "createdAt": "2020-10-30T08:58:49Z", "author": {"login": "jt2594838"}, "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -160,31 +216,28 @@ public LogManagerMeta getMeta() {\n    * Recover all the logs in disk. This function will be called once this instance is created.\n    */\n   @Override\n-  public List<Log> getAllEntries() {\n-    List<Log> logs = recoverLog();\n-    int size = logs.size();\n-    if (size != 0 && meta.getLastLogIndex() <= logs.get(size - 1).getCurrLogIndex()) {\n-      meta.setLastLogTerm(logs.get(size - 1).getCurrLogTerm());\n-      meta.setLastLogIndex(logs.get(size - 1).getCurrLogIndex());\n-      meta.setCommitLogTerm(logs.get(size - 1).getCurrLogTerm());\n-      meta.setCommitLogIndex(logs.get(size - 1).getCurrLogIndex());\n+  public List<Log> getAllEntriesBeforeAppliedIndex() {\n+    logger.debug(\"getAllEntriesBeforeAppliedIndex, maxHaveAppliedCommitIndex={}, commitLogIndex={}\",\n+        meta.getMaxHaveAppliedCommitIndex(), meta.getCommitLogIndex());\n+    if (meta.getMaxHaveAppliedCommitIndex() >= meta.getCommitLogIndex()) {\n+      return Collections.emptyList();\n     }\n-    return logs;\n+    return getLogs(meta.getMaxHaveAppliedCommitIndex(), meta.getCommitLogIndex());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015"}, "originalPosition": 242}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk2MDU3OQ==", "bodyText": "Maybe list.set() is enough for this.", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514960579", "createdAt": "2020-10-30T09:14:10Z", "author": {"login": "jt2594838"}, "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -203,47 +256,113 @@ public void append(List<Log> entries) throws IOException {\n    */\n   private void putLogs(List<Log> entries) {\n     for (Log log : entries) {\n-      logBuffer.mark();\n+      logDataBuffer.mark();\n+      logIndexBuffer.mark();\n       ByteBuffer logData = log.serialize();\n       int size = logData.capacity() + Integer.BYTES;\n       try {\n-        logBuffer.putInt(logData.capacity());\n-        logBuffer.put(logData);\n-        logSizeDeque.addLast(size);\n-        bufferedLogNum++;\n+        logDataBuffer.putInt(logData.capacity());\n+        logDataBuffer.put(logData);\n+        logIndexBuffer.putLong(offsetOfTheCurrentLogDataOutputStream);\n+        logIndexOffsetList.add(offsetOfTheCurrentLogDataOutputStream);\n+        offsetOfTheCurrentLogDataOutputStream += size;\n       } catch (BufferOverflowException e) {\n         logger.info(\"Raft log buffer overflow!\");\n-        logBuffer.reset();\n+        logDataBuffer.reset();\n+        logIndexBuffer.reset();\n         flushLogBuffer();\n-        logBuffer.putInt(logData.capacity());\n-        logBuffer.put(logData);\n-        logSizeDeque.addLast(size);\n-        bufferedLogNum++;\n+        checkCloseCurrentFile(log.getCurrLogIndex() - 1);\n+        logDataBuffer.putInt(logData.capacity());\n+        logDataBuffer.put(logData);\n+        logIndexBuffer.putLong(offsetOfTheCurrentLogDataOutputStream);\n+        logIndexOffsetList.add(offsetOfTheCurrentLogDataOutputStream);\n+        offsetOfTheCurrentLogDataOutputStream += size;\n+      }\n+    }\n+  }\n+\n+  private void checkCloseCurrentFile(long commitIndex) {\n+    if (offsetOfTheCurrentLogDataOutputStream > maxRaftLogPersistDataSizePerFile) {\n+      try {\n+        closeCurrentFile(commitIndex);\n+        serializeMeta(meta);\n+        createNewLogFile(logDir, commitIndex + 1);\n+      } catch (IOException e) {\n+        logger.error(\"check close current file failed\", e);\n       }\n     }\n   }\n \n+  private void closeCurrentFile(long commitIndex) throws IOException {\n+    lock.writeLock().lock();\n+    try {\n+      if (currentLogDataOutputStream != null) {\n+        currentLogDataOutputStream.close();\n+        currentLogDataOutputStream = null;\n+      }\n+\n+      if (currentLogIndexOutputStream != null) {\n+        currentLogIndexOutputStream.close();\n+        currentLogIndexOutputStream = null;\n+      }\n+      File currentLogDataFile = getCurrentLogDataFile();\n+      String newDataFileName = currentLogDataFile.getName()\n+          .replaceAll(String.valueOf(Long.MAX_VALUE), String.valueOf(commitIndex));\n+      File newCurrentLogDatFile = SystemFileFactory.INSTANCE\n+          .getFile(currentLogDataFile.getParent() + File.separator + newDataFileName);\n+      if (!currentLogDataFile.renameTo(newCurrentLogDatFile)) {\n+        logger.error(\"rename log data file={} failed\", currentLogDataFile.getAbsoluteFile());\n+      }\n+      logDataFileList.remove(logDataFileList.size() - 1);\n+      logDataFileList.add(newCurrentLogDatFile);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015"}, "originalPosition": 337}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk2NDM2Mw==", "bodyText": "Is it really startTime? I think it should more clear here.", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514964363", "createdAt": "2020-10-30T09:21:30Z", "author": {"login": "jt2594838"}, "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -50,82 +53,140 @@\n import org.apache.iotdb.db.engine.version.SimpleFileVersionController;\n import org.apache.iotdb.db.engine.version.VersionController;\n import org.apache.iotdb.db.utils.TestOnly;\n+import org.apache.iotdb.tsfile.utils.BytesUtils;\n+import org.apache.iotdb.tsfile.utils.Pair;\n import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n public class SyncLogDequeSerializer implements StableEntryManager {\n \n   private static final Logger logger = LoggerFactory.getLogger(SyncLogDequeSerializer.class);\n-  private static final String LOG_FILE_PREFIX = \".data\";\n+  private static final String LOG_DATA_FILE_SUFFIX = \"data\";\n+  private static final String LOG_INDEX_FILE_SUFFIX = \"idx\";\n+\n+  /**\n+   * the log data files\n+   */\n+  private List<File> logDataFileList;\n+\n+  /**\n+   * the log index files\n+   */\n+  private List<File> logIndexFileList;\n \n-  List<File> logFileList;\n   private LogParser parser = LogParser.getINSTANCE();\n   private File metaFile;\n-  private FileOutputStream currentLogOutputStream;\n-  private Deque<Integer> logSizeDeque = new ArrayDeque<>();\n+  private FileOutputStream currentLogDataOutputStream;\n+  private FileOutputStream currentLogIndexOutputStream;\n   private LogManagerMeta meta;\n   private HardState state;\n-  // mark first log position\n-  private long firstLogPosition = 0;\n-  // removed log size\n-  private long removedLogSize = 0;\n-  // when the removedLogSize larger than this, we actually delete logs\n-  private long maxRemovedLogSize = ClusterDescriptor.getInstance().getConfig()\n-      .getMaxUnsnapshotLogSize();\n-  // min version of available log\n+\n+  /**\n+   * min version of available log\n+   */\n   private long minAvailableVersion = 0;\n-  // max version of available log\n+\n+  /**\n+   * max version of available log\n+   */\n   private long maxAvailableVersion = Long.MAX_VALUE;\n-  // log dir\n+\n   private String logDir;\n-  // version controller\n+\n   private VersionController versionController;\n \n-  private ByteBuffer logBuffer = ByteBuffer\n+  private ByteBuffer logDataBuffer = ByteBuffer\n       .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+  private ByteBuffer logIndexBuffer = ByteBuffer\n+      .allocate(ClusterDescriptor.getInstance().getConfig().getRaftLogBufferSize());\n+\n+  private long offsetOfTheCurrentLogDataOutputStream = 0;\n+\n+  /**\n+   * file name pattern:\n+   * <p>\n+   * for log data file: ${startTime}-${Long.MAX_VALUE}-{version}-data\n+   * <p>\n+   * for log index file: ${startTime}-${Long.MAX_VALUE}-{version}-idx", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3MTM2Mg==", "bodyText": "Better to use buffered stream.", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514971362", "createdAt": "2020-10-30T09:34:10Z", "author": {"login": "jt2594838"}, "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -349,9 +495,136 @@ private void checkLogFile(File file) {\n       } catch (IOException e) {\n         logger.warn(\"Cannot delete outdated log file {}\", file);\n       }\n+      return false;\n+    }\n+\n+    String[] splits = file.getName().split(FILE_NAME_SEPARATOR);\n+    // start index should be smaller than end index\n+    if (Long.parseLong(splits[0]) > Long.parseLong(splits[1])) {\n+      try {\n+        Files.delete(file.toPath());\n+      } catch (IOException e) {\n+        logger.warn(\"Cannot delete incorrect log file {}\", file);\n+      }\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  private void recoverTheLastLogFile() {\n+    if (logIndexFileList.isEmpty()) {\n+      logger.info(\"no log index file to recover\");\n+      return;\n+    }\n+\n+    File lastIndexFile = logIndexFileList.get(logIndexFileList.size() - 1);\n+    long endIndex = Long.parseLong(lastIndexFile.getName().split(FILE_NAME_SEPARATOR)[1]);\n+    boolean success = true;\n+    if (endIndex != Long.MAX_VALUE) {\n+      logger.info(\"last log index file={} no need to recover\", lastIndexFile.getAbsoluteFile());\n+    } else {\n+      success = recoverTheLastLogIndexFile(lastIndexFile);\n+    }\n+\n+    if (!success) {\n+      logger.error(\"recover log index file failed, clear all logs in disk, {}\",\n+          lastIndexFile.getAbsoluteFile());\n+      for (int i = 0; i < logIndexFileList.size(); i++) {\n+        deleteLogDataAndIndexFile(i);\n+      }\n+      clearFirstLogIndex();\n+\n+      return;\n+    }\n+\n+    File lastDataFile = logDataFileList.get(logDataFileList.size() - 1);\n+    endIndex = Long.parseLong(lastDataFile.getName().split(FILE_NAME_SEPARATOR)[1]);\n+    if (endIndex != Long.MAX_VALUE) {\n+      logger.info(\"last log data file={} no need to recover\", lastDataFile.getAbsoluteFile());\n+      return;\n+    }\n+\n+    success = recoverTheLastLogDataFile(logDataFileList.get(logDataFileList.size() - 1));\n+    if (!success) {\n+      logger.error(\"recover log data file failed, clear all logs in disk,{}\",\n+          lastDataFile.getAbsoluteFile());\n+      for (int i = 0; i < logIndexFileList.size(); i++) {\n+        deleteLogDataAndIndexFile(i);\n+      }\n+      clearFirstLogIndex();\n+    }\n+  }\n+\n+  private boolean recoverTheLastLogDataFile(File file) {\n+    String[] splits = file.getName().split(FILE_NAME_SEPARATOR);\n+    long startIndex = Long.parseLong(splits[0]);\n+    Pair<File, Pair<Long, Long>> fileStartAndEndIndex = getLogIndexFile(startIndex);\n+    if (fileStartAndEndIndex.right.left == startIndex) {\n+      long endIndex = fileStartAndEndIndex.right.right;\n+      String newDataFileName = file.getName()\n+          .replaceAll(String.valueOf(Long.MAX_VALUE), String.valueOf(endIndex));\n+      File newLogDataFile = SystemFileFactory.INSTANCE\n+          .getFile(file.getParent() + File.separator + newDataFileName);\n+      if (!file.renameTo(newLogDataFile)) {\n+        logger.error(\"rename log data file={} failed when recover\", file.getAbsoluteFile());\n+      }\n+      logDataFileList.remove(logDataFileList.size() - 1);\n+      logDataFileList.add(newLogDataFile);\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  private boolean recoverTheLastLogIndexFile(File file) {\n+    logger.debug(\"start to recover the last log index file={}\", file.getAbsoluteFile());\n+    String[] splits = file.getName().split(FILE_NAME_SEPARATOR);\n+    long startIndex = Long.parseLong(splits[0]);\n+    int longLength = 8;\n+    byte[] bytes = new byte[longLength];\n+\n+    int totalCount = 0;\n+    long offset = 0;\n+    try (FileInputStream inputStream = new FileInputStream(file)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015"}, "originalPosition": 631}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3NDU2OQ==", "bodyText": "If maxPersistRaftLogNumberOnDisk is too small, is it possible that the current file will be deleted here?", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514974569", "createdAt": "2020-10-30T09:39:55Z", "author": {"login": "jt2594838"}, "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -593,75 +815,385 @@ public void close() {\n   }\n \n   /**\n-   * adjust maxRemovedLogSize to the first log file\n+   * get file version from file The file name structure is as follows\uff1a\n+   * {startLogIndex}-{endLogIndex}-{version}-data)\n+   *\n+   * @param file file\n+   * @return version from file\n+   */\n+  private long getFileVersion(File file) {\n+    return Long.parseLong(file.getName().split(FILE_NAME_SEPARATOR)[2]);\n+  }\n+\n+  public void checkDeletePersistRaftLog() {\n+    // 1. check the log index offset list size\n+    try {\n+      lock.writeLock().lock();\n+      if (logIndexOffsetList.size() > maxRaftLogIndexSizeInMemory) {\n+        int compactIndex = logIndexOffsetList.size() - maxRaftLogIndexSizeInMemory;\n+        logIndexOffsetList.subList(0, compactIndex).clear();\n+        firstLogIndex += compactIndex;\n+      }\n+    } finally {\n+      lock.writeLock().unlock();\n+    }\n+\n+    // 2. check the persist log file number\n+    while (logDataFileList.size() > maxNumberOfPersistRaftLogFiles) {\n+      deleteLogDataAndIndexFile(0);\n+    }\n+\n+    // 3. check the persist log index number\n+    while (!logDataFileList.isEmpty()) {\n+      File firstFile = logDataFileList.get(0);\n+      String[] splits = firstFile.getName().split(FILE_NAME_SEPARATOR);\n+      if (meta.getCommitLogIndex() - Long.parseLong(splits[1]) > maxPersistRaftLogNumberOnDisk) {\n+        deleteLogDataAndIndexFile(0);\n+      } else {\n+        return;\n+      }\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015"}, "originalPosition": 951}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDk3NjE3MQ==", "bodyText": "I think we should enforce a limit on this method to avoid out ot memory when the range is too long.", "url": "https://github.com/apache/iotdb/pull/1865#discussion_r514976171", "createdAt": "2020-10-30T09:42:46Z", "author": {"login": "jt2594838"}, "path": "cluster/src/main/java/org/apache/iotdb/cluster/log/manage/serializable/SyncLogDequeSerializer.java", "diffHunk": "@@ -593,75 +815,385 @@ public void close() {\n   }\n \n   /**\n-   * adjust maxRemovedLogSize to the first log file\n+   * get file version from file The file name structure is as follows\uff1a\n+   * {startLogIndex}-{endLogIndex}-{version}-data)\n+   *\n+   * @param file file\n+   * @return version from file\n+   */\n+  private long getFileVersion(File file) {\n+    return Long.parseLong(file.getName().split(FILE_NAME_SEPARATOR)[2]);\n+  }\n+\n+  public void checkDeletePersistRaftLog() {\n+    // 1. check the log index offset list size\n+    try {\n+      lock.writeLock().lock();\n+      if (logIndexOffsetList.size() > maxRaftLogIndexSizeInMemory) {\n+        int compactIndex = logIndexOffsetList.size() - maxRaftLogIndexSizeInMemory;\n+        logIndexOffsetList.subList(0, compactIndex).clear();\n+        firstLogIndex += compactIndex;\n+      }\n+    } finally {\n+      lock.writeLock().unlock();\n+    }\n+\n+    // 2. check the persist log file number\n+    while (logDataFileList.size() > maxNumberOfPersistRaftLogFiles) {\n+      deleteLogDataAndIndexFile(0);\n+    }\n+\n+    // 3. check the persist log index number\n+    while (!logDataFileList.isEmpty()) {\n+      File firstFile = logDataFileList.get(0);\n+      String[] splits = firstFile.getName().split(FILE_NAME_SEPARATOR);\n+      if (meta.getCommitLogIndex() - Long.parseLong(splits[1]) > maxPersistRaftLogNumberOnDisk) {\n+        deleteLogDataAndIndexFile(0);\n+      } else {\n+        return;\n+      }\n+    }\n+  }\n+\n+  private void deleteLogDataAndIndexFile(int index) {\n+    File logDataFile = null;\n+    File logIndexFile = null;\n+    try {\n+      lock.writeLock().lock();\n+      logDataFile = logDataFileList.get(index);\n+      logIndexFile = logIndexFileList.get(index);\n+      Files.delete(logDataFile.toPath());\n+      Files.delete(logIndexFile.toPath());\n+      logDataFileList.remove(index);\n+      logIndexFileList.remove(index);\n+      logger.debug(\"delete date file={}, index file={}\", logDataFile.getAbsoluteFile(),\n+          logIndexFile.getAbsoluteFile());\n+    } catch (IOException e) {\n+      logger.error(\"delete file failed, index={}, data file={}, index file={}\", index,\n+          logDataFile == null ? null : logDataFile.getAbsoluteFile(),\n+          logIndexFile == null ? null : logIndexFile.getAbsoluteFile());\n+    } finally {\n+      lock.writeLock().unlock();\n+    }\n+  }\n+\n+  /**\n+   * The file name structure is as follows\uff1a {startLogIndex}-{endLogIndex}-{version}-data)\n+   *\n+   * @param file1 File to compare\n+   * @param file2 File to compare\n    */\n-  private void adjustNextThreshold() {\n-    if (!logFileList.isEmpty()) {\n-      maxRemovedLogSize = logFileList.get(0).length();\n+  private int comparePersistLogFileName(File file1, File file2) {\n+    String[] items1 = file1.getName().split(FILE_NAME_SEPARATOR);\n+    String[] items2 = file2.getName().split(FILE_NAME_SEPARATOR);\n+    if (items1.length != FILE_NAME_PART_LENGTH || items2.length != FILE_NAME_PART_LENGTH) {\n+      logger.error(\n+          \"file1={}, file2={} name should be in the following format: startLogIndex-endLogIndex-version-data\",\n+          file1.getAbsoluteFile(), file2.getAbsoluteFile());\n+    }\n+    long startLogIndex1 = Long.parseLong(items1[0]);\n+    long startLogIndex2 = Long.parseLong(items2[0]);\n+    int res = Long.compare(startLogIndex1, startLogIndex2);\n+    if (res == 0) {\n+      return Long.compare(Long.parseLong(items1[1]), Long.parseLong(items2[1]));\n     }\n+    return res;\n   }\n \n   /**\n-   * actually delete the data file which only contains removed data\n+   * @param startIndex the log start index\n+   * @param endIndex   the log end index\n+   * @return the raft log which index between [startIndex, endIndex] or empty if not found\n    */\n-  private void actuallyDeleteFile() {\n-    Iterator<File> logFileIterator = logFileList.iterator();\n-    while (logFileIterator.hasNext()) {\n-      File logFile = logFileIterator.next();\n-      if (logger.isDebugEnabled()) {\n-        logger.debug(\"Examining file for removal, file: {}, len: {}, removedLogSize: {}\", logFile\n-            , logFile.length(), removedLogSize);\n-      }\n-      if (logFile.length() > removedLogSize) {\n-        break;\n-      }\n-\n-      logger.info(\"Removing a log file {}, len: {}, removedLogSize: {}\", logFile,\n-          logFile.length(), removedLogSize);\n-      removedLogSize -= logFile.length();\n-      // if system down before delete, we can use this to delete file during recovery\n-      minAvailableVersion = getFileVersion(logFile);\n-      serializeMeta(meta);\n+  @Override\n+  public List<Log> getLogs(long startIndex, long endIndex) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30542c1d5231ec31a73b0a7728cd0b43a7eb9015"}, "originalPosition": 1027}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bc72be14931a04e0cfce7b575d226840f8524d2b", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/bc72be14931a04e0cfce7b575d226840f8524d2b", "committedDate": "2020-10-30T11:26:50Z", "message": "fix windows ut"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fa7f863325e18177cb0827eccdb2c420a9f53d64", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/fa7f863325e18177cb0827eccdb2c420a9f53d64", "committedDate": "2020-11-01T02:38:48Z", "message": "fix revoew"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e8a950a9571fc3397ef540a366f9b934d42a0417", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/e8a950a9571fc3397ef540a366f9b934d42a0417", "committedDate": "2020-11-01T04:17:55Z", "message": "fix review and add per fetch limit when get logs from disks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6e38d4b3e0e5676820ceba712da1a1b0bbe286b3", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/6e38d4b3e0e5676820ceba712da1a1b0bbe286b3", "committedDate": "2020-11-01T04:58:26Z", "message": "fix ci"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b6e598395f377cf74f54a60f8af3432f226ca4eb", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/b6e598395f377cf74f54a60f8af3432f226ca4eb", "committedDate": "2020-11-01T05:37:35Z", "message": "fix review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "70f8abc6edd45f9006ac9b4ec973e8456c24eaa4", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/70f8abc6edd45f9006ac9b4ec973e8456c24eaa4", "committedDate": "2020-11-02T04:12:19Z", "message": "try to fix ci"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "777d8d7e1b654b438b3d8519173efa3a0c22560d", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/777d8d7e1b654b438b3d8519173efa3a0c22560d", "committedDate": "2020-11-02T06:17:52Z", "message": "try to fix windows ci"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMzYyODkw", "url": "https://github.com/apache/iotdb/pull/1865#pullrequestreview-521362890", "createdAt": "2020-11-02T06:43:57Z", "commit": {"oid": "777d8d7e1b654b438b3d8519173efa3a0c22560d"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "69561c255886cd65aad93435372815d9d4114254", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/69561c255886cd65aad93435372815d9d4114254", "committedDate": "2020-11-02T07:02:48Z", "message": "fix exception not catch"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6de89e2c58e4794ec37adbad5b8894cd14a8739f", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/6de89e2c58e4794ec37adbad5b8894cd14a8739f", "committedDate": "2020-11-02T07:20:50Z", "message": "fix null exception"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "671194506a53bd0fa6bcf435d1cf3bbc2456ae04", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/671194506a53bd0fa6bcf435d1cf3bbc2456ae04", "committedDate": "2020-11-02T08:55:58Z", "message": "resolve confilict"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d19f3b7fb93eb49299b62207c506a09d50cb043f", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/d19f3b7fb93eb49299b62207c506a09d50cb043f", "committedDate": "2020-11-03T02:05:11Z", "message": "fix IT bug"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "abf53f677a41790f4956541d38bbdd50a28802ea", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/abf53f677a41790f4956541d38bbdd50a28802ea", "committedDate": "2020-11-03T04:08:38Z", "message": "Merge branch 'apache_cluster_new' into apache_cluster_new_1023_raft_log_catch_up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c7450996bb79d1a5b58aad50f4d406512a99c190", "author": {"user": {"login": "neuyilan", "name": "Houliang Qi"}}, "url": "https://github.com/apache/iotdb/commit/c7450996bb79d1a5b58aad50f4d406512a99c190", "committedDate": "2020-11-03T06:25:16Z", "message": "fix IT win bug"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3738, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}