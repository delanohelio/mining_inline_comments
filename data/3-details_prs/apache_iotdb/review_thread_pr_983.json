{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk4MjEyNjYz", "number": 983, "reviewThreads": {"totalCount": 31, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxMjowNDoyOVrOD1P6yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwMjo1NjoxOFrOD9DnFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3MTYxOTI4OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/conf/IoTDBConfigCheck.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxMjowNDoyOVrOGKlweg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxNDo0MToyOFrOGKrvNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzc1NzU2Mg==", "bodyText": "Here, v0.9.0's data throws java.io.FileNotFoundException", "url": "https://github.com/apache/iotdb/pull/983#discussion_r413757562", "createdAt": "2020-04-23T12:04:29Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/conf/IoTDBConfigCheck.java", "diffHunk": "@@ -100,46 +102,83 @@ private void checkFile(String filepath) {\n           properties.store(outputStream, \"System properties:\");\n         }\n       }\n+      else if (!file.exists() && tmpPropertiesFile.exists()) {\n+        // rename upgraded system.properties.tmp to system.properties\n+        FSFactoryProducer.getFSFactory().moveFile(tmpPropertiesFile, file);\n+        logger.info(\" {} has been upgraded.\", file.getAbsolutePath());\n+      }\n     } catch (IOException e) {\n       logger.error(\"Can not create {}.\", file.getAbsolutePath(), e);\n     }\n+    \n     // get existed properties from system_properties.txt\n     File inputFile = SystemFileFactory.INSTANCE\n             .getFile(filepath + File.separator + PROPERTIES_FILE_NAME);\n     try (FileInputStream inputStream = new FileInputStream(inputFile.toString())) {\n       properties.load(new InputStreamReader(inputStream, TSFileConfig.STRING_CHARSET));\n-      if (!properties.getProperty(\"timestamp_precision\").equals(timestampPrecision)) {\n-        logger.error(\"Wrong timestamp precision, please set as: \" + properties\n-                .getProperty(\"timestamp_precision\") + \" !\");\n-        System.exit(-1);\n-      }\n-      if (!(Long.parseLong(properties.getProperty(\"storage_group_time_range\"))\n-              == partitionInterval)) {\n-        logger.error(\"Wrong storage group time range, please set as: \" + properties\n-                .getProperty(\"storage_group_time_range\") + \" !\");\n-        System.exit(-1);\n-      }\n-      if (!(properties.getProperty(\"tsfile_storage_fs\").equals(tsfileFileSystem))) {\n-        logger.error(\"Wrong tsfile file system, please set as: \" + properties\n-            .getProperty(\"tsfile_storage_fs\") + \" !\");\n-        System.exit(-1);\n-      }\n-      if (properties.getProperty(\"iotdb_version\") == null) {\n-        logger.info(\"Lower iotdb version detected, upgrading old mlog file... \");\n-        MLogWriter.upgradeMLog(IoTDBDescriptor.getInstance().getConfig().getSchemaDir(), \n-            MetadataConstant.METADATA_LOG);\n-        logger.info(\"Old mlog file is upgraded.\");\n-        try (FileOutputStream outputStream = new FileOutputStream(file.toString())) {\n-          properties.setProperty(\"timestamp_precision\", timestampPrecision);\n-          properties.setProperty(\"storage_group_time_range\", String.valueOf(partitionInterval));\n-          properties.setProperty(\"tsfile_storage_fs\", tsfileFileSystem);\n-          properties.setProperty(\"iotdb_version\", iotdbVersion);\n-          properties.store(outputStream, \"System properties:\");\n+      // need to upgrade\n+      if (!properties.containsKey(\"iotdb_version\")) {\n+        properties.setProperty(\"storage_group_time_range\", String.valueOf(partitionInterval));\n+        properties.setProperty(\"tsfile_storage_fs\", tsfileFileSystem);\n+        properties.setProperty(\"iotdb_version\", iotdbVersion);\n+        // upgrade mlog\n+        try {\n+          MLogWriter.upgradeMLog(IoTDBDescriptor.getInstance().getConfig().getSchemaDir(), \n+              MetadataConstant.METADATA_LOG);\n+        } catch (IOException e) {\n+          logger.error(\"Upgrade mlog.txt from {} failed.\", file.getAbsolutePath(), e);\n         }\n+      } else {\n+        checkProperties();\n+        return;\n       }\n     } catch (IOException e) {\n       logger.error(\"Load system.properties from {} failed.\", file.getAbsolutePath(), e);\n     }\n+\n+    // it's an old version system.properties\n+    // try to add the storage_group_time_range, tsfile_storage_fs \n+    // and iotdb_version property in system.properties.tmp\n+    try (FileOutputStream outputStream = new FileOutputStream(tmpPropertiesFile.toString())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54f5bf9e3466fc2e4333db6399d3bba6cc9e1417"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzg1NTU0Mg==", "bodyText": "I have fixed this bug. This is caused by the wrong name of system.properties.tmp.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r413855542", "createdAt": "2020-04-23T14:41:28Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/conf/IoTDBConfigCheck.java", "diffHunk": "@@ -100,46 +102,83 @@ private void checkFile(String filepath) {\n           properties.store(outputStream, \"System properties:\");\n         }\n       }\n+      else if (!file.exists() && tmpPropertiesFile.exists()) {\n+        // rename upgraded system.properties.tmp to system.properties\n+        FSFactoryProducer.getFSFactory().moveFile(tmpPropertiesFile, file);\n+        logger.info(\" {} has been upgraded.\", file.getAbsolutePath());\n+      }\n     } catch (IOException e) {\n       logger.error(\"Can not create {}.\", file.getAbsolutePath(), e);\n     }\n+    \n     // get existed properties from system_properties.txt\n     File inputFile = SystemFileFactory.INSTANCE\n             .getFile(filepath + File.separator + PROPERTIES_FILE_NAME);\n     try (FileInputStream inputStream = new FileInputStream(inputFile.toString())) {\n       properties.load(new InputStreamReader(inputStream, TSFileConfig.STRING_CHARSET));\n-      if (!properties.getProperty(\"timestamp_precision\").equals(timestampPrecision)) {\n-        logger.error(\"Wrong timestamp precision, please set as: \" + properties\n-                .getProperty(\"timestamp_precision\") + \" !\");\n-        System.exit(-1);\n-      }\n-      if (!(Long.parseLong(properties.getProperty(\"storage_group_time_range\"))\n-              == partitionInterval)) {\n-        logger.error(\"Wrong storage group time range, please set as: \" + properties\n-                .getProperty(\"storage_group_time_range\") + \" !\");\n-        System.exit(-1);\n-      }\n-      if (!(properties.getProperty(\"tsfile_storage_fs\").equals(tsfileFileSystem))) {\n-        logger.error(\"Wrong tsfile file system, please set as: \" + properties\n-            .getProperty(\"tsfile_storage_fs\") + \" !\");\n-        System.exit(-1);\n-      }\n-      if (properties.getProperty(\"iotdb_version\") == null) {\n-        logger.info(\"Lower iotdb version detected, upgrading old mlog file... \");\n-        MLogWriter.upgradeMLog(IoTDBDescriptor.getInstance().getConfig().getSchemaDir(), \n-            MetadataConstant.METADATA_LOG);\n-        logger.info(\"Old mlog file is upgraded.\");\n-        try (FileOutputStream outputStream = new FileOutputStream(file.toString())) {\n-          properties.setProperty(\"timestamp_precision\", timestampPrecision);\n-          properties.setProperty(\"storage_group_time_range\", String.valueOf(partitionInterval));\n-          properties.setProperty(\"tsfile_storage_fs\", tsfileFileSystem);\n-          properties.setProperty(\"iotdb_version\", iotdbVersion);\n-          properties.store(outputStream, \"System properties:\");\n+      // need to upgrade\n+      if (!properties.containsKey(\"iotdb_version\")) {\n+        properties.setProperty(\"storage_group_time_range\", String.valueOf(partitionInterval));\n+        properties.setProperty(\"tsfile_storage_fs\", tsfileFileSystem);\n+        properties.setProperty(\"iotdb_version\", iotdbVersion);\n+        // upgrade mlog\n+        try {\n+          MLogWriter.upgradeMLog(IoTDBDescriptor.getInstance().getConfig().getSchemaDir(), \n+              MetadataConstant.METADATA_LOG);\n+        } catch (IOException e) {\n+          logger.error(\"Upgrade mlog.txt from {} failed.\", file.getAbsolutePath(), e);\n         }\n+      } else {\n+        checkProperties();\n+        return;\n       }\n     } catch (IOException e) {\n       logger.error(\"Load system.properties from {} failed.\", file.getAbsolutePath(), e);\n     }\n+\n+    // it's an old version system.properties\n+    // try to add the storage_group_time_range, tsfile_storage_fs \n+    // and iotdb_version property in system.properties.tmp\n+    try (FileOutputStream outputStream = new FileOutputStream(tmpPropertiesFile.toString())) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzc1NzU2Mg=="}, "originalCommit": {"oid": "54f5bf9e3466fc2e4333db6399d3bba6cc9e1417"}, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3MTc0OTczOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/conf/IoTDBConfigCheck.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxMjozNToyMlrOGKm-hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxNDo1NDoyMlrOGKscYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzc3NzU0Mg==", "bodyText": "when the file exists during upgrade, both the if and elif does not work", "url": "https://github.com/apache/iotdb/pull/983#discussion_r413777542", "createdAt": "2020-04-23T12:35:22Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/conf/IoTDBConfigCheck.java", "diffHunk": "@@ -84,12 +84,14 @@ private void createDir(String filepath) {\n   }\n \n   private void checkFile(String filepath) {\n-    // create file : read timestamp precision from engine.properties, create system_properties.txt\n+    // create file : read timestamp precision from engine.properties, create system.properties\n     // use output stream to write timestamp precision to file.\n     File file = SystemFileFactory.INSTANCE\n             .getFile(filepath + File.separator + PROPERTIES_FILE_NAME);\n+    File tmpPropertiesFile = new File(file.getAbsoluteFile() \n+        + File.separator + \"tmp\");\n     try {\n-      if (!file.exists()) {\n+      if (!file.exists() && !tmpPropertiesFile.exists()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c4e59c6d0fe877516eb7152bed7b40d2cdefec07"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzg2NzEwNg==", "bodyText": "Yes, you are right. When the file exists, both the if and elif does not work. But following this part, I will check if the file need to be upgraded. I don't think there is any problem.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r413867106", "createdAt": "2020-04-23T14:54:22Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/conf/IoTDBConfigCheck.java", "diffHunk": "@@ -84,12 +84,14 @@ private void createDir(String filepath) {\n   }\n \n   private void checkFile(String filepath) {\n-    // create file : read timestamp precision from engine.properties, create system_properties.txt\n+    // create file : read timestamp precision from engine.properties, create system.properties\n     // use output stream to write timestamp precision to file.\n     File file = SystemFileFactory.INSTANCE\n             .getFile(filepath + File.separator + PROPERTIES_FILE_NAME);\n+    File tmpPropertiesFile = new File(file.getAbsoluteFile() \n+        + File.separator + \"tmp\");\n     try {\n-      if (!file.exists()) {\n+      if (!file.exists() && !tmpPropertiesFile.exists()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzc3NzU0Mg=="}, "originalCommit": {"oid": "c4e59c6d0fe877516eb7152bed7b40d2cdefec07"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU3MTg1MTE2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxMjo1NzoyOFrOGKn7Jg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxNDo1NTo1MVrOGKshRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzc5MzA2Mg==", "bodyText": "maybe the comment is upgrading unsequence TsFile resource list", "url": "https://github.com/apache/iotdb/pull/983#discussion_r413793062", "createdAt": "2020-04-23T12:57:28Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -160,9 +145,16 @@\n         return rangeCompare == 0 ? compareFileName(o1.getFile(), o2.getFile()) : rangeCompare;\n       });\n \n+  // upgrading sequence TsFile resource list\n+  private List<TsFileResource> upgradeSeqFileList = new LinkedList<>();\n+\n   private CopyOnReadLinkedList<TsFileProcessor> closingSequenceTsFileProcessor = new CopyOnReadLinkedList<>();\n   // includes sealed and unsealed unSequence TsFiles\n   private List<TsFileResource> unSequenceFileList = new ArrayList<>();\n+\n+  // upgrading sequence TsFile resource list", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c4e59c6d0fe877516eb7152bed7b40d2cdefec07"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzg2ODM1OA==", "bodyText": "fixed", "url": "https://github.com/apache/iotdb/pull/983#discussion_r413868358", "createdAt": "2020-04-23T14:55:51Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -160,9 +145,16 @@\n         return rangeCompare == 0 ? compareFileName(o1.getFile(), o2.getFile()) : rangeCompare;\n       });\n \n+  // upgrading sequence TsFile resource list\n+  private List<TsFileResource> upgradeSeqFileList = new LinkedList<>();\n+\n   private CopyOnReadLinkedList<TsFileProcessor> closingSequenceTsFileProcessor = new CopyOnReadLinkedList<>();\n   // includes sealed and unsealed unSequence TsFiles\n   private List<TsFileResource> unSequenceFileList = new ArrayList<>();\n+\n+  // upgrading sequence TsFile resource list", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzc5MzA2Mg=="}, "originalCommit": {"oid": "c4e59c6d0fe877516eb7152bed7b40d2cdefec07"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMDMyNzEzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/conf/IoTDBConfigCheck.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwOToyNjowNlrOGS5y-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNDoxOToyNVrOGTEnQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ3NDQ5MA==", "bodyText": "this check is not needed", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422474490", "createdAt": "2020-05-09T09:26:06Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/conf/IoTDBConfigCheck.java", "diffHunk": "@@ -99,48 +101,103 @@ private void checkFile(String filepath) {\n           properties.setProperty(\"iotdb_version\", iotdbVersion);\n           properties.store(outputStream, \"System properties:\");\n         }\n+        checkProperties();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY1MTcxMw==", "bodyText": "Fixed", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422651713", "createdAt": "2020-05-10T14:19:25Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/conf/IoTDBConfigCheck.java", "diffHunk": "@@ -99,48 +101,103 @@ private void checkFile(String filepath) {\n           properties.setProperty(\"iotdb_version\", iotdbVersion);\n           properties.store(outputStream, \"System properties:\");\n         }\n+        checkProperties();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQ3NDQ5MA=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTE2ODc2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/metadata/MLogWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwMzoxMjoxN1rOGTADeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwOToxOTo0OFrOGTCcJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU3NzAxNg==", "bodyText": "FSFactoryProducer is not designed for system file. We need to construct a SystemFileProducer", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422577016", "createdAt": "2020-05-10T03:12:17Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/metadata/MLogWriter.java", "diffHunk": "@@ -124,7 +124,8 @@ public void changeOffset(String path, long offset) throws IOException {\n   public static void upgradeMLog(String schemaDir, String logFileName) throws IOException {\n     File logFile = FSFactoryProducer.getFSFactory()\n         .getFile(schemaDir + File.separator + logFileName);\n-    File tmpLogFile = new File(logFile.getAbsolutePath() + \".tmp\");\n+    File tmpLogFile = FSFactoryProducer.getFSFactory()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYxNjEwMg==", "bodyText": "Fixed : )", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422616102", "createdAt": "2020-05-10T09:19:48Z", "author": {"login": "samperson1997"}, "path": "server/src/main/java/org/apache/iotdb/db/metadata/MLogWriter.java", "diffHunk": "@@ -124,7 +124,8 @@ public void changeOffset(String path, long offset) throws IOException {\n   public static void upgradeMLog(String schemaDir, String logFileName) throws IOException {\n     File logFile = FSFactoryProducer.getFSFactory()\n         .getFile(schemaDir + File.separator + logFileName);\n-    File tmpLogFile = new File(logFile.getAbsolutePath() + \".tmp\");\n+    File tmpLogFile = FSFactoryProducer.getFSFactory()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU3NzAxNg=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTE5ODU5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/version/SimpleFileVersionController.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwMzo1ODoyNlrOGTAQxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwMzo1ODoyNlrOGTAQxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4MDQyMQ==", "bodyText": "it's better to store this file in directoryPath / upgrade folder, be consistent with other partitions", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422580421", "createdAt": "2020-05-10T03:58:26Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/version/SimpleFileVersionController.java", "diffHunk": "@@ -54,6 +55,15 @@ public SimpleFileVersionController(String directoryPath, long timePartitionId)\n     restore();\n   }\n \n+  /**\n+   * only used for upgrading\n+   */\n+  public SimpleFileVersionController(String directoryPath) throws IOException {\n+    this.directoryPath = directoryPath;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTE5OTM2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/query/reader/chunk/metadata/DiskChunkMetadataLoader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwMzo1OTo0MVrOGTARJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoxNjoxNlrOGTzs9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4MDUxOA==", "bodyText": "add javadoc: for upgrade? in which case this will be called", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422580518", "createdAt": "2020-05-10T03:59:41Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/query/reader/chunk/metadata/DiskChunkMetadataLoader.java", "diffHunk": "@@ -63,6 +63,10 @@ public DiskChunkMetadataLoader(TsFileResource resource, Path seriesPath, QueryCo\n             || chunkMetaData.getStartTime() > chunkMetaData.getEndTime());\n     return chunkMetadataList;\n   }\n+  \n+  public void setDiskChunkLoader(List<ChunkMetadata> chunkMetadataList) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyMzIyMw==", "bodyText": "Added", "url": "https://github.com/apache/iotdb/pull/983#discussion_r423423223", "createdAt": "2020-05-12T02:16:16Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/query/reader/chunk/metadata/DiskChunkMetadataLoader.java", "diffHunk": "@@ -63,6 +63,10 @@ public DiskChunkMetadataLoader(TsFileResource resource, Path seriesPath, QueryCo\n             || chunkMetaData.getStartTime() > chunkMetaData.getEndTime());\n     return chunkMetadataList;\n   }\n+  \n+  public void setDiskChunkLoader(List<ChunkMetadata> chunkMetadataList) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4MDUxOA=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTIwMDkwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/UpgradeTool.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNDowMTo1NlrOGTAR0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNDoyOTowMVrOGTEr9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4MDY4OQ==", "bodyText": "How about combining this class with TsFileOnlineUpgradeTool", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422580689", "createdAt": "2020-05-10T04:01:56Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/UpgradeTool.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+\n+public class UpgradeTool {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY1MjkxNg==", "bodyText": "Fixed", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422652916", "createdAt": "2020-05-10T14:29:01Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/UpgradeTool.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+\n+public class UpgradeTool {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4MDY4OQ=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTIwMjUwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNDowNDoyN1rOGTASfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNDozMjowOFrOGTEtoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4MDg2Mg==", "bodyText": "why not just use the upgradeSeqFileList.size()?", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422580862", "createdAt": "2020-05-10T04:04:27Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -1313,12 +1450,12 @@ private void closeUnsealedTsFileProcessorCallBack(\n    */\n   public int countUpgradeFiles() {\n     int cntUpgradeFileNum = 0;\n-    for (TsFileResource seqTsFileResource : sequenceFileTreeSet) {\n+    for (TsFileResource seqTsFileResource : upgradeSeqFileList) {\n       if (UpgradeUtils.isNeedUpgrade(seqTsFileResource)) {\n         cntUpgradeFileNum += 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 294}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY1MzM0NQ==", "bodyText": "Yes, using upgradeSeqFileList.size() is straight forward.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422653345", "createdAt": "2020-05-10T14:32:08Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -1313,12 +1450,12 @@ private void closeUnsealedTsFileProcessorCallBack(\n    */\n   public int countUpgradeFiles() {\n     int cntUpgradeFileNum = 0;\n-    for (TsFileResource seqTsFileResource : sequenceFileTreeSet) {\n+    for (TsFileResource seqTsFileResource : upgradeSeqFileList) {\n       if (UpgradeUtils.isNeedUpgrade(seqTsFileResource)) {\n         cntUpgradeFileNum += 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4MDg2Mg=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 294}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTIwMjYwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNDowNDo0NFrOGTASjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNDozMzo1MVrOGTEuow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4MDg3Nw==", "bodyText": "the same with above", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422580877", "createdAt": "2020-05-10T04:04:44Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -1313,12 +1450,12 @@ private void closeUnsealedTsFileProcessorCallBack(\n    */\n   public int countUpgradeFiles() {\n     int cntUpgradeFileNum = 0;\n-    for (TsFileResource seqTsFileResource : sequenceFileTreeSet) {\n+    for (TsFileResource seqTsFileResource : upgradeSeqFileList) {\n       if (UpgradeUtils.isNeedUpgrade(seqTsFileResource)) {\n         cntUpgradeFileNum += 1;\n       }\n     }\n-    for (TsFileResource unseqTsFileResource : unSequenceFileList) {\n+    for (TsFileResource unseqTsFileResource : upgradeUnseqFileList) {\n       if (UpgradeUtils.isNeedUpgrade(unseqTsFileResource)) {\n         cntUpgradeFileNum += 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 300}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY1MzYwMw==", "bodyText": "Fixed", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422653603", "createdAt": "2020-05-10T14:33:51Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -1313,12 +1450,12 @@ private void closeUnsealedTsFileProcessorCallBack(\n    */\n   public int countUpgradeFiles() {\n     int cntUpgradeFileNum = 0;\n-    for (TsFileResource seqTsFileResource : sequenceFileTreeSet) {\n+    for (TsFileResource seqTsFileResource : upgradeSeqFileList) {\n       if (UpgradeUtils.isNeedUpgrade(seqTsFileResource)) {\n         cntUpgradeFileNum += 1;\n       }\n     }\n-    for (TsFileResource unseqTsFileResource : unSequenceFileList) {\n+    for (TsFileResource unseqTsFileResource : upgradeUnseqFileList) {\n       if (UpgradeUtils.isNeedUpgrade(unseqTsFileResource)) {\n         cntUpgradeFileNum += 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4MDg3Nw=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 300}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTIwMzg0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/utils/UpgradeUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNDowNjoyN1rOGTATHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNDowNjoyN1rOGTATHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4MTAyMQ==", "bodyText": "I can not come up with why we need this method. In merge process, the candidate files should all come from the new files.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422581021", "createdAt": "2020-05-10T04:06:27Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/utils/UpgradeUtils.java", "diffHunk": "@@ -76,9 +76,13 @@ public static boolean isNeedUpgrade(TsFileResource tsFileResource) {\n     return false;\n   }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTIwNDUyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/utils/UpgradeUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNDowNzo0OFrOGTATZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNDo0NDowNVrOGTEzsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4MTA5NA==", "bodyText": "add javadoc", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422581094", "createdAt": "2020-05-10T04:07:48Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/utils/UpgradeUtils.java", "diffHunk": "@@ -76,9 +76,13 @@ public static boolean isNeedUpgrade(TsFileResource tsFileResource) {\n     return false;\n   }\n \n-  public static String getUpgradeFileName(File upgradeResource) {\n-    return upgradeResource.getParentFile().getParent() + File.separator + TMP_STRING\n-        + File.separator + UPGRADE_FILE_PREFIX + upgradeResource.getName();\n+  public static String getOneUpgradedFileName(TsFileResource upgradeResource)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY1NDg5OQ==", "bodyText": "Added", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422654899", "createdAt": "2020-05-10T14:44:05Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/utils/UpgradeUtils.java", "diffHunk": "@@ -76,9 +76,13 @@ public static boolean isNeedUpgrade(TsFileResource tsFileResource) {\n     return false;\n   }\n \n-  public static String getUpgradeFileName(File upgradeResource) {\n-    return upgradeResource.getParentFile().getParent() + File.separator + TMP_STRING\n-        + File.separator + UPGRADE_FILE_PREFIX + upgradeResource.getName();\n+  public static String getOneUpgradedFileName(TsFileResource upgradeResource)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4MTA5NA=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTIwNTI5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/utils/UpgradeUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNDowODozOVrOGTATvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNDo0NTo0MFrOGTE0Yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4MTE4Mw==", "bodyText": "I thought upgradedResource does not have a partitionId. Is the upgradeResource a upgraded or to be upgraded?", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422581183", "createdAt": "2020-05-10T04:08:39Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/utils/UpgradeUtils.java", "diffHunk": "@@ -76,9 +76,13 @@ public static boolean isNeedUpgrade(TsFileResource tsFileResource) {\n     return false;\n   }\n \n-  public static String getUpgradeFileName(File upgradeResource) {\n-    return upgradeResource.getParentFile().getParent() + File.separator + TMP_STRING\n-        + File.separator + UPGRADE_FILE_PREFIX + upgradeResource.getName();\n+  public static String getOneUpgradedFileName(TsFileResource upgradeResource)\n+      throws IOException {\n+    upgradeResource.deserialize();\n+    long firstPartitionId = upgradeResource.getTimePartition();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY1NTA3NQ==", "bodyText": "The upgradeResource is the old TsFileResource to be upgraded.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422655075", "createdAt": "2020-05-10T14:45:40Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/utils/UpgradeUtils.java", "diffHunk": "@@ -76,9 +76,13 @@ public static boolean isNeedUpgrade(TsFileResource tsFileResource) {\n     return false;\n   }\n \n-  public static String getUpgradeFileName(File upgradeResource) {\n-    return upgradeResource.getParentFile().getParent() + File.separator + TMP_STRING\n-        + File.separator + UPGRADE_FILE_PREFIX + upgradeResource.getName();\n+  public static String getOneUpgradedFileName(TsFileResource upgradeResource)\n+      throws IOException {\n+    upgradeResource.deserialize();\n+    long firstPartitionId = upgradeResource.getTimePartition();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4MTE4Mw=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTI2NjQ4OnYy", "diffSide": "RIGHT", "path": "tsfile/src/main/java/org/apache/iotdb/tsfile/file/metadata/TimeseriesMetadata.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNTozMDo0NlrOGTAuzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNDo1NDoyMFrOGTE4rQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4ODEwOQ==", "bodyText": "add javadoc   for  old tsfile", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422588109", "createdAt": "2020-05-10T05:30:46Z", "author": {"login": "qiaojialin"}, "path": "tsfile/src/main/java/org/apache/iotdb/tsfile/file/metadata/TimeseriesMetadata.java", "diffHunk": "@@ -114,10 +118,26 @@ public void setChunkMetadataLoader(IChunkMetadataLoader chunkMetadataLoader) {\n     this.chunkMetadataLoader = chunkMetadataLoader;\n   }\n \n+\n   public List<ChunkMetadata> loadChunkMetadataList() throws IOException {\n+    if (chunkMetadataList != null) {\n+      chunkMetadataLoader.setDiskChunkLoader(chunkMetadataList);\n+      return chunkMetadataList;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY1NjE3Mw==", "bodyText": "Added.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422656173", "createdAt": "2020-05-10T14:54:20Z", "author": {"login": "HTHou"}, "path": "tsfile/src/main/java/org/apache/iotdb/tsfile/file/metadata/TimeseriesMetadata.java", "diffHunk": "@@ -114,10 +118,26 @@ public void setChunkMetadataLoader(IChunkMetadataLoader chunkMetadataLoader) {\n     this.chunkMetadataLoader = chunkMetadataLoader;\n   }\n \n+\n   public List<ChunkMetadata> loadChunkMetadataList() throws IOException {\n+    if (chunkMetadataList != null) {\n+      chunkMetadataLoader.setDiskChunkLoader(chunkMetadataList);\n+      return chunkMetadataList;\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU4ODEwOQ=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTM2NDg1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNzowNzowM1rOGTBbDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNDo1NjoxNlrOGTE5iQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU5OTQzNw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    long offsetOfChunkGroupMetaData = oldChunkGroupMetadata.getStartOffsetOfChunkGroup();\n          \n          \n            \n                    long offsetOfChunkGroup = oldChunkGroupMetadata.getStartOffsetOfChunkGroup();", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422599437", "createdAt": "2020-05-10T07:07:03Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "diffHunk": "@@ -0,0 +1,624 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import org.apache.iotdb.db.engine.StorageEngine;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.common.conf.TSFileDescriptor;\n+import org.apache.iotdb.tsfile.compress.IUnCompressor;\n+import org.apache.iotdb.tsfile.encoding.decoder.Decoder;\n+import org.apache.iotdb.tsfile.exception.write.PageException;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+import org.apache.iotdb.tsfile.file.MetaMarker;\n+import org.apache.iotdb.tsfile.file.footer.ChunkGroupFooter;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkGroupMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.CompressionType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSEncoding;\n+import org.apache.iotdb.tsfile.fileSystem.FSFactoryProducer;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.reader.LocalTsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.page.PageReader;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.write.chunk.ChunkWriterImpl;\n+import org.apache.iotdb.tsfile.write.chunk.IChunkWriter;\n+import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n+import org.apache.iotdb.tsfile.write.writer.TsFileIOWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+public class TsfileOnlineUpgradeTool implements AutoCloseable {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(TsfileOnlineUpgradeTool.class);\n+\n+  private TsFileInput tsFileInput;\n+  private long fileMetadataPos;\n+  private int fileMetadataSize;\n+  private ByteBuffer markerBuffer = ByteBuffer.allocate(Byte.BYTES);\n+  protected String file;\n+  \n+  // PartitionId -> TsFileIOWriter \n+  private Map<Long, TsFileIOWriter> partitionWriterMap;\n+\n+  /**\n+   * Create a file reader of the given file. The reader will read the tail of the file to get the\n+   * file metadata size.Then the reader will skip the first TSFileConfig.OLD_MAGIC_STRING.length()\n+   * bytes of the file for preparing reading real data.\n+   *\n+   * @param file the data file\n+   * @throws IOException If some I/O error occurs\n+   */\n+  public TsfileOnlineUpgradeTool(String file) throws IOException {\n+    this(file, true);\n+  }\n+\n+  /**\n+   * construct function for TsfileOnlineUpgradeTool.\n+   *\n+   * @param file -given file name\n+   * @param loadMetadataSize -load meta data size\n+   */\n+  public TsfileOnlineUpgradeTool(String file, boolean loadMetadataSize) throws IOException {\n+    this.file = file;\n+    final java.nio.file.Path path = Paths.get(file);\n+    tsFileInput = new LocalTsFileInput(path);\n+    partitionWriterMap = new HashMap<>();\n+    try {\n+      if (loadMetadataSize) {\n+        loadMetadataSize();\n+      }\n+    } catch (Exception e) {\n+      tsFileInput.close();\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * \n+   */\n+  public void loadMetadataSize() throws IOException {\n+    ByteBuffer metadataSize = ByteBuffer.allocate(Integer.BYTES);\n+    tsFileInput.read(metadataSize,\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES);\n+    metadataSize.flip();\n+    // read file metadata size and position\n+    fileMetadataSize = ReadWriteIOUtils.readInt(metadataSize);\n+    fileMetadataPos =\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES\n+            - fileMetadataSize;\n+    // skip the magic header\n+    position(TSFileConfig.MAGIC_STRING.length());\n+  }\n+\n+  public String readTailMagic() throws IOException {\n+    long totalSize = tsFileInput.size();\n+\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    tsFileInput.read(magicStringBytes, totalSize - TSFileConfig.MAGIC_STRING.length());\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * whether the file is a complete TsFile: only if the head magic and tail magic string exists.\n+   */\n+  public boolean isComplete() throws IOException {\n+    return tsFileInput.size() >= TSFileConfig.MAGIC_STRING.length() * 2 && readTailMagic()\n+        .equals(readHeadMagic());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public String readHeadMagic() throws IOException {\n+    return readHeadMagic(false);\n+  }\n+\n+  /**\n+   * @param movePosition whether move the position of the file reader after reading the magic header\n+   * to the end of the magic head string.\n+   */\n+  public String readHeadMagic(boolean movePosition) throws IOException {\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    if (movePosition) {\n+      tsFileInput.position(0);\n+      tsFileInput.read(magicStringBytes);\n+    } else {\n+      tsFileInput.read(magicStringBytes, 0);\n+    }\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * this function reads version number and checks compatibility of TsFile.\n+   */\n+  public String readVersionNumber() throws IOException {\n+    ByteBuffer versionNumberBytes = ByteBuffer\n+        .allocate(TSFileConfig.VERSION_NUMBER.getBytes().length);\n+    tsFileInput.position(TSFileConfig.MAGIC_STRING.getBytes().length);\n+    tsFileInput.read(versionNumberBytes);\n+    versionNumberBytes.flip();\n+    return new String(versionNumberBytes.array());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsFileMetadata readFileMetadata() throws IOException {\n+    return OldTsFileMetadata.deserializeFrom(readData(fileMetadataPos, fileMetadataSize));\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsDeviceMetadata readTsDeviceMetaData(OldTsDeviceMetadataIndex index) throws IOException {\n+    return OldTsDeviceMetadata.deserializeFrom(readData(index.getOffset(), index.getLen()));\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_GROUP_FOOTER. <br>\n+   * This method is not threadsafe.\n+   *\n+   * @return a CHUNK_GROUP_FOOTER\n+   * @throws IOException io error\n+   */\n+  public ChunkGroupFooter readChunkGroupFooter() throws IOException {\n+    return ChunkGroupFooter.deserializeFrom(tsFileInput.wrapAsInputStream(), true);\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_HEADER. <br> This\n+   * method is not threadsafe.\n+   *\n+   * @return a CHUNK_HEADER\n+   * @throws IOException io error\n+   */\n+  public ChunkHeader readChunkHeader() throws IOException {\n+    return ChunkHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), true, true);\n+  }\n+\n+  /**\n+   * not thread safe.\n+   *\n+   * @param type given tsfile data type\n+   */\n+  public PageHeader readPageHeader(TSDataType type) throws IOException {\n+    return PageHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), type, true);\n+  }\n+\n+  public ByteBuffer readPage(PageHeader header, CompressionType type)\n+      throws IOException {\n+    ByteBuffer buffer = readData(-1, header.getCompressedSize());\n+    IUnCompressor unCompressor = IUnCompressor.getUnCompressor(type);\n+    ByteBuffer uncompressedBuffer = ByteBuffer.allocate(header.getUncompressedSize());\n+    if (type == CompressionType.UNCOMPRESSED) {\n+      return buffer;\n+    }\n+    unCompressor.uncompress(buffer.array(), buffer.position(), buffer.remaining(),\n+        uncompressedBuffer.array(),\n+        0);\n+    return uncompressedBuffer;\n+  }\n+  \n+  public ByteBuffer readCompressedPage(PageHeader header) throws IOException {\n+    return readData(-1, header.getCompressedSize());\n+  }\n+\n+  public long position() throws IOException {\n+    return tsFileInput.position();\n+  }\n+\n+  public void position(long offset) throws IOException {\n+    tsFileInput.position(offset);\n+  }\n+\n+  /**\n+   * read one byte from the input. <br> this method is not thread safe\n+   */\n+  public byte readMarker() throws IOException {\n+    markerBuffer.clear();\n+    if (ReadWriteIOUtils.readAsPossible(tsFileInput, markerBuffer) == 0) {\n+      throw new IOException(\"reach the end of the file.\");\n+    }\n+    markerBuffer.flip();\n+    return markerBuffer.get();\n+  }\n+\n+  public byte readMarker(long position) throws IOException {\n+    return readData(position, Byte.BYTES).get();\n+  }\n+\n+  public void close() throws IOException {\n+    this.tsFileInput.close();\n+  }\n+\n+  public String getFileName() {\n+    return this.file;\n+  }\n+\n+  /**\n+   * read data from tsFileInput, from the current position (if position = -1), or the given\n+   * position. <br> if position = -1, the tsFileInput's position will be changed to the current\n+   * position + real data size that been read. Other wise, the tsFileInput's position is not\n+   * changed.\n+   *\n+   * @param position the start position of data in the tsFileInput, or the current position if\n+   * position = -1\n+   * @param size the size of data that want to read\n+   * @return data that been read.\n+   */\n+  private ByteBuffer readData(long position, int size) throws IOException {\n+    ByteBuffer buffer = ByteBuffer.allocate(size);\n+    if (position == -1) {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    } else {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer, position, size) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    }\n+    buffer.flip();\n+    return buffer;\n+  }\n+\n+  /**\n+   * upgrade file and resource, return the boolean value whether upgrade task completes\n+   * @throws IOException, WriteProcessException \n+   */\n+  public boolean upgradeFile(List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    File oldTsFile = FSFactoryProducer.getFSFactory().getFile(this.file);\n+\n+    // check if the old TsFile has correct header \n+    if (!fileCheck(oldTsFile)) {\n+      return false;\n+    }\n+\n+    // ChunkGroupOffset -> version\n+    Map<Long, Long> oldVersionInfo = new HashMap<>();\n+\n+    // ChunkGroupOffset -> time partition, record the offsets of chunk group that data are in same partition\n+    Map<Long, Long> chunkGroupTimePartitionInfo = new HashMap<>();\n+\n+    // scan metadata to get version Info and chunkGroupTimePartitionInfo\n+    scanMetadata(oldVersionInfo, chunkGroupTimePartitionInfo);\n+    \n+    // start to scan chunks and chunkGroups\n+    long startOffsetOfChunkGroup = 0;\n+    boolean newChunkGroup = true;\n+    long versionOfChunkGroup = 0;\n+    boolean chunkGroupInSamePartition = false;\n+    List<ChunkGroupMetadata> newMetaData = new ArrayList<>();\n+    List<List<PageHeader>> pageHeadersInChunkGroup = new ArrayList<>();\n+    List<List<ByteBuffer>> dataInChunkGroup = new ArrayList<>();\n+    byte marker;\n+    List<MeasurementSchema> measurementSchemaList = new ArrayList<>();\n+    try {\n+      while ((marker = this.readMarker()) != MetaMarker.SEPARATOR) {\n+        switch (marker) {\n+          case MetaMarker.CHUNK_HEADER:\n+            // this is the first chunk of a new ChunkGroup.\n+            if (newChunkGroup) {\n+              newChunkGroup = false;\n+              startOffsetOfChunkGroup = this.position() - 1;\n+              versionOfChunkGroup = oldVersionInfo.get(startOffsetOfChunkGroup);\n+              chunkGroupInSamePartition = chunkGroupTimePartitionInfo\n+                  .containsKey(startOffsetOfChunkGroup);\n+            }\n+            ChunkHeader header = this.readChunkHeader();\n+            MeasurementSchema measurementSchema = new MeasurementSchema(header.getMeasurementID(),\n+                header.getDataType(),\n+                header.getEncodingType(), \n+                header.getCompressionType());\n+            measurementSchemaList.add(measurementSchema);\n+            List<PageHeader> pageHeadersInChunk = new ArrayList<>();\n+            List<ByteBuffer> dataInChunk = new ArrayList<>();\n+            for (int j = 0; j < header.getNumOfPages(); j++) {\n+              PageHeader pageHeader = readPageHeader(header.getDataType());\n+              ByteBuffer pageData = chunkGroupInSamePartition ? \n+                  readCompressedPage(pageHeader) : readPage(pageHeader, header.getCompressionType());\n+              pageHeadersInChunk.add(pageHeader);\n+              dataInChunk.add(pageData);\n+            }\n+            pageHeadersInChunkGroup.add(pageHeadersInChunk);\n+            dataInChunkGroup.add(dataInChunk);\n+            break;\n+          case MetaMarker.CHUNK_GROUP_FOOTER:\n+            // this is the footer of a ChunkGroup.\n+            ChunkGroupFooter chunkGroupFooter = this.readChunkGroupFooter();\n+            String deviceID = chunkGroupFooter.getDeviceID();\n+            if (chunkGroupInSamePartition) {\n+              quickRewrite(oldTsFile, deviceID, measurementSchemaList, pageHeadersInChunkGroup,\n+                  dataInChunkGroup, versionOfChunkGroup, chunkGroupTimePartitionInfo.get(startOffsetOfChunkGroup));\n+            } else {\n+              rewrite(oldTsFile, deviceID, measurementSchemaList, \n+                dataInChunkGroup, versionOfChunkGroup);\n+            }\n+\n+            pageHeadersInChunkGroup.clear();\n+            dataInChunkGroup.clear();\n+            measurementSchemaList.clear();\n+            newChunkGroup = true;\n+            break;\n+\n+          default:\n+            // the disk file is corrupted, using this file may be dangerous\n+            logger.error(\"Unrecognized marker detected, this file may be corrupted\");\n+            return false;\n+        }\n+      }\n+      // close upgraded tsFiles and generate resources for them\n+      for (TsFileIOWriter tsFileIOWriter : partitionWriterMap.values()) {\n+        upgradedResources.add(endFileAndGenerateResource(tsFileIOWriter));\n+      }\n+      return true;\n+    } catch (IOException e2) {\n+      logger.info(\"TsFile upgrade process cannot proceed at position {} after {} chunk groups \"\n+          + \"recovered, because : {}\", this.position(), newMetaData.size(), e2.getMessage());\n+      return false;\n+    } finally {\n+      if (tsFileInput != null) {\n+        tsFileInput.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   *  Rewrite the chunk group to new TsFile.\n+   *  If data of this chunk group are in different time partitions,\n+   *  create multiple new TsFiles and rewrite data in each partition.\n+   */\n+  private void rewrite(File oldTsFile, String deviceId, List<MeasurementSchema> schemas, \n+      List<List<ByteBuffer>> dataInChunkGroup, long versionOfChunkGroup) \n+          throws IOException {\n+\n+    Map<Long, Map<MeasurementSchema, IChunkWriter>> chunkWritersInChunkGroup = new HashMap<>();\n+    for (int i = 0; i < schemas.size(); i++) {\n+      MeasurementSchema schema = schemas.get(i);\n+      Decoder defaultTimeDecoder = Decoder.getDecoderByType(\n+          TSEncoding.valueOf(TSFileDescriptor.getInstance().getConfig().getTimeEncoder()),\n+          TSDataType.INT64);\n+      Decoder valueDecoder = Decoder\n+          .getDecoderByType(schema.getEncodingType(), schema.getType());\n+      List<ByteBuffer> dataInChunk = dataInChunkGroup.get(i);\n+      for (ByteBuffer pageData : dataInChunk) {\n+        valueDecoder.reset();\n+        PageReader pageReader = new PageReader(pageData, schema.getType(), valueDecoder,\n+            defaultTimeDecoder, null);\n+        BatchData batchData = pageReader.getAllSatisfiedPageData();\n+        while (batchData.hasCurrent()) {\n+          long time = batchData.currentTime();\n+          Object value = batchData.currentValue();\n+          long partition = StorageEngine.getTimePartition(time);\n+          \n+          Map<MeasurementSchema, IChunkWriter> chunkWriters = chunkWritersInChunkGroup.getOrDefault(partition, new HashMap<>());\n+          IChunkWriter chunkWriter = chunkWriters.getOrDefault(schema, new ChunkWriterImpl(schema));\n+          TsFileIOWriter tsFileIOWriter = getOrDefaultTsFileIOWriter(oldTsFile, partition);\n+          partitionWriterMap.put(partition, tsFileIOWriter);\n+          switch (schema.getType()) {\n+            case INT32:\n+              chunkWriter.write(time, (int) value);\n+              break;\n+            case INT64:\n+              chunkWriter.write(time, (long) value);\n+              break;\n+            case FLOAT:\n+              chunkWriter.write(time, (float) value);\n+              break;\n+            case DOUBLE:\n+              chunkWriter.write(time, (double) value);\n+              break;\n+            case BOOLEAN:\n+              chunkWriter.write(time, (boolean) value);\n+              break;\n+            case TEXT:\n+              chunkWriter.write(time, (Binary) value);\n+              break;\n+            default:\n+              throw new UnSupportedDataTypeException(\n+                  String.format(\"Data type %s is not supported.\", schema.getType()));\n+            }\n+          chunkWriters.put(schema, chunkWriter);\n+          chunkWritersInChunkGroup.put(partition, chunkWriters);\n+          batchData.next();\n+        }\n+      }\n+    }\n+    // set version info to each upgraded tsFile \n+    for (Entry<Long, Map<MeasurementSchema, IChunkWriter>> entry : chunkWritersInChunkGroup.entrySet()) {\n+      long partition = entry.getKey();\n+      TsFileIOWriter tsFileIOWriter = partitionWriterMap.get(partition);\n+      tsFileIOWriter.startChunkGroup(deviceId);\n+      for (IChunkWriter chunkWriter : entry.getValue().values()) {\n+        chunkWriter.writeToFileWriter(tsFileIOWriter);\n+      }\n+      tsFileIOWriter.endChunkGroup();\n+      tsFileIOWriter.writeVersion(versionOfChunkGroup);\n+    }\n+  }\n+\n+  /**\n+   * \n+   * @param oldTsFile\n+   * @param deviceId\n+   * @param schemas\n+   * @param pageHeadersInChunkGroup\n+   * @param dataInChunkGroup\n+   * @param versionOfChunkGroup\n+   * @param partition\n+   * @throws IOException\n+   * @throws PageException\n+   */\n+  private void quickRewrite(File oldTsFile, String deviceId, List<MeasurementSchema> schemas, \n+      List<List<PageHeader>> pageHeadersInChunkGroup, List<List<ByteBuffer>> dataInChunkGroup, \n+      long versionOfChunkGroup, long partition) throws IOException, PageException {\n+    TsFileIOWriter tsFileIOWriter = getOrDefaultTsFileIOWriter(oldTsFile, partition);\n+    tsFileIOWriter.startChunkGroup(deviceId);\n+    for (int i = 0; i < schemas.size(); i++) {\n+      ChunkWriterImpl chunkWriter = new ChunkWriterImpl(schemas.get(i));\n+      List<PageHeader> pageHeaderList = pageHeadersInChunkGroup.get(i);\n+      List<ByteBuffer> pageList = dataInChunkGroup.get(i);\n+      for (int j = 0; j < pageHeaderList.size(); j++) {\n+        chunkWriter.writePageHeaderAndDataIntoBuff(pageList.get(j), pageHeaderList.get(j));\n+      }\n+      chunkWriter.writeToFileWriter(tsFileIOWriter);\n+    }\n+    tsFileIOWriter.endChunkGroup();\n+    tsFileIOWriter.writeVersion(versionOfChunkGroup);\n+  }\n+\n+  private TsFileIOWriter getOrDefaultTsFileIOWriter(File oldTsFile, long partition) {\n+    return partitionWriterMap.computeIfAbsent(partition, k -> \n+      {\n+        File partitionDir = FSFactoryProducer.getFSFactory().getFile(oldTsFile.getParent()\n+            + File.separator + partition);\n+        if (!partitionDir.exists()) {\n+          partitionDir.mkdirs();\n+        }\n+        File newFile = FSFactoryProducer.getFSFactory().getFile(oldTsFile.getParent()\n+            + File.separator + partition + File.separator+ oldTsFile.getName());\n+        try {\n+          if (!newFile.createNewFile()) {\n+            logger.error(\"The TsFile {} has been created \", newFile);\n+            return null;\n+          }\n+          return new TsFileIOWriter(newFile);\n+        } catch (IOException e) {\n+          logger.error(\"Create new TsFile {} failed \", newFile);\n+          return null;\n+        }\n+      }\n+    );\n+  }\n+\n+  /**\n+   *  check if the file to be upgraded has correct magic strings and version number\n+   *  @param oldTsFile\n+   *  @throws IOException \n+   */\n+  private boolean fileCheck(File oldTsFile) throws IOException {\n+    long fileSize;\n+    if (!oldTsFile.exists()) {\n+      logger.error(\"the file to be updated does not exist, file path: {}\", oldTsFile.getPath());\n+      return false;\n+    } else {\n+      fileSize = oldTsFile.length();\n+    }\n+\n+    String magic = readHeadMagic(true);\n+    if (!magic.equals(TSFileConfig.MAGIC_STRING)) {\n+      logger.error(\"the file's MAGIC STRING is incorrect, file path: {}\", oldTsFile.getPath());\n+      return false;\n+    }\n+    \n+    String versionNumber = readVersionNumber();\n+    if (!versionNumber.equals(TSFileConfig.OLD_VERSION)) {\n+      logger.error(\"the file's Version Number is incorrect, file path: {}\", oldTsFile.getPath());\n+      return false;\n+    }\n+\n+    if (fileSize == TSFileConfig.MAGIC_STRING.length()) {\n+      logger.error(\"the file only contains magic string, file path: {}\", oldTsFile.getPath());\n+      return false;\n+    } else if (!readTailMagic().equals(TSFileConfig.MAGIC_STRING)) {\n+      logger.error(\"the file cannot upgrade, file path: {}\", oldTsFile.getPath());\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  private void scanMetadata(Map<Long, Long> oldVersionInfo, \n+      Map<Long, Long> chunkGroupTimePartitionInfo) throws IOException {\n+    OldTsFileMetadata fileMetadata = readFileMetadata();\n+    List<OldTsDeviceMetadata> oldDeviceMetadataList = new ArrayList<>();\n+    for (OldTsDeviceMetadataIndex index : fileMetadata.getDeviceMap().values()) {\n+      OldTsDeviceMetadata oldDeviceMetadata = readTsDeviceMetaData(index);\n+      oldDeviceMetadataList.add(oldDeviceMetadata);\n+    }\n+\n+    for (OldTsDeviceMetadata oldTsDeviceMetadata : oldDeviceMetadataList) {\n+      for (OldChunkGroupMetaData oldChunkGroupMetadata : oldTsDeviceMetadata\n+          .getChunkGroupMetaDataList()) {\n+        long version = oldChunkGroupMetadata.getVersion();\n+        long offsetOfChunkGroupMetaData = oldChunkGroupMetadata.getStartOffsetOfChunkGroup();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 583}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY1NjM5Mw==", "bodyText": "Fixed", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422656393", "createdAt": "2020-05-10T14:56:16Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "diffHunk": "@@ -0,0 +1,624 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import org.apache.iotdb.db.engine.StorageEngine;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.common.conf.TSFileDescriptor;\n+import org.apache.iotdb.tsfile.compress.IUnCompressor;\n+import org.apache.iotdb.tsfile.encoding.decoder.Decoder;\n+import org.apache.iotdb.tsfile.exception.write.PageException;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+import org.apache.iotdb.tsfile.file.MetaMarker;\n+import org.apache.iotdb.tsfile.file.footer.ChunkGroupFooter;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkGroupMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.CompressionType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSEncoding;\n+import org.apache.iotdb.tsfile.fileSystem.FSFactoryProducer;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.reader.LocalTsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.page.PageReader;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.write.chunk.ChunkWriterImpl;\n+import org.apache.iotdb.tsfile.write.chunk.IChunkWriter;\n+import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n+import org.apache.iotdb.tsfile.write.writer.TsFileIOWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+public class TsfileOnlineUpgradeTool implements AutoCloseable {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(TsfileOnlineUpgradeTool.class);\n+\n+  private TsFileInput tsFileInput;\n+  private long fileMetadataPos;\n+  private int fileMetadataSize;\n+  private ByteBuffer markerBuffer = ByteBuffer.allocate(Byte.BYTES);\n+  protected String file;\n+  \n+  // PartitionId -> TsFileIOWriter \n+  private Map<Long, TsFileIOWriter> partitionWriterMap;\n+\n+  /**\n+   * Create a file reader of the given file. The reader will read the tail of the file to get the\n+   * file metadata size.Then the reader will skip the first TSFileConfig.OLD_MAGIC_STRING.length()\n+   * bytes of the file for preparing reading real data.\n+   *\n+   * @param file the data file\n+   * @throws IOException If some I/O error occurs\n+   */\n+  public TsfileOnlineUpgradeTool(String file) throws IOException {\n+    this(file, true);\n+  }\n+\n+  /**\n+   * construct function for TsfileOnlineUpgradeTool.\n+   *\n+   * @param file -given file name\n+   * @param loadMetadataSize -load meta data size\n+   */\n+  public TsfileOnlineUpgradeTool(String file, boolean loadMetadataSize) throws IOException {\n+    this.file = file;\n+    final java.nio.file.Path path = Paths.get(file);\n+    tsFileInput = new LocalTsFileInput(path);\n+    partitionWriterMap = new HashMap<>();\n+    try {\n+      if (loadMetadataSize) {\n+        loadMetadataSize();\n+      }\n+    } catch (Exception e) {\n+      tsFileInput.close();\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * \n+   */\n+  public void loadMetadataSize() throws IOException {\n+    ByteBuffer metadataSize = ByteBuffer.allocate(Integer.BYTES);\n+    tsFileInput.read(metadataSize,\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES);\n+    metadataSize.flip();\n+    // read file metadata size and position\n+    fileMetadataSize = ReadWriteIOUtils.readInt(metadataSize);\n+    fileMetadataPos =\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES\n+            - fileMetadataSize;\n+    // skip the magic header\n+    position(TSFileConfig.MAGIC_STRING.length());\n+  }\n+\n+  public String readTailMagic() throws IOException {\n+    long totalSize = tsFileInput.size();\n+\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    tsFileInput.read(magicStringBytes, totalSize - TSFileConfig.MAGIC_STRING.length());\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * whether the file is a complete TsFile: only if the head magic and tail magic string exists.\n+   */\n+  public boolean isComplete() throws IOException {\n+    return tsFileInput.size() >= TSFileConfig.MAGIC_STRING.length() * 2 && readTailMagic()\n+        .equals(readHeadMagic());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public String readHeadMagic() throws IOException {\n+    return readHeadMagic(false);\n+  }\n+\n+  /**\n+   * @param movePosition whether move the position of the file reader after reading the magic header\n+   * to the end of the magic head string.\n+   */\n+  public String readHeadMagic(boolean movePosition) throws IOException {\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    if (movePosition) {\n+      tsFileInput.position(0);\n+      tsFileInput.read(magicStringBytes);\n+    } else {\n+      tsFileInput.read(magicStringBytes, 0);\n+    }\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * this function reads version number and checks compatibility of TsFile.\n+   */\n+  public String readVersionNumber() throws IOException {\n+    ByteBuffer versionNumberBytes = ByteBuffer\n+        .allocate(TSFileConfig.VERSION_NUMBER.getBytes().length);\n+    tsFileInput.position(TSFileConfig.MAGIC_STRING.getBytes().length);\n+    tsFileInput.read(versionNumberBytes);\n+    versionNumberBytes.flip();\n+    return new String(versionNumberBytes.array());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsFileMetadata readFileMetadata() throws IOException {\n+    return OldTsFileMetadata.deserializeFrom(readData(fileMetadataPos, fileMetadataSize));\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsDeviceMetadata readTsDeviceMetaData(OldTsDeviceMetadataIndex index) throws IOException {\n+    return OldTsDeviceMetadata.deserializeFrom(readData(index.getOffset(), index.getLen()));\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_GROUP_FOOTER. <br>\n+   * This method is not threadsafe.\n+   *\n+   * @return a CHUNK_GROUP_FOOTER\n+   * @throws IOException io error\n+   */\n+  public ChunkGroupFooter readChunkGroupFooter() throws IOException {\n+    return ChunkGroupFooter.deserializeFrom(tsFileInput.wrapAsInputStream(), true);\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_HEADER. <br> This\n+   * method is not threadsafe.\n+   *\n+   * @return a CHUNK_HEADER\n+   * @throws IOException io error\n+   */\n+  public ChunkHeader readChunkHeader() throws IOException {\n+    return ChunkHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), true, true);\n+  }\n+\n+  /**\n+   * not thread safe.\n+   *\n+   * @param type given tsfile data type\n+   */\n+  public PageHeader readPageHeader(TSDataType type) throws IOException {\n+    return PageHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), type, true);\n+  }\n+\n+  public ByteBuffer readPage(PageHeader header, CompressionType type)\n+      throws IOException {\n+    ByteBuffer buffer = readData(-1, header.getCompressedSize());\n+    IUnCompressor unCompressor = IUnCompressor.getUnCompressor(type);\n+    ByteBuffer uncompressedBuffer = ByteBuffer.allocate(header.getUncompressedSize());\n+    if (type == CompressionType.UNCOMPRESSED) {\n+      return buffer;\n+    }\n+    unCompressor.uncompress(buffer.array(), buffer.position(), buffer.remaining(),\n+        uncompressedBuffer.array(),\n+        0);\n+    return uncompressedBuffer;\n+  }\n+  \n+  public ByteBuffer readCompressedPage(PageHeader header) throws IOException {\n+    return readData(-1, header.getCompressedSize());\n+  }\n+\n+  public long position() throws IOException {\n+    return tsFileInput.position();\n+  }\n+\n+  public void position(long offset) throws IOException {\n+    tsFileInput.position(offset);\n+  }\n+\n+  /**\n+   * read one byte from the input. <br> this method is not thread safe\n+   */\n+  public byte readMarker() throws IOException {\n+    markerBuffer.clear();\n+    if (ReadWriteIOUtils.readAsPossible(tsFileInput, markerBuffer) == 0) {\n+      throw new IOException(\"reach the end of the file.\");\n+    }\n+    markerBuffer.flip();\n+    return markerBuffer.get();\n+  }\n+\n+  public byte readMarker(long position) throws IOException {\n+    return readData(position, Byte.BYTES).get();\n+  }\n+\n+  public void close() throws IOException {\n+    this.tsFileInput.close();\n+  }\n+\n+  public String getFileName() {\n+    return this.file;\n+  }\n+\n+  /**\n+   * read data from tsFileInput, from the current position (if position = -1), or the given\n+   * position. <br> if position = -1, the tsFileInput's position will be changed to the current\n+   * position + real data size that been read. Other wise, the tsFileInput's position is not\n+   * changed.\n+   *\n+   * @param position the start position of data in the tsFileInput, or the current position if\n+   * position = -1\n+   * @param size the size of data that want to read\n+   * @return data that been read.\n+   */\n+  private ByteBuffer readData(long position, int size) throws IOException {\n+    ByteBuffer buffer = ByteBuffer.allocate(size);\n+    if (position == -1) {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    } else {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer, position, size) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    }\n+    buffer.flip();\n+    return buffer;\n+  }\n+\n+  /**\n+   * upgrade file and resource, return the boolean value whether upgrade task completes\n+   * @throws IOException, WriteProcessException \n+   */\n+  public boolean upgradeFile(List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    File oldTsFile = FSFactoryProducer.getFSFactory().getFile(this.file);\n+\n+    // check if the old TsFile has correct header \n+    if (!fileCheck(oldTsFile)) {\n+      return false;\n+    }\n+\n+    // ChunkGroupOffset -> version\n+    Map<Long, Long> oldVersionInfo = new HashMap<>();\n+\n+    // ChunkGroupOffset -> time partition, record the offsets of chunk group that data are in same partition\n+    Map<Long, Long> chunkGroupTimePartitionInfo = new HashMap<>();\n+\n+    // scan metadata to get version Info and chunkGroupTimePartitionInfo\n+    scanMetadata(oldVersionInfo, chunkGroupTimePartitionInfo);\n+    \n+    // start to scan chunks and chunkGroups\n+    long startOffsetOfChunkGroup = 0;\n+    boolean newChunkGroup = true;\n+    long versionOfChunkGroup = 0;\n+    boolean chunkGroupInSamePartition = false;\n+    List<ChunkGroupMetadata> newMetaData = new ArrayList<>();\n+    List<List<PageHeader>> pageHeadersInChunkGroup = new ArrayList<>();\n+    List<List<ByteBuffer>> dataInChunkGroup = new ArrayList<>();\n+    byte marker;\n+    List<MeasurementSchema> measurementSchemaList = new ArrayList<>();\n+    try {\n+      while ((marker = this.readMarker()) != MetaMarker.SEPARATOR) {\n+        switch (marker) {\n+          case MetaMarker.CHUNK_HEADER:\n+            // this is the first chunk of a new ChunkGroup.\n+            if (newChunkGroup) {\n+              newChunkGroup = false;\n+              startOffsetOfChunkGroup = this.position() - 1;\n+              versionOfChunkGroup = oldVersionInfo.get(startOffsetOfChunkGroup);\n+              chunkGroupInSamePartition = chunkGroupTimePartitionInfo\n+                  .containsKey(startOffsetOfChunkGroup);\n+            }\n+            ChunkHeader header = this.readChunkHeader();\n+            MeasurementSchema measurementSchema = new MeasurementSchema(header.getMeasurementID(),\n+                header.getDataType(),\n+                header.getEncodingType(), \n+                header.getCompressionType());\n+            measurementSchemaList.add(measurementSchema);\n+            List<PageHeader> pageHeadersInChunk = new ArrayList<>();\n+            List<ByteBuffer> dataInChunk = new ArrayList<>();\n+            for (int j = 0; j < header.getNumOfPages(); j++) {\n+              PageHeader pageHeader = readPageHeader(header.getDataType());\n+              ByteBuffer pageData = chunkGroupInSamePartition ? \n+                  readCompressedPage(pageHeader) : readPage(pageHeader, header.getCompressionType());\n+              pageHeadersInChunk.add(pageHeader);\n+              dataInChunk.add(pageData);\n+            }\n+            pageHeadersInChunkGroup.add(pageHeadersInChunk);\n+            dataInChunkGroup.add(dataInChunk);\n+            break;\n+          case MetaMarker.CHUNK_GROUP_FOOTER:\n+            // this is the footer of a ChunkGroup.\n+            ChunkGroupFooter chunkGroupFooter = this.readChunkGroupFooter();\n+            String deviceID = chunkGroupFooter.getDeviceID();\n+            if (chunkGroupInSamePartition) {\n+              quickRewrite(oldTsFile, deviceID, measurementSchemaList, pageHeadersInChunkGroup,\n+                  dataInChunkGroup, versionOfChunkGroup, chunkGroupTimePartitionInfo.get(startOffsetOfChunkGroup));\n+            } else {\n+              rewrite(oldTsFile, deviceID, measurementSchemaList, \n+                dataInChunkGroup, versionOfChunkGroup);\n+            }\n+\n+            pageHeadersInChunkGroup.clear();\n+            dataInChunkGroup.clear();\n+            measurementSchemaList.clear();\n+            newChunkGroup = true;\n+            break;\n+\n+          default:\n+            // the disk file is corrupted, using this file may be dangerous\n+            logger.error(\"Unrecognized marker detected, this file may be corrupted\");\n+            return false;\n+        }\n+      }\n+      // close upgraded tsFiles and generate resources for them\n+      for (TsFileIOWriter tsFileIOWriter : partitionWriterMap.values()) {\n+        upgradedResources.add(endFileAndGenerateResource(tsFileIOWriter));\n+      }\n+      return true;\n+    } catch (IOException e2) {\n+      logger.info(\"TsFile upgrade process cannot proceed at position {} after {} chunk groups \"\n+          + \"recovered, because : {}\", this.position(), newMetaData.size(), e2.getMessage());\n+      return false;\n+    } finally {\n+      if (tsFileInput != null) {\n+        tsFileInput.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   *  Rewrite the chunk group to new TsFile.\n+   *  If data of this chunk group are in different time partitions,\n+   *  create multiple new TsFiles and rewrite data in each partition.\n+   */\n+  private void rewrite(File oldTsFile, String deviceId, List<MeasurementSchema> schemas, \n+      List<List<ByteBuffer>> dataInChunkGroup, long versionOfChunkGroup) \n+          throws IOException {\n+\n+    Map<Long, Map<MeasurementSchema, IChunkWriter>> chunkWritersInChunkGroup = new HashMap<>();\n+    for (int i = 0; i < schemas.size(); i++) {\n+      MeasurementSchema schema = schemas.get(i);\n+      Decoder defaultTimeDecoder = Decoder.getDecoderByType(\n+          TSEncoding.valueOf(TSFileDescriptor.getInstance().getConfig().getTimeEncoder()),\n+          TSDataType.INT64);\n+      Decoder valueDecoder = Decoder\n+          .getDecoderByType(schema.getEncodingType(), schema.getType());\n+      List<ByteBuffer> dataInChunk = dataInChunkGroup.get(i);\n+      for (ByteBuffer pageData : dataInChunk) {\n+        valueDecoder.reset();\n+        PageReader pageReader = new PageReader(pageData, schema.getType(), valueDecoder,\n+            defaultTimeDecoder, null);\n+        BatchData batchData = pageReader.getAllSatisfiedPageData();\n+        while (batchData.hasCurrent()) {\n+          long time = batchData.currentTime();\n+          Object value = batchData.currentValue();\n+          long partition = StorageEngine.getTimePartition(time);\n+          \n+          Map<MeasurementSchema, IChunkWriter> chunkWriters = chunkWritersInChunkGroup.getOrDefault(partition, new HashMap<>());\n+          IChunkWriter chunkWriter = chunkWriters.getOrDefault(schema, new ChunkWriterImpl(schema));\n+          TsFileIOWriter tsFileIOWriter = getOrDefaultTsFileIOWriter(oldTsFile, partition);\n+          partitionWriterMap.put(partition, tsFileIOWriter);\n+          switch (schema.getType()) {\n+            case INT32:\n+              chunkWriter.write(time, (int) value);\n+              break;\n+            case INT64:\n+              chunkWriter.write(time, (long) value);\n+              break;\n+            case FLOAT:\n+              chunkWriter.write(time, (float) value);\n+              break;\n+            case DOUBLE:\n+              chunkWriter.write(time, (double) value);\n+              break;\n+            case BOOLEAN:\n+              chunkWriter.write(time, (boolean) value);\n+              break;\n+            case TEXT:\n+              chunkWriter.write(time, (Binary) value);\n+              break;\n+            default:\n+              throw new UnSupportedDataTypeException(\n+                  String.format(\"Data type %s is not supported.\", schema.getType()));\n+            }\n+          chunkWriters.put(schema, chunkWriter);\n+          chunkWritersInChunkGroup.put(partition, chunkWriters);\n+          batchData.next();\n+        }\n+      }\n+    }\n+    // set version info to each upgraded tsFile \n+    for (Entry<Long, Map<MeasurementSchema, IChunkWriter>> entry : chunkWritersInChunkGroup.entrySet()) {\n+      long partition = entry.getKey();\n+      TsFileIOWriter tsFileIOWriter = partitionWriterMap.get(partition);\n+      tsFileIOWriter.startChunkGroup(deviceId);\n+      for (IChunkWriter chunkWriter : entry.getValue().values()) {\n+        chunkWriter.writeToFileWriter(tsFileIOWriter);\n+      }\n+      tsFileIOWriter.endChunkGroup();\n+      tsFileIOWriter.writeVersion(versionOfChunkGroup);\n+    }\n+  }\n+\n+  /**\n+   * \n+   * @param oldTsFile\n+   * @param deviceId\n+   * @param schemas\n+   * @param pageHeadersInChunkGroup\n+   * @param dataInChunkGroup\n+   * @param versionOfChunkGroup\n+   * @param partition\n+   * @throws IOException\n+   * @throws PageException\n+   */\n+  private void quickRewrite(File oldTsFile, String deviceId, List<MeasurementSchema> schemas, \n+      List<List<PageHeader>> pageHeadersInChunkGroup, List<List<ByteBuffer>> dataInChunkGroup, \n+      long versionOfChunkGroup, long partition) throws IOException, PageException {\n+    TsFileIOWriter tsFileIOWriter = getOrDefaultTsFileIOWriter(oldTsFile, partition);\n+    tsFileIOWriter.startChunkGroup(deviceId);\n+    for (int i = 0; i < schemas.size(); i++) {\n+      ChunkWriterImpl chunkWriter = new ChunkWriterImpl(schemas.get(i));\n+      List<PageHeader> pageHeaderList = pageHeadersInChunkGroup.get(i);\n+      List<ByteBuffer> pageList = dataInChunkGroup.get(i);\n+      for (int j = 0; j < pageHeaderList.size(); j++) {\n+        chunkWriter.writePageHeaderAndDataIntoBuff(pageList.get(j), pageHeaderList.get(j));\n+      }\n+      chunkWriter.writeToFileWriter(tsFileIOWriter);\n+    }\n+    tsFileIOWriter.endChunkGroup();\n+    tsFileIOWriter.writeVersion(versionOfChunkGroup);\n+  }\n+\n+  private TsFileIOWriter getOrDefaultTsFileIOWriter(File oldTsFile, long partition) {\n+    return partitionWriterMap.computeIfAbsent(partition, k -> \n+      {\n+        File partitionDir = FSFactoryProducer.getFSFactory().getFile(oldTsFile.getParent()\n+            + File.separator + partition);\n+        if (!partitionDir.exists()) {\n+          partitionDir.mkdirs();\n+        }\n+        File newFile = FSFactoryProducer.getFSFactory().getFile(oldTsFile.getParent()\n+            + File.separator + partition + File.separator+ oldTsFile.getName());\n+        try {\n+          if (!newFile.createNewFile()) {\n+            logger.error(\"The TsFile {} has been created \", newFile);\n+            return null;\n+          }\n+          return new TsFileIOWriter(newFile);\n+        } catch (IOException e) {\n+          logger.error(\"Create new TsFile {} failed \", newFile);\n+          return null;\n+        }\n+      }\n+    );\n+  }\n+\n+  /**\n+   *  check if the file to be upgraded has correct magic strings and version number\n+   *  @param oldTsFile\n+   *  @throws IOException \n+   */\n+  private boolean fileCheck(File oldTsFile) throws IOException {\n+    long fileSize;\n+    if (!oldTsFile.exists()) {\n+      logger.error(\"the file to be updated does not exist, file path: {}\", oldTsFile.getPath());\n+      return false;\n+    } else {\n+      fileSize = oldTsFile.length();\n+    }\n+\n+    String magic = readHeadMagic(true);\n+    if (!magic.equals(TSFileConfig.MAGIC_STRING)) {\n+      logger.error(\"the file's MAGIC STRING is incorrect, file path: {}\", oldTsFile.getPath());\n+      return false;\n+    }\n+    \n+    String versionNumber = readVersionNumber();\n+    if (!versionNumber.equals(TSFileConfig.OLD_VERSION)) {\n+      logger.error(\"the file's Version Number is incorrect, file path: {}\", oldTsFile.getPath());\n+      return false;\n+    }\n+\n+    if (fileSize == TSFileConfig.MAGIC_STRING.length()) {\n+      logger.error(\"the file only contains magic string, file path: {}\", oldTsFile.getPath());\n+      return false;\n+    } else if (!readTailMagic().equals(TSFileConfig.MAGIC_STRING)) {\n+      logger.error(\"the file cannot upgrade, file path: {}\", oldTsFile.getPath());\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  private void scanMetadata(Map<Long, Long> oldVersionInfo, \n+      Map<Long, Long> chunkGroupTimePartitionInfo) throws IOException {\n+    OldTsFileMetadata fileMetadata = readFileMetadata();\n+    List<OldTsDeviceMetadata> oldDeviceMetadataList = new ArrayList<>();\n+    for (OldTsDeviceMetadataIndex index : fileMetadata.getDeviceMap().values()) {\n+      OldTsDeviceMetadata oldDeviceMetadata = readTsDeviceMetaData(index);\n+      oldDeviceMetadataList.add(oldDeviceMetadata);\n+    }\n+\n+    for (OldTsDeviceMetadata oldTsDeviceMetadata : oldDeviceMetadataList) {\n+      for (OldChunkGroupMetaData oldChunkGroupMetadata : oldTsDeviceMetadata\n+          .getChunkGroupMetaDataList()) {\n+        long version = oldChunkGroupMetadata.getVersion();\n+        long offsetOfChunkGroupMetaData = oldChunkGroupMetadata.getStartOffsetOfChunkGroup();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjU5OTQzNw=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 583}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTM4NTM0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNzozMTo1NFrOGTBlFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNTowMjo0OFrOGTE8eQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwMjAwNA==", "bodyText": "add javadoc or rename to rewriteCompressedPage", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422602004", "createdAt": "2020-05-10T07:31:54Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "diffHunk": "@@ -0,0 +1,624 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import org.apache.iotdb.db.engine.StorageEngine;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.common.conf.TSFileDescriptor;\n+import org.apache.iotdb.tsfile.compress.IUnCompressor;\n+import org.apache.iotdb.tsfile.encoding.decoder.Decoder;\n+import org.apache.iotdb.tsfile.exception.write.PageException;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+import org.apache.iotdb.tsfile.file.MetaMarker;\n+import org.apache.iotdb.tsfile.file.footer.ChunkGroupFooter;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkGroupMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.CompressionType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSEncoding;\n+import org.apache.iotdb.tsfile.fileSystem.FSFactoryProducer;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.reader.LocalTsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.page.PageReader;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.write.chunk.ChunkWriterImpl;\n+import org.apache.iotdb.tsfile.write.chunk.IChunkWriter;\n+import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n+import org.apache.iotdb.tsfile.write.writer.TsFileIOWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+public class TsfileOnlineUpgradeTool implements AutoCloseable {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(TsfileOnlineUpgradeTool.class);\n+\n+  private TsFileInput tsFileInput;\n+  private long fileMetadataPos;\n+  private int fileMetadataSize;\n+  private ByteBuffer markerBuffer = ByteBuffer.allocate(Byte.BYTES);\n+  protected String file;\n+  \n+  // PartitionId -> TsFileIOWriter \n+  private Map<Long, TsFileIOWriter> partitionWriterMap;\n+\n+  /**\n+   * Create a file reader of the given file. The reader will read the tail of the file to get the\n+   * file metadata size.Then the reader will skip the first TSFileConfig.OLD_MAGIC_STRING.length()\n+   * bytes of the file for preparing reading real data.\n+   *\n+   * @param file the data file\n+   * @throws IOException If some I/O error occurs\n+   */\n+  public TsfileOnlineUpgradeTool(String file) throws IOException {\n+    this(file, true);\n+  }\n+\n+  /**\n+   * construct function for TsfileOnlineUpgradeTool.\n+   *\n+   * @param file -given file name\n+   * @param loadMetadataSize -load meta data size\n+   */\n+  public TsfileOnlineUpgradeTool(String file, boolean loadMetadataSize) throws IOException {\n+    this.file = file;\n+    final java.nio.file.Path path = Paths.get(file);\n+    tsFileInput = new LocalTsFileInput(path);\n+    partitionWriterMap = new HashMap<>();\n+    try {\n+      if (loadMetadataSize) {\n+        loadMetadataSize();\n+      }\n+    } catch (Exception e) {\n+      tsFileInput.close();\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * \n+   */\n+  public void loadMetadataSize() throws IOException {\n+    ByteBuffer metadataSize = ByteBuffer.allocate(Integer.BYTES);\n+    tsFileInput.read(metadataSize,\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES);\n+    metadataSize.flip();\n+    // read file metadata size and position\n+    fileMetadataSize = ReadWriteIOUtils.readInt(metadataSize);\n+    fileMetadataPos =\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES\n+            - fileMetadataSize;\n+    // skip the magic header\n+    position(TSFileConfig.MAGIC_STRING.length());\n+  }\n+\n+  public String readTailMagic() throws IOException {\n+    long totalSize = tsFileInput.size();\n+\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    tsFileInput.read(magicStringBytes, totalSize - TSFileConfig.MAGIC_STRING.length());\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * whether the file is a complete TsFile: only if the head magic and tail magic string exists.\n+   */\n+  public boolean isComplete() throws IOException {\n+    return tsFileInput.size() >= TSFileConfig.MAGIC_STRING.length() * 2 && readTailMagic()\n+        .equals(readHeadMagic());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public String readHeadMagic() throws IOException {\n+    return readHeadMagic(false);\n+  }\n+\n+  /**\n+   * @param movePosition whether move the position of the file reader after reading the magic header\n+   * to the end of the magic head string.\n+   */\n+  public String readHeadMagic(boolean movePosition) throws IOException {\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    if (movePosition) {\n+      tsFileInput.position(0);\n+      tsFileInput.read(magicStringBytes);\n+    } else {\n+      tsFileInput.read(magicStringBytes, 0);\n+    }\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * this function reads version number and checks compatibility of TsFile.\n+   */\n+  public String readVersionNumber() throws IOException {\n+    ByteBuffer versionNumberBytes = ByteBuffer\n+        .allocate(TSFileConfig.VERSION_NUMBER.getBytes().length);\n+    tsFileInput.position(TSFileConfig.MAGIC_STRING.getBytes().length);\n+    tsFileInput.read(versionNumberBytes);\n+    versionNumberBytes.flip();\n+    return new String(versionNumberBytes.array());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsFileMetadata readFileMetadata() throws IOException {\n+    return OldTsFileMetadata.deserializeFrom(readData(fileMetadataPos, fileMetadataSize));\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsDeviceMetadata readTsDeviceMetaData(OldTsDeviceMetadataIndex index) throws IOException {\n+    return OldTsDeviceMetadata.deserializeFrom(readData(index.getOffset(), index.getLen()));\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_GROUP_FOOTER. <br>\n+   * This method is not threadsafe.\n+   *\n+   * @return a CHUNK_GROUP_FOOTER\n+   * @throws IOException io error\n+   */\n+  public ChunkGroupFooter readChunkGroupFooter() throws IOException {\n+    return ChunkGroupFooter.deserializeFrom(tsFileInput.wrapAsInputStream(), true);\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_HEADER. <br> This\n+   * method is not threadsafe.\n+   *\n+   * @return a CHUNK_HEADER\n+   * @throws IOException io error\n+   */\n+  public ChunkHeader readChunkHeader() throws IOException {\n+    return ChunkHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), true, true);\n+  }\n+\n+  /**\n+   * not thread safe.\n+   *\n+   * @param type given tsfile data type\n+   */\n+  public PageHeader readPageHeader(TSDataType type) throws IOException {\n+    return PageHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), type, true);\n+  }\n+\n+  public ByteBuffer readPage(PageHeader header, CompressionType type)\n+      throws IOException {\n+    ByteBuffer buffer = readData(-1, header.getCompressedSize());\n+    IUnCompressor unCompressor = IUnCompressor.getUnCompressor(type);\n+    ByteBuffer uncompressedBuffer = ByteBuffer.allocate(header.getUncompressedSize());\n+    if (type == CompressionType.UNCOMPRESSED) {\n+      return buffer;\n+    }\n+    unCompressor.uncompress(buffer.array(), buffer.position(), buffer.remaining(),\n+        uncompressedBuffer.array(),\n+        0);\n+    return uncompressedBuffer;\n+  }\n+  \n+  public ByteBuffer readCompressedPage(PageHeader header) throws IOException {\n+    return readData(-1, header.getCompressedSize());\n+  }\n+\n+  public long position() throws IOException {\n+    return tsFileInput.position();\n+  }\n+\n+  public void position(long offset) throws IOException {\n+    tsFileInput.position(offset);\n+  }\n+\n+  /**\n+   * read one byte from the input. <br> this method is not thread safe\n+   */\n+  public byte readMarker() throws IOException {\n+    markerBuffer.clear();\n+    if (ReadWriteIOUtils.readAsPossible(tsFileInput, markerBuffer) == 0) {\n+      throw new IOException(\"reach the end of the file.\");\n+    }\n+    markerBuffer.flip();\n+    return markerBuffer.get();\n+  }\n+\n+  public byte readMarker(long position) throws IOException {\n+    return readData(position, Byte.BYTES).get();\n+  }\n+\n+  public void close() throws IOException {\n+    this.tsFileInput.close();\n+  }\n+\n+  public String getFileName() {\n+    return this.file;\n+  }\n+\n+  /**\n+   * read data from tsFileInput, from the current position (if position = -1), or the given\n+   * position. <br> if position = -1, the tsFileInput's position will be changed to the current\n+   * position + real data size that been read. Other wise, the tsFileInput's position is not\n+   * changed.\n+   *\n+   * @param position the start position of data in the tsFileInput, or the current position if\n+   * position = -1\n+   * @param size the size of data that want to read\n+   * @return data that been read.\n+   */\n+  private ByteBuffer readData(long position, int size) throws IOException {\n+    ByteBuffer buffer = ByteBuffer.allocate(size);\n+    if (position == -1) {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    } else {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer, position, size) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    }\n+    buffer.flip();\n+    return buffer;\n+  }\n+\n+  /**\n+   * upgrade file and resource, return the boolean value whether upgrade task completes\n+   * @throws IOException, WriteProcessException \n+   */\n+  public boolean upgradeFile(List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    File oldTsFile = FSFactoryProducer.getFSFactory().getFile(this.file);\n+\n+    // check if the old TsFile has correct header \n+    if (!fileCheck(oldTsFile)) {\n+      return false;\n+    }\n+\n+    // ChunkGroupOffset -> version\n+    Map<Long, Long> oldVersionInfo = new HashMap<>();\n+\n+    // ChunkGroupOffset -> time partition, record the offsets of chunk group that data are in same partition\n+    Map<Long, Long> chunkGroupTimePartitionInfo = new HashMap<>();\n+\n+    // scan metadata to get version Info and chunkGroupTimePartitionInfo\n+    scanMetadata(oldVersionInfo, chunkGroupTimePartitionInfo);\n+    \n+    // start to scan chunks and chunkGroups\n+    long startOffsetOfChunkGroup = 0;\n+    boolean newChunkGroup = true;\n+    long versionOfChunkGroup = 0;\n+    boolean chunkGroupInSamePartition = false;\n+    List<ChunkGroupMetadata> newMetaData = new ArrayList<>();\n+    List<List<PageHeader>> pageHeadersInChunkGroup = new ArrayList<>();\n+    List<List<ByteBuffer>> dataInChunkGroup = new ArrayList<>();\n+    byte marker;\n+    List<MeasurementSchema> measurementSchemaList = new ArrayList<>();\n+    try {\n+      while ((marker = this.readMarker()) != MetaMarker.SEPARATOR) {\n+        switch (marker) {\n+          case MetaMarker.CHUNK_HEADER:\n+            // this is the first chunk of a new ChunkGroup.\n+            if (newChunkGroup) {\n+              newChunkGroup = false;\n+              startOffsetOfChunkGroup = this.position() - 1;\n+              versionOfChunkGroup = oldVersionInfo.get(startOffsetOfChunkGroup);\n+              chunkGroupInSamePartition = chunkGroupTimePartitionInfo\n+                  .containsKey(startOffsetOfChunkGroup);\n+            }\n+            ChunkHeader header = this.readChunkHeader();\n+            MeasurementSchema measurementSchema = new MeasurementSchema(header.getMeasurementID(),\n+                header.getDataType(),\n+                header.getEncodingType(), \n+                header.getCompressionType());\n+            measurementSchemaList.add(measurementSchema);\n+            List<PageHeader> pageHeadersInChunk = new ArrayList<>();\n+            List<ByteBuffer> dataInChunk = new ArrayList<>();\n+            for (int j = 0; j < header.getNumOfPages(); j++) {\n+              PageHeader pageHeader = readPageHeader(header.getDataType());\n+              ByteBuffer pageData = chunkGroupInSamePartition ? \n+                  readCompressedPage(pageHeader) : readPage(pageHeader, header.getCompressionType());\n+              pageHeadersInChunk.add(pageHeader);\n+              dataInChunk.add(pageData);\n+            }\n+            pageHeadersInChunkGroup.add(pageHeadersInChunk);\n+            dataInChunkGroup.add(dataInChunk);\n+            break;\n+          case MetaMarker.CHUNK_GROUP_FOOTER:\n+            // this is the footer of a ChunkGroup.\n+            ChunkGroupFooter chunkGroupFooter = this.readChunkGroupFooter();\n+            String deviceID = chunkGroupFooter.getDeviceID();\n+            if (chunkGroupInSamePartition) {\n+              quickRewrite(oldTsFile, deviceID, measurementSchemaList, pageHeadersInChunkGroup,\n+                  dataInChunkGroup, versionOfChunkGroup, chunkGroupTimePartitionInfo.get(startOffsetOfChunkGroup));\n+            } else {\n+              rewrite(oldTsFile, deviceID, measurementSchemaList, \n+                dataInChunkGroup, versionOfChunkGroup);\n+            }\n+\n+            pageHeadersInChunkGroup.clear();\n+            dataInChunkGroup.clear();\n+            measurementSchemaList.clear();\n+            newChunkGroup = true;\n+            break;\n+\n+          default:\n+            // the disk file is corrupted, using this file may be dangerous\n+            logger.error(\"Unrecognized marker detected, this file may be corrupted\");\n+            return false;\n+        }\n+      }\n+      // close upgraded tsFiles and generate resources for them\n+      for (TsFileIOWriter tsFileIOWriter : partitionWriterMap.values()) {\n+        upgradedResources.add(endFileAndGenerateResource(tsFileIOWriter));\n+      }\n+      return true;\n+    } catch (IOException e2) {\n+      logger.info(\"TsFile upgrade process cannot proceed at position {} after {} chunk groups \"\n+          + \"recovered, because : {}\", this.position(), newMetaData.size(), e2.getMessage());\n+      return false;\n+    } finally {\n+      if (tsFileInput != null) {\n+        tsFileInput.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   *  Rewrite the chunk group to new TsFile.\n+   *  If data of this chunk group are in different time partitions,\n+   *  create multiple new TsFiles and rewrite data in each partition.\n+   */\n+  private void rewrite(File oldTsFile, String deviceId, List<MeasurementSchema> schemas, \n+      List<List<ByteBuffer>> dataInChunkGroup, long versionOfChunkGroup) \n+          throws IOException {\n+\n+    Map<Long, Map<MeasurementSchema, IChunkWriter>> chunkWritersInChunkGroup = new HashMap<>();\n+    for (int i = 0; i < schemas.size(); i++) {\n+      MeasurementSchema schema = schemas.get(i);\n+      Decoder defaultTimeDecoder = Decoder.getDecoderByType(\n+          TSEncoding.valueOf(TSFileDescriptor.getInstance().getConfig().getTimeEncoder()),\n+          TSDataType.INT64);\n+      Decoder valueDecoder = Decoder\n+          .getDecoderByType(schema.getEncodingType(), schema.getType());\n+      List<ByteBuffer> dataInChunk = dataInChunkGroup.get(i);\n+      for (ByteBuffer pageData : dataInChunk) {\n+        valueDecoder.reset();\n+        PageReader pageReader = new PageReader(pageData, schema.getType(), valueDecoder,\n+            defaultTimeDecoder, null);\n+        BatchData batchData = pageReader.getAllSatisfiedPageData();\n+        while (batchData.hasCurrent()) {\n+          long time = batchData.currentTime();\n+          Object value = batchData.currentValue();\n+          long partition = StorageEngine.getTimePartition(time);\n+          \n+          Map<MeasurementSchema, IChunkWriter> chunkWriters = chunkWritersInChunkGroup.getOrDefault(partition, new HashMap<>());\n+          IChunkWriter chunkWriter = chunkWriters.getOrDefault(schema, new ChunkWriterImpl(schema));\n+          TsFileIOWriter tsFileIOWriter = getOrDefaultTsFileIOWriter(oldTsFile, partition);\n+          partitionWriterMap.put(partition, tsFileIOWriter);\n+          switch (schema.getType()) {\n+            case INT32:\n+              chunkWriter.write(time, (int) value);\n+              break;\n+            case INT64:\n+              chunkWriter.write(time, (long) value);\n+              break;\n+            case FLOAT:\n+              chunkWriter.write(time, (float) value);\n+              break;\n+            case DOUBLE:\n+              chunkWriter.write(time, (double) value);\n+              break;\n+            case BOOLEAN:\n+              chunkWriter.write(time, (boolean) value);\n+              break;\n+            case TEXT:\n+              chunkWriter.write(time, (Binary) value);\n+              break;\n+            default:\n+              throw new UnSupportedDataTypeException(\n+                  String.format(\"Data type %s is not supported.\", schema.getType()));\n+            }\n+          chunkWriters.put(schema, chunkWriter);\n+          chunkWritersInChunkGroup.put(partition, chunkWriters);\n+          batchData.next();\n+        }\n+      }\n+    }\n+    // set version info to each upgraded tsFile \n+    for (Entry<Long, Map<MeasurementSchema, IChunkWriter>> entry : chunkWritersInChunkGroup.entrySet()) {\n+      long partition = entry.getKey();\n+      TsFileIOWriter tsFileIOWriter = partitionWriterMap.get(partition);\n+      tsFileIOWriter.startChunkGroup(deviceId);\n+      for (IChunkWriter chunkWriter : entry.getValue().values()) {\n+        chunkWriter.writeToFileWriter(tsFileIOWriter);\n+      }\n+      tsFileIOWriter.endChunkGroup();\n+      tsFileIOWriter.writeVersion(versionOfChunkGroup);\n+    }\n+  }\n+\n+  /**\n+   * \n+   * @param oldTsFile\n+   * @param deviceId\n+   * @param schemas\n+   * @param pageHeadersInChunkGroup\n+   * @param dataInChunkGroup\n+   * @param versionOfChunkGroup\n+   * @param partition\n+   * @throws IOException\n+   * @throws PageException\n+   */\n+  private void quickRewrite(File oldTsFile, String deviceId, List<MeasurementSchema> schemas, ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 492}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY1NzE0NQ==", "bodyText": "This method is for rewrite the ChunkGroup which is in the same time partition. In this case, we don't need to decode the Chunk data to points, just upgrade the headers of chunks and pages and then write to file. That's why I named it quickRewrite.\nI prefer to add javadoc to explain this.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422657145", "createdAt": "2020-05-10T15:02:48Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "diffHunk": "@@ -0,0 +1,624 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import org.apache.iotdb.db.engine.StorageEngine;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.common.conf.TSFileDescriptor;\n+import org.apache.iotdb.tsfile.compress.IUnCompressor;\n+import org.apache.iotdb.tsfile.encoding.decoder.Decoder;\n+import org.apache.iotdb.tsfile.exception.write.PageException;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+import org.apache.iotdb.tsfile.file.MetaMarker;\n+import org.apache.iotdb.tsfile.file.footer.ChunkGroupFooter;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkGroupMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.CompressionType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSEncoding;\n+import org.apache.iotdb.tsfile.fileSystem.FSFactoryProducer;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.reader.LocalTsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.page.PageReader;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.write.chunk.ChunkWriterImpl;\n+import org.apache.iotdb.tsfile.write.chunk.IChunkWriter;\n+import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n+import org.apache.iotdb.tsfile.write.writer.TsFileIOWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+public class TsfileOnlineUpgradeTool implements AutoCloseable {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(TsfileOnlineUpgradeTool.class);\n+\n+  private TsFileInput tsFileInput;\n+  private long fileMetadataPos;\n+  private int fileMetadataSize;\n+  private ByteBuffer markerBuffer = ByteBuffer.allocate(Byte.BYTES);\n+  protected String file;\n+  \n+  // PartitionId -> TsFileIOWriter \n+  private Map<Long, TsFileIOWriter> partitionWriterMap;\n+\n+  /**\n+   * Create a file reader of the given file. The reader will read the tail of the file to get the\n+   * file metadata size.Then the reader will skip the first TSFileConfig.OLD_MAGIC_STRING.length()\n+   * bytes of the file for preparing reading real data.\n+   *\n+   * @param file the data file\n+   * @throws IOException If some I/O error occurs\n+   */\n+  public TsfileOnlineUpgradeTool(String file) throws IOException {\n+    this(file, true);\n+  }\n+\n+  /**\n+   * construct function for TsfileOnlineUpgradeTool.\n+   *\n+   * @param file -given file name\n+   * @param loadMetadataSize -load meta data size\n+   */\n+  public TsfileOnlineUpgradeTool(String file, boolean loadMetadataSize) throws IOException {\n+    this.file = file;\n+    final java.nio.file.Path path = Paths.get(file);\n+    tsFileInput = new LocalTsFileInput(path);\n+    partitionWriterMap = new HashMap<>();\n+    try {\n+      if (loadMetadataSize) {\n+        loadMetadataSize();\n+      }\n+    } catch (Exception e) {\n+      tsFileInput.close();\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * \n+   */\n+  public void loadMetadataSize() throws IOException {\n+    ByteBuffer metadataSize = ByteBuffer.allocate(Integer.BYTES);\n+    tsFileInput.read(metadataSize,\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES);\n+    metadataSize.flip();\n+    // read file metadata size and position\n+    fileMetadataSize = ReadWriteIOUtils.readInt(metadataSize);\n+    fileMetadataPos =\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES\n+            - fileMetadataSize;\n+    // skip the magic header\n+    position(TSFileConfig.MAGIC_STRING.length());\n+  }\n+\n+  public String readTailMagic() throws IOException {\n+    long totalSize = tsFileInput.size();\n+\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    tsFileInput.read(magicStringBytes, totalSize - TSFileConfig.MAGIC_STRING.length());\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * whether the file is a complete TsFile: only if the head magic and tail magic string exists.\n+   */\n+  public boolean isComplete() throws IOException {\n+    return tsFileInput.size() >= TSFileConfig.MAGIC_STRING.length() * 2 && readTailMagic()\n+        .equals(readHeadMagic());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public String readHeadMagic() throws IOException {\n+    return readHeadMagic(false);\n+  }\n+\n+  /**\n+   * @param movePosition whether move the position of the file reader after reading the magic header\n+   * to the end of the magic head string.\n+   */\n+  public String readHeadMagic(boolean movePosition) throws IOException {\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    if (movePosition) {\n+      tsFileInput.position(0);\n+      tsFileInput.read(magicStringBytes);\n+    } else {\n+      tsFileInput.read(magicStringBytes, 0);\n+    }\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * this function reads version number and checks compatibility of TsFile.\n+   */\n+  public String readVersionNumber() throws IOException {\n+    ByteBuffer versionNumberBytes = ByteBuffer\n+        .allocate(TSFileConfig.VERSION_NUMBER.getBytes().length);\n+    tsFileInput.position(TSFileConfig.MAGIC_STRING.getBytes().length);\n+    tsFileInput.read(versionNumberBytes);\n+    versionNumberBytes.flip();\n+    return new String(versionNumberBytes.array());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsFileMetadata readFileMetadata() throws IOException {\n+    return OldTsFileMetadata.deserializeFrom(readData(fileMetadataPos, fileMetadataSize));\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsDeviceMetadata readTsDeviceMetaData(OldTsDeviceMetadataIndex index) throws IOException {\n+    return OldTsDeviceMetadata.deserializeFrom(readData(index.getOffset(), index.getLen()));\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_GROUP_FOOTER. <br>\n+   * This method is not threadsafe.\n+   *\n+   * @return a CHUNK_GROUP_FOOTER\n+   * @throws IOException io error\n+   */\n+  public ChunkGroupFooter readChunkGroupFooter() throws IOException {\n+    return ChunkGroupFooter.deserializeFrom(tsFileInput.wrapAsInputStream(), true);\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_HEADER. <br> This\n+   * method is not threadsafe.\n+   *\n+   * @return a CHUNK_HEADER\n+   * @throws IOException io error\n+   */\n+  public ChunkHeader readChunkHeader() throws IOException {\n+    return ChunkHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), true, true);\n+  }\n+\n+  /**\n+   * not thread safe.\n+   *\n+   * @param type given tsfile data type\n+   */\n+  public PageHeader readPageHeader(TSDataType type) throws IOException {\n+    return PageHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), type, true);\n+  }\n+\n+  public ByteBuffer readPage(PageHeader header, CompressionType type)\n+      throws IOException {\n+    ByteBuffer buffer = readData(-1, header.getCompressedSize());\n+    IUnCompressor unCompressor = IUnCompressor.getUnCompressor(type);\n+    ByteBuffer uncompressedBuffer = ByteBuffer.allocate(header.getUncompressedSize());\n+    if (type == CompressionType.UNCOMPRESSED) {\n+      return buffer;\n+    }\n+    unCompressor.uncompress(buffer.array(), buffer.position(), buffer.remaining(),\n+        uncompressedBuffer.array(),\n+        0);\n+    return uncompressedBuffer;\n+  }\n+  \n+  public ByteBuffer readCompressedPage(PageHeader header) throws IOException {\n+    return readData(-1, header.getCompressedSize());\n+  }\n+\n+  public long position() throws IOException {\n+    return tsFileInput.position();\n+  }\n+\n+  public void position(long offset) throws IOException {\n+    tsFileInput.position(offset);\n+  }\n+\n+  /**\n+   * read one byte from the input. <br> this method is not thread safe\n+   */\n+  public byte readMarker() throws IOException {\n+    markerBuffer.clear();\n+    if (ReadWriteIOUtils.readAsPossible(tsFileInput, markerBuffer) == 0) {\n+      throw new IOException(\"reach the end of the file.\");\n+    }\n+    markerBuffer.flip();\n+    return markerBuffer.get();\n+  }\n+\n+  public byte readMarker(long position) throws IOException {\n+    return readData(position, Byte.BYTES).get();\n+  }\n+\n+  public void close() throws IOException {\n+    this.tsFileInput.close();\n+  }\n+\n+  public String getFileName() {\n+    return this.file;\n+  }\n+\n+  /**\n+   * read data from tsFileInput, from the current position (if position = -1), or the given\n+   * position. <br> if position = -1, the tsFileInput's position will be changed to the current\n+   * position + real data size that been read. Other wise, the tsFileInput's position is not\n+   * changed.\n+   *\n+   * @param position the start position of data in the tsFileInput, or the current position if\n+   * position = -1\n+   * @param size the size of data that want to read\n+   * @return data that been read.\n+   */\n+  private ByteBuffer readData(long position, int size) throws IOException {\n+    ByteBuffer buffer = ByteBuffer.allocate(size);\n+    if (position == -1) {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    } else {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer, position, size) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    }\n+    buffer.flip();\n+    return buffer;\n+  }\n+\n+  /**\n+   * upgrade file and resource, return the boolean value whether upgrade task completes\n+   * @throws IOException, WriteProcessException \n+   */\n+  public boolean upgradeFile(List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    File oldTsFile = FSFactoryProducer.getFSFactory().getFile(this.file);\n+\n+    // check if the old TsFile has correct header \n+    if (!fileCheck(oldTsFile)) {\n+      return false;\n+    }\n+\n+    // ChunkGroupOffset -> version\n+    Map<Long, Long> oldVersionInfo = new HashMap<>();\n+\n+    // ChunkGroupOffset -> time partition, record the offsets of chunk group that data are in same partition\n+    Map<Long, Long> chunkGroupTimePartitionInfo = new HashMap<>();\n+\n+    // scan metadata to get version Info and chunkGroupTimePartitionInfo\n+    scanMetadata(oldVersionInfo, chunkGroupTimePartitionInfo);\n+    \n+    // start to scan chunks and chunkGroups\n+    long startOffsetOfChunkGroup = 0;\n+    boolean newChunkGroup = true;\n+    long versionOfChunkGroup = 0;\n+    boolean chunkGroupInSamePartition = false;\n+    List<ChunkGroupMetadata> newMetaData = new ArrayList<>();\n+    List<List<PageHeader>> pageHeadersInChunkGroup = new ArrayList<>();\n+    List<List<ByteBuffer>> dataInChunkGroup = new ArrayList<>();\n+    byte marker;\n+    List<MeasurementSchema> measurementSchemaList = new ArrayList<>();\n+    try {\n+      while ((marker = this.readMarker()) != MetaMarker.SEPARATOR) {\n+        switch (marker) {\n+          case MetaMarker.CHUNK_HEADER:\n+            // this is the first chunk of a new ChunkGroup.\n+            if (newChunkGroup) {\n+              newChunkGroup = false;\n+              startOffsetOfChunkGroup = this.position() - 1;\n+              versionOfChunkGroup = oldVersionInfo.get(startOffsetOfChunkGroup);\n+              chunkGroupInSamePartition = chunkGroupTimePartitionInfo\n+                  .containsKey(startOffsetOfChunkGroup);\n+            }\n+            ChunkHeader header = this.readChunkHeader();\n+            MeasurementSchema measurementSchema = new MeasurementSchema(header.getMeasurementID(),\n+                header.getDataType(),\n+                header.getEncodingType(), \n+                header.getCompressionType());\n+            measurementSchemaList.add(measurementSchema);\n+            List<PageHeader> pageHeadersInChunk = new ArrayList<>();\n+            List<ByteBuffer> dataInChunk = new ArrayList<>();\n+            for (int j = 0; j < header.getNumOfPages(); j++) {\n+              PageHeader pageHeader = readPageHeader(header.getDataType());\n+              ByteBuffer pageData = chunkGroupInSamePartition ? \n+                  readCompressedPage(pageHeader) : readPage(pageHeader, header.getCompressionType());\n+              pageHeadersInChunk.add(pageHeader);\n+              dataInChunk.add(pageData);\n+            }\n+            pageHeadersInChunkGroup.add(pageHeadersInChunk);\n+            dataInChunkGroup.add(dataInChunk);\n+            break;\n+          case MetaMarker.CHUNK_GROUP_FOOTER:\n+            // this is the footer of a ChunkGroup.\n+            ChunkGroupFooter chunkGroupFooter = this.readChunkGroupFooter();\n+            String deviceID = chunkGroupFooter.getDeviceID();\n+            if (chunkGroupInSamePartition) {\n+              quickRewrite(oldTsFile, deviceID, measurementSchemaList, pageHeadersInChunkGroup,\n+                  dataInChunkGroup, versionOfChunkGroup, chunkGroupTimePartitionInfo.get(startOffsetOfChunkGroup));\n+            } else {\n+              rewrite(oldTsFile, deviceID, measurementSchemaList, \n+                dataInChunkGroup, versionOfChunkGroup);\n+            }\n+\n+            pageHeadersInChunkGroup.clear();\n+            dataInChunkGroup.clear();\n+            measurementSchemaList.clear();\n+            newChunkGroup = true;\n+            break;\n+\n+          default:\n+            // the disk file is corrupted, using this file may be dangerous\n+            logger.error(\"Unrecognized marker detected, this file may be corrupted\");\n+            return false;\n+        }\n+      }\n+      // close upgraded tsFiles and generate resources for them\n+      for (TsFileIOWriter tsFileIOWriter : partitionWriterMap.values()) {\n+        upgradedResources.add(endFileAndGenerateResource(tsFileIOWriter));\n+      }\n+      return true;\n+    } catch (IOException e2) {\n+      logger.info(\"TsFile upgrade process cannot proceed at position {} after {} chunk groups \"\n+          + \"recovered, because : {}\", this.position(), newMetaData.size(), e2.getMessage());\n+      return false;\n+    } finally {\n+      if (tsFileInput != null) {\n+        tsFileInput.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   *  Rewrite the chunk group to new TsFile.\n+   *  If data of this chunk group are in different time partitions,\n+   *  create multiple new TsFiles and rewrite data in each partition.\n+   */\n+  private void rewrite(File oldTsFile, String deviceId, List<MeasurementSchema> schemas, \n+      List<List<ByteBuffer>> dataInChunkGroup, long versionOfChunkGroup) \n+          throws IOException {\n+\n+    Map<Long, Map<MeasurementSchema, IChunkWriter>> chunkWritersInChunkGroup = new HashMap<>();\n+    for (int i = 0; i < schemas.size(); i++) {\n+      MeasurementSchema schema = schemas.get(i);\n+      Decoder defaultTimeDecoder = Decoder.getDecoderByType(\n+          TSEncoding.valueOf(TSFileDescriptor.getInstance().getConfig().getTimeEncoder()),\n+          TSDataType.INT64);\n+      Decoder valueDecoder = Decoder\n+          .getDecoderByType(schema.getEncodingType(), schema.getType());\n+      List<ByteBuffer> dataInChunk = dataInChunkGroup.get(i);\n+      for (ByteBuffer pageData : dataInChunk) {\n+        valueDecoder.reset();\n+        PageReader pageReader = new PageReader(pageData, schema.getType(), valueDecoder,\n+            defaultTimeDecoder, null);\n+        BatchData batchData = pageReader.getAllSatisfiedPageData();\n+        while (batchData.hasCurrent()) {\n+          long time = batchData.currentTime();\n+          Object value = batchData.currentValue();\n+          long partition = StorageEngine.getTimePartition(time);\n+          \n+          Map<MeasurementSchema, IChunkWriter> chunkWriters = chunkWritersInChunkGroup.getOrDefault(partition, new HashMap<>());\n+          IChunkWriter chunkWriter = chunkWriters.getOrDefault(schema, new ChunkWriterImpl(schema));\n+          TsFileIOWriter tsFileIOWriter = getOrDefaultTsFileIOWriter(oldTsFile, partition);\n+          partitionWriterMap.put(partition, tsFileIOWriter);\n+          switch (schema.getType()) {\n+            case INT32:\n+              chunkWriter.write(time, (int) value);\n+              break;\n+            case INT64:\n+              chunkWriter.write(time, (long) value);\n+              break;\n+            case FLOAT:\n+              chunkWriter.write(time, (float) value);\n+              break;\n+            case DOUBLE:\n+              chunkWriter.write(time, (double) value);\n+              break;\n+            case BOOLEAN:\n+              chunkWriter.write(time, (boolean) value);\n+              break;\n+            case TEXT:\n+              chunkWriter.write(time, (Binary) value);\n+              break;\n+            default:\n+              throw new UnSupportedDataTypeException(\n+                  String.format(\"Data type %s is not supported.\", schema.getType()));\n+            }\n+          chunkWriters.put(schema, chunkWriter);\n+          chunkWritersInChunkGroup.put(partition, chunkWriters);\n+          batchData.next();\n+        }\n+      }\n+    }\n+    // set version info to each upgraded tsFile \n+    for (Entry<Long, Map<MeasurementSchema, IChunkWriter>> entry : chunkWritersInChunkGroup.entrySet()) {\n+      long partition = entry.getKey();\n+      TsFileIOWriter tsFileIOWriter = partitionWriterMap.get(partition);\n+      tsFileIOWriter.startChunkGroup(deviceId);\n+      for (IChunkWriter chunkWriter : entry.getValue().values()) {\n+        chunkWriter.writeToFileWriter(tsFileIOWriter);\n+      }\n+      tsFileIOWriter.endChunkGroup();\n+      tsFileIOWriter.writeVersion(versionOfChunkGroup);\n+    }\n+  }\n+\n+  /**\n+   * \n+   * @param oldTsFile\n+   * @param deviceId\n+   * @param schemas\n+   * @param pageHeadersInChunkGroup\n+   * @param dataInChunkGroup\n+   * @param versionOfChunkGroup\n+   * @param partition\n+   * @throws IOException\n+   * @throws PageException\n+   */\n+  private void quickRewrite(File oldTsFile, String deviceId, List<MeasurementSchema> schemas, ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwMjAwNA=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 492}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTM4NTU1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNzozMjoyMVrOGTBlLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNTowNzowMFrOGTE-iw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwMjAzMQ==", "bodyText": "add javadoc or rename to rewriteUnCompressedPage", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422602031", "createdAt": "2020-05-10T07:32:21Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "diffHunk": "@@ -0,0 +1,624 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import org.apache.iotdb.db.engine.StorageEngine;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.common.conf.TSFileDescriptor;\n+import org.apache.iotdb.tsfile.compress.IUnCompressor;\n+import org.apache.iotdb.tsfile.encoding.decoder.Decoder;\n+import org.apache.iotdb.tsfile.exception.write.PageException;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+import org.apache.iotdb.tsfile.file.MetaMarker;\n+import org.apache.iotdb.tsfile.file.footer.ChunkGroupFooter;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkGroupMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.CompressionType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSEncoding;\n+import org.apache.iotdb.tsfile.fileSystem.FSFactoryProducer;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.reader.LocalTsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.page.PageReader;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.write.chunk.ChunkWriterImpl;\n+import org.apache.iotdb.tsfile.write.chunk.IChunkWriter;\n+import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n+import org.apache.iotdb.tsfile.write.writer.TsFileIOWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+public class TsfileOnlineUpgradeTool implements AutoCloseable {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(TsfileOnlineUpgradeTool.class);\n+\n+  private TsFileInput tsFileInput;\n+  private long fileMetadataPos;\n+  private int fileMetadataSize;\n+  private ByteBuffer markerBuffer = ByteBuffer.allocate(Byte.BYTES);\n+  protected String file;\n+  \n+  // PartitionId -> TsFileIOWriter \n+  private Map<Long, TsFileIOWriter> partitionWriterMap;\n+\n+  /**\n+   * Create a file reader of the given file. The reader will read the tail of the file to get the\n+   * file metadata size.Then the reader will skip the first TSFileConfig.OLD_MAGIC_STRING.length()\n+   * bytes of the file for preparing reading real data.\n+   *\n+   * @param file the data file\n+   * @throws IOException If some I/O error occurs\n+   */\n+  public TsfileOnlineUpgradeTool(String file) throws IOException {\n+    this(file, true);\n+  }\n+\n+  /**\n+   * construct function for TsfileOnlineUpgradeTool.\n+   *\n+   * @param file -given file name\n+   * @param loadMetadataSize -load meta data size\n+   */\n+  public TsfileOnlineUpgradeTool(String file, boolean loadMetadataSize) throws IOException {\n+    this.file = file;\n+    final java.nio.file.Path path = Paths.get(file);\n+    tsFileInput = new LocalTsFileInput(path);\n+    partitionWriterMap = new HashMap<>();\n+    try {\n+      if (loadMetadataSize) {\n+        loadMetadataSize();\n+      }\n+    } catch (Exception e) {\n+      tsFileInput.close();\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * \n+   */\n+  public void loadMetadataSize() throws IOException {\n+    ByteBuffer metadataSize = ByteBuffer.allocate(Integer.BYTES);\n+    tsFileInput.read(metadataSize,\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES);\n+    metadataSize.flip();\n+    // read file metadata size and position\n+    fileMetadataSize = ReadWriteIOUtils.readInt(metadataSize);\n+    fileMetadataPos =\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES\n+            - fileMetadataSize;\n+    // skip the magic header\n+    position(TSFileConfig.MAGIC_STRING.length());\n+  }\n+\n+  public String readTailMagic() throws IOException {\n+    long totalSize = tsFileInput.size();\n+\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    tsFileInput.read(magicStringBytes, totalSize - TSFileConfig.MAGIC_STRING.length());\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * whether the file is a complete TsFile: only if the head magic and tail magic string exists.\n+   */\n+  public boolean isComplete() throws IOException {\n+    return tsFileInput.size() >= TSFileConfig.MAGIC_STRING.length() * 2 && readTailMagic()\n+        .equals(readHeadMagic());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public String readHeadMagic() throws IOException {\n+    return readHeadMagic(false);\n+  }\n+\n+  /**\n+   * @param movePosition whether move the position of the file reader after reading the magic header\n+   * to the end of the magic head string.\n+   */\n+  public String readHeadMagic(boolean movePosition) throws IOException {\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    if (movePosition) {\n+      tsFileInput.position(0);\n+      tsFileInput.read(magicStringBytes);\n+    } else {\n+      tsFileInput.read(magicStringBytes, 0);\n+    }\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * this function reads version number and checks compatibility of TsFile.\n+   */\n+  public String readVersionNumber() throws IOException {\n+    ByteBuffer versionNumberBytes = ByteBuffer\n+        .allocate(TSFileConfig.VERSION_NUMBER.getBytes().length);\n+    tsFileInput.position(TSFileConfig.MAGIC_STRING.getBytes().length);\n+    tsFileInput.read(versionNumberBytes);\n+    versionNumberBytes.flip();\n+    return new String(versionNumberBytes.array());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsFileMetadata readFileMetadata() throws IOException {\n+    return OldTsFileMetadata.deserializeFrom(readData(fileMetadataPos, fileMetadataSize));\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsDeviceMetadata readTsDeviceMetaData(OldTsDeviceMetadataIndex index) throws IOException {\n+    return OldTsDeviceMetadata.deserializeFrom(readData(index.getOffset(), index.getLen()));\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_GROUP_FOOTER. <br>\n+   * This method is not threadsafe.\n+   *\n+   * @return a CHUNK_GROUP_FOOTER\n+   * @throws IOException io error\n+   */\n+  public ChunkGroupFooter readChunkGroupFooter() throws IOException {\n+    return ChunkGroupFooter.deserializeFrom(tsFileInput.wrapAsInputStream(), true);\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_HEADER. <br> This\n+   * method is not threadsafe.\n+   *\n+   * @return a CHUNK_HEADER\n+   * @throws IOException io error\n+   */\n+  public ChunkHeader readChunkHeader() throws IOException {\n+    return ChunkHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), true, true);\n+  }\n+\n+  /**\n+   * not thread safe.\n+   *\n+   * @param type given tsfile data type\n+   */\n+  public PageHeader readPageHeader(TSDataType type) throws IOException {\n+    return PageHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), type, true);\n+  }\n+\n+  public ByteBuffer readPage(PageHeader header, CompressionType type)\n+      throws IOException {\n+    ByteBuffer buffer = readData(-1, header.getCompressedSize());\n+    IUnCompressor unCompressor = IUnCompressor.getUnCompressor(type);\n+    ByteBuffer uncompressedBuffer = ByteBuffer.allocate(header.getUncompressedSize());\n+    if (type == CompressionType.UNCOMPRESSED) {\n+      return buffer;\n+    }\n+    unCompressor.uncompress(buffer.array(), buffer.position(), buffer.remaining(),\n+        uncompressedBuffer.array(),\n+        0);\n+    return uncompressedBuffer;\n+  }\n+  \n+  public ByteBuffer readCompressedPage(PageHeader header) throws IOException {\n+    return readData(-1, header.getCompressedSize());\n+  }\n+\n+  public long position() throws IOException {\n+    return tsFileInput.position();\n+  }\n+\n+  public void position(long offset) throws IOException {\n+    tsFileInput.position(offset);\n+  }\n+\n+  /**\n+   * read one byte from the input. <br> this method is not thread safe\n+   */\n+  public byte readMarker() throws IOException {\n+    markerBuffer.clear();\n+    if (ReadWriteIOUtils.readAsPossible(tsFileInput, markerBuffer) == 0) {\n+      throw new IOException(\"reach the end of the file.\");\n+    }\n+    markerBuffer.flip();\n+    return markerBuffer.get();\n+  }\n+\n+  public byte readMarker(long position) throws IOException {\n+    return readData(position, Byte.BYTES).get();\n+  }\n+\n+  public void close() throws IOException {\n+    this.tsFileInput.close();\n+  }\n+\n+  public String getFileName() {\n+    return this.file;\n+  }\n+\n+  /**\n+   * read data from tsFileInput, from the current position (if position = -1), or the given\n+   * position. <br> if position = -1, the tsFileInput's position will be changed to the current\n+   * position + real data size that been read. Other wise, the tsFileInput's position is not\n+   * changed.\n+   *\n+   * @param position the start position of data in the tsFileInput, or the current position if\n+   * position = -1\n+   * @param size the size of data that want to read\n+   * @return data that been read.\n+   */\n+  private ByteBuffer readData(long position, int size) throws IOException {\n+    ByteBuffer buffer = ByteBuffer.allocate(size);\n+    if (position == -1) {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    } else {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer, position, size) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    }\n+    buffer.flip();\n+    return buffer;\n+  }\n+\n+  /**\n+   * upgrade file and resource, return the boolean value whether upgrade task completes\n+   * @throws IOException, WriteProcessException \n+   */\n+  public boolean upgradeFile(List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    File oldTsFile = FSFactoryProducer.getFSFactory().getFile(this.file);\n+\n+    // check if the old TsFile has correct header \n+    if (!fileCheck(oldTsFile)) {\n+      return false;\n+    }\n+\n+    // ChunkGroupOffset -> version\n+    Map<Long, Long> oldVersionInfo = new HashMap<>();\n+\n+    // ChunkGroupOffset -> time partition, record the offsets of chunk group that data are in same partition\n+    Map<Long, Long> chunkGroupTimePartitionInfo = new HashMap<>();\n+\n+    // scan metadata to get version Info and chunkGroupTimePartitionInfo\n+    scanMetadata(oldVersionInfo, chunkGroupTimePartitionInfo);\n+    \n+    // start to scan chunks and chunkGroups\n+    long startOffsetOfChunkGroup = 0;\n+    boolean newChunkGroup = true;\n+    long versionOfChunkGroup = 0;\n+    boolean chunkGroupInSamePartition = false;\n+    List<ChunkGroupMetadata> newMetaData = new ArrayList<>();\n+    List<List<PageHeader>> pageHeadersInChunkGroup = new ArrayList<>();\n+    List<List<ByteBuffer>> dataInChunkGroup = new ArrayList<>();\n+    byte marker;\n+    List<MeasurementSchema> measurementSchemaList = new ArrayList<>();\n+    try {\n+      while ((marker = this.readMarker()) != MetaMarker.SEPARATOR) {\n+        switch (marker) {\n+          case MetaMarker.CHUNK_HEADER:\n+            // this is the first chunk of a new ChunkGroup.\n+            if (newChunkGroup) {\n+              newChunkGroup = false;\n+              startOffsetOfChunkGroup = this.position() - 1;\n+              versionOfChunkGroup = oldVersionInfo.get(startOffsetOfChunkGroup);\n+              chunkGroupInSamePartition = chunkGroupTimePartitionInfo\n+                  .containsKey(startOffsetOfChunkGroup);\n+            }\n+            ChunkHeader header = this.readChunkHeader();\n+            MeasurementSchema measurementSchema = new MeasurementSchema(header.getMeasurementID(),\n+                header.getDataType(),\n+                header.getEncodingType(), \n+                header.getCompressionType());\n+            measurementSchemaList.add(measurementSchema);\n+            List<PageHeader> pageHeadersInChunk = new ArrayList<>();\n+            List<ByteBuffer> dataInChunk = new ArrayList<>();\n+            for (int j = 0; j < header.getNumOfPages(); j++) {\n+              PageHeader pageHeader = readPageHeader(header.getDataType());\n+              ByteBuffer pageData = chunkGroupInSamePartition ? \n+                  readCompressedPage(pageHeader) : readPage(pageHeader, header.getCompressionType());\n+              pageHeadersInChunk.add(pageHeader);\n+              dataInChunk.add(pageData);\n+            }\n+            pageHeadersInChunkGroup.add(pageHeadersInChunk);\n+            dataInChunkGroup.add(dataInChunk);\n+            break;\n+          case MetaMarker.CHUNK_GROUP_FOOTER:\n+            // this is the footer of a ChunkGroup.\n+            ChunkGroupFooter chunkGroupFooter = this.readChunkGroupFooter();\n+            String deviceID = chunkGroupFooter.getDeviceID();\n+            if (chunkGroupInSamePartition) {\n+              quickRewrite(oldTsFile, deviceID, measurementSchemaList, pageHeadersInChunkGroup,\n+                  dataInChunkGroup, versionOfChunkGroup, chunkGroupTimePartitionInfo.get(startOffsetOfChunkGroup));\n+            } else {\n+              rewrite(oldTsFile, deviceID, measurementSchemaList, \n+                dataInChunkGroup, versionOfChunkGroup);\n+            }\n+\n+            pageHeadersInChunkGroup.clear();\n+            dataInChunkGroup.clear();\n+            measurementSchemaList.clear();\n+            newChunkGroup = true;\n+            break;\n+\n+          default:\n+            // the disk file is corrupted, using this file may be dangerous\n+            logger.error(\"Unrecognized marker detected, this file may be corrupted\");\n+            return false;\n+        }\n+      }\n+      // close upgraded tsFiles and generate resources for them\n+      for (TsFileIOWriter tsFileIOWriter : partitionWriterMap.values()) {\n+        upgradedResources.add(endFileAndGenerateResource(tsFileIOWriter));\n+      }\n+      return true;\n+    } catch (IOException e2) {\n+      logger.info(\"TsFile upgrade process cannot proceed at position {} after {} chunk groups \"\n+          + \"recovered, because : {}\", this.position(), newMetaData.size(), e2.getMessage());\n+      return false;\n+    } finally {\n+      if (tsFileInput != null) {\n+        tsFileInput.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   *  Rewrite the chunk group to new TsFile.\n+   *  If data of this chunk group are in different time partitions,\n+   *  create multiple new TsFiles and rewrite data in each partition.\n+   */\n+  private void rewrite(File oldTsFile, String deviceId, List<MeasurementSchema> schemas, ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 411}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY1NzY3NQ==", "bodyText": "The same reason as above. This method is for rewriting the ChunkGroup which data is in the different time partitions. To rewrite this, we have to decode the data to points, and rewrite the data point to different chunkWriters.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422657675", "createdAt": "2020-05-10T15:07:00Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "diffHunk": "@@ -0,0 +1,624 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import org.apache.iotdb.db.engine.StorageEngine;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.common.conf.TSFileDescriptor;\n+import org.apache.iotdb.tsfile.compress.IUnCompressor;\n+import org.apache.iotdb.tsfile.encoding.decoder.Decoder;\n+import org.apache.iotdb.tsfile.exception.write.PageException;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+import org.apache.iotdb.tsfile.file.MetaMarker;\n+import org.apache.iotdb.tsfile.file.footer.ChunkGroupFooter;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkGroupMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.CompressionType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSEncoding;\n+import org.apache.iotdb.tsfile.fileSystem.FSFactoryProducer;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.reader.LocalTsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.page.PageReader;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.write.chunk.ChunkWriterImpl;\n+import org.apache.iotdb.tsfile.write.chunk.IChunkWriter;\n+import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n+import org.apache.iotdb.tsfile.write.writer.TsFileIOWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+public class TsfileOnlineUpgradeTool implements AutoCloseable {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(TsfileOnlineUpgradeTool.class);\n+\n+  private TsFileInput tsFileInput;\n+  private long fileMetadataPos;\n+  private int fileMetadataSize;\n+  private ByteBuffer markerBuffer = ByteBuffer.allocate(Byte.BYTES);\n+  protected String file;\n+  \n+  // PartitionId -> TsFileIOWriter \n+  private Map<Long, TsFileIOWriter> partitionWriterMap;\n+\n+  /**\n+   * Create a file reader of the given file. The reader will read the tail of the file to get the\n+   * file metadata size.Then the reader will skip the first TSFileConfig.OLD_MAGIC_STRING.length()\n+   * bytes of the file for preparing reading real data.\n+   *\n+   * @param file the data file\n+   * @throws IOException If some I/O error occurs\n+   */\n+  public TsfileOnlineUpgradeTool(String file) throws IOException {\n+    this(file, true);\n+  }\n+\n+  /**\n+   * construct function for TsfileOnlineUpgradeTool.\n+   *\n+   * @param file -given file name\n+   * @param loadMetadataSize -load meta data size\n+   */\n+  public TsfileOnlineUpgradeTool(String file, boolean loadMetadataSize) throws IOException {\n+    this.file = file;\n+    final java.nio.file.Path path = Paths.get(file);\n+    tsFileInput = new LocalTsFileInput(path);\n+    partitionWriterMap = new HashMap<>();\n+    try {\n+      if (loadMetadataSize) {\n+        loadMetadataSize();\n+      }\n+    } catch (Exception e) {\n+      tsFileInput.close();\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * \n+   */\n+  public void loadMetadataSize() throws IOException {\n+    ByteBuffer metadataSize = ByteBuffer.allocate(Integer.BYTES);\n+    tsFileInput.read(metadataSize,\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES);\n+    metadataSize.flip();\n+    // read file metadata size and position\n+    fileMetadataSize = ReadWriteIOUtils.readInt(metadataSize);\n+    fileMetadataPos =\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES\n+            - fileMetadataSize;\n+    // skip the magic header\n+    position(TSFileConfig.MAGIC_STRING.length());\n+  }\n+\n+  public String readTailMagic() throws IOException {\n+    long totalSize = tsFileInput.size();\n+\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    tsFileInput.read(magicStringBytes, totalSize - TSFileConfig.MAGIC_STRING.length());\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * whether the file is a complete TsFile: only if the head magic and tail magic string exists.\n+   */\n+  public boolean isComplete() throws IOException {\n+    return tsFileInput.size() >= TSFileConfig.MAGIC_STRING.length() * 2 && readTailMagic()\n+        .equals(readHeadMagic());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public String readHeadMagic() throws IOException {\n+    return readHeadMagic(false);\n+  }\n+\n+  /**\n+   * @param movePosition whether move the position of the file reader after reading the magic header\n+   * to the end of the magic head string.\n+   */\n+  public String readHeadMagic(boolean movePosition) throws IOException {\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    if (movePosition) {\n+      tsFileInput.position(0);\n+      tsFileInput.read(magicStringBytes);\n+    } else {\n+      tsFileInput.read(magicStringBytes, 0);\n+    }\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * this function reads version number and checks compatibility of TsFile.\n+   */\n+  public String readVersionNumber() throws IOException {\n+    ByteBuffer versionNumberBytes = ByteBuffer\n+        .allocate(TSFileConfig.VERSION_NUMBER.getBytes().length);\n+    tsFileInput.position(TSFileConfig.MAGIC_STRING.getBytes().length);\n+    tsFileInput.read(versionNumberBytes);\n+    versionNumberBytes.flip();\n+    return new String(versionNumberBytes.array());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsFileMetadata readFileMetadata() throws IOException {\n+    return OldTsFileMetadata.deserializeFrom(readData(fileMetadataPos, fileMetadataSize));\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsDeviceMetadata readTsDeviceMetaData(OldTsDeviceMetadataIndex index) throws IOException {\n+    return OldTsDeviceMetadata.deserializeFrom(readData(index.getOffset(), index.getLen()));\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_GROUP_FOOTER. <br>\n+   * This method is not threadsafe.\n+   *\n+   * @return a CHUNK_GROUP_FOOTER\n+   * @throws IOException io error\n+   */\n+  public ChunkGroupFooter readChunkGroupFooter() throws IOException {\n+    return ChunkGroupFooter.deserializeFrom(tsFileInput.wrapAsInputStream(), true);\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_HEADER. <br> This\n+   * method is not threadsafe.\n+   *\n+   * @return a CHUNK_HEADER\n+   * @throws IOException io error\n+   */\n+  public ChunkHeader readChunkHeader() throws IOException {\n+    return ChunkHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), true, true);\n+  }\n+\n+  /**\n+   * not thread safe.\n+   *\n+   * @param type given tsfile data type\n+   */\n+  public PageHeader readPageHeader(TSDataType type) throws IOException {\n+    return PageHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), type, true);\n+  }\n+\n+  public ByteBuffer readPage(PageHeader header, CompressionType type)\n+      throws IOException {\n+    ByteBuffer buffer = readData(-1, header.getCompressedSize());\n+    IUnCompressor unCompressor = IUnCompressor.getUnCompressor(type);\n+    ByteBuffer uncompressedBuffer = ByteBuffer.allocate(header.getUncompressedSize());\n+    if (type == CompressionType.UNCOMPRESSED) {\n+      return buffer;\n+    }\n+    unCompressor.uncompress(buffer.array(), buffer.position(), buffer.remaining(),\n+        uncompressedBuffer.array(),\n+        0);\n+    return uncompressedBuffer;\n+  }\n+  \n+  public ByteBuffer readCompressedPage(PageHeader header) throws IOException {\n+    return readData(-1, header.getCompressedSize());\n+  }\n+\n+  public long position() throws IOException {\n+    return tsFileInput.position();\n+  }\n+\n+  public void position(long offset) throws IOException {\n+    tsFileInput.position(offset);\n+  }\n+\n+  /**\n+   * read one byte from the input. <br> this method is not thread safe\n+   */\n+  public byte readMarker() throws IOException {\n+    markerBuffer.clear();\n+    if (ReadWriteIOUtils.readAsPossible(tsFileInput, markerBuffer) == 0) {\n+      throw new IOException(\"reach the end of the file.\");\n+    }\n+    markerBuffer.flip();\n+    return markerBuffer.get();\n+  }\n+\n+  public byte readMarker(long position) throws IOException {\n+    return readData(position, Byte.BYTES).get();\n+  }\n+\n+  public void close() throws IOException {\n+    this.tsFileInput.close();\n+  }\n+\n+  public String getFileName() {\n+    return this.file;\n+  }\n+\n+  /**\n+   * read data from tsFileInput, from the current position (if position = -1), or the given\n+   * position. <br> if position = -1, the tsFileInput's position will be changed to the current\n+   * position + real data size that been read. Other wise, the tsFileInput's position is not\n+   * changed.\n+   *\n+   * @param position the start position of data in the tsFileInput, or the current position if\n+   * position = -1\n+   * @param size the size of data that want to read\n+   * @return data that been read.\n+   */\n+  private ByteBuffer readData(long position, int size) throws IOException {\n+    ByteBuffer buffer = ByteBuffer.allocate(size);\n+    if (position == -1) {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    } else {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer, position, size) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    }\n+    buffer.flip();\n+    return buffer;\n+  }\n+\n+  /**\n+   * upgrade file and resource, return the boolean value whether upgrade task completes\n+   * @throws IOException, WriteProcessException \n+   */\n+  public boolean upgradeFile(List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    File oldTsFile = FSFactoryProducer.getFSFactory().getFile(this.file);\n+\n+    // check if the old TsFile has correct header \n+    if (!fileCheck(oldTsFile)) {\n+      return false;\n+    }\n+\n+    // ChunkGroupOffset -> version\n+    Map<Long, Long> oldVersionInfo = new HashMap<>();\n+\n+    // ChunkGroupOffset -> time partition, record the offsets of chunk group that data are in same partition\n+    Map<Long, Long> chunkGroupTimePartitionInfo = new HashMap<>();\n+\n+    // scan metadata to get version Info and chunkGroupTimePartitionInfo\n+    scanMetadata(oldVersionInfo, chunkGroupTimePartitionInfo);\n+    \n+    // start to scan chunks and chunkGroups\n+    long startOffsetOfChunkGroup = 0;\n+    boolean newChunkGroup = true;\n+    long versionOfChunkGroup = 0;\n+    boolean chunkGroupInSamePartition = false;\n+    List<ChunkGroupMetadata> newMetaData = new ArrayList<>();\n+    List<List<PageHeader>> pageHeadersInChunkGroup = new ArrayList<>();\n+    List<List<ByteBuffer>> dataInChunkGroup = new ArrayList<>();\n+    byte marker;\n+    List<MeasurementSchema> measurementSchemaList = new ArrayList<>();\n+    try {\n+      while ((marker = this.readMarker()) != MetaMarker.SEPARATOR) {\n+        switch (marker) {\n+          case MetaMarker.CHUNK_HEADER:\n+            // this is the first chunk of a new ChunkGroup.\n+            if (newChunkGroup) {\n+              newChunkGroup = false;\n+              startOffsetOfChunkGroup = this.position() - 1;\n+              versionOfChunkGroup = oldVersionInfo.get(startOffsetOfChunkGroup);\n+              chunkGroupInSamePartition = chunkGroupTimePartitionInfo\n+                  .containsKey(startOffsetOfChunkGroup);\n+            }\n+            ChunkHeader header = this.readChunkHeader();\n+            MeasurementSchema measurementSchema = new MeasurementSchema(header.getMeasurementID(),\n+                header.getDataType(),\n+                header.getEncodingType(), \n+                header.getCompressionType());\n+            measurementSchemaList.add(measurementSchema);\n+            List<PageHeader> pageHeadersInChunk = new ArrayList<>();\n+            List<ByteBuffer> dataInChunk = new ArrayList<>();\n+            for (int j = 0; j < header.getNumOfPages(); j++) {\n+              PageHeader pageHeader = readPageHeader(header.getDataType());\n+              ByteBuffer pageData = chunkGroupInSamePartition ? \n+                  readCompressedPage(pageHeader) : readPage(pageHeader, header.getCompressionType());\n+              pageHeadersInChunk.add(pageHeader);\n+              dataInChunk.add(pageData);\n+            }\n+            pageHeadersInChunkGroup.add(pageHeadersInChunk);\n+            dataInChunkGroup.add(dataInChunk);\n+            break;\n+          case MetaMarker.CHUNK_GROUP_FOOTER:\n+            // this is the footer of a ChunkGroup.\n+            ChunkGroupFooter chunkGroupFooter = this.readChunkGroupFooter();\n+            String deviceID = chunkGroupFooter.getDeviceID();\n+            if (chunkGroupInSamePartition) {\n+              quickRewrite(oldTsFile, deviceID, measurementSchemaList, pageHeadersInChunkGroup,\n+                  dataInChunkGroup, versionOfChunkGroup, chunkGroupTimePartitionInfo.get(startOffsetOfChunkGroup));\n+            } else {\n+              rewrite(oldTsFile, deviceID, measurementSchemaList, \n+                dataInChunkGroup, versionOfChunkGroup);\n+            }\n+\n+            pageHeadersInChunkGroup.clear();\n+            dataInChunkGroup.clear();\n+            measurementSchemaList.clear();\n+            newChunkGroup = true;\n+            break;\n+\n+          default:\n+            // the disk file is corrupted, using this file may be dangerous\n+            logger.error(\"Unrecognized marker detected, this file may be corrupted\");\n+            return false;\n+        }\n+      }\n+      // close upgraded tsFiles and generate resources for them\n+      for (TsFileIOWriter tsFileIOWriter : partitionWriterMap.values()) {\n+        upgradedResources.add(endFileAndGenerateResource(tsFileIOWriter));\n+      }\n+      return true;\n+    } catch (IOException e2) {\n+      logger.info(\"TsFile upgrade process cannot proceed at position {} after {} chunk groups \"\n+          + \"recovered, because : {}\", this.position(), newMetaData.size(), e2.getMessage());\n+      return false;\n+    } finally {\n+      if (tsFileInput != null) {\n+        tsFileInput.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   *  Rewrite the chunk group to new TsFile.\n+   *  If data of this chunk group are in different time partitions,\n+   *  create multiple new TsFiles and rewrite data in each partition.\n+   */\n+  private void rewrite(File oldTsFile, String deviceId, List<MeasurementSchema> schemas, ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwMjAzMQ=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 411}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTM5NjUyOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNzo0NTozMFrOGTBqkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNTo0MToxOFrOGTFPJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwMzQwOQ==", "bodyText": "this could be deduplicated, not a big problem.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422603409", "createdAt": "2020-05-10T07:45:30Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "diffHunk": "@@ -0,0 +1,624 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import org.apache.iotdb.db.engine.StorageEngine;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.common.conf.TSFileDescriptor;\n+import org.apache.iotdb.tsfile.compress.IUnCompressor;\n+import org.apache.iotdb.tsfile.encoding.decoder.Decoder;\n+import org.apache.iotdb.tsfile.exception.write.PageException;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+import org.apache.iotdb.tsfile.file.MetaMarker;\n+import org.apache.iotdb.tsfile.file.footer.ChunkGroupFooter;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkGroupMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.CompressionType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSEncoding;\n+import org.apache.iotdb.tsfile.fileSystem.FSFactoryProducer;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.reader.LocalTsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.page.PageReader;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.write.chunk.ChunkWriterImpl;\n+import org.apache.iotdb.tsfile.write.chunk.IChunkWriter;\n+import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n+import org.apache.iotdb.tsfile.write.writer.TsFileIOWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+public class TsfileOnlineUpgradeTool implements AutoCloseable {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(TsfileOnlineUpgradeTool.class);\n+\n+  private TsFileInput tsFileInput;\n+  private long fileMetadataPos;\n+  private int fileMetadataSize;\n+  private ByteBuffer markerBuffer = ByteBuffer.allocate(Byte.BYTES);\n+  protected String file;\n+  \n+  // PartitionId -> TsFileIOWriter \n+  private Map<Long, TsFileIOWriter> partitionWriterMap;\n+\n+  /**\n+   * Create a file reader of the given file. The reader will read the tail of the file to get the\n+   * file metadata size.Then the reader will skip the first TSFileConfig.OLD_MAGIC_STRING.length()\n+   * bytes of the file for preparing reading real data.\n+   *\n+   * @param file the data file\n+   * @throws IOException If some I/O error occurs\n+   */\n+  public TsfileOnlineUpgradeTool(String file) throws IOException {\n+    this(file, true);\n+  }\n+\n+  /**\n+   * construct function for TsfileOnlineUpgradeTool.\n+   *\n+   * @param file -given file name\n+   * @param loadMetadataSize -load meta data size\n+   */\n+  public TsfileOnlineUpgradeTool(String file, boolean loadMetadataSize) throws IOException {\n+    this.file = file;\n+    final java.nio.file.Path path = Paths.get(file);\n+    tsFileInput = new LocalTsFileInput(path);\n+    partitionWriterMap = new HashMap<>();\n+    try {\n+      if (loadMetadataSize) {\n+        loadMetadataSize();\n+      }\n+    } catch (Exception e) {\n+      tsFileInput.close();\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * \n+   */\n+  public void loadMetadataSize() throws IOException {\n+    ByteBuffer metadataSize = ByteBuffer.allocate(Integer.BYTES);\n+    tsFileInput.read(metadataSize,\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES);\n+    metadataSize.flip();\n+    // read file metadata size and position\n+    fileMetadataSize = ReadWriteIOUtils.readInt(metadataSize);\n+    fileMetadataPos =\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES\n+            - fileMetadataSize;\n+    // skip the magic header\n+    position(TSFileConfig.MAGIC_STRING.length());\n+  }\n+\n+  public String readTailMagic() throws IOException {\n+    long totalSize = tsFileInput.size();\n+\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    tsFileInput.read(magicStringBytes, totalSize - TSFileConfig.MAGIC_STRING.length());\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * whether the file is a complete TsFile: only if the head magic and tail magic string exists.\n+   */\n+  public boolean isComplete() throws IOException {\n+    return tsFileInput.size() >= TSFileConfig.MAGIC_STRING.length() * 2 && readTailMagic()\n+        .equals(readHeadMagic());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public String readHeadMagic() throws IOException {\n+    return readHeadMagic(false);\n+  }\n+\n+  /**\n+   * @param movePosition whether move the position of the file reader after reading the magic header\n+   * to the end of the magic head string.\n+   */\n+  public String readHeadMagic(boolean movePosition) throws IOException {\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    if (movePosition) {\n+      tsFileInput.position(0);\n+      tsFileInput.read(magicStringBytes);\n+    } else {\n+      tsFileInput.read(magicStringBytes, 0);\n+    }\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * this function reads version number and checks compatibility of TsFile.\n+   */\n+  public String readVersionNumber() throws IOException {\n+    ByteBuffer versionNumberBytes = ByteBuffer\n+        .allocate(TSFileConfig.VERSION_NUMBER.getBytes().length);\n+    tsFileInput.position(TSFileConfig.MAGIC_STRING.getBytes().length);\n+    tsFileInput.read(versionNumberBytes);\n+    versionNumberBytes.flip();\n+    return new String(versionNumberBytes.array());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsFileMetadata readFileMetadata() throws IOException {\n+    return OldTsFileMetadata.deserializeFrom(readData(fileMetadataPos, fileMetadataSize));\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsDeviceMetadata readTsDeviceMetaData(OldTsDeviceMetadataIndex index) throws IOException {\n+    return OldTsDeviceMetadata.deserializeFrom(readData(index.getOffset(), index.getLen()));\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_GROUP_FOOTER. <br>\n+   * This method is not threadsafe.\n+   *\n+   * @return a CHUNK_GROUP_FOOTER\n+   * @throws IOException io error\n+   */\n+  public ChunkGroupFooter readChunkGroupFooter() throws IOException {\n+    return ChunkGroupFooter.deserializeFrom(tsFileInput.wrapAsInputStream(), true);\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_HEADER. <br> This\n+   * method is not threadsafe.\n+   *\n+   * @return a CHUNK_HEADER\n+   * @throws IOException io error\n+   */\n+  public ChunkHeader readChunkHeader() throws IOException {\n+    return ChunkHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), true, true);\n+  }\n+\n+  /**\n+   * not thread safe.\n+   *\n+   * @param type given tsfile data type\n+   */\n+  public PageHeader readPageHeader(TSDataType type) throws IOException {\n+    return PageHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), type, true);\n+  }\n+\n+  public ByteBuffer readPage(PageHeader header, CompressionType type)\n+      throws IOException {\n+    ByteBuffer buffer = readData(-1, header.getCompressedSize());\n+    IUnCompressor unCompressor = IUnCompressor.getUnCompressor(type);\n+    ByteBuffer uncompressedBuffer = ByteBuffer.allocate(header.getUncompressedSize());\n+    if (type == CompressionType.UNCOMPRESSED) {\n+      return buffer;\n+    }\n+    unCompressor.uncompress(buffer.array(), buffer.position(), buffer.remaining(),\n+        uncompressedBuffer.array(),\n+        0);\n+    return uncompressedBuffer;\n+  }\n+  \n+  public ByteBuffer readCompressedPage(PageHeader header) throws IOException {\n+    return readData(-1, header.getCompressedSize());\n+  }\n+\n+  public long position() throws IOException {\n+    return tsFileInput.position();\n+  }\n+\n+  public void position(long offset) throws IOException {\n+    tsFileInput.position(offset);\n+  }\n+\n+  /**\n+   * read one byte from the input. <br> this method is not thread safe\n+   */\n+  public byte readMarker() throws IOException {\n+    markerBuffer.clear();\n+    if (ReadWriteIOUtils.readAsPossible(tsFileInput, markerBuffer) == 0) {\n+      throw new IOException(\"reach the end of the file.\");\n+    }\n+    markerBuffer.flip();\n+    return markerBuffer.get();\n+  }\n+\n+  public byte readMarker(long position) throws IOException {\n+    return readData(position, Byte.BYTES).get();\n+  }\n+\n+  public void close() throws IOException {\n+    this.tsFileInput.close();\n+  }\n+\n+  public String getFileName() {\n+    return this.file;\n+  }\n+\n+  /**\n+   * read data from tsFileInput, from the current position (if position = -1), or the given\n+   * position. <br> if position = -1, the tsFileInput's position will be changed to the current\n+   * position + real data size that been read. Other wise, the tsFileInput's position is not\n+   * changed.\n+   *\n+   * @param position the start position of data in the tsFileInput, or the current position if\n+   * position = -1\n+   * @param size the size of data that want to read\n+   * @return data that been read.\n+   */\n+  private ByteBuffer readData(long position, int size) throws IOException {\n+    ByteBuffer buffer = ByteBuffer.allocate(size);\n+    if (position == -1) {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    } else {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer, position, size) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    }\n+    buffer.flip();\n+    return buffer;\n+  }\n+\n+  /**\n+   * upgrade file and resource, return the boolean value whether upgrade task completes\n+   * @throws IOException, WriteProcessException \n+   */\n+  public boolean upgradeFile(List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    File oldTsFile = FSFactoryProducer.getFSFactory().getFile(this.file);\n+\n+    // check if the old TsFile has correct header \n+    if (!fileCheck(oldTsFile)) {\n+      return false;\n+    }\n+\n+    // ChunkGroupOffset -> version\n+    Map<Long, Long> oldVersionInfo = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 318}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY2MTkyNA==", "bodyText": "Can you please tell me the details?", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422661924", "createdAt": "2020-05-10T15:41:18Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "diffHunk": "@@ -0,0 +1,624 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import org.apache.iotdb.db.engine.StorageEngine;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.common.conf.TSFileDescriptor;\n+import org.apache.iotdb.tsfile.compress.IUnCompressor;\n+import org.apache.iotdb.tsfile.encoding.decoder.Decoder;\n+import org.apache.iotdb.tsfile.exception.write.PageException;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+import org.apache.iotdb.tsfile.file.MetaMarker;\n+import org.apache.iotdb.tsfile.file.footer.ChunkGroupFooter;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkGroupMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.CompressionType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSEncoding;\n+import org.apache.iotdb.tsfile.fileSystem.FSFactoryProducer;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.reader.LocalTsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.page.PageReader;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.write.chunk.ChunkWriterImpl;\n+import org.apache.iotdb.tsfile.write.chunk.IChunkWriter;\n+import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n+import org.apache.iotdb.tsfile.write.writer.TsFileIOWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+public class TsfileOnlineUpgradeTool implements AutoCloseable {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(TsfileOnlineUpgradeTool.class);\n+\n+  private TsFileInput tsFileInput;\n+  private long fileMetadataPos;\n+  private int fileMetadataSize;\n+  private ByteBuffer markerBuffer = ByteBuffer.allocate(Byte.BYTES);\n+  protected String file;\n+  \n+  // PartitionId -> TsFileIOWriter \n+  private Map<Long, TsFileIOWriter> partitionWriterMap;\n+\n+  /**\n+   * Create a file reader of the given file. The reader will read the tail of the file to get the\n+   * file metadata size.Then the reader will skip the first TSFileConfig.OLD_MAGIC_STRING.length()\n+   * bytes of the file for preparing reading real data.\n+   *\n+   * @param file the data file\n+   * @throws IOException If some I/O error occurs\n+   */\n+  public TsfileOnlineUpgradeTool(String file) throws IOException {\n+    this(file, true);\n+  }\n+\n+  /**\n+   * construct function for TsfileOnlineUpgradeTool.\n+   *\n+   * @param file -given file name\n+   * @param loadMetadataSize -load meta data size\n+   */\n+  public TsfileOnlineUpgradeTool(String file, boolean loadMetadataSize) throws IOException {\n+    this.file = file;\n+    final java.nio.file.Path path = Paths.get(file);\n+    tsFileInput = new LocalTsFileInput(path);\n+    partitionWriterMap = new HashMap<>();\n+    try {\n+      if (loadMetadataSize) {\n+        loadMetadataSize();\n+      }\n+    } catch (Exception e) {\n+      tsFileInput.close();\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * \n+   */\n+  public void loadMetadataSize() throws IOException {\n+    ByteBuffer metadataSize = ByteBuffer.allocate(Integer.BYTES);\n+    tsFileInput.read(metadataSize,\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES);\n+    metadataSize.flip();\n+    // read file metadata size and position\n+    fileMetadataSize = ReadWriteIOUtils.readInt(metadataSize);\n+    fileMetadataPos =\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES\n+            - fileMetadataSize;\n+    // skip the magic header\n+    position(TSFileConfig.MAGIC_STRING.length());\n+  }\n+\n+  public String readTailMagic() throws IOException {\n+    long totalSize = tsFileInput.size();\n+\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    tsFileInput.read(magicStringBytes, totalSize - TSFileConfig.MAGIC_STRING.length());\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * whether the file is a complete TsFile: only if the head magic and tail magic string exists.\n+   */\n+  public boolean isComplete() throws IOException {\n+    return tsFileInput.size() >= TSFileConfig.MAGIC_STRING.length() * 2 && readTailMagic()\n+        .equals(readHeadMagic());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public String readHeadMagic() throws IOException {\n+    return readHeadMagic(false);\n+  }\n+\n+  /**\n+   * @param movePosition whether move the position of the file reader after reading the magic header\n+   * to the end of the magic head string.\n+   */\n+  public String readHeadMagic(boolean movePosition) throws IOException {\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    if (movePosition) {\n+      tsFileInput.position(0);\n+      tsFileInput.read(magicStringBytes);\n+    } else {\n+      tsFileInput.read(magicStringBytes, 0);\n+    }\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * this function reads version number and checks compatibility of TsFile.\n+   */\n+  public String readVersionNumber() throws IOException {\n+    ByteBuffer versionNumberBytes = ByteBuffer\n+        .allocate(TSFileConfig.VERSION_NUMBER.getBytes().length);\n+    tsFileInput.position(TSFileConfig.MAGIC_STRING.getBytes().length);\n+    tsFileInput.read(versionNumberBytes);\n+    versionNumberBytes.flip();\n+    return new String(versionNumberBytes.array());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsFileMetadata readFileMetadata() throws IOException {\n+    return OldTsFileMetadata.deserializeFrom(readData(fileMetadataPos, fileMetadataSize));\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsDeviceMetadata readTsDeviceMetaData(OldTsDeviceMetadataIndex index) throws IOException {\n+    return OldTsDeviceMetadata.deserializeFrom(readData(index.getOffset(), index.getLen()));\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_GROUP_FOOTER. <br>\n+   * This method is not threadsafe.\n+   *\n+   * @return a CHUNK_GROUP_FOOTER\n+   * @throws IOException io error\n+   */\n+  public ChunkGroupFooter readChunkGroupFooter() throws IOException {\n+    return ChunkGroupFooter.deserializeFrom(tsFileInput.wrapAsInputStream(), true);\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_HEADER. <br> This\n+   * method is not threadsafe.\n+   *\n+   * @return a CHUNK_HEADER\n+   * @throws IOException io error\n+   */\n+  public ChunkHeader readChunkHeader() throws IOException {\n+    return ChunkHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), true, true);\n+  }\n+\n+  /**\n+   * not thread safe.\n+   *\n+   * @param type given tsfile data type\n+   */\n+  public PageHeader readPageHeader(TSDataType type) throws IOException {\n+    return PageHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), type, true);\n+  }\n+\n+  public ByteBuffer readPage(PageHeader header, CompressionType type)\n+      throws IOException {\n+    ByteBuffer buffer = readData(-1, header.getCompressedSize());\n+    IUnCompressor unCompressor = IUnCompressor.getUnCompressor(type);\n+    ByteBuffer uncompressedBuffer = ByteBuffer.allocate(header.getUncompressedSize());\n+    if (type == CompressionType.UNCOMPRESSED) {\n+      return buffer;\n+    }\n+    unCompressor.uncompress(buffer.array(), buffer.position(), buffer.remaining(),\n+        uncompressedBuffer.array(),\n+        0);\n+    return uncompressedBuffer;\n+  }\n+  \n+  public ByteBuffer readCompressedPage(PageHeader header) throws IOException {\n+    return readData(-1, header.getCompressedSize());\n+  }\n+\n+  public long position() throws IOException {\n+    return tsFileInput.position();\n+  }\n+\n+  public void position(long offset) throws IOException {\n+    tsFileInput.position(offset);\n+  }\n+\n+  /**\n+   * read one byte from the input. <br> this method is not thread safe\n+   */\n+  public byte readMarker() throws IOException {\n+    markerBuffer.clear();\n+    if (ReadWriteIOUtils.readAsPossible(tsFileInput, markerBuffer) == 0) {\n+      throw new IOException(\"reach the end of the file.\");\n+    }\n+    markerBuffer.flip();\n+    return markerBuffer.get();\n+  }\n+\n+  public byte readMarker(long position) throws IOException {\n+    return readData(position, Byte.BYTES).get();\n+  }\n+\n+  public void close() throws IOException {\n+    this.tsFileInput.close();\n+  }\n+\n+  public String getFileName() {\n+    return this.file;\n+  }\n+\n+  /**\n+   * read data from tsFileInput, from the current position (if position = -1), or the given\n+   * position. <br> if position = -1, the tsFileInput's position will be changed to the current\n+   * position + real data size that been read. Other wise, the tsFileInput's position is not\n+   * changed.\n+   *\n+   * @param position the start position of data in the tsFileInput, or the current position if\n+   * position = -1\n+   * @param size the size of data that want to read\n+   * @return data that been read.\n+   */\n+  private ByteBuffer readData(long position, int size) throws IOException {\n+    ByteBuffer buffer = ByteBuffer.allocate(size);\n+    if (position == -1) {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    } else {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer, position, size) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    }\n+    buffer.flip();\n+    return buffer;\n+  }\n+\n+  /**\n+   * upgrade file and resource, return the boolean value whether upgrade task completes\n+   * @throws IOException, WriteProcessException \n+   */\n+  public boolean upgradeFile(List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    File oldTsFile = FSFactoryProducer.getFSFactory().getFile(this.file);\n+\n+    // check if the old TsFile has correct header \n+    if (!fileCheck(oldTsFile)) {\n+      return false;\n+    }\n+\n+    // ChunkGroupOffset -> version\n+    Map<Long, Long> oldVersionInfo = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwMzQwOQ=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 318}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTM5NzkzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNzo0NzoyMlrOGTBrUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNTo0MDowMVrOGTFOeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwMzYwMg==", "bodyText": "I prefer to do this check for each page header.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422603602", "createdAt": "2020-05-10T07:47:22Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "diffHunk": "@@ -0,0 +1,624 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import org.apache.iotdb.db.engine.StorageEngine;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.common.conf.TSFileDescriptor;\n+import org.apache.iotdb.tsfile.compress.IUnCompressor;\n+import org.apache.iotdb.tsfile.encoding.decoder.Decoder;\n+import org.apache.iotdb.tsfile.exception.write.PageException;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+import org.apache.iotdb.tsfile.file.MetaMarker;\n+import org.apache.iotdb.tsfile.file.footer.ChunkGroupFooter;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkGroupMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.CompressionType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSEncoding;\n+import org.apache.iotdb.tsfile.fileSystem.FSFactoryProducer;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.reader.LocalTsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.page.PageReader;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.write.chunk.ChunkWriterImpl;\n+import org.apache.iotdb.tsfile.write.chunk.IChunkWriter;\n+import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n+import org.apache.iotdb.tsfile.write.writer.TsFileIOWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+public class TsfileOnlineUpgradeTool implements AutoCloseable {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(TsfileOnlineUpgradeTool.class);\n+\n+  private TsFileInput tsFileInput;\n+  private long fileMetadataPos;\n+  private int fileMetadataSize;\n+  private ByteBuffer markerBuffer = ByteBuffer.allocate(Byte.BYTES);\n+  protected String file;\n+  \n+  // PartitionId -> TsFileIOWriter \n+  private Map<Long, TsFileIOWriter> partitionWriterMap;\n+\n+  /**\n+   * Create a file reader of the given file. The reader will read the tail of the file to get the\n+   * file metadata size.Then the reader will skip the first TSFileConfig.OLD_MAGIC_STRING.length()\n+   * bytes of the file for preparing reading real data.\n+   *\n+   * @param file the data file\n+   * @throws IOException If some I/O error occurs\n+   */\n+  public TsfileOnlineUpgradeTool(String file) throws IOException {\n+    this(file, true);\n+  }\n+\n+  /**\n+   * construct function for TsfileOnlineUpgradeTool.\n+   *\n+   * @param file -given file name\n+   * @param loadMetadataSize -load meta data size\n+   */\n+  public TsfileOnlineUpgradeTool(String file, boolean loadMetadataSize) throws IOException {\n+    this.file = file;\n+    final java.nio.file.Path path = Paths.get(file);\n+    tsFileInput = new LocalTsFileInput(path);\n+    partitionWriterMap = new HashMap<>();\n+    try {\n+      if (loadMetadataSize) {\n+        loadMetadataSize();\n+      }\n+    } catch (Exception e) {\n+      tsFileInput.close();\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * \n+   */\n+  public void loadMetadataSize() throws IOException {\n+    ByteBuffer metadataSize = ByteBuffer.allocate(Integer.BYTES);\n+    tsFileInput.read(metadataSize,\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES);\n+    metadataSize.flip();\n+    // read file metadata size and position\n+    fileMetadataSize = ReadWriteIOUtils.readInt(metadataSize);\n+    fileMetadataPos =\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES\n+            - fileMetadataSize;\n+    // skip the magic header\n+    position(TSFileConfig.MAGIC_STRING.length());\n+  }\n+\n+  public String readTailMagic() throws IOException {\n+    long totalSize = tsFileInput.size();\n+\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    tsFileInput.read(magicStringBytes, totalSize - TSFileConfig.MAGIC_STRING.length());\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * whether the file is a complete TsFile: only if the head magic and tail magic string exists.\n+   */\n+  public boolean isComplete() throws IOException {\n+    return tsFileInput.size() >= TSFileConfig.MAGIC_STRING.length() * 2 && readTailMagic()\n+        .equals(readHeadMagic());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public String readHeadMagic() throws IOException {\n+    return readHeadMagic(false);\n+  }\n+\n+  /**\n+   * @param movePosition whether move the position of the file reader after reading the magic header\n+   * to the end of the magic head string.\n+   */\n+  public String readHeadMagic(boolean movePosition) throws IOException {\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    if (movePosition) {\n+      tsFileInput.position(0);\n+      tsFileInput.read(magicStringBytes);\n+    } else {\n+      tsFileInput.read(magicStringBytes, 0);\n+    }\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * this function reads version number and checks compatibility of TsFile.\n+   */\n+  public String readVersionNumber() throws IOException {\n+    ByteBuffer versionNumberBytes = ByteBuffer\n+        .allocate(TSFileConfig.VERSION_NUMBER.getBytes().length);\n+    tsFileInput.position(TSFileConfig.MAGIC_STRING.getBytes().length);\n+    tsFileInput.read(versionNumberBytes);\n+    versionNumberBytes.flip();\n+    return new String(versionNumberBytes.array());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsFileMetadata readFileMetadata() throws IOException {\n+    return OldTsFileMetadata.deserializeFrom(readData(fileMetadataPos, fileMetadataSize));\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsDeviceMetadata readTsDeviceMetaData(OldTsDeviceMetadataIndex index) throws IOException {\n+    return OldTsDeviceMetadata.deserializeFrom(readData(index.getOffset(), index.getLen()));\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_GROUP_FOOTER. <br>\n+   * This method is not threadsafe.\n+   *\n+   * @return a CHUNK_GROUP_FOOTER\n+   * @throws IOException io error\n+   */\n+  public ChunkGroupFooter readChunkGroupFooter() throws IOException {\n+    return ChunkGroupFooter.deserializeFrom(tsFileInput.wrapAsInputStream(), true);\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_HEADER. <br> This\n+   * method is not threadsafe.\n+   *\n+   * @return a CHUNK_HEADER\n+   * @throws IOException io error\n+   */\n+  public ChunkHeader readChunkHeader() throws IOException {\n+    return ChunkHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), true, true);\n+  }\n+\n+  /**\n+   * not thread safe.\n+   *\n+   * @param type given tsfile data type\n+   */\n+  public PageHeader readPageHeader(TSDataType type) throws IOException {\n+    return PageHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), type, true);\n+  }\n+\n+  public ByteBuffer readPage(PageHeader header, CompressionType type)\n+      throws IOException {\n+    ByteBuffer buffer = readData(-1, header.getCompressedSize());\n+    IUnCompressor unCompressor = IUnCompressor.getUnCompressor(type);\n+    ByteBuffer uncompressedBuffer = ByteBuffer.allocate(header.getUncompressedSize());\n+    if (type == CompressionType.UNCOMPRESSED) {\n+      return buffer;\n+    }\n+    unCompressor.uncompress(buffer.array(), buffer.position(), buffer.remaining(),\n+        uncompressedBuffer.array(),\n+        0);\n+    return uncompressedBuffer;\n+  }\n+  \n+  public ByteBuffer readCompressedPage(PageHeader header) throws IOException {\n+    return readData(-1, header.getCompressedSize());\n+  }\n+\n+  public long position() throws IOException {\n+    return tsFileInput.position();\n+  }\n+\n+  public void position(long offset) throws IOException {\n+    tsFileInput.position(offset);\n+  }\n+\n+  /**\n+   * read one byte from the input. <br> this method is not thread safe\n+   */\n+  public byte readMarker() throws IOException {\n+    markerBuffer.clear();\n+    if (ReadWriteIOUtils.readAsPossible(tsFileInput, markerBuffer) == 0) {\n+      throw new IOException(\"reach the end of the file.\");\n+    }\n+    markerBuffer.flip();\n+    return markerBuffer.get();\n+  }\n+\n+  public byte readMarker(long position) throws IOException {\n+    return readData(position, Byte.BYTES).get();\n+  }\n+\n+  public void close() throws IOException {\n+    this.tsFileInput.close();\n+  }\n+\n+  public String getFileName() {\n+    return this.file;\n+  }\n+\n+  /**\n+   * read data from tsFileInput, from the current position (if position = -1), or the given\n+   * position. <br> if position = -1, the tsFileInput's position will be changed to the current\n+   * position + real data size that been read. Other wise, the tsFileInput's position is not\n+   * changed.\n+   *\n+   * @param position the start position of data in the tsFileInput, or the current position if\n+   * position = -1\n+   * @param size the size of data that want to read\n+   * @return data that been read.\n+   */\n+  private ByteBuffer readData(long position, int size) throws IOException {\n+    ByteBuffer buffer = ByteBuffer.allocate(size);\n+    if (position == -1) {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    } else {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer, position, size) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    }\n+    buffer.flip();\n+    return buffer;\n+  }\n+\n+  /**\n+   * upgrade file and resource, return the boolean value whether upgrade task completes\n+   * @throws IOException, WriteProcessException \n+   */\n+  public boolean upgradeFile(List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    File oldTsFile = FSFactoryProducer.getFSFactory().getFile(this.file);\n+\n+    // check if the old TsFile has correct header \n+    if (!fileCheck(oldTsFile)) {\n+      return false;\n+    }\n+\n+    // ChunkGroupOffset -> version\n+    Map<Long, Long> oldVersionInfo = new HashMap<>();\n+\n+    // ChunkGroupOffset -> time partition, record the offsets of chunk group that data are in same partition\n+    Map<Long, Long> chunkGroupTimePartitionInfo = new HashMap<>();\n+\n+    // scan metadata to get version Info and chunkGroupTimePartitionInfo\n+    scanMetadata(oldVersionInfo, chunkGroupTimePartitionInfo);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 324}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY2MTc1Mg==", "bodyText": "I don't get what you mean, can you please share more details?", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422661752", "createdAt": "2020-05-10T15:40:01Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "diffHunk": "@@ -0,0 +1,624 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import org.apache.iotdb.db.engine.StorageEngine;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.common.conf.TSFileDescriptor;\n+import org.apache.iotdb.tsfile.compress.IUnCompressor;\n+import org.apache.iotdb.tsfile.encoding.decoder.Decoder;\n+import org.apache.iotdb.tsfile.exception.write.PageException;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+import org.apache.iotdb.tsfile.file.MetaMarker;\n+import org.apache.iotdb.tsfile.file.footer.ChunkGroupFooter;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkGroupMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.CompressionType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSEncoding;\n+import org.apache.iotdb.tsfile.fileSystem.FSFactoryProducer;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.reader.LocalTsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.page.PageReader;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.write.chunk.ChunkWriterImpl;\n+import org.apache.iotdb.tsfile.write.chunk.IChunkWriter;\n+import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n+import org.apache.iotdb.tsfile.write.writer.TsFileIOWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+public class TsfileOnlineUpgradeTool implements AutoCloseable {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(TsfileOnlineUpgradeTool.class);\n+\n+  private TsFileInput tsFileInput;\n+  private long fileMetadataPos;\n+  private int fileMetadataSize;\n+  private ByteBuffer markerBuffer = ByteBuffer.allocate(Byte.BYTES);\n+  protected String file;\n+  \n+  // PartitionId -> TsFileIOWriter \n+  private Map<Long, TsFileIOWriter> partitionWriterMap;\n+\n+  /**\n+   * Create a file reader of the given file. The reader will read the tail of the file to get the\n+   * file metadata size.Then the reader will skip the first TSFileConfig.OLD_MAGIC_STRING.length()\n+   * bytes of the file for preparing reading real data.\n+   *\n+   * @param file the data file\n+   * @throws IOException If some I/O error occurs\n+   */\n+  public TsfileOnlineUpgradeTool(String file) throws IOException {\n+    this(file, true);\n+  }\n+\n+  /**\n+   * construct function for TsfileOnlineUpgradeTool.\n+   *\n+   * @param file -given file name\n+   * @param loadMetadataSize -load meta data size\n+   */\n+  public TsfileOnlineUpgradeTool(String file, boolean loadMetadataSize) throws IOException {\n+    this.file = file;\n+    final java.nio.file.Path path = Paths.get(file);\n+    tsFileInput = new LocalTsFileInput(path);\n+    partitionWriterMap = new HashMap<>();\n+    try {\n+      if (loadMetadataSize) {\n+        loadMetadataSize();\n+      }\n+    } catch (Exception e) {\n+      tsFileInput.close();\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * \n+   */\n+  public void loadMetadataSize() throws IOException {\n+    ByteBuffer metadataSize = ByteBuffer.allocate(Integer.BYTES);\n+    tsFileInput.read(metadataSize,\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES);\n+    metadataSize.flip();\n+    // read file metadata size and position\n+    fileMetadataSize = ReadWriteIOUtils.readInt(metadataSize);\n+    fileMetadataPos =\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES\n+            - fileMetadataSize;\n+    // skip the magic header\n+    position(TSFileConfig.MAGIC_STRING.length());\n+  }\n+\n+  public String readTailMagic() throws IOException {\n+    long totalSize = tsFileInput.size();\n+\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    tsFileInput.read(magicStringBytes, totalSize - TSFileConfig.MAGIC_STRING.length());\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * whether the file is a complete TsFile: only if the head magic and tail magic string exists.\n+   */\n+  public boolean isComplete() throws IOException {\n+    return tsFileInput.size() >= TSFileConfig.MAGIC_STRING.length() * 2 && readTailMagic()\n+        .equals(readHeadMagic());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public String readHeadMagic() throws IOException {\n+    return readHeadMagic(false);\n+  }\n+\n+  /**\n+   * @param movePosition whether move the position of the file reader after reading the magic header\n+   * to the end of the magic head string.\n+   */\n+  public String readHeadMagic(boolean movePosition) throws IOException {\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    if (movePosition) {\n+      tsFileInput.position(0);\n+      tsFileInput.read(magicStringBytes);\n+    } else {\n+      tsFileInput.read(magicStringBytes, 0);\n+    }\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * this function reads version number and checks compatibility of TsFile.\n+   */\n+  public String readVersionNumber() throws IOException {\n+    ByteBuffer versionNumberBytes = ByteBuffer\n+        .allocate(TSFileConfig.VERSION_NUMBER.getBytes().length);\n+    tsFileInput.position(TSFileConfig.MAGIC_STRING.getBytes().length);\n+    tsFileInput.read(versionNumberBytes);\n+    versionNumberBytes.flip();\n+    return new String(versionNumberBytes.array());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsFileMetadata readFileMetadata() throws IOException {\n+    return OldTsFileMetadata.deserializeFrom(readData(fileMetadataPos, fileMetadataSize));\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsDeviceMetadata readTsDeviceMetaData(OldTsDeviceMetadataIndex index) throws IOException {\n+    return OldTsDeviceMetadata.deserializeFrom(readData(index.getOffset(), index.getLen()));\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_GROUP_FOOTER. <br>\n+   * This method is not threadsafe.\n+   *\n+   * @return a CHUNK_GROUP_FOOTER\n+   * @throws IOException io error\n+   */\n+  public ChunkGroupFooter readChunkGroupFooter() throws IOException {\n+    return ChunkGroupFooter.deserializeFrom(tsFileInput.wrapAsInputStream(), true);\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_HEADER. <br> This\n+   * method is not threadsafe.\n+   *\n+   * @return a CHUNK_HEADER\n+   * @throws IOException io error\n+   */\n+  public ChunkHeader readChunkHeader() throws IOException {\n+    return ChunkHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), true, true);\n+  }\n+\n+  /**\n+   * not thread safe.\n+   *\n+   * @param type given tsfile data type\n+   */\n+  public PageHeader readPageHeader(TSDataType type) throws IOException {\n+    return PageHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), type, true);\n+  }\n+\n+  public ByteBuffer readPage(PageHeader header, CompressionType type)\n+      throws IOException {\n+    ByteBuffer buffer = readData(-1, header.getCompressedSize());\n+    IUnCompressor unCompressor = IUnCompressor.getUnCompressor(type);\n+    ByteBuffer uncompressedBuffer = ByteBuffer.allocate(header.getUncompressedSize());\n+    if (type == CompressionType.UNCOMPRESSED) {\n+      return buffer;\n+    }\n+    unCompressor.uncompress(buffer.array(), buffer.position(), buffer.remaining(),\n+        uncompressedBuffer.array(),\n+        0);\n+    return uncompressedBuffer;\n+  }\n+  \n+  public ByteBuffer readCompressedPage(PageHeader header) throws IOException {\n+    return readData(-1, header.getCompressedSize());\n+  }\n+\n+  public long position() throws IOException {\n+    return tsFileInput.position();\n+  }\n+\n+  public void position(long offset) throws IOException {\n+    tsFileInput.position(offset);\n+  }\n+\n+  /**\n+   * read one byte from the input. <br> this method is not thread safe\n+   */\n+  public byte readMarker() throws IOException {\n+    markerBuffer.clear();\n+    if (ReadWriteIOUtils.readAsPossible(tsFileInput, markerBuffer) == 0) {\n+      throw new IOException(\"reach the end of the file.\");\n+    }\n+    markerBuffer.flip();\n+    return markerBuffer.get();\n+  }\n+\n+  public byte readMarker(long position) throws IOException {\n+    return readData(position, Byte.BYTES).get();\n+  }\n+\n+  public void close() throws IOException {\n+    this.tsFileInput.close();\n+  }\n+\n+  public String getFileName() {\n+    return this.file;\n+  }\n+\n+  /**\n+   * read data from tsFileInput, from the current position (if position = -1), or the given\n+   * position. <br> if position = -1, the tsFileInput's position will be changed to the current\n+   * position + real data size that been read. Other wise, the tsFileInput's position is not\n+   * changed.\n+   *\n+   * @param position the start position of data in the tsFileInput, or the current position if\n+   * position = -1\n+   * @param size the size of data that want to read\n+   * @return data that been read.\n+   */\n+  private ByteBuffer readData(long position, int size) throws IOException {\n+    ByteBuffer buffer = ByteBuffer.allocate(size);\n+    if (position == -1) {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    } else {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer, position, size) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    }\n+    buffer.flip();\n+    return buffer;\n+  }\n+\n+  /**\n+   * upgrade file and resource, return the boolean value whether upgrade task completes\n+   * @throws IOException, WriteProcessException \n+   */\n+  public boolean upgradeFile(List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    File oldTsFile = FSFactoryProducer.getFSFactory().getFile(this.file);\n+\n+    // check if the old TsFile has correct header \n+    if (!fileCheck(oldTsFile)) {\n+      return false;\n+    }\n+\n+    // ChunkGroupOffset -> version\n+    Map<Long, Long> oldVersionInfo = new HashMap<>();\n+\n+    // ChunkGroupOffset -> time partition, record the offsets of chunk group that data are in same partition\n+    Map<Long, Long> chunkGroupTimePartitionInfo = new HashMap<>();\n+\n+    // scan metadata to get version Info and chunkGroupTimePartitionInfo\n+    scanMetadata(oldVersionInfo, chunkGroupTimePartitionInfo);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwMzYwMg=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 324}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTQwMDgzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNzo1MDoxN1rOGTBssA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNToxODowOFrOGTFDxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwMzk1Mg==", "bodyText": "no need to put again", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422603952", "createdAt": "2020-05-10T07:50:17Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "diffHunk": "@@ -0,0 +1,624 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import org.apache.iotdb.db.engine.StorageEngine;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.common.conf.TSFileDescriptor;\n+import org.apache.iotdb.tsfile.compress.IUnCompressor;\n+import org.apache.iotdb.tsfile.encoding.decoder.Decoder;\n+import org.apache.iotdb.tsfile.exception.write.PageException;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+import org.apache.iotdb.tsfile.file.MetaMarker;\n+import org.apache.iotdb.tsfile.file.footer.ChunkGroupFooter;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkGroupMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.CompressionType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSEncoding;\n+import org.apache.iotdb.tsfile.fileSystem.FSFactoryProducer;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.reader.LocalTsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.page.PageReader;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.write.chunk.ChunkWriterImpl;\n+import org.apache.iotdb.tsfile.write.chunk.IChunkWriter;\n+import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n+import org.apache.iotdb.tsfile.write.writer.TsFileIOWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+public class TsfileOnlineUpgradeTool implements AutoCloseable {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(TsfileOnlineUpgradeTool.class);\n+\n+  private TsFileInput tsFileInput;\n+  private long fileMetadataPos;\n+  private int fileMetadataSize;\n+  private ByteBuffer markerBuffer = ByteBuffer.allocate(Byte.BYTES);\n+  protected String file;\n+  \n+  // PartitionId -> TsFileIOWriter \n+  private Map<Long, TsFileIOWriter> partitionWriterMap;\n+\n+  /**\n+   * Create a file reader of the given file. The reader will read the tail of the file to get the\n+   * file metadata size.Then the reader will skip the first TSFileConfig.OLD_MAGIC_STRING.length()\n+   * bytes of the file for preparing reading real data.\n+   *\n+   * @param file the data file\n+   * @throws IOException If some I/O error occurs\n+   */\n+  public TsfileOnlineUpgradeTool(String file) throws IOException {\n+    this(file, true);\n+  }\n+\n+  /**\n+   * construct function for TsfileOnlineUpgradeTool.\n+   *\n+   * @param file -given file name\n+   * @param loadMetadataSize -load meta data size\n+   */\n+  public TsfileOnlineUpgradeTool(String file, boolean loadMetadataSize) throws IOException {\n+    this.file = file;\n+    final java.nio.file.Path path = Paths.get(file);\n+    tsFileInput = new LocalTsFileInput(path);\n+    partitionWriterMap = new HashMap<>();\n+    try {\n+      if (loadMetadataSize) {\n+        loadMetadataSize();\n+      }\n+    } catch (Exception e) {\n+      tsFileInput.close();\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * \n+   */\n+  public void loadMetadataSize() throws IOException {\n+    ByteBuffer metadataSize = ByteBuffer.allocate(Integer.BYTES);\n+    tsFileInput.read(metadataSize,\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES);\n+    metadataSize.flip();\n+    // read file metadata size and position\n+    fileMetadataSize = ReadWriteIOUtils.readInt(metadataSize);\n+    fileMetadataPos =\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES\n+            - fileMetadataSize;\n+    // skip the magic header\n+    position(TSFileConfig.MAGIC_STRING.length());\n+  }\n+\n+  public String readTailMagic() throws IOException {\n+    long totalSize = tsFileInput.size();\n+\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    tsFileInput.read(magicStringBytes, totalSize - TSFileConfig.MAGIC_STRING.length());\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * whether the file is a complete TsFile: only if the head magic and tail magic string exists.\n+   */\n+  public boolean isComplete() throws IOException {\n+    return tsFileInput.size() >= TSFileConfig.MAGIC_STRING.length() * 2 && readTailMagic()\n+        .equals(readHeadMagic());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public String readHeadMagic() throws IOException {\n+    return readHeadMagic(false);\n+  }\n+\n+  /**\n+   * @param movePosition whether move the position of the file reader after reading the magic header\n+   * to the end of the magic head string.\n+   */\n+  public String readHeadMagic(boolean movePosition) throws IOException {\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    if (movePosition) {\n+      tsFileInput.position(0);\n+      tsFileInput.read(magicStringBytes);\n+    } else {\n+      tsFileInput.read(magicStringBytes, 0);\n+    }\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * this function reads version number and checks compatibility of TsFile.\n+   */\n+  public String readVersionNumber() throws IOException {\n+    ByteBuffer versionNumberBytes = ByteBuffer\n+        .allocate(TSFileConfig.VERSION_NUMBER.getBytes().length);\n+    tsFileInput.position(TSFileConfig.MAGIC_STRING.getBytes().length);\n+    tsFileInput.read(versionNumberBytes);\n+    versionNumberBytes.flip();\n+    return new String(versionNumberBytes.array());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsFileMetadata readFileMetadata() throws IOException {\n+    return OldTsFileMetadata.deserializeFrom(readData(fileMetadataPos, fileMetadataSize));\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsDeviceMetadata readTsDeviceMetaData(OldTsDeviceMetadataIndex index) throws IOException {\n+    return OldTsDeviceMetadata.deserializeFrom(readData(index.getOffset(), index.getLen()));\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_GROUP_FOOTER. <br>\n+   * This method is not threadsafe.\n+   *\n+   * @return a CHUNK_GROUP_FOOTER\n+   * @throws IOException io error\n+   */\n+  public ChunkGroupFooter readChunkGroupFooter() throws IOException {\n+    return ChunkGroupFooter.deserializeFrom(tsFileInput.wrapAsInputStream(), true);\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_HEADER. <br> This\n+   * method is not threadsafe.\n+   *\n+   * @return a CHUNK_HEADER\n+   * @throws IOException io error\n+   */\n+  public ChunkHeader readChunkHeader() throws IOException {\n+    return ChunkHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), true, true);\n+  }\n+\n+  /**\n+   * not thread safe.\n+   *\n+   * @param type given tsfile data type\n+   */\n+  public PageHeader readPageHeader(TSDataType type) throws IOException {\n+    return PageHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), type, true);\n+  }\n+\n+  public ByteBuffer readPage(PageHeader header, CompressionType type)\n+      throws IOException {\n+    ByteBuffer buffer = readData(-1, header.getCompressedSize());\n+    IUnCompressor unCompressor = IUnCompressor.getUnCompressor(type);\n+    ByteBuffer uncompressedBuffer = ByteBuffer.allocate(header.getUncompressedSize());\n+    if (type == CompressionType.UNCOMPRESSED) {\n+      return buffer;\n+    }\n+    unCompressor.uncompress(buffer.array(), buffer.position(), buffer.remaining(),\n+        uncompressedBuffer.array(),\n+        0);\n+    return uncompressedBuffer;\n+  }\n+  \n+  public ByteBuffer readCompressedPage(PageHeader header) throws IOException {\n+    return readData(-1, header.getCompressedSize());\n+  }\n+\n+  public long position() throws IOException {\n+    return tsFileInput.position();\n+  }\n+\n+  public void position(long offset) throws IOException {\n+    tsFileInput.position(offset);\n+  }\n+\n+  /**\n+   * read one byte from the input. <br> this method is not thread safe\n+   */\n+  public byte readMarker() throws IOException {\n+    markerBuffer.clear();\n+    if (ReadWriteIOUtils.readAsPossible(tsFileInput, markerBuffer) == 0) {\n+      throw new IOException(\"reach the end of the file.\");\n+    }\n+    markerBuffer.flip();\n+    return markerBuffer.get();\n+  }\n+\n+  public byte readMarker(long position) throws IOException {\n+    return readData(position, Byte.BYTES).get();\n+  }\n+\n+  public void close() throws IOException {\n+    this.tsFileInput.close();\n+  }\n+\n+  public String getFileName() {\n+    return this.file;\n+  }\n+\n+  /**\n+   * read data from tsFileInput, from the current position (if position = -1), or the given\n+   * position. <br> if position = -1, the tsFileInput's position will be changed to the current\n+   * position + real data size that been read. Other wise, the tsFileInput's position is not\n+   * changed.\n+   *\n+   * @param position the start position of data in the tsFileInput, or the current position if\n+   * position = -1\n+   * @param size the size of data that want to read\n+   * @return data that been read.\n+   */\n+  private ByteBuffer readData(long position, int size) throws IOException {\n+    ByteBuffer buffer = ByteBuffer.allocate(size);\n+    if (position == -1) {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    } else {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer, position, size) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    }\n+    buffer.flip();\n+    return buffer;\n+  }\n+\n+  /**\n+   * upgrade file and resource, return the boolean value whether upgrade task completes\n+   * @throws IOException, WriteProcessException \n+   */\n+  public boolean upgradeFile(List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    File oldTsFile = FSFactoryProducer.getFSFactory().getFile(this.file);\n+\n+    // check if the old TsFile has correct header \n+    if (!fileCheck(oldTsFile)) {\n+      return false;\n+    }\n+\n+    // ChunkGroupOffset -> version\n+    Map<Long, Long> oldVersionInfo = new HashMap<>();\n+\n+    // ChunkGroupOffset -> time partition, record the offsets of chunk group that data are in same partition\n+    Map<Long, Long> chunkGroupTimePartitionInfo = new HashMap<>();\n+\n+    // scan metadata to get version Info and chunkGroupTimePartitionInfo\n+    scanMetadata(oldVersionInfo, chunkGroupTimePartitionInfo);\n+    \n+    // start to scan chunks and chunkGroups\n+    long startOffsetOfChunkGroup = 0;\n+    boolean newChunkGroup = true;\n+    long versionOfChunkGroup = 0;\n+    boolean chunkGroupInSamePartition = false;\n+    List<ChunkGroupMetadata> newMetaData = new ArrayList<>();\n+    List<List<PageHeader>> pageHeadersInChunkGroup = new ArrayList<>();\n+    List<List<ByteBuffer>> dataInChunkGroup = new ArrayList<>();\n+    byte marker;\n+    List<MeasurementSchema> measurementSchemaList = new ArrayList<>();\n+    try {\n+      while ((marker = this.readMarker()) != MetaMarker.SEPARATOR) {\n+        switch (marker) {\n+          case MetaMarker.CHUNK_HEADER:\n+            // this is the first chunk of a new ChunkGroup.\n+            if (newChunkGroup) {\n+              newChunkGroup = false;\n+              startOffsetOfChunkGroup = this.position() - 1;\n+              versionOfChunkGroup = oldVersionInfo.get(startOffsetOfChunkGroup);\n+              chunkGroupInSamePartition = chunkGroupTimePartitionInfo\n+                  .containsKey(startOffsetOfChunkGroup);\n+            }\n+            ChunkHeader header = this.readChunkHeader();\n+            MeasurementSchema measurementSchema = new MeasurementSchema(header.getMeasurementID(),\n+                header.getDataType(),\n+                header.getEncodingType(), \n+                header.getCompressionType());\n+            measurementSchemaList.add(measurementSchema);\n+            List<PageHeader> pageHeadersInChunk = new ArrayList<>();\n+            List<ByteBuffer> dataInChunk = new ArrayList<>();\n+            for (int j = 0; j < header.getNumOfPages(); j++) {\n+              PageHeader pageHeader = readPageHeader(header.getDataType());\n+              ByteBuffer pageData = chunkGroupInSamePartition ? \n+                  readCompressedPage(pageHeader) : readPage(pageHeader, header.getCompressionType());\n+              pageHeadersInChunk.add(pageHeader);\n+              dataInChunk.add(pageData);\n+            }\n+            pageHeadersInChunkGroup.add(pageHeadersInChunk);\n+            dataInChunkGroup.add(dataInChunk);\n+            break;\n+          case MetaMarker.CHUNK_GROUP_FOOTER:\n+            // this is the footer of a ChunkGroup.\n+            ChunkGroupFooter chunkGroupFooter = this.readChunkGroupFooter();\n+            String deviceID = chunkGroupFooter.getDeviceID();\n+            if (chunkGroupInSamePartition) {\n+              quickRewrite(oldTsFile, deviceID, measurementSchemaList, pageHeadersInChunkGroup,\n+                  dataInChunkGroup, versionOfChunkGroup, chunkGroupTimePartitionInfo.get(startOffsetOfChunkGroup));\n+            } else {\n+              rewrite(oldTsFile, deviceID, measurementSchemaList, \n+                dataInChunkGroup, versionOfChunkGroup);\n+            }\n+\n+            pageHeadersInChunkGroup.clear();\n+            dataInChunkGroup.clear();\n+            measurementSchemaList.clear();\n+            newChunkGroup = true;\n+            break;\n+\n+          default:\n+            // the disk file is corrupted, using this file may be dangerous\n+            logger.error(\"Unrecognized marker detected, this file may be corrupted\");\n+            return false;\n+        }\n+      }\n+      // close upgraded tsFiles and generate resources for them\n+      for (TsFileIOWriter tsFileIOWriter : partitionWriterMap.values()) {\n+        upgradedResources.add(endFileAndGenerateResource(tsFileIOWriter));\n+      }\n+      return true;\n+    } catch (IOException e2) {\n+      logger.info(\"TsFile upgrade process cannot proceed at position {} after {} chunk groups \"\n+          + \"recovered, because : {}\", this.position(), newMetaData.size(), e2.getMessage());\n+      return false;\n+    } finally {\n+      if (tsFileInput != null) {\n+        tsFileInput.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   *  Rewrite the chunk group to new TsFile.\n+   *  If data of this chunk group are in different time partitions,\n+   *  create multiple new TsFiles and rewrite data in each partition.\n+   */\n+  private void rewrite(File oldTsFile, String deviceId, List<MeasurementSchema> schemas, \n+      List<List<ByteBuffer>> dataInChunkGroup, long versionOfChunkGroup) \n+          throws IOException {\n+\n+    Map<Long, Map<MeasurementSchema, IChunkWriter>> chunkWritersInChunkGroup = new HashMap<>();\n+    for (int i = 0; i < schemas.size(); i++) {\n+      MeasurementSchema schema = schemas.get(i);\n+      Decoder defaultTimeDecoder = Decoder.getDecoderByType(\n+          TSEncoding.valueOf(TSFileDescriptor.getInstance().getConfig().getTimeEncoder()),\n+          TSDataType.INT64);\n+      Decoder valueDecoder = Decoder\n+          .getDecoderByType(schema.getEncodingType(), schema.getType());\n+      List<ByteBuffer> dataInChunk = dataInChunkGroup.get(i);\n+      for (ByteBuffer pageData : dataInChunk) {\n+        valueDecoder.reset();\n+        PageReader pageReader = new PageReader(pageData, schema.getType(), valueDecoder,\n+            defaultTimeDecoder, null);\n+        BatchData batchData = pageReader.getAllSatisfiedPageData();\n+        while (batchData.hasCurrent()) {\n+          long time = batchData.currentTime();\n+          Object value = batchData.currentValue();\n+          long partition = StorageEngine.getTimePartition(time);\n+          \n+          Map<MeasurementSchema, IChunkWriter> chunkWriters = chunkWritersInChunkGroup.getOrDefault(partition, new HashMap<>());\n+          IChunkWriter chunkWriter = chunkWriters.getOrDefault(schema, new ChunkWriterImpl(schema));\n+          TsFileIOWriter tsFileIOWriter = getOrDefaultTsFileIOWriter(oldTsFile, partition);\n+          partitionWriterMap.put(partition, tsFileIOWriter);\n+          switch (schema.getType()) {\n+            case INT32:\n+              chunkWriter.write(time, (int) value);\n+              break;\n+            case INT64:\n+              chunkWriter.write(time, (long) value);\n+              break;\n+            case FLOAT:\n+              chunkWriter.write(time, (float) value);\n+              break;\n+            case DOUBLE:\n+              chunkWriter.write(time, (double) value);\n+              break;\n+            case BOOLEAN:\n+              chunkWriter.write(time, (boolean) value);\n+              break;\n+            case TEXT:\n+              chunkWriter.write(time, (Binary) value);\n+              break;\n+            default:\n+              throw new UnSupportedDataTypeException(\n+                  String.format(\"Data type %s is not supported.\", schema.getType()));\n+            }\n+          chunkWriters.put(schema, chunkWriter);\n+          chunkWritersInChunkGroup.put(partition, chunkWriters);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 462}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY1OTAxMg==", "bodyText": "I have tried to remove these lines, but I saw the result of upgrading would be wrong. Some data belongs to other time partition is lost.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422659012", "createdAt": "2020-05-10T15:18:08Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "diffHunk": "@@ -0,0 +1,624 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import org.apache.iotdb.db.engine.StorageEngine;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.common.conf.TSFileDescriptor;\n+import org.apache.iotdb.tsfile.compress.IUnCompressor;\n+import org.apache.iotdb.tsfile.encoding.decoder.Decoder;\n+import org.apache.iotdb.tsfile.exception.write.PageException;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+import org.apache.iotdb.tsfile.file.MetaMarker;\n+import org.apache.iotdb.tsfile.file.footer.ChunkGroupFooter;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkGroupMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.CompressionType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSEncoding;\n+import org.apache.iotdb.tsfile.fileSystem.FSFactoryProducer;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.reader.LocalTsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.page.PageReader;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.write.chunk.ChunkWriterImpl;\n+import org.apache.iotdb.tsfile.write.chunk.IChunkWriter;\n+import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n+import org.apache.iotdb.tsfile.write.writer.TsFileIOWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+public class TsfileOnlineUpgradeTool implements AutoCloseable {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(TsfileOnlineUpgradeTool.class);\n+\n+  private TsFileInput tsFileInput;\n+  private long fileMetadataPos;\n+  private int fileMetadataSize;\n+  private ByteBuffer markerBuffer = ByteBuffer.allocate(Byte.BYTES);\n+  protected String file;\n+  \n+  // PartitionId -> TsFileIOWriter \n+  private Map<Long, TsFileIOWriter> partitionWriterMap;\n+\n+  /**\n+   * Create a file reader of the given file. The reader will read the tail of the file to get the\n+   * file metadata size.Then the reader will skip the first TSFileConfig.OLD_MAGIC_STRING.length()\n+   * bytes of the file for preparing reading real data.\n+   *\n+   * @param file the data file\n+   * @throws IOException If some I/O error occurs\n+   */\n+  public TsfileOnlineUpgradeTool(String file) throws IOException {\n+    this(file, true);\n+  }\n+\n+  /**\n+   * construct function for TsfileOnlineUpgradeTool.\n+   *\n+   * @param file -given file name\n+   * @param loadMetadataSize -load meta data size\n+   */\n+  public TsfileOnlineUpgradeTool(String file, boolean loadMetadataSize) throws IOException {\n+    this.file = file;\n+    final java.nio.file.Path path = Paths.get(file);\n+    tsFileInput = new LocalTsFileInput(path);\n+    partitionWriterMap = new HashMap<>();\n+    try {\n+      if (loadMetadataSize) {\n+        loadMetadataSize();\n+      }\n+    } catch (Exception e) {\n+      tsFileInput.close();\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * \n+   */\n+  public void loadMetadataSize() throws IOException {\n+    ByteBuffer metadataSize = ByteBuffer.allocate(Integer.BYTES);\n+    tsFileInput.read(metadataSize,\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES);\n+    metadataSize.flip();\n+    // read file metadata size and position\n+    fileMetadataSize = ReadWriteIOUtils.readInt(metadataSize);\n+    fileMetadataPos =\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES\n+            - fileMetadataSize;\n+    // skip the magic header\n+    position(TSFileConfig.MAGIC_STRING.length());\n+  }\n+\n+  public String readTailMagic() throws IOException {\n+    long totalSize = tsFileInput.size();\n+\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    tsFileInput.read(magicStringBytes, totalSize - TSFileConfig.MAGIC_STRING.length());\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * whether the file is a complete TsFile: only if the head magic and tail magic string exists.\n+   */\n+  public boolean isComplete() throws IOException {\n+    return tsFileInput.size() >= TSFileConfig.MAGIC_STRING.length() * 2 && readTailMagic()\n+        .equals(readHeadMagic());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public String readHeadMagic() throws IOException {\n+    return readHeadMagic(false);\n+  }\n+\n+  /**\n+   * @param movePosition whether move the position of the file reader after reading the magic header\n+   * to the end of the magic head string.\n+   */\n+  public String readHeadMagic(boolean movePosition) throws IOException {\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    if (movePosition) {\n+      tsFileInput.position(0);\n+      tsFileInput.read(magicStringBytes);\n+    } else {\n+      tsFileInput.read(magicStringBytes, 0);\n+    }\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * this function reads version number and checks compatibility of TsFile.\n+   */\n+  public String readVersionNumber() throws IOException {\n+    ByteBuffer versionNumberBytes = ByteBuffer\n+        .allocate(TSFileConfig.VERSION_NUMBER.getBytes().length);\n+    tsFileInput.position(TSFileConfig.MAGIC_STRING.getBytes().length);\n+    tsFileInput.read(versionNumberBytes);\n+    versionNumberBytes.flip();\n+    return new String(versionNumberBytes.array());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsFileMetadata readFileMetadata() throws IOException {\n+    return OldTsFileMetadata.deserializeFrom(readData(fileMetadataPos, fileMetadataSize));\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsDeviceMetadata readTsDeviceMetaData(OldTsDeviceMetadataIndex index) throws IOException {\n+    return OldTsDeviceMetadata.deserializeFrom(readData(index.getOffset(), index.getLen()));\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_GROUP_FOOTER. <br>\n+   * This method is not threadsafe.\n+   *\n+   * @return a CHUNK_GROUP_FOOTER\n+   * @throws IOException io error\n+   */\n+  public ChunkGroupFooter readChunkGroupFooter() throws IOException {\n+    return ChunkGroupFooter.deserializeFrom(tsFileInput.wrapAsInputStream(), true);\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_HEADER. <br> This\n+   * method is not threadsafe.\n+   *\n+   * @return a CHUNK_HEADER\n+   * @throws IOException io error\n+   */\n+  public ChunkHeader readChunkHeader() throws IOException {\n+    return ChunkHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), true, true);\n+  }\n+\n+  /**\n+   * not thread safe.\n+   *\n+   * @param type given tsfile data type\n+   */\n+  public PageHeader readPageHeader(TSDataType type) throws IOException {\n+    return PageHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), type, true);\n+  }\n+\n+  public ByteBuffer readPage(PageHeader header, CompressionType type)\n+      throws IOException {\n+    ByteBuffer buffer = readData(-1, header.getCompressedSize());\n+    IUnCompressor unCompressor = IUnCompressor.getUnCompressor(type);\n+    ByteBuffer uncompressedBuffer = ByteBuffer.allocate(header.getUncompressedSize());\n+    if (type == CompressionType.UNCOMPRESSED) {\n+      return buffer;\n+    }\n+    unCompressor.uncompress(buffer.array(), buffer.position(), buffer.remaining(),\n+        uncompressedBuffer.array(),\n+        0);\n+    return uncompressedBuffer;\n+  }\n+  \n+  public ByteBuffer readCompressedPage(PageHeader header) throws IOException {\n+    return readData(-1, header.getCompressedSize());\n+  }\n+\n+  public long position() throws IOException {\n+    return tsFileInput.position();\n+  }\n+\n+  public void position(long offset) throws IOException {\n+    tsFileInput.position(offset);\n+  }\n+\n+  /**\n+   * read one byte from the input. <br> this method is not thread safe\n+   */\n+  public byte readMarker() throws IOException {\n+    markerBuffer.clear();\n+    if (ReadWriteIOUtils.readAsPossible(tsFileInput, markerBuffer) == 0) {\n+      throw new IOException(\"reach the end of the file.\");\n+    }\n+    markerBuffer.flip();\n+    return markerBuffer.get();\n+  }\n+\n+  public byte readMarker(long position) throws IOException {\n+    return readData(position, Byte.BYTES).get();\n+  }\n+\n+  public void close() throws IOException {\n+    this.tsFileInput.close();\n+  }\n+\n+  public String getFileName() {\n+    return this.file;\n+  }\n+\n+  /**\n+   * read data from tsFileInput, from the current position (if position = -1), or the given\n+   * position. <br> if position = -1, the tsFileInput's position will be changed to the current\n+   * position + real data size that been read. Other wise, the tsFileInput's position is not\n+   * changed.\n+   *\n+   * @param position the start position of data in the tsFileInput, or the current position if\n+   * position = -1\n+   * @param size the size of data that want to read\n+   * @return data that been read.\n+   */\n+  private ByteBuffer readData(long position, int size) throws IOException {\n+    ByteBuffer buffer = ByteBuffer.allocate(size);\n+    if (position == -1) {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    } else {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer, position, size) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    }\n+    buffer.flip();\n+    return buffer;\n+  }\n+\n+  /**\n+   * upgrade file and resource, return the boolean value whether upgrade task completes\n+   * @throws IOException, WriteProcessException \n+   */\n+  public boolean upgradeFile(List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    File oldTsFile = FSFactoryProducer.getFSFactory().getFile(this.file);\n+\n+    // check if the old TsFile has correct header \n+    if (!fileCheck(oldTsFile)) {\n+      return false;\n+    }\n+\n+    // ChunkGroupOffset -> version\n+    Map<Long, Long> oldVersionInfo = new HashMap<>();\n+\n+    // ChunkGroupOffset -> time partition, record the offsets of chunk group that data are in same partition\n+    Map<Long, Long> chunkGroupTimePartitionInfo = new HashMap<>();\n+\n+    // scan metadata to get version Info and chunkGroupTimePartitionInfo\n+    scanMetadata(oldVersionInfo, chunkGroupTimePartitionInfo);\n+    \n+    // start to scan chunks and chunkGroups\n+    long startOffsetOfChunkGroup = 0;\n+    boolean newChunkGroup = true;\n+    long versionOfChunkGroup = 0;\n+    boolean chunkGroupInSamePartition = false;\n+    List<ChunkGroupMetadata> newMetaData = new ArrayList<>();\n+    List<List<PageHeader>> pageHeadersInChunkGroup = new ArrayList<>();\n+    List<List<ByteBuffer>> dataInChunkGroup = new ArrayList<>();\n+    byte marker;\n+    List<MeasurementSchema> measurementSchemaList = new ArrayList<>();\n+    try {\n+      while ((marker = this.readMarker()) != MetaMarker.SEPARATOR) {\n+        switch (marker) {\n+          case MetaMarker.CHUNK_HEADER:\n+            // this is the first chunk of a new ChunkGroup.\n+            if (newChunkGroup) {\n+              newChunkGroup = false;\n+              startOffsetOfChunkGroup = this.position() - 1;\n+              versionOfChunkGroup = oldVersionInfo.get(startOffsetOfChunkGroup);\n+              chunkGroupInSamePartition = chunkGroupTimePartitionInfo\n+                  .containsKey(startOffsetOfChunkGroup);\n+            }\n+            ChunkHeader header = this.readChunkHeader();\n+            MeasurementSchema measurementSchema = new MeasurementSchema(header.getMeasurementID(),\n+                header.getDataType(),\n+                header.getEncodingType(), \n+                header.getCompressionType());\n+            measurementSchemaList.add(measurementSchema);\n+            List<PageHeader> pageHeadersInChunk = new ArrayList<>();\n+            List<ByteBuffer> dataInChunk = new ArrayList<>();\n+            for (int j = 0; j < header.getNumOfPages(); j++) {\n+              PageHeader pageHeader = readPageHeader(header.getDataType());\n+              ByteBuffer pageData = chunkGroupInSamePartition ? \n+                  readCompressedPage(pageHeader) : readPage(pageHeader, header.getCompressionType());\n+              pageHeadersInChunk.add(pageHeader);\n+              dataInChunk.add(pageData);\n+            }\n+            pageHeadersInChunkGroup.add(pageHeadersInChunk);\n+            dataInChunkGroup.add(dataInChunk);\n+            break;\n+          case MetaMarker.CHUNK_GROUP_FOOTER:\n+            // this is the footer of a ChunkGroup.\n+            ChunkGroupFooter chunkGroupFooter = this.readChunkGroupFooter();\n+            String deviceID = chunkGroupFooter.getDeviceID();\n+            if (chunkGroupInSamePartition) {\n+              quickRewrite(oldTsFile, deviceID, measurementSchemaList, pageHeadersInChunkGroup,\n+                  dataInChunkGroup, versionOfChunkGroup, chunkGroupTimePartitionInfo.get(startOffsetOfChunkGroup));\n+            } else {\n+              rewrite(oldTsFile, deviceID, measurementSchemaList, \n+                dataInChunkGroup, versionOfChunkGroup);\n+            }\n+\n+            pageHeadersInChunkGroup.clear();\n+            dataInChunkGroup.clear();\n+            measurementSchemaList.clear();\n+            newChunkGroup = true;\n+            break;\n+\n+          default:\n+            // the disk file is corrupted, using this file may be dangerous\n+            logger.error(\"Unrecognized marker detected, this file may be corrupted\");\n+            return false;\n+        }\n+      }\n+      // close upgraded tsFiles and generate resources for them\n+      for (TsFileIOWriter tsFileIOWriter : partitionWriterMap.values()) {\n+        upgradedResources.add(endFileAndGenerateResource(tsFileIOWriter));\n+      }\n+      return true;\n+    } catch (IOException e2) {\n+      logger.info(\"TsFile upgrade process cannot proceed at position {} after {} chunk groups \"\n+          + \"recovered, because : {}\", this.position(), newMetaData.size(), e2.getMessage());\n+      return false;\n+    } finally {\n+      if (tsFileInput != null) {\n+        tsFileInput.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   *  Rewrite the chunk group to new TsFile.\n+   *  If data of this chunk group are in different time partitions,\n+   *  create multiple new TsFiles and rewrite data in each partition.\n+   */\n+  private void rewrite(File oldTsFile, String deviceId, List<MeasurementSchema> schemas, \n+      List<List<ByteBuffer>> dataInChunkGroup, long versionOfChunkGroup) \n+          throws IOException {\n+\n+    Map<Long, Map<MeasurementSchema, IChunkWriter>> chunkWritersInChunkGroup = new HashMap<>();\n+    for (int i = 0; i < schemas.size(); i++) {\n+      MeasurementSchema schema = schemas.get(i);\n+      Decoder defaultTimeDecoder = Decoder.getDecoderByType(\n+          TSEncoding.valueOf(TSFileDescriptor.getInstance().getConfig().getTimeEncoder()),\n+          TSDataType.INT64);\n+      Decoder valueDecoder = Decoder\n+          .getDecoderByType(schema.getEncodingType(), schema.getType());\n+      List<ByteBuffer> dataInChunk = dataInChunkGroup.get(i);\n+      for (ByteBuffer pageData : dataInChunk) {\n+        valueDecoder.reset();\n+        PageReader pageReader = new PageReader(pageData, schema.getType(), valueDecoder,\n+            defaultTimeDecoder, null);\n+        BatchData batchData = pageReader.getAllSatisfiedPageData();\n+        while (batchData.hasCurrent()) {\n+          long time = batchData.currentTime();\n+          Object value = batchData.currentValue();\n+          long partition = StorageEngine.getTimePartition(time);\n+          \n+          Map<MeasurementSchema, IChunkWriter> chunkWriters = chunkWritersInChunkGroup.getOrDefault(partition, new HashMap<>());\n+          IChunkWriter chunkWriter = chunkWriters.getOrDefault(schema, new ChunkWriterImpl(schema));\n+          TsFileIOWriter tsFileIOWriter = getOrDefaultTsFileIOWriter(oldTsFile, partition);\n+          partitionWriterMap.put(partition, tsFileIOWriter);\n+          switch (schema.getType()) {\n+            case INT32:\n+              chunkWriter.write(time, (int) value);\n+              break;\n+            case INT64:\n+              chunkWriter.write(time, (long) value);\n+              break;\n+            case FLOAT:\n+              chunkWriter.write(time, (float) value);\n+              break;\n+            case DOUBLE:\n+              chunkWriter.write(time, (double) value);\n+              break;\n+            case BOOLEAN:\n+              chunkWriter.write(time, (boolean) value);\n+              break;\n+            case TEXT:\n+              chunkWriter.write(time, (Binary) value);\n+              break;\n+            default:\n+              throw new UnSupportedDataTypeException(\n+                  String.format(\"Data type %s is not supported.\", schema.getType()));\n+            }\n+          chunkWriters.put(schema, chunkWriter);\n+          chunkWritersInChunkGroup.put(partition, chunkWriters);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwMzk1Mg=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 462}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTQwMjkwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNzo1MjowOVrOGTBtrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNToyMjowM1rOGTFFuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwNDIwNg==", "bodyText": "no need to put again.  if the writer is newly created, it will be put into the map and then get.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422604206", "createdAt": "2020-05-10T07:52:09Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "diffHunk": "@@ -0,0 +1,624 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import org.apache.iotdb.db.engine.StorageEngine;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.common.conf.TSFileDescriptor;\n+import org.apache.iotdb.tsfile.compress.IUnCompressor;\n+import org.apache.iotdb.tsfile.encoding.decoder.Decoder;\n+import org.apache.iotdb.tsfile.exception.write.PageException;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+import org.apache.iotdb.tsfile.file.MetaMarker;\n+import org.apache.iotdb.tsfile.file.footer.ChunkGroupFooter;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkGroupMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.CompressionType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSEncoding;\n+import org.apache.iotdb.tsfile.fileSystem.FSFactoryProducer;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.reader.LocalTsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.page.PageReader;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.write.chunk.ChunkWriterImpl;\n+import org.apache.iotdb.tsfile.write.chunk.IChunkWriter;\n+import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n+import org.apache.iotdb.tsfile.write.writer.TsFileIOWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+public class TsfileOnlineUpgradeTool implements AutoCloseable {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(TsfileOnlineUpgradeTool.class);\n+\n+  private TsFileInput tsFileInput;\n+  private long fileMetadataPos;\n+  private int fileMetadataSize;\n+  private ByteBuffer markerBuffer = ByteBuffer.allocate(Byte.BYTES);\n+  protected String file;\n+  \n+  // PartitionId -> TsFileIOWriter \n+  private Map<Long, TsFileIOWriter> partitionWriterMap;\n+\n+  /**\n+   * Create a file reader of the given file. The reader will read the tail of the file to get the\n+   * file metadata size.Then the reader will skip the first TSFileConfig.OLD_MAGIC_STRING.length()\n+   * bytes of the file for preparing reading real data.\n+   *\n+   * @param file the data file\n+   * @throws IOException If some I/O error occurs\n+   */\n+  public TsfileOnlineUpgradeTool(String file) throws IOException {\n+    this(file, true);\n+  }\n+\n+  /**\n+   * construct function for TsfileOnlineUpgradeTool.\n+   *\n+   * @param file -given file name\n+   * @param loadMetadataSize -load meta data size\n+   */\n+  public TsfileOnlineUpgradeTool(String file, boolean loadMetadataSize) throws IOException {\n+    this.file = file;\n+    final java.nio.file.Path path = Paths.get(file);\n+    tsFileInput = new LocalTsFileInput(path);\n+    partitionWriterMap = new HashMap<>();\n+    try {\n+      if (loadMetadataSize) {\n+        loadMetadataSize();\n+      }\n+    } catch (Exception e) {\n+      tsFileInput.close();\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * \n+   */\n+  public void loadMetadataSize() throws IOException {\n+    ByteBuffer metadataSize = ByteBuffer.allocate(Integer.BYTES);\n+    tsFileInput.read(metadataSize,\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES);\n+    metadataSize.flip();\n+    // read file metadata size and position\n+    fileMetadataSize = ReadWriteIOUtils.readInt(metadataSize);\n+    fileMetadataPos =\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES\n+            - fileMetadataSize;\n+    // skip the magic header\n+    position(TSFileConfig.MAGIC_STRING.length());\n+  }\n+\n+  public String readTailMagic() throws IOException {\n+    long totalSize = tsFileInput.size();\n+\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    tsFileInput.read(magicStringBytes, totalSize - TSFileConfig.MAGIC_STRING.length());\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * whether the file is a complete TsFile: only if the head magic and tail magic string exists.\n+   */\n+  public boolean isComplete() throws IOException {\n+    return tsFileInput.size() >= TSFileConfig.MAGIC_STRING.length() * 2 && readTailMagic()\n+        .equals(readHeadMagic());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public String readHeadMagic() throws IOException {\n+    return readHeadMagic(false);\n+  }\n+\n+  /**\n+   * @param movePosition whether move the position of the file reader after reading the magic header\n+   * to the end of the magic head string.\n+   */\n+  public String readHeadMagic(boolean movePosition) throws IOException {\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    if (movePosition) {\n+      tsFileInput.position(0);\n+      tsFileInput.read(magicStringBytes);\n+    } else {\n+      tsFileInput.read(magicStringBytes, 0);\n+    }\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * this function reads version number and checks compatibility of TsFile.\n+   */\n+  public String readVersionNumber() throws IOException {\n+    ByteBuffer versionNumberBytes = ByteBuffer\n+        .allocate(TSFileConfig.VERSION_NUMBER.getBytes().length);\n+    tsFileInput.position(TSFileConfig.MAGIC_STRING.getBytes().length);\n+    tsFileInput.read(versionNumberBytes);\n+    versionNumberBytes.flip();\n+    return new String(versionNumberBytes.array());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsFileMetadata readFileMetadata() throws IOException {\n+    return OldTsFileMetadata.deserializeFrom(readData(fileMetadataPos, fileMetadataSize));\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsDeviceMetadata readTsDeviceMetaData(OldTsDeviceMetadataIndex index) throws IOException {\n+    return OldTsDeviceMetadata.deserializeFrom(readData(index.getOffset(), index.getLen()));\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_GROUP_FOOTER. <br>\n+   * This method is not threadsafe.\n+   *\n+   * @return a CHUNK_GROUP_FOOTER\n+   * @throws IOException io error\n+   */\n+  public ChunkGroupFooter readChunkGroupFooter() throws IOException {\n+    return ChunkGroupFooter.deserializeFrom(tsFileInput.wrapAsInputStream(), true);\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_HEADER. <br> This\n+   * method is not threadsafe.\n+   *\n+   * @return a CHUNK_HEADER\n+   * @throws IOException io error\n+   */\n+  public ChunkHeader readChunkHeader() throws IOException {\n+    return ChunkHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), true, true);\n+  }\n+\n+  /**\n+   * not thread safe.\n+   *\n+   * @param type given tsfile data type\n+   */\n+  public PageHeader readPageHeader(TSDataType type) throws IOException {\n+    return PageHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), type, true);\n+  }\n+\n+  public ByteBuffer readPage(PageHeader header, CompressionType type)\n+      throws IOException {\n+    ByteBuffer buffer = readData(-1, header.getCompressedSize());\n+    IUnCompressor unCompressor = IUnCompressor.getUnCompressor(type);\n+    ByteBuffer uncompressedBuffer = ByteBuffer.allocate(header.getUncompressedSize());\n+    if (type == CompressionType.UNCOMPRESSED) {\n+      return buffer;\n+    }\n+    unCompressor.uncompress(buffer.array(), buffer.position(), buffer.remaining(),\n+        uncompressedBuffer.array(),\n+        0);\n+    return uncompressedBuffer;\n+  }\n+  \n+  public ByteBuffer readCompressedPage(PageHeader header) throws IOException {\n+    return readData(-1, header.getCompressedSize());\n+  }\n+\n+  public long position() throws IOException {\n+    return tsFileInput.position();\n+  }\n+\n+  public void position(long offset) throws IOException {\n+    tsFileInput.position(offset);\n+  }\n+\n+  /**\n+   * read one byte from the input. <br> this method is not thread safe\n+   */\n+  public byte readMarker() throws IOException {\n+    markerBuffer.clear();\n+    if (ReadWriteIOUtils.readAsPossible(tsFileInput, markerBuffer) == 0) {\n+      throw new IOException(\"reach the end of the file.\");\n+    }\n+    markerBuffer.flip();\n+    return markerBuffer.get();\n+  }\n+\n+  public byte readMarker(long position) throws IOException {\n+    return readData(position, Byte.BYTES).get();\n+  }\n+\n+  public void close() throws IOException {\n+    this.tsFileInput.close();\n+  }\n+\n+  public String getFileName() {\n+    return this.file;\n+  }\n+\n+  /**\n+   * read data from tsFileInput, from the current position (if position = -1), or the given\n+   * position. <br> if position = -1, the tsFileInput's position will be changed to the current\n+   * position + real data size that been read. Other wise, the tsFileInput's position is not\n+   * changed.\n+   *\n+   * @param position the start position of data in the tsFileInput, or the current position if\n+   * position = -1\n+   * @param size the size of data that want to read\n+   * @return data that been read.\n+   */\n+  private ByteBuffer readData(long position, int size) throws IOException {\n+    ByteBuffer buffer = ByteBuffer.allocate(size);\n+    if (position == -1) {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    } else {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer, position, size) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    }\n+    buffer.flip();\n+    return buffer;\n+  }\n+\n+  /**\n+   * upgrade file and resource, return the boolean value whether upgrade task completes\n+   * @throws IOException, WriteProcessException \n+   */\n+  public boolean upgradeFile(List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    File oldTsFile = FSFactoryProducer.getFSFactory().getFile(this.file);\n+\n+    // check if the old TsFile has correct header \n+    if (!fileCheck(oldTsFile)) {\n+      return false;\n+    }\n+\n+    // ChunkGroupOffset -> version\n+    Map<Long, Long> oldVersionInfo = new HashMap<>();\n+\n+    // ChunkGroupOffset -> time partition, record the offsets of chunk group that data are in same partition\n+    Map<Long, Long> chunkGroupTimePartitionInfo = new HashMap<>();\n+\n+    // scan metadata to get version Info and chunkGroupTimePartitionInfo\n+    scanMetadata(oldVersionInfo, chunkGroupTimePartitionInfo);\n+    \n+    // start to scan chunks and chunkGroups\n+    long startOffsetOfChunkGroup = 0;\n+    boolean newChunkGroup = true;\n+    long versionOfChunkGroup = 0;\n+    boolean chunkGroupInSamePartition = false;\n+    List<ChunkGroupMetadata> newMetaData = new ArrayList<>();\n+    List<List<PageHeader>> pageHeadersInChunkGroup = new ArrayList<>();\n+    List<List<ByteBuffer>> dataInChunkGroup = new ArrayList<>();\n+    byte marker;\n+    List<MeasurementSchema> measurementSchemaList = new ArrayList<>();\n+    try {\n+      while ((marker = this.readMarker()) != MetaMarker.SEPARATOR) {\n+        switch (marker) {\n+          case MetaMarker.CHUNK_HEADER:\n+            // this is the first chunk of a new ChunkGroup.\n+            if (newChunkGroup) {\n+              newChunkGroup = false;\n+              startOffsetOfChunkGroup = this.position() - 1;\n+              versionOfChunkGroup = oldVersionInfo.get(startOffsetOfChunkGroup);\n+              chunkGroupInSamePartition = chunkGroupTimePartitionInfo\n+                  .containsKey(startOffsetOfChunkGroup);\n+            }\n+            ChunkHeader header = this.readChunkHeader();\n+            MeasurementSchema measurementSchema = new MeasurementSchema(header.getMeasurementID(),\n+                header.getDataType(),\n+                header.getEncodingType(), \n+                header.getCompressionType());\n+            measurementSchemaList.add(measurementSchema);\n+            List<PageHeader> pageHeadersInChunk = new ArrayList<>();\n+            List<ByteBuffer> dataInChunk = new ArrayList<>();\n+            for (int j = 0; j < header.getNumOfPages(); j++) {\n+              PageHeader pageHeader = readPageHeader(header.getDataType());\n+              ByteBuffer pageData = chunkGroupInSamePartition ? \n+                  readCompressedPage(pageHeader) : readPage(pageHeader, header.getCompressionType());\n+              pageHeadersInChunk.add(pageHeader);\n+              dataInChunk.add(pageData);\n+            }\n+            pageHeadersInChunkGroup.add(pageHeadersInChunk);\n+            dataInChunkGroup.add(dataInChunk);\n+            break;\n+          case MetaMarker.CHUNK_GROUP_FOOTER:\n+            // this is the footer of a ChunkGroup.\n+            ChunkGroupFooter chunkGroupFooter = this.readChunkGroupFooter();\n+            String deviceID = chunkGroupFooter.getDeviceID();\n+            if (chunkGroupInSamePartition) {\n+              quickRewrite(oldTsFile, deviceID, measurementSchemaList, pageHeadersInChunkGroup,\n+                  dataInChunkGroup, versionOfChunkGroup, chunkGroupTimePartitionInfo.get(startOffsetOfChunkGroup));\n+            } else {\n+              rewrite(oldTsFile, deviceID, measurementSchemaList, \n+                dataInChunkGroup, versionOfChunkGroup);\n+            }\n+\n+            pageHeadersInChunkGroup.clear();\n+            dataInChunkGroup.clear();\n+            measurementSchemaList.clear();\n+            newChunkGroup = true;\n+            break;\n+\n+          default:\n+            // the disk file is corrupted, using this file may be dangerous\n+            logger.error(\"Unrecognized marker detected, this file may be corrupted\");\n+            return false;\n+        }\n+      }\n+      // close upgraded tsFiles and generate resources for them\n+      for (TsFileIOWriter tsFileIOWriter : partitionWriterMap.values()) {\n+        upgradedResources.add(endFileAndGenerateResource(tsFileIOWriter));\n+      }\n+      return true;\n+    } catch (IOException e2) {\n+      logger.info(\"TsFile upgrade process cannot proceed at position {} after {} chunk groups \"\n+          + \"recovered, because : {}\", this.position(), newMetaData.size(), e2.getMessage());\n+      return false;\n+    } finally {\n+      if (tsFileInput != null) {\n+        tsFileInput.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   *  Rewrite the chunk group to new TsFile.\n+   *  If data of this chunk group are in different time partitions,\n+   *  create multiple new TsFiles and rewrite data in each partition.\n+   */\n+  private void rewrite(File oldTsFile, String deviceId, List<MeasurementSchema> schemas, \n+      List<List<ByteBuffer>> dataInChunkGroup, long versionOfChunkGroup) \n+          throws IOException {\n+\n+    Map<Long, Map<MeasurementSchema, IChunkWriter>> chunkWritersInChunkGroup = new HashMap<>();\n+    for (int i = 0; i < schemas.size(); i++) {\n+      MeasurementSchema schema = schemas.get(i);\n+      Decoder defaultTimeDecoder = Decoder.getDecoderByType(\n+          TSEncoding.valueOf(TSFileDescriptor.getInstance().getConfig().getTimeEncoder()),\n+          TSDataType.INT64);\n+      Decoder valueDecoder = Decoder\n+          .getDecoderByType(schema.getEncodingType(), schema.getType());\n+      List<ByteBuffer> dataInChunk = dataInChunkGroup.get(i);\n+      for (ByteBuffer pageData : dataInChunk) {\n+        valueDecoder.reset();\n+        PageReader pageReader = new PageReader(pageData, schema.getType(), valueDecoder,\n+            defaultTimeDecoder, null);\n+        BatchData batchData = pageReader.getAllSatisfiedPageData();\n+        while (batchData.hasCurrent()) {\n+          long time = batchData.currentTime();\n+          Object value = batchData.currentValue();\n+          long partition = StorageEngine.getTimePartition(time);\n+          \n+          Map<MeasurementSchema, IChunkWriter> chunkWriters = chunkWritersInChunkGroup.getOrDefault(partition, new HashMap<>());\n+          IChunkWriter chunkWriter = chunkWriters.getOrDefault(schema, new ChunkWriterImpl(schema));\n+          TsFileIOWriter tsFileIOWriter = getOrDefaultTsFileIOWriter(oldTsFile, partition);\n+          partitionWriterMap.put(partition, tsFileIOWriter);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 437}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY1OTUxMw==", "bodyText": "Fixed", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422659513", "createdAt": "2020-05-10T15:22:03Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/TsfileOnlineUpgradeTool.java", "diffHunk": "@@ -0,0 +1,624 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import org.apache.iotdb.db.engine.StorageEngine;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.common.conf.TSFileDescriptor;\n+import org.apache.iotdb.tsfile.compress.IUnCompressor;\n+import org.apache.iotdb.tsfile.encoding.decoder.Decoder;\n+import org.apache.iotdb.tsfile.exception.write.PageException;\n+import org.apache.iotdb.tsfile.exception.write.UnSupportedDataTypeException;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+import org.apache.iotdb.tsfile.file.MetaMarker;\n+import org.apache.iotdb.tsfile.file.footer.ChunkGroupFooter;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkGroupMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.CompressionType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSEncoding;\n+import org.apache.iotdb.tsfile.fileSystem.FSFactoryProducer;\n+import org.apache.iotdb.tsfile.read.common.BatchData;\n+import org.apache.iotdb.tsfile.read.reader.LocalTsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.read.reader.page.PageReader;\n+import org.apache.iotdb.tsfile.utils.Binary;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.write.chunk.ChunkWriterImpl;\n+import org.apache.iotdb.tsfile.write.chunk.IChunkWriter;\n+import org.apache.iotdb.tsfile.write.schema.MeasurementSchema;\n+import org.apache.iotdb.tsfile.write.writer.TsFileIOWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+\n+public class TsfileOnlineUpgradeTool implements AutoCloseable {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(TsfileOnlineUpgradeTool.class);\n+\n+  private TsFileInput tsFileInput;\n+  private long fileMetadataPos;\n+  private int fileMetadataSize;\n+  private ByteBuffer markerBuffer = ByteBuffer.allocate(Byte.BYTES);\n+  protected String file;\n+  \n+  // PartitionId -> TsFileIOWriter \n+  private Map<Long, TsFileIOWriter> partitionWriterMap;\n+\n+  /**\n+   * Create a file reader of the given file. The reader will read the tail of the file to get the\n+   * file metadata size.Then the reader will skip the first TSFileConfig.OLD_MAGIC_STRING.length()\n+   * bytes of the file for preparing reading real data.\n+   *\n+   * @param file the data file\n+   * @throws IOException If some I/O error occurs\n+   */\n+  public TsfileOnlineUpgradeTool(String file) throws IOException {\n+    this(file, true);\n+  }\n+\n+  /**\n+   * construct function for TsfileOnlineUpgradeTool.\n+   *\n+   * @param file -given file name\n+   * @param loadMetadataSize -load meta data size\n+   */\n+  public TsfileOnlineUpgradeTool(String file, boolean loadMetadataSize) throws IOException {\n+    this.file = file;\n+    final java.nio.file.Path path = Paths.get(file);\n+    tsFileInput = new LocalTsFileInput(path);\n+    partitionWriterMap = new HashMap<>();\n+    try {\n+      if (loadMetadataSize) {\n+        loadMetadataSize();\n+      }\n+    } catch (Exception e) {\n+      tsFileInput.close();\n+      throw e;\n+    }\n+  }\n+\n+  /**\n+   * \n+   */\n+  public void loadMetadataSize() throws IOException {\n+    ByteBuffer metadataSize = ByteBuffer.allocate(Integer.BYTES);\n+    tsFileInput.read(metadataSize,\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES);\n+    metadataSize.flip();\n+    // read file metadata size and position\n+    fileMetadataSize = ReadWriteIOUtils.readInt(metadataSize);\n+    fileMetadataPos =\n+        tsFileInput.size() - TSFileConfig.MAGIC_STRING.getBytes().length - Integer.BYTES\n+            - fileMetadataSize;\n+    // skip the magic header\n+    position(TSFileConfig.MAGIC_STRING.length());\n+  }\n+\n+  public String readTailMagic() throws IOException {\n+    long totalSize = tsFileInput.size();\n+\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    tsFileInput.read(magicStringBytes, totalSize - TSFileConfig.MAGIC_STRING.length());\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * whether the file is a complete TsFile: only if the head magic and tail magic string exists.\n+   */\n+  public boolean isComplete() throws IOException {\n+    return tsFileInput.size() >= TSFileConfig.MAGIC_STRING.length() * 2 && readTailMagic()\n+        .equals(readHeadMagic());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public String readHeadMagic() throws IOException {\n+    return readHeadMagic(false);\n+  }\n+\n+  /**\n+   * @param movePosition whether move the position of the file reader after reading the magic header\n+   * to the end of the magic head string.\n+   */\n+  public String readHeadMagic(boolean movePosition) throws IOException {\n+    ByteBuffer magicStringBytes = ByteBuffer.allocate(TSFileConfig.MAGIC_STRING.length());\n+    if (movePosition) {\n+      tsFileInput.position(0);\n+      tsFileInput.read(magicStringBytes);\n+    } else {\n+      tsFileInput.read(magicStringBytes, 0);\n+    }\n+    magicStringBytes.flip();\n+    return new String(magicStringBytes.array());\n+  }\n+\n+  /**\n+   * this function reads version number and checks compatibility of TsFile.\n+   */\n+  public String readVersionNumber() throws IOException {\n+    ByteBuffer versionNumberBytes = ByteBuffer\n+        .allocate(TSFileConfig.VERSION_NUMBER.getBytes().length);\n+    tsFileInput.position(TSFileConfig.MAGIC_STRING.getBytes().length);\n+    tsFileInput.read(versionNumberBytes);\n+    versionNumberBytes.flip();\n+    return new String(versionNumberBytes.array());\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsFileMetadata readFileMetadata() throws IOException {\n+    return OldTsFileMetadata.deserializeFrom(readData(fileMetadataPos, fileMetadataSize));\n+  }\n+\n+  /**\n+   * this function does not modify the position of the file reader.\n+   */\n+  public OldTsDeviceMetadata readTsDeviceMetaData(OldTsDeviceMetadataIndex index) throws IOException {\n+    return OldTsDeviceMetadata.deserializeFrom(readData(index.getOffset(), index.getLen()));\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_GROUP_FOOTER. <br>\n+   * This method is not threadsafe.\n+   *\n+   * @return a CHUNK_GROUP_FOOTER\n+   * @throws IOException io error\n+   */\n+  public ChunkGroupFooter readChunkGroupFooter() throws IOException {\n+    return ChunkGroupFooter.deserializeFrom(tsFileInput.wrapAsInputStream(), true);\n+  }\n+\n+  /**\n+   * read data from current position of the input, and deserialize it to a CHUNK_HEADER. <br> This\n+   * method is not threadsafe.\n+   *\n+   * @return a CHUNK_HEADER\n+   * @throws IOException io error\n+   */\n+  public ChunkHeader readChunkHeader() throws IOException {\n+    return ChunkHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), true, true);\n+  }\n+\n+  /**\n+   * not thread safe.\n+   *\n+   * @param type given tsfile data type\n+   */\n+  public PageHeader readPageHeader(TSDataType type) throws IOException {\n+    return PageHeader.deserializeFrom(tsFileInput.wrapAsInputStream(), type, true);\n+  }\n+\n+  public ByteBuffer readPage(PageHeader header, CompressionType type)\n+      throws IOException {\n+    ByteBuffer buffer = readData(-1, header.getCompressedSize());\n+    IUnCompressor unCompressor = IUnCompressor.getUnCompressor(type);\n+    ByteBuffer uncompressedBuffer = ByteBuffer.allocate(header.getUncompressedSize());\n+    if (type == CompressionType.UNCOMPRESSED) {\n+      return buffer;\n+    }\n+    unCompressor.uncompress(buffer.array(), buffer.position(), buffer.remaining(),\n+        uncompressedBuffer.array(),\n+        0);\n+    return uncompressedBuffer;\n+  }\n+  \n+  public ByteBuffer readCompressedPage(PageHeader header) throws IOException {\n+    return readData(-1, header.getCompressedSize());\n+  }\n+\n+  public long position() throws IOException {\n+    return tsFileInput.position();\n+  }\n+\n+  public void position(long offset) throws IOException {\n+    tsFileInput.position(offset);\n+  }\n+\n+  /**\n+   * read one byte from the input. <br> this method is not thread safe\n+   */\n+  public byte readMarker() throws IOException {\n+    markerBuffer.clear();\n+    if (ReadWriteIOUtils.readAsPossible(tsFileInput, markerBuffer) == 0) {\n+      throw new IOException(\"reach the end of the file.\");\n+    }\n+    markerBuffer.flip();\n+    return markerBuffer.get();\n+  }\n+\n+  public byte readMarker(long position) throws IOException {\n+    return readData(position, Byte.BYTES).get();\n+  }\n+\n+  public void close() throws IOException {\n+    this.tsFileInput.close();\n+  }\n+\n+  public String getFileName() {\n+    return this.file;\n+  }\n+\n+  /**\n+   * read data from tsFileInput, from the current position (if position = -1), or the given\n+   * position. <br> if position = -1, the tsFileInput's position will be changed to the current\n+   * position + real data size that been read. Other wise, the tsFileInput's position is not\n+   * changed.\n+   *\n+   * @param position the start position of data in the tsFileInput, or the current position if\n+   * position = -1\n+   * @param size the size of data that want to read\n+   * @return data that been read.\n+   */\n+  private ByteBuffer readData(long position, int size) throws IOException {\n+    ByteBuffer buffer = ByteBuffer.allocate(size);\n+    if (position == -1) {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    } else {\n+      if (ReadWriteIOUtils.readAsPossible(tsFileInput, buffer, position, size) != size) {\n+        throw new IOException(\"reach the end of the data\");\n+      }\n+    }\n+    buffer.flip();\n+    return buffer;\n+  }\n+\n+  /**\n+   * upgrade file and resource, return the boolean value whether upgrade task completes\n+   * @throws IOException, WriteProcessException \n+   */\n+  public boolean upgradeFile(List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    File oldTsFile = FSFactoryProducer.getFSFactory().getFile(this.file);\n+\n+    // check if the old TsFile has correct header \n+    if (!fileCheck(oldTsFile)) {\n+      return false;\n+    }\n+\n+    // ChunkGroupOffset -> version\n+    Map<Long, Long> oldVersionInfo = new HashMap<>();\n+\n+    // ChunkGroupOffset -> time partition, record the offsets of chunk group that data are in same partition\n+    Map<Long, Long> chunkGroupTimePartitionInfo = new HashMap<>();\n+\n+    // scan metadata to get version Info and chunkGroupTimePartitionInfo\n+    scanMetadata(oldVersionInfo, chunkGroupTimePartitionInfo);\n+    \n+    // start to scan chunks and chunkGroups\n+    long startOffsetOfChunkGroup = 0;\n+    boolean newChunkGroup = true;\n+    long versionOfChunkGroup = 0;\n+    boolean chunkGroupInSamePartition = false;\n+    List<ChunkGroupMetadata> newMetaData = new ArrayList<>();\n+    List<List<PageHeader>> pageHeadersInChunkGroup = new ArrayList<>();\n+    List<List<ByteBuffer>> dataInChunkGroup = new ArrayList<>();\n+    byte marker;\n+    List<MeasurementSchema> measurementSchemaList = new ArrayList<>();\n+    try {\n+      while ((marker = this.readMarker()) != MetaMarker.SEPARATOR) {\n+        switch (marker) {\n+          case MetaMarker.CHUNK_HEADER:\n+            // this is the first chunk of a new ChunkGroup.\n+            if (newChunkGroup) {\n+              newChunkGroup = false;\n+              startOffsetOfChunkGroup = this.position() - 1;\n+              versionOfChunkGroup = oldVersionInfo.get(startOffsetOfChunkGroup);\n+              chunkGroupInSamePartition = chunkGroupTimePartitionInfo\n+                  .containsKey(startOffsetOfChunkGroup);\n+            }\n+            ChunkHeader header = this.readChunkHeader();\n+            MeasurementSchema measurementSchema = new MeasurementSchema(header.getMeasurementID(),\n+                header.getDataType(),\n+                header.getEncodingType(), \n+                header.getCompressionType());\n+            measurementSchemaList.add(measurementSchema);\n+            List<PageHeader> pageHeadersInChunk = new ArrayList<>();\n+            List<ByteBuffer> dataInChunk = new ArrayList<>();\n+            for (int j = 0; j < header.getNumOfPages(); j++) {\n+              PageHeader pageHeader = readPageHeader(header.getDataType());\n+              ByteBuffer pageData = chunkGroupInSamePartition ? \n+                  readCompressedPage(pageHeader) : readPage(pageHeader, header.getCompressionType());\n+              pageHeadersInChunk.add(pageHeader);\n+              dataInChunk.add(pageData);\n+            }\n+            pageHeadersInChunkGroup.add(pageHeadersInChunk);\n+            dataInChunkGroup.add(dataInChunk);\n+            break;\n+          case MetaMarker.CHUNK_GROUP_FOOTER:\n+            // this is the footer of a ChunkGroup.\n+            ChunkGroupFooter chunkGroupFooter = this.readChunkGroupFooter();\n+            String deviceID = chunkGroupFooter.getDeviceID();\n+            if (chunkGroupInSamePartition) {\n+              quickRewrite(oldTsFile, deviceID, measurementSchemaList, pageHeadersInChunkGroup,\n+                  dataInChunkGroup, versionOfChunkGroup, chunkGroupTimePartitionInfo.get(startOffsetOfChunkGroup));\n+            } else {\n+              rewrite(oldTsFile, deviceID, measurementSchemaList, \n+                dataInChunkGroup, versionOfChunkGroup);\n+            }\n+\n+            pageHeadersInChunkGroup.clear();\n+            dataInChunkGroup.clear();\n+            measurementSchemaList.clear();\n+            newChunkGroup = true;\n+            break;\n+\n+          default:\n+            // the disk file is corrupted, using this file may be dangerous\n+            logger.error(\"Unrecognized marker detected, this file may be corrupted\");\n+            return false;\n+        }\n+      }\n+      // close upgraded tsFiles and generate resources for them\n+      for (TsFileIOWriter tsFileIOWriter : partitionWriterMap.values()) {\n+        upgradedResources.add(endFileAndGenerateResource(tsFileIOWriter));\n+      }\n+      return true;\n+    } catch (IOException e2) {\n+      logger.info(\"TsFile upgrade process cannot proceed at position {} after {} chunk groups \"\n+          + \"recovered, because : {}\", this.position(), newMetaData.size(), e2.getMessage());\n+      return false;\n+    } finally {\n+      if (tsFileInput != null) {\n+        tsFileInput.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   *  Rewrite the chunk group to new TsFile.\n+   *  If data of this chunk group are in different time partitions,\n+   *  create multiple new TsFiles and rewrite data in each partition.\n+   */\n+  private void rewrite(File oldTsFile, String deviceId, List<MeasurementSchema> schemas, \n+      List<List<ByteBuffer>> dataInChunkGroup, long versionOfChunkGroup) \n+          throws IOException {\n+\n+    Map<Long, Map<MeasurementSchema, IChunkWriter>> chunkWritersInChunkGroup = new HashMap<>();\n+    for (int i = 0; i < schemas.size(); i++) {\n+      MeasurementSchema schema = schemas.get(i);\n+      Decoder defaultTimeDecoder = Decoder.getDecoderByType(\n+          TSEncoding.valueOf(TSFileDescriptor.getInstance().getConfig().getTimeEncoder()),\n+          TSDataType.INT64);\n+      Decoder valueDecoder = Decoder\n+          .getDecoderByType(schema.getEncodingType(), schema.getType());\n+      List<ByteBuffer> dataInChunk = dataInChunkGroup.get(i);\n+      for (ByteBuffer pageData : dataInChunk) {\n+        valueDecoder.reset();\n+        PageReader pageReader = new PageReader(pageData, schema.getType(), valueDecoder,\n+            defaultTimeDecoder, null);\n+        BatchData batchData = pageReader.getAllSatisfiedPageData();\n+        while (batchData.hasCurrent()) {\n+          long time = batchData.currentTime();\n+          Object value = batchData.currentValue();\n+          long partition = StorageEngine.getTimePartition(time);\n+          \n+          Map<MeasurementSchema, IChunkWriter> chunkWriters = chunkWritersInChunkGroup.getOrDefault(partition, new HashMap<>());\n+          IChunkWriter chunkWriter = chunkWriters.getOrDefault(schema, new ChunkWriterImpl(schema));\n+          TsFileIOWriter tsFileIOWriter = getOrDefaultTsFileIOWriter(oldTsFile, partition);\n+          partitionWriterMap.put(partition, tsFileIOWriter);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwNDIwNg=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 437}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTQwODI2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/UpgradeTool.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwNzo1NjoyNlrOGTBwKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNToyNjo0M1rOGTFILw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwNDg0Mw==", "bodyText": "I suggest not return a boolean, because we usually do not handly it. Check code to see whether you clean the crashed files when restarting, Do you handle crashed upgraded files by covering?", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422604843", "createdAt": "2020-05-10T07:56:26Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/UpgradeTool.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+\n+public class UpgradeTool {\n+  \n+  private UpgradeTool() {\n+  }\n+\n+  /**\n+   * upgrade a single tsfile\n+   *\n+   * @param tsfileName old version tsFile's absolute path\n+   * @param upgradedResources new version tsFiles' resources\n+   * @throws WriteProcessException \n+   */\n+  public static void upgradeOneTsfile(String tsFileName, List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    try (TsfileOnlineUpgradeTool updater = new TsfileOnlineUpgradeTool(tsFileName)) {\n+      updater.upgradeFile(upgradedResources);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY2MDE0Mw==", "bodyText": "I changed the return type to void.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422660143", "createdAt": "2020-05-10T15:26:43Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/tools/upgrade/UpgradeTool.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.tools.upgrade;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.tsfile.exception.write.WriteProcessException;\n+\n+public class UpgradeTool {\n+  \n+  private UpgradeTool() {\n+  }\n+\n+  /**\n+   * upgrade a single tsfile\n+   *\n+   * @param tsfileName old version tsFile's absolute path\n+   * @param upgradedResources new version tsFiles' resources\n+   * @throws WriteProcessException \n+   */\n+  public static void upgradeOneTsfile(String tsFileName, List<TsFileResource> upgradedResources) \n+      throws IOException, WriteProcessException {\n+    try (TsfileOnlineUpgradeTool updater = new TsfileOnlineUpgradeTool(tsFileName)) {\n+      updater.upgradeFile(upgradedResources);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwNDg0Mw=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTQzMjI0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/upgrade/UpgradeTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwODoxNToyOVrOGTB7Kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNToyODo0N1rOGTFJTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwNzY1OQ==", "bodyText": "the upgradedResources is under control, no need to getTimepartition with check", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422607659", "createdAt": "2020-05-10T08:15:29Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/upgrade/UpgradeTask.java", "diffHunk": "@@ -40,30 +47,69 @@ public UpgradeTask(TsFileResource upgradeResource) {\n   @Override\n   public void runMayThrow() {\n     try {\n-      upgradeResource.getWriteQueryLock().readLock().lock();\n-      String tsfilePathBefore = upgradeResource.getFile().getAbsolutePath();\n-      String tsfilePathAfter = UpgradeUtils.getUpgradeFileName(upgradeResource.getFile());\n-\n-      UpgradeLog.writeUpgradeLogFile(\n-          tsfilePathBefore + COMMA_SEPERATOR + UpgradeCheckStatus.BEGIN_UPGRADE_FILE);\n+      List<TsFileResource> upgradedResources = generateUpgradedFiles();\n       upgradeResource.getWriteQueryLock().writeLock().lock();\n+      String oldTsfilePath = upgradeResource.getFile().getAbsolutePath();\n       try {\n-        FSFactoryProducer.getFSFactory().getFile(tsfilePathBefore).delete();\n-        FSFactoryProducer.getFSFactory()\n-            .moveFile(FSFactoryProducer.getFSFactory().getFile(tsfilePathAfter),\n-                FSFactoryProducer.getFSFactory().getFile(tsfilePathBefore));\n+        // delete old TsFile\n+        upgradeResource.remove();\n+        // move upgraded TsFiles to their own partition directories\n+        for (TsFileResource upgradedResource : upgradedResources) {\n+          File upgradedFile = upgradedResource.getFile();\n+          long partition = upgradedResource.getTimePartitionWithCheck();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY2MDQyOA==", "bodyText": "Fixed", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422660428", "createdAt": "2020-05-10T15:28:47Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/upgrade/UpgradeTask.java", "diffHunk": "@@ -40,30 +47,69 @@ public UpgradeTask(TsFileResource upgradeResource) {\n   @Override\n   public void runMayThrow() {\n     try {\n-      upgradeResource.getWriteQueryLock().readLock().lock();\n-      String tsfilePathBefore = upgradeResource.getFile().getAbsolutePath();\n-      String tsfilePathAfter = UpgradeUtils.getUpgradeFileName(upgradeResource.getFile());\n-\n-      UpgradeLog.writeUpgradeLogFile(\n-          tsfilePathBefore + COMMA_SEPERATOR + UpgradeCheckStatus.BEGIN_UPGRADE_FILE);\n+      List<TsFileResource> upgradedResources = generateUpgradedFiles();\n       upgradeResource.getWriteQueryLock().writeLock().lock();\n+      String oldTsfilePath = upgradeResource.getFile().getAbsolutePath();\n       try {\n-        FSFactoryProducer.getFSFactory().getFile(tsfilePathBefore).delete();\n-        FSFactoryProducer.getFSFactory()\n-            .moveFile(FSFactoryProducer.getFSFactory().getFile(tsfilePathAfter),\n-                FSFactoryProducer.getFSFactory().getFile(tsfilePathBefore));\n+        // delete old TsFile\n+        upgradeResource.remove();\n+        // move upgraded TsFiles to their own partition directories\n+        for (TsFileResource upgradedResource : upgradedResources) {\n+          File upgradedFile = upgradedResource.getFile();\n+          long partition = upgradedResource.getTimePartitionWithCheck();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwNzY1OQ=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTQzNzcxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/upgrade/UpgradeTask.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwODoyMDoxN1rOGTB9wQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQxNTozNToxN1rOGTFMUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwODMyMQ==", "bodyText": "this may produce NullPointerException, better to have a check", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422608321", "createdAt": "2020-05-10T08:20:17Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/upgrade/UpgradeTask.java", "diffHunk": "@@ -40,30 +47,69 @@ public UpgradeTask(TsFileResource upgradeResource) {\n   @Override\n   public void runMayThrow() {\n     try {\n-      upgradeResource.getWriteQueryLock().readLock().lock();\n-      String tsfilePathBefore = upgradeResource.getFile().getAbsolutePath();\n-      String tsfilePathAfter = UpgradeUtils.getUpgradeFileName(upgradeResource.getFile());\n-\n-      UpgradeLog.writeUpgradeLogFile(\n-          tsfilePathBefore + COMMA_SEPERATOR + UpgradeCheckStatus.BEGIN_UPGRADE_FILE);\n+      List<TsFileResource> upgradedResources = generateUpgradedFiles();\n       upgradeResource.getWriteQueryLock().writeLock().lock();\n+      String oldTsfilePath = upgradeResource.getFile().getAbsolutePath();\n       try {\n-        FSFactoryProducer.getFSFactory().getFile(tsfilePathBefore).delete();\n-        FSFactoryProducer.getFSFactory()\n-            .moveFile(FSFactoryProducer.getFSFactory().getFile(tsfilePathAfter),\n-                FSFactoryProducer.getFSFactory().getFile(tsfilePathBefore));\n+        // delete old TsFile\n+        upgradeResource.remove();\n+        // move upgraded TsFiles to their own partition directories\n+        for (TsFileResource upgradedResource : upgradedResources) {\n+          File upgradedFile = upgradedResource.getFile();\n+          long partition = upgradedResource.getTimePartitionWithCheck();\n+          String storageGroupPath = upgradedFile.getParentFile().getParentFile().getParent();\n+          File partitionDir = FSFactoryProducer.getFSFactory().getFile(storageGroupPath, partition + \"\");\n+          if (!partitionDir.exists()) {\n+            partitionDir.mkdir();\n+          }\n+          FSFactoryProducer.getFSFactory().moveFile(upgradedFile,\n+              FSFactoryProducer.getFSFactory().getFile(partitionDir, upgradedFile.getName()));\n+          upgradedResource.setFile(\n+              FSFactoryProducer.getFSFactory().getFile(partitionDir, upgradedFile.getName()));\n+          upgradedResource.serialize();\n+          // delete tmp partition folder when it is empty\n+          if (upgradedFile.getParentFile().isDirectory() \n+              && upgradedFile.getParentFile().listFiles().length == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjY2MTIwMg==", "bodyText": "Isn't checking isDirectory() enough?\nThe javadoc of listFiles said\nIf this abstract pathname does not denote a directory, then this method returns null .  Otherwise an array of File objects is returned, one for each file or directory in the directory.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422661202", "createdAt": "2020-05-10T15:35:17Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/upgrade/UpgradeTask.java", "diffHunk": "@@ -40,30 +47,69 @@ public UpgradeTask(TsFileResource upgradeResource) {\n   @Override\n   public void runMayThrow() {\n     try {\n-      upgradeResource.getWriteQueryLock().readLock().lock();\n-      String tsfilePathBefore = upgradeResource.getFile().getAbsolutePath();\n-      String tsfilePathAfter = UpgradeUtils.getUpgradeFileName(upgradeResource.getFile());\n-\n-      UpgradeLog.writeUpgradeLogFile(\n-          tsfilePathBefore + COMMA_SEPERATOR + UpgradeCheckStatus.BEGIN_UPGRADE_FILE);\n+      List<TsFileResource> upgradedResources = generateUpgradedFiles();\n       upgradeResource.getWriteQueryLock().writeLock().lock();\n+      String oldTsfilePath = upgradeResource.getFile().getAbsolutePath();\n       try {\n-        FSFactoryProducer.getFSFactory().getFile(tsfilePathBefore).delete();\n-        FSFactoryProducer.getFSFactory()\n-            .moveFile(FSFactoryProducer.getFSFactory().getFile(tsfilePathAfter),\n-                FSFactoryProducer.getFSFactory().getFile(tsfilePathBefore));\n+        // delete old TsFile\n+        upgradeResource.remove();\n+        // move upgraded TsFiles to their own partition directories\n+        for (TsFileResource upgradedResource : upgradedResources) {\n+          File upgradedFile = upgradedResource.getFile();\n+          long partition = upgradedResource.getTimePartitionWithCheck();\n+          String storageGroupPath = upgradedFile.getParentFile().getParentFile().getParent();\n+          File partitionDir = FSFactoryProducer.getFSFactory().getFile(storageGroupPath, partition + \"\");\n+          if (!partitionDir.exists()) {\n+            partitionDir.mkdir();\n+          }\n+          FSFactoryProducer.getFSFactory().moveFile(upgradedFile,\n+              FSFactoryProducer.getFSFactory().getFile(partitionDir, upgradedFile.getName()));\n+          upgradedResource.setFile(\n+              FSFactoryProducer.getFSFactory().getFile(partitionDir, upgradedFile.getName()));\n+          upgradedResource.serialize();\n+          // delete tmp partition folder when it is empty\n+          if (upgradedFile.getParentFile().isDirectory() \n+              && upgradedFile.getParentFile().listFiles().length == 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYwODMyMQ=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTQ1ODk3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwODozNzo1OFrOGTCHmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwODozNzo1OFrOGTCHmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYxMDg0MA==", "bodyText": "lock once is enough", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422610840", "createdAt": "2020-05-10T08:37:58Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -1106,6 +1220,18 @@ public void writeUnlock() {\n         closeQueryLock.readLock().unlock();\n       }\n     }\n+    // for upgrade files and old files must be closed\n+    for (TsFileResource tsFileResource : upgradeTsFileResources) {\n+      if (!isTsFileResourceSatisfied(tsFileResource, deviceId, timeFilter)) {\n+        continue;\n+      }\n+      closeQueryLock.readLock().lock();\n+      try {\n+        tsfileResourcesForQuery.add(tsFileResource);\n+      } finally {\n+        closeQueryLock.readLock().unlock();\n+      }\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 258}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMTQ3MDE0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMFQwODo0ODozMFrOGTCNBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoyNjozNlrOGTz23w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYxMjIyOA==", "bodyText": "check if it is end with .tsfile, do not warning", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422612228", "createdAt": "2020-05-10T08:48:30Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -316,40 +377,93 @@ private VersionController getVersionControllerByTimePartitionId(long timePartiti\n         });\n   }\n \n-  private List<TsFileResource> getAllFiles(List<String> folders) {\n+  private Pair<List<TsFileResource>, List<TsFileResource>> getAllFiles(List<String> folders) throws IOException {\n     List<File> tsFiles = new ArrayList<>();\n+    List<File> upgradeFiles = new ArrayList<>();\n     for (String baseDir : folders) {\n       File fileFolder = fsFactory.getFile(baseDir, storageGroupName);\n       if (!fileFolder.exists()) {\n         continue;\n       }\n \n+      // old version\n+      // some TsFileResource may be being persisted when the system crashed, try recovering such\n+      // resources\n+      continueFailedRenames(fileFolder, TEMP_SUFFIX);\n+\n+      // some TsFiles were going to be replaced by the merged files when the system crashed and\n+      // the process was interrupted before the merged files could be named\n+      continueFailedRenames(fileFolder, MERGE_SUFFIX);\n+\n+      File[] oldTsfileArray = fsFactory.listFilesBySuffix(fileFolder.getAbsolutePath(), TSFILE_SUFFIX);\n+      File[] oldResourceFileArray = fsFactory.listFilesBySuffix(fileFolder.getAbsolutePath(), TsFileResource.RESOURCE_SUFFIX);\n+      File upgradeFolder = fsFactory.getFile(fileFolder, IoTDBConstant.UPGRADE_FOLDER_NAME);\n+      // move the old files to upgrade folder if exists\n+      if (oldTsfileArray.length != 0 || oldResourceFileArray.length != 0) {\n+        // create upgrade directory if not exist\n+        if (upgradeFolder.mkdirs()) {\n+          logger.info(\"Upgrade Directory {} doesn't exist, create it\",\n+              upgradeFolder.getPath());\n+        } else if (!upgradeFolder.exists()) {\n+          logger.error(\"Create upgrade Directory {} failed\",\n+              upgradeFolder.getPath());\n+        }\n+        // move .tsfile to upgrade folder\n+        for (File file : oldTsfileArray) {\n+          if (!file.renameTo(fsFactory.getFile(upgradeFolder, file.getName()))) {\n+            logger.error(\"Failed to move {} to upgrade folder\", file);\n+          }\n+        }\n+        // move .resource to upgrade folder\n+        for (File file : oldResourceFileArray) {\n+          if (!file.renameTo(fsFactory.getFile(upgradeFolder, file.getName()))) {\n+            logger.error(\"Failed to move {} to upgrade folder\", file);\n+          }\n+        }\n+\n+        Collections.addAll(upgradeFiles,\n+            fsFactory.listFilesBySuffix(upgradeFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+      }\n+      // if already move old files to upgradeFolder \n+      else if (upgradeFolder.exists()) {\n+        Collections.addAll(upgradeFiles,\n+            fsFactory.listFilesBySuffix(upgradeFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+      }\n+\n       File[] subFiles = fileFolder.listFiles();\n       if (subFiles != null) {\n         for (File partitionFolder : subFiles) {\n-          // some TsFileResource may be being persisted when the system crashed, try recovering such\n-          // resources\n-          continueFailedRenames(partitionFolder, TEMP_SUFFIX);\n-\n-          // some TsFiles were going to be replaced by the merged files when the system crashed and\n-          // the process was interrupted before the merged files could be named\n-          continueFailedRenames(partitionFolder, MERGE_SUFFIX);\n-\n-        if (!partitionFolder.isDirectory()) {\n-          logger.warn(\"{} is not a directory.\", partitionFolder.getAbsolutePath());\n-          continue;\n-        }\n-\n-        Collections.addAll(tsFiles,\n-            fsFactory.listFilesBySuffix(partitionFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+          if (!partitionFolder.isDirectory()) {\n+            logger.warn(\"{} is not a directory.\", partitionFolder.getAbsolutePath());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 188}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyNTc1OQ==", "bodyText": "This check is for checking if there are other irrelevant files such as the .DS_Store in MacOS", "url": "https://github.com/apache/iotdb/pull/983#discussion_r423425759", "createdAt": "2020-05-12T02:26:36Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -316,40 +377,93 @@ private VersionController getVersionControllerByTimePartitionId(long timePartiti\n         });\n   }\n \n-  private List<TsFileResource> getAllFiles(List<String> folders) {\n+  private Pair<List<TsFileResource>, List<TsFileResource>> getAllFiles(List<String> folders) throws IOException {\n     List<File> tsFiles = new ArrayList<>();\n+    List<File> upgradeFiles = new ArrayList<>();\n     for (String baseDir : folders) {\n       File fileFolder = fsFactory.getFile(baseDir, storageGroupName);\n       if (!fileFolder.exists()) {\n         continue;\n       }\n \n+      // old version\n+      // some TsFileResource may be being persisted when the system crashed, try recovering such\n+      // resources\n+      continueFailedRenames(fileFolder, TEMP_SUFFIX);\n+\n+      // some TsFiles were going to be replaced by the merged files when the system crashed and\n+      // the process was interrupted before the merged files could be named\n+      continueFailedRenames(fileFolder, MERGE_SUFFIX);\n+\n+      File[] oldTsfileArray = fsFactory.listFilesBySuffix(fileFolder.getAbsolutePath(), TSFILE_SUFFIX);\n+      File[] oldResourceFileArray = fsFactory.listFilesBySuffix(fileFolder.getAbsolutePath(), TsFileResource.RESOURCE_SUFFIX);\n+      File upgradeFolder = fsFactory.getFile(fileFolder, IoTDBConstant.UPGRADE_FOLDER_NAME);\n+      // move the old files to upgrade folder if exists\n+      if (oldTsfileArray.length != 0 || oldResourceFileArray.length != 0) {\n+        // create upgrade directory if not exist\n+        if (upgradeFolder.mkdirs()) {\n+          logger.info(\"Upgrade Directory {} doesn't exist, create it\",\n+              upgradeFolder.getPath());\n+        } else if (!upgradeFolder.exists()) {\n+          logger.error(\"Create upgrade Directory {} failed\",\n+              upgradeFolder.getPath());\n+        }\n+        // move .tsfile to upgrade folder\n+        for (File file : oldTsfileArray) {\n+          if (!file.renameTo(fsFactory.getFile(upgradeFolder, file.getName()))) {\n+            logger.error(\"Failed to move {} to upgrade folder\", file);\n+          }\n+        }\n+        // move .resource to upgrade folder\n+        for (File file : oldResourceFileArray) {\n+          if (!file.renameTo(fsFactory.getFile(upgradeFolder, file.getName()))) {\n+            logger.error(\"Failed to move {} to upgrade folder\", file);\n+          }\n+        }\n+\n+        Collections.addAll(upgradeFiles,\n+            fsFactory.listFilesBySuffix(upgradeFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+      }\n+      // if already move old files to upgradeFolder \n+      else if (upgradeFolder.exists()) {\n+        Collections.addAll(upgradeFiles,\n+            fsFactory.listFilesBySuffix(upgradeFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+      }\n+\n       File[] subFiles = fileFolder.listFiles();\n       if (subFiles != null) {\n         for (File partitionFolder : subFiles) {\n-          // some TsFileResource may be being persisted when the system crashed, try recovering such\n-          // resources\n-          continueFailedRenames(partitionFolder, TEMP_SUFFIX);\n-\n-          // some TsFiles were going to be replaced by the merged files when the system crashed and\n-          // the process was interrupted before the merged files could be named\n-          continueFailedRenames(partitionFolder, MERGE_SUFFIX);\n-\n-        if (!partitionFolder.isDirectory()) {\n-          logger.warn(\"{} is not a directory.\", partitionFolder.getAbsolutePath());\n-          continue;\n-        }\n-\n-        Collections.addAll(tsFiles,\n-            fsFactory.listFilesBySuffix(partitionFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+          if (!partitionFolder.isDirectory()) {\n+            logger.warn(\"{} is not a directory.\", partitionFolder.getAbsolutePath());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjYxMjIyOA=="}, "originalCommit": {"oid": "e42cbb6a5bae08661155c7927ae6b97e9012aa05"}, "originalPosition": 188}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMjU1NjQ0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMjo1MDoxMVrOGTKvHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQwMjoyMDo0OFrOGTzxbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc1MjAyOQ==", "bodyText": "I think a name like updateLastestFlushedTime suits better.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422752029", "createdAt": "2020-05-11T02:50:11Z", "author": {"login": "jt2594838"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -298,6 +319,46 @@ private void updatePartitionFileVersion(long partitionNum, long fileVersion) {\n     }\n   }\n \n+  /**\n+   * use old seq file to update latestTimeForEachDevice, globalLatestFlushedTimeForEachDevice,\n+   * partitionLatestFlushedTimeForEachDevice and timePartitionIdVersionControllerMap\n+   *\n+   */\n+  private void doUpdate() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "80ac01444d906bf153c8fffb32c316e06d2eb921"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzQyNDM2NA==", "bodyText": "Fixed", "url": "https://github.com/apache/iotdb/pull/983#discussion_r423424364", "createdAt": "2020-05-12T02:20:48Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -298,6 +319,46 @@ private void updatePartitionFileVersion(long partitionNum, long fileVersion) {\n     }\n   }\n \n+  /**\n+   * use old seq file to update latestTimeForEachDevice, globalLatestFlushedTimeForEachDevice,\n+   * partitionLatestFlushedTimeForEachDevice and timePartitionIdVersionControllerMap\n+   *\n+   */\n+  private void doUpdate() throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc1MjAyOQ=="}, "originalCommit": {"oid": "80ac01444d906bf153c8fffb32c316e06d2eb921"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYzMjYwNjk3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwMzoyNjowM1rOGTLMBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMVQwODoyMDoxMVrOGTRrtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc1OTQzMA==", "bodyText": "This is my understanding:\nFiles that are already in the partitioned folders (like data/sg1/0/1-1-1.tsfile) will not be upgraded, but files that are not in the partitioned folders  (like data/sg1/1-1-1.tsfile) will be moved to an upgrading folder (like data/sg1/upgrade/1-1-1.tsfile), upgraded and reloaded into IoTDB.\nBut what if the files in the partitioned folders also need upgrading?", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422759430", "createdAt": "2020-05-11T03:26:03Z", "author": {"login": "jt2594838"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -316,40 +377,93 @@ private VersionController getVersionControllerByTimePartitionId(long timePartiti\n         });\n   }\n \n-  private List<TsFileResource> getAllFiles(List<String> folders) {\n+  private Pair<List<TsFileResource>, List<TsFileResource>> getAllFiles(List<String> folders) throws IOException {\n     List<File> tsFiles = new ArrayList<>();\n+    List<File> upgradeFiles = new ArrayList<>();\n     for (String baseDir : folders) {\n       File fileFolder = fsFactory.getFile(baseDir, storageGroupName);\n       if (!fileFolder.exists()) {\n         continue;\n       }\n \n+      // old version\n+      // some TsFileResource may be being persisted when the system crashed, try recovering such\n+      // resources\n+      continueFailedRenames(fileFolder, TEMP_SUFFIX);\n+\n+      // some TsFiles were going to be replaced by the merged files when the system crashed and\n+      // the process was interrupted before the merged files could be named\n+      continueFailedRenames(fileFolder, MERGE_SUFFIX);\n+\n+      File[] oldTsfileArray = fsFactory.listFilesBySuffix(fileFolder.getAbsolutePath(), TSFILE_SUFFIX);\n+      File[] oldResourceFileArray = fsFactory.listFilesBySuffix(fileFolder.getAbsolutePath(), TsFileResource.RESOURCE_SUFFIX);\n+      File upgradeFolder = fsFactory.getFile(fileFolder, IoTDBConstant.UPGRADE_FOLDER_NAME);\n+      // move the old files to upgrade folder if exists\n+      if (oldTsfileArray.length != 0 || oldResourceFileArray.length != 0) {\n+        // create upgrade directory if not exist\n+        if (upgradeFolder.mkdirs()) {\n+          logger.info(\"Upgrade Directory {} doesn't exist, create it\",\n+              upgradeFolder.getPath());\n+        } else if (!upgradeFolder.exists()) {\n+          logger.error(\"Create upgrade Directory {} failed\",\n+              upgradeFolder.getPath());\n+        }\n+        // move .tsfile to upgrade folder\n+        for (File file : oldTsfileArray) {\n+          if (!file.renameTo(fsFactory.getFile(upgradeFolder, file.getName()))) {\n+            logger.error(\"Failed to move {} to upgrade folder\", file);\n+          }\n+        }\n+        // move .resource to upgrade folder\n+        for (File file : oldResourceFileArray) {\n+          if (!file.renameTo(fsFactory.getFile(upgradeFolder, file.getName()))) {\n+            logger.error(\"Failed to move {} to upgrade folder\", file);\n+          }\n+        }\n+\n+        Collections.addAll(upgradeFiles,\n+            fsFactory.listFilesBySuffix(upgradeFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+      }\n+      // if already move old files to upgradeFolder \n+      else if (upgradeFolder.exists()) {\n+        Collections.addAll(upgradeFiles,\n+            fsFactory.listFilesBySuffix(upgradeFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+      }\n+\n       File[] subFiles = fileFolder.listFiles();\n       if (subFiles != null) {\n         for (File partitionFolder : subFiles) {\n-          // some TsFileResource may be being persisted when the system crashed, try recovering such\n-          // resources\n-          continueFailedRenames(partitionFolder, TEMP_SUFFIX);\n-\n-          // some TsFiles were going to be replaced by the merged files when the system crashed and\n-          // the process was interrupted before the merged files could be named\n-          continueFailedRenames(partitionFolder, MERGE_SUFFIX);\n-\n-        if (!partitionFolder.isDirectory()) {\n-          logger.warn(\"{} is not a directory.\", partitionFolder.getAbsolutePath());\n-          continue;\n-        }\n-\n-        Collections.addAll(tsFiles,\n-            fsFactory.listFilesBySuffix(partitionFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+          if (!partitionFolder.isDirectory()) {\n+            logger.warn(\"{} is not a directory.\", partitionFolder.getAbsolutePath());\n+          } else if (!partitionFolder.getName().equals(IoTDBConstant.UPGRADE_FOLDER_NAME)) {\n+            // some TsFileResource may be being persisted when the system crashed, try recovering such\n+            // resources\n+            continueFailedRenames(partitionFolder, TEMP_SUFFIX);\n+\n+            // some TsFiles were going to be replaced by the merged files when the system crashed and\n+            // the process was interrupted before the merged files could be named\n+            continueFailedRenames(partitionFolder, MERGE_SUFFIX);\n+\n+            Collections.addAll(tsFiles,\n+                fsFactory.listFilesBySuffix(partitionFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+          }\n         }\n       }\n \n     }\n     tsFiles.sort(this::compareFileName);\n     List<TsFileResource> ret = new ArrayList<>();\n     tsFiles.forEach(f -> ret.add(new TsFileResource(f)));\n-    return ret;\n+    upgradeFiles.sort(this::compareFileName);\n+    List<TsFileResource> upgradeRet = new ArrayList<>();\n+    for (File f : upgradeFiles) {\n+      TsFileResource fileResource = new TsFileResource(f);\n+      fileResource.setClosed(true);\n+      // make sure the flush command is called before IoTDB is down.\n+      fileResource.deserialize();\n+      upgradeRet.add(fileResource);\n+    }\n+    return new Pair<>(ret, upgradeRet);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "80ac01444d906bf153c8fffb32c316e06d2eb921"}, "originalPosition": 218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc4NTc3MA==", "bodyText": "This upgrade is only for 0.9.x -> 0.10.x, if they are in partitioned folder, they should already  be the 0.10.x version, because we do not have partition folder in 0.9.x", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422785770", "createdAt": "2020-05-11T05:17:46Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -316,40 +377,93 @@ private VersionController getVersionControllerByTimePartitionId(long timePartiti\n         });\n   }\n \n-  private List<TsFileResource> getAllFiles(List<String> folders) {\n+  private Pair<List<TsFileResource>, List<TsFileResource>> getAllFiles(List<String> folders) throws IOException {\n     List<File> tsFiles = new ArrayList<>();\n+    List<File> upgradeFiles = new ArrayList<>();\n     for (String baseDir : folders) {\n       File fileFolder = fsFactory.getFile(baseDir, storageGroupName);\n       if (!fileFolder.exists()) {\n         continue;\n       }\n \n+      // old version\n+      // some TsFileResource may be being persisted when the system crashed, try recovering such\n+      // resources\n+      continueFailedRenames(fileFolder, TEMP_SUFFIX);\n+\n+      // some TsFiles were going to be replaced by the merged files when the system crashed and\n+      // the process was interrupted before the merged files could be named\n+      continueFailedRenames(fileFolder, MERGE_SUFFIX);\n+\n+      File[] oldTsfileArray = fsFactory.listFilesBySuffix(fileFolder.getAbsolutePath(), TSFILE_SUFFIX);\n+      File[] oldResourceFileArray = fsFactory.listFilesBySuffix(fileFolder.getAbsolutePath(), TsFileResource.RESOURCE_SUFFIX);\n+      File upgradeFolder = fsFactory.getFile(fileFolder, IoTDBConstant.UPGRADE_FOLDER_NAME);\n+      // move the old files to upgrade folder if exists\n+      if (oldTsfileArray.length != 0 || oldResourceFileArray.length != 0) {\n+        // create upgrade directory if not exist\n+        if (upgradeFolder.mkdirs()) {\n+          logger.info(\"Upgrade Directory {} doesn't exist, create it\",\n+              upgradeFolder.getPath());\n+        } else if (!upgradeFolder.exists()) {\n+          logger.error(\"Create upgrade Directory {} failed\",\n+              upgradeFolder.getPath());\n+        }\n+        // move .tsfile to upgrade folder\n+        for (File file : oldTsfileArray) {\n+          if (!file.renameTo(fsFactory.getFile(upgradeFolder, file.getName()))) {\n+            logger.error(\"Failed to move {} to upgrade folder\", file);\n+          }\n+        }\n+        // move .resource to upgrade folder\n+        for (File file : oldResourceFileArray) {\n+          if (!file.renameTo(fsFactory.getFile(upgradeFolder, file.getName()))) {\n+            logger.error(\"Failed to move {} to upgrade folder\", file);\n+          }\n+        }\n+\n+        Collections.addAll(upgradeFiles,\n+            fsFactory.listFilesBySuffix(upgradeFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+      }\n+      // if already move old files to upgradeFolder \n+      else if (upgradeFolder.exists()) {\n+        Collections.addAll(upgradeFiles,\n+            fsFactory.listFilesBySuffix(upgradeFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+      }\n+\n       File[] subFiles = fileFolder.listFiles();\n       if (subFiles != null) {\n         for (File partitionFolder : subFiles) {\n-          // some TsFileResource may be being persisted when the system crashed, try recovering such\n-          // resources\n-          continueFailedRenames(partitionFolder, TEMP_SUFFIX);\n-\n-          // some TsFiles were going to be replaced by the merged files when the system crashed and\n-          // the process was interrupted before the merged files could be named\n-          continueFailedRenames(partitionFolder, MERGE_SUFFIX);\n-\n-        if (!partitionFolder.isDirectory()) {\n-          logger.warn(\"{} is not a directory.\", partitionFolder.getAbsolutePath());\n-          continue;\n-        }\n-\n-        Collections.addAll(tsFiles,\n-            fsFactory.listFilesBySuffix(partitionFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+          if (!partitionFolder.isDirectory()) {\n+            logger.warn(\"{} is not a directory.\", partitionFolder.getAbsolutePath());\n+          } else if (!partitionFolder.getName().equals(IoTDBConstant.UPGRADE_FOLDER_NAME)) {\n+            // some TsFileResource may be being persisted when the system crashed, try recovering such\n+            // resources\n+            continueFailedRenames(partitionFolder, TEMP_SUFFIX);\n+\n+            // some TsFiles were going to be replaced by the merged files when the system crashed and\n+            // the process was interrupted before the merged files could be named\n+            continueFailedRenames(partitionFolder, MERGE_SUFFIX);\n+\n+            Collections.addAll(tsFiles,\n+                fsFactory.listFilesBySuffix(partitionFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+          }\n         }\n       }\n \n     }\n     tsFiles.sort(this::compareFileName);\n     List<TsFileResource> ret = new ArrayList<>();\n     tsFiles.forEach(f -> ret.add(new TsFileResource(f)));\n-    return ret;\n+    upgradeFiles.sort(this::compareFileName);\n+    List<TsFileResource> upgradeRet = new ArrayList<>();\n+    for (File f : upgradeFiles) {\n+      TsFileResource fileResource = new TsFileResource(f);\n+      fileResource.setClosed(true);\n+      // make sure the flush command is called before IoTDB is down.\n+      fileResource.deserialize();\n+      upgradeRet.add(fileResource);\n+    }\n+    return new Pair<>(ret, upgradeRet);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc1OTQzMA=="}, "originalCommit": {"oid": "80ac01444d906bf153c8fffb32c316e06d2eb921"}, "originalPosition": 218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjg2NTg0NA==", "bodyText": "Then you should make sure that later versions of 0.10 will never change the file structure, or it will be painful.", "url": "https://github.com/apache/iotdb/pull/983#discussion_r422865844", "createdAt": "2020-05-11T08:20:11Z", "author": {"login": "jt2594838"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -316,40 +377,93 @@ private VersionController getVersionControllerByTimePartitionId(long timePartiti\n         });\n   }\n \n-  private List<TsFileResource> getAllFiles(List<String> folders) {\n+  private Pair<List<TsFileResource>, List<TsFileResource>> getAllFiles(List<String> folders) throws IOException {\n     List<File> tsFiles = new ArrayList<>();\n+    List<File> upgradeFiles = new ArrayList<>();\n     for (String baseDir : folders) {\n       File fileFolder = fsFactory.getFile(baseDir, storageGroupName);\n       if (!fileFolder.exists()) {\n         continue;\n       }\n \n+      // old version\n+      // some TsFileResource may be being persisted when the system crashed, try recovering such\n+      // resources\n+      continueFailedRenames(fileFolder, TEMP_SUFFIX);\n+\n+      // some TsFiles were going to be replaced by the merged files when the system crashed and\n+      // the process was interrupted before the merged files could be named\n+      continueFailedRenames(fileFolder, MERGE_SUFFIX);\n+\n+      File[] oldTsfileArray = fsFactory.listFilesBySuffix(fileFolder.getAbsolutePath(), TSFILE_SUFFIX);\n+      File[] oldResourceFileArray = fsFactory.listFilesBySuffix(fileFolder.getAbsolutePath(), TsFileResource.RESOURCE_SUFFIX);\n+      File upgradeFolder = fsFactory.getFile(fileFolder, IoTDBConstant.UPGRADE_FOLDER_NAME);\n+      // move the old files to upgrade folder if exists\n+      if (oldTsfileArray.length != 0 || oldResourceFileArray.length != 0) {\n+        // create upgrade directory if not exist\n+        if (upgradeFolder.mkdirs()) {\n+          logger.info(\"Upgrade Directory {} doesn't exist, create it\",\n+              upgradeFolder.getPath());\n+        } else if (!upgradeFolder.exists()) {\n+          logger.error(\"Create upgrade Directory {} failed\",\n+              upgradeFolder.getPath());\n+        }\n+        // move .tsfile to upgrade folder\n+        for (File file : oldTsfileArray) {\n+          if (!file.renameTo(fsFactory.getFile(upgradeFolder, file.getName()))) {\n+            logger.error(\"Failed to move {} to upgrade folder\", file);\n+          }\n+        }\n+        // move .resource to upgrade folder\n+        for (File file : oldResourceFileArray) {\n+          if (!file.renameTo(fsFactory.getFile(upgradeFolder, file.getName()))) {\n+            logger.error(\"Failed to move {} to upgrade folder\", file);\n+          }\n+        }\n+\n+        Collections.addAll(upgradeFiles,\n+            fsFactory.listFilesBySuffix(upgradeFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+      }\n+      // if already move old files to upgradeFolder \n+      else if (upgradeFolder.exists()) {\n+        Collections.addAll(upgradeFiles,\n+            fsFactory.listFilesBySuffix(upgradeFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+      }\n+\n       File[] subFiles = fileFolder.listFiles();\n       if (subFiles != null) {\n         for (File partitionFolder : subFiles) {\n-          // some TsFileResource may be being persisted when the system crashed, try recovering such\n-          // resources\n-          continueFailedRenames(partitionFolder, TEMP_SUFFIX);\n-\n-          // some TsFiles were going to be replaced by the merged files when the system crashed and\n-          // the process was interrupted before the merged files could be named\n-          continueFailedRenames(partitionFolder, MERGE_SUFFIX);\n-\n-        if (!partitionFolder.isDirectory()) {\n-          logger.warn(\"{} is not a directory.\", partitionFolder.getAbsolutePath());\n-          continue;\n-        }\n-\n-        Collections.addAll(tsFiles,\n-            fsFactory.listFilesBySuffix(partitionFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+          if (!partitionFolder.isDirectory()) {\n+            logger.warn(\"{} is not a directory.\", partitionFolder.getAbsolutePath());\n+          } else if (!partitionFolder.getName().equals(IoTDBConstant.UPGRADE_FOLDER_NAME)) {\n+            // some TsFileResource may be being persisted when the system crashed, try recovering such\n+            // resources\n+            continueFailedRenames(partitionFolder, TEMP_SUFFIX);\n+\n+            // some TsFiles were going to be replaced by the merged files when the system crashed and\n+            // the process was interrupted before the merged files could be named\n+            continueFailedRenames(partitionFolder, MERGE_SUFFIX);\n+\n+            Collections.addAll(tsFiles,\n+                fsFactory.listFilesBySuffix(partitionFolder.getAbsolutePath(), TSFILE_SUFFIX));\n+          }\n         }\n       }\n \n     }\n     tsFiles.sort(this::compareFileName);\n     List<TsFileResource> ret = new ArrayList<>();\n     tsFiles.forEach(f -> ret.add(new TsFileResource(f)));\n-    return ret;\n+    upgradeFiles.sort(this::compareFileName);\n+    List<TsFileResource> upgradeRet = new ArrayList<>();\n+    for (File f : upgradeFiles) {\n+      TsFileResource fileResource = new TsFileResource(f);\n+      fileResource.setClosed(true);\n+      // make sure the flush command is called before IoTDB is down.\n+      fileResource.deserialize();\n+      upgradeRet.add(fileResource);\n+    }\n+    return new Pair<>(ret, upgradeRet);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjc1OTQzMA=="}, "originalCommit": {"oid": "80ac01444d906bf153c8fffb32c316e06d2eb921"}, "originalPosition": 218}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzQ3NzMyOnYy", "diffSide": "RIGHT", "path": "tsfile/src/main/java/org/apache/iotdb/tsfile/v1/file/metadata/OldChunkGroupMetaData.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwMjozNjozNlrOGWXqHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwNDozMjozMVrOGWYIrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEwOTQ3MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public class OldChunkGroupMetaData {\n          \n          \n            \n            public class ChunkGroupMetaDataV1 {", "url": "https://github.com/apache/iotdb/pull/983#discussion_r426109471", "createdAt": "2020-05-16T02:36:36Z", "author": {"login": "qiaojialin"}, "path": "tsfile/src/main/java/org/apache/iotdb/tsfile/v1/file/metadata/OldChunkGroupMetaData.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iotdb.tsfile.v1.file.metadata;\n+\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Metadata of ChunkGroup.\n+ */\n+public class OldChunkGroupMetaData {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dab1f48c8f6a797e5a5f2797d54536943be59588"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjExNzI5Mg==", "bodyText": "Fixed", "url": "https://github.com/apache/iotdb/pull/983#discussion_r426117292", "createdAt": "2020-05-16T04:32:31Z", "author": {"login": "HTHou"}, "path": "tsfile/src/main/java/org/apache/iotdb/tsfile/v1/file/metadata/OldChunkGroupMetaData.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iotdb.tsfile.v1.file.metadata;\n+\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+/**\n+ * Metadata of ChunkGroup.\n+ */\n+public class OldChunkGroupMetaData {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjEwOTQ3MQ=="}, "originalCommit": {"oid": "dab1f48c8f6a797e5a5f2797d54536943be59588"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzQ4NDI3OnYy", "diffSide": "RIGHT", "path": "tsfile/src/main/java/org/apache/iotdb/tsfile/v1/read/TsFileSequenceReaderForOldFile.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwMjo0ODo0NlrOGWXtpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwNDozMjoxM1rOGWYImQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjExMDM3NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            public class TsFileSequenceReaderForOldFile extends TsFileSequenceReader {\n          \n          \n            \n            public class TsFileSequenceReaderForV1 extends TsFileSequenceReader {", "url": "https://github.com/apache/iotdb/pull/983#discussion_r426110374", "createdAt": "2020-05-16T02:48:46Z", "author": {"login": "qiaojialin"}, "path": "tsfile/src/main/java/org/apache/iotdb/tsfile/v1/read/TsFileSequenceReaderForOldFile.java", "diffHunk": "@@ -0,0 +1,409 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.tsfile.v1.read;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.statistics.Statistics;\n+import org.apache.iotdb.tsfile.read.TsFileSequenceReader;\n+import org.apache.iotdb.tsfile.read.common.Chunk;\n+import org.apache.iotdb.tsfile.read.common.Path;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.utils.BloomFilter;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.v1.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.v1.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.v1.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.v1.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.v1.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.v1.file.metadata.TimeseriesMetadataForOldFile;\n+import org.apache.iotdb.tsfile.v1.file.utils.HeaderUtils;\n+\n+public class TsFileSequenceReaderForOldFile extends TsFileSequenceReader {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dab1f48c8f6a797e5a5f2797d54536943be59588"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjExNzI3Mw==", "bodyText": "Fixed", "url": "https://github.com/apache/iotdb/pull/983#discussion_r426117273", "createdAt": "2020-05-16T04:32:13Z", "author": {"login": "HTHou"}, "path": "tsfile/src/main/java/org/apache/iotdb/tsfile/v1/read/TsFileSequenceReaderForOldFile.java", "diffHunk": "@@ -0,0 +1,409 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.tsfile.v1.read;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Map.Entry;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+\n+import org.apache.iotdb.tsfile.common.conf.TSFileConfig;\n+import org.apache.iotdb.tsfile.file.header.ChunkHeader;\n+import org.apache.iotdb.tsfile.file.header.PageHeader;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.TimeseriesMetadata;\n+import org.apache.iotdb.tsfile.file.metadata.enums.TSDataType;\n+import org.apache.iotdb.tsfile.file.metadata.statistics.Statistics;\n+import org.apache.iotdb.tsfile.read.TsFileSequenceReader;\n+import org.apache.iotdb.tsfile.read.common.Chunk;\n+import org.apache.iotdb.tsfile.read.common.Path;\n+import org.apache.iotdb.tsfile.read.reader.TsFileInput;\n+import org.apache.iotdb.tsfile.utils.BloomFilter;\n+import org.apache.iotdb.tsfile.utils.ReadWriteIOUtils;\n+import org.apache.iotdb.tsfile.v1.file.metadata.OldChunkGroupMetaData;\n+import org.apache.iotdb.tsfile.v1.file.metadata.OldChunkMetadata;\n+import org.apache.iotdb.tsfile.v1.file.metadata.OldTsDeviceMetadata;\n+import org.apache.iotdb.tsfile.v1.file.metadata.OldTsDeviceMetadataIndex;\n+import org.apache.iotdb.tsfile.v1.file.metadata.OldTsFileMetadata;\n+import org.apache.iotdb.tsfile.v1.file.metadata.TimeseriesMetadataForOldFile;\n+import org.apache.iotdb.tsfile.v1.file.utils.HeaderUtils;\n+\n+public class TsFileSequenceReaderForOldFile extends TsFileSequenceReader {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjExMDM3NA=="}, "originalCommit": {"oid": "dab1f48c8f6a797e5a5f2797d54536943be59588"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY1MzQ4ODg3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/query/control/FileReaderManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwMjo1NjoxOFrOGWXv5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQwNDozMjo0M1rOGWYIvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjExMDk1MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    try (TsFileSequenceReader reader = new TsFileSequenceReader(filePath)) {\n          \n          \n            \n                      if (reader.readVersionNumber().equals(TSFileConfig.OLD_VERSION)) {\n          \n          \n            \n                        tsFileReader = new TsFileSequenceReaderForOldFile(filePath);\n          \n          \n            \n                      }\n          \n          \n            \n                      else {\n          \n          \n            \n                        tsFileReader = new TsFileSequenceReader(filePath);\n          \n          \n            \n                      }\n          \n          \n            \n                    }\n          \n          \n            \n                    tsfileReader = new TsFileSequenceReader(filePath);\n          \n          \n            \n                    switch (tsfileReader.readVersionNumber) {\n          \n          \n            \n                        case TsFileConfig.V1:\n          \n          \n            \n                            reader.close();\n          \n          \n            \n                            tsfileReader = new TsFileSequenceReaderForOldFile(filePath);\n          \n          \n            \n                            break;\n          \n          \n            \n                        case TsFileConfig.V2:\n          \n          \n            \n                            break;\n          \n          \n            \n                    }", "url": "https://github.com/apache/iotdb/pull/983#discussion_r426110951", "createdAt": "2020-05-16T02:56:18Z", "author": {"login": "qiaojialin"}, "path": "server/src/main/java/org/apache/iotdb/db/query/control/FileReaderManager.java", "diffHunk": "@@ -158,9 +160,21 @@ public synchronized TsFileSequenceReader get(String filePath, boolean isClosed)\n         logger.warn(\"Query has opened {} files !\", readerMap.size());\n       }\n \n-      TsFileSequenceReader tsFileReader = !isClosed ? new UnClosedTsFileReader(filePath)\n-          : new TsFileSequenceReader(filePath);\n-\n+      TsFileSequenceReader tsFileReader = null;\n+      // check if the file is old version\n+      if (!isClosed) {\n+        tsFileReader = new UnClosedTsFileReader(filePath);\n+      }\n+      else {\n+        try (TsFileSequenceReader reader = new TsFileSequenceReader(filePath)) {\n+          if (reader.readVersionNumber().equals(TSFileConfig.OLD_VERSION)) {\n+            tsFileReader = new TsFileSequenceReaderForOldFile(filePath);\n+          }\n+          else {\n+            tsFileReader = new TsFileSequenceReader(filePath);\n+          }\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dab1f48c8f6a797e5a5f2797d54536943be59588"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjExNzMwOQ==", "bodyText": "Fixed", "url": "https://github.com/apache/iotdb/pull/983#discussion_r426117309", "createdAt": "2020-05-16T04:32:43Z", "author": {"login": "HTHou"}, "path": "server/src/main/java/org/apache/iotdb/db/query/control/FileReaderManager.java", "diffHunk": "@@ -158,9 +160,21 @@ public synchronized TsFileSequenceReader get(String filePath, boolean isClosed)\n         logger.warn(\"Query has opened {} files !\", readerMap.size());\n       }\n \n-      TsFileSequenceReader tsFileReader = !isClosed ? new UnClosedTsFileReader(filePath)\n-          : new TsFileSequenceReader(filePath);\n-\n+      TsFileSequenceReader tsFileReader = null;\n+      // check if the file is old version\n+      if (!isClosed) {\n+        tsFileReader = new UnClosedTsFileReader(filePath);\n+      }\n+      else {\n+        try (TsFileSequenceReader reader = new TsFileSequenceReader(filePath)) {\n+          if (reader.readVersionNumber().equals(TSFileConfig.OLD_VERSION)) {\n+            tsFileReader = new TsFileSequenceReaderForOldFile(filePath);\n+          }\n+          else {\n+            tsFileReader = new TsFileSequenceReader(filePath);\n+          }\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjExMDk1MQ=="}, "originalCommit": {"oid": "dab1f48c8f6a797e5a5f2797d54536943be59588"}, "originalPosition": 31}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 184, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}