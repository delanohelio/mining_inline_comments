{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkyMTY4Njgw", "number": 1758, "reviewThreads": {"totalCount": 26, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNzowNTozN1rOEql77A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNToxOTowOFrOEv1ZxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDk3MTk2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/cache/FileChunkPointSizeCache.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNzowNTozN1rOHc4mGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwNzowNTozN1rOHc4mGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDA0OTQzMg==", "bodyText": "this reader should be closed here", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r500049432", "createdAt": "2020-10-06T07:05:37Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/cache/FileChunkPointSizeCache.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iotdb.db.engine.cache;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iotdb.tsfile.file.metadata.ChunkMetadata;\n+import org.apache.iotdb.tsfile.read.TsFileSequenceReader;\n+import org.apache.iotdb.tsfile.read.common.Path;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class FileChunkPointSizeCache {\n+\n+  private static final Logger logger = LoggerFactory.getLogger(FileChunkPointSizeCache.class);\n+\n+  // (absolute path,avg chunk point size)\n+  private Map<String, Map<String, Long>> tsfileDeviceChunkPointCache;\n+\n+  private FileChunkPointSizeCache() {\n+    tsfileDeviceChunkPointCache = new HashMap<>();\n+  }\n+\n+  public static FileChunkPointSizeCache getInstance() {\n+    return FileChunkPointSizeCacheHolder.INSTANCE;\n+  }\n+\n+  public Map<String, Long> get(File tsfile) {\n+    String path = tsfile.getAbsolutePath();\n+    return tsfileDeviceChunkPointCache.computeIfAbsent(path, k -> {\n+      Map<String, Long> deviceChunkPointMap = new HashMap<>();\n+      try {\n+        if (tsfile.exists()) {\n+          TsFileSequenceReader reader = new TsFileSequenceReader(path);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc9c37b6b2d28cef0174a24a752426789f5f294c"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMTMzNTM5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/TsFileProcessor.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwODo0NDoyN1rOHc8G6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNlQwODo0NDoyN1rOHc8G6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDEwNjk4Ng==", "bodyText": "seems this function is never used", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r500106986", "createdAt": "2020-10-06T08:44:27Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/TsFileProcessor.java", "diffHunk": "@@ -644,8 +650,29 @@ public void flushOneMemTable() {\n     }\n   }\n \n+  private void updateDeviceChunkPointSizeCache() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc9c37b6b2d28cef0174a24a752426789f5f294c"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDc4Mzg3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/merge/manage/MergeManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMToyMDoxN1rOHhOOAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxMjoyNzowMVrOHk5wug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDU5ODAxNg==", "bodyText": "every time using getMergeWriteRateLimiter() will call this set method setWriteMergeRate ,if this para can be changed after IoTDB start, this is OK, otherwise...", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504598016", "createdAt": "2020-10-14T11:20:17Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/merge/manage/MergeManager.java", "diffHunk": "@@ -74,13 +75,18 @@\n   private MergeManager() {\n   }\n \n-  public RateLimiter getMergeRateLimiter() {\n-    setMergeRate(IoTDBDescriptor.getInstance().getConfig().getMergeThroughputMbPerSec());\n-    return mergeRateLimiter;\n+  public RateLimiter getMergeWriteRateLimiter() {\n+    setWriteMergeRate(IoTDBDescriptor.getInstance().getConfig().getMergeWriteThroughputMbPerSec());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1NzE0Ng==", "bodyText": "It may can be set afterward in cli", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508457146", "createdAt": "2020-10-20T12:27:01Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/merge/manage/MergeManager.java", "diffHunk": "@@ -74,13 +75,18 @@\n   private MergeManager() {\n   }\n \n-  public RateLimiter getMergeRateLimiter() {\n-    setMergeRate(IoTDBDescriptor.getInstance().getConfig().getMergeThroughputMbPerSec());\n-    return mergeRateLimiter;\n+  public RateLimiter getMergeWriteRateLimiter() {\n+    setWriteMergeRate(IoTDBDescriptor.getInstance().getConfig().getMergeWriteThroughputMbPerSec());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDU5ODAxNg=="}, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDc5MzU2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMToyMzoyMFrOHhOT2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxMjoyODo0NVrOHk51Aw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDU5OTUxMw==", "bodyText": "it would be better to set it as a private variable", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504599513", "createdAt": "2020-10-14T11:23:20Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -124,7 +116,7 @@\n  */\n public class StorageGroupProcessor {\n \n-  private static final String MERGING_MODIFICATION_FILE_NAME = \"merge.mods\";\n+  public static final String MERGING_MODIFICATION_FILE_NAME = \"merge.mods\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1ODI0Mw==", "bodyText": "TsFileManage also use it", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508458243", "createdAt": "2020-10-20T12:28:45Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -124,7 +116,7 @@\n  */\n public class StorageGroupProcessor {\n \n-  private static final String MERGING_MODIFICATION_FILE_NAME = \"merge.mods\";\n+  public static final String MERGING_MODIFICATION_FILE_NAME = \"merge.mods\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDU5OTUxMw=="}, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDc5NjQ5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMToyNDoxN1rOHhOVlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxMjoyODo1NVrOHk51bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDU5OTk1Ng==", "bodyText": "it would be better to set it as a private variable", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504599956", "createdAt": "2020-10-14T11:24:17Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -208,26 +200,15 @@\n   private File storageGroupSysDir;\n \n   // manage seqFileList and unSeqFileList\n-  private TsFileManagement tsFileManagement;\n+  public TsFileManagement tsFileManagement;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1ODM1MA==", "bodyText": "okay, I will solve it", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508458350", "createdAt": "2020-10-20T12:28:55Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -208,26 +200,15 @@\n   private File storageGroupSysDir;\n \n   // manage seqFileList and unSeqFileList\n-  private TsFileManagement tsFileManagement;\n+  public TsFileManagement tsFileManagement;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDU5OTk1Ng=="}, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDgyNjQxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/TsFileProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMTozMzowNVrOHhOnkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxMjoyOTowNVrOHk515g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYwNDU2Mg==", "bodyText": "if the comment is used, it shoule be deleted then", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504604562", "createdAt": "2020-10-14T11:33:05Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/TsFileProcessor.java", "diffHunk": "@@ -647,6 +650,7 @@ public void flushOneMemTable() {\n \n   private void endFile() throws IOException, TsFileProcessorException {\n     long closeStartTime = System.currentTimeMillis();\n+//    updateDeviceChunkPointSizeCache();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1ODQ3MA==", "bodyText": "okay", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508458470", "createdAt": "2020-10-20T12:29:05Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/TsFileProcessor.java", "diffHunk": "@@ -647,6 +650,7 @@ public void flushOneMemTable() {\n \n   private void endFile() throws IOException, TsFileProcessorException {\n     long closeStartTime = System.currentTimeMillis();\n+//    updateDeviceChunkPointSizeCache();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYwNDU2Mg=="}, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDkwMjcxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMTo1NDo1MFrOHhPVDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMTo1NDo1MFrOHhPVDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYxNjIwNQ==", "bodyText": "unused comments should be deleted then", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504616205", "createdAt": "2020-10-14T11:54:50Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -67,17 +74,24 @@\n   private final int maxLevelNum = IoTDBDescriptor.getInstance().getConfig().getMaxLevelNum();\n   private final int maxFileNumInEachLevel = IoTDBDescriptor.getInstance().getConfig()\n       .getMaxFileNumInEachLevel();\n+  private final int maxUnseqLevelNum = IoTDBDescriptor.getInstance().getConfig()\n+      .getMaxUnseqLevelNum();\n+  private final int maxUnseqFileNumInEachLevel = IoTDBDescriptor.getInstance().getConfig()\n+      .getMaxFileNumInEachLevel();\n   private final int maxChunkPointNum = IoTDBDescriptor.getInstance().getConfig()\n       .getMergeChunkPointNumberThreshold();\n+  private final boolean isForceFullMerge = IoTDBDescriptor.getInstance().getConfig()\n+      .isForceFullMerge();\n   // First map is partition list; Second list is level list; Third list is file list in level;\n   private final Map<Long, List<TreeSet<TsFileResource>>> sequenceTsFileResources = new ConcurrentSkipListMap<>();\n   private final Map<Long, List<List<TsFileResource>>> unSequenceTsFileResources = new ConcurrentSkipListMap<>();\n   private final List<List<TsFileResource>> forkedSequenceTsFileResources = new ArrayList<>();\n   private final List<List<TsFileResource>> forkedUnSequenceTsFileResources = new ArrayList<>();\n-  private long forkedSeqListPointNum = 0;\n-  private Map<Path, MeasurementSchema> forkedSeqListPathMeasurementSchemaMap = new HashMap<>();\n-  private long forkedUnSeqListPointNum = 0;\n-  private Map<Path, MeasurementSchema> forkedUnSeqListPathMeasurementSchemaMap = new HashMap<>();\n+\n+//  private double forkedSeqListPointNum = 0;\n+//  private double forkedSeqListMeasurementSize = 0;\n+//  private double forkedUnSeqListPointNum = 0;\n+//  private double forkedUnSeqListMeasurementSize = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDkxNDQzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMTo1ODowMVrOHhPb-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxMjoyOToxM1rOHk52Sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYxNzk3OA==", "bodyText": "unused function here", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504617978", "createdAt": "2020-10-14T11:58:01Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -113,10 +129,11 @@ private void flushAllFilesToLastLevel(long timePartitionId,\n       List<List<TsFileResource>> currMergeFiles,\n       HotCompactionLogger hotCompactionLogger, boolean sequence) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1ODU3MA==", "bodyText": "use it now", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508458570", "createdAt": "2020-10-20T12:29:13Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -113,10 +129,11 @@ private void flushAllFilesToLastLevel(long timePartitionId,\n       List<List<TsFileResource>> currMergeFiles,\n       HotCompactionLogger hotCompactionLogger, boolean sequence) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYxNzk3OA=="}, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDkzMDY0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMjowMjozMFrOHhPl1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxMjoyOToyMlrOHk52sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyMDUwMA==", "bodyText": "remove unused code here", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504620500", "createdAt": "2020-10-14T12:02:30Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -407,107 +430,141 @@ public void recover() {\n   }\n \n   @Override\n-  public void forkCurrentFileList(long timePartition) throws IOException {\n-    Pair<Long, Map<Path, MeasurementSchema>> seqResult = forkTsFileList(\n+  public void forkCurrentFileList(long timePartition) {\n+    forkTsFileList(\n         forkedSequenceTsFileResources,\n-        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources));\n-    forkedSeqListPointNum = seqResult.left;\n-    forkedSeqListPathMeasurementSchemaMap = seqResult.right;\n-    Pair<Long, Map<Path, MeasurementSchema>> unSeqResult = forkTsFileList(\n+        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources),\n+        maxLevelNum);\n+    forkTsFileList(\n         forkedUnSequenceTsFileResources,\n         unSequenceTsFileResources\n-            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources));\n-    forkedUnSeqListPointNum = unSeqResult.left;\n-    forkedUnSeqListPathMeasurementSchemaMap = unSeqResult.right;\n+            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources), maxUnseqLevelNum);\n   }\n \n-  private Pair<Long, Map<Path, MeasurementSchema>> forkTsFileList(\n+//  private Pair<Double, Double> forkTsFileList(\n+//      List<List<TsFileResource>> forkedTsFileResources,\n+//      List rawTsFileResources, int currMaxLevel) {\n+//    forkedTsFileResources.clear();\n+//    // just fork part of the TsFile list, controlled by max_merge_chunk_point\n+//    long pointNum = 0;\n+//    // all flush to target file\n+//    ICardinality measurementSet = new HyperLogLog(13);\n+//    for (int i = 0; i < currMaxLevel - 1; i++) {\n+//      List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n+//      Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n+//          .get(i);\n+//      synchronized (levelRawTsFileResources) {\n+//        for (TsFileResource tsFileResource : levelRawTsFileResources) {\n+//          if (tsFileResource.isClosed()) {\n+//            String path = tsFileResource.getTsFile().getAbsolutePath();\n+//            try {\n+//              if (tsFileResource.getTsFile().exists()) {\n+//                TsFileSequenceReader reader = new TsFileSequenceReader(path);\n+//                List<Path> pathList = reader.getAllPaths();\n+//                for (Path sensorPath : pathList) {\n+//                  measurementSet.offer(sensorPath.getFullPath());\n+//                  List<ChunkMetadata> chunkMetadataList = reader.getChunkMetadataList(sensorPath);\n+//                  for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n+//                    pointNum += chunkMetadata.getNumOfPoints();\n+//                  }\n+//                }\n+//              } else {\n+//                logger.info(\"{} tsfile does not exist\", path);\n+//              }\n+//            } catch (IOException e) {\n+//              logger.error(\n+//                  \"{} tsfile reader creates error\", path, e);\n+//            }\n+//          }\n+//          if (measurementSet.cardinality() > 0\n+//              && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n+//            forkedLevelTsFileResources.add(tsFileResource);\n+//            break;\n+//          }\n+//          forkedLevelTsFileResources.add(tsFileResource);\n+//        }\n+//      }\n+//\n+//      if (measurementSet.cardinality() > 0\n+//          && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n+//        forkedTsFileResources.add(forkedLevelTsFileResources);\n+//        break;\n+//      }\n+//      forkedTsFileResources.add(forkedLevelTsFileResources);\n+//    }\n+//\n+//    // fill in empty file\n+//    while (forkedTsFileResources.size() < currMaxLevel) {\n+//      List<TsFileResource> emptyForkedLevelTsFileResources = new ArrayList<>();\n+//      forkedTsFileResources.add(emptyForkedLevelTsFileResources);\n+//    }\n+//\n+//    return new Pair<>((double) pointNum, (double) measurementSet.cardinality());\n+//  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 245}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1ODY3NA==", "bodyText": "use it now", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508458674", "createdAt": "2020-10-20T12:29:22Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -407,107 +430,141 @@ public void recover() {\n   }\n \n   @Override\n-  public void forkCurrentFileList(long timePartition) throws IOException {\n-    Pair<Long, Map<Path, MeasurementSchema>> seqResult = forkTsFileList(\n+  public void forkCurrentFileList(long timePartition) {\n+    forkTsFileList(\n         forkedSequenceTsFileResources,\n-        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources));\n-    forkedSeqListPointNum = seqResult.left;\n-    forkedSeqListPathMeasurementSchemaMap = seqResult.right;\n-    Pair<Long, Map<Path, MeasurementSchema>> unSeqResult = forkTsFileList(\n+        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources),\n+        maxLevelNum);\n+    forkTsFileList(\n         forkedUnSequenceTsFileResources,\n         unSequenceTsFileResources\n-            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources));\n-    forkedUnSeqListPointNum = unSeqResult.left;\n-    forkedUnSeqListPathMeasurementSchemaMap = unSeqResult.right;\n+            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources), maxUnseqLevelNum);\n   }\n \n-  private Pair<Long, Map<Path, MeasurementSchema>> forkTsFileList(\n+//  private Pair<Double, Double> forkTsFileList(\n+//      List<List<TsFileResource>> forkedTsFileResources,\n+//      List rawTsFileResources, int currMaxLevel) {\n+//    forkedTsFileResources.clear();\n+//    // just fork part of the TsFile list, controlled by max_merge_chunk_point\n+//    long pointNum = 0;\n+//    // all flush to target file\n+//    ICardinality measurementSet = new HyperLogLog(13);\n+//    for (int i = 0; i < currMaxLevel - 1; i++) {\n+//      List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n+//      Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n+//          .get(i);\n+//      synchronized (levelRawTsFileResources) {\n+//        for (TsFileResource tsFileResource : levelRawTsFileResources) {\n+//          if (tsFileResource.isClosed()) {\n+//            String path = tsFileResource.getTsFile().getAbsolutePath();\n+//            try {\n+//              if (tsFileResource.getTsFile().exists()) {\n+//                TsFileSequenceReader reader = new TsFileSequenceReader(path);\n+//                List<Path> pathList = reader.getAllPaths();\n+//                for (Path sensorPath : pathList) {\n+//                  measurementSet.offer(sensorPath.getFullPath());\n+//                  List<ChunkMetadata> chunkMetadataList = reader.getChunkMetadataList(sensorPath);\n+//                  for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n+//                    pointNum += chunkMetadata.getNumOfPoints();\n+//                  }\n+//                }\n+//              } else {\n+//                logger.info(\"{} tsfile does not exist\", path);\n+//              }\n+//            } catch (IOException e) {\n+//              logger.error(\n+//                  \"{} tsfile reader creates error\", path, e);\n+//            }\n+//          }\n+//          if (measurementSet.cardinality() > 0\n+//              && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n+//            forkedLevelTsFileResources.add(tsFileResource);\n+//            break;\n+//          }\n+//          forkedLevelTsFileResources.add(tsFileResource);\n+//        }\n+//      }\n+//\n+//      if (measurementSet.cardinality() > 0\n+//          && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n+//        forkedTsFileResources.add(forkedLevelTsFileResources);\n+//        break;\n+//      }\n+//      forkedTsFileResources.add(forkedLevelTsFileResources);\n+//    }\n+//\n+//    // fill in empty file\n+//    while (forkedTsFileResources.size() < currMaxLevel) {\n+//      List<TsFileResource> emptyForkedLevelTsFileResources = new ArrayList<>();\n+//      forkedTsFileResources.add(emptyForkedLevelTsFileResources);\n+//    }\n+//\n+//    return new Pair<>((double) pointNum, (double) measurementSet.cardinality());\n+//  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyMDUwMA=="}, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 245}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDk3NzMwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMjoxNTo0MVrOHhQCFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxMjoyOTozOVrOHk53Tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyNzczMw==", "bodyText": "remove unused comment", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504627733", "createdAt": "2020-10-14T12:15:41Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -407,107 +430,141 @@ public void recover() {\n   }\n \n   @Override\n-  public void forkCurrentFileList(long timePartition) throws IOException {\n-    Pair<Long, Map<Path, MeasurementSchema>> seqResult = forkTsFileList(\n+  public void forkCurrentFileList(long timePartition) {\n+    forkTsFileList(\n         forkedSequenceTsFileResources,\n-        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources));\n-    forkedSeqListPointNum = seqResult.left;\n-    forkedSeqListPathMeasurementSchemaMap = seqResult.right;\n-    Pair<Long, Map<Path, MeasurementSchema>> unSeqResult = forkTsFileList(\n+        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources),\n+        maxLevelNum);\n+    forkTsFileList(\n         forkedUnSequenceTsFileResources,\n         unSequenceTsFileResources\n-            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources));\n-    forkedUnSeqListPointNum = unSeqResult.left;\n-    forkedUnSeqListPathMeasurementSchemaMap = unSeqResult.right;\n+            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources), maxUnseqLevelNum);\n   }\n \n-  private Pair<Long, Map<Path, MeasurementSchema>> forkTsFileList(\n+//  private Pair<Double, Double> forkTsFileList(\n+//      List<List<TsFileResource>> forkedTsFileResources,\n+//      List rawTsFileResources, int currMaxLevel) {\n+//    forkedTsFileResources.clear();\n+//    // just fork part of the TsFile list, controlled by max_merge_chunk_point\n+//    long pointNum = 0;\n+//    // all flush to target file\n+//    ICardinality measurementSet = new HyperLogLog(13);\n+//    for (int i = 0; i < currMaxLevel - 1; i++) {\n+//      List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n+//      Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n+//          .get(i);\n+//      synchronized (levelRawTsFileResources) {\n+//        for (TsFileResource tsFileResource : levelRawTsFileResources) {\n+//          if (tsFileResource.isClosed()) {\n+//            String path = tsFileResource.getTsFile().getAbsolutePath();\n+//            try {\n+//              if (tsFileResource.getTsFile().exists()) {\n+//                TsFileSequenceReader reader = new TsFileSequenceReader(path);\n+//                List<Path> pathList = reader.getAllPaths();\n+//                for (Path sensorPath : pathList) {\n+//                  measurementSet.offer(sensorPath.getFullPath());\n+//                  List<ChunkMetadata> chunkMetadataList = reader.getChunkMetadataList(sensorPath);\n+//                  for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n+//                    pointNum += chunkMetadata.getNumOfPoints();\n+//                  }\n+//                }\n+//              } else {\n+//                logger.info(\"{} tsfile does not exist\", path);\n+//              }\n+//            } catch (IOException e) {\n+//              logger.error(\n+//                  \"{} tsfile reader creates error\", path, e);\n+//            }\n+//          }\n+//          if (measurementSet.cardinality() > 0\n+//              && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n+//            forkedLevelTsFileResources.add(tsFileResource);\n+//            break;\n+//          }\n+//          forkedLevelTsFileResources.add(tsFileResource);\n+//        }\n+//      }\n+//\n+//      if (measurementSet.cardinality() > 0\n+//          && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n+//        forkedTsFileResources.add(forkedLevelTsFileResources);\n+//        break;\n+//      }\n+//      forkedTsFileResources.add(forkedLevelTsFileResources);\n+//    }\n+//\n+//    // fill in empty file\n+//    while (forkedTsFileResources.size() < currMaxLevel) {\n+//      List<TsFileResource> emptyForkedLevelTsFileResources = new ArrayList<>();\n+//      forkedTsFileResources.add(emptyForkedLevelTsFileResources);\n+//    }\n+//\n+//    return new Pair<>((double) pointNum, (double) measurementSet.cardinality());\n+//  }\n+\n+  private void forkTsFileList(\n       List<List<TsFileResource>> forkedTsFileResources,\n-      List rawTsFileResources) throws IOException {\n+      List rawTsFileResources, int currMaxLevel) {\n     forkedTsFileResources.clear();\n-    // just fork part of the TsFile list, controlled by max_merge_chunk_point\n-    long pointNum = 0;\n-    // all flush to target file\n-    Map<Path, MeasurementSchema> pathMeasurementSchemaMap = new HashMap<>();\n-    for (int i = 0; i < maxLevelNum - 1; i++) {\n+    for (int i = 0; i < currMaxLevel - 1; i++) {\n       List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n       Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n           .get(i);\n-      for (TsFileResource tsFileResource : levelRawTsFileResources) {\n-        if (tsFileResource.isClosed()) {\n-          RestorableTsFileIOWriter writer;\n-          try {\n-            writer = new RestorableTsFileIOWriter(\n-                tsFileResource.getTsFile());\n-          } catch (Exception e) {\n-            logger.error(\"[Hot Compaction] {} open writer failed\",\n-                tsFileResource.getTsFile().getPath(), e);\n-            continue;\n+      synchronized (levelRawTsFileResources) {\n+        for (TsFileResource tsFileResource : levelRawTsFileResources) {\n+          if (tsFileResource.isClosed()) {\n+            forkedLevelTsFileResources.add(tsFileResource);\n           }\n-          Map<String, Map<String, List<ChunkMetadata>>> schemaMap = writer\n-              .getMetadatasForQuery();\n-          for (Entry<String, Map<String, List<ChunkMetadata>>> schemaMapEntry : schemaMap\n-              .entrySet()) {\n-            String device = schemaMapEntry.getKey();\n-            for (Entry<String, List<ChunkMetadata>> entry : schemaMapEntry.getValue()\n-                .entrySet()) {\n-              String measurement = entry.getKey();\n-              List<ChunkMetadata> chunkMetadataList = entry.getValue();\n-              for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n-                pointNum += chunkMetadata.getNumOfPoints();\n-              }\n-              pathMeasurementSchemaMap.computeIfAbsent(new Path(device, measurement), k ->\n-                  new MeasurementSchema(measurement, chunkMetadataList.get(0).getDataType()));\n-            }\n-          }\n-          writer.close();\n-          forkedLevelTsFileResources.add(tsFileResource);\n-        }\n-        if (pathMeasurementSchemaMap.size() > 0\n-            && pointNum / pathMeasurementSchemaMap.size() >= maxChunkPointNum) {\n-          break;\n         }\n       }\n-      if (pathMeasurementSchemaMap.size() > 0\n-          && pointNum / pathMeasurementSchemaMap.size() >= maxChunkPointNum) {\n-        break;\n-      }\n       forkedTsFileResources.add(forkedLevelTsFileResources);\n     }\n-    return new Pair<>(pointNum, pathMeasurementSchemaMap);\n+\n+    // fill in empty file\n+    while (forkedTsFileResources.size() < currMaxLevel) {\n+      List<TsFileResource> emptyForkedLevelTsFileResources = new ArrayList<>();\n+      forkedTsFileResources.add(emptyForkedLevelTsFileResources);\n+    }\n   }\n \n   @Override\n   protected void merge(long timePartition) {\n-    merge(forkedSequenceTsFileResources, true, timePartition);\n-    merge(forkedUnSequenceTsFileResources, false, timePartition);\n+    merge(forkedSequenceTsFileResources, true, timePartition, maxLevelNum, maxFileNumInEachLevel);\n+    if (maxUnseqLevelNum <= 1) {\n+      merge(isForceFullMerge, getTsFileList(true), forkedUnSequenceTsFileResources.get(0),\n+          Long.MAX_VALUE);\n+    } else {\n+      merge(forkedUnSequenceTsFileResources, false, timePartition, maxUnseqLevelNum,\n+          maxUnseqFileNumInEachLevel);\n+    }\n   }\n \n   @SuppressWarnings(\"squid:S3776\")\n   private void merge(List<List<TsFileResource>> mergeResources, boolean sequence,\n-      long timePartition) {\n+      long timePartition, int currMaxLevel, int currMaxFileNumInEachLevel) {\n     long startTimeMillis = System.currentTimeMillis();\n     try {\n       logger.info(\"{} start to filter hot compaction condition\", storageGroupName);\n-      long pointNum = sequence ? forkedSeqListPointNum : forkedUnSeqListPointNum;\n-      Map<Path, MeasurementSchema> pathMeasurementSchemaMap =\n-          sequence ? forkedSeqListPathMeasurementSchemaMap\n-              : forkedUnSeqListPathMeasurementSchemaMap;\n-      logger.info(\"{} current sg subLevel point num: {}, measurement num: {}\", storageGroupName,\n-          pointNum, pathMeasurementSchemaMap.size());\n+//      double pointNum = sequence ? forkedSeqListPointNum : forkedUnSeqListPointNum;\n+//      double measurementSize =\n+//          sequence ? forkedSeqListMeasurementSize : forkedUnSeqListMeasurementSize;\n+//      logger\n+//          .info(\"{} current sg subLevel point num: {}, approximate measurement num: {}\",\n+//              storageGroupName, pointNum,\n+//              measurementSize);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 348}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1ODgzMA==", "bodyText": "use it now", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508458830", "createdAt": "2020-10-20T12:29:39Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -407,107 +430,141 @@ public void recover() {\n   }\n \n   @Override\n-  public void forkCurrentFileList(long timePartition) throws IOException {\n-    Pair<Long, Map<Path, MeasurementSchema>> seqResult = forkTsFileList(\n+  public void forkCurrentFileList(long timePartition) {\n+    forkTsFileList(\n         forkedSequenceTsFileResources,\n-        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources));\n-    forkedSeqListPointNum = seqResult.left;\n-    forkedSeqListPathMeasurementSchemaMap = seqResult.right;\n-    Pair<Long, Map<Path, MeasurementSchema>> unSeqResult = forkTsFileList(\n+        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources),\n+        maxLevelNum);\n+    forkTsFileList(\n         forkedUnSequenceTsFileResources,\n         unSequenceTsFileResources\n-            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources));\n-    forkedUnSeqListPointNum = unSeqResult.left;\n-    forkedUnSeqListPathMeasurementSchemaMap = unSeqResult.right;\n+            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources), maxUnseqLevelNum);\n   }\n \n-  private Pair<Long, Map<Path, MeasurementSchema>> forkTsFileList(\n+//  private Pair<Double, Double> forkTsFileList(\n+//      List<List<TsFileResource>> forkedTsFileResources,\n+//      List rawTsFileResources, int currMaxLevel) {\n+//    forkedTsFileResources.clear();\n+//    // just fork part of the TsFile list, controlled by max_merge_chunk_point\n+//    long pointNum = 0;\n+//    // all flush to target file\n+//    ICardinality measurementSet = new HyperLogLog(13);\n+//    for (int i = 0; i < currMaxLevel - 1; i++) {\n+//      List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n+//      Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n+//          .get(i);\n+//      synchronized (levelRawTsFileResources) {\n+//        for (TsFileResource tsFileResource : levelRawTsFileResources) {\n+//          if (tsFileResource.isClosed()) {\n+//            String path = tsFileResource.getTsFile().getAbsolutePath();\n+//            try {\n+//              if (tsFileResource.getTsFile().exists()) {\n+//                TsFileSequenceReader reader = new TsFileSequenceReader(path);\n+//                List<Path> pathList = reader.getAllPaths();\n+//                for (Path sensorPath : pathList) {\n+//                  measurementSet.offer(sensorPath.getFullPath());\n+//                  List<ChunkMetadata> chunkMetadataList = reader.getChunkMetadataList(sensorPath);\n+//                  for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n+//                    pointNum += chunkMetadata.getNumOfPoints();\n+//                  }\n+//                }\n+//              } else {\n+//                logger.info(\"{} tsfile does not exist\", path);\n+//              }\n+//            } catch (IOException e) {\n+//              logger.error(\n+//                  \"{} tsfile reader creates error\", path, e);\n+//            }\n+//          }\n+//          if (measurementSet.cardinality() > 0\n+//              && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n+//            forkedLevelTsFileResources.add(tsFileResource);\n+//            break;\n+//          }\n+//          forkedLevelTsFileResources.add(tsFileResource);\n+//        }\n+//      }\n+//\n+//      if (measurementSet.cardinality() > 0\n+//          && pointNum / measurementSet.cardinality() >= maxChunkPointNum) {\n+//        forkedTsFileResources.add(forkedLevelTsFileResources);\n+//        break;\n+//      }\n+//      forkedTsFileResources.add(forkedLevelTsFileResources);\n+//    }\n+//\n+//    // fill in empty file\n+//    while (forkedTsFileResources.size() < currMaxLevel) {\n+//      List<TsFileResource> emptyForkedLevelTsFileResources = new ArrayList<>();\n+//      forkedTsFileResources.add(emptyForkedLevelTsFileResources);\n+//    }\n+//\n+//    return new Pair<>((double) pointNum, (double) measurementSet.cardinality());\n+//  }\n+\n+  private void forkTsFileList(\n       List<List<TsFileResource>> forkedTsFileResources,\n-      List rawTsFileResources) throws IOException {\n+      List rawTsFileResources, int currMaxLevel) {\n     forkedTsFileResources.clear();\n-    // just fork part of the TsFile list, controlled by max_merge_chunk_point\n-    long pointNum = 0;\n-    // all flush to target file\n-    Map<Path, MeasurementSchema> pathMeasurementSchemaMap = new HashMap<>();\n-    for (int i = 0; i < maxLevelNum - 1; i++) {\n+    for (int i = 0; i < currMaxLevel - 1; i++) {\n       List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n       Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n           .get(i);\n-      for (TsFileResource tsFileResource : levelRawTsFileResources) {\n-        if (tsFileResource.isClosed()) {\n-          RestorableTsFileIOWriter writer;\n-          try {\n-            writer = new RestorableTsFileIOWriter(\n-                tsFileResource.getTsFile());\n-          } catch (Exception e) {\n-            logger.error(\"[Hot Compaction] {} open writer failed\",\n-                tsFileResource.getTsFile().getPath(), e);\n-            continue;\n+      synchronized (levelRawTsFileResources) {\n+        for (TsFileResource tsFileResource : levelRawTsFileResources) {\n+          if (tsFileResource.isClosed()) {\n+            forkedLevelTsFileResources.add(tsFileResource);\n           }\n-          Map<String, Map<String, List<ChunkMetadata>>> schemaMap = writer\n-              .getMetadatasForQuery();\n-          for (Entry<String, Map<String, List<ChunkMetadata>>> schemaMapEntry : schemaMap\n-              .entrySet()) {\n-            String device = schemaMapEntry.getKey();\n-            for (Entry<String, List<ChunkMetadata>> entry : schemaMapEntry.getValue()\n-                .entrySet()) {\n-              String measurement = entry.getKey();\n-              List<ChunkMetadata> chunkMetadataList = entry.getValue();\n-              for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n-                pointNum += chunkMetadata.getNumOfPoints();\n-              }\n-              pathMeasurementSchemaMap.computeIfAbsent(new Path(device, measurement), k ->\n-                  new MeasurementSchema(measurement, chunkMetadataList.get(0).getDataType()));\n-            }\n-          }\n-          writer.close();\n-          forkedLevelTsFileResources.add(tsFileResource);\n-        }\n-        if (pathMeasurementSchemaMap.size() > 0\n-            && pointNum / pathMeasurementSchemaMap.size() >= maxChunkPointNum) {\n-          break;\n         }\n       }\n-      if (pathMeasurementSchemaMap.size() > 0\n-          && pointNum / pathMeasurementSchemaMap.size() >= maxChunkPointNum) {\n-        break;\n-      }\n       forkedTsFileResources.add(forkedLevelTsFileResources);\n     }\n-    return new Pair<>(pointNum, pathMeasurementSchemaMap);\n+\n+    // fill in empty file\n+    while (forkedTsFileResources.size() < currMaxLevel) {\n+      List<TsFileResource> emptyForkedLevelTsFileResources = new ArrayList<>();\n+      forkedTsFileResources.add(emptyForkedLevelTsFileResources);\n+    }\n   }\n \n   @Override\n   protected void merge(long timePartition) {\n-    merge(forkedSequenceTsFileResources, true, timePartition);\n-    merge(forkedUnSequenceTsFileResources, false, timePartition);\n+    merge(forkedSequenceTsFileResources, true, timePartition, maxLevelNum, maxFileNumInEachLevel);\n+    if (maxUnseqLevelNum <= 1) {\n+      merge(isForceFullMerge, getTsFileList(true), forkedUnSequenceTsFileResources.get(0),\n+          Long.MAX_VALUE);\n+    } else {\n+      merge(forkedUnSequenceTsFileResources, false, timePartition, maxUnseqLevelNum,\n+          maxUnseqFileNumInEachLevel);\n+    }\n   }\n \n   @SuppressWarnings(\"squid:S3776\")\n   private void merge(List<List<TsFileResource>> mergeResources, boolean sequence,\n-      long timePartition) {\n+      long timePartition, int currMaxLevel, int currMaxFileNumInEachLevel) {\n     long startTimeMillis = System.currentTimeMillis();\n     try {\n       logger.info(\"{} start to filter hot compaction condition\", storageGroupName);\n-      long pointNum = sequence ? forkedSeqListPointNum : forkedUnSeqListPointNum;\n-      Map<Path, MeasurementSchema> pathMeasurementSchemaMap =\n-          sequence ? forkedSeqListPathMeasurementSchemaMap\n-              : forkedUnSeqListPathMeasurementSchemaMap;\n-      logger.info(\"{} current sg subLevel point num: {}, measurement num: {}\", storageGroupName,\n-          pointNum, pathMeasurementSchemaMap.size());\n+//      double pointNum = sequence ? forkedSeqListPointNum : forkedUnSeqListPointNum;\n+//      double measurementSize =\n+//          sequence ? forkedSeqListMeasurementSize : forkedUnSeqListMeasurementSize;\n+//      logger\n+//          .info(\"{} current sg subLevel point num: {}, approximate measurement num: {}\",\n+//              storageGroupName, pointNum,\n+//              measurementSize);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyNzczMw=="}, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 348}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MDk4Mzc3OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMjoxNzoxOFrOHhQF1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxMjozMDowM1rOHk54dQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyODY5NQ==", "bodyText": "I think some comments should be added, for the software is open sourced", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504628695", "createdAt": "2020-10-14T12:17:18Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "diffHunk": "@@ -53,32 +55,23 @@\n public class HotCompactionUtils {\n \n   private static final Logger logger = LoggerFactory.getLogger(HotCompactionUtils.class);\n+  private static final int mergePagePointNum = IoTDBDescriptor.getInstance().getConfig()\n+      .getMergePagePointNumberThreshold();\n \n   private HotCompactionUtils() {\n     throw new IllegalStateException(\"Utility class\");\n   }\n \n-  private static Pair<ChunkMetadata, Chunk> writeSeqChunk(String storageGroup,\n-      Map<String, TsFileSequenceReader> tsFileSequenceReaderMap, String deviceId,\n-      String measurementId,\n-      List<TsFileResource> levelResources)\n-      throws IOException {\n+  private static Pair<ChunkMetadata, Chunk> readByAppendMerge(RateLimiter compactionReadRateLimiter,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODQ1OTEyNQ==", "bodyText": "okay, I will add some comment", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508459125", "createdAt": "2020-10-20T12:30:03Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "diffHunk": "@@ -53,32 +55,23 @@\n public class HotCompactionUtils {\n \n   private static final Logger logger = LoggerFactory.getLogger(HotCompactionUtils.class);\n+  private static final int mergePagePointNum = IoTDBDescriptor.getInstance().getConfig()\n+      .getMergePagePointNumberThreshold();\n \n   private HotCompactionUtils() {\n     throw new IllegalStateException(\"Utility class\");\n   }\n \n-  private static Pair<ChunkMetadata, Chunk> writeSeqChunk(String storageGroup,\n-      Map<String, TsFileSequenceReader> tsFileSequenceReaderMap, String deviceId,\n-      String measurementId,\n-      List<TsFileResource> levelResources)\n-      throws IOException {\n+  private static Pair<ChunkMetadata, Chunk> readByAppendMerge(RateLimiter compactionReadRateLimiter,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDYyODY5NQ=="}, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MTA1Nzk5OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMjozNjowMFrOHhQywA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxMzo1MTo1M1rOHk9vpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0MDE5Mg==", "bodyText": "This code is consistent with the if, can be extracted", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504640192", "createdAt": "2020-10-14T12:36:00Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "diffHunk": "@@ -91,130 +84,204 @@ private HotCompactionUtils() {\n     return new Pair<>(newChunkMetadata, newChunk);\n   }\n \n-  private static long readUnseqChunk(String storageGroup,\n-      Map<String, TsFileSequenceReader> tsFileSequenceReaderMap, String deviceId, long maxVersion,\n-      String measurementId,\n-      Map<Long, TimeValuePair> timeValuePairMap, List<TsFileResource> levelResources)\n+  private static long readByDeserializeMerge(RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadataMap, long maxVersion,\n+      Map<Long, TimeValuePair> timeValuePairMap)\n       throws IOException {\n-    for (TsFileResource levelResource : levelResources) {\n-      TsFileSequenceReader reader = buildReaderFromTsFileResource(levelResource,\n-          tsFileSequenceReaderMap,\n-          storageGroup);\n-      if (reader == null) {\n-        continue;\n-      }\n-      List<ChunkMetadata> chunkMetadataList = reader\n-          .getChunkMetadataList(new Path(deviceId, measurementId));\n+    for (Entry<TsFileSequenceReader, List<ChunkMetadata>> entry : readerChunkMetadataMap\n+        .entrySet()) {\n+      TsFileSequenceReader reader = entry.getKey();\n+      List<ChunkMetadata> chunkMetadataList = entry.getValue();\n       for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n         maxVersion = Math.max(chunkMetadata.getVersion(), maxVersion);\n         IChunkReader chunkReader = new ChunkReaderByTimestamp(\n             reader.readMemChunk(chunkMetadata));\n+        long chunkSize = 0;\n         while (chunkReader.hasNextSatisfiedPage()) {\n           IPointReader iPointReader = new BatchDataIterator(\n               chunkReader.nextPageData());\n           while (iPointReader.hasNextTimeValuePair()) {\n             TimeValuePair timeValuePair = iPointReader.nextTimeValuePair();\n+            chunkSize += timeValuePair.getSize();\n             timeValuePairMap.put(timeValuePair.getTimestamp(), timeValuePair);\n           }\n         }\n+        MergeManager\n+            .mergeRateLimiterAcquire(compactionReadRateLimiter, chunkSize);\n       }\n     }\n     return maxVersion;\n   }\n \n-  private static void fillDeviceMeasurementMap(Set<String> devices,\n-      Map<String, Map<String, MeasurementSchema>> deviceMeasurementMap,\n-      List<TsFileResource> subLevelResources,\n+  private static long writeByAppendMerge(long maxVersion, String device,\n+      RateLimiter compactionWriteRateLimiter, RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadatasMap,\n+      TsFileResource targetResource, RestorableTsFileIOWriter writer) throws IOException {\n+    Pair<ChunkMetadata, Chunk> chunkPair = readByAppendMerge(compactionReadRateLimiter,\n+        readerChunkMetadatasMap);\n+    ChunkMetadata newChunkMetadata = chunkPair.left;\n+    Chunk newChunk = chunkPair.right;\n+    if (newChunkMetadata != null && newChunk != null) {\n+      maxVersion = Math.max(newChunkMetadata.getVersion(), maxVersion);\n+      // wait for limit write\n+      MergeManager.mergeRateLimiterAcquire(compactionWriteRateLimiter,\n+          newChunk.getHeader().getDataSize() + newChunk.getData().position());\n+      writer.writeChunk(newChunk, newChunkMetadata);\n+      targetResource.updateStartTime(device, newChunkMetadata.getStartTime());\n+      targetResource.updateEndTime(device, newChunkMetadata.getEndTime());\n+    }\n+    return maxVersion;\n+  }\n+\n+  private static long writeByDeserializeMerge(long maxVersion, String device,\n+      RateLimiter compactionRateLimiter, RateLimiter compactionReadRateLimiter,\n+      Entry<String, Map<TsFileSequenceReader, List<ChunkMetadata>>> entry,\n+      TsFileResource targetResource, RestorableTsFileIOWriter writer) throws IOException {\n+    Map<Long, TimeValuePair> timeValuePairMap = new TreeMap<>();\n+    maxVersion = readByDeserializeMerge(compactionReadRateLimiter, entry.getValue(), maxVersion,\n+        timeValuePairMap);\n+    Iterator<List<ChunkMetadata>> chunkMetadataListIterator = entry.getValue().values()\n+        .iterator();\n+    if (!chunkMetadataListIterator.hasNext()) {\n+      return maxVersion;\n+    }\n+    List<ChunkMetadata> chunkMetadataList = chunkMetadataListIterator.next();\n+    if (chunkMetadataList.size() <= 0) {\n+      return maxVersion;\n+    }\n+    IChunkWriter chunkWriter = new ChunkWriterImpl(\n+        new MeasurementSchema(entry.getKey(), chunkMetadataList.get(0).getDataType()));\n+    for (TimeValuePair timeValuePair : timeValuePairMap.values()) {\n+      writeTVPair(timeValuePair, chunkWriter);\n+      targetResource.updateStartTime(device, timeValuePair.getTimestamp());\n+      targetResource.updateEndTime(device, timeValuePair.getTimestamp());\n+    }\n+    // wait for limit write\n+    MergeManager\n+        .mergeRateLimiterAcquire(compactionRateLimiter, chunkWriter.getCurrentChunkSize());\n+    chunkWriter.writeToFileWriter(writer);\n+    return maxVersion;\n+  }\n+\n+  private static Set<String> getTsFileDevicesSet(List<TsFileResource> subLevelResources,\n       Map<String, TsFileSequenceReader> tsFileSequenceReaderMap, String storageGroup)\n       throws IOException {\n+    Set<String> tsFileDevicesSet = new HashSet<>();\n     for (TsFileResource levelResource : subLevelResources) {\n       TsFileSequenceReader reader = buildReaderFromTsFileResource(levelResource,\n           tsFileSequenceReaderMap,\n           storageGroup);\n       if (reader == null) {\n         continue;\n       }\n-      List<Path> allPaths = reader.getAllPaths();\n-      Map<String, TSDataType> allMeasurements = reader.getAllMeasurements();\n-      // device, measurement -> chunk metadata list\n-      for (Path path : allPaths) {\n-        if (devices.contains(path.getDevice())) {\n-          continue;\n-        }\n-        Map<String, MeasurementSchema> measurementSchemaMap = deviceMeasurementMap\n-            .computeIfAbsent(path.getDevice(), k -> new HashMap<>());\n-\n-        // measurement, chunk metadata list\n-        measurementSchemaMap.computeIfAbsent(path.getMeasurement(), k ->\n-            new MeasurementSchema(k, allMeasurements.get(path.getMeasurement())));\n-      }\n+      tsFileDevicesSet.addAll(reader.getAllDevices());\n     }\n+    return tsFileDevicesSet;\n   }\n \n+  /**\n+   * @param targetResource the target resource to be merged to\n+   * @param tsFileResources the source resource to be merged\n+   * @param storageGroup the storage group name\n+   * @param hotCompactionLogger the logger\n+   * @param devices the devices to be skipped(used by recover)\n+   */\n   @SuppressWarnings(\"squid:S3776\") // Suppress high Cognitive Complexity warning\n   public static void merge(TsFileResource targetResource,\n       List<TsFileResource> tsFileResources, String storageGroup,\n       HotCompactionLogger hotCompactionLogger,\n       Set<String> devices, boolean sequence) throws IOException {\n     RestorableTsFileIOWriter writer = new RestorableTsFileIOWriter(targetResource.getTsFile());\n     Map<String, TsFileSequenceReader> tsFileSequenceReaderMap = new HashMap<>();\n-    Map<String, Map<String, MeasurementSchema>> deviceMeasurementMap = new HashMap<>();\n-    RateLimiter compactionRateLimiter = MergeManager.getINSTANCE().getMergeRateLimiter();\n-    fillDeviceMeasurementMap(devices, deviceMeasurementMap, tsFileResources,\n-        tsFileSequenceReaderMap, storageGroup);\n-    if (!sequence) {\n-      for (Entry<String, Map<String, MeasurementSchema>> deviceMeasurementEntry : deviceMeasurementMap\n-          .entrySet()) {\n-        String deviceId = deviceMeasurementEntry.getKey();\n-        writer.startChunkGroup(deviceId);\n+    RateLimiter compactionWriteRateLimiter = MergeManager.getINSTANCE().getMergeWriteRateLimiter();\n+    RateLimiter compactionReadRateLimiter = MergeManager.getINSTANCE().getMergeReadRateLimiter();\n+    Set<String> tsFileDevicesMap = getTsFileDevicesSet(tsFileResources, tsFileSequenceReaderMap,\n+        storageGroup);\n+    for (String device : tsFileDevicesMap) {\n+      if (devices.contains(device)) {\n+        continue;\n+      }\n+      writer.startChunkGroup(device);\n+      // sort chunkMeta by measurement\n+      Map<String, Map<TsFileSequenceReader, List<ChunkMetadata>>> measurementChunkMetadataMap = new HashMap<>();\n+      for (TsFileResource levelResource : tsFileResources) {\n+        TsFileSequenceReader reader = buildReaderFromTsFileResource(levelResource,\n+            tsFileSequenceReaderMap, storageGroup);\n+        Map<String, List<ChunkMetadata>> chunkMetadataMap = reader\n+            .readChunkMetadataInDevice(device);\n+        long chunkMetadataSize = 0;\n+        for (Entry<String, List<ChunkMetadata>> entry : chunkMetadataMap.entrySet()) {\n+          for (ChunkMetadata chunkMetadata : entry.getValue()) {\n+            chunkMetadataSize += chunkMetadata.getStatistics().calculateRamSize();\n+            Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadataMap;\n+            String measurementUid = chunkMetadata.getMeasurementUid();\n+            if (measurementChunkMetadataMap.containsKey(measurementUid)) {\n+              readerChunkMetadataMap = measurementChunkMetadataMap.get(measurementUid);\n+            } else {\n+              readerChunkMetadataMap = new LinkedHashMap<>();\n+            }\n+            List<ChunkMetadata> chunkMetadataList;\n+            if (readerChunkMetadataMap.containsKey(reader)) {\n+              chunkMetadataList = readerChunkMetadataMap.get(reader);\n+            } else {\n+              chunkMetadataList = new ArrayList<>();\n+            }\n+            chunkMetadataList.add(chunkMetadata);\n+            readerChunkMetadataMap.put(reader, chunkMetadataList);\n+            measurementChunkMetadataMap\n+                .put(chunkMetadata.getMeasurementUid(), readerChunkMetadataMap);\n+          }\n+        }\n+        // wait for limit read\n+        MergeManager\n+            .mergeRateLimiterAcquire(compactionReadRateLimiter, chunkMetadataSize);\n+      }\n+      if (!sequence) {\n         long maxVersion = Long.MIN_VALUE;\n-        for (Entry<String, MeasurementSchema> entry : deviceMeasurementEntry.getValue()\n+        for (Entry<String, Map<TsFileSequenceReader, List<ChunkMetadata>>> entry : measurementChunkMetadataMap\n             .entrySet()) {\n-          String measurementId = entry.getKey();\n-          Map<Long, TimeValuePair> timeValuePairMap = new TreeMap<>();\n-          maxVersion = readUnseqChunk(storageGroup, tsFileSequenceReaderMap, deviceId,\n-              maxVersion, measurementId, timeValuePairMap, tsFileResources);\n-          IChunkWriter chunkWriter = new ChunkWriterImpl(entry.getValue());\n-          for (TimeValuePair timeValuePair : timeValuePairMap.values()) {\n-            writeTVPair(timeValuePair, chunkWriter);\n-            targetResource.updateStartTime(deviceId, timeValuePair.getTimestamp());\n-            targetResource.updateEndTime(deviceId, timeValuePair.getTimestamp());\n-          }\n-          // wait for limit write\n-          MergeManager\n-              .mergeRateLimiterAcquire(compactionRateLimiter, chunkWriter.getCurrentChunkSize());\n-          chunkWriter.writeToFileWriter(writer);\n+          maxVersion = writeByDeserializeMerge(maxVersion, device, compactionWriteRateLimiter,\n+              compactionReadRateLimiter,\n+              entry,\n+              targetResource, writer);\n         }\n-        writer.writeVersion(maxVersion);\n         writer.endChunkGroup();\n+        writer.writeVersion(maxVersion);\n         if (hotCompactionLogger != null) {\n-          hotCompactionLogger.logDevice(deviceId, writer.getPos());\n+          hotCompactionLogger.logDevice(device, writer.getPos());\n         }\n-      }\n-    } else {\n-      for (Entry<String, Map<String, MeasurementSchema>> deviceMeasurementEntry : deviceMeasurementMap\n-          .entrySet()) {\n-        String deviceId = deviceMeasurementEntry.getKey();\n-        writer.startChunkGroup(deviceId);\n-        for (Entry<String, MeasurementSchema> entry : deviceMeasurementEntry.getValue()\n+      } else {\n+        long maxVersion = Long.MIN_VALUE;\n+        for (Entry<String, Map<TsFileSequenceReader, List<ChunkMetadata>>> entry : measurementChunkMetadataMap\n             .entrySet()) {\n-          String measurementId = entry.getKey();\n-          Pair<ChunkMetadata, Chunk> chunkPair = writeSeqChunk(storageGroup,\n-              tsFileSequenceReaderMap, deviceId, measurementId, tsFileResources);\n-          ChunkMetadata newChunkMetadata = chunkPair.left;\n-          Chunk newChunk = chunkPair.right;\n-          if (newChunkMetadata != null && newChunk != null) {\n-            // wait for limit write\n-            MergeManager.mergeRateLimiterAcquire(compactionRateLimiter,\n-                (long)newChunk.getHeader().getDataSize() + newChunk.getData().position());\n-            writer.writeChunk(newChunk, newChunkMetadata);\n-            targetResource.updateStartTime(deviceId, newChunkMetadata.getStartTime());\n-            targetResource.updateEndTime(deviceId, newChunkMetadata.getEndTime());\n+          Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadatasMap = entry.getValue();\n+          boolean isPageEnoughLarge = true;\n+          for (List<ChunkMetadata> chunkMetadatas : readerChunkMetadatasMap.values()) {\n+            for (ChunkMetadata chunkMetadata : chunkMetadatas) {\n+              if (chunkMetadata.getNumOfPoints() < mergePagePointNum) {\n+                isPageEnoughLarge = false;\n+                break;\n+              }\n+            }\n+          }\n+          if (isPageEnoughLarge) {\n+            logger.info(\"{} [Hot Compaction] page enough large, use append merge\", storageGroup);\n+            maxVersion = writeByAppendMerge(maxVersion, device, compactionWriteRateLimiter,\n+                compactionReadRateLimiter,\n+                readerChunkMetadatasMap, targetResource, writer);\n+          } else {\n+            logger\n+                .info(\"{} [Hot Compaction] page enough large, use deserialize merge\", storageGroup);\n+            maxVersion = writeByDeserializeMerge(maxVersion, device, compactionWriteRateLimiter,\n+                compactionReadRateLimiter,\n+                entry,\n+                targetResource, writer);\n           }\n         }\n         writer.endChunkGroup();\n+        writer.writeVersion(maxVersion);\n         if (hotCompactionLogger != null) {\n-          hotCompactionLogger.logDevice(deviceId, writer.getPos());\n+          hotCompactionLogger.logDevice(device, writer.getPos());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 346}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUyMjQwNA==", "bodyText": "good idea", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508522404", "createdAt": "2020-10-20T13:51:53Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "diffHunk": "@@ -91,130 +84,204 @@ private HotCompactionUtils() {\n     return new Pair<>(newChunkMetadata, newChunk);\n   }\n \n-  private static long readUnseqChunk(String storageGroup,\n-      Map<String, TsFileSequenceReader> tsFileSequenceReaderMap, String deviceId, long maxVersion,\n-      String measurementId,\n-      Map<Long, TimeValuePair> timeValuePairMap, List<TsFileResource> levelResources)\n+  private static long readByDeserializeMerge(RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadataMap, long maxVersion,\n+      Map<Long, TimeValuePair> timeValuePairMap)\n       throws IOException {\n-    for (TsFileResource levelResource : levelResources) {\n-      TsFileSequenceReader reader = buildReaderFromTsFileResource(levelResource,\n-          tsFileSequenceReaderMap,\n-          storageGroup);\n-      if (reader == null) {\n-        continue;\n-      }\n-      List<ChunkMetadata> chunkMetadataList = reader\n-          .getChunkMetadataList(new Path(deviceId, measurementId));\n+    for (Entry<TsFileSequenceReader, List<ChunkMetadata>> entry : readerChunkMetadataMap\n+        .entrySet()) {\n+      TsFileSequenceReader reader = entry.getKey();\n+      List<ChunkMetadata> chunkMetadataList = entry.getValue();\n       for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n         maxVersion = Math.max(chunkMetadata.getVersion(), maxVersion);\n         IChunkReader chunkReader = new ChunkReaderByTimestamp(\n             reader.readMemChunk(chunkMetadata));\n+        long chunkSize = 0;\n         while (chunkReader.hasNextSatisfiedPage()) {\n           IPointReader iPointReader = new BatchDataIterator(\n               chunkReader.nextPageData());\n           while (iPointReader.hasNextTimeValuePair()) {\n             TimeValuePair timeValuePair = iPointReader.nextTimeValuePair();\n+            chunkSize += timeValuePair.getSize();\n             timeValuePairMap.put(timeValuePair.getTimestamp(), timeValuePair);\n           }\n         }\n+        MergeManager\n+            .mergeRateLimiterAcquire(compactionReadRateLimiter, chunkSize);\n       }\n     }\n     return maxVersion;\n   }\n \n-  private static void fillDeviceMeasurementMap(Set<String> devices,\n-      Map<String, Map<String, MeasurementSchema>> deviceMeasurementMap,\n-      List<TsFileResource> subLevelResources,\n+  private static long writeByAppendMerge(long maxVersion, String device,\n+      RateLimiter compactionWriteRateLimiter, RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadatasMap,\n+      TsFileResource targetResource, RestorableTsFileIOWriter writer) throws IOException {\n+    Pair<ChunkMetadata, Chunk> chunkPair = readByAppendMerge(compactionReadRateLimiter,\n+        readerChunkMetadatasMap);\n+    ChunkMetadata newChunkMetadata = chunkPair.left;\n+    Chunk newChunk = chunkPair.right;\n+    if (newChunkMetadata != null && newChunk != null) {\n+      maxVersion = Math.max(newChunkMetadata.getVersion(), maxVersion);\n+      // wait for limit write\n+      MergeManager.mergeRateLimiterAcquire(compactionWriteRateLimiter,\n+          newChunk.getHeader().getDataSize() + newChunk.getData().position());\n+      writer.writeChunk(newChunk, newChunkMetadata);\n+      targetResource.updateStartTime(device, newChunkMetadata.getStartTime());\n+      targetResource.updateEndTime(device, newChunkMetadata.getEndTime());\n+    }\n+    return maxVersion;\n+  }\n+\n+  private static long writeByDeserializeMerge(long maxVersion, String device,\n+      RateLimiter compactionRateLimiter, RateLimiter compactionReadRateLimiter,\n+      Entry<String, Map<TsFileSequenceReader, List<ChunkMetadata>>> entry,\n+      TsFileResource targetResource, RestorableTsFileIOWriter writer) throws IOException {\n+    Map<Long, TimeValuePair> timeValuePairMap = new TreeMap<>();\n+    maxVersion = readByDeserializeMerge(compactionReadRateLimiter, entry.getValue(), maxVersion,\n+        timeValuePairMap);\n+    Iterator<List<ChunkMetadata>> chunkMetadataListIterator = entry.getValue().values()\n+        .iterator();\n+    if (!chunkMetadataListIterator.hasNext()) {\n+      return maxVersion;\n+    }\n+    List<ChunkMetadata> chunkMetadataList = chunkMetadataListIterator.next();\n+    if (chunkMetadataList.size() <= 0) {\n+      return maxVersion;\n+    }\n+    IChunkWriter chunkWriter = new ChunkWriterImpl(\n+        new MeasurementSchema(entry.getKey(), chunkMetadataList.get(0).getDataType()));\n+    for (TimeValuePair timeValuePair : timeValuePairMap.values()) {\n+      writeTVPair(timeValuePair, chunkWriter);\n+      targetResource.updateStartTime(device, timeValuePair.getTimestamp());\n+      targetResource.updateEndTime(device, timeValuePair.getTimestamp());\n+    }\n+    // wait for limit write\n+    MergeManager\n+        .mergeRateLimiterAcquire(compactionRateLimiter, chunkWriter.getCurrentChunkSize());\n+    chunkWriter.writeToFileWriter(writer);\n+    return maxVersion;\n+  }\n+\n+  private static Set<String> getTsFileDevicesSet(List<TsFileResource> subLevelResources,\n       Map<String, TsFileSequenceReader> tsFileSequenceReaderMap, String storageGroup)\n       throws IOException {\n+    Set<String> tsFileDevicesSet = new HashSet<>();\n     for (TsFileResource levelResource : subLevelResources) {\n       TsFileSequenceReader reader = buildReaderFromTsFileResource(levelResource,\n           tsFileSequenceReaderMap,\n           storageGroup);\n       if (reader == null) {\n         continue;\n       }\n-      List<Path> allPaths = reader.getAllPaths();\n-      Map<String, TSDataType> allMeasurements = reader.getAllMeasurements();\n-      // device, measurement -> chunk metadata list\n-      for (Path path : allPaths) {\n-        if (devices.contains(path.getDevice())) {\n-          continue;\n-        }\n-        Map<String, MeasurementSchema> measurementSchemaMap = deviceMeasurementMap\n-            .computeIfAbsent(path.getDevice(), k -> new HashMap<>());\n-\n-        // measurement, chunk metadata list\n-        measurementSchemaMap.computeIfAbsent(path.getMeasurement(), k ->\n-            new MeasurementSchema(k, allMeasurements.get(path.getMeasurement())));\n-      }\n+      tsFileDevicesSet.addAll(reader.getAllDevices());\n     }\n+    return tsFileDevicesSet;\n   }\n \n+  /**\n+   * @param targetResource the target resource to be merged to\n+   * @param tsFileResources the source resource to be merged\n+   * @param storageGroup the storage group name\n+   * @param hotCompactionLogger the logger\n+   * @param devices the devices to be skipped(used by recover)\n+   */\n   @SuppressWarnings(\"squid:S3776\") // Suppress high Cognitive Complexity warning\n   public static void merge(TsFileResource targetResource,\n       List<TsFileResource> tsFileResources, String storageGroup,\n       HotCompactionLogger hotCompactionLogger,\n       Set<String> devices, boolean sequence) throws IOException {\n     RestorableTsFileIOWriter writer = new RestorableTsFileIOWriter(targetResource.getTsFile());\n     Map<String, TsFileSequenceReader> tsFileSequenceReaderMap = new HashMap<>();\n-    Map<String, Map<String, MeasurementSchema>> deviceMeasurementMap = new HashMap<>();\n-    RateLimiter compactionRateLimiter = MergeManager.getINSTANCE().getMergeRateLimiter();\n-    fillDeviceMeasurementMap(devices, deviceMeasurementMap, tsFileResources,\n-        tsFileSequenceReaderMap, storageGroup);\n-    if (!sequence) {\n-      for (Entry<String, Map<String, MeasurementSchema>> deviceMeasurementEntry : deviceMeasurementMap\n-          .entrySet()) {\n-        String deviceId = deviceMeasurementEntry.getKey();\n-        writer.startChunkGroup(deviceId);\n+    RateLimiter compactionWriteRateLimiter = MergeManager.getINSTANCE().getMergeWriteRateLimiter();\n+    RateLimiter compactionReadRateLimiter = MergeManager.getINSTANCE().getMergeReadRateLimiter();\n+    Set<String> tsFileDevicesMap = getTsFileDevicesSet(tsFileResources, tsFileSequenceReaderMap,\n+        storageGroup);\n+    for (String device : tsFileDevicesMap) {\n+      if (devices.contains(device)) {\n+        continue;\n+      }\n+      writer.startChunkGroup(device);\n+      // sort chunkMeta by measurement\n+      Map<String, Map<TsFileSequenceReader, List<ChunkMetadata>>> measurementChunkMetadataMap = new HashMap<>();\n+      for (TsFileResource levelResource : tsFileResources) {\n+        TsFileSequenceReader reader = buildReaderFromTsFileResource(levelResource,\n+            tsFileSequenceReaderMap, storageGroup);\n+        Map<String, List<ChunkMetadata>> chunkMetadataMap = reader\n+            .readChunkMetadataInDevice(device);\n+        long chunkMetadataSize = 0;\n+        for (Entry<String, List<ChunkMetadata>> entry : chunkMetadataMap.entrySet()) {\n+          for (ChunkMetadata chunkMetadata : entry.getValue()) {\n+            chunkMetadataSize += chunkMetadata.getStatistics().calculateRamSize();\n+            Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadataMap;\n+            String measurementUid = chunkMetadata.getMeasurementUid();\n+            if (measurementChunkMetadataMap.containsKey(measurementUid)) {\n+              readerChunkMetadataMap = measurementChunkMetadataMap.get(measurementUid);\n+            } else {\n+              readerChunkMetadataMap = new LinkedHashMap<>();\n+            }\n+            List<ChunkMetadata> chunkMetadataList;\n+            if (readerChunkMetadataMap.containsKey(reader)) {\n+              chunkMetadataList = readerChunkMetadataMap.get(reader);\n+            } else {\n+              chunkMetadataList = new ArrayList<>();\n+            }\n+            chunkMetadataList.add(chunkMetadata);\n+            readerChunkMetadataMap.put(reader, chunkMetadataList);\n+            measurementChunkMetadataMap\n+                .put(chunkMetadata.getMeasurementUid(), readerChunkMetadataMap);\n+          }\n+        }\n+        // wait for limit read\n+        MergeManager\n+            .mergeRateLimiterAcquire(compactionReadRateLimiter, chunkMetadataSize);\n+      }\n+      if (!sequence) {\n         long maxVersion = Long.MIN_VALUE;\n-        for (Entry<String, MeasurementSchema> entry : deviceMeasurementEntry.getValue()\n+        for (Entry<String, Map<TsFileSequenceReader, List<ChunkMetadata>>> entry : measurementChunkMetadataMap\n             .entrySet()) {\n-          String measurementId = entry.getKey();\n-          Map<Long, TimeValuePair> timeValuePairMap = new TreeMap<>();\n-          maxVersion = readUnseqChunk(storageGroup, tsFileSequenceReaderMap, deviceId,\n-              maxVersion, measurementId, timeValuePairMap, tsFileResources);\n-          IChunkWriter chunkWriter = new ChunkWriterImpl(entry.getValue());\n-          for (TimeValuePair timeValuePair : timeValuePairMap.values()) {\n-            writeTVPair(timeValuePair, chunkWriter);\n-            targetResource.updateStartTime(deviceId, timeValuePair.getTimestamp());\n-            targetResource.updateEndTime(deviceId, timeValuePair.getTimestamp());\n-          }\n-          // wait for limit write\n-          MergeManager\n-              .mergeRateLimiterAcquire(compactionRateLimiter, chunkWriter.getCurrentChunkSize());\n-          chunkWriter.writeToFileWriter(writer);\n+          maxVersion = writeByDeserializeMerge(maxVersion, device, compactionWriteRateLimiter,\n+              compactionReadRateLimiter,\n+              entry,\n+              targetResource, writer);\n         }\n-        writer.writeVersion(maxVersion);\n         writer.endChunkGroup();\n+        writer.writeVersion(maxVersion);\n         if (hotCompactionLogger != null) {\n-          hotCompactionLogger.logDevice(deviceId, writer.getPos());\n+          hotCompactionLogger.logDevice(device, writer.getPos());\n         }\n-      }\n-    } else {\n-      for (Entry<String, Map<String, MeasurementSchema>> deviceMeasurementEntry : deviceMeasurementMap\n-          .entrySet()) {\n-        String deviceId = deviceMeasurementEntry.getKey();\n-        writer.startChunkGroup(deviceId);\n-        for (Entry<String, MeasurementSchema> entry : deviceMeasurementEntry.getValue()\n+      } else {\n+        long maxVersion = Long.MIN_VALUE;\n+        for (Entry<String, Map<TsFileSequenceReader, List<ChunkMetadata>>> entry : measurementChunkMetadataMap\n             .entrySet()) {\n-          String measurementId = entry.getKey();\n-          Pair<ChunkMetadata, Chunk> chunkPair = writeSeqChunk(storageGroup,\n-              tsFileSequenceReaderMap, deviceId, measurementId, tsFileResources);\n-          ChunkMetadata newChunkMetadata = chunkPair.left;\n-          Chunk newChunk = chunkPair.right;\n-          if (newChunkMetadata != null && newChunk != null) {\n-            // wait for limit write\n-            MergeManager.mergeRateLimiterAcquire(compactionRateLimiter,\n-                (long)newChunk.getHeader().getDataSize() + newChunk.getData().position());\n-            writer.writeChunk(newChunk, newChunkMetadata);\n-            targetResource.updateStartTime(deviceId, newChunkMetadata.getStartTime());\n-            targetResource.updateEndTime(deviceId, newChunkMetadata.getEndTime());\n+          Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadatasMap = entry.getValue();\n+          boolean isPageEnoughLarge = true;\n+          for (List<ChunkMetadata> chunkMetadatas : readerChunkMetadatasMap.values()) {\n+            for (ChunkMetadata chunkMetadata : chunkMetadatas) {\n+              if (chunkMetadata.getNumOfPoints() < mergePagePointNum) {\n+                isPageEnoughLarge = false;\n+                break;\n+              }\n+            }\n+          }\n+          if (isPageEnoughLarge) {\n+            logger.info(\"{} [Hot Compaction] page enough large, use append merge\", storageGroup);\n+            maxVersion = writeByAppendMerge(maxVersion, device, compactionWriteRateLimiter,\n+                compactionReadRateLimiter,\n+                readerChunkMetadatasMap, targetResource, writer);\n+          } else {\n+            logger\n+                .info(\"{} [Hot Compaction] page enough large, use deserialize merge\", storageGroup);\n+            maxVersion = writeByDeserializeMerge(maxVersion, device, compactionWriteRateLimiter,\n+                compactionReadRateLimiter,\n+                entry,\n+                targetResource, writer);\n           }\n         }\n         writer.endChunkGroup();\n+        writer.writeVersion(maxVersion);\n         if (hotCompactionLogger != null) {\n-          hotCompactionLogger.logDevice(deviceId, writer.getPos());\n+          hotCompactionLogger.logDevice(device, writer.getPos());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0MDE5Mg=="}, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 346}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MTA2NzY4OnYy", "diffSide": "LEFT", "path": "server/src/test/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessorTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMjozODozOFrOHhQ4_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNDowMToyNlrOHk-bHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0MTc4OQ==", "bodyText": "why this UT is skipped", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504641789", "createdAt": "2020-10-14T12:38:38Z", "author": {"login": "EJTTianYu"}, "path": "server/src/test/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessorTest.java", "diffHunk": "@@ -268,291 +270,8 @@ public void testSeqAndUnSeqSyncClose()\n   }\n \n   @Test\n-  public void testEnableDiscardOutOfOrderDataForInsertRowPlan()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzMzUzNA==", "bodyText": "revert it", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508533534", "createdAt": "2020-10-20T14:01:26Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/test/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessorTest.java", "diffHunk": "@@ -268,291 +270,8 @@ public void testSeqAndUnSeqSyncClose()\n   }\n \n   @Test\n-  public void testEnableDiscardOutOfOrderDataForInsertRowPlan()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY0MTc4OQ=="}, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2MTIxODMxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxMzoxMjoyNVrOHhSTmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNDowMzozM1rOHk-iEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY2NDk4Nw==", "bodyText": "any difference between this if , else\uff0cI think both can use get(level)", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r504664987", "createdAt": "2020-10-14T13:12:25Z", "author": {"login": "EJTTianYu"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -209,25 +232,25 @@ public void removeAll(List<TsFileResource> tsFileResourceList, boolean sequence)\n   public void add(TsFileResource tsFileResource, boolean sequence) {\n     long timePartitionId = tsFileResource.getTimePartition();\n     int level = getMergeLevel(tsFileResource.getTsFile());\n-    if (level <= maxLevelNum - 1) {\n-      if (sequence) {\n+    if (sequence) {\n+      if (level <= maxLevelNum - 1) {\n         sequenceTsFileResources\n             .computeIfAbsent(timePartitionId, this::newSequenceTsFileResources).get(level)\n             .add(tsFileResource);\n       } else {\n-        unSequenceTsFileResources\n-            .computeIfAbsent(timePartitionId, this::newUnSequenceTsFileResources).get(level)\n+        sequenceTsFileResources\n+            .computeIfAbsent(timePartitionId, this::newSequenceTsFileResources).get(maxLevelNum - 1)\n             .add(tsFileResource);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzNTMxNQ==", "bodyText": "add comment", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508535315", "createdAt": "2020-10-20T14:03:33Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -209,25 +232,25 @@ public void removeAll(List<TsFileResource> tsFileResourceList, boolean sequence)\n   public void add(TsFileResource tsFileResource, boolean sequence) {\n     long timePartitionId = tsFileResource.getTimePartition();\n     int level = getMergeLevel(tsFileResource.getTsFile());\n-    if (level <= maxLevelNum - 1) {\n-      if (sequence) {\n+    if (sequence) {\n+      if (level <= maxLevelNum - 1) {\n         sequenceTsFileResources\n             .computeIfAbsent(timePartitionId, this::newSequenceTsFileResources).get(level)\n             .add(tsFileResource);\n       } else {\n-        unSequenceTsFileResources\n-            .computeIfAbsent(timePartitionId, this::newUnSequenceTsFileResources).get(level)\n+        sequenceTsFileResources\n+            .computeIfAbsent(timePartitionId, this::newSequenceTsFileResources).get(maxLevelNum - 1)\n             .add(tsFileResource);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDY2NDk4Nw=="}, "originalCommit": {"oid": "44d47316308ebf003d9babebf98d312db7f6e85c"}, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MDEwODMwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQxMTo1OTowOFrOHkK2Ug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNDowMzo1MlrOHk-jPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzY4ODUzMA==", "bodyText": "Remove the useless import", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r507688530", "createdAt": "2020-10-19T11:59:08Z", "author": {"login": "samperson1997"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -31,29 +31,36 @@\n import java.nio.file.Files;\n import java.util.ArrayList;\n import java.util.Collection;\n-import java.util.HashMap;\n import java.util.HashSet;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n-import java.util.Map.Entry;\n import java.util.Set;\n import java.util.TreeSet;\n import java.util.concurrent.ConcurrentSkipListMap;\n import java.util.concurrent.CopyOnWriteArrayList;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLog;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n import org.apache.iotdb.db.conf.IoTDBDescriptor;\n import org.apache.iotdb.db.engine.cache.ChunkMetadataCache;\n+import org.apache.iotdb.db.engine.merge.manage.MergeManager;\n+import org.apache.iotdb.db.engine.merge.manage.MergeResource;\n+import org.apache.iotdb.db.engine.merge.selector.IMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.task.MergeTask;\n+import org.apache.iotdb.db.engine.modification.ModificationFile;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzNTYxMw==", "bodyText": "solve it", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508535613", "createdAt": "2020-10-20T14:03:52Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -31,29 +31,36 @@\n import java.nio.file.Files;\n import java.util.ArrayList;\n import java.util.Collection;\n-import java.util.HashMap;\n import java.util.HashSet;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n-import java.util.Map.Entry;\n import java.util.Set;\n import java.util.TreeSet;\n import java.util.concurrent.ConcurrentSkipListMap;\n import java.util.concurrent.CopyOnWriteArrayList;\n+\n+import com.clearspring.analytics.stream.cardinality.HyperLogLog;\n+import com.clearspring.analytics.stream.cardinality.ICardinality;\n import org.apache.iotdb.db.conf.IoTDBDescriptor;\n import org.apache.iotdb.db.engine.cache.ChunkMetadataCache;\n+import org.apache.iotdb.db.engine.merge.manage.MergeManager;\n+import org.apache.iotdb.db.engine.merge.manage.MergeResource;\n+import org.apache.iotdb.db.engine.merge.selector.IMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.task.MergeTask;\n+import org.apache.iotdb.db.engine.modification.ModificationFile;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzY4ODUzMA=="}, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MzA1NTM1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMToxMTowMFrOHkm_1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNDowNjoyOVrOHk-skg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE0OTcxNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      mergeLog.delete();\n          \n          \n            \n                      Files.delete(mergeLog.toPath());", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508149716", "createdAt": "2020-10-20T01:11:00Z", "author": {"login": "samperson1997"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "diffHunk": "@@ -146,4 +178,207 @@ public void run() {\n       closeHotCompactionMergeCallBack.call();\n     }\n   }\n+\n+  public void merge(boolean fullMerge, List<TsFileResource> seqMergeList,\n+      List<TsFileResource> unSeqMergeList, long dataTTL) {\n+    if (isUnseqMerging) {\n+      if (logger.isInfoEnabled()) {\n+        logger.info(\"{} Last merge is ongoing, currently consumed time: {}ms\", storageGroupName,\n+            (System.currentTimeMillis() - mergeStartTime));\n+      }\n+      return;\n+    }\n+    logger.info(\"{} will close all files for starting a merge (fullmerge = {})\", storageGroupName,\n+        fullMerge);\n+\n+    if (seqMergeList.isEmpty()) {\n+      logger.info(\"{} no seq files to be merged\", storageGroupName);\n+      return;\n+    }\n+\n+    if (unSeqMergeList.isEmpty()) {\n+      logger.info(\"{} no unseq files to be merged\", storageGroupName);\n+      return;\n+    }\n+\n+    long budget = IoTDBDescriptor.getInstance().getConfig().getMergeMemoryBudget();\n+    long timeLowerBound = System.currentTimeMillis() - dataTTL;\n+    MergeResource mergeResource = new MergeResource(seqMergeList, unSeqMergeList, timeLowerBound);\n+\n+    IMergeFileSelector fileSelector = getMergeFileSelector(budget, mergeResource);\n+    try {\n+      List[] mergeFiles = fileSelector.select();\n+      if (mergeFiles.length == 0) {\n+        logger.info(\"{} cannot select merge candidates under the budget {}\", storageGroupName,\n+            budget);\n+        return;\n+      }\n+      // avoid pending tasks holds the metadata and streams\n+      mergeResource.clear();\n+      String taskName = storageGroupName + \"-\" + System.currentTimeMillis();\n+      // do not cache metadata until true candidates are chosen, or too much metadata will be\n+      // cached during selection\n+      mergeResource.setCacheDeviceMeta(true);\n+\n+      for (TsFileResource tsFileResource : mergeResource.getSeqFiles()) {\n+        tsFileResource.setMerging(true);\n+      }\n+      for (TsFileResource tsFileResource : mergeResource.getUnseqFiles()) {\n+        tsFileResource.setMerging(true);\n+      }\n+\n+      MergeTask mergeTask = new MergeTask(mergeResource, storageGroupDir,\n+          this::mergeEndAction, taskName, fullMerge, fileSelector.getConcurrentMergeNum(),\n+          storageGroupName);\n+      mergingModification = new ModificationFile(\n+          storageGroupDir + File.separator + MERGING_MODIFICATION_FILE_NAME);\n+      MergeManager.getINSTANCE().submitMainTask(mergeTask);\n+      if (logger.isInfoEnabled()) {\n+        logger.info(\"{} submits a merge task {}, merging {} seqFiles, {} unseqFiles\",\n+            storageGroupName, taskName, mergeFiles[0].size(), mergeFiles[1].size());\n+      }\n+      isUnseqMerging = true;\n+      mergeStartTime = System.currentTimeMillis();\n+\n+    } catch (MergeException | IOException e) {\n+      logger.error(\"{} cannot select file for merge\", storageGroupName, e);\n+    }\n+  }\n+\n+  private IMergeFileSelector getMergeFileSelector(long budget, MergeResource resource) {\n+    MergeFileStrategy strategy = IoTDBDescriptor.getInstance().getConfig().getMergeFileStrategy();\n+    switch (strategy) {\n+      case MAX_FILE_NUM:\n+        return new MaxFileMergeFileSelector(resource, budget);\n+      case MAX_SERIES_NUM:\n+        return new MaxSeriesMergeFileSelector(resource, budget);\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown MergeFileStrategy \" + strategy);\n+    }\n+  }\n+\n+  /**\n+   * acquire the write locks of the resource , the merge lock and the hot compaction lock\n+   */\n+  private void doubleWriteLock(TsFileResource seqFile) {\n+    boolean fileLockGot;\n+    boolean mergeLockGot;\n+    boolean hotCompactionLockGot;\n+    while (true) {\n+      fileLockGot = seqFile.tryWriteLock();\n+      mergeLockGot = mergeLock.writeLock().tryLock();\n+      hotCompactionLockGot = tryWriteLock();\n+\n+      if (fileLockGot && mergeLockGot && hotCompactionLockGot) {\n+        break;\n+      } else {\n+        // did not get all of them, release the gotten one and retry\n+        if (hotCompactionLockGot) {\n+          writeUnlock();\n+        }\n+        if (mergeLockGot) {\n+          mergeLock.writeLock().unlock();\n+        }\n+        if (fileLockGot) {\n+          seqFile.writeUnlock();\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * release the write locks of the resource , the merge lock and the hot compaction lock\n+   */\n+  private void doubleWriteUnlock(TsFileResource seqFile) {\n+    writeUnlock();\n+    mergeLock.writeLock().unlock();\n+    seqFile.writeUnlock();\n+  }\n+\n+  private void removeUnseqFiles(List<TsFileResource> unseqFiles) {\n+    mergeLock.writeLock().lock();\n+    writeLock();\n+    try {\n+      removeAll(unseqFiles, false);\n+    } finally {\n+      writeUnlock();\n+      mergeLock.writeLock().unlock();\n+    }\n+\n+    for (TsFileResource unseqFile : unseqFiles) {\n+      unseqFile.writeLock();\n+      try {\n+        unseqFile.remove();\n+      } finally {\n+        unseqFile.writeUnlock();\n+      }\n+    }\n+  }\n+\n+  @SuppressWarnings(\"squid:S1141\")\n+  private void updateMergeModification(TsFileResource seqFile) {\n+    try {\n+      // remove old modifications and write modifications generated during merge\n+      seqFile.removeModFile();\n+      if (mergingModification != null) {\n+        for (Modification modification : mergingModification.getModifications()) {\n+          seqFile.getModFile().write(modification);\n+        }\n+        try {\n+          seqFile.getModFile().close();\n+        } catch (IOException e) {\n+          logger\n+              .error(\"Cannot close the ModificationFile {}\", seqFile.getModFile().getFilePath(), e);\n+        }\n+      }\n+    } catch (IOException e) {\n+      logger.error(\"{} cannot clean the ModificationFile of {} after merge\", storageGroupName,\n+          seqFile.getTsFile(), e);\n+    }\n+  }\n+\n+  private void removeMergingModification() {\n+    try {\n+      if (mergingModification != null) {\n+        mergingModification.remove();\n+        mergingModification = null;\n+      }\n+    } catch (IOException e) {\n+      logger.error(\"{} cannot remove merging modification \", storageGroupName, e);\n+    }\n+  }\n+\n+  public void mergeEndAction(List<TsFileResource> seqFiles, List<TsFileResource> unseqFiles,\n+      File mergeLog) {\n+    logger.info(\"{} a merge task is ending...\", storageGroupName);\n+\n+    if (unseqFiles.isEmpty()) {\n+      // merge runtime exception arose, just end this merge\n+      isUnseqMerging = false;\n+      logger.info(\"{} a merge task abnormally ends\", storageGroupName);\n+      return;\n+    }\n+\n+    removeUnseqFiles(unseqFiles);\n+\n+    for (int i = 0; i < seqFiles.size(); i++) {\n+      TsFileResource seqFile = seqFiles.get(i);\n+      // get both seqFile lock and merge lock\n+      doubleWriteLock(seqFile);\n+\n+      try {\n+        updateMergeModification(seqFile);\n+        if (i == seqFiles.size() - 1) {\n+          //FIXME if there is an exception, the the modification file will be not closed.\n+          removeMergingModification();\n+          isUnseqMerging = false;\n+          mergeLog.delete();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 255}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzODAwMg==", "bodyText": "accept it", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508538002", "createdAt": "2020-10-20T14:06:29Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "diffHunk": "@@ -146,4 +178,207 @@ public void run() {\n       closeHotCompactionMergeCallBack.call();\n     }\n   }\n+\n+  public void merge(boolean fullMerge, List<TsFileResource> seqMergeList,\n+      List<TsFileResource> unSeqMergeList, long dataTTL) {\n+    if (isUnseqMerging) {\n+      if (logger.isInfoEnabled()) {\n+        logger.info(\"{} Last merge is ongoing, currently consumed time: {}ms\", storageGroupName,\n+            (System.currentTimeMillis() - mergeStartTime));\n+      }\n+      return;\n+    }\n+    logger.info(\"{} will close all files for starting a merge (fullmerge = {})\", storageGroupName,\n+        fullMerge);\n+\n+    if (seqMergeList.isEmpty()) {\n+      logger.info(\"{} no seq files to be merged\", storageGroupName);\n+      return;\n+    }\n+\n+    if (unSeqMergeList.isEmpty()) {\n+      logger.info(\"{} no unseq files to be merged\", storageGroupName);\n+      return;\n+    }\n+\n+    long budget = IoTDBDescriptor.getInstance().getConfig().getMergeMemoryBudget();\n+    long timeLowerBound = System.currentTimeMillis() - dataTTL;\n+    MergeResource mergeResource = new MergeResource(seqMergeList, unSeqMergeList, timeLowerBound);\n+\n+    IMergeFileSelector fileSelector = getMergeFileSelector(budget, mergeResource);\n+    try {\n+      List[] mergeFiles = fileSelector.select();\n+      if (mergeFiles.length == 0) {\n+        logger.info(\"{} cannot select merge candidates under the budget {}\", storageGroupName,\n+            budget);\n+        return;\n+      }\n+      // avoid pending tasks holds the metadata and streams\n+      mergeResource.clear();\n+      String taskName = storageGroupName + \"-\" + System.currentTimeMillis();\n+      // do not cache metadata until true candidates are chosen, or too much metadata will be\n+      // cached during selection\n+      mergeResource.setCacheDeviceMeta(true);\n+\n+      for (TsFileResource tsFileResource : mergeResource.getSeqFiles()) {\n+        tsFileResource.setMerging(true);\n+      }\n+      for (TsFileResource tsFileResource : mergeResource.getUnseqFiles()) {\n+        tsFileResource.setMerging(true);\n+      }\n+\n+      MergeTask mergeTask = new MergeTask(mergeResource, storageGroupDir,\n+          this::mergeEndAction, taskName, fullMerge, fileSelector.getConcurrentMergeNum(),\n+          storageGroupName);\n+      mergingModification = new ModificationFile(\n+          storageGroupDir + File.separator + MERGING_MODIFICATION_FILE_NAME);\n+      MergeManager.getINSTANCE().submitMainTask(mergeTask);\n+      if (logger.isInfoEnabled()) {\n+        logger.info(\"{} submits a merge task {}, merging {} seqFiles, {} unseqFiles\",\n+            storageGroupName, taskName, mergeFiles[0].size(), mergeFiles[1].size());\n+      }\n+      isUnseqMerging = true;\n+      mergeStartTime = System.currentTimeMillis();\n+\n+    } catch (MergeException | IOException e) {\n+      logger.error(\"{} cannot select file for merge\", storageGroupName, e);\n+    }\n+  }\n+\n+  private IMergeFileSelector getMergeFileSelector(long budget, MergeResource resource) {\n+    MergeFileStrategy strategy = IoTDBDescriptor.getInstance().getConfig().getMergeFileStrategy();\n+    switch (strategy) {\n+      case MAX_FILE_NUM:\n+        return new MaxFileMergeFileSelector(resource, budget);\n+      case MAX_SERIES_NUM:\n+        return new MaxSeriesMergeFileSelector(resource, budget);\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown MergeFileStrategy \" + strategy);\n+    }\n+  }\n+\n+  /**\n+   * acquire the write locks of the resource , the merge lock and the hot compaction lock\n+   */\n+  private void doubleWriteLock(TsFileResource seqFile) {\n+    boolean fileLockGot;\n+    boolean mergeLockGot;\n+    boolean hotCompactionLockGot;\n+    while (true) {\n+      fileLockGot = seqFile.tryWriteLock();\n+      mergeLockGot = mergeLock.writeLock().tryLock();\n+      hotCompactionLockGot = tryWriteLock();\n+\n+      if (fileLockGot && mergeLockGot && hotCompactionLockGot) {\n+        break;\n+      } else {\n+        // did not get all of them, release the gotten one and retry\n+        if (hotCompactionLockGot) {\n+          writeUnlock();\n+        }\n+        if (mergeLockGot) {\n+          mergeLock.writeLock().unlock();\n+        }\n+        if (fileLockGot) {\n+          seqFile.writeUnlock();\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * release the write locks of the resource , the merge lock and the hot compaction lock\n+   */\n+  private void doubleWriteUnlock(TsFileResource seqFile) {\n+    writeUnlock();\n+    mergeLock.writeLock().unlock();\n+    seqFile.writeUnlock();\n+  }\n+\n+  private void removeUnseqFiles(List<TsFileResource> unseqFiles) {\n+    mergeLock.writeLock().lock();\n+    writeLock();\n+    try {\n+      removeAll(unseqFiles, false);\n+    } finally {\n+      writeUnlock();\n+      mergeLock.writeLock().unlock();\n+    }\n+\n+    for (TsFileResource unseqFile : unseqFiles) {\n+      unseqFile.writeLock();\n+      try {\n+        unseqFile.remove();\n+      } finally {\n+        unseqFile.writeUnlock();\n+      }\n+    }\n+  }\n+\n+  @SuppressWarnings(\"squid:S1141\")\n+  private void updateMergeModification(TsFileResource seqFile) {\n+    try {\n+      // remove old modifications and write modifications generated during merge\n+      seqFile.removeModFile();\n+      if (mergingModification != null) {\n+        for (Modification modification : mergingModification.getModifications()) {\n+          seqFile.getModFile().write(modification);\n+        }\n+        try {\n+          seqFile.getModFile().close();\n+        } catch (IOException e) {\n+          logger\n+              .error(\"Cannot close the ModificationFile {}\", seqFile.getModFile().getFilePath(), e);\n+        }\n+      }\n+    } catch (IOException e) {\n+      logger.error(\"{} cannot clean the ModificationFile of {} after merge\", storageGroupName,\n+          seqFile.getTsFile(), e);\n+    }\n+  }\n+\n+  private void removeMergingModification() {\n+    try {\n+      if (mergingModification != null) {\n+        mergingModification.remove();\n+        mergingModification = null;\n+      }\n+    } catch (IOException e) {\n+      logger.error(\"{} cannot remove merging modification \", storageGroupName, e);\n+    }\n+  }\n+\n+  public void mergeEndAction(List<TsFileResource> seqFiles, List<TsFileResource> unseqFiles,\n+      File mergeLog) {\n+    logger.info(\"{} a merge task is ending...\", storageGroupName);\n+\n+    if (unseqFiles.isEmpty()) {\n+      // merge runtime exception arose, just end this merge\n+      isUnseqMerging = false;\n+      logger.info(\"{} a merge task abnormally ends\", storageGroupName);\n+      return;\n+    }\n+\n+    removeUnseqFiles(unseqFiles);\n+\n+    for (int i = 0; i < seqFiles.size(); i++) {\n+      TsFileResource seqFile = seqFiles.get(i);\n+      // get both seqFile lock and merge lock\n+      doubleWriteLock(seqFile);\n+\n+      try {\n+        updateMergeModification(seqFile);\n+        if (i == seqFiles.size() - 1) {\n+          //FIXME if there is an exception, the the modification file will be not closed.\n+          removeMergingModification();\n+          isUnseqMerging = false;\n+          mergeLog.delete();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE0OTcxNg=="}, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 255}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MzA1OTkzOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMToxMzozMlrOHknCdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNDowNzowOFrOHk-u2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE1MDM4OA==", "bodyText": "reader should be closed", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508150388", "createdAt": "2020-10-20T01:13:32Z", "author": {"login": "samperson1997"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -407,135 +430,152 @@ public void recover() {\n   }\n \n   @Override\n-  public void forkCurrentFileList(long timePartition) throws IOException {\n-    Pair<Long, Map<Path, MeasurementSchema>> seqResult = forkTsFileList(\n+  public void forkCurrentFileList(long timePartition) {\n+    Pair<Double, Double> seqStatisticsPair = forkTsFileList(\n         forkedSequenceTsFileResources,\n-        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources));\n-    forkedSeqListPointNum = seqResult.left;\n-    forkedSeqListPathMeasurementSchemaMap = seqResult.right;\n-    Pair<Long, Map<Path, MeasurementSchema>> unSeqResult = forkTsFileList(\n+        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources),\n+        maxLevelNum);\n+    forkedSeqListPointNum = seqStatisticsPair.left;\n+    forkedSeqListMeasurementSize = seqStatisticsPair.right;\n+    Pair<Double, Double> unSeqStatisticsPair = forkTsFileList(\n         forkedUnSequenceTsFileResources,\n         unSequenceTsFileResources\n-            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources));\n-    forkedUnSeqListPointNum = unSeqResult.left;\n-    forkedUnSeqListPathMeasurementSchemaMap = unSeqResult.right;\n+            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources), maxUnseqLevelNum);\n+    forkedUnSeqListPointNum = unSeqStatisticsPair.left;\n+    forkedUnSeqListMeasurementSize = unSeqStatisticsPair.right;\n   }\n \n-  private Pair<Long, Map<Path, MeasurementSchema>> forkTsFileList(\n+  private Pair<Double, Double> forkTsFileList(\n       List<List<TsFileResource>> forkedTsFileResources,\n-      List rawTsFileResources) throws IOException {\n+      List rawTsFileResources, int currMaxLevel) {\n     forkedTsFileResources.clear();\n     // just fork part of the TsFile list, controlled by max_merge_chunk_point\n     long pointNum = 0;\n     // all flush to target file\n-    Map<Path, MeasurementSchema> pathMeasurementSchemaMap = new HashMap<>();\n-    for (int i = 0; i < maxLevelNum - 1; i++) {\n+    ICardinality measurementSet = new HyperLogLog(13);\n+    for (int i = 0; i < currMaxLevel - 1; i++) {\n       List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n       Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n           .get(i);\n-      for (TsFileResource tsFileResource : levelRawTsFileResources) {\n-        if (tsFileResource.isClosed()) {\n-          RestorableTsFileIOWriter writer;\n-          try {\n-            writer = new RestorableTsFileIOWriter(\n-                tsFileResource.getTsFile());\n-          } catch (Exception e) {\n-            logger.error(\"[Hot Compaction] {} open writer failed\",\n-                tsFileResource.getTsFile().getPath(), e);\n-            continue;\n-          }\n-          Map<String, Map<String, List<ChunkMetadata>>> schemaMap = writer\n-              .getMetadatasForQuery();\n-          for (Entry<String, Map<String, List<ChunkMetadata>>> schemaMapEntry : schemaMap\n-              .entrySet()) {\n-            String device = schemaMapEntry.getKey();\n-            for (Entry<String, List<ChunkMetadata>> entry : schemaMapEntry.getValue()\n-                .entrySet()) {\n-              String measurement = entry.getKey();\n-              List<ChunkMetadata> chunkMetadataList = entry.getValue();\n-              for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n-                pointNum += chunkMetadata.getNumOfPoints();\n+      synchronized (levelRawTsFileResources) {\n+        for (TsFileResource tsFileResource : levelRawTsFileResources) {\n+          if (tsFileResource.isClosed()) {\n+            String path = tsFileResource.getTsFile().getAbsolutePath();\n+            try {\n+              if (tsFileResource.getTsFile().exists()) {\n+                TsFileSequenceReader reader = new TsFileSequenceReader(path);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 233}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzODU4NA==", "bodyText": "close it afterward", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508538584", "createdAt": "2020-10-20T14:07:08Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/level/LevelTsFileManagement.java", "diffHunk": "@@ -407,135 +430,152 @@ public void recover() {\n   }\n \n   @Override\n-  public void forkCurrentFileList(long timePartition) throws IOException {\n-    Pair<Long, Map<Path, MeasurementSchema>> seqResult = forkTsFileList(\n+  public void forkCurrentFileList(long timePartition) {\n+    Pair<Double, Double> seqStatisticsPair = forkTsFileList(\n         forkedSequenceTsFileResources,\n-        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources));\n-    forkedSeqListPointNum = seqResult.left;\n-    forkedSeqListPathMeasurementSchemaMap = seqResult.right;\n-    Pair<Long, Map<Path, MeasurementSchema>> unSeqResult = forkTsFileList(\n+        sequenceTsFileResources.computeIfAbsent(timePartition, this::newSequenceTsFileResources),\n+        maxLevelNum);\n+    forkedSeqListPointNum = seqStatisticsPair.left;\n+    forkedSeqListMeasurementSize = seqStatisticsPair.right;\n+    Pair<Double, Double> unSeqStatisticsPair = forkTsFileList(\n         forkedUnSequenceTsFileResources,\n         unSequenceTsFileResources\n-            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources));\n-    forkedUnSeqListPointNum = unSeqResult.left;\n-    forkedUnSeqListPathMeasurementSchemaMap = unSeqResult.right;\n+            .computeIfAbsent(timePartition, this::newUnSequenceTsFileResources), maxUnseqLevelNum);\n+    forkedUnSeqListPointNum = unSeqStatisticsPair.left;\n+    forkedUnSeqListMeasurementSize = unSeqStatisticsPair.right;\n   }\n \n-  private Pair<Long, Map<Path, MeasurementSchema>> forkTsFileList(\n+  private Pair<Double, Double> forkTsFileList(\n       List<List<TsFileResource>> forkedTsFileResources,\n-      List rawTsFileResources) throws IOException {\n+      List rawTsFileResources, int currMaxLevel) {\n     forkedTsFileResources.clear();\n     // just fork part of the TsFile list, controlled by max_merge_chunk_point\n     long pointNum = 0;\n     // all flush to target file\n-    Map<Path, MeasurementSchema> pathMeasurementSchemaMap = new HashMap<>();\n-    for (int i = 0; i < maxLevelNum - 1; i++) {\n+    ICardinality measurementSet = new HyperLogLog(13);\n+    for (int i = 0; i < currMaxLevel - 1; i++) {\n       List<TsFileResource> forkedLevelTsFileResources = new ArrayList<>();\n       Collection<TsFileResource> levelRawTsFileResources = (Collection<TsFileResource>) rawTsFileResources\n           .get(i);\n-      for (TsFileResource tsFileResource : levelRawTsFileResources) {\n-        if (tsFileResource.isClosed()) {\n-          RestorableTsFileIOWriter writer;\n-          try {\n-            writer = new RestorableTsFileIOWriter(\n-                tsFileResource.getTsFile());\n-          } catch (Exception e) {\n-            logger.error(\"[Hot Compaction] {} open writer failed\",\n-                tsFileResource.getTsFile().getPath(), e);\n-            continue;\n-          }\n-          Map<String, Map<String, List<ChunkMetadata>>> schemaMap = writer\n-              .getMetadatasForQuery();\n-          for (Entry<String, Map<String, List<ChunkMetadata>>> schemaMapEntry : schemaMap\n-              .entrySet()) {\n-            String device = schemaMapEntry.getKey();\n-            for (Entry<String, List<ChunkMetadata>> entry : schemaMapEntry.getValue()\n-                .entrySet()) {\n-              String measurement = entry.getKey();\n-              List<ChunkMetadata> chunkMetadataList = entry.getValue();\n-              for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n-                pointNum += chunkMetadata.getNumOfPoints();\n+      synchronized (levelRawTsFileResources) {\n+        for (TsFileResource tsFileResource : levelRawTsFileResources) {\n+          if (tsFileResource.isClosed()) {\n+            String path = tsFileResource.getTsFile().getAbsolutePath();\n+            try {\n+              if (tsFileResource.getTsFile().exists()) {\n+                TsFileSequenceReader reader = new TsFileSequenceReader(path);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE1MDM4OA=="}, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 233}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MzA2MDk1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMToxNDowOVrOHknDAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNDowNzozNVrOHk-wRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE1MDUyOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      newChunk.getHeader().getDataSize() + newChunk.getData().position());\n          \n          \n            \n                      (long) newChunk.getHeader().getDataSize() + newChunk.getData().position());", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508150529", "createdAt": "2020-10-20T01:14:09Z", "author": {"login": "samperson1997"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "diffHunk": "@@ -91,130 +84,204 @@ private HotCompactionUtils() {\n     return new Pair<>(newChunkMetadata, newChunk);\n   }\n \n-  private static long readUnseqChunk(String storageGroup,\n-      Map<String, TsFileSequenceReader> tsFileSequenceReaderMap, String deviceId, long maxVersion,\n-      String measurementId,\n-      Map<Long, TimeValuePair> timeValuePairMap, List<TsFileResource> levelResources)\n+  private static long readByDeserializeMerge(RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadataMap, long maxVersion,\n+      Map<Long, TimeValuePair> timeValuePairMap)\n       throws IOException {\n-    for (TsFileResource levelResource : levelResources) {\n-      TsFileSequenceReader reader = buildReaderFromTsFileResource(levelResource,\n-          tsFileSequenceReaderMap,\n-          storageGroup);\n-      if (reader == null) {\n-        continue;\n-      }\n-      List<ChunkMetadata> chunkMetadataList = reader\n-          .getChunkMetadataList(new Path(deviceId, measurementId));\n+    for (Entry<TsFileSequenceReader, List<ChunkMetadata>> entry : readerChunkMetadataMap\n+        .entrySet()) {\n+      TsFileSequenceReader reader = entry.getKey();\n+      List<ChunkMetadata> chunkMetadataList = entry.getValue();\n       for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n         maxVersion = Math.max(chunkMetadata.getVersion(), maxVersion);\n         IChunkReader chunkReader = new ChunkReaderByTimestamp(\n             reader.readMemChunk(chunkMetadata));\n+        long chunkSize = 0;\n         while (chunkReader.hasNextSatisfiedPage()) {\n           IPointReader iPointReader = new BatchDataIterator(\n               chunkReader.nextPageData());\n           while (iPointReader.hasNextTimeValuePair()) {\n             TimeValuePair timeValuePair = iPointReader.nextTimeValuePair();\n+            chunkSize += timeValuePair.getSize();\n             timeValuePairMap.put(timeValuePair.getTimestamp(), timeValuePair);\n           }\n         }\n+        MergeManager\n+            .mergeRateLimiterAcquire(compactionReadRateLimiter, chunkSize);\n       }\n     }\n     return maxVersion;\n   }\n \n-  private static void fillDeviceMeasurementMap(Set<String> devices,\n-      Map<String, Map<String, MeasurementSchema>> deviceMeasurementMap,\n-      List<TsFileResource> subLevelResources,\n+  private static long writeByAppendMerge(long maxVersion, String device,\n+      RateLimiter compactionWriteRateLimiter, RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadatasMap,\n+      TsFileResource targetResource, RestorableTsFileIOWriter writer) throws IOException {\n+    Pair<ChunkMetadata, Chunk> chunkPair = readByAppendMerge(compactionReadRateLimiter,\n+        readerChunkMetadatasMap);\n+    ChunkMetadata newChunkMetadata = chunkPair.left;\n+    Chunk newChunk = chunkPair.right;\n+    if (newChunkMetadata != null && newChunk != null) {\n+      maxVersion = Math.max(newChunkMetadata.getVersion(), maxVersion);\n+      // wait for limit write\n+      MergeManager.mergeRateLimiterAcquire(compactionWriteRateLimiter,\n+          newChunk.getHeader().getDataSize() + newChunk.getData().position());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzODk1MQ==", "bodyText": "accept", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508538951", "createdAt": "2020-10-20T14:07:35Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "diffHunk": "@@ -91,130 +84,204 @@ private HotCompactionUtils() {\n     return new Pair<>(newChunkMetadata, newChunk);\n   }\n \n-  private static long readUnseqChunk(String storageGroup,\n-      Map<String, TsFileSequenceReader> tsFileSequenceReaderMap, String deviceId, long maxVersion,\n-      String measurementId,\n-      Map<Long, TimeValuePair> timeValuePairMap, List<TsFileResource> levelResources)\n+  private static long readByDeserializeMerge(RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadataMap, long maxVersion,\n+      Map<Long, TimeValuePair> timeValuePairMap)\n       throws IOException {\n-    for (TsFileResource levelResource : levelResources) {\n-      TsFileSequenceReader reader = buildReaderFromTsFileResource(levelResource,\n-          tsFileSequenceReaderMap,\n-          storageGroup);\n-      if (reader == null) {\n-        continue;\n-      }\n-      List<ChunkMetadata> chunkMetadataList = reader\n-          .getChunkMetadataList(new Path(deviceId, measurementId));\n+    for (Entry<TsFileSequenceReader, List<ChunkMetadata>> entry : readerChunkMetadataMap\n+        .entrySet()) {\n+      TsFileSequenceReader reader = entry.getKey();\n+      List<ChunkMetadata> chunkMetadataList = entry.getValue();\n       for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n         maxVersion = Math.max(chunkMetadata.getVersion(), maxVersion);\n         IChunkReader chunkReader = new ChunkReaderByTimestamp(\n             reader.readMemChunk(chunkMetadata));\n+        long chunkSize = 0;\n         while (chunkReader.hasNextSatisfiedPage()) {\n           IPointReader iPointReader = new BatchDataIterator(\n               chunkReader.nextPageData());\n           while (iPointReader.hasNextTimeValuePair()) {\n             TimeValuePair timeValuePair = iPointReader.nextTimeValuePair();\n+            chunkSize += timeValuePair.getSize();\n             timeValuePairMap.put(timeValuePair.getTimestamp(), timeValuePair);\n           }\n         }\n+        MergeManager\n+            .mergeRateLimiterAcquire(compactionReadRateLimiter, chunkSize);\n       }\n     }\n     return maxVersion;\n   }\n \n-  private static void fillDeviceMeasurementMap(Set<String> devices,\n-      Map<String, Map<String, MeasurementSchema>> deviceMeasurementMap,\n-      List<TsFileResource> subLevelResources,\n+  private static long writeByAppendMerge(long maxVersion, String device,\n+      RateLimiter compactionWriteRateLimiter, RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadatasMap,\n+      TsFileResource targetResource, RestorableTsFileIOWriter writer) throws IOException {\n+    Pair<ChunkMetadata, Chunk> chunkPair = readByAppendMerge(compactionReadRateLimiter,\n+        readerChunkMetadatasMap);\n+    ChunkMetadata newChunkMetadata = chunkPair.left;\n+    Chunk newChunk = chunkPair.right;\n+    if (newChunkMetadata != null && newChunk != null) {\n+      maxVersion = Math.max(newChunkMetadata.getVersion(), maxVersion);\n+      // wait for limit write\n+      MergeManager.mergeRateLimiterAcquire(compactionWriteRateLimiter,\n+          newChunk.getHeader().getDataSize() + newChunk.getData().position());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE1MDUyOQ=="}, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MzA2NDE2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMToxNTo0MFrOHknEvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNDowNzo0NVrOHk-w3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE1MDk3Mw==", "bodyText": "Why public?", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508150973", "createdAt": "2020-10-20T01:15:40Z", "author": {"login": "samperson1997"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -208,26 +200,15 @@\n   private File storageGroupSysDir;\n \n   // manage seqFileList and unSeqFileList\n-  private TsFileManagement tsFileManagement;\n+  public TsFileManagement tsFileManagement;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzOTEwMA==", "bodyText": "use outside in test", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508539100", "createdAt": "2020-10-20T14:07:45Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/storagegroup/StorageGroupProcessor.java", "diffHunk": "@@ -208,26 +200,15 @@\n   private File storageGroupSysDir;\n \n   // manage seqFileList and unSeqFileList\n-  private TsFileManagement tsFileManagement;\n+  public TsFileManagement tsFileManagement;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE1MDk3Mw=="}, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MzE3MjY0OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMjoxNDo0NlrOHkoDlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNDowODowNlrOHk-yCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2NzA2Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              public ReentrantReadWriteLock mergeLock = new ReentrantReadWriteLock();\n          \n          \n            \n              private final ReentrantReadWriteLock mergeLock = new ReentrantReadWriteLock();", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508167063", "createdAt": "2020-10-20T02:14:46Z", "author": {"login": "samperson1997"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "diffHunk": "@@ -19,24 +19,56 @@\n \n package org.apache.iotdb.db.engine.tsfilemanagement;\n \n+import static org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.MERGING_MODIFICATION_FILE_NAME;\n+\n+import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.locks.ReadWriteLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.apache.iotdb.db.conf.IoTDBDescriptor;\n+import org.apache.iotdb.db.engine.merge.manage.MergeManager;\n+import org.apache.iotdb.db.engine.merge.manage.MergeResource;\n+import org.apache.iotdb.db.engine.merge.selector.IMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxFileMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxSeriesMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MergeFileStrategy;\n+import org.apache.iotdb.db.engine.merge.task.MergeTask;\n+import org.apache.iotdb.db.engine.modification.Modification;\n+import org.apache.iotdb.db.engine.modification.ModificationFile;\n import org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.CloseHotCompactionMergeCallBack;\n import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.engine.tsfilemanagement.level.LevelTsFileManagement;\n+import org.apache.iotdb.db.exception.MergeException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public abstract class TsFileManagement {\n \n+  private static final Logger logger = LoggerFactory.getLogger(TsFileManagement.class);\n   protected String storageGroupName;\n   protected String storageGroupDir;\n+\n+  /**\n+   * mergeLock is to be used in the merge process. Concurrent queries, deletions and merges may\n+   * result in losing some deletion in the merged new file, so a lock is necessary.\n+   */\n+  public ReentrantReadWriteLock mergeLock = new ReentrantReadWriteLock();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzOTQwMw==", "bodyText": "accept it", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508539403", "createdAt": "2020-10-20T14:08:06Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "diffHunk": "@@ -19,24 +19,56 @@\n \n package org.apache.iotdb.db.engine.tsfilemanagement;\n \n+import static org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.MERGING_MODIFICATION_FILE_NAME;\n+\n+import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.locks.ReadWriteLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.apache.iotdb.db.conf.IoTDBDescriptor;\n+import org.apache.iotdb.db.engine.merge.manage.MergeManager;\n+import org.apache.iotdb.db.engine.merge.manage.MergeResource;\n+import org.apache.iotdb.db.engine.merge.selector.IMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxFileMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxSeriesMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MergeFileStrategy;\n+import org.apache.iotdb.db.engine.merge.task.MergeTask;\n+import org.apache.iotdb.db.engine.modification.Modification;\n+import org.apache.iotdb.db.engine.modification.ModificationFile;\n import org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.CloseHotCompactionMergeCallBack;\n import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.engine.tsfilemanagement.level.LevelTsFileManagement;\n+import org.apache.iotdb.db.exception.MergeException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public abstract class TsFileManagement {\n \n+  private static final Logger logger = LoggerFactory.getLogger(TsFileManagement.class);\n   protected String storageGroupName;\n   protected String storageGroupDir;\n+\n+  /**\n+   * mergeLock is to be used in the merge process. Concurrent queries, deletions and merges may\n+   * result in losing some deletion in the merged new file, so a lock is necessary.\n+   */\n+  public ReentrantReadWriteLock mergeLock = new ReentrantReadWriteLock();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2NzA2Mw=="}, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MzE3Mzg2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMjoxNToyMlrOHkoESg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNDowODoyOVrOHk-zMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2NzI0Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              public volatile boolean isUnseqMerging = false;\n          \n          \n            \n              private volatile boolean isUnseqMerging = false;", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508167242", "createdAt": "2020-10-20T02:15:22Z", "author": {"login": "samperson1997"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "diffHunk": "@@ -19,24 +19,56 @@\n \n package org.apache.iotdb.db.engine.tsfilemanagement;\n \n+import static org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.MERGING_MODIFICATION_FILE_NAME;\n+\n+import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.locks.ReadWriteLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.apache.iotdb.db.conf.IoTDBDescriptor;\n+import org.apache.iotdb.db.engine.merge.manage.MergeManager;\n+import org.apache.iotdb.db.engine.merge.manage.MergeResource;\n+import org.apache.iotdb.db.engine.merge.selector.IMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxFileMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxSeriesMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MergeFileStrategy;\n+import org.apache.iotdb.db.engine.merge.task.MergeTask;\n+import org.apache.iotdb.db.engine.modification.Modification;\n+import org.apache.iotdb.db.engine.modification.ModificationFile;\n import org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.CloseHotCompactionMergeCallBack;\n import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.engine.tsfilemanagement.level.LevelTsFileManagement;\n+import org.apache.iotdb.db.exception.MergeException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public abstract class TsFileManagement {\n \n+  private static final Logger logger = LoggerFactory.getLogger(TsFileManagement.class);\n   protected String storageGroupName;\n   protected String storageGroupDir;\n+\n+  /**\n+   * mergeLock is to be used in the merge process. Concurrent queries, deletions and merges may\n+   * result in losing some deletion in the merged new file, so a lock is necessary.\n+   */\n+  public ReentrantReadWriteLock mergeLock = new ReentrantReadWriteLock();\n   /**\n    * hotCompactionMergeLock is used to wait for TsFile list change in hot compaction merge\n    * processor.\n    */\n   private final ReadWriteLock hotCompactionMergeLock = new ReentrantReadWriteLock();\n \n+  public volatile boolean isUnseqMerging = false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODUzOTY5Ng==", "bodyText": "use in test", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508539696", "createdAt": "2020-10-20T14:08:29Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "diffHunk": "@@ -19,24 +19,56 @@\n \n package org.apache.iotdb.db.engine.tsfilemanagement;\n \n+import static org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.MERGING_MODIFICATION_FILE_NAME;\n+\n+import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.locks.ReadWriteLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.apache.iotdb.db.conf.IoTDBDescriptor;\n+import org.apache.iotdb.db.engine.merge.manage.MergeManager;\n+import org.apache.iotdb.db.engine.merge.manage.MergeResource;\n+import org.apache.iotdb.db.engine.merge.selector.IMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxFileMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxSeriesMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MergeFileStrategy;\n+import org.apache.iotdb.db.engine.merge.task.MergeTask;\n+import org.apache.iotdb.db.engine.modification.Modification;\n+import org.apache.iotdb.db.engine.modification.ModificationFile;\n import org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.CloseHotCompactionMergeCallBack;\n import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.engine.tsfilemanagement.level.LevelTsFileManagement;\n+import org.apache.iotdb.db.exception.MergeException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public abstract class TsFileManagement {\n \n+  private static final Logger logger = LoggerFactory.getLogger(TsFileManagement.class);\n   protected String storageGroupName;\n   protected String storageGroupDir;\n+\n+  /**\n+   * mergeLock is to be used in the merge process. Concurrent queries, deletions and merges may\n+   * result in losing some deletion in the merged new file, so a lock is necessary.\n+   */\n+  public ReentrantReadWriteLock mergeLock = new ReentrantReadWriteLock();\n   /**\n    * hotCompactionMergeLock is used to wait for TsFile list change in hot compaction merge\n    * processor.\n    */\n   private final ReadWriteLock hotCompactionMergeLock = new ReentrantReadWriteLock();\n \n+  public volatile boolean isUnseqMerging = false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2NzI0Mg=="}, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MzE4MTYxOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMjoxOTozOVrOHkoIqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNDowODo0OFrOHk-0fA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2ODM2Mw==", "bodyText": "Please check the accessibility of all new fields ... Class variable fields should not have public accessibility\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              public long mergeStartTime;\n          \n          \n            \n              private long mergeStartTime;", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508168363", "createdAt": "2020-10-20T02:19:39Z", "author": {"login": "samperson1997"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "diffHunk": "@@ -19,24 +19,56 @@\n \n package org.apache.iotdb.db.engine.tsfilemanagement;\n \n+import static org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.MERGING_MODIFICATION_FILE_NAME;\n+\n+import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.locks.ReadWriteLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.apache.iotdb.db.conf.IoTDBDescriptor;\n+import org.apache.iotdb.db.engine.merge.manage.MergeManager;\n+import org.apache.iotdb.db.engine.merge.manage.MergeResource;\n+import org.apache.iotdb.db.engine.merge.selector.IMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxFileMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxSeriesMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MergeFileStrategy;\n+import org.apache.iotdb.db.engine.merge.task.MergeTask;\n+import org.apache.iotdb.db.engine.modification.Modification;\n+import org.apache.iotdb.db.engine.modification.ModificationFile;\n import org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.CloseHotCompactionMergeCallBack;\n import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.engine.tsfilemanagement.level.LevelTsFileManagement;\n+import org.apache.iotdb.db.exception.MergeException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public abstract class TsFileManagement {\n \n+  private static final Logger logger = LoggerFactory.getLogger(TsFileManagement.class);\n   protected String storageGroupName;\n   protected String storageGroupDir;\n+\n+  /**\n+   * mergeLock is to be used in the merge process. Concurrent queries, deletions and merges may\n+   * result in losing some deletion in the merged new file, so a lock is necessary.\n+   */\n+  public ReentrantReadWriteLock mergeLock = new ReentrantReadWriteLock();\n   /**\n    * hotCompactionMergeLock is used to wait for TsFile list change in hot compaction merge\n    * processor.\n    */\n   private final ReadWriteLock hotCompactionMergeLock = new ReentrantReadWriteLock();\n \n+  public volatile boolean isUnseqMerging = false;\n+  /**\n+   * This is the modification file of the result of the current merge. Because the merged file may\n+   * be invisible at this moment, without this, deletion/update during merge could be lost.\n+   */\n+  public ModificationFile mergingModification;\n+  public long mergeStartTime;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU0MDAyOA==", "bodyText": "accept it", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508540028", "createdAt": "2020-10-20T14:08:48Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "diffHunk": "@@ -19,24 +19,56 @@\n \n package org.apache.iotdb.db.engine.tsfilemanagement;\n \n+import static org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.MERGING_MODIFICATION_FILE_NAME;\n+\n+import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.locks.ReadWriteLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.apache.iotdb.db.conf.IoTDBDescriptor;\n+import org.apache.iotdb.db.engine.merge.manage.MergeManager;\n+import org.apache.iotdb.db.engine.merge.manage.MergeResource;\n+import org.apache.iotdb.db.engine.merge.selector.IMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxFileMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxSeriesMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MergeFileStrategy;\n+import org.apache.iotdb.db.engine.merge.task.MergeTask;\n+import org.apache.iotdb.db.engine.modification.Modification;\n+import org.apache.iotdb.db.engine.modification.ModificationFile;\n import org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.CloseHotCompactionMergeCallBack;\n import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.engine.tsfilemanagement.level.LevelTsFileManagement;\n+import org.apache.iotdb.db.exception.MergeException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public abstract class TsFileManagement {\n \n+  private static final Logger logger = LoggerFactory.getLogger(TsFileManagement.class);\n   protected String storageGroupName;\n   protected String storageGroupDir;\n+\n+  /**\n+   * mergeLock is to be used in the merge process. Concurrent queries, deletions and merges may\n+   * result in losing some deletion in the merged new file, so a lock is necessary.\n+   */\n+  public ReentrantReadWriteLock mergeLock = new ReentrantReadWriteLock();\n   /**\n    * hotCompactionMergeLock is used to wait for TsFile list change in hot compaction merge\n    * processor.\n    */\n   private final ReadWriteLock hotCompactionMergeLock = new ReentrantReadWriteLock();\n \n+  public volatile boolean isUnseqMerging = false;\n+  /**\n+   * This is the modification file of the result of the current merge. Because the merged file may\n+   * be invisible at this moment, without this, deletion/update during merge could be lost.\n+   */\n+  public ModificationFile mergingModification;\n+  public long mergeStartTime;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2ODM2Mw=="}, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MzE4OTAwOnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMjoyMzozNVrOHkoM2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNDowOToyNFrOHk-2aA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2OTQzNQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              private static final int mergePagePointNum = IoTDBDescriptor.getInstance().getConfig()\n          \n          \n            \n              private static final int MERGE_PAGE_POINT_NUM = IoTDBDescriptor.getInstance().getConfig()", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508169435", "createdAt": "2020-10-20T02:23:35Z", "author": {"login": "samperson1997"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "diffHunk": "@@ -53,32 +55,23 @@\n public class HotCompactionUtils {\n \n   private static final Logger logger = LoggerFactory.getLogger(HotCompactionUtils.class);\n+  private static final int mergePagePointNum = IoTDBDescriptor.getInstance().getConfig()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU0MDUyMA==", "bodyText": "accept it", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508540520", "createdAt": "2020-10-20T14:09:24Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "diffHunk": "@@ -53,32 +55,23 @@\n public class HotCompactionUtils {\n \n   private static final Logger logger = LoggerFactory.getLogger(HotCompactionUtils.class);\n+  private static final int mergePagePointNum = IoTDBDescriptor.getInstance().getConfig()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2OTQzNQ=="}, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MzE5MDM2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMjoyNDoyMVrOHkoNqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNDowOTo1NFrOHk-4Pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2OTY0MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                if (chunkMetadataList.size() <= 0) {\n          \n          \n            \n                if (chunkMetadataList.isEmpty()) {", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508169641", "createdAt": "2020-10-20T02:24:21Z", "author": {"login": "samperson1997"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "diffHunk": "@@ -91,130 +84,204 @@ private HotCompactionUtils() {\n     return new Pair<>(newChunkMetadata, newChunk);\n   }\n \n-  private static long readUnseqChunk(String storageGroup,\n-      Map<String, TsFileSequenceReader> tsFileSequenceReaderMap, String deviceId, long maxVersion,\n-      String measurementId,\n-      Map<Long, TimeValuePair> timeValuePairMap, List<TsFileResource> levelResources)\n+  private static long readByDeserializeMerge(RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadataMap, long maxVersion,\n+      Map<Long, TimeValuePair> timeValuePairMap)\n       throws IOException {\n-    for (TsFileResource levelResource : levelResources) {\n-      TsFileSequenceReader reader = buildReaderFromTsFileResource(levelResource,\n-          tsFileSequenceReaderMap,\n-          storageGroup);\n-      if (reader == null) {\n-        continue;\n-      }\n-      List<ChunkMetadata> chunkMetadataList = reader\n-          .getChunkMetadataList(new Path(deviceId, measurementId));\n+    for (Entry<TsFileSequenceReader, List<ChunkMetadata>> entry : readerChunkMetadataMap\n+        .entrySet()) {\n+      TsFileSequenceReader reader = entry.getKey();\n+      List<ChunkMetadata> chunkMetadataList = entry.getValue();\n       for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n         maxVersion = Math.max(chunkMetadata.getVersion(), maxVersion);\n         IChunkReader chunkReader = new ChunkReaderByTimestamp(\n             reader.readMemChunk(chunkMetadata));\n+        long chunkSize = 0;\n         while (chunkReader.hasNextSatisfiedPage()) {\n           IPointReader iPointReader = new BatchDataIterator(\n               chunkReader.nextPageData());\n           while (iPointReader.hasNextTimeValuePair()) {\n             TimeValuePair timeValuePair = iPointReader.nextTimeValuePair();\n+            chunkSize += timeValuePair.getSize();\n             timeValuePairMap.put(timeValuePair.getTimestamp(), timeValuePair);\n           }\n         }\n+        MergeManager\n+            .mergeRateLimiterAcquire(compactionReadRateLimiter, chunkSize);\n       }\n     }\n     return maxVersion;\n   }\n \n-  private static void fillDeviceMeasurementMap(Set<String> devices,\n-      Map<String, Map<String, MeasurementSchema>> deviceMeasurementMap,\n-      List<TsFileResource> subLevelResources,\n+  private static long writeByAppendMerge(long maxVersion, String device,\n+      RateLimiter compactionWriteRateLimiter, RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadatasMap,\n+      TsFileResource targetResource, RestorableTsFileIOWriter writer) throws IOException {\n+    Pair<ChunkMetadata, Chunk> chunkPair = readByAppendMerge(compactionReadRateLimiter,\n+        readerChunkMetadatasMap);\n+    ChunkMetadata newChunkMetadata = chunkPair.left;\n+    Chunk newChunk = chunkPair.right;\n+    if (newChunkMetadata != null && newChunk != null) {\n+      maxVersion = Math.max(newChunkMetadata.getVersion(), maxVersion);\n+      // wait for limit write\n+      MergeManager.mergeRateLimiterAcquire(compactionWriteRateLimiter,\n+          newChunk.getHeader().getDataSize() + newChunk.getData().position());\n+      writer.writeChunk(newChunk, newChunkMetadata);\n+      targetResource.updateStartTime(device, newChunkMetadata.getStartTime());\n+      targetResource.updateEndTime(device, newChunkMetadata.getEndTime());\n+    }\n+    return maxVersion;\n+  }\n+\n+  private static long writeByDeserializeMerge(long maxVersion, String device,\n+      RateLimiter compactionRateLimiter, RateLimiter compactionReadRateLimiter,\n+      Entry<String, Map<TsFileSequenceReader, List<ChunkMetadata>>> entry,\n+      TsFileResource targetResource, RestorableTsFileIOWriter writer) throws IOException {\n+    Map<Long, TimeValuePair> timeValuePairMap = new TreeMap<>();\n+    maxVersion = readByDeserializeMerge(compactionReadRateLimiter, entry.getValue(), maxVersion,\n+        timeValuePairMap);\n+    Iterator<List<ChunkMetadata>> chunkMetadataListIterator = entry.getValue().values()\n+        .iterator();\n+    if (!chunkMetadataListIterator.hasNext()) {\n+      return maxVersion;\n+    }\n+    List<ChunkMetadata> chunkMetadataList = chunkMetadataListIterator.next();\n+    if (chunkMetadataList.size() <= 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU0MDk5MQ==", "bodyText": "good idea", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508540991", "createdAt": "2020-10-20T14:09:54Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/utils/HotCompactionUtils.java", "diffHunk": "@@ -91,130 +84,204 @@ private HotCompactionUtils() {\n     return new Pair<>(newChunkMetadata, newChunk);\n   }\n \n-  private static long readUnseqChunk(String storageGroup,\n-      Map<String, TsFileSequenceReader> tsFileSequenceReaderMap, String deviceId, long maxVersion,\n-      String measurementId,\n-      Map<Long, TimeValuePair> timeValuePairMap, List<TsFileResource> levelResources)\n+  private static long readByDeserializeMerge(RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadataMap, long maxVersion,\n+      Map<Long, TimeValuePair> timeValuePairMap)\n       throws IOException {\n-    for (TsFileResource levelResource : levelResources) {\n-      TsFileSequenceReader reader = buildReaderFromTsFileResource(levelResource,\n-          tsFileSequenceReaderMap,\n-          storageGroup);\n-      if (reader == null) {\n-        continue;\n-      }\n-      List<ChunkMetadata> chunkMetadataList = reader\n-          .getChunkMetadataList(new Path(deviceId, measurementId));\n+    for (Entry<TsFileSequenceReader, List<ChunkMetadata>> entry : readerChunkMetadataMap\n+        .entrySet()) {\n+      TsFileSequenceReader reader = entry.getKey();\n+      List<ChunkMetadata> chunkMetadataList = entry.getValue();\n       for (ChunkMetadata chunkMetadata : chunkMetadataList) {\n         maxVersion = Math.max(chunkMetadata.getVersion(), maxVersion);\n         IChunkReader chunkReader = new ChunkReaderByTimestamp(\n             reader.readMemChunk(chunkMetadata));\n+        long chunkSize = 0;\n         while (chunkReader.hasNextSatisfiedPage()) {\n           IPointReader iPointReader = new BatchDataIterator(\n               chunkReader.nextPageData());\n           while (iPointReader.hasNextTimeValuePair()) {\n             TimeValuePair timeValuePair = iPointReader.nextTimeValuePair();\n+            chunkSize += timeValuePair.getSize();\n             timeValuePairMap.put(timeValuePair.getTimestamp(), timeValuePair);\n           }\n         }\n+        MergeManager\n+            .mergeRateLimiterAcquire(compactionReadRateLimiter, chunkSize);\n       }\n     }\n     return maxVersion;\n   }\n \n-  private static void fillDeviceMeasurementMap(Set<String> devices,\n-      Map<String, Map<String, MeasurementSchema>> deviceMeasurementMap,\n-      List<TsFileResource> subLevelResources,\n+  private static long writeByAppendMerge(long maxVersion, String device,\n+      RateLimiter compactionWriteRateLimiter, RateLimiter compactionReadRateLimiter,\n+      Map<TsFileSequenceReader, List<ChunkMetadata>> readerChunkMetadatasMap,\n+      TsFileResource targetResource, RestorableTsFileIOWriter writer) throws IOException {\n+    Pair<ChunkMetadata, Chunk> chunkPair = readByAppendMerge(compactionReadRateLimiter,\n+        readerChunkMetadatasMap);\n+    ChunkMetadata newChunkMetadata = chunkPair.left;\n+    Chunk newChunk = chunkPair.right;\n+    if (newChunkMetadata != null && newChunk != null) {\n+      maxVersion = Math.max(newChunkMetadata.getVersion(), maxVersion);\n+      // wait for limit write\n+      MergeManager.mergeRateLimiterAcquire(compactionWriteRateLimiter,\n+          newChunk.getHeader().getDataSize() + newChunk.getData().position());\n+      writer.writeChunk(newChunk, newChunkMetadata);\n+      targetResource.updateStartTime(device, newChunkMetadata.getStartTime());\n+      targetResource.updateEndTime(device, newChunkMetadata.getEndTime());\n+    }\n+    return maxVersion;\n+  }\n+\n+  private static long writeByDeserializeMerge(long maxVersion, String device,\n+      RateLimiter compactionRateLimiter, RateLimiter compactionReadRateLimiter,\n+      Entry<String, Map<TsFileSequenceReader, List<ChunkMetadata>>> entry,\n+      TsFileResource targetResource, RestorableTsFileIOWriter writer) throws IOException {\n+    Map<Long, TimeValuePair> timeValuePairMap = new TreeMap<>();\n+    maxVersion = readByDeserializeMerge(compactionReadRateLimiter, entry.getValue(), maxVersion,\n+        timeValuePairMap);\n+    Iterator<List<ChunkMetadata>> chunkMetadataListIterator = entry.getValue().values()\n+        .iterator();\n+    if (!chunkMetadataListIterator.hasNext()) {\n+      return maxVersion;\n+    }\n+    List<ChunkMetadata> chunkMetadataList = chunkMetadataListIterator.next();\n+    if (chunkMetadataList.size() <= 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE2OTY0MQ=="}, "originalCommit": {"oid": "726a0bdf3bd1bb2102db7540a6a237063b3032d2"}, "originalPosition": 151}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NTkzMjE1OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/HotCompactionMergeTaskPoolManager.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNToxODozM1rOHlCbvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNToyMDo0MFrOHlCioA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU5OTIyOA==", "bodyText": "RejectedExecutionException is a runtime exception. No need to throw it.", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508599228", "createdAt": "2020-10-20T15:18:33Z", "author": {"login": "samperson1997"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/HotCompactionMergeTaskPoolManager.java", "diffHunk": "@@ -100,7 +104,8 @@ public ServiceType getID() {\n     return ServiceType.HOT_COMPACTION_SERVICE;\n   }\n \n-  public void submitTask(HotCompactionMergeTask hotCompactionMergeTask) {\n+  public void submitTask(HotCompactionMergeTask hotCompactionMergeTask)\n+      throws RejectedExecutionException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e76b91d910155b95946682b04f5e55db4020d70"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODYwMDk5Mg==", "bodyText": "No\uff0cI need to catch it out to close current compaction logic", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508600992", "createdAt": "2020-10-20T15:20:40Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/HotCompactionMergeTaskPoolManager.java", "diffHunk": "@@ -100,7 +104,8 @@ public ServiceType getID() {\n     return ServiceType.HOT_COMPACTION_SERVICE;\n   }\n \n-  public void submitTask(HotCompactionMergeTask hotCompactionMergeTask) {\n+  public void submitTask(HotCompactionMergeTask hotCompactionMergeTask)\n+      throws RejectedExecutionException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU5OTIyOA=="}, "originalCommit": {"oid": "5e76b91d910155b95946682b04f5e55db4020d70"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4NTkzNDc2OnYy", "diffSide": "RIGHT", "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNToxOTowOFrOHlCdjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQxNToyNjowNlrOHlC8IQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU5OTY5NQ==", "bodyText": "Remove unused import", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508599695", "createdAt": "2020-10-20T15:19:08Z", "author": {"login": "samperson1997"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "diffHunk": "@@ -19,24 +19,57 @@\n \n package org.apache.iotdb.db.engine.tsfilemanagement;\n \n+import static org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.MERGING_MODIFICATION_FILE_NAME;\n+\n+import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Files;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.locks.ReadWriteLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.apache.iotdb.db.conf.IoTDBDescriptor;\n+import org.apache.iotdb.db.engine.merge.manage.MergeManager;\n+import org.apache.iotdb.db.engine.merge.manage.MergeResource;\n+import org.apache.iotdb.db.engine.merge.selector.IMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxFileMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxSeriesMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MergeFileStrategy;\n+import org.apache.iotdb.db.engine.merge.task.MergeTask;\n+import org.apache.iotdb.db.engine.modification.Modification;\n+import org.apache.iotdb.db.engine.modification.ModificationFile;\n import org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.CloseHotCompactionMergeCallBack;\n import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.engine.tsfilemanagement.level.LevelTsFileManagement;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e76b91d910155b95946682b04f5e55db4020d70"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODYwNzUyMQ==", "bodyText": "okay, I will remove all useless import in whole project", "url": "https://github.com/apache/iotdb/pull/1758#discussion_r508607521", "createdAt": "2020-10-20T15:26:06Z", "author": {"login": "zhanglingzhe0820"}, "path": "server/src/main/java/org/apache/iotdb/db/engine/tsfilemanagement/TsFileManagement.java", "diffHunk": "@@ -19,24 +19,57 @@\n \n package org.apache.iotdb.db.engine.tsfilemanagement;\n \n+import static org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.MERGING_MODIFICATION_FILE_NAME;\n+\n+import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Files;\n import java.util.Iterator;\n import java.util.List;\n import java.util.concurrent.locks.ReadWriteLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import org.apache.iotdb.db.conf.IoTDBDescriptor;\n+import org.apache.iotdb.db.engine.merge.manage.MergeManager;\n+import org.apache.iotdb.db.engine.merge.manage.MergeResource;\n+import org.apache.iotdb.db.engine.merge.selector.IMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxFileMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MaxSeriesMergeFileSelector;\n+import org.apache.iotdb.db.engine.merge.selector.MergeFileStrategy;\n+import org.apache.iotdb.db.engine.merge.task.MergeTask;\n+import org.apache.iotdb.db.engine.modification.Modification;\n+import org.apache.iotdb.db.engine.modification.ModificationFile;\n import org.apache.iotdb.db.engine.storagegroup.StorageGroupProcessor.CloseHotCompactionMergeCallBack;\n import org.apache.iotdb.db.engine.storagegroup.TsFileResource;\n+import org.apache.iotdb.db.engine.tsfilemanagement.level.LevelTsFileManagement;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODU5OTY5NQ=="}, "originalCommit": {"oid": "5e76b91d910155b95946682b04f5e55db4020d70"}, "originalPosition": 25}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 752, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}