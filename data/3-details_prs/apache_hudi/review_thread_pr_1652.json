{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIxMzM1MzQz", "number": 1652, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxNzozMzowNFrOD_Bgbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxNzozMzowNFrOD_Bgbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NDExNTY3OnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "isResolved": false, "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxNzozMzowNFrOGZeyXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yN1QxNjoxMjo0NlrOGbSfNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM3MTk5OA==", "bodyText": "Do you mean something like setting sourceLimit to 5 but you have 10 kafka partitions? Why setting sourceLimit to this small?", "url": "https://github.com/apache/hudi/pull/1652#discussion_r429371998", "createdAt": "2020-05-22T17:33:04Z", "author": {"login": "garyli1019"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -207,6 +208,11 @@ public KafkaOffsetGen(TypedProperties props) {\n     maxEventsToReadFromKafka = (maxEventsToReadFromKafka == Long.MAX_VALUE || maxEventsToReadFromKafka == Integer.MAX_VALUE)\n         ? Config.maxEventsFromKafkaSource : maxEventsToReadFromKafka;\n     long numEvents = sourceLimit == Long.MAX_VALUE ? maxEventsToReadFromKafka : sourceLimit;\n+\n+    if (numEvents < toOffsets.size()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0699e214ef7d6f112fc9f74c8279e445407617d0"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5MDM1Mg==", "bodyText": "The general production environment will not have such operations. But I found this problem in the test environment, and feel that I should give at least one hint", "url": "https://github.com/apache/hudi/pull/1652#discussion_r429590352", "createdAt": "2020-05-24T01:21:23Z", "author": {"login": "liujinhui1994"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -207,6 +208,11 @@ public KafkaOffsetGen(TypedProperties props) {\n     maxEventsToReadFromKafka = (maxEventsToReadFromKafka == Long.MAX_VALUE || maxEventsToReadFromKafka == Integer.MAX_VALUE)\n         ? Config.maxEventsFromKafkaSource : maxEventsToReadFromKafka;\n     long numEvents = sourceLimit == Long.MAX_VALUE ? maxEventsToReadFromKafka : sourceLimit;\n+\n+    if (numEvents < toOffsets.size()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM3MTk5OA=="}, "originalCommit": {"oid": "0699e214ef7d6f112fc9f74c8279e445407617d0"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTY4MDU3Mw==", "bodyText": "Delta streamer has --continues mode, which means it is intended to not consuming data in some cases. Throwing an exception here will interrupt the application unexpectedly.\nIf the goal is to provide a hint to the user, I think we can add a log to computeOffsetRanges.", "url": "https://github.com/apache/hudi/pull/1652#discussion_r429680573", "createdAt": "2020-05-24T22:18:55Z", "author": {"login": "garyli1019"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -207,6 +208,11 @@ public KafkaOffsetGen(TypedProperties props) {\n     maxEventsToReadFromKafka = (maxEventsToReadFromKafka == Long.MAX_VALUE || maxEventsToReadFromKafka == Integer.MAX_VALUE)\n         ? Config.maxEventsFromKafkaSource : maxEventsToReadFromKafka;\n     long numEvents = sourceLimit == Long.MAX_VALUE ? maxEventsToReadFromKafka : sourceLimit;\n+\n+    if (numEvents < toOffsets.size()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM3MTk5OA=="}, "originalCommit": {"oid": "0699e214ef7d6f112fc9f74c8279e445407617d0"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTk3Mjc3Ng==", "bodyText": "Hi @garyli1019, IMO, throwing an exception makes sense, because the 'sourceLimit' is one-time config binding with the job. If the 'sourceLimit' is lesser than the partitions of Kafka, the job will never consume data from Kafka, running correctly without exception is meaningless.", "url": "https://github.com/apache/hudi/pull/1652#discussion_r429972776", "createdAt": "2020-05-25T14:47:12Z", "author": {"login": "wangxianghu"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -207,6 +208,11 @@ public KafkaOffsetGen(TypedProperties props) {\n     maxEventsToReadFromKafka = (maxEventsToReadFromKafka == Long.MAX_VALUE || maxEventsToReadFromKafka == Integer.MAX_VALUE)\n         ? Config.maxEventsFromKafkaSource : maxEventsToReadFromKafka;\n     long numEvents = sourceLimit == Long.MAX_VALUE ? maxEventsToReadFromKafka : sourceLimit;\n+\n+    if (numEvents < toOffsets.size()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM3MTk5OA=="}, "originalCommit": {"oid": "0699e214ef7d6f112fc9f74c8279e445407617d0"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDA3NDcwNw==", "bodyText": "I think if people really set sourceLimit to 0 then they should consume no new data, which is intended behavior, instead of throwing an exception. If they want to consume data then they should set this number higher.", "url": "https://github.com/apache/hudi/pull/1652#discussion_r430074707", "createdAt": "2020-05-25T21:07:14Z", "author": {"login": "garyli1019"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -207,6 +208,11 @@ public KafkaOffsetGen(TypedProperties props) {\n     maxEventsToReadFromKafka = (maxEventsToReadFromKafka == Long.MAX_VALUE || maxEventsToReadFromKafka == Integer.MAX_VALUE)\n         ? Config.maxEventsFromKafkaSource : maxEventsToReadFromKafka;\n     long numEvents = sourceLimit == Long.MAX_VALUE ? maxEventsToReadFromKafka : sourceLimit;\n+\n+    if (numEvents < toOffsets.size()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM3MTk5OA=="}, "originalCommit": {"oid": "0699e214ef7d6f112fc9f74c8279e445407617d0"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDA3NTkwMA==", "bodyText": "@pratyakshsharma thoughts?", "url": "https://github.com/apache/hudi/pull/1652#discussion_r430075900", "createdAt": "2020-05-25T21:13:41Z", "author": {"login": "garyli1019"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -207,6 +208,11 @@ public KafkaOffsetGen(TypedProperties props) {\n     maxEventsToReadFromKafka = (maxEventsToReadFromKafka == Long.MAX_VALUE || maxEventsToReadFromKafka == Integer.MAX_VALUE)\n         ? Config.maxEventsFromKafkaSource : maxEventsToReadFromKafka;\n     long numEvents = sourceLimit == Long.MAX_VALUE ? maxEventsToReadFromKafka : sourceLimit;\n+\n+    if (numEvents < toOffsets.size()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM3MTk5OA=="}, "originalCommit": {"oid": "0699e214ef7d6f112fc9f74c8279e445407617d0"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU2MDgxNQ==", "bodyText": "Interesting! I guess both of you have valid points but am somewhat more inclined towards actually throwing the exception. The reason being throwing this exception will help users detect misconfigs around sourceLimit, for example, if someone sets sourceLimit to very small number in production setup by mistake :).\nAlso would like to hear from @vinothchandar on this :)", "url": "https://github.com/apache/hudi/pull/1652#discussion_r430560815", "createdAt": "2020-05-26T16:48:32Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -207,6 +208,11 @@ public KafkaOffsetGen(TypedProperties props) {\n     maxEventsToReadFromKafka = (maxEventsToReadFromKafka == Long.MAX_VALUE || maxEventsToReadFromKafka == Integer.MAX_VALUE)\n         ? Config.maxEventsFromKafkaSource : maxEventsToReadFromKafka;\n     long numEvents = sourceLimit == Long.MAX_VALUE ? maxEventsToReadFromKafka : sourceLimit;\n+\n+    if (numEvents < toOffsets.size()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM3MTk5OA=="}, "originalCommit": {"oid": "0699e214ef7d6f112fc9f74c8279e445407617d0"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTE3NDc5NQ==", "bodyText": "@garyli1019 maybe adding a warning log to computeOffsetRanges and giving some reminders on the description of --source-limit is also doable, WDYT?\nAfter all, there won't be such situation in the actual prod scene.", "url": "https://github.com/apache/hudi/pull/1652#discussion_r431174795", "createdAt": "2020-05-27T14:26:53Z", "author": {"login": "wangxianghu"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -207,6 +208,11 @@ public KafkaOffsetGen(TypedProperties props) {\n     maxEventsToReadFromKafka = (maxEventsToReadFromKafka == Long.MAX_VALUE || maxEventsToReadFromKafka == Integer.MAX_VALUE)\n         ? Config.maxEventsFromKafkaSource : maxEventsToReadFromKafka;\n     long numEvents = sourceLimit == Long.MAX_VALUE ? maxEventsToReadFromKafka : sourceLimit;\n+\n+    if (numEvents < toOffsets.size()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM3MTk5OA=="}, "originalCommit": {"oid": "0699e214ef7d6f112fc9f74c8279e445407617d0"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI2NzYzNg==", "bodyText": "Thanks for all the feedback here! I don't have a strong opinion here and we have more votes on throwing the exception, so let's go for it. I agree this issue is most likely caused by misconfigs.", "url": "https://github.com/apache/hudi/pull/1652#discussion_r431267636", "createdAt": "2020-05-27T16:12:46Z", "author": {"login": "garyli1019"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -207,6 +208,11 @@ public KafkaOffsetGen(TypedProperties props) {\n     maxEventsToReadFromKafka = (maxEventsToReadFromKafka == Long.MAX_VALUE || maxEventsToReadFromKafka == Integer.MAX_VALUE)\n         ? Config.maxEventsFromKafkaSource : maxEventsToReadFromKafka;\n     long numEvents = sourceLimit == Long.MAX_VALUE ? maxEventsToReadFromKafka : sourceLimit;\n+\n+    if (numEvents < toOffsets.size()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTM3MTk5OA=="}, "originalCommit": {"oid": "0699e214ef7d6f112fc9f74c8279e445407617d0"}, "originalPosition": 13}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4671, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}