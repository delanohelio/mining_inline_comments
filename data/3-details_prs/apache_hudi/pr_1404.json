{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg4MDgxNzI3", "number": 1404, "title": "[HUDI-344] Improve exporter tests", "bodyText": "What is the purpose of the pull request\n\nUse HoodieWriteClient to prepare data for testing\nRe-enabled tests that were commented out\nRemove duplicated test util class\n\nVerify this pull request\nThis change added tests and can be verified as follows:\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-03-14T00:33:51Z", "url": "https://github.com/apache/hudi/pull/1404", "merged": true, "mergeCommit": {"oid": "14323cb10012bdbf80cbb838928af9301cb42ba0"}, "closed": true, "closedAt": "2020-03-15T12:24:31Z", "author": {"login": "xushiyan"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcNZ9MjAFqTM3NDY4MTA3MA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcN4lqRgFqTM3NDc5NDU5Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0NjgxMDcw", "url": "https://github.com/apache/hudi/pull/1404#pullrequestreview-374681070", "createdAt": "2020-03-14T00:39:08Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNFQwMDozOTowOFrOF2WsLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNFQwMDo0MDoyN1rOF2WtBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjUzOTE4MA==", "bodyText": "This was copied over from hudi-spark module. it is not needed when using write client to prepare data", "url": "https://github.com/apache/hudi/pull/1404#discussion_r392539180", "createdAt": "2020-03-14T00:39:08Z", "author": {"login": "xushiyan"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/DataSourceTestUtils.java", "diffHunk": "@@ -1,50 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *      http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hudi.utilities;\n-\n-import org.apache.hudi.common.TestRawTripPayload;\n-import org.apache.hudi.common.model.HoodieRecord;\n-import org.apache.hudi.common.util.Option;\n-\n-import java.io.IOException;\n-import java.util.List;\n-import java.util.stream.Collectors;\n-\n-/**\n- * Test utils for data source tests.\n- */\n-public class DataSourceTestUtils {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjUzOTM5OA==", "bodyText": "Make sure data generated fall into 1 partition for ease of verification", "url": "https://github.com/apache/hudi/pull/1404#discussion_r392539398", "createdAt": "2020-03-14T00:40:27Z", "author": {"login": "xushiyan"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java", "diffHunk": "@@ -18,205 +18,144 @@\n \n package org.apache.hudi.utilities;\n \n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileSystem;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.hadoop.fs.LocatedFileStatus;\n import org.apache.hadoop.fs.Path;\n-import org.apache.hudi.DataSourceWriteOptions;\n-import org.apache.hudi.common.HoodieCommonTestHarness;\n+import org.apache.hadoop.fs.RemoteIterator;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.common.HoodieClientTestHarness;\n import org.apache.hudi.common.HoodieTestDataGenerator;\n-import org.apache.hudi.common.model.HoodieTestUtils;\n-import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.model.HoodieAvroPayload;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.config.HoodieIndexConfig;\n import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.spark.api.java.JavaSparkContext;\n-import org.apache.spark.sql.Dataset;\n-import org.apache.spark.sql.Row;\n-import org.apache.spark.sql.SaveMode;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.apache.hudi.utilities.HoodieSnapshotExporter.Config;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.sql.SparkSession;\n-\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n \n-import java.io.File;\n-import java.io.IOException;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-\n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertTrue;\n+public class TestHoodieSnapshotExporter extends HoodieClientTestHarness {\n \n-public class TestHoodieSnapshotExporter extends HoodieCommonTestHarness {\n-  private static String TEST_WRITE_TOKEN = \"1-0-1\";\n-\n-  private SparkSession spark = null;\n-  private HoodieTestDataGenerator dataGen = null;\n-  private String outputPath = null;\n-  private String rootPath = null;\n-  private FileSystem fs = null;\n-  private Map commonOpts;\n-  private HoodieSnapshotExporter.Config cfg;\n-  private JavaSparkContext jsc = null;\n+  private static final Logger LOG = LogManager.getLogger(TestHoodieSnapshotExporter.class);\n+  private static final int NUM_RECORDS = 100;\n+  private static final String COMMIT_TIME = \"20200101000000\";\n+  private static final String PARTITION_PATH = \"2020/01/01\";\n+  private static final String TABLE_NAME = \"testing\";\n+  private String sourcePath;\n+  private String targetPath;\n \n   @Before\n-  public void initialize() throws IOException {\n-    spark = SparkSession.builder()\n-        .appName(\"Hoodie Datasource test\")\n-        .master(\"local[2]\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n-        .getOrCreate();\n-    jsc = new JavaSparkContext(spark.sparkContext());\n-    dataGen = new HoodieTestDataGenerator();\n-    folder.create();\n-    basePath = folder.getRoot().getAbsolutePath();\n-    fs = FSUtils.getFs(basePath, spark.sparkContext().hadoopConfiguration());\n-    commonOpts = new HashMap();\n-\n-    commonOpts.put(\"hoodie.insert.shuffle.parallelism\", \"4\");\n-    commonOpts.put(\"hoodie.upsert.shuffle.parallelism\", \"4\");\n-    commonOpts.put(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\");\n-    commonOpts.put(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\");\n-    commonOpts.put(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\");\n-    commonOpts.put(HoodieWriteConfig.TABLE_NAME, \"hoodie_test\");\n-\n-\n-    cfg = new HoodieSnapshotExporter.Config();\n-\n-    cfg.sourceBasePath = basePath;\n-    cfg.targetOutputPath = outputPath = basePath + \"/target\";\n-    cfg.outputFormat = \"json\";\n-    cfg.outputPartitionField = \"partition\";\n-\n+  public void setUp() throws Exception {\n+    initSparkContexts();\n+    initDFS();\n+    dataGen = new HoodieTestDataGenerator(new String[]{PARTITION_PATH});", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 107}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0NzMzMDcz", "url": "https://github.com/apache/hudi/pull/1404#pullrequestreview-374733073", "createdAt": "2020-03-14T15:12:39Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNFQxNToxMjozOVrOF2aHjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNFQxNToxMjozOVrOF2aHjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjU5NTM0MA==", "bodyText": "Does '/_SUCCESS' auto generated by hudi HoodieSnapshotExporte? I would only see '/_SUCCESS' in HoodieSnapshotCopier.", "url": "https://github.com/apache/hudi/pull/1404#discussion_r392595340", "createdAt": "2020-03-14T15:12:39Z", "author": {"login": "leesf"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieSnapshotExporter.java", "diffHunk": "@@ -18,205 +18,144 @@\n \n package org.apache.hudi.utilities;\n \n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n-import org.apache.hudi.DataSourceWriteOptions;\n-import org.apache.hudi.common.HoodieCommonTestHarness;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.common.HoodieClientTestHarness;\n import org.apache.hudi.common.HoodieTestDataGenerator;\n-import org.apache.hudi.common.model.HoodieTestUtils;\n-import org.apache.hudi.common.util.FSUtils;\n+import org.apache.hudi.common.model.HoodieAvroPayload;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.config.HoodieIndexConfig;\n import org.apache.hudi.config.HoodieWriteConfig;\n-import org.apache.spark.api.java.JavaSparkContext;\n-import org.apache.spark.sql.Dataset;\n-import org.apache.spark.sql.Row;\n-import org.apache.spark.sql.SaveMode;\n-import org.apache.spark.sql.SparkSession;\n+import org.apache.hudi.index.HoodieIndex.IndexType;\n+import org.apache.hudi.utilities.HoodieSnapshotExporter.Config;\n \n+import org.apache.hadoop.fs.LocatedFileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RemoteIterator;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.sql.SparkSession;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n \n-import java.io.File;\n import java.io.IOException;\n-import java.util.HashMap;\n+import java.util.Arrays;\n import java.util.List;\n-import java.util.Map;\n \n import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n \n-public class TestHoodieSnapshotExporter extends HoodieCommonTestHarness {\n-  private static String TEST_WRITE_TOKEN = \"1-0-1\";\n+public class TestHoodieSnapshotExporter extends HoodieClientTestHarness {\n \n-  private SparkSession spark = null;\n-  private HoodieTestDataGenerator dataGen = null;\n-  private String outputPath = null;\n-  private String rootPath = null;\n-  private FileSystem fs = null;\n-  private Map commonOpts;\n-  private HoodieSnapshotExporter.Config cfg;\n-  private JavaSparkContext jsc = null;\n+  private static final Logger LOG = LogManager.getLogger(TestHoodieSnapshotExporter.class);\n+  private static final int NUM_RECORDS = 100;\n+  private static final String COMMIT_TIME = \"20200101000000\";\n+  private static final String PARTITION_PATH = \"2020/01/01\";\n+  private static final String TABLE_NAME = \"testing\";\n+  private String sourcePath;\n+  private String targetPath;\n \n   @Before\n-  public void initialize() throws IOException {\n-    spark = SparkSession.builder()\n-        .appName(\"Hoodie Datasource test\")\n-        .master(\"local[2]\")\n-        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n-        .getOrCreate();\n-    jsc = new JavaSparkContext(spark.sparkContext());\n-    dataGen = new HoodieTestDataGenerator();\n-    folder.create();\n-    basePath = folder.getRoot().getAbsolutePath();\n-    fs = FSUtils.getFs(basePath, spark.sparkContext().hadoopConfiguration());\n-    commonOpts = new HashMap();\n-\n-    commonOpts.put(\"hoodie.insert.shuffle.parallelism\", \"4\");\n-    commonOpts.put(\"hoodie.upsert.shuffle.parallelism\", \"4\");\n-    commonOpts.put(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY(), \"_row_key\");\n-    commonOpts.put(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY(), \"partition\");\n-    commonOpts.put(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY(), \"timestamp\");\n-    commonOpts.put(HoodieWriteConfig.TABLE_NAME, \"hoodie_test\");\n-\n-\n-    cfg = new HoodieSnapshotExporter.Config();\n-\n-    cfg.sourceBasePath = basePath;\n-    cfg.targetOutputPath = outputPath = basePath + \"/target\";\n-    cfg.outputFormat = \"json\";\n-    cfg.outputPartitionField = \"partition\";\n-\n+  public void setUp() throws Exception {\n+    initSparkContexts();\n+    initDFS();\n+    dataGen = new HoodieTestDataGenerator(new String[] {PARTITION_PATH});\n+\n+    // Initialize test data dirs\n+    sourcePath = dfsBasePath + \"/source/\";\n+    targetPath = dfsBasePath + \"/target/\";\n+    dfs.mkdirs(new Path(sourcePath));\n+    dfs.mkdirs(new Path(targetPath));\n+    HoodieTableMetaClient\n+        .initTableType(jsc.hadoopConfiguration(), sourcePath, HoodieTableType.COPY_ON_WRITE, TABLE_NAME,\n+            HoodieAvroPayload.class.getName());\n+\n+    // Prepare data as source Hudi dataset\n+    HoodieWriteConfig cfg = getHoodieWriteConfig(sourcePath);\n+    HoodieWriteClient hdfsWriteClient = new HoodieWriteClient(jsc, cfg);\n+    hdfsWriteClient.startCommitWithTime(COMMIT_TIME);\n+    List<HoodieRecord> records = dataGen.generateInserts(COMMIT_TIME, NUM_RECORDS);\n+    JavaRDD<HoodieRecord> recordsRDD = jsc.parallelize(records, 1);\n+    hdfsWriteClient.bulkInsert(recordsRDD, COMMIT_TIME);\n+    hdfsWriteClient.close();\n+\n+    RemoteIterator<LocatedFileStatus> itr = dfs.listFiles(new Path(sourcePath), true);\n+    while (itr.hasNext()) {\n+      LOG.info(\">>> Prepared test file: \" + itr.next().getPath());\n+    }\n   }\n \n   @After\n-  public void cleanup() {\n-    if (spark != null) {\n-      spark.stop();\n-    }\n+  public void tearDown() throws Exception {\n+    cleanupSparkContexts();\n+    cleanupDFS();\n+    cleanupTestDataGenerator();\n   }\n \n-  @Test\n-  public void testSnapshotExporter() throws IOException {\n-    // Insert Operation\n-    List<String> records = DataSourceTestUtils.convertToStringList(dataGen.generateInserts(\"000\", 100));\n-    Dataset<Row> inputDF = spark.read().json(new JavaSparkContext(spark.sparkContext()).parallelize(records, 2));\n-    inputDF.write().format(\"hudi\")\n-        .options(commonOpts)\n-        .option(DataSourceWriteOptions.OPERATION_OPT_KEY(), DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL())\n-        .mode(SaveMode.Overwrite)\n-        .save(basePath);\n-    long sourceCount = inputDF.count();\n-\n-    HoodieSnapshotExporter hoodieSnapshotExporter = new HoodieSnapshotExporter();\n-    hoodieSnapshotExporter.export(spark, cfg);\n-\n-    long targetCount = spark.read().json(outputPath).count();\n-\n-    assertTrue(sourceCount == targetCount);\n-\n-    // Test Invalid OutputFormat\n-    cfg.outputFormat = \"foo\";\n-    int isError = hoodieSnapshotExporter.export(spark, cfg);\n-    assertTrue(isError == -1);\n+  private HoodieWriteConfig getHoodieWriteConfig(String basePath) {\n+    return HoodieWriteConfig.newBuilder()\n+        .withPath(basePath)\n+        .withEmbeddedTimelineServerEnabled(false)\n+        .withSchema(HoodieTestDataGenerator.TRIP_EXAMPLE_SCHEMA)\n+        .withParallelism(2, 2)\n+        .withBulkInsertParallelism(2)\n+        .forTable(TABLE_NAME)\n+        .withIndexConfig(HoodieIndexConfig.newBuilder().withIndexType(IndexType.BLOOM).build())\n+        .build();\n   }\n \n-  // for testEmptySnapshotCopy\n-  public void init() throws IOException {\n-    TemporaryFolder folder = new TemporaryFolder();\n-    folder.create();\n-    rootPath = \"file://\" + folder.getRoot().getAbsolutePath();\n-    basePath = rootPath + \"/\" + HoodieTestUtils.RAW_TRIPS_TEST_NAME;\n-    outputPath = rootPath + \"/output\";\n-\n-    final Configuration hadoopConf = HoodieTestUtils.getDefaultHadoopConf();\n-    fs = FSUtils.getFs(basePath, hadoopConf);\n-    HoodieTestUtils.init(hadoopConf, basePath);\n+  @Test\n+  public void testExportAsParquet() throws IOException {\n+    HoodieSnapshotExporter.Config cfg = new Config();\n+    cfg.sourceBasePath = sourcePath;\n+    cfg.targetOutputPath = targetPath;\n+    cfg.outputFormat = \"parquet\";\n+    new HoodieSnapshotExporter().export(SparkSession.builder().config(jsc.getConf()).getOrCreate(), cfg);\n+    assertEquals(NUM_RECORDS, sqlContext.read().parquet(targetPath).count());\n+    assertTrue(dfs.exists(new Path(targetPath + \"/_SUCCESS\")));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 194}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0NzMzMTUx", "url": "https://github.com/apache/hudi/pull/1404#pullrequestreview-374733151", "createdAt": "2020-03-14T15:14:00Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5dc21d6855b85683ba680f90e9409e289a8142e2", "author": {"user": {"login": "xushiyan", "name": "Raymond Xu"}}, "url": "https://github.com/apache/hudi/commit/5dc21d6855b85683ba680f90e9409e289a8142e2", "committedDate": "2020-03-14T17:49:09Z", "message": "[HUDI-344] Improve exporter tests\n\n* Use HoodieWriteClient to prepare data for testing\n* Re-enabled tests that were commented out\n* Remove duplicated test util class"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "ca7df189c88c4c851a7077c497673e9a703678b3", "author": {"user": {"login": "xushiyan", "name": "Raymond Xu"}}, "url": "https://github.com/apache/hudi/commit/ca7df189c88c4c851a7077c497673e9a703678b3", "committedDate": "2020-03-14T21:18:37Z", "message": "[HUDI-344] Parameterize testcases"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "ca7df189c88c4c851a7077c497673e9a703678b3", "author": {"user": {"login": "xushiyan", "name": "Raymond Xu"}}, "url": "https://github.com/apache/hudi/commit/ca7df189c88c4c851a7077c497673e9a703678b3", "committedDate": "2020-03-14T21:18:37Z", "message": "[HUDI-344] Parameterize testcases"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0Nzk0NTk3", "url": "https://github.com/apache/hudi/pull/1404#pullrequestreview-374794597", "createdAt": "2020-03-15T12:22:55Z", "commit": {"oid": "ca7df189c88c4c851a7077c497673e9a703678b3"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3870, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}