{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEzMTgzMTgy", "number": 1592, "title": "[HUDI-822] decouple Hudi related logics from HoodieInputFormat", "bodyText": "What is the purpose of the pull request\nThis PR is to decouple Hudi related functions from Hive related InputFormat, RecordReader e.t.c\nRFC: https://cwiki.apache.org/confluence/display/HUDI/RFC+-+16+Abstraction+for+HoodieInputFormat+and+RecordReader\nJIRA: https://issues.apache.org/jira/browse/HUDI-822\nBrief change log\n\nMove hudi related methods from InputFormat to Util classes.\n\nVerify this pull request\nThis change added tests and can be verified as follows:\n\nAlready covered by the existing tests\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-05-04T21:02:22Z", "url": "https://github.com/apache/hudi/pull/1592", "merged": true, "mergeCommit": {"oid": "37838cea6094ddc66191df42e8b2c84f132d1623"}, "closed": true, "closedAt": "2020-06-09T13:10:16Z", "author": {"login": "garyli1019"}, "timelineItems": {"totalCount": 29, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABceLlZmgBqjMzMDI0NjkzMzg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcpkx6wgFqTQyNzEyNDU5MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "525f36047d7e2ab38c67ad47531420eb6e707f12", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/525f36047d7e2ab38c67ad47531420eb6e707f12", "committedDate": "2020-05-04T20:59:45Z", "message": "HUDI-69 Spark datasource support for MOR table"}, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIxMzg3MDMw", "url": "https://github.com/apache/hudi/pull/1592#pullrequestreview-421387030", "createdAt": "2020-05-30T07:51:48Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQwNzo1MTo0OFrOGcxRMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQwNzo1MTo0OFrOGcxRMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgyMDUyOA==", "bodyText": "Does it more proper to a new Constant class to hold constants in HoodieRealtimeInputFormatUtils and HoodieRealtimeRecordReaderUtils, wdyt?", "url": "https://github.com/apache/hudi/pull/1592#discussion_r432820528", "createdAt": "2020-05-30T07:51:48Z", "author": {"login": "leesf"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/realtime/HoodieParquetRealtimeInputFormat.java", "diffHunk": "@@ -199,9 +110,9 @@ private static Configuration addProjectionField(Configuration conf, String field\n \n   private static void addRequiredProjectionFields(Configuration configuration) {\n     // Need this to do merge records in HoodieRealtimeRecordReader\n-    addProjectionField(configuration, HoodieRecord.RECORD_KEY_METADATA_FIELD, HOODIE_RECORD_KEY_COL_POS);\n-    addProjectionField(configuration, HoodieRecord.COMMIT_TIME_METADATA_FIELD, HOODIE_COMMIT_TIME_COL_POS);\n-    addProjectionField(configuration, HoodieRecord.PARTITION_PATH_METADATA_FIELD, HOODIE_PARTITION_PATH_COL_POS);\n+    addProjectionField(configuration, HoodieRecord.RECORD_KEY_METADATA_FIELD, HoodieRealtimeInputFormatUtils.HOODIE_RECORD_KEY_COL_POS);\n+    addProjectionField(configuration, HoodieRecord.COMMIT_TIME_METADATA_FIELD, HoodieRealtimeInputFormatUtils.HOODIE_COMMIT_TIME_COL_POS);\n+    addProjectionField(configuration, HoodieRecord.PARTITION_PATH_METADATA_FIELD, HoodieRealtimeInputFormatUtils.HOODIE_PARTITION_PATH_COL_POS);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 140}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIxMzg3MjY2", "url": "https://github.com/apache/hudi/pull/1592#pullrequestreview-421387266", "createdAt": "2020-05-30T07:56:26Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQwNzo1NjoyNlrOGcxSLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQwNzo1NjoyNlrOGcxSLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjgyMDc4Mw==", "bodyText": "could we move some methods like readSchema, arrayWritableToString, generateProjectionSchema, getNameToFieldMap, avroToArrayWritable etc in AbstractRealtimeRecordReader to this class?", "url": "https://github.com/apache/hudi/pull/1592#discussion_r432820783", "createdAt": "2020-05-30T07:56:26Z", "author": {"login": "leesf"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeRecordReaderUtils.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+/**\n+ * Class to hold HoodieRealtimeRecordReader related props.\n+ */\n+public final class HoodieRealtimeRecordReaderUtils {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIxMzg3MzA0", "url": "https://github.com/apache/hudi/pull/1592#pullrequestreview-421387304", "createdAt": "2020-05-30T07:57:03Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIxNDQ3MDA2", "url": "https://github.com/apache/hudi/pull/1592#pullrequestreview-421447006", "createdAt": "2020-05-30T22:09:07Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0zMFQyMjowOTowN1rOGc1k5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwMjozOTo0NlrOGdgNsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5MTEwOQ==", "bodyText": "Calling this config is a bit misleading.. these are just constants", "url": "https://github.com/apache/hudi/pull/1592#discussion_r432891109", "createdAt": "2020-05-30T22:09:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/config/HoodieRealtimeConfig.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.config;\n+\n+/**\n+ * Class to hold props related to Hoodie RealtimeInputFormat and RealtimeRecordReader.\n+ */\n+public final class HoodieRealtimeConfig {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjg5MTI0Mg==", "bodyText": "These do seem like configs. So just leave these out in a separate file? And move lines 25-28 back to the input format?", "url": "https://github.com/apache/hudi/pull/1592#discussion_r432891242", "createdAt": "2020-05-30T22:11:58Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/config/HoodieRealtimeConfig.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.config;\n+\n+/**\n+ * Class to hold props related to Hoodie RealtimeInputFormat and RealtimeRecordReader.\n+ */\n+public final class HoodieRealtimeConfig {\n+  // These positions have to be deterministic across all tables\n+  public static final int HOODIE_COMMIT_TIME_COL_POS = 0;\n+  public static final int HOODIE_RECORD_KEY_COL_POS = 2;\n+  public static final int HOODIE_PARTITION_PATH_COL_POS = 3;\n+  public static final String HOODIE_READ_COLUMNS_PROP = \"hoodie.read.columns.set\";\n+\n+  // Fraction of mapper/reducer task memory used for compaction of log files\n+  public static final String COMPACTION_MEMORY_FRACTION_PROP = \"compaction.memory.fraction\";\n+  public static final String DEFAULT_COMPACTION_MEMORY_FRACTION = \"0.75\";\n+  // used to choose a trade off between IO vs Memory when performing compaction process\n+  // Depending on outputfile size and memory provided, choose true to avoid OOM for large file\n+  // size + small memory\n+  public static final String COMPACTION_LAZY_BLOCK_READ_ENABLED_PROP = \"compaction.lazy.block.read.enabled\";\n+  public static final String DEFAULT_COMPACTION_LAZY_BLOCK_READ_ENABLED = \"true\";\n+\n+  // Property to set the max memory for dfs inputstream buffer size\n+  public static final String MAX_DFS_STREAM_BUFFER_SIZE_PROP = \"hoodie.memory.dfs.buffer.max.size\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4Nzg0Ng==", "bodyText": "rename: getCommitsForIncrementalQuery() to give more context.", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433587846", "createdAt": "2020-06-02T02:31:41Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.table.view.TableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+public class HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieInputFormatUtils.class);\n+\n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   * @param timeline\n+   * @return\n+   */\n+  public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n+    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline\n+        .filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {\n+      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline\n+          .findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n+      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n+          - instantsTimeline.getCommitsTimeline().countInstants();\n+      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n+          + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n+\n+      return instantsTimeline;\n+    } else {\n+      return timeline;\n+    }\n+  }\n+\n+  /**\n+   * Extract partitions touched by the commitsToCheck.\n+   * @param commitsToCheck\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param inputPaths\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsToCheck,\n+                                      HoodieTableMetaClient tableMetaClient,\n+                                      HoodieTimeline timeline,\n+                                      List<Path> inputPaths) throws IOException {\n+    Set<String> partitionsToList = new HashSet<>();\n+    for (HoodieInstant commit : commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n+    }\n+    if (partitionsToList.isEmpty()) {\n+      return Option.empty();\n+    }\n+    String incrementalInputPaths = partitionsToList.stream()\n+        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n+        .filter(s -> {\n+          /*\n+           * Ensure to return only results from the original input path that has incremental changes\n+           * This check is needed for the following corner case -  When the caller invokes\n+           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n+           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n+           * accidentally return all incremental changes for the entire table in every listStatus()\n+           * call. This will create redundant splits. Instead we only want to return the incremental\n+           * changes (if so any) in that batch of input paths.\n+           *\n+           * NOTE on Hive queries that are executed using Fetch task:\n+           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n+           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n+           * disable fetch tasks using the hive session property for incremental queries:\n+           * `set hive.fetch.task.conversion=none;`\n+           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n+           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n+           * those partitions.\n+           */\n+          for (Path path : inputPaths) {\n+            if (path.toString().contains(s)) {\n+              return true;\n+            }\n+          }\n+          return false;\n+        })\n+        .collect(Collectors.joining(\",\"));\n+    return Option.of(incrementalInputPaths);\n+  }\n+\n+  /**\n+   * Extract HoodieTimeline based on HoodieTableMetaClient.\n+   * @param job\n+   * @param tableMetaClient\n+   * @return\n+   */\n+  public static Option<HoodieTimeline> getTimeline(Job job, HoodieTableMetaClient tableMetaClient) {\n+    String tableName = tableMetaClient.getTableConfig().getTableName();\n+    HoodieDefaultTimeline baseTimeline;\n+    if (HoodieHiveUtils.stopAtCompaction(job, tableName)) {\n+      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n+    } else {\n+      baseTimeline = tableMetaClient.getActiveTimeline();\n+    }\n+    return Option.of(baseTimeline.getCommitsTimeline().filterCompletedInstants());\n+  }\n+\n+  /**\n+   * Get commits to check from Hive map reduce configuration.\n+   * @param job\n+   * @param tableName\n+   * @param timeline\n+   * @return\n+   */\n+  public static Option<List<HoodieInstant>> getCommitsToCheck(Job job, String tableName, HoodieTimeline timeline) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4ODA3Mg==", "bodyText": "rename : getFilteredCommitsTimeline() to provide more context as well, now that the code has been moved out of the original source file.", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433588072", "createdAt": "2020-06-02T02:32:29Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.table.view.TableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+public class HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieInputFormatUtils.class);\n+\n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   * @param timeline\n+   * @return\n+   */\n+  public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n+    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline\n+        .filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {\n+      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline\n+          .findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n+      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n+          - instantsTimeline.getCommitsTimeline().countInstants();\n+      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n+          + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n+\n+      return instantsTimeline;\n+    } else {\n+      return timeline;\n+    }\n+  }\n+\n+  /**\n+   * Extract partitions touched by the commitsToCheck.\n+   * @param commitsToCheck\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param inputPaths\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsToCheck,\n+                                      HoodieTableMetaClient tableMetaClient,\n+                                      HoodieTimeline timeline,\n+                                      List<Path> inputPaths) throws IOException {\n+    Set<String> partitionsToList = new HashSet<>();\n+    for (HoodieInstant commit : commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n+    }\n+    if (partitionsToList.isEmpty()) {\n+      return Option.empty();\n+    }\n+    String incrementalInputPaths = partitionsToList.stream()\n+        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n+        .filter(s -> {\n+          /*\n+           * Ensure to return only results from the original input path that has incremental changes\n+           * This check is needed for the following corner case -  When the caller invokes\n+           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n+           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n+           * accidentally return all incremental changes for the entire table in every listStatus()\n+           * call. This will create redundant splits. Instead we only want to return the incremental\n+           * changes (if so any) in that batch of input paths.\n+           *\n+           * NOTE on Hive queries that are executed using Fetch task:\n+           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n+           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n+           * disable fetch tasks using the hive session property for incremental queries:\n+           * `set hive.fetch.task.conversion=none;`\n+           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n+           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n+           * those partitions.\n+           */\n+          for (Path path : inputPaths) {\n+            if (path.toString().contains(s)) {\n+              return true;\n+            }\n+          }\n+          return false;\n+        })\n+        .collect(Collectors.joining(\",\"));\n+    return Option.of(incrementalInputPaths);\n+  }\n+\n+  /**\n+   * Extract HoodieTimeline based on HoodieTableMetaClient.\n+   * @param job\n+   * @param tableMetaClient\n+   * @return\n+   */\n+  public static Option<HoodieTimeline> getTimeline(Job job, HoodieTableMetaClient tableMetaClient) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4ODMyNw==", "bodyText": "rename: getMetaClientByBasePath() which is whatthis really is", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433588327", "createdAt": "2020-06-02T02:33:37Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.table.view.TableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+public class HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieInputFormatUtils.class);\n+\n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   * @param timeline\n+   * @return\n+   */\n+  public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n+    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline\n+        .filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {\n+      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline\n+          .findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n+      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n+          - instantsTimeline.getCommitsTimeline().countInstants();\n+      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n+          + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n+\n+      return instantsTimeline;\n+    } else {\n+      return timeline;\n+    }\n+  }\n+\n+  /**\n+   * Extract partitions touched by the commitsToCheck.\n+   * @param commitsToCheck\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param inputPaths\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsToCheck,\n+                                      HoodieTableMetaClient tableMetaClient,\n+                                      HoodieTimeline timeline,\n+                                      List<Path> inputPaths) throws IOException {\n+    Set<String> partitionsToList = new HashSet<>();\n+    for (HoodieInstant commit : commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n+    }\n+    if (partitionsToList.isEmpty()) {\n+      return Option.empty();\n+    }\n+    String incrementalInputPaths = partitionsToList.stream()\n+        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n+        .filter(s -> {\n+          /*\n+           * Ensure to return only results from the original input path that has incremental changes\n+           * This check is needed for the following corner case -  When the caller invokes\n+           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n+           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n+           * accidentally return all incremental changes for the entire table in every listStatus()\n+           * call. This will create redundant splits. Instead we only want to return the incremental\n+           * changes (if so any) in that batch of input paths.\n+           *\n+           * NOTE on Hive queries that are executed using Fetch task:\n+           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n+           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n+           * disable fetch tasks using the hive session property for incremental queries:\n+           * `set hive.fetch.task.conversion=none;`\n+           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n+           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n+           * those partitions.\n+           */\n+          for (Path path : inputPaths) {\n+            if (path.toString().contains(s)) {\n+              return true;\n+            }\n+          }\n+          return false;\n+        })\n+        .collect(Collectors.joining(\",\"));\n+    return Option.of(incrementalInputPaths);\n+  }\n+\n+  /**\n+   * Extract HoodieTimeline based on HoodieTableMetaClient.\n+   * @param job\n+   * @param tableMetaClient\n+   * @return\n+   */\n+  public static Option<HoodieTimeline> getTimeline(Job job, HoodieTableMetaClient tableMetaClient) {\n+    String tableName = tableMetaClient.getTableConfig().getTableName();\n+    HoodieDefaultTimeline baseTimeline;\n+    if (HoodieHiveUtils.stopAtCompaction(job, tableName)) {\n+      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n+    } else {\n+      baseTimeline = tableMetaClient.getActiveTimeline();\n+    }\n+    return Option.of(baseTimeline.getCommitsTimeline().filterCompletedInstants());\n+  }\n+\n+  /**\n+   * Get commits to check from Hive map reduce configuration.\n+   * @param job\n+   * @param tableName\n+   * @param timeline\n+   * @return\n+   */\n+  public static Option<List<HoodieInstant>> getCommitsToCheck(Job job, String tableName, HoodieTimeline timeline) {\n+    String lastIncrementalTs = HoodieHiveUtils.readStartCommitTime(job, tableName);\n+    // Total number of commits to return in this batch. Set this to -1 to get all the commits.\n+    Integer maxCommits = HoodieHiveUtils.readMaxCommits(job, tableName);\n+    LOG.info(\"Last Incremental timestamp was set as \" + lastIncrementalTs);\n+    return Option.of(timeline.findInstantsAfter(lastIncrementalTs, maxCommits)\n+        .getInstants().collect(Collectors.toList()));\n+  }\n+\n+  /**\n+   * Generate a HoodieTableMetaClient for a given partition.\n+   * @param conf\n+   * @param partitions\n+   * @return\n+   */\n+  public static Map<Path, HoodieTableMetaClient> getMetaClientPerPartition(Configuration conf, Set<Path> partitions) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4ODQxOQ==", "bodyText": "this comment is wrong.", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433588419", "createdAt": "2020-06-02T02:34:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.table.view.TableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+public class HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieInputFormatUtils.class);\n+\n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   * @param timeline\n+   * @return\n+   */\n+  public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n+    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline\n+        .filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {\n+      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline\n+          .findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n+      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n+          - instantsTimeline.getCommitsTimeline().countInstants();\n+      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n+          + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n+\n+      return instantsTimeline;\n+    } else {\n+      return timeline;\n+    }\n+  }\n+\n+  /**\n+   * Extract partitions touched by the commitsToCheck.\n+   * @param commitsToCheck\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param inputPaths\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsToCheck,\n+                                      HoodieTableMetaClient tableMetaClient,\n+                                      HoodieTimeline timeline,\n+                                      List<Path> inputPaths) throws IOException {\n+    Set<String> partitionsToList = new HashSet<>();\n+    for (HoodieInstant commit : commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n+    }\n+    if (partitionsToList.isEmpty()) {\n+      return Option.empty();\n+    }\n+    String incrementalInputPaths = partitionsToList.stream()\n+        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n+        .filter(s -> {\n+          /*\n+           * Ensure to return only results from the original input path that has incremental changes\n+           * This check is needed for the following corner case -  When the caller invokes\n+           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n+           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n+           * accidentally return all incremental changes for the entire table in every listStatus()\n+           * call. This will create redundant splits. Instead we only want to return the incremental\n+           * changes (if so any) in that batch of input paths.\n+           *\n+           * NOTE on Hive queries that are executed using Fetch task:\n+           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n+           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n+           * disable fetch tasks using the hive session property for incremental queries:\n+           * `set hive.fetch.task.conversion=none;`\n+           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n+           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n+           * those partitions.\n+           */\n+          for (Path path : inputPaths) {\n+            if (path.toString().contains(s)) {\n+              return true;\n+            }\n+          }\n+          return false;\n+        })\n+        .collect(Collectors.joining(\",\"));\n+    return Option.of(incrementalInputPaths);\n+  }\n+\n+  /**\n+   * Extract HoodieTimeline based on HoodieTableMetaClient.\n+   * @param job\n+   * @param tableMetaClient\n+   * @return\n+   */\n+  public static Option<HoodieTimeline> getTimeline(Job job, HoodieTableMetaClient tableMetaClient) {\n+    String tableName = tableMetaClient.getTableConfig().getTableName();\n+    HoodieDefaultTimeline baseTimeline;\n+    if (HoodieHiveUtils.stopAtCompaction(job, tableName)) {\n+      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n+    } else {\n+      baseTimeline = tableMetaClient.getActiveTimeline();\n+    }\n+    return Option.of(baseTimeline.getCommitsTimeline().filterCompletedInstants());\n+  }\n+\n+  /**\n+   * Get commits to check from Hive map reduce configuration.\n+   * @param job\n+   * @param tableName\n+   * @param timeline\n+   * @return\n+   */\n+  public static Option<List<HoodieInstant>> getCommitsToCheck(Job job, String tableName, HoodieTimeline timeline) {\n+    String lastIncrementalTs = HoodieHiveUtils.readStartCommitTime(job, tableName);\n+    // Total number of commits to return in this batch. Set this to -1 to get all the commits.\n+    Integer maxCommits = HoodieHiveUtils.readMaxCommits(job, tableName);\n+    LOG.info(\"Last Incremental timestamp was set as \" + lastIncrementalTs);\n+    return Option.of(timeline.findInstantsAfter(lastIncrementalTs, maxCommits)\n+        .getInstants().collect(Collectors.toList()));\n+  }\n+\n+  /**\n+   * Generate a HoodieTableMetaClient for a given partition.\n+   * @param conf\n+   * @param partitions\n+   * @return\n+   */\n+  public static Map<Path, HoodieTableMetaClient> getMetaClientPerPartition(Configuration conf, Set<Path> partitions) {\n+    Map<String, HoodieTableMetaClient> metaClientMap = new HashMap<>();\n+    return partitions.stream().collect(Collectors.toMap(Function.identity(), p -> {\n+      // find if we have a metaclient already for this partition.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4ODUzOA==", "bodyText": "rename: getTableMetaClientForBasePath()", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433588538", "createdAt": "2020-06-02T02:34:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.table.view.TableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+public class HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieInputFormatUtils.class);\n+\n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   * @param timeline\n+   * @return\n+   */\n+  public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n+    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline\n+        .filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {\n+      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline\n+          .findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n+      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n+          - instantsTimeline.getCommitsTimeline().countInstants();\n+      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n+          + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n+\n+      return instantsTimeline;\n+    } else {\n+      return timeline;\n+    }\n+  }\n+\n+  /**\n+   * Extract partitions touched by the commitsToCheck.\n+   * @param commitsToCheck\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param inputPaths\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsToCheck,\n+                                      HoodieTableMetaClient tableMetaClient,\n+                                      HoodieTimeline timeline,\n+                                      List<Path> inputPaths) throws IOException {\n+    Set<String> partitionsToList = new HashSet<>();\n+    for (HoodieInstant commit : commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n+    }\n+    if (partitionsToList.isEmpty()) {\n+      return Option.empty();\n+    }\n+    String incrementalInputPaths = partitionsToList.stream()\n+        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n+        .filter(s -> {\n+          /*\n+           * Ensure to return only results from the original input path that has incremental changes\n+           * This check is needed for the following corner case -  When the caller invokes\n+           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n+           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n+           * accidentally return all incremental changes for the entire table in every listStatus()\n+           * call. This will create redundant splits. Instead we only want to return the incremental\n+           * changes (if so any) in that batch of input paths.\n+           *\n+           * NOTE on Hive queries that are executed using Fetch task:\n+           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n+           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n+           * disable fetch tasks using the hive session property for incremental queries:\n+           * `set hive.fetch.task.conversion=none;`\n+           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n+           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n+           * those partitions.\n+           */\n+          for (Path path : inputPaths) {\n+            if (path.toString().contains(s)) {\n+              return true;\n+            }\n+          }\n+          return false;\n+        })\n+        .collect(Collectors.joining(\",\"));\n+    return Option.of(incrementalInputPaths);\n+  }\n+\n+  /**\n+   * Extract HoodieTimeline based on HoodieTableMetaClient.\n+   * @param job\n+   * @param tableMetaClient\n+   * @return\n+   */\n+  public static Option<HoodieTimeline> getTimeline(Job job, HoodieTableMetaClient tableMetaClient) {\n+    String tableName = tableMetaClient.getTableConfig().getTableName();\n+    HoodieDefaultTimeline baseTimeline;\n+    if (HoodieHiveUtils.stopAtCompaction(job, tableName)) {\n+      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n+    } else {\n+      baseTimeline = tableMetaClient.getActiveTimeline();\n+    }\n+    return Option.of(baseTimeline.getCommitsTimeline().filterCompletedInstants());\n+  }\n+\n+  /**\n+   * Get commits to check from Hive map reduce configuration.\n+   * @param job\n+   * @param tableName\n+   * @param timeline\n+   * @return\n+   */\n+  public static Option<List<HoodieInstant>> getCommitsToCheck(Job job, String tableName, HoodieTimeline timeline) {\n+    String lastIncrementalTs = HoodieHiveUtils.readStartCommitTime(job, tableName);\n+    // Total number of commits to return in this batch. Set this to -1 to get all the commits.\n+    Integer maxCommits = HoodieHiveUtils.readMaxCommits(job, tableName);\n+    LOG.info(\"Last Incremental timestamp was set as \" + lastIncrementalTs);\n+    return Option.of(timeline.findInstantsAfter(lastIncrementalTs, maxCommits)\n+        .getInstants().collect(Collectors.toList()));\n+  }\n+\n+  /**\n+   * Generate a HoodieTableMetaClient for a given partition.\n+   * @param conf\n+   * @param partitions\n+   * @return\n+   */\n+  public static Map<Path, HoodieTableMetaClient> getMetaClientPerPartition(Configuration conf, Set<Path> partitions) {\n+    Map<String, HoodieTableMetaClient> metaClientMap = new HashMap<>();\n+    return partitions.stream().collect(Collectors.toMap(Function.identity(), p -> {\n+      // find if we have a metaclient already for this partition.\n+      Option<String> matchingBasePath = Option.fromJavaOptional(\n+          metaClientMap.keySet().stream().filter(basePath -> p.toString().startsWith(basePath)).findFirst());\n+      if (matchingBasePath.isPresent()) {\n+        return metaClientMap.get(matchingBasePath.get());\n+      }\n+\n+      try {\n+        HoodieTableMetaClient metaClient = getTableMetaClient(p.getFileSystem(conf), p);\n+        metaClientMap.put(metaClient.getBasePath(), metaClient);\n+        return metaClient;\n+      } catch (IOException e) {\n+        throw new HoodieIOException(\"Error creating hoodie meta client against : \" + p, e);\n+      }\n+    }));\n+  }\n+\n+  /**\n+   * Extract HoodieTableMetaClient from a partition path(not base path).\n+   * @param fs\n+   * @param dataPath\n+   * @return\n+   * @throws IOException\n+   */\n+  public static HoodieTableMetaClient getTableMetaClient(FileSystem fs, Path dataPath) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4ODYyNQ==", "bodyText": "optional: consistent folding style for args?", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433588625", "createdAt": "2020-06-02T02:35:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.table.view.TableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+public class HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieInputFormatUtils.class);\n+\n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   * @param timeline\n+   * @return\n+   */\n+  public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n+    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline\n+        .filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {\n+      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline\n+          .findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n+      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n+          - instantsTimeline.getCommitsTimeline().countInstants();\n+      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n+          + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n+\n+      return instantsTimeline;\n+    } else {\n+      return timeline;\n+    }\n+  }\n+\n+  /**\n+   * Extract partitions touched by the commitsToCheck.\n+   * @param commitsToCheck\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param inputPaths\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsToCheck,\n+                                      HoodieTableMetaClient tableMetaClient,\n+                                      HoodieTimeline timeline,\n+                                      List<Path> inputPaths) throws IOException {\n+    Set<String> partitionsToList = new HashSet<>();\n+    for (HoodieInstant commit : commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n+    }\n+    if (partitionsToList.isEmpty()) {\n+      return Option.empty();\n+    }\n+    String incrementalInputPaths = partitionsToList.stream()\n+        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n+        .filter(s -> {\n+          /*\n+           * Ensure to return only results from the original input path that has incremental changes\n+           * This check is needed for the following corner case -  When the caller invokes\n+           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n+           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n+           * accidentally return all incremental changes for the entire table in every listStatus()\n+           * call. This will create redundant splits. Instead we only want to return the incremental\n+           * changes (if so any) in that batch of input paths.\n+           *\n+           * NOTE on Hive queries that are executed using Fetch task:\n+           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n+           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n+           * disable fetch tasks using the hive session property for incremental queries:\n+           * `set hive.fetch.task.conversion=none;`\n+           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n+           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n+           * those partitions.\n+           */\n+          for (Path path : inputPaths) {\n+            if (path.toString().contains(s)) {\n+              return true;\n+            }\n+          }\n+          return false;\n+        })\n+        .collect(Collectors.joining(\",\"));\n+    return Option.of(incrementalInputPaths);\n+  }\n+\n+  /**\n+   * Extract HoodieTimeline based on HoodieTableMetaClient.\n+   * @param job\n+   * @param tableMetaClient\n+   * @return\n+   */\n+  public static Option<HoodieTimeline> getTimeline(Job job, HoodieTableMetaClient tableMetaClient) {\n+    String tableName = tableMetaClient.getTableConfig().getTableName();\n+    HoodieDefaultTimeline baseTimeline;\n+    if (HoodieHiveUtils.stopAtCompaction(job, tableName)) {\n+      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n+    } else {\n+      baseTimeline = tableMetaClient.getActiveTimeline();\n+    }\n+    return Option.of(baseTimeline.getCommitsTimeline().filterCompletedInstants());\n+  }\n+\n+  /**\n+   * Get commits to check from Hive map reduce configuration.\n+   * @param job\n+   * @param tableName\n+   * @param timeline\n+   * @return\n+   */\n+  public static Option<List<HoodieInstant>> getCommitsToCheck(Job job, String tableName, HoodieTimeline timeline) {\n+    String lastIncrementalTs = HoodieHiveUtils.readStartCommitTime(job, tableName);\n+    // Total number of commits to return in this batch. Set this to -1 to get all the commits.\n+    Integer maxCommits = HoodieHiveUtils.readMaxCommits(job, tableName);\n+    LOG.info(\"Last Incremental timestamp was set as \" + lastIncrementalTs);\n+    return Option.of(timeline.findInstantsAfter(lastIncrementalTs, maxCommits)\n+        .getInstants().collect(Collectors.toList()));\n+  }\n+\n+  /**\n+   * Generate a HoodieTableMetaClient for a given partition.\n+   * @param conf\n+   * @param partitions\n+   * @return\n+   */\n+  public static Map<Path, HoodieTableMetaClient> getMetaClientPerPartition(Configuration conf, Set<Path> partitions) {\n+    Map<String, HoodieTableMetaClient> metaClientMap = new HashMap<>();\n+    return partitions.stream().collect(Collectors.toMap(Function.identity(), p -> {\n+      // find if we have a metaclient already for this partition.\n+      Option<String> matchingBasePath = Option.fromJavaOptional(\n+          metaClientMap.keySet().stream().filter(basePath -> p.toString().startsWith(basePath)).findFirst());\n+      if (matchingBasePath.isPresent()) {\n+        return metaClientMap.get(matchingBasePath.get());\n+      }\n+\n+      try {\n+        HoodieTableMetaClient metaClient = getTableMetaClient(p.getFileSystem(conf), p);\n+        metaClientMap.put(metaClient.getBasePath(), metaClient);\n+        return metaClient;\n+      } catch (IOException e) {\n+        throw new HoodieIOException(\"Error creating hoodie meta client against : \" + p, e);\n+      }\n+    }));\n+  }\n+\n+  /**\n+   * Extract HoodieTableMetaClient from a partition path(not base path).\n+   * @param fs\n+   * @param dataPath\n+   * @return\n+   * @throws IOException\n+   */\n+  public static HoodieTableMetaClient getTableMetaClient(FileSystem fs, Path dataPath) throws IOException {\n+    int levels = HoodieHiveUtils.DEFAULT_LEVELS_TO_BASEPATH;\n+    if (HoodiePartitionMetadata.hasPartitionMetadata(fs, dataPath)) {\n+      HoodiePartitionMetadata metadata = new HoodiePartitionMetadata(fs, dataPath);\n+      metadata.readFromFS();\n+      levels = metadata.getPartitionDepth();\n+    }\n+    Path baseDir = HoodieHiveUtils.getNthParent(dataPath, levels);\n+    LOG.info(\"Reading hoodie metadata from path \" + baseDir.toString());\n+    return new HoodieTableMetaClient(fs.getConf(), baseDir.toString());\n+  }\n+\n+  /**\n+   * Filter a list of FileStatus based on commitsToCheck for incremental view.\n+   * @param job\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param fileStatuses\n+   * @param commitsToCheck\n+   * @return\n+   */\n+  public static List<FileStatus> filterIncrementalFileStatus(\n+      Job job, HoodieTableMetaClient tableMetaClient, HoodieTimeline timeline,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 233}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4OTA4NA==", "bodyText": "rename: refreshFileStatus", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433589084", "createdAt": "2020-06-02T02:37:12Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieDefaultTimeline;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.table.view.TableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+\n+public class HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieInputFormatUtils.class);\n+\n+  /**\n+   * Filter any specific instants that we do not want to process.\n+   * example timeline:\n+   *\n+   * t0 -> create bucket1.parquet\n+   * t1 -> create and append updates bucket1.log\n+   * t2 -> request compaction\n+   * t3 -> create bucket2.parquet\n+   *\n+   * if compaction at t2 takes a long time, incremental readers on RO tables can move to t3 and would skip updates in t1\n+   *\n+   * To workaround this problem, we want to stop returning data belonging to commits > t2.\n+   * After compaction is complete, incremental reader would see updates in t2, t3, so on.\n+   * @param timeline\n+   * @return\n+   */\n+  public static HoodieDefaultTimeline filterInstantsTimeline(HoodieDefaultTimeline timeline) {\n+    HoodieDefaultTimeline commitsAndCompactionTimeline = timeline.getCommitsAndCompactionTimeline();\n+    Option<HoodieInstant> pendingCompactionInstant = commitsAndCompactionTimeline\n+        .filterPendingCompactionTimeline().firstInstant();\n+    if (pendingCompactionInstant.isPresent()) {\n+      HoodieDefaultTimeline instantsTimeline = commitsAndCompactionTimeline\n+          .findInstantsBefore(pendingCompactionInstant.get().getTimestamp());\n+      int numCommitsFilteredByCompaction = commitsAndCompactionTimeline.getCommitsTimeline().countInstants()\n+          - instantsTimeline.getCommitsTimeline().countInstants();\n+      LOG.info(\"Earliest pending compaction instant is: \" + pendingCompactionInstant.get().getTimestamp()\n+          + \" skipping \" + numCommitsFilteredByCompaction + \" commits\");\n+\n+      return instantsTimeline;\n+    } else {\n+      return timeline;\n+    }\n+  }\n+\n+  /**\n+   * Extract partitions touched by the commitsToCheck.\n+   * @param commitsToCheck\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param inputPaths\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Option<String> getAffectedPartitions(List<HoodieInstant> commitsToCheck,\n+                                      HoodieTableMetaClient tableMetaClient,\n+                                      HoodieTimeline timeline,\n+                                      List<Path> inputPaths) throws IOException {\n+    Set<String> partitionsToList = new HashSet<>();\n+    for (HoodieInstant commit : commitsToCheck) {\n+      HoodieCommitMetadata commitMetadata = HoodieCommitMetadata.fromBytes(timeline.getInstantDetails(commit).get(),\n+          HoodieCommitMetadata.class);\n+      partitionsToList.addAll(commitMetadata.getPartitionToWriteStats().keySet());\n+    }\n+    if (partitionsToList.isEmpty()) {\n+      return Option.empty();\n+    }\n+    String incrementalInputPaths = partitionsToList.stream()\n+        .map(s -> tableMetaClient.getBasePath() + Path.SEPARATOR + s)\n+        .filter(s -> {\n+          /*\n+           * Ensure to return only results from the original input path that has incremental changes\n+           * This check is needed for the following corner case -  When the caller invokes\n+           * HoodieInputFormat.listStatus multiple times (with small batches of Hive partitions each\n+           * time. Ex. Hive fetch task calls listStatus for every partition once) we do not want to\n+           * accidentally return all incremental changes for the entire table in every listStatus()\n+           * call. This will create redundant splits. Instead we only want to return the incremental\n+           * changes (if so any) in that batch of input paths.\n+           *\n+           * NOTE on Hive queries that are executed using Fetch task:\n+           * Since Fetch tasks invoke InputFormat.listStatus() per partition, Hoodie metadata can be\n+           * listed in every such listStatus() call. In order to avoid this, it might be useful to\n+           * disable fetch tasks using the hive session property for incremental queries:\n+           * `set hive.fetch.task.conversion=none;`\n+           * This would ensure Map Reduce execution is chosen for a Hive query, which combines\n+           * partitions (comma separated) and calls InputFormat.listStatus() only once with all\n+           * those partitions.\n+           */\n+          for (Path path : inputPaths) {\n+            if (path.toString().contains(s)) {\n+              return true;\n+            }\n+          }\n+          return false;\n+        })\n+        .collect(Collectors.joining(\",\"));\n+    return Option.of(incrementalInputPaths);\n+  }\n+\n+  /**\n+   * Extract HoodieTimeline based on HoodieTableMetaClient.\n+   * @param job\n+   * @param tableMetaClient\n+   * @return\n+   */\n+  public static Option<HoodieTimeline> getTimeline(Job job, HoodieTableMetaClient tableMetaClient) {\n+    String tableName = tableMetaClient.getTableConfig().getTableName();\n+    HoodieDefaultTimeline baseTimeline;\n+    if (HoodieHiveUtils.stopAtCompaction(job, tableName)) {\n+      baseTimeline = filterInstantsTimeline(tableMetaClient.getActiveTimeline());\n+    } else {\n+      baseTimeline = tableMetaClient.getActiveTimeline();\n+    }\n+    return Option.of(baseTimeline.getCommitsTimeline().filterCompletedInstants());\n+  }\n+\n+  /**\n+   * Get commits to check from Hive map reduce configuration.\n+   * @param job\n+   * @param tableName\n+   * @param timeline\n+   * @return\n+   */\n+  public static Option<List<HoodieInstant>> getCommitsToCheck(Job job, String tableName, HoodieTimeline timeline) {\n+    String lastIncrementalTs = HoodieHiveUtils.readStartCommitTime(job, tableName);\n+    // Total number of commits to return in this batch. Set this to -1 to get all the commits.\n+    Integer maxCommits = HoodieHiveUtils.readMaxCommits(job, tableName);\n+    LOG.info(\"Last Incremental timestamp was set as \" + lastIncrementalTs);\n+    return Option.of(timeline.findInstantsAfter(lastIncrementalTs, maxCommits)\n+        .getInstants().collect(Collectors.toList()));\n+  }\n+\n+  /**\n+   * Generate a HoodieTableMetaClient for a given partition.\n+   * @param conf\n+   * @param partitions\n+   * @return\n+   */\n+  public static Map<Path, HoodieTableMetaClient> getMetaClientPerPartition(Configuration conf, Set<Path> partitions) {\n+    Map<String, HoodieTableMetaClient> metaClientMap = new HashMap<>();\n+    return partitions.stream().collect(Collectors.toMap(Function.identity(), p -> {\n+      // find if we have a metaclient already for this partition.\n+      Option<String> matchingBasePath = Option.fromJavaOptional(\n+          metaClientMap.keySet().stream().filter(basePath -> p.toString().startsWith(basePath)).findFirst());\n+      if (matchingBasePath.isPresent()) {\n+        return metaClientMap.get(matchingBasePath.get());\n+      }\n+\n+      try {\n+        HoodieTableMetaClient metaClient = getTableMetaClient(p.getFileSystem(conf), p);\n+        metaClientMap.put(metaClient.getBasePath(), metaClient);\n+        return metaClient;\n+      } catch (IOException e) {\n+        throw new HoodieIOException(\"Error creating hoodie meta client against : \" + p, e);\n+      }\n+    }));\n+  }\n+\n+  /**\n+   * Extract HoodieTableMetaClient from a partition path(not base path).\n+   * @param fs\n+   * @param dataPath\n+   * @return\n+   * @throws IOException\n+   */\n+  public static HoodieTableMetaClient getTableMetaClient(FileSystem fs, Path dataPath) throws IOException {\n+    int levels = HoodieHiveUtils.DEFAULT_LEVELS_TO_BASEPATH;\n+    if (HoodiePartitionMetadata.hasPartitionMetadata(fs, dataPath)) {\n+      HoodiePartitionMetadata metadata = new HoodiePartitionMetadata(fs, dataPath);\n+      metadata.readFromFS();\n+      levels = metadata.getPartitionDepth();\n+    }\n+    Path baseDir = HoodieHiveUtils.getNthParent(dataPath, levels);\n+    LOG.info(\"Reading hoodie metadata from path \" + baseDir.toString());\n+    return new HoodieTableMetaClient(fs.getConf(), baseDir.toString());\n+  }\n+\n+  /**\n+   * Filter a list of FileStatus based on commitsToCheck for incremental view.\n+   * @param job\n+   * @param tableMetaClient\n+   * @param timeline\n+   * @param fileStatuses\n+   * @param commitsToCheck\n+   * @return\n+   */\n+  public static List<FileStatus> filterIncrementalFileStatus(\n+      Job job, HoodieTableMetaClient tableMetaClient, HoodieTimeline timeline,\n+      FileStatus[] fileStatuses, List<HoodieInstant> commitsToCheck) {\n+    TableFileSystemView.BaseFileOnlyView roView = new HoodieTableFileSystemView(tableMetaClient, timeline, fileStatuses);\n+    List<String> commitsList = commitsToCheck.stream().map(HoodieInstant::getTimestamp).collect(Collectors.toList());\n+    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFilesInRange(commitsList).collect(Collectors.toList());\n+    List<FileStatus> returns = new ArrayList<>();\n+    for (HoodieBaseFile filteredFile : filteredFiles) {\n+      LOG.debug(\"Processing incremental hoodie file - \" + filteredFile.getPath());\n+      filteredFile = checkFileStatus(job.getConfiguration(), filteredFile);\n+      returns.add(filteredFile.getFileStatus());\n+    }\n+    LOG.info(\"Total paths to process after hoodie incremental filter \" + filteredFiles.size());\n+    return returns;\n+  }\n+\n+  /**\n+   * Takes in a list of filesStatus and a list of table metadatas. Groups the files status list\n+   * based on given table metadata.\n+   * @param fileStatuses\n+   * @param metaClientList\n+   * @return\n+   * @throws IOException\n+   */\n+  public static Map<HoodieTableMetaClient, List<FileStatus>> groupFileStatusForSnapshotPaths(\n+      FileStatus[] fileStatuses, Collection<HoodieTableMetaClient> metaClientList) {\n+    // This assumes the paths for different tables are grouped together\n+    Map<HoodieTableMetaClient, List<FileStatus>> grouped = new HashMap<>();\n+    HoodieTableMetaClient metadata = null;\n+    for (FileStatus status : fileStatuses) {\n+      Path inputPath = status.getPath();\n+      if (!inputPath.getName().endsWith(\".parquet\")) {\n+        //FIXME(vc): skip non parquet files for now. This wont be needed once log file name start\n+        // with \".\"\n+        continue;\n+      }\n+      if ((metadata == null) || (!inputPath.toString().contains(metadata.getBasePath()))) {\n+        for (HoodieTableMetaClient metaClient : metaClientList) {\n+          if (inputPath.toString().contains(metaClient.getBasePath())) {\n+            metadata = metaClient;\n+            if (!grouped.containsKey(metadata)) {\n+              grouped.put(metadata, new ArrayList<>());\n+            }\n+            break;\n+          }\n+        }\n+      }\n+      grouped.get(metadata).add(status);\n+    }\n+    return grouped;\n+  }\n+\n+  /**\n+   * Filters data files for a snapshot queried table.\n+   * @param job\n+   * @param metadata\n+   * @param fileStatuses\n+   * @return\n+   */\n+  public static List<FileStatus> filterFileStatusForSnapshotMode(\n+      JobConf job, HoodieTableMetaClient metadata, List<FileStatus> fileStatuses) {\n+    FileStatus[] statuses = fileStatuses.toArray(new FileStatus[0]);\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"Hoodie Metadata initialized with completed commit Ts as :\" + metadata);\n+    }\n+    // Get all commits, delta commits, compactions, as all of them produce a base parquet file today\n+    HoodieTimeline timeline = metadata.getActiveTimeline().getCommitsTimeline().filterCompletedInstants();\n+    TableFileSystemView.BaseFileOnlyView roView = new HoodieTableFileSystemView(metadata, timeline, statuses);\n+    // filter files on the latest commit found\n+    List<HoodieBaseFile> filteredFiles = roView.getLatestBaseFiles().collect(Collectors.toList());\n+    LOG.info(\"Total paths to process after hoodie filter \" + filteredFiles.size());\n+    List<FileStatus> returns = new ArrayList<>();\n+    for (HoodieBaseFile filteredFile : filteredFiles) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Processing latest hoodie file - \" + filteredFile.getPath());\n+      }\n+      filteredFile = checkFileStatus(job, filteredFile);\n+      returns.add(filteredFile.getFileStatus());\n+    }\n+    return returns;\n+  }\n+\n+  /**\n+   * Checks the file status for a race condition which can set the file size to 0. 1. HiveInputFormat does\n+   * super.listStatus() and gets back a FileStatus[] 2. Then it creates the HoodieTableMetaClient for the paths listed.\n+   * 3. Generation of splits looks at FileStatus size to create splits, which skips this file\n+   * @param conf\n+   * @param dataFile\n+   * @return\n+   */\n+  private static HoodieBaseFile checkFileStatus(Configuration conf, HoodieBaseFile dataFile) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 322}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4OTQ2Nw==", "bodyText": "rename: groupLogsByBaseFile()", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433589467", "createdAt": "2020-06-02T02:38:45Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeInputFormatUtils.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.CollectionUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.hadoop.realtime.HoodieRealtimeFileSplit;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+public class HoodieRealtimeInputFormatUtils extends HoodieInputFormatUtils {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieRealtimeInputFormatUtils.class);\n+\n+  public static InputSplit[] getRealtimeSplits(Configuration conf, Stream<FileSplit> fileSplits) throws IOException {\n+    Map<Path, List<FileSplit>> partitionsToParquetSplits =\n+        fileSplits.collect(Collectors.groupingBy(split -> split.getPath().getParent()));\n+    // TODO(vc): Should we handle also non-hoodie splits here?\n+    Map<Path, HoodieTableMetaClient> partitionsToMetaClient = getMetaClientPerPartition(conf, partitionsToParquetSplits.keySet());\n+\n+    // for all unique split parents, obtain all delta files based on delta commit timeline,\n+    // grouped on file id\n+    List<HoodieRealtimeFileSplit> rtSplits = new ArrayList<>();\n+    partitionsToParquetSplits.keySet().forEach(partitionPath -> {\n+      // for each partition path obtain the data & log file groupings, then map back to inputsplits\n+      HoodieTableMetaClient metaClient = partitionsToMetaClient.get(partitionPath);\n+      HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient, metaClient.getActiveTimeline());\n+      String relPartitionPath = FSUtils.getRelativePartitionPath(new Path(metaClient.getBasePath()), partitionPath);\n+\n+      try {\n+        // Both commit and delta-commits are included - pick the latest completed one\n+        Option<HoodieInstant> latestCompletedInstant =\n+            metaClient.getActiveTimeline().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+\n+        Stream<FileSlice> latestFileSlices = latestCompletedInstant\n+            .map(instant -> fsView.getLatestMergedFileSlicesBeforeOrOn(relPartitionPath, instant.getTimestamp()))\n+            .orElse(Stream.empty());\n+\n+        // subgroup splits again by file id & match with log files.\n+        Map<String, List<FileSplit>> groupedInputSplits = partitionsToParquetSplits.get(partitionPath).stream()\n+            .collect(Collectors.groupingBy(split -> FSUtils.getFileId(split.getPath().getName())));\n+        latestFileSlices.forEach(fileSlice -> {\n+          List<FileSplit> dataFileSplits = groupedInputSplits.get(fileSlice.getFileId());\n+          dataFileSplits.forEach(split -> {\n+            try {\n+              List<String> logFilePaths = fileSlice.getLogFiles().sorted(HoodieLogFile.getLogFileComparator())\n+                  .map(logFile -> logFile.getPath().toString()).collect(Collectors.toList());\n+              // Get the maxCommit from the last delta or compaction or commit - when\n+              // bootstrapped from COW table\n+              String maxCommitTime = metaClient\n+                  .getActiveTimeline().getTimelineOfActions(CollectionUtils.createSet(HoodieTimeline.COMMIT_ACTION,\n+                      HoodieTimeline.ROLLBACK_ACTION, HoodieTimeline.DELTA_COMMIT_ACTION))\n+                  .filterCompletedInstants().lastInstant().get().getTimestamp();\n+              rtSplits.add(new HoodieRealtimeFileSplit(split, metaClient.getBasePath(), logFilePaths, maxCommitTime));\n+            } catch (IOException e) {\n+              throw new HoodieIOException(\"Error creating hoodie real time split \", e);\n+            }\n+          });\n+        });\n+      } catch (Exception e) {\n+        throw new HoodieException(\"Error obtaining data file/log file grouping: \" + partitionPath, e);\n+      }\n+    });\n+    LOG.info(\"Returning a total splits of \" + rtSplits.size());\n+    return rtSplits.toArray(new InputSplit[0]);\n+  }\n+\n+  // Return parquet file with a list of log files in the same file group.\n+  public static Map<String, List<String>> getRealtimeFileGroup(Configuration conf, Stream<FileStatus> fileStatuses) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4OTY4MQ==", "bodyText": "we should really see if we can move this to hudi-common ParquetUtils", "url": "https://github.com/apache/hudi/pull/1592#discussion_r433589681", "createdAt": "2020-06-02T02:39:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/utils/HoodieRealtimeRecordReaderUtils.java", "diffHunk": "@@ -0,0 +1,261 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.hadoop.utils;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.exception.HoodieIOException;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericArray;\n+import org.apache.avro.generic.GenericFixed;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.serde2.io.DoubleWritable;\n+import org.apache.hadoop.io.ArrayWritable;\n+import org.apache.hadoop.io.BooleanWritable;\n+import org.apache.hadoop.io.BytesWritable;\n+import org.apache.hadoop.io.FloatWritable;\n+import org.apache.hadoop.io.IntWritable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.LinkedHashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeMap;\n+import java.util.stream.Collectors;\n+\n+public class HoodieRealtimeRecordReaderUtils {\n+\n+  /**\n+   * Reads the schema from the parquet file. This is different from ParquetUtils as it uses the twitter parquet to\n+   * support hive 1.1.0\n+   */\n+  public static MessageType readSchema(Configuration conf, Path parquetFilePath) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "a47f09a579e90890b73dff8a4262a84bd6a4d351", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/a47f09a579e90890b73dff8a4262a84bd6a4d351", "committedDate": "2020-06-02T15:43:50Z", "message": "HUDI-822 Rename some utils methods"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a47f09a579e90890b73dff8a4262a84bd6a4d351", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/a47f09a579e90890b73dff8a4262a84bd6a4d351", "committedDate": "2020-06-02T15:43:50Z", "message": "HUDI-822 Rename some utils methods"}, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "56d2a95959863c1ce44c493680424efd268bbe4f", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/56d2a95959863c1ce44c493680424efd268bbe4f", "committedDate": "2020-06-09T03:37:48Z", "message": "HUDI 822 decouple Hudi related logics from HoodieInputFormat"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "bfb5a8b978045e9346571cbad4d4e9dcaf6f6c84", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/bfb5a8b978045e9346571cbad4d4e9dcaf6f6c84", "committedDate": "2020-06-09T07:03:54Z", "message": "HUDI 822 rename some utils methods"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "bfb5a8b978045e9346571cbad4d4e9dcaf6f6c84", "author": {"user": {"login": "garyli1019", "name": "Gary Li"}}, "url": "https://github.com/apache/hudi/commit/bfb5a8b978045e9346571cbad4d4e9dcaf6f6c84", "committedDate": "2020-06-09T07:03:54Z", "message": "HUDI 822 rename some utils methods"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI3MTI0NTkw", "url": "https://github.com/apache/hudi/pull/1592#pullrequestreview-427124590", "createdAt": "2020-06-09T13:08:05Z", "commit": {"oid": "bfb5a8b978045e9346571cbad4d4e9dcaf6f6c84"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3081, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}