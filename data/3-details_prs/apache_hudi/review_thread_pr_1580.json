{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEyMDI2ODcw", "number": 1580, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQxNDoyMToyMlrOD4qWeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxODo1NDozNFrOD5HuOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYwNzQwNzI4OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQxNDoyMToyMlrOGPjkJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQwMDozODozOVrOGPwxuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODk2NDUxOQ==", "bodyText": "I think this check is applicable to both the if and else clause. Can you verify that and move it to may be line 85 (before the if clause) ?", "url": "https://github.com/apache/hudi/pull/1580#discussion_r418964519", "createdAt": "2020-05-02T14:21:22Z", "author": {"login": "bhasudha"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -118,6 +118,12 @@ private[hudi] object HoodieSparkSqlWriter {\n         fs.delete(basePath, true)\n         exists = false\n       }\n+      if (exists && mode == SaveMode.Append) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c71fb111582a0ad1bffb00360f0a912755e4166c"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE3NzMwNA==", "bodyText": "Thanks for the comment. I have updated the code.", "url": "https://github.com/apache/hudi/pull/1580#discussion_r419177304", "createdAt": "2020-05-03T23:59:02Z", "author": {"login": "AakashPradeep"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -118,6 +118,12 @@ private[hudi] object HoodieSparkSqlWriter {\n         fs.delete(basePath, true)\n         exists = false\n       }\n+      if (exists && mode == SaveMode.Append) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODk2NDUxOQ=="}, "originalCommit": {"oid": "c71fb111582a0ad1bffb00360f0a912755e4166c"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTE4MDk4NA==", "bodyText": "@bhasudha please review!\nThanks!", "url": "https://github.com/apache/hudi/pull/1580#discussion_r419180984", "createdAt": "2020-05-04T00:38:39Z", "author": {"login": "AakashPradeep"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -118,6 +118,12 @@ private[hudi] object HoodieSparkSqlWriter {\n         fs.delete(basePath, true)\n         exists = false\n       }\n+      if (exists && mode == SaveMode.Append) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODk2NDUxOQ=="}, "originalCommit": {"oid": "c71fb111582a0ad1bffb00360f0a912755e4166c"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMjIxOTQ0OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxODo1NDozNFrOGQNs5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNzoyNzo1NlrOGQ0ALg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY1NDg4NQ==", "bodyText": "actually we could have pushed this check into the guts of hoodie-client.. Every write will initialize a HoodieTableMetaClient anyway.. And have it error out from inside, it will save this extra tableMetaClient initialization..  @bhasudha let's file a follow up, if you agree", "url": "https://github.com/apache/hudi/pull/1580#discussion_r419654885", "createdAt": "2020-05-04T18:54:34Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -83,6 +83,13 @@ private[hudi] object HoodieSparkSqlWriter {\n     val fs = basePath.getFileSystem(sparkContext.hadoopConfiguration)\n     var exists = fs.exists(new Path(basePath, HoodieTableMetaClient.METAFOLDER_NAME))\n \n+    if (exists && mode == SaveMode.Append) {\n+      val existingTableName = new HoodieTableMetaClient(sparkContext.hadoopConfiguration, path.get).getTableConfig.getTableName\n+      if (!existingTableName.equals(tblName.get)) {\n+        throw new HoodieException(s\"hoodie table with name $existingTableName already exist at $basePath\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "679218955ed5488915c360a3096000b48a920952"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTgzNzYwNg==", "bodyText": "Thanks for the comment @vinothchandar.\nI would suggest the following :\n\n\nI can use HoodieTableConfig here instead of HoodieTableMetaClient (which seems little  expensive here)\n\n\nI will explore the hoodie-client code. But I would suggest to either keep all the check based on save mode in this class or move all to hoodie-client.   The earlier it throws exception better it would be, but I would leave that on you guys to decide.\n\n\nIf we decide to keep all the checks as it is then I will suggest moving checks at Line number 116 to the beginning of the if section so that we can fail fast and avoid a lot of initialization. Same for the table existence check at 172, it should be moved to the beginning of else section.\n\n\nPlease let me know if it sounds reasonable to you. I can file another Jira for improvement.\nThanks!", "url": "https://github.com/apache/hudi/pull/1580#discussion_r419837606", "createdAt": "2020-05-05T02:53:02Z", "author": {"login": "AakashPradeep"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -83,6 +83,13 @@ private[hudi] object HoodieSparkSqlWriter {\n     val fs = basePath.getFileSystem(sparkContext.hadoopConfiguration)\n     var exists = fs.exists(new Path(basePath, HoodieTableMetaClient.METAFOLDER_NAME))\n \n+    if (exists && mode == SaveMode.Append) {\n+      val existingTableName = new HoodieTableMetaClient(sparkContext.hadoopConfiguration, path.get).getTableConfig.getTableName\n+      if (!existingTableName.equals(tblName.get)) {\n+        throw new HoodieException(s\"hoodie table with name $existingTableName already exist at $basePath\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY1NDg4NQ=="}, "originalCommit": {"oid": "679218955ed5488915c360a3096000b48a920952"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE3NDA2MA==", "bodyText": "@AakashPradeep Thanks for the follow up..\nFor other save modes, e.g OVERWRITE (we first delete the existing dataset), this does not apply anyway.. So I was wondering if we can enforce this at the HoodieTable.create() level, which always reads hoodie.properties anyway... We can add a ValidationUtils.checkArgument() there to simply check the writeConfig table name matches what we read from props file?\nI will let you and @bhasudha take it from here.. :)", "url": "https://github.com/apache/hudi/pull/1580#discussion_r420174060", "createdAt": "2020-05-05T14:55:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -83,6 +83,13 @@ private[hudi] object HoodieSparkSqlWriter {\n     val fs = basePath.getFileSystem(sparkContext.hadoopConfiguration)\n     var exists = fs.exists(new Path(basePath, HoodieTableMetaClient.METAFOLDER_NAME))\n \n+    if (exists && mode == SaveMode.Append) {\n+      val existingTableName = new HoodieTableMetaClient(sparkContext.hadoopConfiguration, path.get).getTableConfig.getTableName\n+      if (!existingTableName.equals(tblName.get)) {\n+        throw new HoodieException(s\"hoodie table with name $existingTableName already exist at $basePath\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY1NDg4NQ=="}, "originalCommit": {"oid": "679218955ed5488915c360a3096000b48a920952"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDI4MjQxNA==", "bodyText": "Thanks !", "url": "https://github.com/apache/hudi/pull/1580#discussion_r420282414", "createdAt": "2020-05-05T17:27:56Z", "author": {"login": "AakashPradeep"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -83,6 +83,13 @@ private[hudi] object HoodieSparkSqlWriter {\n     val fs = basePath.getFileSystem(sparkContext.hadoopConfiguration)\n     var exists = fs.exists(new Path(basePath, HoodieTableMetaClient.METAFOLDER_NAME))\n \n+    if (exists && mode == SaveMode.Append) {\n+      val existingTableName = new HoodieTableMetaClient(sparkContext.hadoopConfiguration, path.get).getTableConfig.getTableName\n+      if (!existingTableName.equals(tblName.get)) {\n+        throw new HoodieException(s\"hoodie table with name $existingTableName already exist at $basePath\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY1NDg4NQ=="}, "originalCommit": {"oid": "679218955ed5488915c360a3096000b48a920952"}, "originalPosition": 20}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4612, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}