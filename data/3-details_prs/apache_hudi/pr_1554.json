{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA3NjQxOTI1", "number": 1554, "title": "[HUDI-704]Add test for RepairsCommand", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nAdd test for RepairsCommand in hudi-cli module\nBrief change log\n\nAdd test for RepairsCommand\n\nVerify this pull request\nThis pull request is a trivial rework / code cleanup without any test coverage.\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-04-23T02:23:26Z", "url": "https://github.com/apache/hudi/pull/1554", "merged": true, "mergeCommit": {"oid": "f921469afcd03baaf2a4f3266c8ba97e516d6935"}, "closed": true, "closedAt": "2020-05-07T15:02:29Z", "author": {"login": "hddong"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcaU5zWgBqjMyNjMyOTA3NTU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABce-oYhAFqTQwNzU2MjQwOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwNDg5NjI2", "url": "https://github.com/apache/hudi/pull/1554#pullrequestreview-400489626", "createdAt": "2020-04-26T10:55:37Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxMDo1NTozOFrOGMCvrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxMDo1NTozOFrOGMCvrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4MTA2OQ==", "bodyText": "Let us use the constant from HoodieTestDataGenerator rather than using this string when running actual command.", "url": "https://github.com/apache/hudi/pull/1554#discussion_r415281069", "createdAt": "2020-04-26T10:55:38Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010101.parquet\";\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(\"20160401010202\"))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());\n+    assertEquals(\"There should be 3 files.\", 3, filteredStatuses.size());\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[filteredStatuses.size()]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = \"2016/03/15\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 126}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwNDg5OTUw", "url": "https://github.com/apache/hudi/pull/1554#pullrequestreview-400489950", "createdAt": "2020-04-26T10:58:58Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxMDo1ODo1OVrOGMCyOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxMDo1ODo1OVrOGMCyOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4MTcyMQ==", "bodyText": "This seems a bit misleading. If the file has time of 20160401010101, how can the records have time of 20160401010202? Rather we should have the file as 3_0_20160401010202.parquet and generate one more .commit file in meta folder for this.\nPlease correct me if I am missing something.", "url": "https://github.com/apache/hudi/pull/1554#discussion_r415281721", "createdAt": "2020-04-26T10:58:59Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010101.parquet\";\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(\"20160401010202\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 103}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwNDkwMDI2", "url": "https://github.com/apache/hudi/pull/1554#pullrequestreview-400490026", "createdAt": "2020-04-26T10:59:51Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxMDo1OTo1MlrOGMCy2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxMDo1OTo1MlrOGMCy2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4MTg4MA==", "bodyText": "same here as well. Let us use the constant.", "url": "https://github.com/apache/hudi/pull/1554#discussion_r415281880", "createdAt": "2020-04-26T10:59:52Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010101.parquet\";\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(\"20160401010202\"))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());\n+    assertEquals(\"There should be 3 files.\", 3, filteredStatuses.size());\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[filteredStatuses.size()]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = \"2016/03/15\";\n+    String cmdStr = \"repair deduplicate --duplicatedPartitionPath \" + partitionPath\n+        + \" --repairedOutputPath \" + repairedOutputPath + \" --sparkMaster local\";\n+    CommandResult cr = getShell().executeCommand(cmdStr);\n+    assertTrue(cr.isSuccess());\n+    assertEquals(RepairsCommand.DEDUPLICATE_RETURN_PREFIX + repairedOutputPath, cr.getResult().toString());\n+\n+    // After deduplicate, there are 200 records\n+    FileStatus[] fileStatus = fs.listStatus(new Path(repairedOutputPath));\n+    files = Arrays.stream(fileStatus).map(status -> status.getPath().toString()).toArray(String[]::new);\n+    Dataset result = sqlContext.read().parquet(files);\n+    assertEquals(200, result.count());\n+  }\n+\n+  /**\n+   * Test case for real run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicateWithReal() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());\n+    assertEquals(\"There should be 3 files.\", 3, filteredStatuses.size());\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[filteredStatuses.size()]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = \"2016/03/15\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 157}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwNDkwNTYy", "url": "https://github.com/apache/hudi/pull/1554#pullrequestreview-400490562", "createdAt": "2020-04-26T11:05:09Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxMTowNTowOVrOGMC21w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNlQxMTowNTowOVrOGMC21w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4MjkwMw==", "bodyText": "Any specific reason for having a separate class for RepairsCommand#deduplicate method?", "url": "https://github.com/apache/hudi/pull/1554#discussion_r415282903", "createdAt": "2020-04-26T11:05:09Z", "author": {"login": "pratyakshsharma"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0MDM0NjYy", "url": "https://github.com/apache/hudi/pull/1554#pullrequestreview-404034662", "createdAt": "2020-05-01T06:10:13Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwNjoxMDoxM1rOGPC9YA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwNjozMjoyMFrOGPDPAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMDMwNA==", "bodyText": "Use Files.createFile()?", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418430304", "createdAt": "2020-05-01T06:10:13Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.HoodieTableHeaderFields;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Test class for {@link RepairsCommand}.\n+ */\n+public class TestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+  }\n+\n+  /**\n+   * Test case for dry run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithDryRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMDUyNw==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418430527", "createdAt": "2020-05-01T06:11:38Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.HoodieTableHeaderFields;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Test class for {@link RepairsCommand}.\n+ */\n+public class TestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+  }\n+\n+  /**\n+   * Test case for dry run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithDryRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();\n+\n+    // create partition path\n+    String partition1 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String partition2 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+    String partition3 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_THIRD_PARTITION_PATH;\n+    fs.mkdirs(new Path(partition1));\n+    fs.mkdirs(new Path(partition2));\n+    fs.mkdirs(new Path(partition3));\n+\n+    // default is dry run.\n+    CommandResult cr = getShell().executeCommand(\"repair addpartitionmeta\");\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    // expected all 'No'.\n+    String[][] rows = FSUtils.getAllPartitionFoldersThreeLevelsDown(fs, tablePath)\n+        .stream()\n+        .map(partition -> new String[]{partition, \"No\", \"None\"})\n+        .toArray(String[][]::new);\n+    String expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for real run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithRealRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMTE3Ng==", "bodyText": "Multiple methods have this code snippet. Can we extract it into init method? Or Judge exists before create?", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418431176", "createdAt": "2020-05-01T06:15:11Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.HoodieTableHeaderFields;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Test class for {@link RepairsCommand}.\n+ */\n+public class TestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+  }\n+\n+  /**\n+   * Test case for dry run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithDryRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();\n+\n+    // create partition path\n+    String partition1 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String partition2 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+    String partition3 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_THIRD_PARTITION_PATH;\n+    fs.mkdirs(new Path(partition1));\n+    fs.mkdirs(new Path(partition2));\n+    fs.mkdirs(new Path(partition3));\n+\n+    // default is dry run.\n+    CommandResult cr = getShell().executeCommand(\"repair addpartitionmeta\");\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    // expected all 'No'.\n+    String[][] rows = FSUtils.getAllPartitionFoldersThreeLevelsDown(fs, tablePath)\n+        .stream()\n+        .map(partition -> new String[]{partition, \"No\", \"None\"})\n+        .toArray(String[][]::new);\n+    String expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for real run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithRealRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();\n+\n+    // create partition path\n+    String partition1 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String partition2 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+    String partition3 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_THIRD_PARTITION_PATH;\n+    fs.mkdirs(new Path(partition1));\n+    fs.mkdirs(new Path(partition2));\n+    fs.mkdirs(new Path(partition3));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMTQ4NQ==", "bodyText": "metaclient -> meta client?", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418431485", "createdAt": "2020-05-01T06:16:46Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.HoodieTableHeaderFields;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Test class for {@link RepairsCommand}.\n+ */\n+public class TestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+  }\n+\n+  /**\n+   * Test case for dry run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithDryRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();\n+\n+    // create partition path\n+    String partition1 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String partition2 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+    String partition3 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_THIRD_PARTITION_PATH;\n+    fs.mkdirs(new Path(partition1));\n+    fs.mkdirs(new Path(partition2));\n+    fs.mkdirs(new Path(partition3));\n+\n+    // default is dry run.\n+    CommandResult cr = getShell().executeCommand(\"repair addpartitionmeta\");\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    // expected all 'No'.\n+    String[][] rows = FSUtils.getAllPartitionFoldersThreeLevelsDown(fs, tablePath)\n+        .stream()\n+        .map(partition -> new String[]{partition, \"No\", \"None\"})\n+        .toArray(String[][]::new);\n+    String expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for real run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithRealRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();\n+\n+    // create partition path\n+    String partition1 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String partition2 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+    String partition3 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_THIRD_PARTITION_PATH;\n+    fs.mkdirs(new Path(partition1));\n+    fs.mkdirs(new Path(partition2));\n+    fs.mkdirs(new Path(partition3));\n+\n+    CommandResult cr = getShell().executeCommand(\"repair addpartitionmeta --dryrun false\");\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    List<String> paths = FSUtils.getAllPartitionFoldersThreeLevelsDown(fs, tablePath);\n+    // after dry run, the action will be 'Repaired'\n+    String[][] rows = paths.stream()\n+        .map(partition -> new String[]{partition, \"No\", \"Repaired\"})\n+        .toArray(String[][]::new);\n+    String expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+\n+    cr = getShell().executeCommand(\"repair addpartitionmeta\");\n+\n+    // after real run, Metadata is present now.\n+    rows = paths.stream()\n+        .map(partition -> new String[]{partition, \"Yes\", \"None\"})\n+        .toArray(String[][]::new);\n+    expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for 'repair overwrite-hoodie-props'.\n+   */\n+  @Test\n+  public void testOverwriteHoodieProperties() throws IOException {\n+    URL newProps = this.getClass().getClassLoader().getResource(\"table-config.properties\");\n+    assertNotNull(\"New property file must exist\", newProps);\n+\n+    CommandResult cr = getShell().executeCommand(\"repair overwrite-hoodie-props --new-props-file \" + newProps.getPath());\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    Map<String, String> oldProps = HoodieCLI.getTableMetaClient().getTableConfig().getProps();\n+\n+    // after overwrite, the stored value in .hoodie is equals to which read from properties.\n+    Map<String, String> result = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient()).getTableConfig().getProps();\n+    Properties expectProps = new Properties();\n+    expectProps.load(new FileInputStream(new File(newProps.getPath())));\n+\n+    Map<String, String> expected = expectProps.entrySet().stream()\n+        .collect(Collectors.toMap(e -> String.valueOf(e.getKey()), e -> String.valueOf(e.getValue())));\n+    Assert.assertEquals(expected, result);\n+\n+    // check result\n+    List<String> allPropsStr = Arrays.asList(\"hoodie.table.name\", \"hoodie.table.type\",\n+        \"hoodie.archivelog.folder\", \"hoodie.timeline.layout.version\");\n+    String[][] rows = allPropsStr.stream().sorted().map(key -> new String[]{key,\n+        oldProps.getOrDefault(key, null), result.getOrDefault(key, null)})\n+        .toArray(String[][]::new);\n+    String expect = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_HOODIE_PROPERTY,\n+        HoodieTableHeaderFields.HEADER_OLD_VALUE, HoodieTableHeaderFields.HEADER_NEW_VALUE}, rows);\n+\n+    Assert.assertEquals(expect, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for 'repair corrupted clean files'.\n+   */\n+  @Test\n+  public void testRemoveCorruptedPendingCleanAction() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+\n+    // Create four requested files\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Write corrupted requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionRequestedFile(tablePath, timestamp, conf);\n+    }\n+\n+    // reload metaclient\n+    metaClient = HoodieTableMetaClient.reload(metaClient);\n+    // first, there are four instants\n+    assertEquals(4, metaClient.getActiveTimeline().filterInflightsAndRequested().getInstants().count());\n+\n+    CommandResult cr = getShell().executeCommand(\"repair corrupted clean files\");\n+    assertTrue(cr.isSuccess());\n+\n+    // reload metaclient", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMTU0MQ==", "bodyText": "metaclient -> meta client?", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418431541", "createdAt": "2020-05-01T06:17:04Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/commands/TestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,206 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.commands;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.HoodiePrintHelper;\n+import org.apache.hudi.cli.HoodieTableHeaderFields;\n+import org.apache.hudi.cli.common.HoodieTestCommitMetadataGenerator;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.stream.Collectors;\n+\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Test class for {@link RepairsCommand}.\n+ */\n+public class TestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String tablePath;\n+\n+  @Before\n+  public void init() throws IOException {\n+    String tableName = \"test_table\";\n+    tablePath = basePath + File.separator + tableName;\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+  }\n+\n+  /**\n+   * Test case for dry run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithDryRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();\n+\n+    // create partition path\n+    String partition1 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String partition2 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+    String partition3 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_THIRD_PARTITION_PATH;\n+    fs.mkdirs(new Path(partition1));\n+    fs.mkdirs(new Path(partition2));\n+    fs.mkdirs(new Path(partition3));\n+\n+    // default is dry run.\n+    CommandResult cr = getShell().executeCommand(\"repair addpartitionmeta\");\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    // expected all 'No'.\n+    String[][] rows = FSUtils.getAllPartitionFoldersThreeLevelsDown(fs, tablePath)\n+        .stream()\n+        .map(partition -> new String[]{partition, \"No\", \"None\"})\n+        .toArray(String[][]::new);\n+    String expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for real run 'repair addpartitionmeta'.\n+   */\n+  @Test\n+  public void testAddPartitionMetaWithRealRun() throws IOException {\n+    // create commit instant\n+    new File(tablePath + \"/.hoodie/100.commit\").createNewFile();\n+\n+    // create partition path\n+    String partition1 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String partition2 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_SECOND_PARTITION_PATH;\n+    String partition3 = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_THIRD_PARTITION_PATH;\n+    fs.mkdirs(new Path(partition1));\n+    fs.mkdirs(new Path(partition2));\n+    fs.mkdirs(new Path(partition3));\n+\n+    CommandResult cr = getShell().executeCommand(\"repair addpartitionmeta --dryrun false\");\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    List<String> paths = FSUtils.getAllPartitionFoldersThreeLevelsDown(fs, tablePath);\n+    // after dry run, the action will be 'Repaired'\n+    String[][] rows = paths.stream()\n+        .map(partition -> new String[]{partition, \"No\", \"Repaired\"})\n+        .toArray(String[][]::new);\n+    String expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+\n+    cr = getShell().executeCommand(\"repair addpartitionmeta\");\n+\n+    // after real run, Metadata is present now.\n+    rows = paths.stream()\n+        .map(partition -> new String[]{partition, \"Yes\", \"None\"})\n+        .toArray(String[][]::new);\n+    expected = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_PARTITION_PATH,\n+        HoodieTableHeaderFields.HEADER_METADATA_PRESENT, HoodieTableHeaderFields.HEADER_REPAIR_ACTION}, rows);\n+    Assert.assertEquals(expected, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for 'repair overwrite-hoodie-props'.\n+   */\n+  @Test\n+  public void testOverwriteHoodieProperties() throws IOException {\n+    URL newProps = this.getClass().getClassLoader().getResource(\"table-config.properties\");\n+    assertNotNull(\"New property file must exist\", newProps);\n+\n+    CommandResult cr = getShell().executeCommand(\"repair overwrite-hoodie-props --new-props-file \" + newProps.getPath());\n+    Assert.assertTrue(cr.isSuccess());\n+\n+    Map<String, String> oldProps = HoodieCLI.getTableMetaClient().getTableConfig().getProps();\n+\n+    // after overwrite, the stored value in .hoodie is equals to which read from properties.\n+    Map<String, String> result = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient()).getTableConfig().getProps();\n+    Properties expectProps = new Properties();\n+    expectProps.load(new FileInputStream(new File(newProps.getPath())));\n+\n+    Map<String, String> expected = expectProps.entrySet().stream()\n+        .collect(Collectors.toMap(e -> String.valueOf(e.getKey()), e -> String.valueOf(e.getValue())));\n+    Assert.assertEquals(expected, result);\n+\n+    // check result\n+    List<String> allPropsStr = Arrays.asList(\"hoodie.table.name\", \"hoodie.table.type\",\n+        \"hoodie.archivelog.folder\", \"hoodie.timeline.layout.version\");\n+    String[][] rows = allPropsStr.stream().sorted().map(key -> new String[]{key,\n+        oldProps.getOrDefault(key, null), result.getOrDefault(key, null)})\n+        .toArray(String[][]::new);\n+    String expect = HoodiePrintHelper.print(new String[] {HoodieTableHeaderFields.HEADER_HOODIE_PROPERTY,\n+        HoodieTableHeaderFields.HEADER_OLD_VALUE, HoodieTableHeaderFields.HEADER_NEW_VALUE}, rows);\n+\n+    Assert.assertEquals(expect, cr.getResult().toString());\n+  }\n+\n+  /**\n+   * Test case for 'repair corrupted clean files'.\n+   */\n+  @Test\n+  public void testRemoveCorruptedPendingCleanAction() throws IOException {\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    Configuration conf = HoodieCLI.conf;\n+\n+    metaClient = HoodieCLI.getTableMetaClient();\n+\n+    // Create four requested files\n+    for (int i = 100; i < 104; i++) {\n+      String timestamp = String.valueOf(i);\n+      // Write corrupted requested Compaction\n+      HoodieTestCommitMetadataGenerator.createCompactionRequestedFile(tablePath, timestamp, conf);\n+    }\n+\n+    // reload metaclient", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMzQwMA==", "bodyText": "It would be better to add the description you mentioned into the Javadoc of this class?", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418433400", "createdAt": "2020-05-01T06:25:14Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTI4MjkwMw=="}, "originalCommit": null, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzMzYzOQ==", "bodyText": "add an empty line before these two lines and mark them private.", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418433639", "createdAt": "2020-05-01T06:26:22Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzNDAxNw==", "bodyText": "Files.createFile", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418434017", "createdAt": "2020-05-01T06:28:08Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzNDA1MA==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418434050", "createdAt": "2020-05-01T06:28:21Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzNDExNw==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418434117", "createdAt": "2020-05-01T06:28:40Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzNDMxNQ==", "bodyText": "map(f -> f.getPath()) -> map(HoodieBaseFile::getPath)", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418434315", "createdAt": "2020-05-01T06:29:36Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzNDcxNA==", "bodyText": "We do not need to specify the size for toArray(). new String[0] is OK.", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418434714", "createdAt": "2020-05-01T06:31:47Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());\n+    assertEquals(\"There should be 3 files.\", 3, filteredStatuses.size());\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[filteredStatuses.size()]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzNDc5OQ==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418434799", "createdAt": "2020-05-01T06:32:13Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());\n+    assertEquals(\"There should be 3 files.\", 3, filteredStatuses.size());\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[filteredStatuses.size()]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String cmdStr = \"repair deduplicate --duplicatedPartitionPath \" + partitionPath\n+        + \" --repairedOutputPath \" + repairedOutputPath + \" --sparkMaster local\";\n+    CommandResult cr = getShell().executeCommand(cmdStr);\n+    assertTrue(cr.isSuccess());\n+    assertEquals(RepairsCommand.DEDUPLICATE_RETURN_PREFIX + repairedOutputPath, cr.getResult().toString());\n+\n+    // After deduplicate, there are 200 records\n+    FileStatus[] fileStatus = fs.listStatus(new Path(repairedOutputPath));\n+    files = Arrays.stream(fileStatus).map(status -> status.getPath().toString()).toArray(String[]::new);\n+    Dataset result = sqlContext.read().parquet(files);\n+    assertEquals(200, result.count());\n+  }\n+\n+  /**\n+   * Test case for real run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicateWithReal() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQzNDgxNg==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/1554#discussion_r418434816", "createdAt": "2020-05-01T06:32:20Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+  String duplicatedPartitionPath;\n+  String repairedOutputPath;\n+\n+  @Before\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    new File(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken))\n+        .createNewFile();\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    new File(tablePath + \"/.hoodie/\" + commitTime + \".commit\").createNewFile();\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());\n+    assertEquals(\"There should be 3 files.\", 3, filteredStatuses.size());\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[filteredStatuses.size()]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String cmdStr = \"repair deduplicate --duplicatedPartitionPath \" + partitionPath\n+        + \" --repairedOutputPath \" + repairedOutputPath + \" --sparkMaster local\";\n+    CommandResult cr = getShell().executeCommand(cmdStr);\n+    assertTrue(cr.isSuccess());\n+    assertEquals(RepairsCommand.DEDUPLICATE_RETURN_PREFIX + repairedOutputPath, cr.getResult().toString());\n+\n+    // After deduplicate, there are 200 records\n+    FileStatus[] fileStatus = fs.listStatus(new Path(repairedOutputPath));\n+    files = Arrays.stream(fileStatus).map(status -> status.getPath().toString()).toArray(String[]::new);\n+    Dataset result = sqlContext.read().parquet(files);\n+    assertEquals(200, result.count());\n+  }\n+\n+  /**\n+   * Test case for real run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicateWithReal() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(f -> f.getPath()).collect(Collectors.toList());\n+    assertEquals(\"There should be 3 files.\", 3, filteredStatuses.size());\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[filteredStatuses.size()]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 155}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "22955d0e28152138d32be764265fbbd227a4122f", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/22955d0e28152138d32be764265fbbd227a4122f", "committedDate": "2020-05-04T02:31:50Z", "message": "Add test for RepairsCommand rebase to master"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "22955d0e28152138d32be764265fbbd227a4122f", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/22955d0e28152138d32be764265fbbd227a4122f", "committedDate": "2020-05-04T02:31:50Z", "message": "Add test for RepairsCommand rebase to master"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA2NjA1MTc0", "url": "https://github.com/apache/hudi/pull/1554#pullrequestreview-406605174", "createdAt": "2020-05-06T13:22:01Z", "commit": {"oid": "22955d0e28152138d32be764265fbbd227a4122f"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzoyMjowMVrOGRSrGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQxMzoyOTo0NFrOGRTBPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NDkyMA==", "bodyText": "\"Spark Master \" -> \"Spark Master\"?", "url": "https://github.com/apache/hudi/pull/1554#discussion_r420784920", "createdAt": "2020-05-06T13:22:01Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -64,19 +69,35 @@ public String deduplicate(\n       @CliOption(key = {\"repairedOutputPath\"}, help = \"Location to place the repaired files\",\n           mandatory = true) final String repairedOutputPath,\n       @CliOption(key = {\"sparkProperties\"}, help = \"Spark Properties File Path\",\n-          mandatory = true) final String sparkPropertiesPath)\n+          unspecifiedDefaultValue = \"\") String sparkPropertiesPath,\n+      @CliOption(key = \"sparkMaster\", unspecifiedDefaultValue = \"\", help = \"Spark Master \") String master,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22955d0e28152138d32be764265fbbd227a4122f"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NTc1OQ==", "bodyText": "The same suggestion, we should try to define a data structure? We can refactor it later.", "url": "https://github.com/apache/hudi/pull/1554#discussion_r420785759", "createdAt": "2020-05-06T13:23:10Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/RepairsCommand.java", "diffHunk": "@@ -64,19 +69,35 @@ public String deduplicate(\n       @CliOption(key = {\"repairedOutputPath\"}, help = \"Location to place the repaired files\",\n           mandatory = true) final String repairedOutputPath,\n       @CliOption(key = {\"sparkProperties\"}, help = \"Spark Properties File Path\",\n-          mandatory = true) final String sparkPropertiesPath)\n+          unspecifiedDefaultValue = \"\") String sparkPropertiesPath,\n+      @CliOption(key = \"sparkMaster\", unspecifiedDefaultValue = \"\", help = \"Spark Master \") String master,\n+      @CliOption(key = \"sparkMemory\", unspecifiedDefaultValue = \"4G\",\n+          help = \"Spark executor memory\") final String sparkMemory,\n+      @CliOption(key = {\"dryrun\"},\n+          help = \"Should we actually remove duplicates or just run and store result to repairedOutputPath\",\n+          unspecifiedDefaultValue = \"true\") final boolean dryRun)\n       throws Exception {\n+    if (StringUtils.isNullOrEmpty(sparkPropertiesPath)) {\n+      sparkPropertiesPath =\n+          Utils.getDefaultPropertiesFile(JavaConverters.mapAsScalaMapConverter(System.getenv()).asScala());\n+    }\n+\n     SparkLauncher sparkLauncher = SparkUtil.initLauncher(sparkPropertiesPath);\n-    sparkLauncher.addAppArgs(SparkMain.SparkCommand.DEDUPLICATE.toString(), duplicatedPartitionPath, repairedOutputPath,\n-        HoodieCLI.getTableMetaClient().getBasePath());\n+    sparkLauncher.addAppArgs(SparkMain.SparkCommand.DEDUPLICATE.toString(), master, sparkMemory,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22955d0e28152138d32be764265fbbd227a4122f"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc4NjgzMQ==", "bodyText": "IMHO, we also need to refactor the arg parse. But not in this PR.", "url": "https://github.com/apache/hudi/pull/1554#discussion_r420786831", "createdAt": "2020-05-06T13:24:39Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/SparkMain.java", "diffHunk": "@@ -73,8 +73,8 @@ public static void main(String[] args) throws Exception {\n         returnCode = rollback(jsc, args[1], args[2]);\n         break;\n       case DEDUPLICATE:\n-        assert (args.length == 4);\n-        returnCode = deduplicatePartitionPath(jsc, args[1], args[2], args[3]);\n+        assert (args.length == 7);\n+        returnCode = deduplicatePartitionPath(jsc, args[3], args[4], args[5], args[6]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22955d0e28152138d32be764265fbbd227a4122f"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc5MDQ0Ng==", "bodyText": "Can we use String.format(xxx) here?", "url": "https://github.com/apache/hudi/pull/1554#discussion_r420790446", "createdAt": "2020-05-06T13:29:33Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ * <p/>\n+ * A command use SparkLauncher need load jars under lib which generate during mvn package.\n+ * Use integration test instead of unit test.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String duplicatedPartitionPath;\n+  private String repairedOutputPath;\n+\n+  @BeforeEach\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    Files.createFile(Paths.get(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken)));\n+    Files.createFile(Paths.get(tablePath + \"/.hoodie/\" + commitTime + \".commit\"));\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    Files.createFile(Paths.get(tablePath + \"/.hoodie/\" + commitTime + \".commit\"));\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(HoodieBaseFile::getPath).collect(Collectors.toList());\n+    assertEquals(3, filteredStatuses.size(), \"There should be 3 files.\");\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[0]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String cmdStr = \"repair deduplicate --duplicatedPartitionPath \" + partitionPath", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22955d0e28152138d32be764265fbbd227a4122f"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDc5MDU5MQ==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/1554#discussion_r420790591", "createdAt": "2020-05-06T13:29:44Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/test/java/org/apache/hudi/cli/integ/ITTestRepairsCommand.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.cli.integ;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.cli.AbstractShellIntegrationTest;\n+import org.apache.hudi.cli.HoodieCLI;\n+import org.apache.hudi.cli.commands.RepairsCommand;\n+import org.apache.hudi.cli.commands.TableCommand;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.versioning.TimelineLayoutVersion;\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView;\n+import org.apache.hudi.common.util.SchemaTestUtil;\n+import org.apache.spark.sql.Dataset;\n+\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.springframework.shell.core.CommandResult;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.spark.sql.functions.lit;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Integration test class for {@link RepairsCommand#deduplicate}.\n+ * <p/>\n+ * A command use SparkLauncher need load jars under lib which generate during mvn package.\n+ * Use integration test instead of unit test.\n+ */\n+public class ITTestRepairsCommand extends AbstractShellIntegrationTest {\n+\n+  private String duplicatedPartitionPath;\n+  private String repairedOutputPath;\n+\n+  @BeforeEach\n+  public void init() throws IOException, URISyntaxException {\n+    String tablePath = basePath + File.separator + \"test_table\";\n+    duplicatedPartitionPath = tablePath + File.separator + HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    repairedOutputPath = basePath + File.separator + \"tmp\";\n+\n+    HoodieCLI.conf = jsc.hadoopConfiguration();\n+\n+    // Create table and connect\n+    new TableCommand().createTable(\n+        tablePath, \"test_table\", HoodieTableType.COPY_ON_WRITE.name(),\n+        \"\", TimelineLayoutVersion.VERSION_1, \"org.apache.hudi.common.model.HoodieAvroPayload\");\n+\n+    // generate 200 records\n+    Schema schema = HoodieAvroUtils.addMetadataFields(SchemaTestUtil.getSimpleSchema());\n+\n+    String fileName1 = \"1_0_20160401010101.parquet\";\n+    String fileName2 = \"2_0_20160401010101.parquet\";\n+\n+    List<HoodieRecord> hoodieRecords1 = SchemaTestUtil.generateHoodieTestRecords(0, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName1, hoodieRecords1, schema, null, false);\n+    List<HoodieRecord> hoodieRecords2 = SchemaTestUtil.generateHoodieTestRecords(100, 100, schema);\n+    HoodieClientTestUtils.writeParquetFile(tablePath, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH,\n+        fileName2, hoodieRecords2, schema, null, false);\n+\n+    // generate commit file\n+    String fileId1 = UUID.randomUUID().toString();\n+    String testWriteToken = \"1-0-1\";\n+    String commitTime = FSUtils.getCommitTime(fileName1);\n+    Files.createFile(Paths.get(duplicatedPartitionPath + \"/\"\n+        + FSUtils.makeLogFileName(fileId1, HoodieLogFile.DELTA_EXTENSION, commitTime, 1, testWriteToken)));\n+    Files.createFile(Paths.get(tablePath + \"/.hoodie/\" + commitTime + \".commit\"));\n+\n+    // read records and get 10 to generate duplicates\n+    Dataset df = sqlContext.read().parquet(duplicatedPartitionPath);\n+\n+    String fileName3 = \"3_0_20160401010202.parquet\";\n+    commitTime = FSUtils.getCommitTime(fileName3);\n+    df.limit(10).withColumn(\"_hoodie_commit_time\", lit(commitTime))\n+        .write().parquet(duplicatedPartitionPath + File.separator + fileName3);\n+    Files.createFile(Paths.get(tablePath + \"/.hoodie/\" + commitTime + \".commit\"));\n+\n+    metaClient = HoodieTableMetaClient.reload(HoodieCLI.getTableMetaClient());\n+  }\n+\n+  /**\n+   * Test case for dry run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicate() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(HoodieBaseFile::getPath).collect(Collectors.toList());\n+    assertEquals(3, filteredStatuses.size(), \"There should be 3 files.\");\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[0]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String cmdStr = \"repair deduplicate --duplicatedPartitionPath \" + partitionPath\n+        + \" --repairedOutputPath \" + repairedOutputPath + \" --sparkMaster local\";\n+    CommandResult cr = getShell().executeCommand(cmdStr);\n+    assertTrue(cr.isSuccess());\n+    assertEquals(RepairsCommand.DEDUPLICATE_RETURN_PREFIX + repairedOutputPath, cr.getResult().toString());\n+\n+    // After deduplicate, there are 200 records\n+    FileStatus[] fileStatus = fs.listStatus(new Path(repairedOutputPath));\n+    files = Arrays.stream(fileStatus).map(status -> status.getPath().toString()).toArray(String[]::new);\n+    Dataset result = sqlContext.read().parquet(files);\n+    assertEquals(200, result.count());\n+  }\n+\n+  /**\n+   * Test case for real run deduplicate.\n+   */\n+  @Test\n+  public void testDeduplicateWithReal() throws IOException {\n+    // get fs and check number of latest files\n+    HoodieTableFileSystemView fsView = new HoodieTableFileSystemView(metaClient,\n+        metaClient.getActiveTimeline().getCommitTimeline().filterCompletedInstants(),\n+        fs.listStatus(new Path(duplicatedPartitionPath)));\n+    List<String> filteredStatuses = fsView.getLatestBaseFiles().map(HoodieBaseFile::getPath).collect(Collectors.toList());\n+    assertEquals(3, filteredStatuses.size(), \"There should be 3 files.\");\n+\n+    // Before deduplicate, all files contain 210 records\n+    String[] files = filteredStatuses.toArray(new String[0]);\n+    Dataset df = sqlContext.read().parquet(files);\n+    assertEquals(210, df.count());\n+\n+    String partitionPath = HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH;\n+    String cmdStr = \"repair deduplicate --duplicatedPartitionPath \" + partitionPath", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "22955d0e28152138d32be764265fbbd227a4122f"}, "originalPosition": 167}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "95b9e09a708f778262ba0040d651f408e9d6a3d1", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/95b9e09a708f778262ba0040d651f408e9d6a3d1", "committedDate": "2020-05-07T09:58:47Z", "message": "fix"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA3NTYyNDA4", "url": "https://github.com/apache/hudi/pull/1554#pullrequestreview-407562408", "createdAt": "2020-05-07T15:02:02Z", "commit": {"oid": "95b9e09a708f778262ba0040d651f408e9d6a3d1"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3510, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}