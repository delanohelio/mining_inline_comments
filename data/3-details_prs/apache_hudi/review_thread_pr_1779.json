{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQyNDkzNTI4", "number": 1779, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwMzoxMDoyOVrOEKuqgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMzoyNzoxOFrOELVsWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5Njg1NzYxOnYy", "diffSide": "LEFT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwMzoxMDoyOVrOGr768w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwMzo1NzoyMFrOGr8k0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcyMzY5OQ==", "bodyText": "Why do you remove this check statement? If the user sets the value of Config.MAX_EVENTS_FROM_KAFKA_SOURCE_PROP to Long.MAX_VALUE. How do you reset it to Config.maxEventsFromKafkaSource(5000000)?", "url": "https://github.com/apache/hudi/pull/1779#discussion_r448723699", "createdAt": "2020-07-02T03:10:29Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -202,9 +202,14 @@ public KafkaOffsetGen(TypedProperties props) {\n     // Come up with final set of OffsetRanges to read (account for new partitions, limit number of events)\n     long maxEventsToReadFromKafka = props.getLong(Config.MAX_EVENTS_FROM_KAFKA_SOURCE_PROP,\n         Config.maxEventsFromKafkaSource);\n-    maxEventsToReadFromKafka = (maxEventsToReadFromKafka == Long.MAX_VALUE || maxEventsToReadFromKafka == Integer.MAX_VALUE)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcyODExNA==", "bodyText": "Why do you remove this check statement? If the user sets the value of Config.MAX_EVENTS_FROM_KAFKA_SOURCE_PROP to Long.MAX_VALUE. How do you reset it to Config.maxEventsFromKafkaSource(5000000)?\n\ndetailed discuss is here: https://issues.apache.org/jira/projects/HUDI/issues/HUDI-340?filter=allissues\nthis check can not achieve its goal to set a limit to max events read from kafka in one batch, and this goal is not absolute\uff0cit is allowable to set a bigger num than Config.maxEventsFromKafkaSource as discussed above.", "url": "https://github.com/apache/hudi/pull/1779#discussion_r448728114", "createdAt": "2020-07-02T03:29:30Z", "author": {"login": "wangxianghu"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -202,9 +202,14 @@ public KafkaOffsetGen(TypedProperties props) {\n     // Come up with final set of OffsetRanges to read (account for new partitions, limit number of events)\n     long maxEventsToReadFromKafka = props.getLong(Config.MAX_EVENTS_FROM_KAFKA_SOURCE_PROP,\n         Config.maxEventsFromKafkaSource);\n-    maxEventsToReadFromKafka = (maxEventsToReadFromKafka == Long.MAX_VALUE || maxEventsToReadFromKafka == Integer.MAX_VALUE)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcyMzY5OQ=="}, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODczNDQxNg==", "bodyText": "Got it. Sg.", "url": "https://github.com/apache/hudi/pull/1779#discussion_r448734416", "createdAt": "2020-07-02T03:57:20Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -202,9 +202,14 @@ public KafkaOffsetGen(TypedProperties props) {\n     // Come up with final set of OffsetRanges to read (account for new partitions, limit number of events)\n     long maxEventsToReadFromKafka = props.getLong(Config.MAX_EVENTS_FROM_KAFKA_SOURCE_PROP,\n         Config.maxEventsFromKafkaSource);\n-    maxEventsToReadFromKafka = (maxEventsToReadFromKafka == Long.MAX_VALUE || maxEventsToReadFromKafka == Integer.MAX_VALUE)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcyMzY5OQ=="}, "originalCommit": null, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwMzI1MjExOnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestKafkaSource.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMzoyNzoxOFrOGs4qzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNFQwNjoxNDo0NFrOGs6N2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxODk5MA==", "bodyText": "why exactly does this test have to change? could you please clarify", "url": "https://github.com/apache/hudi/pull/1779#discussion_r449718990", "createdAt": "2020-07-03T23:27:18Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestKafkaSource.java", "diffHunk": "@@ -191,13 +191,13 @@ public void testJsonKafkaSourceWithDefaultUpperCap() {\n      */\n     testUtils.sendMessages(TEST_TOPIC_NAME, Helpers.jsonifyRecords(dataGenerator.generateInserts(\"000\", 1000)));\n     InputBatch<JavaRDD<GenericRecord>> fetch1 = kafkaSource.fetchNewDataInAvroFormat(Option.empty(), Long.MAX_VALUE);\n-    assertEquals(500, fetch1.getBatch().get().count());\n+    assertEquals(1000, fetch1.getBatch().get().count());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d0f342c18760cd11a782b5aa035179ed2e1181bb"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTczMTEzNQ==", "bodyText": "why exactly does this test have to change? could you please clarify\nhi @vinothchandar,  the value of maxEventsToReadFromKafka has changed in this commit, so the value of fetch1.getBatch().get().count() also changes in the test case.\n\n\nthere is an empty git file in this commit.. can you please remove this?\n@vinothchandar I have removed this file.", "url": "https://github.com/apache/hudi/pull/1779#discussion_r449731135", "createdAt": "2020-07-04T02:34:11Z", "author": {"login": "Trevor-zhang"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestKafkaSource.java", "diffHunk": "@@ -191,13 +191,13 @@ public void testJsonKafkaSourceWithDefaultUpperCap() {\n      */\n     testUtils.sendMessages(TEST_TOPIC_NAME, Helpers.jsonifyRecords(dataGenerator.generateInserts(\"000\", 1000)));\n     InputBatch<JavaRDD<GenericRecord>> fetch1 = kafkaSource.fetchNewDataInAvroFormat(Option.empty(), Long.MAX_VALUE);\n-    assertEquals(500, fetch1.getBatch().get().count());\n+    assertEquals(1000, fetch1.getBatch().get().count());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxODk5MA=="}, "originalCommit": {"oid": "d0f342c18760cd11a782b5aa035179ed2e1181bb"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTc0NDE3Nw==", "bodyText": "Hi @Trevor-zhang Would please add a new test case to verify the sourceLimit less than the generated insert record's num?", "url": "https://github.com/apache/hudi/pull/1779#discussion_r449744177", "createdAt": "2020-07-04T06:11:38Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestKafkaSource.java", "diffHunk": "@@ -191,13 +191,13 @@ public void testJsonKafkaSourceWithDefaultUpperCap() {\n      */\n     testUtils.sendMessages(TEST_TOPIC_NAME, Helpers.jsonifyRecords(dataGenerator.generateInserts(\"000\", 1000)));\n     InputBatch<JavaRDD<GenericRecord>> fetch1 = kafkaSource.fetchNewDataInAvroFormat(Option.empty(), Long.MAX_VALUE);\n-    assertEquals(500, fetch1.getBatch().get().count());\n+    assertEquals(1000, fetch1.getBatch().get().count());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxODk5MA=="}, "originalCommit": {"oid": "d0f342c18760cd11a782b5aa035179ed2e1181bb"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTc0NDM0Nw==", "bodyText": "Hi @Trevor-zhang Would please add a new test case to verify the sourceLimit less than the generated insert record's num?\n\n@yanghua  ok,i'd like to.", "url": "https://github.com/apache/hudi/pull/1779#discussion_r449744347", "createdAt": "2020-07-04T06:14:44Z", "author": {"login": "Trevor-zhang"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/sources/TestKafkaSource.java", "diffHunk": "@@ -191,13 +191,13 @@ public void testJsonKafkaSourceWithDefaultUpperCap() {\n      */\n     testUtils.sendMessages(TEST_TOPIC_NAME, Helpers.jsonifyRecords(dataGenerator.generateInserts(\"000\", 1000)));\n     InputBatch<JavaRDD<GenericRecord>> fetch1 = kafkaSource.fetchNewDataInAvroFormat(Option.empty(), Long.MAX_VALUE);\n-    assertEquals(500, fetch1.getBatch().get().count());\n+    assertEquals(1000, fetch1.getBatch().get().count());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxODk5MA=="}, "originalCommit": {"oid": "d0f342c18760cd11a782b5aa035179ed2e1181bb"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4523, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}