{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ1NTg2ODAy", "number": 2379, "title": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nsupport a independent clustering spark job to asynchronously clustering  HoodieClustering.java.\nBrief change log\n(for example:)\n\nModify AnnotationLocation checkstyle rule in checkstyle.xml\n\nVerify this pull request\n(Please pick either of the following options)\nThis pull request is a trivial rework / code cleanup without any test coverage.\n(or)\nThis pull request is already covered by existing tests, such as (please describe tests).\n(or)\nThis change added tests and can be verified as follows:\n(example:)\n\nAdded integration tests for end-to-end.\nAdded HoodieClientWriteTest to verify the change.\nManually verified the change by running a job locally.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-12-25T12:35:48Z", "url": "https://github.com/apache/hudi/pull/2379", "merged": true, "mergeCommit": {"oid": "368c1a8f5c36d06ed49706b4afde4a83073a9011"}, "closed": true, "closedAt": "2021-01-10T01:30:17Z", "author": {"login": "lw309637554"}, "timelineItems": {"totalCount": 36, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdozNL5AH2gAyNTQ1NTg2ODAyOjc2ZGJmNGE1N2I4N2UyZjU5Y2M0OWIyZWQyOGY2MTE1YWQ0ZTJlMDU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdunpiQgFqTU2NDgyMTcxOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "76dbf4a57b87e2f59cc49b2ed28f6115ad4e2e05", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/76dbf4a57b87e2f59cc49b2ed28f6115ad4e2e05", "committedDate": "2020-12-22T23:33:46Z", "message": "[HUDI-1481]  add  structured streaming and delta streamer clustering unit test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4ODM0OTAx", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-558834901", "createdAt": "2020-12-25T19:54:42Z", "commit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNVQxOTo1NDo0M1rOILemUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNVQyMDowMjowMlrOILeokA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNjU3OQ==", "bodyText": "I think we need separate methods for scheduling and executing (Similar to compaction). This is because single writer limitation - scheduling can only happen when ingestion is not running. Where as executing clustering plan can happen in parallel to ingestion.", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548906579", "createdAt": "2020-12-25T19:54:43Z", "author": {"login": "satishkotha"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClustering.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClustering {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClustering.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClustering(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = true)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--schema-file\", \"-sf\"}, description = \"path for Avro schema file\", required = true)\n+    public String schemaFile = null;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+\n+    @Parameter(names = {\"--props\"}, description = \"path to properties file on localfs or dfs, with configurations for \"\n+        + \"hoodie client for clustering\")\n+    public String propsFilePath = null;\n+\n+    @Parameter(names = {\"--hoodie-conf\"}, description = \"Any configuration that can be set in the properties file \"\n+        + \"(using the CLI parameter \\\"--props\\\") can also be passed command line using this parameter. This can be repeated\",\n+            splitter = IdentitySplitter.class)\n+    public List<String> configs = new ArrayList<>();\n+  }\n+\n+  public static void main(String[] args) {\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    final JavaSparkContext jsc = UtilHelpers.buildSparkContext(\"clustering-\" + cfg.tableName, cfg.sparkMaster, cfg.sparkMemory);\n+    HoodieClustering clustering = new HoodieClustering(jsc, cfg);\n+    clustering.cluster(cfg.retry);\n+  }\n+\n+  public int cluster(int retry) {\n+    this.fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+    int ret = -1;\n+    try {\n+      do {\n+        ret = scheduleAndCluster(jsc);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNjg3Ng==", "bodyText": "are we missing few porperties like\n\nsort_columns to use for clustering\nschedule clustering strategy\nexecute clustering strategy\nclustering targetFileSize\n\nAre these expected to be in propsFile? Can you give me an example props file that you used to test?", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548906876", "createdAt": "2020-12-25T19:58:11Z", "author": {"login": "satishkotha"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClustering.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClustering {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClustering.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClustering(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = true)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--schema-file\", \"-sf\"}, description = \"path for Avro schema file\", required = true)\n+    public String schemaFile = null;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNjk5MQ==", "bodyText": "were you able to run this? Can you give me an example command line on how you ran this?", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548906991", "createdAt": "2020-12-25T19:59:15Z", "author": {"login": "satishkotha"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClustering.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClustering {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClustering.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClustering(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = true)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--schema-file\", \"-sf\"}, description = \"path for Avro schema file\", required = true)\n+    public String schemaFile = null;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+\n+    @Parameter(names = {\"--props\"}, description = \"path to properties file on localfs or dfs, with configurations for \"\n+        + \"hoodie client for clustering\")\n+    public String propsFilePath = null;\n+\n+    @Parameter(names = {\"--hoodie-conf\"}, description = \"Any configuration that can be set in the properties file \"\n+        + \"(using the CLI parameter \\\"--props\\\") can also be passed command line using this parameter. This can be repeated\",\n+            splitter = IdentitySplitter.class)\n+    public List<String> configs = new ArrayList<>();\n+  }\n+\n+  public static void main(String[] args) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODkwNzE1Mg==", "bodyText": "schedule returns a boolean. We can only run clustering if return value is true. You may want to add that check\nboolean isScheduled =     client.scheduleClusteringAtInstant(cfg.clusteringInstantTime, Option.empty());\nif (isScheduled) {\n// run clustering\n} else {\n// log that there is no enough data to run clustering\n}", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548907152", "createdAt": "2020-12-25T20:02:02Z", "author": {"login": "satishkotha"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClustering.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClustering {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClustering.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClustering(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = true)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--schema-file\", \"-sf\"}, description = \"path for Avro schema file\", required = true)\n+    public String schemaFile = null;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+\n+    @Parameter(names = {\"--props\"}, description = \"path to properties file on localfs or dfs, with configurations for \"\n+        + \"hoodie client for clustering\")\n+    public String propsFilePath = null;\n+\n+    @Parameter(names = {\"--hoodie-conf\"}, description = \"Any configuration that can be set in the properties file \"\n+        + \"(using the CLI parameter \\\"--props\\\") can also be passed command line using this parameter. This can be repeated\",\n+            splitter = IdentitySplitter.class)\n+    public List<String> configs = new ArrayList<>();\n+  }\n+\n+  public static void main(String[] args) {\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    final JavaSparkContext jsc = UtilHelpers.buildSparkContext(\"clustering-\" + cfg.tableName, cfg.sparkMaster, cfg.sparkMemory);\n+    HoodieClustering clustering = new HoodieClustering(jsc, cfg);\n+    clustering.cluster(cfg.retry);\n+  }\n+\n+  public int cluster(int retry) {\n+    this.fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+    int ret = -1;\n+    try {\n+      do {\n+        ret = scheduleAndCluster(jsc);\n+      } while (ret != 0 && retry-- > 0);\n+    } catch (Throwable t) {\n+      LOG.error(t);\n+    }\n+    return ret;\n+  }\n+\n+  private int scheduleAndCluster(JavaSparkContext jsc) throws Exception {\n+    String schemaStr = new Schema.Parser().parse(fs.open(new Path(cfg.schemaFile))).toString();\n+    SparkRDDWriteClient client =\n+        UtilHelpers.createHoodieClient(jsc, cfg.basePath, schemaStr, cfg.parallelism, Option.empty(), props);\n+    client.scheduleClusteringAtInstant(cfg.clusteringInstantTime, Option.empty());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "originalPosition": 123}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4ODQ4NTM4", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-558848538", "createdAt": "2020-12-26T05:44:45Z", "commit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNlQwNTo0NDo0NVrOILhEGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNlQwNTo0NDo0NVrOILhEGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk0Njk3MA==", "bodyText": "revert this change?", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548946970", "createdAt": "2020-12-26T05:44:45Z", "author": {"login": "leesf"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -107,22 +107,23 @@ class TestCOWDataSource extends HoodieClientTestBase {\n       .options(commonOpts)\n       .mode(SaveMode.Append)\n       .save(basePath)\n+    val commitInstantTime2 = HoodieDataSourceHelpers.latestCommit(fs, basePath)\n \n     val snapshotDF2 = spark.read.format(\"hudi\").load(basePath + \"/*/*/*/*\")\n     assertEquals(100, snapshotDF2.count())\n     assertEquals(updatedVerificationVal, snapshotDF2.filter(col(\"_row_key\") === verificationRowKey).select(verificationCol).first.getString(0))\n \n     // Upsert Operation without Hudi metadata columns\n     val records2 = recordsToStrings(dataGen.generateUpdates(\"001\", 100)).toList\n-    val inputDF2 = spark.read.json(spark.sparkContext.parallelize(records2, 2))\n+    val inputDF2 = spark.read.json(spark.sparkContext.parallelize(records2 , 2))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4ODQ4NjM3", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-558848637", "createdAt": "2020-12-26T05:48:14Z", "commit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNlQwNTo0ODoxNFrOILhFXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNlQwNTo0ODoxNFrOILhFXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk0NzI5Mg==", "bodyText": "change to msg?", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548947292", "createdAt": "2020-12-26T05:48:14Z", "author": {"login": "leesf"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestStructuredStreaming.scala", "diffHunk": "@@ -177,4 +188,112 @@ class TestStructuredStreaming extends HoodieClientTestBase {\n     if (!success) throw new IllegalStateException(\"Timed-out waiting for \" + numCommits + \" commits to appear in \" + tablePath)\n     numInstants\n   }\n+\n+  def getInlineClusteringOpts( isInlineClustering: String, clusteringNumCommit: String, fileMaxRecordNum: Int):Map[String, String] = {\n+    commonOpts + (HoodieClusteringConfig.INLINE_CLUSTERING_PROP -> isInlineClustering,\n+      HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP -> clusteringNumCommit,\n+      HoodieStorageConfig.PARQUET_FILE_MAX_BYTES -> dataGen.getEstimatedFileSizeInBytes(fileMaxRecordNum).toString\n+    )\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      // check have schedule clustering and clustering file group to one\n+      waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+      metaClient.reloadActiveTimeline()\n+      assertEquals(1, getLatestFileGroupsFileId.size)\n+    }\n+    structuredStreamingForTestClusteringRunner(sourcePath, destPath, true, checkClusteringResult)\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithoutInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      val msg = \"Should have replace commit completed\"\n+      assertThrows(classOf[IllegalStateException], new Executable {\n+        override def execute(): Unit = {\n+          waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+        }\n+      }\n+        , \"Should have replace commit completed\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "originalPosition": 138}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4ODQ4OTYx", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-558848961", "createdAt": "2020-12-26T06:00:54Z", "commit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNlQwNjowMDo1NVrOILhI3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNlQwNjowMDo1NVrOILhI3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk0ODE4OA==", "bodyText": "would replace the below three HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH with one parameter? it is a little weird to use HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH in getLatestFileGroupsFileId.", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548948188", "createdAt": "2020-12-26T06:00:55Z", "author": {"login": "leesf"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestStructuredStreaming.scala", "diffHunk": "@@ -177,4 +188,112 @@ class TestStructuredStreaming extends HoodieClientTestBase {\n     if (!success) throw new IllegalStateException(\"Timed-out waiting for \" + numCommits + \" commits to appear in \" + tablePath)\n     numInstants\n   }\n+\n+  def getInlineClusteringOpts( isInlineClustering: String, clusteringNumCommit: String, fileMaxRecordNum: Int):Map[String, String] = {\n+    commonOpts + (HoodieClusteringConfig.INLINE_CLUSTERING_PROP -> isInlineClustering,\n+      HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP -> clusteringNumCommit,\n+      HoodieStorageConfig.PARQUET_FILE_MAX_BYTES -> dataGen.getEstimatedFileSizeInBytes(fileMaxRecordNum).toString\n+    )\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      // check have schedule clustering and clustering file group to one\n+      waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+      metaClient.reloadActiveTimeline()\n+      assertEquals(1, getLatestFileGroupsFileId.size)\n+    }\n+    structuredStreamingForTestClusteringRunner(sourcePath, destPath, true, checkClusteringResult)\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithoutInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      val msg = \"Should have replace commit completed\"\n+      assertThrows(classOf[IllegalStateException], new Executable {\n+        override def execute(): Unit = {\n+          waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+        }\n+      }\n+        , \"Should have replace commit completed\")\n+      println(msg)\n+    }\n+    structuredStreamingForTestClusteringRunner(sourcePath, destPath, false, checkClusteringResult)\n+  }\n+\n+  def structuredStreamingForTestClusteringRunner(sourcePath: String, destPath: String,\n+                                           isInlineClustering: Boolean, checkClusteringResult: String => Unit): Unit = {\n+    // First insert of data\n+    val records1 = recordsToStrings(dataGen.generateInsertsForPartition(\"000\", 100, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)).toList", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "originalPosition": 147}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4ODQ5NDE1", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-558849415", "createdAt": "2020-12-26T06:19:35Z", "commit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNlQwNjoxOTozNlrOILhOHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNlQwNjoxOTozNlrOILhOHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk0OTUzMg==", "bodyText": "format", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548949532", "createdAt": "2020-12-26T06:19:36Z", "author": {"login": "leesf"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestStructuredStreaming.scala", "diffHunk": "@@ -177,4 +188,112 @@ class TestStructuredStreaming extends HoodieClientTestBase {\n     if (!success) throw new IllegalStateException(\"Timed-out waiting for \" + numCommits + \" commits to appear in \" + tablePath)\n     numInstants\n   }\n+\n+  def getInlineClusteringOpts( isInlineClustering: String, clusteringNumCommit: String, fileMaxRecordNum: Int):Map[String, String] = {\n+    commonOpts + (HoodieClusteringConfig.INLINE_CLUSTERING_PROP -> isInlineClustering,\n+      HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP -> clusteringNumCommit,\n+      HoodieStorageConfig.PARQUET_FILE_MAX_BYTES -> dataGen.getEstimatedFileSizeInBytes(fileMaxRecordNum).toString\n+    )\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      // check have schedule clustering and clustering file group to one\n+      waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+      metaClient.reloadActiveTimeline()\n+      assertEquals(1, getLatestFileGroupsFileId.size)\n+    }\n+    structuredStreamingForTestClusteringRunner(sourcePath, destPath, true, checkClusteringResult)\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithoutInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      val msg = \"Should have replace commit completed\"\n+      assertThrows(classOf[IllegalStateException], new Executable {\n+        override def execute(): Unit = {\n+          waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+        }\n+      }\n+        , \"Should have replace commit completed\")\n+      println(msg)\n+    }\n+    structuredStreamingForTestClusteringRunner(sourcePath, destPath, false, checkClusteringResult)\n+  }\n+\n+  def structuredStreamingForTestClusteringRunner(sourcePath: String, destPath: String,\n+                                           isInlineClustering: Boolean, checkClusteringResult: String => Unit): Unit = {\n+    // First insert of data\n+    val records1 = recordsToStrings(dataGen.generateInsertsForPartition(\"000\", 100, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)).toList\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(records1, 2))\n+\n+    // Second insert of data\n+    val records2 = recordsToStrings(dataGen.generateInsertsForPartition(\"001\", 100, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)).toList\n+    val inputDF2 = spark.read.json(spark.sparkContext.parallelize(records2, 2))\n+\n+    val hudiOptions = getInlineClusteringOpts(isInlineClustering.toString, \"2\", 100)\n+    val f1 = initStreamingWriteFuture(inputDF1.schema, sourcePath, destPath, hudiOptions)\n+\n+    val f2 = Future {\n+      inputDF1.coalesce(1).write.mode(SaveMode.Append).json(sourcePath)\n+      // wait for spark streaming to process one microbatch\n+      val currNumCommits = waitTillAtleastNCommits(fs, destPath, 1, 120, 5)\n+      assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, destPath, \"000\"))\n+\n+      inputDF2.coalesce(1).write.mode(SaveMode.Append).json(sourcePath)\n+      // wait for spark streaming to process second microbatch\n+      waitTillAtleastNCommits(fs, destPath, currNumCommits + 1, 120, 5)\n+      assertEquals(2, HoodieDataSourceHelpers.listCommitsSince(fs, destPath, \"000\").size())\n+\n+      // check have more than one file group\n+      this.metaClient = new HoodieTableMetaClient(fs.getConf, destPath, true)\n+      assertTrue(getLatestFileGroupsFileId().size > 1)\n+\n+      // check clustering result\n+      checkClusteringResult(destPath)\n+\n+      // check data correct after clustering\n+      val hoodieROViewDF2 = spark.read.format(\"org.apache.hudi\")\n+        .load(destPath + \"/*/*/*/*\")\n+      assertEquals(200, hoodieROViewDF2.count())\n+    }\n+    Await.result(Future.sequence(Seq(f1, f2)), Duration.Inf)\n+  }\n+\n+  private def getLatestFileGroupsFileId():Array[String] = {\n+    getHoodieTableFileSystemView(metaClient, metaClient.getActiveTimeline,\n+      HoodieTestTable.of(metaClient).listAllBaseFiles())\n+    tableView.getLatestFileSlices(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)\n+      .toArray().map(slice => slice.asInstanceOf[FileSlice].getFileGroupId.getFileId)\n+  }\n+\n+  @throws[InterruptedException]\n+  private def waitTillHasCompletedReplaceInstant(tablePath: String,\n+                                                timeoutSecs: Int, sleepSecsAfterEachRun: Int) = {\n+    val beginTime = System.currentTimeMillis\n+    var currTime = beginTime\n+    val timeoutMsecs = timeoutSecs * 1000\n+    var success = false\n+    while ({!success && (currTime - beginTime) < timeoutMsecs}) try {\n+      this.metaClient.reloadActiveTimeline()\n+      val completeReplaceSize = this.metaClient.getActiveTimeline.getCompletedReplaceTimeline().getInstants.toArray.size\n+      println(\"completeReplaceSize:\" + completeReplaceSize)\n+      if(completeReplaceSize > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "originalPosition": 201}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4ODQ5NDI5", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-558849429", "createdAt": "2020-12-26T06:20:18Z", "commit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNlQwNjoyMDoxOFrOILhOPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNlQwNjoyMDoxOFrOILhOPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk0OTU2NA==", "bodyText": "Timed-out waiting for completing replace instant appear in \" + tablePath", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548949564", "createdAt": "2020-12-26T06:20:18Z", "author": {"login": "leesf"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestStructuredStreaming.scala", "diffHunk": "@@ -177,4 +188,112 @@ class TestStructuredStreaming extends HoodieClientTestBase {\n     if (!success) throw new IllegalStateException(\"Timed-out waiting for \" + numCommits + \" commits to appear in \" + tablePath)\n     numInstants\n   }\n+\n+  def getInlineClusteringOpts( isInlineClustering: String, clusteringNumCommit: String, fileMaxRecordNum: Int):Map[String, String] = {\n+    commonOpts + (HoodieClusteringConfig.INLINE_CLUSTERING_PROP -> isInlineClustering,\n+      HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP -> clusteringNumCommit,\n+      HoodieStorageConfig.PARQUET_FILE_MAX_BYTES -> dataGen.getEstimatedFileSizeInBytes(fileMaxRecordNum).toString\n+    )\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      // check have schedule clustering and clustering file group to one\n+      waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+      metaClient.reloadActiveTimeline()\n+      assertEquals(1, getLatestFileGroupsFileId.size)\n+    }\n+    structuredStreamingForTestClusteringRunner(sourcePath, destPath, true, checkClusteringResult)\n+  }\n+\n+  @Test\n+  def testStructuredStreamingWithoutInlineClustering(): Unit = {\n+    val (sourcePath, destPath) = initStreamingSourceAndDestPath(\"source\", \"dest\")\n+\n+    def checkClusteringResult(destPath: String):Unit = {\n+      val msg = \"Should have replace commit completed\"\n+      assertThrows(classOf[IllegalStateException], new Executable {\n+        override def execute(): Unit = {\n+          waitTillHasCompletedReplaceInstant(destPath, 120, 5)\n+        }\n+      }\n+        , \"Should have replace commit completed\")\n+      println(msg)\n+    }\n+    structuredStreamingForTestClusteringRunner(sourcePath, destPath, false, checkClusteringResult)\n+  }\n+\n+  def structuredStreamingForTestClusteringRunner(sourcePath: String, destPath: String,\n+                                           isInlineClustering: Boolean, checkClusteringResult: String => Unit): Unit = {\n+    // First insert of data\n+    val records1 = recordsToStrings(dataGen.generateInsertsForPartition(\"000\", 100, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)).toList\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(records1, 2))\n+\n+    // Second insert of data\n+    val records2 = recordsToStrings(dataGen.generateInsertsForPartition(\"001\", 100, HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)).toList\n+    val inputDF2 = spark.read.json(spark.sparkContext.parallelize(records2, 2))\n+\n+    val hudiOptions = getInlineClusteringOpts(isInlineClustering.toString, \"2\", 100)\n+    val f1 = initStreamingWriteFuture(inputDF1.schema, sourcePath, destPath, hudiOptions)\n+\n+    val f2 = Future {\n+      inputDF1.coalesce(1).write.mode(SaveMode.Append).json(sourcePath)\n+      // wait for spark streaming to process one microbatch\n+      val currNumCommits = waitTillAtleastNCommits(fs, destPath, 1, 120, 5)\n+      assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, destPath, \"000\"))\n+\n+      inputDF2.coalesce(1).write.mode(SaveMode.Append).json(sourcePath)\n+      // wait for spark streaming to process second microbatch\n+      waitTillAtleastNCommits(fs, destPath, currNumCommits + 1, 120, 5)\n+      assertEquals(2, HoodieDataSourceHelpers.listCommitsSince(fs, destPath, \"000\").size())\n+\n+      // check have more than one file group\n+      this.metaClient = new HoodieTableMetaClient(fs.getConf, destPath, true)\n+      assertTrue(getLatestFileGroupsFileId().size > 1)\n+\n+      // check clustering result\n+      checkClusteringResult(destPath)\n+\n+      // check data correct after clustering\n+      val hoodieROViewDF2 = spark.read.format(\"org.apache.hudi\")\n+        .load(destPath + \"/*/*/*/*\")\n+      assertEquals(200, hoodieROViewDF2.count())\n+    }\n+    Await.result(Future.sequence(Seq(f1, f2)), Duration.Inf)\n+  }\n+\n+  private def getLatestFileGroupsFileId():Array[String] = {\n+    getHoodieTableFileSystemView(metaClient, metaClient.getActiveTimeline,\n+      HoodieTestTable.of(metaClient).listAllBaseFiles())\n+    tableView.getLatestFileSlices(HoodieTestDataGenerator.DEFAULT_FIRST_PARTITION_PATH)\n+      .toArray().map(slice => slice.asInstanceOf[FileSlice].getFileGroupId.getFileId)\n+  }\n+\n+  @throws[InterruptedException]\n+  private def waitTillHasCompletedReplaceInstant(tablePath: String,\n+                                                timeoutSecs: Int, sleepSecsAfterEachRun: Int) = {\n+    val beginTime = System.currentTimeMillis\n+    var currTime = beginTime\n+    val timeoutMsecs = timeoutSecs * 1000\n+    var success = false\n+    while ({!success && (currTime - beginTime) < timeoutMsecs}) try {\n+      this.metaClient.reloadActiveTimeline()\n+      val completeReplaceSize = this.metaClient.getActiveTimeline.getCompletedReplaceTimeline().getInstants.toArray.size\n+      println(\"completeReplaceSize:\" + completeReplaceSize)\n+      if(completeReplaceSize > 0) {\n+        success = true\n+      }\n+    } catch {\n+      case te: TableNotFoundException =>\n+        log.info(\"Got table not found exception. Retrying\")\n+    } finally {\n+      Thread.sleep(sleepSecsAfterEachRun * 1000)\n+      currTime = System.currentTimeMillis\n+    }\n+    if (!success) throw new IllegalStateException(\"Timed-out waiting for \"  + \" have completed replace instant appear in \" + tablePath)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "originalPosition": 211}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4ODQ5NDcx", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-558849471", "createdAt": "2020-12-26T06:22:18Z", "commit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNlQwNjoyMjoxOFrOILhOqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yNlQwNjoyMjoxOFrOILhOqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODk0OTY3Mw==", "bodyText": "would rename to HoodieClusterer?", "url": "https://github.com/apache/hudi/pull/2379#discussion_r548949673", "createdAt": "2020-12-26T06:22:18Z", "author": {"login": "leesf"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClustering.java", "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClustering {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975"}, "originalPosition": 40}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "80a946a4ad9f352b4c45d23307a933cb646c6880", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/80a946a4ad9f352b4c45d23307a933cb646c6880", "committedDate": "2020-12-27T10:49:16Z", "message": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "484f36ba2c739788c9b861b67f01508bf6503975", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/484f36ba2c739788c9b861b67f01508bf6503975", "committedDate": "2020-12-25T12:34:13Z", "message": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering"}, "afterCommit": {"oid": "80a946a4ad9f352b4c45d23307a933cb646c6880", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/80a946a4ad9f352b4c45d23307a933cb646c6880", "committedDate": "2020-12-27T10:49:16Z", "message": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9dfdb716d1c0e0b765591b16749be23c25f47672", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/9dfdb716d1c0e0b765591b16749be23c25f47672", "committedDate": "2020-12-28T06:52:17Z", "message": "Merge remote-tracking branch 'upstream/master' into  HUDI-1399"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1ecb22ea13b3560d272e7e2c5cae100036c8f020", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/1ecb22ea13b3560d272e7e2c5cae100036c8f020", "committedDate": "2020-12-28T05:52:55Z", "message": "Merge remote-tracking branch 'upstream/master' into HUDI-1399"}, "afterCommit": {"oid": "9dfdb716d1c0e0b765591b16749be23c25f47672", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/9dfdb716d1c0e0b765591b16749be23c25f47672", "committedDate": "2020-12-28T06:52:17Z", "message": "Merge remote-tracking branch 'upstream/master' into  HUDI-1399"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "eead0281d7c38b0c7523c265ff62922ab83ea2b2", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/eead0281d7c38b0c7523c265ff62922ab83ea2b2", "committedDate": "2020-12-28T13:22:25Z", "message": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering"}, "afterCommit": {"oid": "e9149d9d169757503c97cbcaf263a0ec5d24596d", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/e9149d9d169757503c97cbcaf263a0ec5d24596d", "committedDate": "2020-12-28T14:56:48Z", "message": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e9149d9d169757503c97cbcaf263a0ec5d24596d", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/e9149d9d169757503c97cbcaf263a0ec5d24596d", "committedDate": "2020-12-28T14:56:48Z", "message": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering"}, "afterCommit": {"oid": "c825980fa8dd4fd2b6ca55cecf8948e8cfa2c28e", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/c825980fa8dd4fd2b6ca55cecf8948e8cfa2c28e", "committedDate": "2020-12-29T03:40:35Z", "message": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c825980fa8dd4fd2b6ca55cecf8948e8cfa2c28e", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/c825980fa8dd4fd2b6ca55cecf8948e8cfa2c28e", "committedDate": "2020-12-29T03:40:35Z", "message": "[HUDI-1399] support a independent clustering spark job to asynchronously clustering"}, "afterCommit": {"oid": "44d0c65d07367065e010d84c06f0a5370d37b862", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/44d0c65d07367065e010d84c06f0a5370d37b862", "committedDate": "2020-12-29T05:07:42Z", "message": "[HUDI-1399] support  a independent clustering spark job to asynchronously clustering"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "44d0c65d07367065e010d84c06f0a5370d37b862", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/44d0c65d07367065e010d84c06f0a5370d37b862", "committedDate": "2020-12-29T05:07:42Z", "message": "[HUDI-1399] support  a independent clustering spark job to asynchronously clustering"}, "afterCommit": {"oid": "e457785c1c81f7e6a5e9709607d2b18e7e5b75d5", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/e457785c1c81f7e6a5e9709607d2b18e7e5b75d5", "committedDate": "2020-12-29T06:12:35Z", "message": "[HUDI-1399]  support  a independent clustering spark job to asynchronously clustering"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e457785c1c81f7e6a5e9709607d2b18e7e5b75d5", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/e457785c1c81f7e6a5e9709607d2b18e7e5b75d5", "committedDate": "2020-12-29T06:12:35Z", "message": "[HUDI-1399]  support  a independent clustering spark job to asynchronously clustering"}, "afterCommit": {"oid": "370be89f339280cd96bf89a18d3aab7545d3a6fe", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/370be89f339280cd96bf89a18d3aab7545d3a6fe", "committedDate": "2020-12-29T07:08:51Z", "message": "[HUDI-1399]  support a  independent clustering spark job to asynchronously clustering"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5Njc0MDc1", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-559674075", "createdAt": "2020-12-29T19:38:55Z", "commit": {"oid": "370be89f339280cd96bf89a18d3aab7545d3a6fe"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQxOTozODo1NVrOIMWm8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQxOTo0NzoyOFrOIMWvyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTgyNDI0Mg==", "bodyText": "hmm, this is actually tricky. what if the replace_commit belongs to insert_overwrite? We have to rollback insert_overwrite related replacecommits, but not clustering related ones.\nWe could move this method out of timeline into a utils method and parse content to filter only clustering instants.", "url": "https://github.com/apache/hudi/pull/2379#discussion_r549824242", "createdAt": "2020-12-29T19:38:55Z", "author": {"login": "satishkotha"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/timeline/HoodieDefaultTimeline.java", "diffHunk": "@@ -95,6 +95,13 @@ public HoodieTimeline filterPendingExcludingCompaction() {\n             && (!instant.getAction().equals(HoodieTimeline.COMPACTION_ACTION))), details);\n   }\n \n+  @Override\n+  public HoodieTimeline filterPendingExcludingCompactionAndClustering() {\n+    return new HoodieDefaultTimeline(instants.stream().filter(instant -> (!instant.isCompleted())\n+        && (!instant.getAction().equals(HoodieTimeline.COMPACTION_ACTION))\n+        && (!instant.getAction().equals(HoodieTimeline.REPLACE_COMMIT_ACTION))), details);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "370be89f339280cd96bf89a18d3aab7545d3a6fe"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTgyNTQ2Mg==", "bodyText": "can we get schema from latest commit instead of requiring additional config to simplify?", "url": "https://github.com/apache/hudi/pull/2379#discussion_r549825462", "createdAt": "2020-12-29T19:43:53Z", "author": {"login": "satishkotha"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClusteringJob {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClusteringJob.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClusteringJob(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = false)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--schema-file\", \"-sf\"}, description = \"path for Avro schema file\", required = true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "370be89f339280cd96bf89a18d3aab7545d3a6fe"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTgyNjUwNA==", "bodyText": "I think you created a ticket for this TODO? could you link the ticket here?", "url": "https://github.com/apache/hudi/pull/2379#discussion_r549826504", "createdAt": "2020-12-29T19:47:28Z", "author": {"login": "satishkotha"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java", "diffHunk": "@@ -296,7 +296,9 @@ public void refreshTimeline() throws IOException {\n     // Retrieve the previous round checkpoints, if any\n     Option<String> resumeCheckpointStr = Option.empty();\n     if (commitTimelineOpt.isPresent()) {\n-      Option<HoodieInstant> lastCommit = commitTimelineOpt.get().lastInstant();\n+      // TODO: now not support replace action", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "370be89f339280cd96bf89a18d3aab7545d3a6fe"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwMjE0NDQ4", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-560214448", "createdAt": "2020-12-30T22:06:02Z", "commit": {"oid": "370be89f339280cd96bf89a18d3aab7545d3a6fe"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQyMjowNjowMlrOIM2GFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQyMjowNjowMlrOIM2GFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDM0MDExOA==", "bodyText": "@lw309637554 @satishkotha Is this supposed to be a async service that can run in the deltastreamer / independently ? If yes, we should implement this as an HoodieAsyncService. Take a look at an example async cleaner service here -> https://github.com/apache/hudi/blob/master/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AsyncCleanerService.java", "url": "https://github.com/apache/hudi/pull/2379#discussion_r550340118", "createdAt": "2020-12-30T22:06:02Z", "author": {"login": "n3nash"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClusteringJob {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "370be89f339280cd96bf89a18d3aab7545d3a6fe"}, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwMjE0NjAx", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-560214601", "createdAt": "2020-12-30T22:06:41Z", "commit": {"oid": "370be89f339280cd96bf89a18d3aab7545d3a6fe"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "370be89f339280cd96bf89a18d3aab7545d3a6fe", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/370be89f339280cd96bf89a18d3aab7545d3a6fe", "committedDate": "2020-12-29T07:08:51Z", "message": "[HUDI-1399]  support a  independent clustering spark job to asynchronously clustering"}, "afterCommit": {"oid": "3dde53db5afb4e859b8c8c07faa62c9330c4feb6", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/3dde53db5afb4e859b8c8c07faa62c9330c4feb6", "committedDate": "2020-12-31T03:18:44Z", "message": "[HUDI-1399]  support a  independent clustering spark job to asynchronously clustering"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "committedDate": "2020-12-31T05:06:52Z", "message": "[HUDI-1399]  support a  independent clustering spark job to asynchronously clustering"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3dde53db5afb4e859b8c8c07faa62c9330c4feb6", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/3dde53db5afb4e859b8c8c07faa62c9330c4feb6", "committedDate": "2020-12-31T03:18:44Z", "message": "[HUDI-1399]  support a  independent clustering spark job to asynchronously clustering"}, "afterCommit": {"oid": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39", "committedDate": "2020-12-31T05:06:52Z", "message": "[HUDI-1399]  support a  independent clustering spark job to asynchronously clustering"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxNDE3MjM5", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-561417239", "createdAt": "2021-01-04T23:25:42Z", "commit": {"oid": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMzoyNTo0MlrOIOEsNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMzo0NDoxMlrOIOFJhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYyNzgzMA==", "bodyText": "i think its better to create 'instantTime' instead of using user-specified instant time for scheduling. At least, we should validate instant time specified is greater than latest commit on the table. Otherwise, there can be unexpected side-effects.", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551627830", "createdAt": "2021-01-04T23:25:42Z", "author": {"login": "satishkotha"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClusteringJob {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClusteringJob.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClusteringJob(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = false)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+\n+    @Parameter(names = {\"--schedule\", \"-sc\"}, description = \"Schedule clustering\")\n+    public Boolean runSchedule = false;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+\n+    @Parameter(names = {\"--props\"}, description = \"path to properties file on localfs or dfs, with configurations for \"\n+        + \"hoodie client for clustering\")\n+    public String propsFilePath = null;\n+\n+    @Parameter(names = {\"--hoodie-conf\"}, description = \"Any configuration that can be set in the properties file \"\n+        + \"(using the CLI parameter \\\"--props\\\") can also be passed command line using this parameter. This can be repeated\",\n+            splitter = IdentitySplitter.class)\n+    public List<String> configs = new ArrayList<>();\n+  }\n+\n+  public static void main(String[] args) {\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    final JavaSparkContext jsc = UtilHelpers.buildSparkContext(\"clustering-\" + cfg.tableName, cfg.sparkMaster, cfg.sparkMemory);\n+    HoodieClusteringJob clusteringJob = new HoodieClusteringJob(jsc, cfg);\n+    int result = clusteringJob.cluster(cfg.retry);\n+    String resultMsg = String.format(\"Clustering with basePath: %s, tableName: %s, runSchedule: %s, clusteringInstantTime: %s\",\n+        cfg.basePath, cfg.tableName, cfg.runSchedule, cfg.clusteringInstantTime);\n+    if (result == -1) {\n+      LOG.error(resultMsg + \" failed\");\n+    } else {\n+      LOG.info(resultMsg + \" success\");\n+    }\n+    jsc.stop();\n+  }\n+\n+  public int cluster(int retry) {\n+    this.fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+    int ret = -1;\n+    try {\n+      do {\n+        if (cfg.runSchedule) {\n+          LOG.info(\"Do schedule\");\n+          ret = doSchedule(jsc);\n+        } else {\n+          LOG.info(\"Do cluster\");\n+          ret = doCluster(jsc);\n+        }\n+      } while (ret != 0 && retry-- > 0);\n+    } catch (Throwable t) {\n+      LOG.error(\"Cluster failed\", t);\n+    }\n+    return ret;\n+  }\n+\n+  private String getSchemaFromLatestInstant() throws Exception {\n+    HoodieTableMetaClient metaClient =  new HoodieTableMetaClient(jsc.hadoopConfiguration(), cfg.basePath, true);\n+    TableSchemaResolver schemaUtil = new TableSchemaResolver(metaClient);\n+    Schema schema = schemaUtil.getTableAvroSchema(false);\n+    return schema.toString();\n+  }\n+\n+  private int doCluster(JavaSparkContext jsc) throws Exception {\n+    String schemaStr = getSchemaFromLatestInstant();\n+    SparkRDDWriteClient client =\n+        UtilHelpers.createHoodieClient(jsc, cfg.basePath, schemaStr, cfg.parallelism, Option.empty(), props);\n+    JavaRDD<WriteStatus> writeResponse =\n+        (JavaRDD<WriteStatus>) client.cluster(cfg.clusteringInstantTime, true).getWriteStatuses();\n+    return UtilHelpers.handleErrors(jsc, cfg.clusteringInstantTime, writeResponse);\n+  }\n+\n+  private int doSchedule(JavaSparkContext jsc) throws Exception {\n+    String schemaStr = getSchemaFromLatestInstant();\n+    SparkRDDWriteClient client =\n+        UtilHelpers.createHoodieClient(jsc, cfg.basePath, schemaStr, cfg.parallelism, Option.empty(), props);\n+    return client.scheduleClusteringAtInstant(cfg.clusteringInstantTime, Option.empty()) ? 0 : -1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzMDkyMA==", "bodyText": "This is probably minor. Looks like 'getTableAvroSchema' throws runtime exception if there are no completed commits. may be add a check to verify commit timeline is not empty before calling this method? This way we can throw meaningful error message: say \"Cannot run clustering without any completed commits\"", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551630920", "createdAt": "2021-01-04T23:35:03Z", "author": {"login": "satishkotha"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClusteringJob {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClusteringJob.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClusteringJob(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = false)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+\n+    @Parameter(names = {\"--schedule\", \"-sc\"}, description = \"Schedule clustering\")\n+    public Boolean runSchedule = false;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+\n+    @Parameter(names = {\"--props\"}, description = \"path to properties file on localfs or dfs, with configurations for \"\n+        + \"hoodie client for clustering\")\n+    public String propsFilePath = null;\n+\n+    @Parameter(names = {\"--hoodie-conf\"}, description = \"Any configuration that can be set in the properties file \"\n+        + \"(using the CLI parameter \\\"--props\\\") can also be passed command line using this parameter. This can be repeated\",\n+            splitter = IdentitySplitter.class)\n+    public List<String> configs = new ArrayList<>();\n+  }\n+\n+  public static void main(String[] args) {\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    final JavaSparkContext jsc = UtilHelpers.buildSparkContext(\"clustering-\" + cfg.tableName, cfg.sparkMaster, cfg.sparkMemory);\n+    HoodieClusteringJob clusteringJob = new HoodieClusteringJob(jsc, cfg);\n+    int result = clusteringJob.cluster(cfg.retry);\n+    String resultMsg = String.format(\"Clustering with basePath: %s, tableName: %s, runSchedule: %s, clusteringInstantTime: %s\",\n+        cfg.basePath, cfg.tableName, cfg.runSchedule, cfg.clusteringInstantTime);\n+    if (result == -1) {\n+      LOG.error(resultMsg + \" failed\");\n+    } else {\n+      LOG.info(resultMsg + \" success\");\n+    }\n+    jsc.stop();\n+  }\n+\n+  public int cluster(int retry) {\n+    this.fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+    int ret = -1;\n+    try {\n+      do {\n+        if (cfg.runSchedule) {\n+          LOG.info(\"Do schedule\");\n+          ret = doSchedule(jsc);\n+        } else {\n+          LOG.info(\"Do cluster\");\n+          ret = doCluster(jsc);\n+        }\n+      } while (ret != 0 && retry-- > 0);\n+    } catch (Throwable t) {\n+      LOG.error(\"Cluster failed\", t);\n+    }\n+    return ret;\n+  }\n+\n+  private String getSchemaFromLatestInstant() throws Exception {\n+    HoodieTableMetaClient metaClient =  new HoodieTableMetaClient(jsc.hadoopConfiguration(), cfg.basePath, true);\n+    TableSchemaResolver schemaUtil = new TableSchemaResolver(metaClient);\n+    Schema schema = schemaUtil.getTableAvroSchema(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39"}, "originalPosition": 139}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzMTEyNw==", "bodyText": "I'd suggest make this required only for executing clustering. For scheduling, we should auto-generate instant time to make it easy to run.", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551631127", "createdAt": "2021-01-04T23:35:42Z", "author": {"login": "satishkotha"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClusteringJob {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClusteringJob.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClusteringJob(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time\", required = true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzNTMzNQ==", "bodyText": "I dont see any asserts related to clustering. Can we add basic validation that new files generated by clustering have same data as previous files? Let me know if I'm misreading.", "url": "https://github.com/apache/hudi/pull/2379#discussion_r551635335", "createdAt": "2021-01-04T23:44:12Z", "author": {"login": "satishkotha"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java", "diffHunk": "@@ -682,6 +693,58 @@ public void testInlineClustering() throws Exception {\n     });\n   }\n \n+  private HoodieClusteringJob.Config buildHoodieClusteringUtilConfig(String basePath,\n+                                                                  String clusteringInstantTime, boolean runSchedule) {\n+    HoodieClusteringJob.Config config = new HoodieClusteringJob.Config();\n+    config.basePath = basePath;\n+    config.clusteringInstantTime = clusteringInstantTime;\n+    config.runSchedule = runSchedule;\n+    config.propsFilePath = dfsBasePath + \"/clusteringjob.properties\";\n+    return config;\n+  }\n+\n+  @Test\n+  public void testHoodieAsyncClusteringJob() throws Exception {\n+    String tableBasePath = dfsBasePath + \"/asyncClustering\";\n+    // Keep it higher than batch-size to test continuous mode\n+    int totalRecords = 3000;\n+\n+    // Initial bulk insert\n+    HoodieDeltaStreamer.Config cfg = TestHelpers.makeConfig(tableBasePath, WriteOperationType.UPSERT);\n+    cfg.continuousMode = true;\n+    cfg.tableType = HoodieTableType.COPY_ON_WRITE.name();\n+    cfg.configs.add(String.format(\"%s=%d\", SourceConfigs.MAX_UNIQUE_RECORDS_PROP, totalRecords));\n+    cfg.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n+    cfg.configs.add(String.format(\"%s=true\", HoodieClusteringConfig.ASYNC_CLUSTERING_ENABLE_OPT_KEY));\n+    HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);\n+    deltaStreamerTestRunner(ds, cfg, (r) -> {\n+      TestHelpers.assertAtLeastNCommits(2, tableBasePath, dfs);\n+      // for not confiict with delta streamer commit, just add 3600s\n+      String clusterInstantTime = HoodieActiveTimeline.COMMIT_FORMATTER\n+          .format(new Date(System.currentTimeMillis() + 3600 * 1000));\n+      LOG.info(\"Cluster instant time \" + clusterInstantTime);\n+      HoodieClusteringJob.Config scheduleClusteringConfig = buildHoodieClusteringUtilConfig(tableBasePath,\n+          clusterInstantTime, true);\n+      HoodieClusteringJob scheduleClusteringJob = new HoodieClusteringJob(jsc, scheduleClusteringConfig);\n+      int scheduleClusteringResult = scheduleClusteringJob.cluster(scheduleClusteringConfig.retry);\n+      if (scheduleClusteringResult == 0) {\n+        LOG.info(\"Schedule clustering success, now cluster\");\n+        HoodieClusteringJob.Config clusterClusteringConfig = buildHoodieClusteringUtilConfig(tableBasePath,\n+            clusterInstantTime, false);\n+        HoodieClusteringJob clusterClusteringJob = new HoodieClusteringJob(jsc, clusterClusteringConfig);\n+        clusterClusteringJob.cluster(clusterClusteringConfig.retry);\n+        LOG.info(\"Cluster success\");\n+      } else {\n+        LOG.warn(\"Schedule clustering failed\");\n+      }\n+      HoodieTableMetaClient metaClient =  new HoodieTableMetaClient(this.dfs.getConf(), tableBasePath, true);\n+      int pendingReplaceSize = metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().toArray().length;\n+      int completeReplaceSize = metaClient.getActiveTimeline().getCompletedReplaceTimeline().getInstants().toArray().length;\n+      System.out.println(\"PendingReplaceSize=\" + pendingReplaceSize + \",completeReplaceSize = \" + completeReplaceSize);\n+      return completeReplaceSize > 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "369dc27e7c5d68a63ea6c5eaa09f91d09ef5dc39"}, "originalPosition": 140}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "152ea1c63a7759168a20f1c50e6441c6246e9619", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/152ea1c63a7759168a20f1c50e6441c6246e9619", "committedDate": "2021-01-05T02:30:19Z", "message": "[HUDI-1498] Read clustering plan from requested file for inflight instant (#2389)"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "70ffbbad0c7db6408599a66d078f61b51cd98409", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/70ffbbad0c7db6408599a66d078f61b51cd98409", "committedDate": "2021-01-05T11:59:13Z", "message": "[HUDI-1399] support a independent clustering spark job with schedule generate instant time"}, "afterCommit": {"oid": "4cc4b350d51240a866e71b16b92df13348ceeefe", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/4cc4b350d51240a866e71b16b92df13348ceeefe", "committedDate": "2021-01-05T14:40:28Z", "message": "[HUDI-1399]  support a independent clustering spark job with schedule generate instant time"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyMDU0NDgz", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-562054483", "createdAt": "2021-01-05T19:05:10Z", "commit": {"oid": "4cc4b350d51240a866e71b16b92df13348ceeefe"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQxOTowNToxMVrOIOjj3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQxOTowNjozMVrOIOjmqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjEzMzU5OA==", "bodyText": "Can you change this to a LOG message?", "url": "https://github.com/apache/hudi/pull/2379#discussion_r552133598", "createdAt": "2021-01-05T19:05:11Z", "author": {"login": "satishkotha"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -109,6 +111,9 @@ public static void main(String[] args) {\n     if (result == -1) {\n       LOG.error(resultMsg + \" failed\");\n     } else {\n+      if (cfg.runSchedule) {\n+        System.out.println(\"The schedule instant time is \" + cfg.clusteringInstantTime);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4cc4b350d51240a866e71b16b92df13348ceeefe"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjEzNDMxMw==", "bodyText": "changing config at this stage seems a little awkward. Do you think its better to return Option instantTime from this method? Or maybe just add the log line here with instantTime?", "url": "https://github.com/apache/hudi/pull/2379#discussion_r552134313", "createdAt": "2021-01-05T19:06:31Z", "author": {"login": "satishkotha"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -153,7 +161,12 @@ private int doSchedule(JavaSparkContext jsc) throws Exception {\n     String schemaStr = getSchemaFromLatestInstant();\n     SparkRDDWriteClient client =\n         UtilHelpers.createHoodieClient(jsc, cfg.basePath, schemaStr, cfg.parallelism, Option.empty(), props);\n-    return client.scheduleClusteringAtInstant(cfg.clusteringInstantTime, Option.empty()) ? 0 : -1;\n+    Option<String> instantTime = client.scheduleClustering(Option.empty());\n+    if (instantTime.isPresent()) {\n+      cfg.clusteringInstantTime = instantTime.get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4cc4b350d51240a866e71b16b92df13348ceeefe"}, "originalPosition": 54}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4cc4b350d51240a866e71b16b92df13348ceeefe", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/4cc4b350d51240a866e71b16b92df13348ceeefe", "committedDate": "2021-01-05T14:40:28Z", "message": "[HUDI-1399]  support a independent clustering spark job with schedule generate instant time"}, "afterCommit": {"oid": "d53595e73e60a2f9cf799d8eeaba00416dd336ba", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/d53595e73e60a2f9cf799d8eeaba00416dd336ba", "committedDate": "2021-01-06T02:30:56Z", "message": "[HUDI-1399]  support a independent clustering spark job with schedule generate instant time"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d53595e73e60a2f9cf799d8eeaba00416dd336ba", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/d53595e73e60a2f9cf799d8eeaba00416dd336ba", "committedDate": "2021-01-06T02:30:56Z", "message": "[HUDI-1399]  support a independent clustering spark job with schedule generate instant time"}, "afterCommit": {"oid": "b165739d03ebe8fc393e069cfd099a8e1098dead", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/b165739d03ebe8fc393e069cfd099a8e1098dead", "committedDate": "2021-01-06T07:27:34Z", "message": "[HUDI-1399]  support  a independent clustering spark job with schedule generate instant time"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY0Njg5NTI5", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-564689529", "createdAt": "2021-01-09T06:49:43Z", "commit": {"oid": "b165739d03ebe8fc393e069cfd099a8e1098dead"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOVQwNjo0OTo0M1rOIQn3HQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOVQwNjo0OTo0M1rOIQn3HQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDMwMTIxMw==", "bodyText": "@lw309637554 Can you add this API to the HoodieTimeline ? You can name it filterPendingExcludingCompactionAndReplace for now.", "url": "https://github.com/apache/hudi/pull/2379#discussion_r554301213", "createdAt": "2021-01-09T06:49:43Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -711,12 +711,30 @@ public void rollbackInflightCompaction(HoodieInstant inflightInstant, HoodieTabl\n     table.getActiveTimeline().revertCompactionInflightToRequested(inflightInstant);\n   }\n \n+  /**\n+   * Get inflight time line exclude compaction and clustering.\n+   * @param table\n+   * @return\n+   */\n+  private HoodieTimeline getInflightTimelineExcludeCompactionAndClustering(HoodieTable<T, I, K, O> table) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b165739d03ebe8fc393e069cfd099a8e1098dead"}, "originalPosition": 9}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY0Njg5NzQx", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-564689741", "createdAt": "2021-01-09T06:54:56Z", "commit": {"oid": "b165739d03ebe8fc393e069cfd099a8e1098dead"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOVQwNjo1NDo1N1rOIQn44Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOVQwNjo1NDo1N1rOIQn44Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDMwMTY2NQ==", "bodyText": "This code about retry is repeated in HoodieCompactor as well, is there a way to reuse this using some kinds of Utils class ?", "url": "https://github.com/apache/hudi/pull/2379#discussion_r554301665", "createdAt": "2021-01-09T06:54:57Z", "author": {"login": "n3nash"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HoodieClusteringJob.java", "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.utilities;\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.TableSchemaResolver;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.jetbrains.annotations.TestOnly;\n+\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieClusteringJob {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieClusteringJob.class);\n+  private final Config cfg;\n+  private transient FileSystem fs;\n+  private TypedProperties props;\n+  private final JavaSparkContext jsc;\n+\n+  public HoodieClusteringJob(JavaSparkContext jsc, Config cfg) {\n+    this.cfg = cfg;\n+    this.jsc = jsc;\n+    this.props = cfg.propsFilePath == null\n+        ? UtilHelpers.buildProperties(cfg.configs)\n+        : readConfigFromFileSystem(jsc, cfg);\n+  }\n+\n+  private TypedProperties readConfigFromFileSystem(JavaSparkContext jsc, Config cfg) {\n+    final FileSystem fs = FSUtils.getFs(cfg.basePath, jsc.hadoopConfiguration());\n+\n+    return UtilHelpers\n+        .readConfig(fs, new Path(cfg.propsFilePath), cfg.configs)\n+        .getConfig();\n+  }\n+\n+  public static class Config implements Serializable {\n+    @Parameter(names = {\"--base-path\", \"-sp\"}, description = \"Base path for the table\", required = true)\n+    public String basePath = null;\n+    @Parameter(names = {\"--table-name\", \"-tn\"}, description = \"Table name\", required = true)\n+    public String tableName = null;\n+    @Parameter(names = {\"--instant-time\", \"-it\"}, description = \"Clustering Instant time, only need when cluster. \"\n+        + \"And schedule clustering can generate it.\", required = false)\n+    public String clusteringInstantTime = null;\n+    @Parameter(names = {\"--parallelism\", \"-pl\"}, description = \"Parallelism for hoodie insert\", required = false)\n+    public int parallelism = 1;\n+    @Parameter(names = {\"--spark-master\", \"-ms\"}, description = \"Spark master\", required = false)\n+    public String sparkMaster = null;\n+    @Parameter(names = {\"--spark-memory\", \"-sm\"}, description = \"spark memory to use\", required = true)\n+    public String sparkMemory = null;\n+    @Parameter(names = {\"--retry\", \"-rt\"}, description = \"number of retries\", required = false)\n+    public int retry = 0;\n+\n+    @Parameter(names = {\"--schedule\", \"-sc\"}, description = \"Schedule clustering\")\n+    public Boolean runSchedule = false;\n+    @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+    public Boolean help = false;\n+\n+    @Parameter(names = {\"--props\"}, description = \"path to properties file on localfs or dfs, with configurations for \"\n+        + \"hoodie client for clustering\")\n+    public String propsFilePath = null;\n+\n+    @Parameter(names = {\"--hoodie-conf\"}, description = \"Any configuration that can be set in the properties file \"\n+        + \"(using the CLI parameter \\\"--props\\\") can also be passed command line using this parameter. This can be repeated\",\n+            splitter = IdentitySplitter.class)\n+    public List<String> configs = new ArrayList<>();\n+  }\n+\n+  public static void main(String[] args) {\n+    final Config cfg = new Config();\n+    JCommander cmd = new JCommander(cfg, null, args);\n+    if (cfg.help || args.length == 0 || (!cfg.runSchedule && cfg.clusteringInstantTime == null)) {\n+      cmd.usage();\n+      System.exit(1);\n+    }\n+    final JavaSparkContext jsc = UtilHelpers.buildSparkContext(\"clustering-\" + cfg.tableName, cfg.sparkMaster, cfg.sparkMemory);\n+    HoodieClusteringJob clusteringJob = new HoodieClusteringJob(jsc, cfg);\n+    int result = clusteringJob.cluster(cfg.retry);\n+    String resultMsg = String.format(\"Clustering with basePath: %s, tableName: %s, runSchedule: %s\",\n+        cfg.basePath, cfg.tableName, cfg.runSchedule);\n+    if (result == -1) {\n+      LOG.error(resultMsg + \" failed\");\n+    } else {\n+      LOG.info(resultMsg + \" success\");\n+    }\n+    jsc.stop();\n+  }\n+\n+  public int cluster(int retry) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b165739d03ebe8fc393e069cfd099a8e1098dead"}, "originalPosition": 120}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY0Njg5OTEy", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-564689912", "createdAt": "2021-01-09T06:58:19Z", "commit": {"oid": "b165739d03ebe8fc393e069cfd099a8e1098dead"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOVQwNjo1ODoxOVrOIQn6Rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOVQwNjo1ODoxOVrOIQn6Rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDMwMjAyMw==", "bodyText": "Why is this specific test case in TestHoodieDeltaStreamer ? Is the intention to just test if clustering works with deltastreamer ? I'm worried this is polluting the tests in hoodie delta streamer, is this test case to be refactored after deltastreamer natively supports clustering ? If yes, can we open a ticket for the follow up..", "url": "https://github.com/apache/hudi/pull/2379#discussion_r554302023", "createdAt": "2021-01-09T06:58:19Z", "author": {"login": "n3nash"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java", "diffHunk": "@@ -672,14 +681,72 @@ public void testInlineClustering() throws Exception {\n     cfg.configs.add(String.format(\"%s=false\", HoodieCompactionConfig.AUTO_CLEAN_PROP));\n     cfg.configs.add(String.format(\"%s=%s\", HoodieClusteringConfig.INLINE_CLUSTERING_PROP, \"true\"));\n     cfg.configs.add(String.format(\"%s=%s\", HoodieClusteringConfig.INLINE_CLUSTERING_MAX_COMMIT_PROP, \"2\"));\n-\n-    deltaStreamerTestRunner(cfg, (r) -> {\n+    HoodieDeltaStreamer ds = new HoodieDeltaStreamer(cfg, jsc);\n+    deltaStreamerTestRunner(ds, cfg, (r) -> {\n       HoodieTableMetaClient metaClient =  new HoodieTableMetaClient(this.dfs.getConf(), tableBasePath, true);\n       int pendingReplaceSize = metaClient.getActiveTimeline().filterPendingReplaceTimeline().getInstants().toArray().length;\n       int completeReplaceSize = metaClient.getActiveTimeline().getCompletedReplaceTimeline().getInstants().toArray().length;\n       LOG.info(\"PendingReplaceSize=\" + pendingReplaceSize + \",completeReplaceSize = \" + completeReplaceSize);\n       return completeReplaceSize > 0;\n     });\n+    HoodieTableMetaClient metaClient =  new HoodieTableMetaClient(this.dfs.getConf(), tableBasePath, true);\n+    assertEquals(1, metaClient.getActiveTimeline().getCompletedReplaceTimeline().getInstants().toArray().length);\n+  }\n+\n+  private HoodieClusteringJob.Config buildHoodieClusteringUtilConfig(String basePath,\n+                                                                  String clusteringInstantTime, boolean runSchedule) {\n+    HoodieClusteringJob.Config config = new HoodieClusteringJob.Config();\n+    config.basePath = basePath;\n+    config.clusteringInstantTime = clusteringInstantTime;\n+    config.runSchedule = runSchedule;\n+    config.propsFilePath = dfsBasePath + \"/clusteringjob.properties\";\n+    return config;\n+  }\n+\n+  @Test\n+  public void testHoodieAsyncClusteringJob() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b165739d03ebe8fc393e069cfd099a8e1098dead"}, "originalPosition": 90}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY0NjkwNDUz", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-564690453", "createdAt": "2021-01-09T07:10:14Z", "commit": {"oid": "b165739d03ebe8fc393e069cfd099a8e1098dead"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1550ea52488201911fcbce75d52990d1ae883ce5", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/1550ea52488201911fcbce75d52990d1ae883ce5", "committedDate": "2021-01-09T13:54:16Z", "message": "[HUDI-1399]  support  a independent clustering spark job with schedule generate instant time"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b165739d03ebe8fc393e069cfd099a8e1098dead", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/b165739d03ebe8fc393e069cfd099a8e1098dead", "committedDate": "2021-01-06T07:27:34Z", "message": "[HUDI-1399]  support  a independent clustering spark job with schedule generate instant time"}, "afterCommit": {"oid": "1550ea52488201911fcbce75d52990d1ae883ce5", "author": {"user": {"login": "lw309637554", "name": "lw0090"}}, "url": "https://github.com/apache/hudi/commit/1550ea52488201911fcbce75d52990d1ae883ce5", "committedDate": "2021-01-09T13:54:16Z", "message": "[HUDI-1399]  support  a independent clustering spark job with schedule generate instant time"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY0ODIxNzE5", "url": "https://github.com/apache/hudi/pull/2379#pullrequestreview-564821719", "createdAt": "2021-01-10T01:29:25Z", "commit": {"oid": "1550ea52488201911fcbce75d52990d1ae883ce5"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4106, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}