{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI3NTEyMTk3", "number": 1702, "title": "[HUDI-426] Bootstrap datasource integration", "bodyText": "What is the purpose of the pull request\nThis PR consolidates changes related to Hudi data source and hive sync integration.\nBrief change log\n(for example:)\n\nModify AnnotationLocation checkstyle rule in checkstyle.xml\n\nVerify this pull request\n(Please pick either of the following options)\nThis pull request is a trivial rework / code cleanup without any test coverage.\n(or)\nThis pull request is already covered by existing tests, such as (please describe tests).\n(or)\nThis change added tests and can be verified as follows:\n(example:)\n\nAdded integration tests for end-to-end.\nAdded HoodieClientWriteTest to verify the change.\nManually verified the change by running a job locally.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-06-03T23:57:53Z", "url": "https://github.com/apache/hudi/pull/1702", "merged": true, "mergeCommit": {"oid": "e4a2d98f79f920ea60346fb78ef4b5af1f874afd"}, "closed": true, "closedAt": "2020-08-09T21:06:14Z", "author": {"login": "umehrot2"}, "timelineItems": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcn07ZQAFqTQyNDA2NTYzNw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc9S6ElgH2gAyNDI3NTEyMTk3OjFjZTQyMmU1M2Y2ZGU0ZmJlYzA1OTE4YWM0ODliMjlhNjVkZTViZDU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI0MDY1NjM3", "url": "https://github.com/apache/hudi/pull/1702#pullrequestreview-424065637", "createdAt": "2020-06-04T02:49:03Z", "commit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQwMjo0OTowM1rOGez_FA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQwMjo0OTowM1rOGez_FA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDk2MjE5Ng==", "bodyText": "@umehrot2 I think we are on the same page \ud83d\ude04\nas long as we load the data as RDD[Row], then it's very flexible. We can union different formats together", "url": "https://github.com/apache/hudi/pull/1702#discussion_r434962196", "createdAt": "2020-06-04T02:49:03Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRelation.scala", "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, Filter, PrunedFilteredScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+/**\n+  * This is Spark relation that can be used for querying metadata/fully bootstrapped query hudi tables, as well as\n+  * non-bootstrapped tables. It implements PrunedFilteredScan interface in order to support column pruning and filter\n+  * push-down. For metadata bootstrapped files, if we query columns from both metadata and actual data then it will\n+  * perform a merge of both to return the result.\n+  *\n+  * Caveat: Filter push-down does not work when querying both metadata and actual data columns over metadata\n+  * bootstrapped files, because then the metadata file and data file can return different number of rows causing errors\n+  * merging.\n+  *\n+  * @param _sqlContext Spark SQL Context\n+  * @param userSchema User specified schema in the datasource query\n+  * @param globPaths Globbed paths obtained from the user provided path for querying\n+  * @param metaClient Hudi table meta client\n+  * @param optParams DataSource options passed by the user\n+  */\n+class HudiBootstrapRelation(@transient val _sqlContext: SQLContext,\n+                            val userSchema: StructType,\n+                            val globPaths: Seq[Path],\n+                            val metaClient: HoodieTableMetaClient,\n+                            val optParams: Map[String, String]) extends BaseRelation\n+  with PrunedFilteredScan with Logging {\n+\n+  val skeletonSchema: StructType = HudiSparkUtils.getHudiMetadataSchema\n+  var dataSchema: StructType = _\n+  var fullSchema: StructType = _\n+\n+  val fileIndex: HudiBootstrapFileIndex = buildFileIndex()\n+\n+  override def sqlContext: SQLContext = _sqlContext\n+\n+  override val needConversion: Boolean = false\n+\n+  override def schema: StructType = inferFullSchema()\n+\n+  override def buildScan(requiredColumns: Array[String], filters: Array[Filter]): RDD[Row] = {\n+    logInfo(\"Starting scan..\")\n+\n+    // Compute splits\n+    val bootstrapSplits = fileIndex.files.map(hoodieBaseFile => {\n+      var skeletonFile: Option[PartitionedFile] = Option.empty\n+      var dataFile: PartitionedFile = null\n+\n+      if (hoodieBaseFile.getExternalBaseFile.isPresent) {\n+        skeletonFile = Option(PartitionedFile(InternalRow.empty, hoodieBaseFile.getPath, 0, hoodieBaseFile.getFileLen))\n+        dataFile = PartitionedFile(InternalRow.empty, hoodieBaseFile.getExternalBaseFile.get().getPath, 0,\n+          hoodieBaseFile.getExternalBaseFile.get().getFileLen)\n+      } else {\n+        dataFile = PartitionedFile(InternalRow.empty, hoodieBaseFile.getPath, 0, hoodieBaseFile.getFileLen)\n+      }\n+      HudiBootstrapSplit(dataFile, skeletonFile)\n+    })\n+    val tableState = HudiBootstrapTableState(bootstrapSplits)\n+\n+    // Get required schemas for column pruning\n+    var requiredDataSchema = StructType(Seq())\n+    var requiredSkeletonSchema = StructType(Seq())\n+    requiredColumns.foreach(col => {\n+      var field = dataSchema.find(_.name == col)\n+      if (field.isDefined) {\n+        requiredDataSchema = requiredDataSchema.add(field.get)\n+      } else {\n+        field = skeletonSchema.find(_.name == col)\n+        requiredSkeletonSchema = requiredSkeletonSchema.add(field.get)\n+      }\n+    })\n+\n+    // Prepare readers for reading data file and skeleton files\n+    val dataReadFunction = new ParquetFileFormat()\n+        .buildReaderWithPartitionValues(\n+          sparkSession = _sqlContext.sparkSession,\n+          dataSchema = dataSchema,\n+          partitionSchema = StructType(Seq.empty),\n+          requiredSchema = requiredDataSchema,\n+          filters = if (requiredSkeletonSchema.isEmpty) filters else Seq() ,\n+          options = Map.empty,\n+          hadoopConf = _sqlContext.sparkSession.sessionState.newHadoopConf()\n+        )\n+\n+    val skeletonReadFunction = new ParquetFileFormat()\n+      .buildReaderWithPartitionValues(\n+        sparkSession = _sqlContext.sparkSession,\n+        dataSchema = skeletonSchema,\n+        partitionSchema = StructType(Seq.empty),\n+        requiredSchema = requiredSkeletonSchema,\n+        filters = if (requiredDataSchema.isEmpty) filters else Seq(),\n+        options = Map.empty,\n+        hadoopConf = _sqlContext.sparkSession.sessionState.newHadoopConf()\n+      )\n+\n+    val regularReadFunction = new ParquetFileFormat()\n+      .buildReaderWithPartitionValues(\n+        sparkSession = _sqlContext.sparkSession,\n+        dataSchema = fullSchema,\n+        partitionSchema = StructType(Seq.empty),\n+        requiredSchema = StructType(requiredSkeletonSchema.fields ++ requiredDataSchema.fields),\n+        filters = filters,\n+        options = Map.empty,\n+        hadoopConf = _sqlContext.sparkSession.sessionState.newHadoopConf())\n+\n+    val rdd = new HudiBootstrapRDD(_sqlContext.sparkSession, dataReadFunction, skeletonReadFunction,\n+      regularReadFunction, requiredDataSchema, requiredSkeletonSchema, requiredColumns, tableState)\n+    rdd.asInstanceOf[RDD[Row]]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 139}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMwMjYwNzI5", "url": "https://github.com/apache/hudi/pull/1702#pullrequestreview-430260729", "createdAt": "2020-06-14T23:28:57Z", "commit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNFQyMzoyODo1N1rOGjgDTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQwMDowMTo0NFrOGjgO8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODQ3Nw==", "bodyText": "If we use vectorized reader this way, does it still have a huge performance boost?\nFrom my understanding, the regular reader iterator will read the whole row as UnsafeRow then do the column pruning before load it into memory. The vectorized reader will do the column pruning and loading data in one step. So theoretically vectorized reader would still be faster even we read it as InternalRow\nThe description I found from Spark code\nThis class can either return InternalRows or ColumnarBatches. With whole stage codegen enabled, this class returns ColumnarBatches which offers significant performance gains.", "url": "https://github.com/apache/hudi/pull/1702#discussion_r439878477", "createdAt": "2020-06-14T23:28:57Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRDD.scala", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+class HudiBootstrapRDD(@transient spark: SparkSession,\n+                       dataReadFunction: PartitionedFile => Iterator[Any],\n+                       skeletonReadFunction: PartitionedFile => Iterator[Any],\n+                       regularReadFunction: PartitionedFile => Iterator[Any],\n+                       dataSchema: StructType,\n+                       skeletonSchema: StructType,\n+                       requiredColumns: Array[String],\n+                       tableState: HudiBootstrapTableState)\n+  extends RDD[InternalRow](spark.sparkContext, Nil) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val bootstrapPartition = split.asInstanceOf[HudiBootstrapPartition]\n+\n+    if (log.isDebugEnabled) {\n+      if (bootstrapPartition.split.skeletonFile.isDefined) {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath + \", Skeleton File: \"\n+          + bootstrapPartition.split.skeletonFile.get.filePath)\n+      } else {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath)\n+      }\n+    }\n+\n+    var partitionedFileIterator: Iterator[InternalRow] = null\n+\n+    if (bootstrapPartition.split.skeletonFile.isDefined) {\n+      // It is a bootstrap split. Check both skeleton and data files.\n+      if (dataSchema.isEmpty) {\n+        // No data column to fetch, hence fetch only from skeleton file\n+        partitionedFileIterator = read(bootstrapPartition.split.skeletonFile.get,  skeletonReadFunction)\n+      } else if (skeletonSchema.isEmpty) {\n+        // No metadata column to fetch, hence fetch only from data file\n+        partitionedFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+      } else {\n+        // Fetch from both data and skeleton file, and merge\n+        val dataFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+        val skeletonFileIterator = read(bootstrapPartition.split.skeletonFile.get, skeletonReadFunction)\n+        partitionedFileIterator = merge(skeletonFileIterator, dataFileIterator)\n+      }\n+    } else {\n+      partitionedFileIterator = read(bootstrapPartition.split.dataFile, regularReadFunction)\n+    }\n+    partitionedFileIterator\n+  }\n+\n+  def merge(skeletonFileIterator: Iterator[InternalRow], dataFileIterator: Iterator[InternalRow])\n+  : Iterator[InternalRow] = {\n+    new Iterator[InternalRow] {\n+      override def hasNext: Boolean = dataFileIterator.hasNext && skeletonFileIterator.hasNext\n+      override def next(): InternalRow = {\n+        mergeInternalRow(skeletonFileIterator.next(), dataFileIterator.next())\n+      }\n+    }\n+  }\n+\n+  def mergeInternalRow(skeletonRow: InternalRow, dataRow: InternalRow): InternalRow = {\n+    val skeletonArr  = skeletonRow.copy().toSeq(skeletonSchema)\n+    val dataArr = dataRow.copy().toSeq(dataSchema)\n+    // We need to return it in the order requested\n+    val mergedArr = requiredColumns.map(col => {\n+      if (skeletonSchema.fieldNames.contains(col)) {\n+        val idx = skeletonSchema.fieldIndex(col)\n+        skeletonArr(idx)\n+      } else {\n+        val idx = dataSchema.fieldIndex(col)\n+        dataArr(idx)\n+      }\n+    })\n+\n+    logDebug(\"Merged data and skeleton values => \" + mergedArr.mkString(\",\"))\n+    val mergedRow = InternalRow.fromSeq(mergedArr)\n+    mergedRow\n+  }\n+\n+  def read(partitionedFile: PartitionedFile, readFileFunction: PartitionedFile => Iterator[Any])\n+    : Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+\n+    import scala.collection.JavaConverters._\n+\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3OTI1MA==", "bodyText": "Are we changing the metadata columns?", "url": "https://github.com/apache/hudi/pull/1702#discussion_r439879250", "createdAt": "2020-06-14T23:38:24Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/IncrementalRelation.scala", "diffHunk": "@@ -71,13 +78,16 @@ class IncrementalRelation(val sqlContext: SQLContext,\n     optParams.getOrElse(DataSourceReadOptions.END_INSTANTTIME_OPT_KEY, lastInstant.getTimestamp))\n     .getInstants.iterator().toList\n \n-  // use schema from latest metadata, if not present, read schema from the data file\n-  private val latestSchema = {\n-    val schemaUtil = new TableSchemaResolver(metaClient)\n-    val tableSchema = HoodieAvroUtils.createHoodieWriteSchema(schemaUtil.getTableAvroSchemaWithoutMetadataFields);\n-    AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+  // use schema from a file produced in the latest instant\n+  val latestSchema: StructType = {\n+    log.info(\"Inferring schema..\")\n+    val schemaResolver = new TableSchemaResolver(metaClient)\n+    val tableSchema = schemaResolver.getTableAvroSchemaWithoutMetadataFields\n+    val dataSchema = AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+    StructType(skeletonSchema.fields ++ dataSchema.fields)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3OTUwMA==", "bodyText": "Should we avoid var here? seem like avoidable.", "url": "https://github.com/apache/hudi/pull/1702#discussion_r439879500", "createdAt": "2020-06-14T23:41:33Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/IncrementalRelation.scala", "diffHunk": "@@ -92,36 +102,69 @@ class IncrementalRelation(val sqlContext: SQLContext,\n   override def schema: StructType = latestSchema\n \n   override def buildScan(): RDD[Row] = {\n-    val fileIdToFullPath = mutable.HashMap[String, String]()\n+    val regularFileIdToFullPath = mutable.HashMap[String, String]()\n+    var metaBootstrapFileIdToFullPath = mutable.HashMap[String, String]()\n+\n     for (commit <- commitsToReturn) {\n       val metadata: HoodieCommitMetadata = HoodieCommitMetadata.fromBytes(commitTimeline.getInstantDetails(commit)\n         .get, classOf[HoodieCommitMetadata])\n-      fileIdToFullPath ++= metadata.getFileIdAndFullPaths(basePath).toMap\n+\n+      if (HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS == commit.getTimestamp) {\n+        metaBootstrapFileIdToFullPath ++= metadata.getFileIdAndFullPaths(basePath).toMap\n+      } else {\n+        regularFileIdToFullPath ++= metadata.getFileIdAndFullPaths(basePath).toMap\n+      }\n+    }\n+\n+    if (metaBootstrapFileIdToFullPath.nonEmpty) {\n+      // filer out meta bootstrap files that have had more commits since metadata bootstrap\n+      metaBootstrapFileIdToFullPath = metaBootstrapFileIdToFullPath\n+        .filterNot(fileIdFullPath => regularFileIdToFullPath.contains(fileIdFullPath._1))\n     }\n+\n     val pathGlobPattern = optParams.getOrElse(\n       DataSourceReadOptions.INCR_PATH_GLOB_OPT_KEY,\n       DataSourceReadOptions.DEFAULT_INCR_PATH_GLOB_OPT_VAL)\n-    val filteredFullPath = if(!pathGlobPattern.equals(DataSourceReadOptions.DEFAULT_INCR_PATH_GLOB_OPT_VAL)) {\n-      val globMatcher = new GlobPattern(\"*\" + pathGlobPattern)\n-      fileIdToFullPath.filter(p => globMatcher.matches(p._2))\n-    } else {\n-      fileIdToFullPath\n+    val (filteredRegularFullPaths, filteredMetaBootstrapFullPaths) = {\n+      if(!pathGlobPattern.equals(DataSourceReadOptions.DEFAULT_INCR_PATH_GLOB_OPT_VAL)) {\n+        val globMatcher = new GlobPattern(\"*\" + pathGlobPattern)\n+        (regularFileIdToFullPath.filter(p => globMatcher.matches(p._2)).values,\n+          metaBootstrapFileIdToFullPath.filter(p => globMatcher.matches(p._2)).values)\n+      } else {\n+        (regularFileIdToFullPath.values, metaBootstrapFileIdToFullPath.values)\n+      }\n     }\n     // unset the path filter, otherwise if end_instant_time is not the latest instant, path filter set for RO view\n     // will filter out all the files incorrectly.\n     sqlContext.sparkContext.hadoopConfiguration.unset(\"mapreduce.input.pathFilter.class\")\n     val sOpts = optParams.filter(p => !p._1.equalsIgnoreCase(\"path\"))\n-    if (filteredFullPath.isEmpty) {\n+    if (filteredRegularFullPaths.isEmpty && filteredMetaBootstrapFullPaths.isEmpty) {\n       sqlContext.sparkContext.emptyRDD[Row]\n     } else {\n       log.info(\"Additional Filters to be applied to incremental source are :\" + filters)\n-      filters.foldLeft(sqlContext.read.options(sOpts)\n-        .schema(latestSchema)\n-        .parquet(filteredFullPath.values.toList: _*)\n-        .filter(String.format(\"%s >= '%s'\", HoodieRecord.COMMIT_TIME_METADATA_FIELD, commitsToReturn.head.getTimestamp))\n-        .filter(String.format(\"%s <= '%s'\",\n-          HoodieRecord.COMMIT_TIME_METADATA_FIELD, commitsToReturn.last.getTimestamp)))((e, f) => e.filter(f))\n-        .toDF().rdd\n+\n+      var df: DataFrame = sqlContext.createDataFrame(sqlContext.sparkContext.emptyRDD[Row], latestSchema)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg4MTA1MA==", "bodyText": "IIUC, the skeleton file will only exist if there is a data file. Did I miss anything that there is a case with a stand-alone skeleton file?", "url": "https://github.com/apache/hudi/pull/1702#discussion_r439881050", "createdAt": "2020-06-14T23:57:53Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRDD.scala", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+class HudiBootstrapRDD(@transient spark: SparkSession,\n+                       dataReadFunction: PartitionedFile => Iterator[Any],\n+                       skeletonReadFunction: PartitionedFile => Iterator[Any],\n+                       regularReadFunction: PartitionedFile => Iterator[Any],\n+                       dataSchema: StructType,\n+                       skeletonSchema: StructType,\n+                       requiredColumns: Array[String],\n+                       tableState: HudiBootstrapTableState)\n+  extends RDD[InternalRow](spark.sparkContext, Nil) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val bootstrapPartition = split.asInstanceOf[HudiBootstrapPartition]\n+\n+    if (log.isDebugEnabled) {\n+      if (bootstrapPartition.split.skeletonFile.isDefined) {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath + \", Skeleton File: \"\n+          + bootstrapPartition.split.skeletonFile.get.filePath)\n+      } else {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath)\n+      }\n+    }\n+\n+    var partitionedFileIterator: Iterator[InternalRow] = null\n+\n+    if (bootstrapPartition.split.skeletonFile.isDefined) {\n+      // It is a bootstrap split. Check both skeleton and data files.\n+      if (dataSchema.isEmpty) {\n+        // No data column to fetch, hence fetch only from skeleton file\n+        partitionedFileIterator = read(bootstrapPartition.split.skeletonFile.get,  skeletonReadFunction)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg4MTQ1Ng==", "bodyText": "Is this the regular COW table data files?", "url": "https://github.com/apache/hudi/pull/1702#discussion_r439881456", "createdAt": "2020-06-15T00:01:44Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRDD.scala", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+class HudiBootstrapRDD(@transient spark: SparkSession,\n+                       dataReadFunction: PartitionedFile => Iterator[Any],\n+                       skeletonReadFunction: PartitionedFile => Iterator[Any],\n+                       regularReadFunction: PartitionedFile => Iterator[Any],\n+                       dataSchema: StructType,\n+                       skeletonSchema: StructType,\n+                       requiredColumns: Array[String],\n+                       tableState: HudiBootstrapTableState)\n+  extends RDD[InternalRow](spark.sparkContext, Nil) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val bootstrapPartition = split.asInstanceOf[HudiBootstrapPartition]\n+\n+    if (log.isDebugEnabled) {\n+      if (bootstrapPartition.split.skeletonFile.isDefined) {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath + \", Skeleton File: \"\n+          + bootstrapPartition.split.skeletonFile.get.filePath)\n+      } else {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath)\n+      }\n+    }\n+\n+    var partitionedFileIterator: Iterator[InternalRow] = null\n+\n+    if (bootstrapPartition.split.skeletonFile.isDefined) {\n+      // It is a bootstrap split. Check both skeleton and data files.\n+      if (dataSchema.isEmpty) {\n+        // No data column to fetch, hence fetch only from skeleton file\n+        partitionedFileIterator = read(bootstrapPartition.split.skeletonFile.get,  skeletonReadFunction)\n+      } else if (skeletonSchema.isEmpty) {\n+        // No metadata column to fetch, hence fetch only from data file\n+        partitionedFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 62}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNzE3NjM1", "url": "https://github.com/apache/hudi/pull/1702#pullrequestreview-442717635", "createdAt": "2020-07-05T23:56:24Z", "commit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNVQyMzo1NjoyNFrOGtFvqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNVQyMzo1NjoyNFrOGtFvqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzMzIyNQ==", "bodyText": "I think this approach is better than extending the FileFormat. Ultimately, we can have a HudiRDD to handle all the file loading and merging(bootstrap files, parquet, orc, logs). Union will trigger shuffle and grouping files on the driver then use different FileFormat to read is not as clean as this approach.\nI will add the MOR stuff on top of this PR after this merged.", "url": "https://github.com/apache/hudi/pull/1702#discussion_r449933225", "createdAt": "2020-07-05T23:56:24Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRDD.scala", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+class HudiBootstrapRDD(@transient spark: SparkSession,\n+                       dataReadFunction: PartitionedFile => Iterator[Any],\n+                       skeletonReadFunction: PartitionedFile => Iterator[Any],\n+                       regularReadFunction: PartitionedFile => Iterator[Any],\n+                       dataSchema: StructType,\n+                       skeletonSchema: StructType,\n+                       requiredColumns: Array[String],\n+                       tableState: HudiBootstrapTableState)\n+  extends RDD[InternalRow](spark.sparkContext, Nil) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val bootstrapPartition = split.asInstanceOf[HudiBootstrapPartition]\n+\n+    if (log.isDebugEnabled) {\n+      if (bootstrapPartition.split.skeletonFile.isDefined) {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath + \", Skeleton File: \"\n+          + bootstrapPartition.split.skeletonFile.get.filePath)\n+      } else {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath)\n+      }\n+    }\n+\n+    var partitionedFileIterator: Iterator[InternalRow] = null\n+\n+    if (bootstrapPartition.split.skeletonFile.isDefined) {\n+      // It is a bootstrap split. Check both skeleton and data files.\n+      if (dataSchema.isEmpty) {\n+        // No data column to fetch, hence fetch only from skeleton file\n+        partitionedFileIterator = read(bootstrapPartition.split.skeletonFile.get,  skeletonReadFunction)\n+      } else if (skeletonSchema.isEmpty) {\n+        // No metadata column to fetch, hence fetch only from data file\n+        partitionedFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+      } else {\n+        // Fetch from both data and skeleton file, and merge\n+        val dataFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+        val skeletonFileIterator = read(bootstrapPartition.split.skeletonFile.get, skeletonReadFunction)\n+        partitionedFileIterator = merge(skeletonFileIterator, dataFileIterator)\n+      }\n+    } else {\n+      partitionedFileIterator = read(bootstrapPartition.split.dataFile, regularReadFunction)\n+    }\n+    partitionedFileIterator\n+  }\n+\n+  def merge(skeletonFileIterator: Iterator[InternalRow], dataFileIterator: Iterator[InternalRow])", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 75}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNzE5MjE4", "url": "https://github.com/apache/hudi/pull/1702#pullrequestreview-442719218", "createdAt": "2020-07-06T00:12:41Z", "commit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQwMDoxMjo0MVrOGtF2QA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQwMDoyODowNVrOGtF9Bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNDkxMg==", "bodyText": "Are these additional paths on top of the path? Any example of the use cases?", "url": "https://github.com/apache/hudi/pull/1702#discussion_r449934912", "createdAt": "2020-07-06T00:12:41Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/DefaultSource.scala", "diffHunk": "@@ -54,29 +58,54 @@ class DefaultSource extends RelationProvider\n     val parameters = Map(QUERY_TYPE_OPT_KEY -> DEFAULT_QUERY_TYPE_OPT_VAL) ++ translateViewTypesToQueryTypes(optParams)\n \n     val path = parameters.get(\"path\")\n-    if (path.isEmpty) {\n-      throw new HoodieException(\"'path' must be specified.\")\n-    }\n \n     if (parameters(QUERY_TYPE_OPT_KEY).equals(QUERY_TYPE_SNAPSHOT_OPT_VAL)) {\n-      // this is just effectively RO view only, where `path` can contain a mix of\n-      // non-hoodie/hoodie path files. set the path filter up\n-      sqlContext.sparkContext.hadoopConfiguration.setClass(\n-        \"mapreduce.input.pathFilter.class\",\n-        classOf[HoodieROTablePathFilter],\n-        classOf[org.apache.hadoop.fs.PathFilter])\n-\n-      log.info(\"Constructing hoodie (as parquet) data source with options :\" + parameters)\n-      log.warn(\"Snapshot view not supported yet via data source, for MERGE_ON_READ tables. \" +\n-        \"Please query the Hive table registered using Spark SQL.\")\n-      // simply return as a regular parquet relation\n-      DataSource.apply(\n-        sparkSession = sqlContext.sparkSession,\n-        userSpecifiedSchema = Option(schema),\n-        className = \"parquet\",\n-        options = parameters)\n-        .resolveRelation()\n+      val readPathsStr = parameters.get(DataSourceReadOptions.READ_PATHS_OPT_KEY)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNjY0Nw==", "bodyText": "We probably have to use rowIterator since we will need to merge on row level anyway, same for MOR table too. Agree that Spark will convert ColumnBatch to row at some point and it is very difficult to locate.", "url": "https://github.com/apache/hudi/pull/1702#discussion_r449936647", "createdAt": "2020-07-06T00:28:05Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRDD.scala", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+class HudiBootstrapRDD(@transient spark: SparkSession,\n+                       dataReadFunction: PartitionedFile => Iterator[Any],\n+                       skeletonReadFunction: PartitionedFile => Iterator[Any],\n+                       regularReadFunction: PartitionedFile => Iterator[Any],\n+                       dataSchema: StructType,\n+                       skeletonSchema: StructType,\n+                       requiredColumns: Array[String],\n+                       tableState: HudiBootstrapTableState)\n+  extends RDD[InternalRow](spark.sparkContext, Nil) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val bootstrapPartition = split.asInstanceOf[HudiBootstrapPartition]\n+\n+    if (log.isDebugEnabled) {\n+      if (bootstrapPartition.split.skeletonFile.isDefined) {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath + \", Skeleton File: \"\n+          + bootstrapPartition.split.skeletonFile.get.filePath)\n+      } else {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath)\n+      }\n+    }\n+\n+    var partitionedFileIterator: Iterator[InternalRow] = null\n+\n+    if (bootstrapPartition.split.skeletonFile.isDefined) {\n+      // It is a bootstrap split. Check both skeleton and data files.\n+      if (dataSchema.isEmpty) {\n+        // No data column to fetch, hence fetch only from skeleton file\n+        partitionedFileIterator = read(bootstrapPartition.split.skeletonFile.get,  skeletonReadFunction)\n+      } else if (skeletonSchema.isEmpty) {\n+        // No metadata column to fetch, hence fetch only from data file\n+        partitionedFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+      } else {\n+        // Fetch from both data and skeleton file, and merge\n+        val dataFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+        val skeletonFileIterator = read(bootstrapPartition.split.skeletonFile.get, skeletonReadFunction)\n+        partitionedFileIterator = merge(skeletonFileIterator, dataFileIterator)\n+      }\n+    } else {\n+      partitionedFileIterator = read(bootstrapPartition.split.dataFile, regularReadFunction)\n+    }\n+    partitionedFileIterator\n+  }\n+\n+  def merge(skeletonFileIterator: Iterator[InternalRow], dataFileIterator: Iterator[InternalRow])\n+  : Iterator[InternalRow] = {\n+    new Iterator[InternalRow] {\n+      override def hasNext: Boolean = dataFileIterator.hasNext && skeletonFileIterator.hasNext\n+      override def next(): InternalRow = {\n+        mergeInternalRow(skeletonFileIterator.next(), dataFileIterator.next())\n+      }\n+    }\n+  }\n+\n+  def mergeInternalRow(skeletonRow: InternalRow, dataRow: InternalRow): InternalRow = {\n+    val skeletonArr  = skeletonRow.copy().toSeq(skeletonSchema)\n+    val dataArr = dataRow.copy().toSeq(dataSchema)\n+    // We need to return it in the order requested\n+    val mergedArr = requiredColumns.map(col => {\n+      if (skeletonSchema.fieldNames.contains(col)) {\n+        val idx = skeletonSchema.fieldIndex(col)\n+        skeletonArr(idx)\n+      } else {\n+        val idx = dataSchema.fieldIndex(col)\n+        dataArr(idx)\n+      }\n+    })\n+\n+    logDebug(\"Merged data and skeleton values => \" + mergedArr.mkString(\",\"))\n+    val mergedRow = InternalRow.fromSeq(mergedArr)\n+    mergedRow\n+  }\n+\n+  def read(partitionedFile: PartitionedFile, readFileFunction: PartitionedFile => Iterator[Any])\n+    : Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+\n+    import scala.collection.JavaConverters._\n+\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODQ3Nw=="}, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 112}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM2MjM2OTg0", "url": "https://github.com/apache/hudi/pull/1702#pullrequestreview-436236984", "createdAt": "2020-06-23T23:58:26Z", "commit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMzo1ODoyNlrOGn-eAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMzowNDowOVrOGx9FJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU3MTEzOQ==", "bodyText": "is it possible to keep this to hudi-spark ?", "url": "https://github.com/apache/hudi/pull/1702#discussion_r444571139", "createdAt": "2020-06-23T23:58:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/pom.xml", "diffHunk": "@@ -101,6 +101,11 @@\n       <groupId>org.apache.spark</groupId>\n       <artifactId>spark-sql_${scala.binary.version}</artifactId>\n     </dependency>\n+    <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-avro_${scala.binary.version}</artifactId>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAyNDAyMA==", "bodyText": "some of this stuff is good to do, even in the regular path? in place of the current path filter approach?", "url": "https://github.com/apache/hudi/pull/1702#discussion_r455024020", "createdAt": "2020-07-15T12:47:20Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiSparkUtils.scala", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hadoop.fs.{FileSystem, Path}\n+import org.apache.hudi.common.model.HoodieRecord\n+import org.apache.spark.deploy.SparkHadoopUtil\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.execution.datasources.{FileStatusCache, InMemoryFileIndex}\n+import org.apache.spark.sql.types.{StringType, StructField, StructType}\n+import scala.collection.JavaConverters._\n+\n+\n+object HudiSparkUtils {\n+\n+  def getHudiMetadataSchema: StructType = {\n+    StructType(HoodieRecord.HOODIE_META_COLUMNS.asScala.map(col => {\n+        StructField(col, StringType, nullable = true)\n+    }))\n+  }\n+\n+  def checkAndGlobPathIfNecessary(paths: Seq[String], fs: FileSystem): Seq[Path] = {\n+    paths.flatMap(path => {\n+      val qualified = new Path(path).makeQualified(fs.getUri, fs.getWorkingDirectory)\n+      val globPaths = SparkHadoopUtil.get.globPathIfNecessary(fs, qualified)\n+      globPaths\n+    })\n+  }\n+\n+  def createInMemoryFileIndex(sparkSession: SparkSession, globbedPaths: Seq[Path]): InMemoryFileIndex = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAyNTg0Nw==", "bodyText": "IIUC this will leverage Spark's caching and filter on top of that based on our timeline", "url": "https://github.com/apache/hudi/pull/1702#discussion_r455025847", "createdAt": "2020-07-15T12:50:23Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRelation.scala", "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hudi.common.model.HoodieBaseFile\n+import org.apache.hudi.common.table.{HoodieTableMetaClient, TableSchemaResolver}\n+import org.apache.hudi.common.table.view.HoodieTableFileSystemView\n+import org.apache.hudi.exception.HoodieException\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.sources.{BaseRelation, Filter, PrunedFilteredScan}\n+import org.apache.spark.sql.types.StructType\n+\n+import scala.collection.JavaConverters._\n+\n+/**\n+  * This is Spark relation that can be used for querying metadata/fully bootstrapped query hudi tables, as well as\n+  * non-bootstrapped tables. It implements PrunedFilteredScan interface in order to support column pruning and filter\n+  * push-down. For metadata bootstrapped files, if we query columns from both metadata and actual data then it will\n+  * perform a merge of both to return the result.\n+  *\n+  * Caveat: Filter push-down does not work when querying both metadata and actual data columns over metadata\n+  * bootstrapped files, because then the metadata file and data file can return different number of rows causing errors\n+  * merging.\n+  *\n+  * @param _sqlContext Spark SQL Context\n+  * @param userSchema User specified schema in the datasource query\n+  * @param globPaths Globbed paths obtained from the user provided path for querying\n+  * @param metaClient Hudi table meta client\n+  * @param optParams DataSource options passed by the user\n+  */\n+class HudiBootstrapRelation(@transient val _sqlContext: SQLContext,\n+                            val userSchema: StructType,\n+                            val globPaths: Seq[Path],\n+                            val metaClient: HoodieTableMetaClient,\n+                            val optParams: Map[String, String]) extends BaseRelation\n+  with PrunedFilteredScan with Logging {\n+\n+  val skeletonSchema: StructType = HudiSparkUtils.getHudiMetadataSchema\n+  var dataSchema: StructType = _\n+  var fullSchema: StructType = _\n+\n+  val fileIndex: HudiBootstrapFileIndex = buildFileIndex()\n+\n+  override def sqlContext: SQLContext = _sqlContext\n+\n+  override val needConversion: Boolean = false\n+\n+  override def schema: StructType = inferFullSchema()\n+\n+  override def buildScan(requiredColumns: Array[String], filters: Array[Filter]): RDD[Row] = {\n+    logInfo(\"Starting scan..\")\n+\n+    // Compute splits\n+    val bootstrapSplits = fileIndex.files.map(hoodieBaseFile => {\n+      var skeletonFile: Option[PartitionedFile] = Option.empty\n+      var dataFile: PartitionedFile = null\n+\n+      if (hoodieBaseFile.getExternalBaseFile.isPresent) {\n+        skeletonFile = Option(PartitionedFile(InternalRow.empty, hoodieBaseFile.getPath, 0, hoodieBaseFile.getFileLen))\n+        dataFile = PartitionedFile(InternalRow.empty, hoodieBaseFile.getExternalBaseFile.get().getPath, 0,\n+          hoodieBaseFile.getExternalBaseFile.get().getFileLen)\n+      } else {\n+        dataFile = PartitionedFile(InternalRow.empty, hoodieBaseFile.getPath, 0, hoodieBaseFile.getFileLen)\n+      }\n+      HudiBootstrapSplit(dataFile, skeletonFile)\n+    })\n+    val tableState = HudiBootstrapTableState(bootstrapSplits)\n+\n+    // Get required schemas for column pruning\n+    var requiredDataSchema = StructType(Seq())\n+    var requiredSkeletonSchema = StructType(Seq())\n+    requiredColumns.foreach(col => {\n+      var field = dataSchema.find(_.name == col)\n+      if (field.isDefined) {\n+        requiredDataSchema = requiredDataSchema.add(field.get)\n+      } else {\n+        field = skeletonSchema.find(_.name == col)\n+        requiredSkeletonSchema = requiredSkeletonSchema.add(field.get)\n+      }\n+    })\n+\n+    // Prepare readers for reading data file and skeleton files\n+    val dataReadFunction = new ParquetFileFormat()\n+        .buildReaderWithPartitionValues(\n+          sparkSession = _sqlContext.sparkSession,\n+          dataSchema = dataSchema,\n+          partitionSchema = StructType(Seq.empty),\n+          requiredSchema = requiredDataSchema,\n+          filters = if (requiredSkeletonSchema.isEmpty) filters else Seq() ,\n+          options = Map.empty,\n+          hadoopConf = _sqlContext.sparkSession.sessionState.newHadoopConf()\n+        )\n+\n+    val skeletonReadFunction = new ParquetFileFormat()\n+      .buildReaderWithPartitionValues(\n+        sparkSession = _sqlContext.sparkSession,\n+        dataSchema = skeletonSchema,\n+        partitionSchema = StructType(Seq.empty),\n+        requiredSchema = requiredSkeletonSchema,\n+        filters = if (requiredDataSchema.isEmpty) filters else Seq(),\n+        options = Map.empty,\n+        hadoopConf = _sqlContext.sparkSession.sessionState.newHadoopConf()\n+      )\n+\n+    val regularReadFunction = new ParquetFileFormat()\n+      .buildReaderWithPartitionValues(\n+        sparkSession = _sqlContext.sparkSession,\n+        dataSchema = fullSchema,\n+        partitionSchema = StructType(Seq.empty),\n+        requiredSchema = StructType(requiredSkeletonSchema.fields ++ requiredDataSchema.fields),\n+        filters = filters,\n+        options = Map.empty,\n+        hadoopConf = _sqlContext.sparkSession.sessionState.newHadoopConf())\n+\n+    val rdd = new HudiBootstrapRDD(_sqlContext.sparkSession, dataReadFunction, skeletonReadFunction,\n+      regularReadFunction, requiredDataSchema, requiredSkeletonSchema, requiredColumns, tableState)\n+    rdd.asInstanceOf[RDD[Row]]\n+  }\n+\n+  def inferFullSchema(): StructType = {\n+    if (fullSchema == null) {\n+      logInfo(\"Inferring schema..\")\n+      val schemaResolver = new TableSchemaResolver(metaClient)\n+      val tableSchema = schemaResolver.getTableAvroSchemaWithoutMetadataFields\n+      dataSchema = AvroConversionUtils.convertAvroSchemaToStructType(tableSchema)\n+      fullSchema = StructType(skeletonSchema.fields ++ dataSchema.fields)\n+    }\n+    fullSchema\n+  }\n+\n+  def buildFileIndex(): HudiBootstrapFileIndex = {\n+    logInfo(\"Building file index..\")\n+    val inMemoryFileIndex = HudiSparkUtils.createInMemoryFileIndex(_sqlContext.sparkSession, globPaths)\n+    val fileStatuses = inMemoryFileIndex.allFiles()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAyOTU4MQ==", "bodyText": "+1 here we wrap the FileFormat. Seems much simpler.. Thanks @umehrot2 for this.. educative!", "url": "https://github.com/apache/hudi/pull/1702#discussion_r455029581", "createdAt": "2020-07-15T12:56:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRDD.scala", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+class HudiBootstrapRDD(@transient spark: SparkSession,\n+                       dataReadFunction: PartitionedFile => Iterator[Any],\n+                       skeletonReadFunction: PartitionedFile => Iterator[Any],\n+                       regularReadFunction: PartitionedFile => Iterator[Any],\n+                       dataSchema: StructType,\n+                       skeletonSchema: StructType,\n+                       requiredColumns: Array[String],\n+                       tableState: HudiBootstrapTableState)\n+  extends RDD[InternalRow](spark.sparkContext, Nil) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val bootstrapPartition = split.asInstanceOf[HudiBootstrapPartition]\n+\n+    if (log.isDebugEnabled) {\n+      if (bootstrapPartition.split.skeletonFile.isDefined) {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath + \", Skeleton File: \"\n+          + bootstrapPartition.split.skeletonFile.get.filePath)\n+      } else {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath)\n+      }\n+    }\n+\n+    var partitionedFileIterator: Iterator[InternalRow] = null\n+\n+    if (bootstrapPartition.split.skeletonFile.isDefined) {\n+      // It is a bootstrap split. Check both skeleton and data files.\n+      if (dataSchema.isEmpty) {\n+        // No data column to fetch, hence fetch only from skeleton file\n+        partitionedFileIterator = read(bootstrapPartition.split.skeletonFile.get,  skeletonReadFunction)\n+      } else if (skeletonSchema.isEmpty) {\n+        // No metadata column to fetch, hence fetch only from data file\n+        partitionedFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+      } else {\n+        // Fetch from both data and skeleton file, and merge\n+        val dataFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+        val skeletonFileIterator = read(bootstrapPartition.split.skeletonFile.get, skeletonReadFunction)\n+        partitionedFileIterator = merge(skeletonFileIterator, dataFileIterator)\n+      }\n+    } else {\n+      partitionedFileIterator = read(bootstrapPartition.split.dataFile, regularReadFunction)\n+    }\n+    partitionedFileIterator\n+  }\n+\n+  def merge(skeletonFileIterator: Iterator[InternalRow], dataFileIterator: Iterator[InternalRow])", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzMzIyNQ=="}, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAzMTcyMQ==", "bodyText": "on avoiding the merge cost, my understanding is - its hard for this case where you need to actually merge these two values, re-order etc.", "url": "https://github.com/apache/hudi/pull/1702#discussion_r455031721", "createdAt": "2020-07-15T12:59:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRDD.scala", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+class HudiBootstrapRDD(@transient spark: SparkSession,\n+                       dataReadFunction: PartitionedFile => Iterator[Any],\n+                       skeletonReadFunction: PartitionedFile => Iterator[Any],\n+                       regularReadFunction: PartitionedFile => Iterator[Any],\n+                       dataSchema: StructType,\n+                       skeletonSchema: StructType,\n+                       requiredColumns: Array[String],\n+                       tableState: HudiBootstrapTableState)\n+  extends RDD[InternalRow](spark.sparkContext, Nil) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val bootstrapPartition = split.asInstanceOf[HudiBootstrapPartition]\n+\n+    if (log.isDebugEnabled) {\n+      if (bootstrapPartition.split.skeletonFile.isDefined) {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath + \", Skeleton File: \"\n+          + bootstrapPartition.split.skeletonFile.get.filePath)\n+      } else {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath)\n+      }\n+    }\n+\n+    var partitionedFileIterator: Iterator[InternalRow] = null\n+\n+    if (bootstrapPartition.split.skeletonFile.isDefined) {\n+      // It is a bootstrap split. Check both skeleton and data files.\n+      if (dataSchema.isEmpty) {\n+        // No data column to fetch, hence fetch only from skeleton file\n+        partitionedFileIterator = read(bootstrapPartition.split.skeletonFile.get,  skeletonReadFunction)\n+      } else if (skeletonSchema.isEmpty) {\n+        // No metadata column to fetch, hence fetch only from data file\n+        partitionedFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+      } else {\n+        // Fetch from both data and skeleton file, and merge\n+        val dataFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+        val skeletonFileIterator = read(bootstrapPartition.split.skeletonFile.get, skeletonReadFunction)\n+        partitionedFileIterator = merge(skeletonFileIterator, dataFileIterator)\n+      }\n+    } else {\n+      partitionedFileIterator = read(bootstrapPartition.split.dataFile, regularReadFunction)\n+    }\n+    partitionedFileIterator\n+  }\n+\n+  def merge(skeletonFileIterator: Iterator[InternalRow], dataFileIterator: Iterator[InternalRow])\n+  : Iterator[InternalRow] = {\n+    new Iterator[InternalRow] {\n+      override def hasNext: Boolean = dataFileIterator.hasNext && skeletonFileIterator.hasNext\n+      override def next(): InternalRow = {\n+        mergeInternalRow(skeletonFileIterator.next(), dataFileIterator.next())\n+      }\n+    }\n+  }\n+\n+  def mergeInternalRow(skeletonRow: InternalRow, dataRow: InternalRow): InternalRow = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTAzNDE0OA==", "bodyText": "For MOR table, I have some ideas to speed things up by pre-reading the delete/rollback blocks and simply \"skip\" rows as long as OverwritewithLatestPayload is used..  If the user does specify a merge function, then its hard to get away from.. we can take this discussion in a separate forum.", "url": "https://github.com/apache/hudi/pull/1702#discussion_r455034148", "createdAt": "2020-07-15T13:04:09Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRDD.scala", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+class HudiBootstrapRDD(@transient spark: SparkSession,\n+                       dataReadFunction: PartitionedFile => Iterator[Any],\n+                       skeletonReadFunction: PartitionedFile => Iterator[Any],\n+                       regularReadFunction: PartitionedFile => Iterator[Any],\n+                       dataSchema: StructType,\n+                       skeletonSchema: StructType,\n+                       requiredColumns: Array[String],\n+                       tableState: HudiBootstrapTableState)\n+  extends RDD[InternalRow](spark.sparkContext, Nil) {\n+\n+  override def compute(split: Partition, context: TaskContext): Iterator[InternalRow] = {\n+    val bootstrapPartition = split.asInstanceOf[HudiBootstrapPartition]\n+\n+    if (log.isDebugEnabled) {\n+      if (bootstrapPartition.split.skeletonFile.isDefined) {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath + \", Skeleton File: \"\n+          + bootstrapPartition.split.skeletonFile.get.filePath)\n+      } else {\n+        logDebug(\"Got Split => Index: \" + bootstrapPartition.index + \", Data File: \"\n+          + bootstrapPartition.split.dataFile.filePath)\n+      }\n+    }\n+\n+    var partitionedFileIterator: Iterator[InternalRow] = null\n+\n+    if (bootstrapPartition.split.skeletonFile.isDefined) {\n+      // It is a bootstrap split. Check both skeleton and data files.\n+      if (dataSchema.isEmpty) {\n+        // No data column to fetch, hence fetch only from skeleton file\n+        partitionedFileIterator = read(bootstrapPartition.split.skeletonFile.get,  skeletonReadFunction)\n+      } else if (skeletonSchema.isEmpty) {\n+        // No metadata column to fetch, hence fetch only from data file\n+        partitionedFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+      } else {\n+        // Fetch from both data and skeleton file, and merge\n+        val dataFileIterator = read(bootstrapPartition.split.dataFile, dataReadFunction)\n+        val skeletonFileIterator = read(bootstrapPartition.split.skeletonFile.get, skeletonReadFunction)\n+        partitionedFileIterator = merge(skeletonFileIterator, dataFileIterator)\n+      }\n+    } else {\n+      partitionedFileIterator = read(bootstrapPartition.split.dataFile, regularReadFunction)\n+    }\n+    partitionedFileIterator\n+  }\n+\n+  def merge(skeletonFileIterator: Iterator[InternalRow], dataFileIterator: Iterator[InternalRow])\n+  : Iterator[InternalRow] = {\n+    new Iterator[InternalRow] {\n+      override def hasNext: Boolean = dataFileIterator.hasNext && skeletonFileIterator.hasNext\n+      override def next(): InternalRow = {\n+        mergeInternalRow(skeletonFileIterator.next(), dataFileIterator.next())\n+      }\n+    }\n+  }\n+\n+  def mergeInternalRow(skeletonRow: InternalRow, dataRow: InternalRow): InternalRow = {\n+    val skeletonArr  = skeletonRow.copy().toSeq(skeletonSchema)\n+    val dataArr = dataRow.copy().toSeq(dataSchema)\n+    // We need to return it in the order requested\n+    val mergedArr = requiredColumns.map(col => {\n+      if (skeletonSchema.fieldNames.contains(col)) {\n+        val idx = skeletonSchema.fieldIndex(col)\n+        skeletonArr(idx)\n+      } else {\n+        val idx = dataSchema.fieldIndex(col)\n+        dataArr(idx)\n+      }\n+    })\n+\n+    logDebug(\"Merged data and skeleton values => \" + mergedArr.mkString(\",\"))\n+    val mergedRow = InternalRow.fromSeq(mergedArr)\n+    mergedRow\n+  }\n+\n+  def read(partitionedFile: PartitionedFile, readFileFunction: PartitionedFile => Iterator[Any])\n+    : Iterator[InternalRow] = {\n+    val fileIterator = readFileFunction(partitionedFile)\n+\n+    import scala.collection.JavaConverters._\n+\n+    val rows = fileIterator.flatMap(_ match {\n+      case r: InternalRow => Seq(r)\n+      case b: ColumnarBatch => b.rowIterator().asScala", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTg3ODQ3Nw=="}, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 112}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/2af69135eea3d773561cebc017b36a8794e71ca4", "committedDate": "2020-06-03T23:47:04Z", "message": "Bootstrap datasource changes"}, "afterCommit": {"oid": "c8295f28784d0b5cd7882a8b767b15fc271421f7", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/c8295f28784d0b5cd7882a8b767b15fc271421f7", "committedDate": "2020-08-04T21:41:16Z", "message": "Bootstrap datasource integration"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c8295f28784d0b5cd7882a8b767b15fc271421f7", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/c8295f28784d0b5cd7882a8b767b15fc271421f7", "committedDate": "2020-08-04T21:41:16Z", "message": "Bootstrap datasource integration"}, "afterCommit": {"oid": "9d21da8bff4d8f23bcf8cc6a88b4feef840d08bb", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/9d21da8bff4d8f23bcf8cc6a88b4feef840d08bb", "committedDate": "2020-08-05T00:02:24Z", "message": "Bootstrap datasource integration"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9d21da8bff4d8f23bcf8cc6a88b4feef840d08bb", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/9d21da8bff4d8f23bcf8cc6a88b4feef840d08bb", "committedDate": "2020-08-05T00:02:24Z", "message": "Bootstrap datasource integration"}, "afterCommit": {"oid": "313385d95fa406a5adb7b198899abc14b572ee4a", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/313385d95fa406a5adb7b198899abc14b572ee4a", "committedDate": "2020-08-05T23:41:36Z", "message": "Bootstrap datasource integration"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "313385d95fa406a5adb7b198899abc14b572ee4a", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/313385d95fa406a5adb7b198899abc14b572ee4a", "committedDate": "2020-08-05T23:41:36Z", "message": "Bootstrap datasource integration"}, "afterCommit": {"oid": "08e84812173d8126bc01c00b1c79ffaa5b80d623", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/08e84812173d8126bc01c00b1c79ffaa5b80d623", "committedDate": "2020-08-06T02:30:25Z", "message": "Bootstrap datasource integration"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyMjgwMDQ3", "url": "https://github.com/apache/hudi/pull/1702#pullrequestreview-462280047", "createdAt": "2020-08-06T08:06:46Z", "commit": {"oid": "08e84812173d8126bc01c00b1c79ffaa5b80d623"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwODowNjo0NlrOG8n86Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwODoxNTo1NVrOG8oRXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyMjMxMw==", "bodyText": "the bootstrap.base.path is now in hoodie.properties. Should can we make this transparent for the user?", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466222313", "createdAt": "2020-08-06T08:06:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/DefaultSource.scala", "diffHunk": "@@ -54,29 +58,54 @@ class DefaultSource extends RelationProvider\n     val parameters = Map(QUERY_TYPE_OPT_KEY -> DEFAULT_QUERY_TYPE_OPT_VAL) ++ translateViewTypesToQueryTypes(optParams)\n \n     val path = parameters.get(\"path\")\n-    if (path.isEmpty) {\n-      throw new HoodieException(\"'path' must be specified.\")\n-    }\n \n     if (parameters(QUERY_TYPE_OPT_KEY).equals(QUERY_TYPE_SNAPSHOT_OPT_VAL)) {\n-      // this is just effectively RO view only, where `path` can contain a mix of\n-      // non-hoodie/hoodie path files. set the path filter up\n-      sqlContext.sparkContext.hadoopConfiguration.setClass(\n-        \"mapreduce.input.pathFilter.class\",\n-        classOf[HoodieROTablePathFilter],\n-        classOf[org.apache.hadoop.fs.PathFilter])\n-\n-      log.info(\"Constructing hoodie (as parquet) data source with options :\" + parameters)\n-      log.warn(\"Snapshot view not supported yet via data source, for MERGE_ON_READ tables. \" +\n-        \"Please query the Hive table registered using Spark SQL.\")\n-      // simply return as a regular parquet relation\n-      DataSource.apply(\n-        sparkSession = sqlContext.sparkSession,\n-        userSpecifiedSchema = Option(schema),\n-        className = \"parquet\",\n-        options = parameters)\n-        .resolveRelation()\n+      val readPathsStr = parameters.get(DataSourceReadOptions.READ_PATHS_OPT_KEY)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTkzNDkxMg=="}, "originalCommit": {"oid": "2af69135eea3d773561cebc017b36a8794e71ca4"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyMzI1MQ==", "bodyText": "we can just check metaClient.getTableConfig() for the bootstrap base path?", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466223251", "createdAt": "2020-08-06T08:08:17Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/DefaultSource.scala", "diffHunk": "@@ -56,29 +58,56 @@ class DefaultSource extends RelationProvider\n     val parameters = Map(QUERY_TYPE_OPT_KEY -> DEFAULT_QUERY_TYPE_OPT_VAL) ++ translateViewTypesToQueryTypes(optParams)\n \n     val path = parameters.get(\"path\")\n-    if (path.isEmpty) {\n-      throw new HoodieException(\"'path' must be specified.\")\n-    }\n \n     if (parameters(QUERY_TYPE_OPT_KEY).equals(QUERY_TYPE_SNAPSHOT_OPT_VAL)) {\n-      // this is just effectively RO view only, where `path` can contain a mix of\n-      // non-hoodie/hoodie path files. set the path filter up\n-      sqlContext.sparkContext.hadoopConfiguration.setClass(\n-        \"mapreduce.input.pathFilter.class\",\n-        classOf[HoodieROTablePathFilter],\n-        classOf[org.apache.hadoop.fs.PathFilter])\n-\n-      log.info(\"Constructing hoodie (as parquet) data source with options :\" + parameters)\n-      log.warn(\"Snapshot view not supported yet via data source, for MERGE_ON_READ tables. \" +\n-        \"Please query the Hive table registered using Spark SQL.\")\n-      // simply return as a regular parquet relation\n-      DataSource.apply(\n-        sparkSession = sqlContext.sparkSession,\n-        userSpecifiedSchema = Option(schema),\n-        className = \"parquet\",\n-        options = parameters)\n-        .resolveRelation()\n+      val readPathsStr = parameters.get(DataSourceReadOptions.READ_PATHS_OPT_KEY)\n+      if (path.isEmpty && readPathsStr.isEmpty) {\n+        throw new HoodieException(s\"'path' or '$READ_PATHS_OPT_KEY' or both must be specified.\")\n+      }\n+\n+      val readPaths = readPathsStr.map(p => p.split(\",\").toSeq).getOrElse(Seq())\n+      val allPaths = path.map(p => Seq(p)).getOrElse(Seq()) ++ readPaths\n+\n+      val fs = FSUtils.getFs(allPaths.head, sqlContext.sparkContext.hadoopConfiguration)\n+      val globPaths = HudiSparkUtils.checkAndGlobPathIfNecessary(allPaths, fs)\n+\n+      val tablePath = DataSourceUtils.getTablePath(fs, globPaths.toArray)\n+      log.info(\"Obtained hudi table path: \" + tablePath)\n+\n+      val metaClient = new HoodieTableMetaClient(fs.getConf, tablePath)\n+      val bootstrapIndex = BootstrapIndex.getBootstrapIndex(metaClient)\n+\n+      val isBootstrappedTable = bootstrapIndex.useIndex()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08e84812173d8126bc01c00b1c79ffaa5b80d623"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyNTY4NA==", "bodyText": "please name the classes Hoodie consistent with rest of the code base. I renamed few other classes that were checked in earlier, on gary's pr", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466225684", "createdAt": "2020-08-06T08:12:37Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HudiBootstrapRDD.scala", "diffHunk": "@@ -0,0 +1,131 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi\n+\n+import org.apache.spark.{Partition, TaskContext}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.execution.datasources.PartitionedFile\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.vectorized.ColumnarBatch\n+\n+class HudiBootstrapRDD(@transient spark: SparkSession,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08e84812173d8126bc01c00b1c79ffaa5b80d623"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIyNzU1MA==", "bodyText": "bootstrap is one time operation done during migration. not sure if the user wants to incrementally fetch that existing data as a single batch. Not sure what it means to incrementally query from bootstrap instant ts?  can we just avoid this whole thing by only supporting incremental query on commit timestamps after bootstrap?", "url": "https://github.com/apache/hudi/pull/1702#discussion_r466227550", "createdAt": "2020-08-06T08:15:55Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/IncrementalRelation.scala", "diffHunk": "@@ -92,36 +102,69 @@ class IncrementalRelation(val sqlContext: SQLContext,\n   override def schema: StructType = latestSchema\n \n   override def buildScan(): RDD[Row] = {\n-    val fileIdToFullPath = mutable.HashMap[String, String]()\n+    val regularFileIdToFullPath = mutable.HashMap[String, String]()\n+    var metaBootstrapFileIdToFullPath = mutable.HashMap[String, String]()\n+\n     for (commit <- commitsToReturn) {\n       val metadata: HoodieCommitMetadata = HoodieCommitMetadata.fromBytes(commitTimeline.getInstantDetails(commit)\n         .get, classOf[HoodieCommitMetadata])\n-      fileIdToFullPath ++= metadata.getFileIdAndFullPaths(basePath).toMap\n+\n+      if (HoodieTimeline.METADATA_BOOTSTRAP_INSTANT_TS == commit.getTimestamp) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08e84812173d8126bc01c00b1c79ffaa5b80d623"}, "originalPosition": 71}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "08e84812173d8126bc01c00b1c79ffaa5b80d623", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/08e84812173d8126bc01c00b1c79ffaa5b80d623", "committedDate": "2020-08-06T02:30:25Z", "message": "Bootstrap datasource integration"}, "afterCommit": {"oid": "4fcd7fee5a57f7e38089dea31f713e36892c43e1", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/4fcd7fee5a57f7e38089dea31f713e36892c43e1", "committedDate": "2020-08-06T23:44:12Z", "message": "Bootstrap datasource integration"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4fcd7fee5a57f7e38089dea31f713e36892c43e1", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/4fcd7fee5a57f7e38089dea31f713e36892c43e1", "committedDate": "2020-08-06T23:44:12Z", "message": "Bootstrap datasource integration"}, "afterCommit": {"oid": "923a67826ee904ccadc6225917bfc8974b118ef6", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/923a67826ee904ccadc6225917bfc8974b118ef6", "committedDate": "2020-08-07T09:00:10Z", "message": "Bootstrap datasource integration"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "923a67826ee904ccadc6225917bfc8974b118ef6", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/923a67826ee904ccadc6225917bfc8974b118ef6", "committedDate": "2020-08-07T09:00:10Z", "message": "Bootstrap datasource integration"}, "afterCommit": {"oid": "7fe1bfaedd14266440e7339e6dfa15a4d62f8034", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/7fe1bfaedd14266440e7339e6dfa15a4d62f8034", "committedDate": "2020-08-07T22:16:41Z", "message": "Bootstrap datasource integration"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7fe1bfaedd14266440e7339e6dfa15a4d62f8034", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/7fe1bfaedd14266440e7339e6dfa15a4d62f8034", "committedDate": "2020-08-07T22:16:41Z", "message": "Bootstrap datasource integration"}, "afterCommit": {"oid": "3f7ecde4438fe79d282187616557c5ca451055bf", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/3f7ecde4438fe79d282187616557c5ca451055bf", "committedDate": "2020-08-07T22:56:08Z", "message": "Bootstrap datasource integration"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzNzAyOTM5", "url": "https://github.com/apache/hudi/pull/1702#pullrequestreview-463702939", "createdAt": "2020-08-08T01:00:21Z", "commit": {"oid": "3f7ecde4438fe79d282187616557c5ca451055bf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOFQwMTowMDoyMVrOG9sQCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOFQwMTowMDoyMVrOG9sQCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzM0MTMyMg==", "bodyText": "I believe add https://github.com/apache/hudi/blob/master/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala#L55 will resolve this issue.", "url": "https://github.com/apache/hudi/pull/1702#discussion_r467341322", "createdAt": "2020-08-08T01:00:21Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/test/scala/org/apache/hudi/functional/TestDataSourceForBootstrap.scala", "diffHunk": "@@ -0,0 +1,616 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.functional\n+\n+import java.time.Instant\n+import java.util.Collections\n+\n+import collection.JavaConverters._\n+import org.apache.hadoop.fs.FileSystem\n+import org.apache.hudi.bootstrap.SparkParquetBootstrapDataProvider\n+import org.apache.hudi.client.TestBootstrap\n+import org.apache.hudi.client.bootstrap.selector.FullRecordBootstrapModeSelector\n+import org.apache.hudi.{DataSourceReadOptions, DataSourceWriteOptions, HoodieDataSourceHelpers}\n+import org.apache.hudi.common.fs.FSUtils\n+import org.apache.hudi.common.table.timeline.HoodieTimeline\n+import org.apache.hudi.config.{HoodieBootstrapConfig, HoodieCompactionConfig, HoodieWriteConfig}\n+import org.apache.hudi.keygen.SimpleKeyGenerator\n+import org.apache.spark.api.java.JavaSparkContext\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.{SaveMode, SparkSession}\n+import org.junit.jupiter.api.Assertions.assertEquals\n+import org.junit.jupiter.api.{BeforeEach, Test}\n+import org.junit.jupiter.api.io.TempDir\n+\n+class TestDataSourceForBootstrap {\n+\n+  var spark: SparkSession = _\n+  val commonOpts = Map(\n+    HoodieWriteConfig.INSERT_PARALLELISM -> \"4\",\n+    HoodieWriteConfig.UPSERT_PARALLELISM -> \"4\",\n+    HoodieWriteConfig.DELETE_PARALLELISM -> \"4\",\n+    HoodieWriteConfig.BULKINSERT_PARALLELISM -> \"4\",\n+    HoodieWriteConfig.FINALIZE_WRITE_PARALLELISM -> \"4\",\n+    HoodieBootstrapConfig.BOOTSTRAP_PARALLELISM -> \"4\",\n+    DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY -> \"_row_key\",\n+    DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY -> \"partition\",\n+    DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY -> \"timestamp\",\n+    HoodieWriteConfig.TABLE_NAME -> \"hoodie_test\"\n+  )\n+  var basePath: String = _\n+  var srcPath: String = _\n+  var fs: FileSystem = _\n+\n+  @BeforeEach def initialize(@TempDir tempDir: java.nio.file.Path) {\n+    spark = SparkSession.builder\n+      .appName(\"Hoodie Datasource test\")\n+      .master(\"local[2]\")\n+      .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n+      .getOrCreate\n+    basePath = tempDir.toAbsolutePath.toString + \"/base\"\n+    srcPath = tempDir.toAbsolutePath.toString + \"/src\"\n+    fs = FSUtils.getFs(basePath, spark.sparkContext.hadoopConfiguration)\n+  }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f7ecde4438fe79d282187616557c5ca451055bf"}, "originalPosition": 69}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3f7ecde4438fe79d282187616557c5ca451055bf", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/3f7ecde4438fe79d282187616557c5ca451055bf", "committedDate": "2020-08-07T22:56:08Z", "message": "Bootstrap datasource integration"}, "afterCommit": {"oid": "caa597a30293eeb7c09d50c03a7feb3f79678495", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/caa597a30293eeb7c09d50c03a7feb3f79678495", "committedDate": "2020-08-08T01:59:15Z", "message": "Bootstrap datasource integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "952a499de55625b66eaab84829c369d8d5e77d22", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/952a499de55625b66eaab84829c369d8d5e77d22", "committedDate": "2020-08-08T02:03:07Z", "message": "Bootstrap datasource integration"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "caa597a30293eeb7c09d50c03a7feb3f79678495", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/caa597a30293eeb7c09d50c03a7feb3f79678495", "committedDate": "2020-08-08T01:59:15Z", "message": "Bootstrap datasource integration"}, "afterCommit": {"oid": "952a499de55625b66eaab84829c369d8d5e77d22", "author": {"user": {"login": "umehrot2", "name": "Udit Mehrotra"}}, "url": "https://github.com/apache/hudi/commit/952a499de55625b66eaab84829c369d8d5e77d22", "committedDate": "2020-08-08T02:03:07Z", "message": "Bootstrap datasource integration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ff41ded339c5fc9d079c08bf40a51b97891bfeb9", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/ff41ded339c5fc9d079c08bf40a51b97891bfeb9", "committedDate": "2020-08-09T09:31:44Z", "message": "Merge branch 'master' into umehrot2_hudi_rfc12_code_review"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzODU4Nzky", "url": "https://github.com/apache/hudi/pull/1702#pullrequestreview-463858792", "createdAt": "2020-08-09T09:40:00Z", "commit": {"oid": "ff41ded339c5fc9d079c08bf40a51b97891bfeb9"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e8c3361cc860ee3e64cacd3acebead9ecd5e996b", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/e8c3361cc860ee3e64cacd3acebead9ecd5e996b", "committedDate": "2020-08-09T16:18:52Z", "message": "Fix unit-tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzODk3NjMx", "url": "https://github.com/apache/hudi/pull/1702#pullrequestreview-463897631", "createdAt": "2020-08-09T19:28:59Z", "commit": {"oid": "e8c3361cc860ee3e64cacd3acebead9ecd5e996b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxOToyOTowMFrOG99QCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wOVQxOToyOTowMFrOG99QCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzYxOTg0OQ==", "bodyText": "@umehrot2 : ITTestBootstrapCommand is failing with the below exception. Adding a sanitization API to remove illegal characters from avro field names\nException in thread \"main\" org.apache.avro.SchemaParseException: Illegal character in: test-table_record\n    at org.apache.avro.Schema.validateName(Schema.java:1151)\n    at org.apache.avro.Schema.access$200(Schema.java:81)\n    at org.apache.avro.Schema$Name.<init>(Schema.java:489)\n    at org.apache.avro.Schema.createRecord(Schema.java:161)\n    at org.apache.avro.SchemaBuilder$RecordBuilder.fields(SchemaBuilder.java:1732)\n    at org.apache.spark.sql.avro.SchemaConverters$.toAvroType(SchemaConverters.scala:173)\n    at org.apache.spark.sql.avro.SchemaConverters.toAvroType(SchemaConverters.scala)\n    at org.apache.hudi.client.bootstrap.BootstrapSchemaProvider.getBootstrapSourceSchema(BootstrapSchemaProvider.java:97)\n    at org.apache.hudi.client.bootstrap.BootstrapSchemaProvider.getBootstrapSchema(BootstrapSchemaProvider.java:66)\n    at org.apache.hudi.table.action.bootstrap.BootstrapCommitActionExecutor.listAndProcessSourcePartitions(BootstrapCommitActionExecutor.java:288)", "url": "https://github.com/apache/hudi/pull/1702#discussion_r467619849", "createdAt": "2020-08-09T19:29:00Z", "author": {"login": "bvaradar"}, "path": "hudi-client/src/main/java/org/apache/hudi/client/bootstrap/BootstrapSchemaProvider.java", "diffHunk": "@@ -64,14 +75,25 @@ public final Schema getBootstrapSchema(JavaSparkContext jsc, List<Pair<String, L\n    */\n   protected Schema getBootstrapSourceSchema(JavaSparkContext jsc,\n       List<Pair<String, List<HoodieFileStatus>>> partitions) {\n-    return partitions.stream().flatMap(p -> p.getValue().stream())\n-        .map(fs -> {\n-          try {\n-            Path filePath = FileStatusUtils.toPath(fs.getPath());\n-            return ParquetUtils.readAvroSchema(jsc.hadoopConfiguration(), filePath);\n-          } catch (Exception ex) {\n-            return null;\n-          }\n-        }).filter(x -> x != null).findAny().get();\n+    MessageType parquetSchema = partitions.stream().flatMap(p -> p.getValue().stream()).map(fs -> {\n+      try {\n+        Path filePath = FileStatusUtils.toPath(fs.getPath());\n+        return ParquetUtils.readSchema(jsc.hadoopConfiguration(), filePath);\n+      } catch (Exception ex) {\n+        return null;\n+      }\n+    }).filter(Objects::nonNull).findAny()\n+        .orElseThrow(() -> new HoodieException(\"Could not determine schema from the data files.\"));\n+\n+\n+    ParquetToSparkSchemaConverter converter = new ParquetToSparkSchemaConverter(\n+            Boolean.parseBoolean(SQLConf.PARQUET_BINARY_AS_STRING().defaultValueString()),\n+            Boolean.parseBoolean(SQLConf.PARQUET_INT96_AS_TIMESTAMP().defaultValueString()));\n+    StructType sparkSchema = converter.convert(parquetSchema);\n+    String tableName = writeConfig.getTableName();\n+    String structName = tableName + \"_record\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e8c3361cc860ee3e64cacd3acebead9ecd5e996b"}, "originalPosition": 67}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1ce422e53f6de4fbec05918ac489b29a65de5bd5", "author": {"user": {"login": "bvaradar", "name": "Balaji Varadarajan"}}, "url": "https://github.com/apache/hudi/commit/1ce422e53f6de4fbec05918ac489b29a65de5bd5", "committedDate": "2020-08-09T19:37:11Z", "message": "Add spark-avro dependency in hudi-cli and sanitize Avro field names"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4918, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}