{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg0NDUwNzQ0", "number": 1377, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQwNDoxNToxNFrODlrNxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQwNDoxNToxNFrODlrNxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQwODMxOTQxOnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "isResolved": true, "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQwNDoxNToxNFrOFystSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMlQwMTowNzo1NFrOF1ORGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODcwNTYwOA==", "bodyText": "I think this may potentially hide some concerning errors.\ne.g. The delta streamer is consuming Kafka source, but a hidden bug happens and stored an empty checkpoint.  The next run will just ignore the empty checkpoint and reset to the LATEST. Then there will be data loss", "url": "https://github.com/apache/hudi/pull/1377#discussion_r388705608", "createdAt": "2020-03-06T04:15:14Z", "author": {"login": "garyli1019"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -180,7 +180,7 @@ public KafkaOffsetGen(TypedProperties props) {\n               .map(x -> new TopicPartition(x.topic(), x.partition())).collect(Collectors.toSet());\n \n       // Determine the offset ranges to read from\n-      if (lastCheckpointStr.isPresent()) {\n+      if (lastCheckpointStr.isPresent() && !lastCheckpointStr.get().isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "821c45490919046be36a84a22fda12c65670934e"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODkzMDI4Ng==", "bodyText": "hi, if a hidden bug happens, the *.commit file will not be created, the next run will still read from the last successful commit, so no data will be lossed.", "url": "https://github.com/apache/hudi/pull/1377#discussion_r388930286", "createdAt": "2020-03-06T14:23:59Z", "author": {"login": "lamberken"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -180,7 +180,7 @@ public KafkaOffsetGen(TypedProperties props) {\n               .map(x -> new TopicPartition(x.topic(), x.partition())).collect(Collectors.toSet());\n \n       // Determine the offset ranges to read from\n-      if (lastCheckpointStr.isPresent()) {\n+      if (lastCheckpointStr.isPresent() && !lastCheckpointStr.get().isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODcwNTYwOA=="}, "originalCommit": {"oid": "821c45490919046be36a84a22fda12c65670934e"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTA4OTMwMA==", "bodyText": "Right. As you mentioned it is still possible that some wrong user behaviors might lead to an empty checkpoint. From a user perspective, I'd say if there is an empty checkpoint in the last commit, I will prefer to let the job fail other than automatically reset the checkpoint. Throw an exception if the checkpoint is empty would make more sense to me and let the user decide whether they wanna reset or not. WDYT?", "url": "https://github.com/apache/hudi/pull/1377#discussion_r389089300", "createdAt": "2020-03-06T19:10:39Z", "author": {"login": "garyli1019"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -180,7 +180,7 @@ public KafkaOffsetGen(TypedProperties props) {\n               .map(x -> new TopicPartition(x.topic(), x.partition())).collect(Collectors.toSet());\n \n       // Determine the offset ranges to read from\n-      if (lastCheckpointStr.isPresent()) {\n+      if (lastCheckpointStr.isPresent() && !lastCheckpointStr.get().isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODcwNTYwOA=="}, "originalCommit": {"oid": "821c45490919046be36a84a22fda12c65670934e"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQ2NjIzOA==", "bodyText": "@lamber-ken : Also, Instead of handling the empty checkpoints only for kafka, can we handle it generically in DeltaSync (https://github.com/apache/incubator-hudi/blob/77d5b92d88d6583bdfc09e4c10ecfe7ddbb04806/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java#L262) so that we can have an uniform handling of checkpoints across sources ?", "url": "https://github.com/apache/hudi/pull/1377#discussion_r390466238", "createdAt": "2020-03-10T16:58:18Z", "author": {"login": "bvaradar"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -180,7 +180,7 @@ public KafkaOffsetGen(TypedProperties props) {\n               .map(x -> new TopicPartition(x.topic(), x.partition())).collect(Collectors.toSet());\n \n       // Determine the offset ranges to read from\n-      if (lastCheckpointStr.isPresent()) {\n+      if (lastCheckpointStr.isPresent() && !lastCheckpointStr.get().isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODcwNTYwOA=="}, "originalCommit": {"oid": "821c45490919046be36a84a22fda12c65670934e"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY5NTUxNg==", "bodyText": "hi @bvaradar, add !commitMetadata.getMetadata(CHECKPOINT_KEY).isEmpty().\nbut if we do that, the application will always throw HoodieDeltaStreamerException", "url": "https://github.com/apache/hudi/pull/1377#discussion_r390695516", "createdAt": "2020-03-11T01:09:22Z", "author": {"login": "lamberken"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -180,7 +180,7 @@ public KafkaOffsetGen(TypedProperties props) {\n               .map(x -> new TopicPartition(x.topic(), x.partition())).collect(Collectors.toSet());\n \n       // Determine the offset ranges to read from\n-      if (lastCheckpointStr.isPresent()) {\n+      if (lastCheckpointStr.isPresent() && !lastCheckpointStr.get().isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODcwNTYwOA=="}, "originalCommit": {"oid": "821c45490919046be36a84a22fda12c65670934e"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM0NDg2Nw==", "bodyText": "I agree with @bvaradar here.\nI tried\nelse if (commitMetadata.getMetadata(CHECKPOINT_KEY) != null && !commitMetadata.getMetadata(CHECKPOINT_KEY).isEmpty()) {\n          resumeCheckpointStr = Option.of(commitMetadata.getMetadata(CHECKPOINT_KEY));\n        }\nThe unit tests were passed.\nIf you mean when the checkpoint was empty, the application was throwing an Exception. I think it should the desired behavior, WDYT?", "url": "https://github.com/apache/hudi/pull/1377#discussion_r391344867", "createdAt": "2020-03-12T00:35:24Z", "author": {"login": "garyli1019"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -180,7 +180,7 @@ public KafkaOffsetGen(TypedProperties props) {\n               .map(x -> new TopicPartition(x.topic(), x.partition())).collect(Collectors.toSet());\n \n       // Determine the offset ranges to read from\n-      if (lastCheckpointStr.isPresent()) {\n+      if (lastCheckpointStr.isPresent() && !lastCheckpointStr.get().isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODcwNTYwOA=="}, "originalCommit": {"oid": "821c45490919046be36a84a22fda12c65670934e"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM0ODExMQ==", "bodyText": "hi @garyli1019, let's imagine a scenario that a topic with no data, the first commit will save empty checkpoint, then the second commit will always throw exception(even if we send msg to kafka). In that case, EARLIEST or LATEST will no longer has any effect.", "url": "https://github.com/apache/hudi/pull/1377#discussion_r391348111", "createdAt": "2020-03-12T00:48:35Z", "author": {"login": "lamberken"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -180,7 +180,7 @@ public KafkaOffsetGen(TypedProperties props) {\n               .map(x -> new TopicPartition(x.topic(), x.partition())).collect(Collectors.toSet());\n \n       // Determine the offset ranges to read from\n-      if (lastCheckpointStr.isPresent()) {\n+      if (lastCheckpointStr.isPresent() && !lastCheckpointStr.get().isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODcwNTYwOA=="}, "originalCommit": {"oid": "821c45490919046be36a84a22fda12c65670934e"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM0ODkxNg==", "bodyText": "Prefer to pushdown the control bebavior to datasource(e.g kafka / pulsar)", "url": "https://github.com/apache/hudi/pull/1377#discussion_r391348916", "createdAt": "2020-03-12T00:51:36Z", "author": {"login": "lamberken"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -180,7 +180,7 @@ public KafkaOffsetGen(TypedProperties props) {\n               .map(x -> new TopicPartition(x.topic(), x.partition())).collect(Collectors.toSet());\n \n       // Determine the offset ranges to read from\n-      if (lastCheckpointStr.isPresent()) {\n+      if (lastCheckpointStr.isPresent() && !lastCheckpointStr.get().isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODcwNTYwOA=="}, "originalCommit": {"oid": "821c45490919046be36a84a22fda12c65670934e"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM1MDQwMQ==", "bodyText": "In this case I think the user could use cfg.checkpoint = xxx to reset the checkpoint. It would be concerning for me if the deltastreamer automatically reset the checkpoint for me and I didn't aware of it.", "url": "https://github.com/apache/hudi/pull/1377#discussion_r391350401", "createdAt": "2020-03-12T00:58:17Z", "author": {"login": "garyli1019"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -180,7 +180,7 @@ public KafkaOffsetGen(TypedProperties props) {\n               .map(x -> new TopicPartition(x.topic(), x.partition())).collect(Collectors.toSet());\n \n       // Determine the offset ranges to read from\n-      if (lastCheckpointStr.isPresent()) {\n+      if (lastCheckpointStr.isPresent() && !lastCheckpointStr.get().isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODcwNTYwOA=="}, "originalCommit": {"oid": "821c45490919046be36a84a22fda12c65670934e"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTM1MjYwMQ==", "bodyText": "Thanks, I need to think about it, wait a moment : )", "url": "https://github.com/apache/hudi/pull/1377#discussion_r391352601", "createdAt": "2020-03-12T01:07:54Z", "author": {"login": "lamberken"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/sources/helpers/KafkaOffsetGen.java", "diffHunk": "@@ -180,7 +180,7 @@ public KafkaOffsetGen(TypedProperties props) {\n               .map(x -> new TopicPartition(x.topic(), x.partition())).collect(Collectors.toSet());\n \n       // Determine the offset ranges to read from\n-      if (lastCheckpointStr.isPresent()) {\n+      if (lastCheckpointStr.isPresent() && !lastCheckpointStr.get().isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODcwNTYwOA=="}, "originalCommit": {"oid": "821c45490919046be36a84a22fda12c65670934e"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4884, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}