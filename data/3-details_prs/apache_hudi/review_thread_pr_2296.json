{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMyMzg0MzA3", "number": 2296, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxNDoyMjo0MVrOFC8zrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNy0yOVQwMDoxMTozOFrOGbtufQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NjM3NzQwOnYy", "diffSide": "LEFT", "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxNDoyMjo0MVrOICXHRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwNjoyMzoyOFrOIC3-hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTM0Njc1Ng==", "bodyText": "Hi @pengzhiwei2018  I think this is necessary. It is possible that the incoming rdd is empty and we need to handle that. Are you seeing any performance issues here?\nWorth to mention that this is the first Spark action, if you are seeing isEmpty() take too long in Spark UI, it's probably the transformation before this action.", "url": "https://github.com/apache/hudi/pull/2296#discussion_r539346756", "createdAt": "2020-12-09T14:22:41Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -178,11 +178,6 @@ private[hudi] object HoodieSparkSqlWriter {\n             } else {\n               hoodieAllIncomingRecords\n             }\n-\n-          if (hoodieRecords.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "19bdadefb4eab5f42c12ce0260fbbf3fb3590acd"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTgxNDQxNQ==", "bodyText": "Hi @garyli1019 ,In our case,the incoming rdd is very complex, this isEmpty test spend so much time.And if the incoming rdd is empty, the code that follows will execute quickly.So I think this isEmpty test is unnecessary.", "url": "https://github.com/apache/hudi/pull/2296#discussion_r539814415", "createdAt": "2020-12-10T03:19:32Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -178,11 +178,6 @@ private[hudi] object HoodieSparkSqlWriter {\n             } else {\n               hoodieAllIncomingRecords\n             }\n-\n-          if (hoodieRecords.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTM0Njc1Ng=="}, "originalCommit": {"oid": "19bdadefb4eab5f42c12ce0260fbbf3fb3590acd"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTgyODI0MQ==", "bodyText": "Hi @pengzhiwei2018 , I guess this isEmpty() triggerred the complex rdd transformations, the actual time consuming part is the transformation before this check. We can double check by put something like show(1) before the isEmpty check, then the time-consuming part should become the show(1) and the isEmpty check should finish fast.\nIIRC, an empty rdd will trigger an error later if we don't have this check here. If that's not the case, would you write a unit test to verify?", "url": "https://github.com/apache/hudi/pull/2296#discussion_r539828241", "createdAt": "2020-12-10T04:02:06Z", "author": {"login": "garyli1019"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -178,11 +178,6 @@ private[hudi] object HoodieSparkSqlWriter {\n             } else {\n               hoodieAllIncomingRecords\n             }\n-\n-          if (hoodieRecords.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTM0Njc1Ng=="}, "originalCommit": {"oid": "19bdadefb4eab5f42c12ce0260fbbf3fb3590acd"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTg4NTE5MQ==", "bodyText": "Hi @garyli1019 ,Thanks for you suggestion.I will add some test case later.", "url": "https://github.com/apache/hudi/pull/2296#discussion_r539885191", "createdAt": "2020-12-10T06:23:28Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -178,11 +178,6 @@ private[hudi] object HoodieSparkSqlWriter {\n             } else {\n               hoodieAllIncomingRecords\n             }\n-\n-          if (hoodieRecords.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTM0Njc1Ng=="}, "originalCommit": {"oid": "19bdadefb4eab5f42c12ce0260fbbf3fb3590acd"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMTcxMzY5OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "isResolved": false, "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwMjo0ODoyMlrOIF2XDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxNTowODo1M1rOIeVUQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzAwNDQzMA==", "bodyText": "I don't think we should make an empty commit. Empty commits might pollute the timeline, cleaning, compactions e.t.c. and trigger some unexpected behaviors. WDYT?", "url": "https://github.com/apache/hudi/pull/2296#discussion_r543004430", "createdAt": "2020-12-15T02:48:22Z", "author": {"login": "garyli1019"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -320,4 +320,21 @@ class TestCOWDataSource extends HoodieClientTestBase {\n \n     assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))\n   }\n+\n+  @Test def testWithEmptyInput(): Unit = {\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(Seq.empty[String], 1))\n+    inputDF1.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Overwrite)\n+      .save(basePath)\n+    assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f86f5b5619394a9e99ccf6c855a1a82b3a095b7d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzEyNzg3OA==", "bodyText": "I agree. We may have unexpected bugs for this in the future.\nWhy do SparkRDDWriteClient or HoodieTable create a new commit in case of empty dataset (or no record received)?", "url": "https://github.com/apache/hudi/pull/2296#discussion_r543127878", "createdAt": "2020-12-15T08:06:33Z", "author": {"login": "mauropelucchi"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -320,4 +320,21 @@ class TestCOWDataSource extends HoodieClientTestBase {\n \n     assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))\n   }\n+\n+  @Test def testWithEmptyInput(): Unit = {\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(Seq.empty[String], 1))\n+    inputDF1.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Overwrite)\n+      .save(basePath)\n+    assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzAwNDQzMA=="}, "originalCommit": {"oid": "f86f5b5619394a9e99ccf6c855a1a82b3a095b7d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzE2MjE2OA==", "bodyText": "Since this PR is trying to remove the isEmpty() check. If we have that check, we won't make an empty commit.", "url": "https://github.com/apache/hudi/pull/2296#discussion_r543162168", "createdAt": "2020-12-15T09:00:17Z", "author": {"login": "garyli1019"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -320,4 +320,21 @@ class TestCOWDataSource extends HoodieClientTestBase {\n \n     assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))\n   }\n+\n+  @Test def testWithEmptyInput(): Unit = {\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(Seq.empty[String], 1))\n+    inputDF1.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Overwrite)\n+      .save(basePath)\n+    assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzAwNDQzMA=="}, "originalCommit": {"oid": "f86f5b5619394a9e99ccf6c855a1a82b3a095b7d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzIyMjgwMA==", "bodyText": "Well, it seems that we miss a return key word for the isEmpty if-branch in the origin implement. So I think this PR should not bring new bugs.", "url": "https://github.com/apache/hudi/pull/2296#discussion_r543222800", "createdAt": "2020-12-15T10:25:21Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -320,4 +320,21 @@ class TestCOWDataSource extends HoodieClientTestBase {\n \n     assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))\n   }\n+\n+  @Test def testWithEmptyInput(): Unit = {\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(Seq.empty[String], 1))\n+    inputDF1.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Overwrite)\n+      .save(basePath)\n+    assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzAwNDQzMA=="}, "originalCommit": {"oid": "f86f5b5619394a9e99ccf6c855a1a82b3a095b7d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Mzg1MTY3NA==", "bodyText": "This seems like a bug introduced here... https://github.com/apache/hudi/pull/1121/files\nWe should definitely return if the incoming record is empty. cc: @vinothchandar WDYT?", "url": "https://github.com/apache/hudi/pull/2296#discussion_r543851674", "createdAt": "2020-12-16T02:43:10Z", "author": {"login": "garyli1019"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -320,4 +320,21 @@ class TestCOWDataSource extends HoodieClientTestBase {\n \n     assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))\n   }\n+\n+  @Test def testWithEmptyInput(): Unit = {\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(Seq.empty[String], 1))\n+    inputDF1.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Overwrite)\n+      .save(basePath)\n+    assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzAwNDQzMA=="}, "originalCommit": {"oid": "f86f5b5619394a9e99ccf6c855a1a82b3a095b7d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk5Mzk1MA==", "bodyText": "I might be mistaken here. I see delta streamer and Flink could also make an empty commit. If empty commits are not considering as a bug, then this should be fine.\ncc: @bvaradar @vinothchandar can you guys make the call here?", "url": "https://github.com/apache/hudi/pull/2296#discussion_r544993950", "createdAt": "2020-12-17T10:51:25Z", "author": {"login": "garyli1019"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -320,4 +320,21 @@ class TestCOWDataSource extends HoodieClientTestBase {\n \n     assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))\n   }\n+\n+  @Test def testWithEmptyInput(): Unit = {\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(Seq.empty[String], 1))\n+    inputDF1.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Overwrite)\n+      .save(basePath)\n+    assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzAwNDQzMA=="}, "originalCommit": {"oid": "f86f5b5619394a9e99ccf6c855a1a82b3a095b7d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0MDYwOQ==", "bodyText": "I too vote for not making an empty commit if its empty. Will wait to hear from Vinoth/balaji", "url": "https://github.com/apache/hudi/pull/2296#discussion_r567240609", "createdAt": "2021-01-30T12:32:38Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -320,4 +320,21 @@ class TestCOWDataSource extends HoodieClientTestBase {\n \n     assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))\n   }\n+\n+  @Test def testWithEmptyInput(): Unit = {\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(Seq.empty[String], 1))\n+    inputDF1.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Overwrite)\n+      .save(basePath)\n+    assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzAwNDQzMA=="}, "originalCommit": {"oid": "f86f5b5619394a9e99ccf6c855a1a82b3a095b7d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI1MDIyMA==", "bodyText": "Hi @nsivabalan ,I also agree that not making an empty commit. Because it would produce many useless commit files for streaming job when the source of the streaming is in the business trough. And I have update the code to skip the empty commit.", "url": "https://github.com/apache/hudi/pull/2296#discussion_r567250220", "createdAt": "2021-01-30T14:07:37Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -320,4 +320,21 @@ class TestCOWDataSource extends HoodieClientTestBase {\n \n     assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))\n   }\n+\n+  @Test def testWithEmptyInput(): Unit = {\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(Seq.empty[String], 1))\n+    inputDF1.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Overwrite)\n+      .save(basePath)\n+    assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzAwNDQzMA=="}, "originalCommit": {"oid": "f86f5b5619394a9e99ccf6c855a1a82b3a095b7d"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY3NzQ0Mw==", "bodyText": "The empty commits has skip now. And the test case has fixed @nsivabalan @garyli1019 . Please take a look when you have time. Thanks~", "url": "https://github.com/apache/hudi/pull/2296#discussion_r568677443", "createdAt": "2021-02-02T15:08:53Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -320,4 +320,21 @@ class TestCOWDataSource extends HoodieClientTestBase {\n \n     assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))\n   }\n+\n+  @Test def testWithEmptyInput(): Unit = {\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(Seq.empty[String], 1))\n+    inputDF1.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Overwrite)\n+      .save(basePath)\n+    assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzAwNDQzMA=="}, "originalCommit": {"oid": "f86f5b5619394a9e99ccf6c855a1a82b3a095b7d"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQxMTcyNzMxOnYy", "diffSide": "LEFT", "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwMjo1Mzo0OVrOIF2eZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNVQwMjo1Mzo0OVrOIF2eZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MzAwNjMxMQ==", "bodyText": "Have you done any experiments on this? Like putting another Spark action before this.\nrdd.isEmpty() is the most efficient way I could find that will do the empty check. Maybe we can change the description of this action so the user won't misunderstand?", "url": "https://github.com/apache/hudi/pull/2296#discussion_r543006311", "createdAt": "2020-12-15T02:53:49Z", "author": {"login": "garyli1019"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -182,11 +182,6 @@ private[hudi] object HoodieSparkSqlWriter {\n             } else {\n               hoodieAllIncomingRecords\n             }\n-\n-          if (hoodieRecords.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f86f5b5619394a9e99ccf6c855a1a82b3a095b7d"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4MzMwOTM2OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxNToyNjozOVrOIeWOYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNy0xNFQxNTowMjo0MVrOJ-rRvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MjMyMw==", "bodyText": "Will this check impact the performance of non-empty commit? Will the previous stages being cached?\nI personally feel it's better to do the empty check before run into hudi's operation.", "url": "https://github.com/apache/hudi/pull/2296#discussion_r568692323", "createdAt": "2021-02-02T15:26:39Z", "author": {"login": "garyli1019"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -173,6 +173,10 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n \n   public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n                              String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+    // Skip the empty commit\n+    if (stats.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "673e8e7b1b2893676a6b507b66b12e2625409ab7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODcyODI5OA==", "bodyText": "Hi @garyli1019 , it is just a java list#isEmpty here ,so it should has no impact for the performance. In our previous implement, we use the RDD#isEmpty to skip the empty commit, It is a heavy operation.", "url": "https://github.com/apache/hudi/pull/2296#discussion_r568728298", "createdAt": "2021-02-02T16:10:29Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -173,6 +173,10 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n \n   public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n                              String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+    // Skip the empty commit\n+    if (stats.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MjMyMw=="}, "originalCommit": {"oid": "673e8e7b1b2893676a6b507b66b12e2625409ab7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2OTIyMDgxNg==", "bodyText": "What I was not sure about is if this will change the Spark DAGs. But looks like it will not.\n@vinothchandar @nsivabalan can you guys take a look as well?", "url": "https://github.com/apache/hudi/pull/2296#discussion_r569220816", "createdAt": "2021-02-03T08:34:36Z", "author": {"login": "garyli1019"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -173,6 +173,10 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n \n   public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n                              String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+    // Skip the empty commit\n+    if (stats.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MjMyMw=="}, "originalCommit": {"oid": "673e8e7b1b2893676a6b507b66b12e2625409ab7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDIyNDAyMQ==", "bodyText": "Guess this is code path when auto commit is disabled. Here is the path when autoCommit is set to true. May be we need to fix here as well. But would be nice if we clean the commit property rather than just returning( .requested, .inflight etc). Basically rollback this pending commit.\nbut let's hear @vinothchandar thoughts as well whether can we delay an empty dataset to this extent or should we fail fast before itself.", "url": "https://github.com/apache/hudi/pull/2296#discussion_r570224021", "createdAt": "2021-02-04T13:31:20Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -173,6 +173,10 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n \n   public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n                              String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+    // Skip the empty commit\n+    if (stats.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MjMyMw=="}, "originalCommit": {"oid": "673e8e7b1b2893676a6b507b66b12e2625409ab7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDQ3NjYwMw==", "bodyText": "I think there was an explicit ask to allow the empty commit before. Lets take deltastreamer which stores the offset of the kafka checkpoints in the commit metadata. If we don't commit when stats are empty the checkpoint will never advance. The transformer  in delta streamer could filter out all records read in that batch for e.g and lead to an empty commit. but the kafka offsets would have advanced. So its not good to do this IMO", "url": "https://github.com/apache/hudi/pull/2296#discussion_r570476603", "createdAt": "2021-02-04T19:10:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -173,6 +173,10 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n \n   public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n                              String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+    // Skip the empty commit\n+    if (stats.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MjMyMw=="}, "originalCommit": {"oid": "673e8e7b1b2893676a6b507b66b12e2625409ab7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NTgzMTU3Ng==", "bodyText": "lets control this using a new config?", "url": "https://github.com/apache/hudi/pull/2296#discussion_r665831576", "createdAt": "2021-07-08T02:48:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -173,6 +173,10 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n \n   public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n                              String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+    // Skip the empty commit\n+    if (stats.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MjMyMw=="}, "originalCommit": {"oid": "673e8e7b1b2893676a6b507b66b12e2625409ab7"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2OTcwMDU0MA==", "bodyText": "yes, it is better for this.  And by default we can allow empty commits.", "url": "https://github.com/apache/hudi/pull/2296#discussion_r669700540", "createdAt": "2021-07-14T15:02:41Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -173,6 +173,10 @@ public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Opti\n \n   public boolean commitStats(String instantTime, List<HoodieWriteStat> stats, Option<Map<String, String>> extraMetadata,\n                              String commitActionType, Map<String, List<String>> partitionToReplaceFileIds) {\n+    // Skip the empty commit\n+    if (stats.isEmpty()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MjMyMw=="}, "originalCommit": {"oid": "673e8e7b1b2893676a6b507b66b12e2625409ab7"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU4MzMxMzY5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestHoodieCompactor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxNToyNzoyOFrOIeWRGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wMlQxNjoxMTo0MVrOIeYepA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MzAxNw==", "bodyText": "should remain unchanged?", "url": "https://github.com/apache/hudi/pull/2296#discussion_r568693017", "createdAt": "2021-02-02T15:27:28Z", "author": {"login": "garyli1019"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestHoodieCompactor.java", "diffHunk": "@@ -186,7 +191,7 @@ public void testWriteStatusContentsAfterCompaction() throws Exception {\n       for (String partitionPath : dataGen.getPartitionPaths()) {\n         List<WriteStatus> writeStatuses = result.collect();\n         assertTrue(writeStatuses.stream()\n-            .filter(writeStatus -> writeStatus.getStat().getPartitionPath().contentEquals(partitionPath)).count() > 0);\n+            .filter(writeStatus1 -> writeStatus1.getStat().getPartitionPath().contentEquals(partitionPath)).count() > 0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "673e8e7b1b2893676a6b507b66b12e2625409ab7"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODcyOTI1Mg==", "bodyText": "Thanks for the suggestion!", "url": "https://github.com/apache/hudi/pull/2296#discussion_r568729252", "createdAt": "2021-02-02T16:11:41Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/compact/TestHoodieCompactor.java", "diffHunk": "@@ -186,7 +191,7 @@ public void testWriteStatusContentsAfterCompaction() throws Exception {\n       for (String partitionPath : dataGen.getPartitionPaths()) {\n         List<WriteStatus> writeStatuses = result.collect();\n         assertTrue(writeStatuses.stream()\n-            .filter(writeStatus -> writeStatus.getStat().getPartitionPath().contentEquals(partitionPath)).count() > 0);\n+            .filter(writeStatus1 -> writeStatus1.getStat().getPartitionPath().contentEquals(partitionPath)).count() > 0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2ODY5MzAxNw=="}, "originalCommit": {"oid": "673e8e7b1b2893676a6b507b66b12e2625409ab7"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkNDIzMDgwNzY0OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNy0wOFQwMjo0OTozMFrOJ6_JZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNy0xNFQxNDo1ODoyOVrOJ-rCng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NTgzMTc4Mg==", "bodyText": "empty input, you mean?", "url": "https://github.com/apache/hudi/pull/2296#discussion_r665831782", "createdAt": "2021-07-08T02:49:30Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -348,4 +348,23 @@ class TestCOWDataSource extends HoodieClientTestBase {\n \n     assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))\n   }\n+\n+  @Test def testWithEmptyInput(): Unit = {\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(Seq.empty[String], 1))\n+    inputDF1.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Overwrite)\n+      .save(basePath)\n+    // Empty commit does not has a new commit", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4e3d89e0c02f1b9a704a61627f7aafec14e67e6"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2OTY5NjY3MA==", "bodyText": "yes, will correct it.", "url": "https://github.com/apache/hudi/pull/2296#discussion_r669696670", "createdAt": "2021-07-14T14:58:29Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -348,4 +348,23 @@ class TestCOWDataSource extends HoodieClientTestBase {\n \n     assertTrue(HoodieDataSourceHelpers.hasNewCommits(fs, basePath, \"000\"))\n   }\n+\n+  @Test def testWithEmptyInput(): Unit = {\n+    val inputDF1 = spark.read.json(spark.sparkContext.parallelize(Seq.empty[String], 1))\n+    inputDF1.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, DataSourceWriteOptions.INSERT_OPERATION_OPT_VAL)\n+      .mode(SaveMode.Overwrite)\n+      .save(basePath)\n+    // Empty commit does not has a new commit", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NTgzMTc4Mg=="}, "originalCommit": {"oid": "d4e3d89e0c02f1b9a704a61627f7aafec14e67e6"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkNDIzMDgxMDczOnYy", "diffSide": "RIGHT", "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNy0wOFQwMjo1MDo1NVrOJ6_LGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNy0xNFQxNTowMzozOVrOJ-rVJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NTgzMjIxNg==", "bodyText": "we should actually have this generate an empty commit and test. If we don't then we checkpoints won't move.\nConsider this scenario, when deltastreamer reads from kafka using a custom transformer. If the transformer filters out all records from Kafka, we will have empty input for write, but the kafka offsets have to move ahead.", "url": "https://github.com/apache/hudi/pull/2296#discussion_r665832216", "createdAt": "2021-07-08T02:50:55Z", "author": {"login": "vinothchandar"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java", "diffHunk": "@@ -932,15 +932,10 @@ public void testFilterDupes() throws Exception {\n     ds2.sync();\n     mClient = new HoodieTableMetaClient(jsc.hadoopConfiguration(), tableBasePath, true);\n     HoodieInstant newLastFinished = mClient.getCommitsTimeline().filterCompletedInstants().lastInstant().get();\n-    assertTrue(HoodieTimeline.compareTimestamps(newLastFinished.getTimestamp(), HoodieTimeline.GREATER_THAN, lastFinished.getTimestamp()\n+    // there is not new commit generate for empty commits", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4e3d89e0c02f1b9a704a61627f7aafec14e67e6"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2OTcwMTQxMg==", "bodyText": "Yes, can understand this. Will add config to control this and by default we allow the empty commits.", "url": "https://github.com/apache/hudi/pull/2296#discussion_r669701412", "createdAt": "2021-07-14T15:03:39Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/functional/TestHoodieDeltaStreamer.java", "diffHunk": "@@ -932,15 +932,10 @@ public void testFilterDupes() throws Exception {\n     ds2.sync();\n     mClient = new HoodieTableMetaClient(jsc.hadoopConfiguration(), tableBasePath, true);\n     HoodieInstant newLastFinished = mClient.getCommitsTimeline().filterCompletedInstants().lastInstant().get();\n-    assertTrue(HoodieTimeline.compareTimestamps(newLastFinished.getTimestamp(), HoodieTimeline.GREATER_THAN, lastFinished.getTimestamp()\n+    // there is not new commit generate for empty commits", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY2NTgzMjIxNg=="}, "originalCommit": {"oid": "d4e3d89e0c02f1b9a704a61627f7aafec14e67e6"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkNDMxNzEzOTE3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNy0yOVQwMDoxMTozOFrOKHSjzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNy0yOVQwMDoxMTozOFrOKHSjzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3ODczMjc0OA==", "bodyText": "reword: Whether to allow generation of empty commits, even if no data was written in the commit. It's useful in cases where extra metadata needs to be published regardless e.g tracking source offsets when ingesting data", "url": "https://github.com/apache/hudi/pull/2296#discussion_r678732748", "createdAt": "2021-07-29T00:11:38Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -366,6 +366,11 @@\n       .withDocumentation(\"When enabled, records in older schema are rewritten into newer schema during upsert,delete and background\"\n           + \" compaction,clustering operations.\");\n \n+  public static final ConfigProperty<Boolean> ALLOW_EMPTY_COMMIT = ConfigProperty\n+       .key(\"hoodie.allow.empty.commmit\")\n+       .defaultValue(true)\n+       .withDocumentation(\"Whether to allow generate empty commit when the input is empty.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6ac9d507dc541a7e17741c3e1fcd8a6bb28aecc6"}, "originalPosition": 7}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4219, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}