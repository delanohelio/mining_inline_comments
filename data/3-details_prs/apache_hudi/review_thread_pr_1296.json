{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY5NTYxOTc5", "number": 1296, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQwMTowMDo1M1rODcZLqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQwMTowMjowOVrODcZMGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDk5MzA3OnYy", "diffSide": "RIGHT", "path": "docs/_pages/releases.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQwMTowMDo1M1rOFkbhyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMlQxMzozNzo0M1rOFkhydg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc0NDA3Mg==", "bodyText": "Would you please also change to --storage-type , --table-type, since I find --storage-type and --table-type are changed to -storage-type and -table-type after regenerated in the site(https://hudi.apache.org/releases.html), thanks.", "url": "https://github.com/apache/hudi/pull/1296#discussion_r373744072", "createdAt": "2020-02-01T01:00:53Z", "author": {"login": "leesf"}, "path": "docs/_pages/releases.md", "diffHunk": "@@ -13,36 +13,36 @@ last_modified_at: 2019-12-30T15:59:57-04:00\n  * Apache Hudi (incubating) jars corresponding to this release is available [here](https://repository.apache.org/#nexus-search;quick~hudi)\n \n ### Release Highlights\n-* Dependency Version Upgrades\n-    * Upgrade from Spark 2.1.0 to Spark 2.4.4\n-    * Upgrade from Avro 1.7.7 to Avro 1.8.2\n-    * Upgrade from Parquet 1.8.1 to Parquet 1.10.1\n-    * Upgrade from Kafka 0.8.2.1 to Kafka 2.0.0 as a result of updating spark-streaming-kafka artifact from 0.8_2.11/2.12 to 0.10_2.11/2.12.\n-* **IMPORTANT** This version requires your runtime spark version to be upgraded to 2.4+.\n-* Hudi now supports both Scala 2.11 and Scala 2.12, please refer to [Build with Scala 2.12](https://github.com/apache/incubator-hudi#build-with-scala-212) to build with Scala 2.12.\n-Also, the packages hudi-spark, hudi-utilities, hudi-spark-bundle and hudi-utilities-bundle are changed correspondingly to hudi-spark_{scala_version}, hudi-spark_{scala_version}, hudi-utilities_{scala_version}, hudi-spark-bundle_{scala_version} and hudi-utilities-bundle_{scala_version}.\n-Note that scala_version here is one of (2.11, 2.12).\n-* With 0.5.1, we added functionality to stop using renames for Hudi timeline metadata operations. This feature is automatically enabled for newly created Hudi tables. For existing tables, this feature is turned off by default. Please read this [section](https://hudi.apache.org/docs/deployment.html#upgrading), before enabling this feature for existing hudi tables.\n-To enable the new hudi timeline layout which avoids renames, use the write config \"hoodie.timeline.layout.version=1\". Alternatively, you can use \"repair overwrite-hoodie-props\" to append the line \"hoodie.timeline.layout.version=1\" to hoodie.properties. Note that in any case, upgrade hudi readers (query engines) first with 0.5.1-incubating release before upgrading writer.\n-* CLI supports `repair overwrite-hoodie-props` to overwrite the table's hoodie.properties with specified file, for one-time updates to table name or even enabling the new timeline layout above. Note that few queries may temporarily fail while the overwrite happens (few milliseconds).\n-* DeltaStreamer CLI parameter for capturing table type is changed from --storage-type to --table-type. Refer to [wiki](https://cwiki.apache.org/confluence/display/HUDI/Design+And+Architecture) with more latest terminologies.\n-* Configuration Value change for Kafka Reset Offset Strategies. Enum values are changed from LARGEST to LATEST, SMALLEST to EARLIEST for configuring Kafka reset offset strategies with configuration(auto.offset.reset) in deltastreamer.\n-* When using spark-shell to give a quick peek at Hudi, please provide `--packages org.apache.spark:spark-avro_2.11:2.4.4`, more details would refer to [latest quickstart docs](https://hudi.apache.org/docs/quick-start-guide.html)\n-* Key generator moved to separate package under org.apache.hudi.keygen. If you are using overridden key generator classes (configuration (\"hoodie.datasource.write.keygenerator.class\")) that comes with hudi package, please ensure the fully qualified class name is changed accordingly.\n-* Hive Sync tool will register RO tables for MOR with a _ro suffix, so query with _ro suffix. You would use `--skip-ro-suffix` in sync config in sync config to retain the old naming without the _ro suffix.\n-* With 0.5.1, hudi-hadoop-mr-bundle which is used by query engines such as presto and hive includes shaded avro package to support hudi real time queries through these engines. Hudi supports pluggable logic for merging of records. Users provide their own implementation of [HoodieRecordPayload](https://github.com/apache/incubator-hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordPayload.java).\n-If you are using this feature, you need to relocate the avro dependencies in your custom record payload class to be consistent with internal hudi shading. You need to add the following relocation when shading the package containing the record payload implementation.\n-\n- ```xml\n-<relocation>\n-    <pattern>org.apache.avro.</pattern>\n-    <shadedPattern>org.apache.hudi.org.apache.avro.</shadedPattern>\n-</relocation>\n- ```\n+ * Dependency Version Upgrades\n+   - Upgrade from Spark 2.1.0 to Spark 2.4.4\n+   - Upgrade from Avro 1.7.7 to Avro 1.8.2\n+   - Upgrade from Parquet 1.8.1 to Parquet 1.10.1\n+   - Upgrade from Kafka 0.8.2.1 to Kafka 2.0.0 as a result of updating spark-streaming-kafka artifact from 0.8_2.11/2.12 to 0.10_2.11/2.12.\n+ * **IMPORTANT** This version requires your runtime spark version to be upgraded to 2.4+.\n+ * Hudi now supports both Scala 2.11 and Scala 2.12, please refer to [Build with Scala 2.12](https://github.com/apache/incubator-hudi#build-with-scala-212) to build with Scala 2.12.\n+   Also, the packages hudi-spark, hudi-utilities, hudi-spark-bundle and hudi-utilities-bundle are changed correspondingly to hudi-spark_{scala_version}, hudi-spark_{scala_version}, hudi-utilities_{scala_version}, hudi-spark-bundle_{scala_version} and hudi-utilities-bundle_{scala_version}.\n+   Note that scala_version here is one of (2.11, 2.12).\n+ * With 0.5.1, we added functionality to stop using renames for Hudi timeline metadata operations. This feature is automatically enabled for newly created Hudi tables. For existing tables, this feature is turned off by default. Please read this [section](https://hudi.apache.org/docs/deployment.html#upgrading), before enabling this feature for existing hudi tables.\n+   To enable the new hudi timeline layout which avoids renames, use the write config \"hoodie.timeline.layout.version=1\". Alternatively, you can use \"repair overwrite-hoodie-props\" to append the line \"hoodie.timeline.layout.version=1\" to hoodie.properties. Note that in any case, upgrade hudi readers (query engines) first with 0.5.1-incubating release before upgrading writer.\n+ * CLI supports `repair overwrite-hoodie-props` to overwrite the table's hoodie.properties with specified file, for one-time updates to table name or even enabling the new timeline layout above. Note that few queries may temporarily fail while the overwrite happens (few milliseconds).\n+ * DeltaStreamer CLI parameter for capturing table type is changed from --storage-type to --table-type. Refer to [wiki](https://cwiki.apache.org/confluence/display/HUDI/Design+And+Architecture) with more latest terminologies.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e4370699b8418b7cc91e807fa0e509c41830602"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzg0NjY0Ng==", "bodyText": "Done", "url": "https://github.com/apache/hudi/pull/1296#discussion_r373846646", "createdAt": "2020-02-02T13:37:43Z", "author": {"login": "lamberken"}, "path": "docs/_pages/releases.md", "diffHunk": "@@ -13,36 +13,36 @@ last_modified_at: 2019-12-30T15:59:57-04:00\n  * Apache Hudi (incubating) jars corresponding to this release is available [here](https://repository.apache.org/#nexus-search;quick~hudi)\n \n ### Release Highlights\n-* Dependency Version Upgrades\n-    * Upgrade from Spark 2.1.0 to Spark 2.4.4\n-    * Upgrade from Avro 1.7.7 to Avro 1.8.2\n-    * Upgrade from Parquet 1.8.1 to Parquet 1.10.1\n-    * Upgrade from Kafka 0.8.2.1 to Kafka 2.0.0 as a result of updating spark-streaming-kafka artifact from 0.8_2.11/2.12 to 0.10_2.11/2.12.\n-* **IMPORTANT** This version requires your runtime spark version to be upgraded to 2.4+.\n-* Hudi now supports both Scala 2.11 and Scala 2.12, please refer to [Build with Scala 2.12](https://github.com/apache/incubator-hudi#build-with-scala-212) to build with Scala 2.12.\n-Also, the packages hudi-spark, hudi-utilities, hudi-spark-bundle and hudi-utilities-bundle are changed correspondingly to hudi-spark_{scala_version}, hudi-spark_{scala_version}, hudi-utilities_{scala_version}, hudi-spark-bundle_{scala_version} and hudi-utilities-bundle_{scala_version}.\n-Note that scala_version here is one of (2.11, 2.12).\n-* With 0.5.1, we added functionality to stop using renames for Hudi timeline metadata operations. This feature is automatically enabled for newly created Hudi tables. For existing tables, this feature is turned off by default. Please read this [section](https://hudi.apache.org/docs/deployment.html#upgrading), before enabling this feature for existing hudi tables.\n-To enable the new hudi timeline layout which avoids renames, use the write config \"hoodie.timeline.layout.version=1\". Alternatively, you can use \"repair overwrite-hoodie-props\" to append the line \"hoodie.timeline.layout.version=1\" to hoodie.properties. Note that in any case, upgrade hudi readers (query engines) first with 0.5.1-incubating release before upgrading writer.\n-* CLI supports `repair overwrite-hoodie-props` to overwrite the table's hoodie.properties with specified file, for one-time updates to table name or even enabling the new timeline layout above. Note that few queries may temporarily fail while the overwrite happens (few milliseconds).\n-* DeltaStreamer CLI parameter for capturing table type is changed from --storage-type to --table-type. Refer to [wiki](https://cwiki.apache.org/confluence/display/HUDI/Design+And+Architecture) with more latest terminologies.\n-* Configuration Value change for Kafka Reset Offset Strategies. Enum values are changed from LARGEST to LATEST, SMALLEST to EARLIEST for configuring Kafka reset offset strategies with configuration(auto.offset.reset) in deltastreamer.\n-* When using spark-shell to give a quick peek at Hudi, please provide `--packages org.apache.spark:spark-avro_2.11:2.4.4`, more details would refer to [latest quickstart docs](https://hudi.apache.org/docs/quick-start-guide.html)\n-* Key generator moved to separate package under org.apache.hudi.keygen. If you are using overridden key generator classes (configuration (\"hoodie.datasource.write.keygenerator.class\")) that comes with hudi package, please ensure the fully qualified class name is changed accordingly.\n-* Hive Sync tool will register RO tables for MOR with a _ro suffix, so query with _ro suffix. You would use `--skip-ro-suffix` in sync config in sync config to retain the old naming without the _ro suffix.\n-* With 0.5.1, hudi-hadoop-mr-bundle which is used by query engines such as presto and hive includes shaded avro package to support hudi real time queries through these engines. Hudi supports pluggable logic for merging of records. Users provide their own implementation of [HoodieRecordPayload](https://github.com/apache/incubator-hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/model/HoodieRecordPayload.java).\n-If you are using this feature, you need to relocate the avro dependencies in your custom record payload class to be consistent with internal hudi shading. You need to add the following relocation when shading the package containing the record payload implementation.\n-\n- ```xml\n-<relocation>\n-    <pattern>org.apache.avro.</pattern>\n-    <shadedPattern>org.apache.hudi.org.apache.avro.</shadedPattern>\n-</relocation>\n- ```\n+ * Dependency Version Upgrades\n+   - Upgrade from Spark 2.1.0 to Spark 2.4.4\n+   - Upgrade from Avro 1.7.7 to Avro 1.8.2\n+   - Upgrade from Parquet 1.8.1 to Parquet 1.10.1\n+   - Upgrade from Kafka 0.8.2.1 to Kafka 2.0.0 as a result of updating spark-streaming-kafka artifact from 0.8_2.11/2.12 to 0.10_2.11/2.12.\n+ * **IMPORTANT** This version requires your runtime spark version to be upgraded to 2.4+.\n+ * Hudi now supports both Scala 2.11 and Scala 2.12, please refer to [Build with Scala 2.12](https://github.com/apache/incubator-hudi#build-with-scala-212) to build with Scala 2.12.\n+   Also, the packages hudi-spark, hudi-utilities, hudi-spark-bundle and hudi-utilities-bundle are changed correspondingly to hudi-spark_{scala_version}, hudi-spark_{scala_version}, hudi-utilities_{scala_version}, hudi-spark-bundle_{scala_version} and hudi-utilities-bundle_{scala_version}.\n+   Note that scala_version here is one of (2.11, 2.12).\n+ * With 0.5.1, we added functionality to stop using renames for Hudi timeline metadata operations. This feature is automatically enabled for newly created Hudi tables. For existing tables, this feature is turned off by default. Please read this [section](https://hudi.apache.org/docs/deployment.html#upgrading), before enabling this feature for existing hudi tables.\n+   To enable the new hudi timeline layout which avoids renames, use the write config \"hoodie.timeline.layout.version=1\". Alternatively, you can use \"repair overwrite-hoodie-props\" to append the line \"hoodie.timeline.layout.version=1\" to hoodie.properties. Note that in any case, upgrade hudi readers (query engines) first with 0.5.1-incubating release before upgrading writer.\n+ * CLI supports `repair overwrite-hoodie-props` to overwrite the table's hoodie.properties with specified file, for one-time updates to table name or even enabling the new timeline layout above. Note that few queries may temporarily fail while the overwrite happens (few milliseconds).\n+ * DeltaStreamer CLI parameter for capturing table type is changed from --storage-type to --table-type. Refer to [wiki](https://cwiki.apache.org/confluence/display/HUDI/Design+And+Architecture) with more latest terminologies.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc0NDA3Mg=="}, "originalCommit": {"oid": "5e4370699b8418b7cc91e807fa0e509c41830602"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMxMDk5NDE3OnYy", "diffSide": "RIGHT", "path": "docs/_sass/hudi_style/_variables.scss", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMVQwMTowMjowOVrOFkbiZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0wMlQxMzozMToyOVrOFkhwow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc0NDIzMQ==", "bodyText": "This line changed on purpose?", "url": "https://github.com/apache/hudi/pull/1296#discussion_r373744231", "createdAt": "2020-02-01T01:02:09Z", "author": {"login": "leesf"}, "path": "docs/_sass/hudi_style/_variables.scss", "diffHunk": "@@ -54,7 +54,7 @@ $light-gray: mix(#fff, $gray, 50%) !default;\n $lighter-gray: mix(#fff, $gray, 90%) !default;\n \n $background-color: #fff !default;\n-$code-background-color: #fafafa !default;\n+$code-background-color: #f3f3f3 !default;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5e4370699b8418b7cc91e807fa0e509c41830602"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzg0NjE3OQ==", "bodyText": "Darken background color.", "url": "https://github.com/apache/hudi/pull/1296#discussion_r373846179", "createdAt": "2020-02-02T13:31:29Z", "author": {"login": "lamberken"}, "path": "docs/_sass/hudi_style/_variables.scss", "diffHunk": "@@ -54,7 +54,7 @@ $light-gray: mix(#fff, $gray, 50%) !default;\n $lighter-gray: mix(#fff, $gray, 90%) !default;\n \n $background-color: #fff !default;\n-$code-background-color: #fafafa !default;\n+$code-background-color: #f3f3f3 !default;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc0NDIzMQ=="}, "originalCommit": {"oid": "5e4370699b8418b7cc91e807fa0e509c41830602"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 30, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}