{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAyNTEwMDc2", "number": 1511, "title": "[HUDI-789]Adjust logic of upsert in HDFSParquetImporter", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\n*In HDFSParquetImporter, upsert is equivalent to insert (remove old metadata, then create and insert data). But upsert means update and insert on old data. *\nBrief change log\n(for example:)\n\nAdjust logic of upsert in HDFSParquetImporter\n\nVerify this pull request\nThis pull request is a trivial rework / code cleanup without any test coverage.\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-04-13T07:15:12Z", "url": "https://github.com/apache/hudi/pull/1511", "merged": true, "mergeCommit": {"oid": "84dd9047d3902650d7ff5bc95b9789d6880ca8e2"}, "closed": true, "closedAt": "2020-04-21T06:21:31Z", "author": {"login": "hddong"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcXJhqJgH2gAyNDAyNTEwMDc2OmEwNjRkZmNmODc1MmRkMzk1MzNkMzFlZjM5N2I5MzhkYjAzMWQ5ZGE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcZtly8AFqTM5NzAxNzkxNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "a064dfcf8752dd39533d31ef397b938db031d9da", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/a064dfcf8752dd39533d31ef397b938db031d9da", "committedDate": "2020-04-13T07:12:15Z", "message": "Adjust logic of upsert in HDFSParquetImporter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35", "committedDate": "2020-04-13T09:53:03Z", "message": "fix"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyNTQ3MzY2", "url": "https://github.com/apache/hudi/pull/1511#pullrequestreview-392547366", "createdAt": "2020-04-14T01:40:42Z", "commit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMTo0MDo0MlrOGE7FzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQwMTo0MDo0MlrOGE7FzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzgxNTYyOQ==", "bodyText": "use boolean flag ?", "url": "https://github.com/apache/hudi/pull/1511#discussion_r407815629", "createdAt": "2020-04-14T01:40:42Z", "author": {"login": "hmatu"}, "path": "hudi-utilities/src/main/java/org/apache/hudi/utilities/HDFSParquetImporter.java", "diffHunk": "@@ -100,6 +100,10 @@ public static void main(String[] args) {\n \n   }\n \n+  private boolean isUpsert() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0NDI1OTQ0", "url": "https://github.com/apache/hudi/pull/1511#pullrequestreview-394425944", "createdAt": "2020-04-16T08:50:31Z", "commit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwODo1MDozMVrOGGbJcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQwOTowNToxMFrOGGbvFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM4OTQyNQ==", "bodyText": "Does this method use to init records for inserting? IMO, we should distinguish it with upsert.", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409389425", "createdAt": "2020-04-16T08:50:31Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      List<HoodieModel> expected = insertData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test upsert data and verify data consistency.\n+   */\n+  @Test\n+  public void testImportWithUpsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+\n+      // Create schema file.\n+      String schemaFile = new Path(basePath, \"file.schema\").toString();\n+\n+      Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n+      List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n+\n+      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n+          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+      cfg.command = \"upsert\";\n+      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+      dataImporter.dataImport(jsc, 0);\n+\n+      // construct result, remove top 10 and add upsert data.\n+      List<GenericRecord> expectData = insertData.subList(11, 96);\n+      expectData.addAll(upsertData);\n+\n+      // read latest data\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      // get expected result\n+      List<HoodieModel> expected = expectData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  private List<GenericRecord> createRecords(Path srcFolder) throws ParseException, IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5MzIwNg==", "bodyText": "Here exists redundant boxing issues. It would be better to use Double.parseDouble(xxx).", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409393206", "createdAt": "2020-04-16T08:56:15Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      List<HoodieModel> expected = insertData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test upsert data and verify data consistency.\n+   */\n+  @Test\n+  public void testImportWithUpsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+\n+      // Create schema file.\n+      String schemaFile = new Path(basePath, \"file.schema\").toString();\n+\n+      Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n+      List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n+\n+      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n+          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+      cfg.command = \"upsert\";\n+      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+      dataImporter.dataImport(jsc, 0);\n+\n+      // construct result, remove top 10 and add upsert data.\n+      List<GenericRecord> expectData = insertData.subList(11, 96);\n+      expectData.addAll(upsertData);\n+\n+      // read latest data\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      // get expected result\n+      List<HoodieModel> expected = expectData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5NDY3NA==", "bodyText": "Can we use try-with-resource here?", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409394674", "createdAt": "2020-04-16T08:58:19Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      List<HoodieModel> expected = insertData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test upsert data and verify data consistency.\n+   */\n+  @Test\n+  public void testImportWithUpsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5NTY2OA==", "bodyText": "I would suggest one field one line for so many arguments.", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409395668", "createdAt": "2020-04-16T08:59:50Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      List<HoodieModel> expected = insertData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test upsert data and verify data consistency.\n+   */\n+  @Test\n+  public void testImportWithUpsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+\n+      // Create schema file.\n+      String schemaFile = new Path(basePath, \"file.schema\").toString();\n+\n+      Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n+      List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n+\n+      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n+          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+      cfg.command = \"upsert\";\n+      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+      dataImporter.dataImport(jsc, 0);\n+\n+      // construct result, remove top 10 and add upsert data.\n+      List<GenericRecord> expectData = insertData.subList(11, 96);\n+      expectData.addAll(upsertData);\n+\n+      // read latest data\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      // get expected result\n+      List<HoodieModel> expected = expectData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5NjE1Ng==", "bodyText": "ditto, try-with-resource?", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409396156", "createdAt": "2020-04-16T09:00:36Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5NjgzOQ==", "bodyText": "ParseException would never be thrown.", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409396839", "createdAt": "2020-04-16T09:01:42Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM5OTA2MA==", "bodyText": "As a good habit, when we override the equals method, it would also be better to override the hashCode even though we did use it here.", "url": "https://github.com/apache/hudi/pull/1511#discussion_r409399060", "createdAt": "2020-04-16T09:05:10Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -275,4 +403,44 @@ private JavaSparkContext getJavaSparkContext() {\n     sparkConf = HoodieWriteClient.registerClasses(sparkConf);\n     return new JavaSparkContext(HoodieReadClient.addHoodieSupport(sparkConf));\n   }\n+\n+  /**\n+   * Class used for compare result and expected.\n+   */\n+  private class HoodieModel {\n+    double timestamp;\n+    String rowKey;\n+    String rider;\n+    String driver;\n+    double beginLat;\n+    double beginLon;\n+    double endLat;\n+    double endLon;\n+\n+    private HoodieModel(double timestamp, String rowKey, String rider, String driver, double beginLat,\n+        double beginLon, double endLat, double endLon) {\n+      this.timestamp = timestamp;\n+      this.rowKey = rowKey;\n+      this.rider = rider;\n+      this.driver = driver;\n+      this.beginLat = beginLat;\n+      this.beginLon = beginLon;\n+      this.endLat = endLat;\n+      this.endLon = endLon;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 271}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e1a47ff32f900d9c723dc907784210ce756915f9", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/e1a47ff32f900d9c723dc907784210ce756915f9", "committedDate": "2020-04-17T05:41:26Z", "message": "Address"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6c68f1bfb76fb1144e6ec88678dd7eead4a60376", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/6c68f1bfb76fb1144e6ec88678dd7eead4a60376", "committedDate": "2020-04-17T07:04:52Z", "message": "remove use HashCodeBuilder"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/56bc39dfda9158e1f060dffa1c18a423ebc0ffed", "committedDate": "2020-04-17T09:10:05Z", "message": "Reset"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2MTAzNDA3", "url": "https://github.com/apache/hudi/pull/1511#pullrequestreview-396103407", "createdAt": "2020-04-20T02:24:05Z", "commit": {"oid": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQwMjoyNDowNVrOGIAqRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMFQwMzoxNzowMFrOGIBfAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA1MjYxNQ==", "bodyText": "It's a bit long. Here, I would suggest using Objects.hash(...).", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411052615", "createdAt": "2020-04-20T02:24:05Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -275,4 +380,70 @@ private JavaSparkContext getJavaSparkContext() {\n     sparkConf = HoodieWriteClient.registerClasses(sparkConf);\n     return new JavaSparkContext(HoodieReadClient.addHoodieSupport(sparkConf));\n   }\n+\n+  /**\n+   * Class used for compare result and expected.\n+   */\n+  private class HoodieModel {\n+    double timestamp;\n+    String rowKey;\n+    String rider;\n+    String driver;\n+    double beginLat;\n+    double beginLon;\n+    double endLat;\n+    double endLon;\n+\n+    private HoodieModel(double timestamp, String rowKey, String rider, String driver, double beginLat,\n+        double beginLon, double endLat, double endLon) {\n+      this.timestamp = timestamp;\n+      this.rowKey = rowKey;\n+      this.rider = rider;\n+      this.driver = driver;\n+      this.beginLat = beginLat;\n+      this.beginLon = beginLon;\n+      this.endLat = endLat;\n+      this.endLon = endLon;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      if (this == o) {\n+        return true;\n+      }\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+      HoodieModel other = (HoodieModel) o;\n+      return timestamp == other.timestamp && rowKey.equals(other.rowKey) && rider.equals(other.rider)\n+          && driver.equals(other.driver) && beginLat == other.beginLat && beginLon == other.beginLon\n+          && endLat == other.endLat && endLon == other.endLon;\n+    }\n+\n+    @Override\n+    public int hashCode() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed"}, "originalPosition": 332}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA1MzUxMQ==", "bodyText": "Can we mark this class to be a static class and rename it to HoodieTripModel?", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411053511", "createdAt": "2020-04-20T02:27:55Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -275,4 +380,70 @@ private JavaSparkContext getJavaSparkContext() {\n     sparkConf = HoodieWriteClient.registerClasses(sparkConf);\n     return new JavaSparkContext(HoodieReadClient.addHoodieSupport(sparkConf));\n   }\n+\n+  /**\n+   * Class used for compare result and expected.\n+   */\n+  private class HoodieModel {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed"}, "originalPosition": 295}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA1NDE0Ng==", "bodyText": "I mean for matching purpose, can we rename it to createInsertRecords?", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411054146", "createdAt": "2020-04-20T02:30:11Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -157,7 +176,103 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException, ParseException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      List<HoodieModel> expected = insertData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Test upsert data and verify data consistency.\n+   */\n+  @Test\n+  public void testImportWithUpsert() throws IOException, ParseException {\n+    JavaSparkContext jsc = null;\n+    try {\n+      jsc = getJavaSparkContext();\n+      insert(jsc);\n+\n+      // Create schema file.\n+      String schemaFile = new Path(basePath, \"file.schema\").toString();\n+\n+      Path upsertFolder = new Path(basePath, \"testUpsertSrc\");\n+      List<GenericRecord> upsertData = createUpsertRecords(upsertFolder);\n+\n+      HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(upsertFolder.toString(), hoodieFolder.toString(),\n+          \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+      cfg.command = \"upsert\";\n+      HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+      dataImporter.dataImport(jsc, 0);\n+\n+      // construct result, remove top 10 and add upsert data.\n+      List<GenericRecord> expectData = insertData.subList(11, 96);\n+      expectData.addAll(upsertData);\n+\n+      // read latest data\n+      SQLContext sqlContext = new SQLContext(jsc);\n+      Dataset<Row> ds = HoodieClientTestUtils.read(jsc, basePath + \"/testTarget\", sqlContext, dfs, basePath + \"/testTarget/*/*/*/*\");\n+\n+      List<Row> readData = ds.select(\"timestamp\", \"_row_key\", \"rider\", \"driver\", \"begin_lat\", \"begin_lon\", \"end_lat\", \"end_lon\").collectAsList();\n+      List<HoodieModel> result = readData.stream().map(row ->\n+          new HoodieModel(row.getDouble(0), row.getString(1), row.getString(2), row.getString(3), row.getDouble(4),\n+              row.getDouble(5), row.getDouble(6), row.getDouble(7)))\n+          .collect(Collectors.toList());\n+\n+      // get expected result\n+      List<HoodieModel> expected = expectData.stream().map(g ->\n+          new HoodieModel(Double.valueOf(g.get(\"timestamp\").toString()), g.get(\"_row_key\").toString(), g.get(\"rider\").toString(), g.get(\"driver\").toString(),\n+              Double.valueOf(g.get(\"begin_lat\").toString()), Double.valueOf(g.get(\"begin_lon\").toString()), Double.valueOf(g.get(\"end_lat\").toString()),\n+              Double.valueOf(g.get(\"end_lon\").toString())))\n+          .collect(Collectors.toList());\n+\n+      assertTrue(result.containsAll(expected) && expected.containsAll(result) && result.size() == expected.size());\n+    } finally {\n+      if (jsc != null) {\n+        jsc.stop();\n+      }\n+    }\n+  }\n+\n+  private List<GenericRecord> createRecords(Path srcFolder) throws ParseException, IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTM4OTQyNQ=="}, "originalCommit": {"oid": "e0c6fc56a5717b1eb6b5e12cce3b7c4853e45c35"}, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA1NTM0OA==", "bodyText": "This method throws IOException, so the .close method should be wrapped into a finally or try-with-resource block. The same issue exists in createRecords method.", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411055348", "createdAt": "2020-04-20T02:34:51Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -171,6 +277,30 @@ private void createRecords(Path srcFolder) throws ParseException, IOException {\n       writer.write(record);\n     }\n     writer.close();\n+    return records;\n+  }\n+\n+  private List<GenericRecord> createUpsertRecords(Path srcFolder) throws ParseException, IOException {\n+    Path srcFile = new Path(srcFolder.toString(), \"file1.parquet\");\n+    long startTime = HoodieActiveTimeline.COMMIT_FORMATTER.parse(\"20170203000000\").getTime() / 1000;\n+    List<GenericRecord> records = new ArrayList<GenericRecord>();\n+    // 10 for update\n+    for (long recordNum = 0; recordNum < 11; recordNum++) {\n+      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n+          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n+    }\n+    // 4 for insert\n+    for (long recordNum = 96; recordNum < 100; recordNum++) {\n+      records.add(HoodieTestDataGenerator.generateGenericRecord(Long.toString(recordNum), \"rider-upsert-\" + recordNum,\n+          \"driver-upsert\" + recordNum, startTime + TimeUnit.HOURS.toSeconds(recordNum)));\n+    }\n+    ParquetWriter<GenericRecord> writer = AvroParquetWriter.<GenericRecord>builder(srcFile)\n+        .withSchema(HoodieTestDataGenerator.AVRO_SCHEMA).withConf(HoodieTestUtils.getDefaultHadoopConf()).build();\n+    for (GenericRecord record : records) {\n+      writer.write(record);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed"}, "originalPosition": 223}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA2NjExMg==", "bodyText": "rename to testImportWithInsert?", "url": "https://github.com/apache/hudi/pull/1511#discussion_r411066112", "createdAt": "2020-04-20T03:17:00Z", "author": {"login": "yanghua"}, "path": "hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHDFSParquetImporter.java", "diffHunk": "@@ -150,14 +166,104 @@ protected int dataImport(JavaSparkContext jsc) throws IOException {\n       for (Entry<String, Long> e : recordCounts.entrySet()) {\n         assertEquals(\"missing records\", 24, e.getValue().longValue());\n       }\n-    } finally {\n-      if (jsc != null) {\n-        jsc.stop();\n-      }\n     }\n   }\n \n-  private void createRecords(Path srcFolder) throws ParseException, IOException {\n+  private void insert(JavaSparkContext jsc) throws IOException {\n+    // Create schema file.\n+    String schemaFile = new Path(basePath, \"file.schema\").toString();\n+    createSchemaFile(schemaFile);\n+\n+    HDFSParquetImporter.Config cfg = getHDFSParquetImporterConfig(srcFolder.toString(), hoodieFolder.toString(),\n+        \"testTable\", \"COPY_ON_WRITE\", \"_row_key\", \"timestamp\", 1, schemaFile);\n+    HDFSParquetImporter dataImporter = new HDFSParquetImporter(cfg);\n+\n+    dataImporter.dataImport(jsc, 0);\n+  }\n+\n+  /**\n+   * Test successful insert and verify data consistency.\n+   */\n+  @Test\n+  public void testImportInsert() throws IOException, ParseException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56bc39dfda9158e1f060dffa1c18a423ebc0ffed"}, "originalPosition": 117}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f7a8a063fd6eebe8eac0c47a602edb9168fda439", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/f7a8a063fd6eebe8eac0c47a602edb9168fda439", "committedDate": "2020-04-20T10:14:42Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9381b9b92458c986b60058be2f0cc0c0ef9e5ce7", "author": {"user": {"login": "hddong", "name": "hongdd"}}, "url": "https://github.com/apache/hudi/commit/9381b9b92458c986b60058be2f0cc0c0ef9e5ce7", "committedDate": "2020-04-21T01:21:55Z", "message": "reset"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk3MDE3OTE0", "url": "https://github.com/apache/hudi/pull/1511#pullrequestreview-397017914", "createdAt": "2020-04-21T06:21:12Z", "commit": {"oid": "9381b9b92458c986b60058be2f0cc0c0ef9e5ce7"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3375, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}