{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY1NTg4MTE5", "number": 1267, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQxNTo1NzozMVrODZ4kpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQxNjowMTozOFrODZ4qzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NDY3ODc4OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_6_deployment.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQxNTo1NzozMVrOFghcLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QyMjo0OToxNFrOFhPMqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTY0NjYzOQ==", "bodyText": "can you add a sentence above, first introducing the two aspects : sync vs async compaction, continuous-vs-non continuous writing, so it flows well ?", "url": "https://github.com/apache/hudi/pull/1267#discussion_r369646639", "createdAt": "2020-01-22T15:57:31Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_6_deployment.md", "diffHunk": "@@ -23,15 +23,25 @@ All in all, Hudi deploys with no long running servers or additional infrastructu\n using existing infrastructure and its heartening to see other systems adopting similar approaches as well. Hudi writing is done via Spark jobs (DeltaStreamer or custom Spark datasource jobs), deployed per standard Apache Spark [recommendations](https://spark.apache.org/docs/latest/cluster-overview.html).\n Querying Hudi tables happens via libraries installed into Apache Hive, Apache Spark or Presto and hence no additional infrastructure is necessary. \n \n+### DeltaStreamer", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM5NjMzMQ==", "bodyText": "Done.", "url": "https://github.com/apache/hudi/pull/1267#discussion_r370396331", "createdAt": "2020-01-23T22:49:14Z", "author": {"login": "bvaradar"}, "path": "docs/_docs/2_6_deployment.md", "diffHunk": "@@ -23,15 +23,25 @@ All in all, Hudi deploys with no long running servers or additional infrastructu\n using existing infrastructure and its heartening to see other systems adopting similar approaches as well. Hudi writing is done via Spark jobs (DeltaStreamer or custom Spark datasource jobs), deployed per standard Apache Spark [recommendations](https://spark.apache.org/docs/latest/cluster-overview.html).\n Querying Hudi tables happens via libraries installed into Apache Hive, Apache Spark or Presto and hence no additional infrastructure is necessary. \n \n+### DeltaStreamer", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTY0NjYzOQ=="}, "originalCommit": null, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NDY3OTk2OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_6_deployment.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQxNTo1Nzo0OVrOFghc_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QyMzo0NToyM1rOFhQPgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTY0Njg0Ng==", "bodyText": "call out this is the default?", "url": "https://github.com/apache/hudi/pull/1267#discussion_r369646846", "createdAt": "2020-01-22T15:57:49Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_6_deployment.md", "diffHunk": "@@ -23,15 +23,25 @@ All in all, Hudi deploys with no long running servers or additional infrastructu\n using existing infrastructure and its heartening to see other systems adopting similar approaches as well. Hudi writing is done via Spark jobs (DeltaStreamer or custom Spark datasource jobs), deployed per standard Apache Spark [recommendations](https://spark.apache.org/docs/latest/cluster-overview.html).\n Querying Hudi tables happens via libraries installed into Apache Hive, Apache Spark or Presto and hence no additional infrastructure is necessary. \n \n+### DeltaStreamer\n \n+[DeltaStreamer](/docs/writing_data.html#deltastreamer) is the standalone utility to incrementally pull upstream changes from varied sources such as DFS, Kafka and DB Changelogs and ingest them to hudi tables. It runs as a spark application in 2 modes.\n+\n+ - **Run Once Mode** : In this mode, Deltastreamer performs one ingestion round which includes incrementally pulling events from upstream sources and ingesting them to hudi table. Background operations like cleaning old file versions and archiving hoodie timeline are automatically executed as part of the run. For Merge-On-Read tables, Compaction is also run inline as part of ingestion unless disabled by passing the flag \"--disable-compaction\". By default, Compaction is run inline for every ingestion run and this can be changed by setting the property \"hoodie.compact.inline.max.delta.commits\". You can either manually run this spark application or use any cron trigger or workflow orchestrator such as Apache Airflow to spawn this application.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTY0NzQwNA==", "bodyText": "You can either manually run this spark application or use any cron trigger or workflow orchestrator such as Apache Airflow to spawn this application.\n\nlink to running this spark application and some commands to do so?", "url": "https://github.com/apache/hudi/pull/1267#discussion_r369647404", "createdAt": "2020-01-22T15:58:42Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_6_deployment.md", "diffHunk": "@@ -23,15 +23,25 @@ All in all, Hudi deploys with no long running servers or additional infrastructu\n using existing infrastructure and its heartening to see other systems adopting similar approaches as well. Hudi writing is done via Spark jobs (DeltaStreamer or custom Spark datasource jobs), deployed per standard Apache Spark [recommendations](https://spark.apache.org/docs/latest/cluster-overview.html).\n Querying Hudi tables happens via libraries installed into Apache Hive, Apache Spark or Presto and hence no additional infrastructure is necessary. \n \n+### DeltaStreamer\n \n+[DeltaStreamer](/docs/writing_data.html#deltastreamer) is the standalone utility to incrementally pull upstream changes from varied sources such as DFS, Kafka and DB Changelogs and ingest them to hudi tables. It runs as a spark application in 2 modes.\n+\n+ - **Run Once Mode** : In this mode, Deltastreamer performs one ingestion round which includes incrementally pulling events from upstream sources and ingesting them to hudi table. Background operations like cleaning old file versions and archiving hoodie timeline are automatically executed as part of the run. For Merge-On-Read tables, Compaction is also run inline as part of ingestion unless disabled by passing the flag \"--disable-compaction\". By default, Compaction is run inline for every ingestion run and this can be changed by setting the property \"hoodie.compact.inline.max.delta.commits\". You can either manually run this spark application or use any cron trigger or workflow orchestrator such as Apache Airflow to spawn this application.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTY0Njg0Ng=="}, "originalCommit": null, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDQxMzQ0Mw==", "bodyText": "Done.", "url": "https://github.com/apache/hudi/pull/1267#discussion_r370413443", "createdAt": "2020-01-23T23:45:23Z", "author": {"login": "bvaradar"}, "path": "docs/_docs/2_6_deployment.md", "diffHunk": "@@ -23,15 +23,25 @@ All in all, Hudi deploys with no long running servers or additional infrastructu\n using existing infrastructure and its heartening to see other systems adopting similar approaches as well. Hudi writing is done via Spark jobs (DeltaStreamer or custom Spark datasource jobs), deployed per standard Apache Spark [recommendations](https://spark.apache.org/docs/latest/cluster-overview.html).\n Querying Hudi tables happens via libraries installed into Apache Hive, Apache Spark or Presto and hence no additional infrastructure is necessary. \n \n+### DeltaStreamer\n \n+[DeltaStreamer](/docs/writing_data.html#deltastreamer) is the standalone utility to incrementally pull upstream changes from varied sources such as DFS, Kafka and DB Changelogs and ingest them to hudi tables. It runs as a spark application in 2 modes.\n+\n+ - **Run Once Mode** : In this mode, Deltastreamer performs one ingestion round which includes incrementally pulling events from upstream sources and ingesting them to hudi table. Background operations like cleaning old file versions and archiving hoodie timeline are automatically executed as part of the run. For Merge-On-Read tables, Compaction is also run inline as part of ingestion unless disabled by passing the flag \"--disable-compaction\". By default, Compaction is run inline for every ingestion run and this can be changed by setting the property \"hoodie.compact.inline.max.delta.commits\". You can either manually run this spark application or use any cron trigger or workflow orchestrator such as Apache Airflow to spawn this application.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTY0Njg0Ng=="}, "originalCommit": null, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NDY4ODI0OnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_6_deployment.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQxNTo1OTo1N1rOFghiRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QyMjo0ODozOVrOFhPL4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTY0ODE5Ng==", "bodyText": "reminds me, that we should have async compaction working for spark streaming as well? may be file a JIRA if you agree?", "url": "https://github.com/apache/hudi/pull/1267#discussion_r369648196", "createdAt": "2020-01-22T15:59:57Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_6_deployment.md", "diffHunk": "@@ -23,15 +23,25 @@ All in all, Hudi deploys with no long running servers or additional infrastructu\n using existing infrastructure and its heartening to see other systems adopting similar approaches as well. Hudi writing is done via Spark jobs (DeltaStreamer or custom Spark datasource jobs), deployed per standard Apache Spark [recommendations](https://spark.apache.org/docs/latest/cluster-overview.html).\n Querying Hudi tables happens via libraries installed into Apache Hive, Apache Spark or Presto and hence no additional infrastructure is necessary. \n \n+### DeltaStreamer\n \n+[DeltaStreamer](/docs/writing_data.html#deltastreamer) is the standalone utility to incrementally pull upstream changes from varied sources such as DFS, Kafka and DB Changelogs and ingest them to hudi tables. It runs as a spark application in 2 modes.\n+\n+ - **Run Once Mode** : In this mode, Deltastreamer performs one ingestion round which includes incrementally pulling events from upstream sources and ingesting them to hudi table. Background operations like cleaning old file versions and archiving hoodie timeline are automatically executed as part of the run. For Merge-On-Read tables, Compaction is also run inline as part of ingestion unless disabled by passing the flag \"--disable-compaction\". By default, Compaction is run inline for every ingestion run and this can be changed by setting the property \"hoodie.compact.inline.max.delta.commits\". You can either manually run this spark application or use any cron trigger or workflow orchestrator such as Apache Airflow to spawn this application.\n+ - **Continuous Mode** :  Here, deltastreamer runs an infinite loop with each round performing one ingestion round as described in **Run Once Mode**. For Merge-On-Read tables, Compaction is run in asynchronous fashion concurrently with ingestion unless disabled by passing the flag \"--disable-compaction\". Every ingestion run triggers a compaction request asynchronously and this frequency can be changed by setting the property \"hoodie.compact.inline.max.delta.commits\". As both ingestion and compaction is running in the same spark context, you can use resource allocation configuration in DeltaStreamer CLI such as (\"--delta-sync-scheduling-weight\", \"--compact-scheduling-weight\", \"\"--delta-sync-scheduling-minshare\", and \"--compact-scheduling-minshare\") to control executor allocation between ingestion and compaction.\n+\n+### Spark Datasource Writer Jobs\n+\n+As described in [Writing Data](/docs/writing_data.html#datasource-writer), you can use spark datasource to ingest to hudi table. This mechanism allows you to ingest any spark dataframe in Hudi format. Hudi Spark DataSource also supports spark streaming to ingest a streaming source to Hudi table. For Merge On Read table types, inline compaction is turned on by default which runs after every ingestion run. The compaction frequency can be changed by setting the property \"hoodie.compact.inline.max.delta.commits\". ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM5NjEyOQ==", "bodyText": "Agree. https://jira.apache.org/jira/browse/HUDI-575", "url": "https://github.com/apache/hudi/pull/1267#discussion_r370396129", "createdAt": "2020-01-23T22:48:39Z", "author": {"login": "bvaradar"}, "path": "docs/_docs/2_6_deployment.md", "diffHunk": "@@ -23,15 +23,25 @@ All in all, Hudi deploys with no long running servers or additional infrastructu\n using existing infrastructure and its heartening to see other systems adopting similar approaches as well. Hudi writing is done via Spark jobs (DeltaStreamer or custom Spark datasource jobs), deployed per standard Apache Spark [recommendations](https://spark.apache.org/docs/latest/cluster-overview.html).\n Querying Hudi tables happens via libraries installed into Apache Hive, Apache Spark or Presto and hence no additional infrastructure is necessary. \n \n+### DeltaStreamer\n \n+[DeltaStreamer](/docs/writing_data.html#deltastreamer) is the standalone utility to incrementally pull upstream changes from varied sources such as DFS, Kafka and DB Changelogs and ingest them to hudi tables. It runs as a spark application in 2 modes.\n+\n+ - **Run Once Mode** : In this mode, Deltastreamer performs one ingestion round which includes incrementally pulling events from upstream sources and ingesting them to hudi table. Background operations like cleaning old file versions and archiving hoodie timeline are automatically executed as part of the run. For Merge-On-Read tables, Compaction is also run inline as part of ingestion unless disabled by passing the flag \"--disable-compaction\". By default, Compaction is run inline for every ingestion run and this can be changed by setting the property \"hoodie.compact.inline.max.delta.commits\". You can either manually run this spark application or use any cron trigger or workflow orchestrator such as Apache Airflow to spawn this application.\n+ - **Continuous Mode** :  Here, deltastreamer runs an infinite loop with each round performing one ingestion round as described in **Run Once Mode**. For Merge-On-Read tables, Compaction is run in asynchronous fashion concurrently with ingestion unless disabled by passing the flag \"--disable-compaction\". Every ingestion run triggers a compaction request asynchronously and this frequency can be changed by setting the property \"hoodie.compact.inline.max.delta.commits\". As both ingestion and compaction is running in the same spark context, you can use resource allocation configuration in DeltaStreamer CLI such as (\"--delta-sync-scheduling-weight\", \"--compact-scheduling-weight\", \"\"--delta-sync-scheduling-minshare\", and \"--compact-scheduling-minshare\") to control executor allocation between ingestion and compaction.\n+\n+### Spark Datasource Writer Jobs\n+\n+As described in [Writing Data](/docs/writing_data.html#datasource-writer), you can use spark datasource to ingest to hudi table. This mechanism allows you to ingest any spark dataframe in Hudi format. Hudi Spark DataSource also supports spark streaming to ingest a streaming source to Hudi table. For Merge On Read table types, inline compaction is turned on by default which runs after every ingestion run. The compaction frequency can be changed by setting the property \"hoodie.compact.inline.max.delta.commits\". ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTY0ODE5Ng=="}, "originalCommit": null, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI4NDY5NDUyOnYy", "diffSide": "RIGHT", "path": "docs/_docs/2_6_deployment.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMlQxNjowMTozOFrOFghmOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yM1QyMjo1MDo1OFrOFhPPVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTY0OTIxMA==", "bodyText": "also worth noting here is the config for controlling the sync frequency.. --min-sync-interval-seconds", "url": "https://github.com/apache/hudi/pull/1267#discussion_r369649210", "createdAt": "2020-01-22T16:01:38Z", "author": {"login": "vinothchandar"}, "path": "docs/_docs/2_6_deployment.md", "diffHunk": "@@ -23,15 +23,25 @@ All in all, Hudi deploys with no long running servers or additional infrastructu\n using existing infrastructure and its heartening to see other systems adopting similar approaches as well. Hudi writing is done via Spark jobs (DeltaStreamer or custom Spark datasource jobs), deployed per standard Apache Spark [recommendations](https://spark.apache.org/docs/latest/cluster-overview.html).\n Querying Hudi tables happens via libraries installed into Apache Hive, Apache Spark or Presto and hence no additional infrastructure is necessary. \n \n+### DeltaStreamer\n \n+[DeltaStreamer](/docs/writing_data.html#deltastreamer) is the standalone utility to incrementally pull upstream changes from varied sources such as DFS, Kafka and DB Changelogs and ingest them to hudi tables. It runs as a spark application in 2 modes.\n+\n+ - **Run Once Mode** : In this mode, Deltastreamer performs one ingestion round which includes incrementally pulling events from upstream sources and ingesting them to hudi table. Background operations like cleaning old file versions and archiving hoodie timeline are automatically executed as part of the run. For Merge-On-Read tables, Compaction is also run inline as part of ingestion unless disabled by passing the flag \"--disable-compaction\". By default, Compaction is run inline for every ingestion run and this can be changed by setting the property \"hoodie.compact.inline.max.delta.commits\". You can either manually run this spark application or use any cron trigger or workflow orchestrator such as Apache Airflow to spawn this application.\n+ - **Continuous Mode** :  Here, deltastreamer runs an infinite loop with each round performing one ingestion round as described in **Run Once Mode**. For Merge-On-Read tables, Compaction is run in asynchronous fashion concurrently with ingestion unless disabled by passing the flag \"--disable-compaction\". Every ingestion run triggers a compaction request asynchronously and this frequency can be changed by setting the property \"hoodie.compact.inline.max.delta.commits\". As both ingestion and compaction is running in the same spark context, you can use resource allocation configuration in DeltaStreamer CLI such as (\"--delta-sync-scheduling-weight\", \"--compact-scheduling-weight\", \"\"--delta-sync-scheduling-minshare\", and \"--compact-scheduling-minshare\") to control executor allocation between ingestion and compaction.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MDM5NzAxNQ==", "bodyText": "Done", "url": "https://github.com/apache/hudi/pull/1267#discussion_r370397015", "createdAt": "2020-01-23T22:50:58Z", "author": {"login": "bvaradar"}, "path": "docs/_docs/2_6_deployment.md", "diffHunk": "@@ -23,15 +23,25 @@ All in all, Hudi deploys with no long running servers or additional infrastructu\n using existing infrastructure and its heartening to see other systems adopting similar approaches as well. Hudi writing is done via Spark jobs (DeltaStreamer or custom Spark datasource jobs), deployed per standard Apache Spark [recommendations](https://spark.apache.org/docs/latest/cluster-overview.html).\n Querying Hudi tables happens via libraries installed into Apache Hive, Apache Spark or Presto and hence no additional infrastructure is necessary. \n \n+### DeltaStreamer\n \n+[DeltaStreamer](/docs/writing_data.html#deltastreamer) is the standalone utility to incrementally pull upstream changes from varied sources such as DFS, Kafka and DB Changelogs and ingest them to hudi tables. It runs as a spark application in 2 modes.\n+\n+ - **Run Once Mode** : In this mode, Deltastreamer performs one ingestion round which includes incrementally pulling events from upstream sources and ingesting them to hudi table. Background operations like cleaning old file versions and archiving hoodie timeline are automatically executed as part of the run. For Merge-On-Read tables, Compaction is also run inline as part of ingestion unless disabled by passing the flag \"--disable-compaction\". By default, Compaction is run inline for every ingestion run and this can be changed by setting the property \"hoodie.compact.inline.max.delta.commits\". You can either manually run this spark application or use any cron trigger or workflow orchestrator such as Apache Airflow to spawn this application.\n+ - **Continuous Mode** :  Here, deltastreamer runs an infinite loop with each round performing one ingestion round as described in **Run Once Mode**. For Merge-On-Read tables, Compaction is run in asynchronous fashion concurrently with ingestion unless disabled by passing the flag \"--disable-compaction\". Every ingestion run triggers a compaction request asynchronously and this frequency can be changed by setting the property \"hoodie.compact.inline.max.delta.commits\". As both ingestion and compaction is running in the same spark context, you can use resource allocation configuration in DeltaStreamer CLI such as (\"--delta-sync-scheduling-weight\", \"--compact-scheduling-weight\", \"\"--delta-sync-scheduling-minshare\", and \"--compact-scheduling-minshare\") to control executor allocation between ingestion and compaction.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2OTY0OTIxMA=="}, "originalCommit": null, "originalPosition": 21}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 0, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}