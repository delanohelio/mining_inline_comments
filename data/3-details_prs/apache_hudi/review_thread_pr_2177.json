{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAzNTI0NDcx", "number": 2177, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQxMDozNjo0M1rOEubYZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQxMDozNjo0M1rOEubYZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE3MTE4NTY3OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQxMDozNjo0M1rOHi0pMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xN1QxNTozNjoyM1rOHjeD4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjI3NjE0Nw==", "bodyText": "here (38, 18) is decided by spark?", "url": "https://github.com/apache/hudi/pull/2177#discussion_r506276147", "createdAt": "2020-10-16T10:36:43Z", "author": {"login": "leesf"}, "path": "hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -194,4 +199,31 @@ class TestCOWDataSource extends HoodieClientTestBase {\n       .load(basePath)\n     assertEquals(hoodieIncViewDF2.count(), insert2NewKeyCnt)\n   }\n+\n+  @Test def testComplexDataTypeWriteAndReadConsistency(): Unit = {\n+    val records = Seq(ComplexDataTypeRecord(\"11\", \"Andy\", Timestamp.valueOf(\"1970-01-01 13:31:24\"), Date.valueOf(\"1991-11-07\"), BigDecimal.valueOf(1.0), 11, 1),\n+      ComplexDataTypeRecord(\"22\", \"lisi\", Timestamp.valueOf(\"1970-01-02 13:31:24\"), Date.valueOf(\"1991-11-08\"), BigDecimal.valueOf(2.0), 11, 1),\n+      ComplexDataTypeRecord(\"33\", \"zhangsan\", Timestamp.valueOf(\"1970-01-03 13:31:24\"), Date.valueOf(\"1991-11-09\"), BigDecimal.valueOf(3.0), 11, 1))\n+\n+    val rdd = jsc.parallelize(records)\n+    val  recordsDF = spark.createDataFrame(rdd)\n+    recordsDF.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .mode(SaveMode.Append)\n+      .save(basePath)\n+\n+    val recordsReadDF = spark.read.format(\"org.apache.hudi\")\n+      .load(basePath + \"/*/*\")\n+    recordsReadDF.schema.foreach(f => {\n+      f.name match {\n+        case \"timeStampValue\" =>\n+          assertEquals(f.dataType, org.apache.spark.sql.types.TimestampType)\n+        case \"dateValue\" =>\n+          assertEquals(f.dataType, org.apache.spark.sql.types.DateType)\n+        case \"bigDecimalValue\" =>\n+          assertEquals(f.dataType, org.apache.spark.sql.types.DecimalType(38, 18))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "154554865a280bab1b769159df629405e64e9b0b"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjI4NzU4NQ==", "bodyText": "yes,  spark will transform java  bigdecimal to SYSTEM_DEFAULT  decimal(38,18)", "url": "https://github.com/apache/hudi/pull/2177#discussion_r506287585", "createdAt": "2020-10-16T10:51:10Z", "author": {"login": "lw309637554"}, "path": "hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -194,4 +199,31 @@ class TestCOWDataSource extends HoodieClientTestBase {\n       .load(basePath)\n     assertEquals(hoodieIncViewDF2.count(), insert2NewKeyCnt)\n   }\n+\n+  @Test def testComplexDataTypeWriteAndReadConsistency(): Unit = {\n+    val records = Seq(ComplexDataTypeRecord(\"11\", \"Andy\", Timestamp.valueOf(\"1970-01-01 13:31:24\"), Date.valueOf(\"1991-11-07\"), BigDecimal.valueOf(1.0), 11, 1),\n+      ComplexDataTypeRecord(\"22\", \"lisi\", Timestamp.valueOf(\"1970-01-02 13:31:24\"), Date.valueOf(\"1991-11-08\"), BigDecimal.valueOf(2.0), 11, 1),\n+      ComplexDataTypeRecord(\"33\", \"zhangsan\", Timestamp.valueOf(\"1970-01-03 13:31:24\"), Date.valueOf(\"1991-11-09\"), BigDecimal.valueOf(3.0), 11, 1))\n+\n+    val rdd = jsc.parallelize(records)\n+    val  recordsDF = spark.createDataFrame(rdd)\n+    recordsDF.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .mode(SaveMode.Append)\n+      .save(basePath)\n+\n+    val recordsReadDF = spark.read.format(\"org.apache.hudi\")\n+      .load(basePath + \"/*/*\")\n+    recordsReadDF.schema.foreach(f => {\n+      f.name match {\n+        case \"timeStampValue\" =>\n+          assertEquals(f.dataType, org.apache.spark.sql.types.TimestampType)\n+        case \"dateValue\" =>\n+          assertEquals(f.dataType, org.apache.spark.sql.types.DateType)\n+        case \"bigDecimalValue\" =>\n+          assertEquals(f.dataType, org.apache.spark.sql.types.DecimalType(38, 18))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjI3NjE0Nw=="}, "originalCommit": {"oid": "154554865a280bab1b769159df629405e64e9b0b"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjk1NDcyMQ==", "bodyText": "@leesf  has set the schema use DataTypes.createDecimalType(15, 10) clearly.  This will be more clear", "url": "https://github.com/apache/hudi/pull/2177#discussion_r506954721", "createdAt": "2020-10-17T15:36:23Z", "author": {"login": "lw309637554"}, "path": "hudi-spark/src/test/scala/org/apache/hudi/functional/TestCOWDataSource.scala", "diffHunk": "@@ -194,4 +199,31 @@ class TestCOWDataSource extends HoodieClientTestBase {\n       .load(basePath)\n     assertEquals(hoodieIncViewDF2.count(), insert2NewKeyCnt)\n   }\n+\n+  @Test def testComplexDataTypeWriteAndReadConsistency(): Unit = {\n+    val records = Seq(ComplexDataTypeRecord(\"11\", \"Andy\", Timestamp.valueOf(\"1970-01-01 13:31:24\"), Date.valueOf(\"1991-11-07\"), BigDecimal.valueOf(1.0), 11, 1),\n+      ComplexDataTypeRecord(\"22\", \"lisi\", Timestamp.valueOf(\"1970-01-02 13:31:24\"), Date.valueOf(\"1991-11-08\"), BigDecimal.valueOf(2.0), 11, 1),\n+      ComplexDataTypeRecord(\"33\", \"zhangsan\", Timestamp.valueOf(\"1970-01-03 13:31:24\"), Date.valueOf(\"1991-11-09\"), BigDecimal.valueOf(3.0), 11, 1))\n+\n+    val rdd = jsc.parallelize(records)\n+    val  recordsDF = spark.createDataFrame(rdd)\n+    recordsDF.write.format(\"org.apache.hudi\")\n+      .options(commonOpts)\n+      .mode(SaveMode.Append)\n+      .save(basePath)\n+\n+    val recordsReadDF = spark.read.format(\"org.apache.hudi\")\n+      .load(basePath + \"/*/*\")\n+    recordsReadDF.schema.foreach(f => {\n+      f.name match {\n+        case \"timeStampValue\" =>\n+          assertEquals(f.dataType, org.apache.spark.sql.types.TimestampType)\n+        case \"dateValue\" =>\n+          assertEquals(f.dataType, org.apache.spark.sql.types.DateType)\n+        case \"bigDecimalValue\" =>\n+          assertEquals(f.dataType, org.apache.spark.sql.types.DecimalType(38, 18))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjI3NjE0Nw=="}, "originalCommit": {"oid": "154554865a280bab1b769159df629405e64e9b0b"}, "originalPosition": 45}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4096, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}