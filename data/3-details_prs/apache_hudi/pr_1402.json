{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg3NTMzOTAz", "number": 1402, "title": "[HUDI-407] Adding Simple Index", "bodyText": "What is the purpose of the pull request\nAdding a simple index which fetches rows from Parquet for interested fields(record keys and partition path) and joins with incoming records to find the location.\nBrief change log\n\nIntroduced HoodieSimpleIndex that can tag incoming records with right HoodieRecordLocations\n\nVerify this pull request\nThis change added tests and can be verified as follows:\n\n\nAdded few more tests for all indexes in general as part of this patch.\n\n\nParametrized TestHoodieClientOnCopyOnWriteStorage to run for every index type including simple.\n\nAdded integration tests for end-to-end.\nAdded HoodieClientWriteTest to verify the change.\nManually verified the change by running a job locally.\n\n\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-03-13T00:29:54Z", "url": "https://github.com/apache/hudi/pull/1402", "merged": true, "mergeCommit": {"oid": "29edf4b3b8ade64ec7822d6b7b2a125d5ca781c4"}, "closed": true, "closedAt": "2020-05-18T01:32:25Z", "author": {"login": "nsivabalan"}, "timelineItems": {"totalCount": 44, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcNFM32gFqTM3Mzk4MTA4NQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABciVn72AFqTQxMzIyNTgyOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczOTgxMDg1", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-373981085", "createdAt": "2020-03-13T00:30:41Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QwMDozMDo0MVrOF10HAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QwMDozMDo0MVrOF10HAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk3MjYwOA==", "bodyText": "crux of the change is in this method.", "url": "https://github.com/apache/hudi/pull/1402#discussion_r391972608", "createdAt": "2020-03-13T00:30:41Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    // Cache the result, for subsequent stages.\n+    if (config.getBloomIndexUseCaching()) {\n+      keyFilenamePairRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairRDD.count();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+      keyFilenamePairRDD.unpersist();\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = partitionRecordKeyPairRDD.countByKey();\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<Tuple2<String, String>> fileInfoList =\n+        loadInvolvedFileIds(affectedPartitionPathList, jsc, hoodieTable);\n+\n+    return findMatchingFilesForRecordKeys(jsc, fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  @VisibleForTesting\n+  List<Tuple2<String, String>> loadInvolvedFileIds(List<String> partitions, final JavaSparkContext jsc,\n+                                                   final HoodieTable hoodieTable) {\n+\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList =\n+        jsc.parallelize(partitions, Math.max(partitions.size(), 1)).flatMap(partitionPath -> {\n+          Option<HoodieInstant> latestCommitTime =\n+              hoodieTable.getMetaClient().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+          List<Pair<String, String>> filteredFiles = new ArrayList<>();\n+          if (latestCommitTime.isPresent()) {\n+            filteredFiles = hoodieTable.getROFileSystemView()\n+                .getLatestDataFilesBeforeOrOn(partitionPath, latestCommitTime.get().getTimestamp())\n+                .map(f -> Pair.of(partitionPath, f.getFileId())).collect(toList());\n+          }\n+          return filteredFiles.iterator();\n+        }).collect();\n+    return partitionPathFileIDList.stream()\n+        .map(pf -> new Tuple2<>(pf.getKey(), pf.getValue())).collect(toList());\n+  }\n+\n+  /**\n+   * Find <HoodieKey, HoodieRecordLocation> for all incoming HoodieKeys\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(JavaSparkContext jsc,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 214}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczOTgxNDYy", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-373981462", "createdAt": "2020-03-13T00:32:07Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QwMDozMjowN1rOF10ITA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QwMDozMjowN1rOF10ITA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk3Mjk0MA==", "bodyText": "@vinothchandar @bvaradar : As part of testing the new simple index, I have parametrized this test (TestHoodieClientOnCopyOnWriteStorage) and have added other index types as well. is that fine or do you think the increase in build/test time is not worth it.", "url": "https://github.com/apache/hudi/pull/1402#discussion_r391972940", "createdAt": "2020-03-13T00:32:07Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -79,9 +82,22 @@\n import static org.mockito.Mockito.when;\n \n @SuppressWarnings(\"unchecked\")\n+@RunWith(Parameterized.class)\n public class TestHoodieClientOnCopyOnWriteStorage extends TestHoodieClientBase {\n \n   private static final Logger LOG = LogManager.getLogger(TestHoodieClientOnCopyOnWriteStorage.class);\n+  private final IndexType indexType;\n+\n+  @Parameterized.Parameters(name = \"{index}: Test with IndexType={0}\")\n+  public static Collection<Object[]> data() {\n+    Object[][] data =\n+        new Object[][] {{IndexType.BLOOM},{IndexType.GLOBAL_BLOOM},{IndexType.SIMPLE}};", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzczOTgxODI5", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-373981829", "createdAt": "2020-03-13T00:33:26Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QwMDozMzoyNlrOF10JaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QwMDozMzoyNlrOF10JaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk3MzIyNQ==", "bodyText": "As of now, I don't do the lazy record iterator. Thought will make that in a diff patch as I want to get this out sooner.", "url": "https://github.com/apache/hudi/pull/1402#discussion_r391973225", "createdAt": "2020-03-13T00:33:26Z", "author": {"login": "nsivabalan"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java", "diffHunk": "@@ -103,6 +120,42 @@\n     return rowKeys;\n   }\n \n+  /**\n+   * Read the rows with record key and partition path from the given parquet file\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return Set Set of row keys matching candidateRecordKeys\n+   */\n+  public static List<Pair<Pair<String, String>, Option<HoodieRecordLocation>>> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                                                                      String baseInstantTime,\n+                                                                                                                      String fileId) {\n+    List<Pair<Pair<String, String>, Option<HoodieRecordLocation>>> rows = new ArrayList<>();\n+    try {\n+      if (!filePath.getFileSystem(configuration).exists(filePath)) {\n+        return new ArrayList<>();\n+      }\n+      Configuration conf = new Configuration(configuration);\n+      conf.addResource(FSUtils.getFs(filePath.toString(), conf).getConf());\n+      Schema readSchema = HoodieAvroUtils.getRecordKeyPartitionPathSchema();\n+      AvroReadSupport.setAvroReadSchema(conf, readSchema);\n+      AvroReadSupport.setRequestedProjection(conf, readSchema);\n+      ParquetReader reader = AvroParquetReader.builder(filePath).withConf(conf).build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 70}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0MDcyNjYy", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-374072662", "createdAt": "2020-03-13T06:26:45Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QwNjoyNjo0NVrOF141Zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QwNjo1NzozN1rOF15TsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1MDAyMg==", "bodyText": "why these changes?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392050022", "createdAt": "2020-03-13T06:26:45Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieIndex.java", "diffHunk": "@@ -77,15 +80,15 @@ protected HoodieIndex(HoodieWriteConfig config) {\n    * present).\n    */\n   public abstract JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n-      HoodieTable<T> hoodieTable) throws HoodieIndexException;\n+                                                       HoodieTable<T> hoodieTable) throws HoodieIndexException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NTA4OA==", "bodyText": "new configs for this class?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392055088", "createdAt": "2020-03-13T06:47:27Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NTE0Mw==", "bodyText": "same here", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392055143", "createdAt": "2020-03-13T06:47:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NjI5MA==", "bodyText": "We orginally did this to avoid shuffling the input data to do the join..against the bloom filter/range information.. I think in this scenario, all we are doing is a single join against all of records in the DFS partitions.. So I suggest, we make it much simpler and lower overhead, by  the following approach\n\ncache the recordRDD  alone\nFrom that extract affectedPartitions\nPrep the RDD of records in these partitions as partitionFileIdRecLocationEntryPairRDD\nJust join recordRDD with that..\n\nit may simpify a few things..", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392056290", "createdAt": "2020-03-13T06:52:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NjQ4Mw==", "bodyText": "overall you are following the same code structure as BloomIndex.. any way to share more code?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392056483", "createdAt": "2020-03-13T06:52:48Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    // Cache the result, for subsequent stages.\n+    if (config.getBloomIndexUseCaching()) {\n+      keyFilenamePairRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairRDD.count();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+      keyFilenamePairRDD.unpersist();\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1Njk5Ng==", "bodyText": "We need to create some helper classes to make this code + BloomIndex more readable ... for e.g\nClass PartitionRecordKey {\n    String ..;\n    String ..;\n}\n\nand use JavaRDD?  or is n't this really the HoodieKey ?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392056996", "createdAt": "2020-03-13T06:54:55Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    // Cache the result, for subsequent stages.\n+    if (config.getBloomIndexUseCaching()) {\n+      keyFilenamePairRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairRDD.count();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+      keyFilenamePairRDD.unpersist();\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NzA2NA==", "bodyText": "code reuse please", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392057064", "createdAt": "2020-03-13T06:55:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    // Cache the result, for subsequent stages.\n+    if (config.getBloomIndexUseCaching()) {\n+      keyFilenamePairRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairRDD.count();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+      keyFilenamePairRDD.unpersist();\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = partitionRecordKeyPairRDD.countByKey();\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<Tuple2<String, String>> fileInfoList =\n+        loadInvolvedFileIds(affectedPartitionPathList, jsc, hoodieTable);\n+\n+    return findMatchingFilesForRecordKeys(jsc, fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  @VisibleForTesting\n+  List<Tuple2<String, String>> loadInvolvedFileIds(List<String> partitions, final JavaSparkContext jsc,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NzQzNg==", "bodyText": "so this only works for COW ? or assumed inserts never go to logs? I would like to remove such restrictions from this index if possible..", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392057436", "createdAt": "2020-03-13T06:56:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    // Cache the result, for subsequent stages.\n+    if (config.getBloomIndexUseCaching()) {\n+      keyFilenamePairRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairRDD.count();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+      keyFilenamePairRDD.unpersist();\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = partitionRecordKeyPairRDD.countByKey();\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<Tuple2<String, String>> fileInfoList =\n+        loadInvolvedFileIds(affectedPartitionPathList, jsc, hoodieTable);\n+\n+    return findMatchingFilesForRecordKeys(jsc, fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  @VisibleForTesting\n+  List<Tuple2<String, String>> loadInvolvedFileIds(List<String> partitions, final JavaSparkContext jsc,\n+                                                   final HoodieTable hoodieTable) {\n+\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList =\n+        jsc.parallelize(partitions, Math.max(partitions.size(), 1)).flatMap(partitionPath -> {\n+          Option<HoodieInstant> latestCommitTime =\n+              hoodieTable.getMetaClient().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+          List<Pair<String, String>> filteredFiles = new ArrayList<>();\n+          if (latestCommitTime.isPresent()) {\n+            filteredFiles = hoodieTable.getROFileSystemView()\n+                .getLatestDataFilesBeforeOrOn(partitionPath, latestCommitTime.get().getTimestamp())\n+                .map(f -> Pair.of(partitionPath, f.getFileId())).collect(toList());\n+          }\n+          return filteredFiles.iterator();\n+        }).collect();\n+    return partitionPathFileIDList.stream()\n+        .map(pf -> new Tuple2<>(pf.getKey(), pf.getValue())).collect(toList());\n+  }\n+\n+  /**\n+   * Find <HoodieKey, HoodieRecordLocation> for all incoming HoodieKeys\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(JavaSparkContext jsc,\n+                                                                                      List<Tuple2<String, String>> partitionToFileIndexInfo,\n+                                                                                      JavaPairRDD<String, String> partitionRecordKeyPairRDD, HoodieTable hoodieTable) {\n+    // Step 1: Create JavaPairRDD< Tuple2<PartitionPath, RecordKey>, Optional<HoodieRecordLocation> > from input with  Optional<HoodieRecordLocation> as Empty.\n+    JavaPairRDD<Tuple2<String, String>, Optional<HoodieRecordLocation>> partitionRecordPairs =\n+        partitionRecordKeyPairRDD.mapToPair((PairFunction<Tuple2<String, String>, Tuple2<String, String>, Optional<HoodieRecordLocation>>) stringStringTuple2\n+            -> new Tuple2(stringStringTuple2, Optional.absent()));\n+\n+    // Step 2: Create JavaRDD< Tuple2 <Partition, FileId> > from partitions to be touched\n+    JavaRDD<Tuple2<String, String>> partitionFileIdTupleRDD = jsc.parallelize(partitionToFileIndexInfo);\n+\n+    // Step 3: For each partiion, fileId Tuple -> Fetch RDD of triplets ( Tuple2 <PartitionPath, recordKey>, HoodieRecordLocation )\n+    JavaRDD<Tuple2<Tuple2<String, String>, Option<HoodieRecordLocation>>> partitionFileIdLocationEntries = partitionFileIdTupleRDD.flatMap(partitionFileIdTuple -> {\n+      HoodieDataFile latestDataFile = getLatestDataFile(hoodieTable, Pair.of(partitionFileIdTuple._1, partitionFileIdTuple._2));\n+      List<Pair<Pair<String, String>, Option<HoodieRecordLocation>>> resultList = ParquetUtils.fetchRecordKeyPartitionPathFromParquet(hoodieTable.getHadoopConf(), new Path(latestDataFile.getPath()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 228}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NzYyNA==", "bodyText": "We should also implement a GLOBAL_SIMPLE?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392057624", "createdAt": "2020-03-13T06:57:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieIndex.java", "diffHunk": "@@ -118,9 +121,10 @@ protected HoodieIndex(HoodieWriteConfig config) {\n   /**\n    * Each index type should implement it's own logic to release any resources acquired during the process.\n    */\n-  public void close() {}\n+  public void close() {\n+  }\n \n   public enum IndexType {\n-    HBASE, INMEMORY, BLOOM, GLOBAL_BLOOM\n+    HBASE, INMEMORY, BLOOM, GLOBAL_BLOOM, SIMPLE", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1Nzc3Ng==", "bodyText": "fix copy-pastd comments everywhere?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r392057776", "createdAt": "2020-03-13T06:57:37Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    // Cache the result, for subsequent stages.\n+    if (config.getBloomIndexUseCaching()) {\n+      keyFilenamePairRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairRDD.count();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+      keyFilenamePairRDD.unpersist();\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 164}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgxNTk3NDgx", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-381597481", "createdAt": "2020-03-25T22:58:58Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMjo1ODo1OVrOF7xgYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQyMzowNDoxN1rOF7xn7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIyMTQwOQ==", "bodyText": "call it SimpleBloom when it does not use any bloom filters is very confusing.. Can we make it just SimpleIndex?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r398221409", "createdAt": "2020-03-25T22:58:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieIndexConfig.java", "diffHunk": "@@ -58,6 +58,11 @@\n   public static final String DEFAULT_BLOOM_INDEX_FILTER_TYPE = BloomFilterTypeCode.SIMPLE.name();\n   public static final String HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES = \"hoodie.bloom.index.filter.dynamic.max.entries\";\n   public static final String DEFAULT_HOODIE_BLOOM_INDEX_FILTER_DYNAMIC_MAX_ENTRIES = \"100000\";\n+  public static final String SIMPLE_BLOOM_INDEX_USE_CACHING_PROP = \"hoodie.simple.bloom.index.use.caching\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIyMjA1NA==", "bodyText": "let's write docs using higher level abstractions like file groups/slices, base/log file?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r398222054", "createdAt": "2020-03-25T23:00:44Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIyMjY4OA==", "bodyText": "I think this very confusing.. may be we need to push a bunch to HoodieIndex abstract class or have a new class introduced..", "url": "https://github.com/apache/hudi/pull/1402#discussion_r398222688", "createdAt": "2020-03-25T23:02:28Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIyMzM0MQ==", "bodyText": "I think we can fix it in this patch itself.. its a critical aspect", "url": "https://github.com/apache/hudi/pull/1402#discussion_r398223341", "createdAt": "2020-03-25T23:04:17Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java", "diffHunk": "@@ -103,6 +120,42 @@\n     return rowKeys;\n   }\n \n+  /**\n+   * Read the rows with record key and partition path from the given parquet file\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return Set Set of row keys matching candidateRecordKeys\n+   */\n+  public static List<Pair<Pair<String, String>, Option<HoodieRecordLocation>>> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                                                                      String baseInstantTime,\n+                                                                                                                      String fileId) {\n+    List<Pair<Pair<String, String>, Option<HoodieRecordLocation>>> rows = new ArrayList<>();\n+    try {\n+      if (!filePath.getFileSystem(configuration).exists(filePath)) {\n+        return new ArrayList<>();\n+      }\n+      Configuration conf = new Configuration(configuration);\n+      conf.addResource(FSUtils.getFs(filePath.toString(), conf).getConf());\n+      Schema readSchema = HoodieAvroUtils.getRecordKeyPartitionPathSchema();\n+      AvroReadSupport.setAvroReadSchema(conf, readSchema);\n+      AvroReadSupport.setRequestedProjection(conf, readSchema);\n+      ParquetReader reader = AvroParquetReader.builder(filePath).withConf(conf).build();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk3MzIyNQ=="}, "originalCommit": null, "originalPosition": 70}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgzMDM1NTMz", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-383035533", "createdAt": "2020-03-27T16:53:37Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QxNjo1MzozN1rOF85zOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yN1QxNjo1NzowNFrOF858Gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQwNTg4Mg==", "bodyText": "I did some refactoring in HoodieBloomIndex to reuse by SimpleIndex. You can check it out.", "url": "https://github.com/apache/hudi/pull/1402#discussion_r399405882", "createdAt": "2020-03-27T16:53:37Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIyMjY4OA=="}, "originalCommit": null, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQwNzA0Mw==", "bodyText": "yes, but at some later stage, we do combine all recordKeys per partition and hence didn't disturb the flow much. If you think, we should look to re-use HoodieKey from get go, I can give it a shot.", "url": "https://github.com/apache/hudi/pull/1402#discussion_r399407043", "createdAt": "2020-03-27T16:55:22Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    // Cache the result, for subsequent stages.\n+    if (config.getBloomIndexUseCaching()) {\n+      keyFilenamePairRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairRDD.count();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+      keyFilenamePairRDD.unpersist();\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1Njk5Ng=="}, "originalCommit": null, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTQwODE1NA==", "bodyText": "Since the incoming records are files, I am not sure how much value we get in introducing a LazyIterator. I have both options listed here(inline vs lazy iterator). Let me know your thoughts. Interested in understanding the nuances.", "url": "https://github.com/apache/hudi/pull/1402#discussion_r399408154", "createdAt": "2020-03-27T16:57:04Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.func.LazyIterableIterator;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.PairFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from parquet and joins with incoming records to find the\n+ * tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.persist(config.getSimpleIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(\n+        loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable)).sortBy(Tuple2::_1, true, config.getSimpleIndexParallelism());\n+\n+    // Step 3: Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD = findMatchingFilesForRecordKeys(fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  @Override\n+  protected JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = partitionRecordKeyPairRDD.countByKey();\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    JavaRDD<Tuple2<String, String>> fileInfoList =\n+        jsc.parallelize(loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable));\n+\n+    return findMatchingFilesForRecordKeys(fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  @VisibleForTesting\n+  List<Tuple2<String, String>> loadAllFilesForPartitions(List<String> partitions, final JavaSparkContext jsc,\n+                                                         final HoodieTable hoodieTable) {\n+\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList = loadLatestDataFilesForAllPartitions(partitions,\n+        jsc, hoodieTable);\n+    return partitionPathFileIDList.stream()\n+        .map(pf -> new Tuple2<>(pf.getKey(), pf.getValue())).collect(toList());\n+  }\n+\n+  /**\n+   * Find <HoodieKey, HoodieRecordLocation> for all incoming HoodieKeys\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(JavaRDD<Tuple2<String, String>> partitionToFileIndexInfo,\n+                                                                                      JavaPairRDD<String, String> partitionRecordKeyPairRDD, HoodieTable hoodieTable) {\n+    // Step 1: for each <partition, fileId> pair, co locate records to be searched for\n+    JavaPairRDD<String, String> partitionFileIndexInfoPairRDD = partitionToFileIndexInfo.mapToPair(entry -> new Tuple2<>(entry._1, entry._2));\n+    JavaPairRDD<String, List<String>> partitionToRecords = partitionRecordKeyPairRDD.groupByKey().mapToPair(entry -> new Tuple2(entry._1, Lists.newArrayList(entry._2)));\n+    JavaPairRDD<String, Tuple2<String, List<String>>> partitionToFileIdAndRecordsPairRDD = partitionFileIndexInfoPairRDD.join(partitionToRecords);\n+\n+    // Step 2: For each partition, fileId, List<recordKeys> Triplet -> Fetch RDD of triplets ( Tuple2< HoodieKey, HoodieRecordLocation >)\n+\n+    /*Option1:\n+    return partitionToFileIdAndRecordsPairRDD.flatMapToPair(\n+        (PairFlatMapFunction<Tuple2<String, Tuple2<String, List<String>>>, HoodieKey, HoodieRecordLocation>) stringTuple2Tuple2 -> {\n+          HoodieDataFile latestDataFile = getLatestDataFile(hoodieTable, Pair.of(stringTuple2Tuple2._1, stringTuple2Tuple2._2._1));\n+          List<Pair<HoodieKey, HoodieRecordLocation>> resultList = ParquetUtils.fetchRecordKeyPartitionPathFromParquet(hoodieTable.getHadoopConf(), new Path(latestDataFile.getPath()),\n+              latestDataFile.getCommitTime(), stringTuple2Tuple2._2._1, stringTuple2Tuple2._2._2);\n+          List<Tuple2<HoodieKey, HoodieRecordLocation>> toReturn = new ArrayList<>();\n+          resultList.forEach(rec -> toReturn.add(new Tuple2<>(rec.getLeft(), rec.getRight())));\n+          return toReturn.iterator();\n+        });*/\n+\n+    // Option2:\n+    return partitionToFileIdAndRecordsPairRDD.mapPartitions(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 161}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg0MTQ1MDI1", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-384145025", "createdAt": "2020-03-30T19:01:46Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxOTowMTo0N1rOF9389Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxOToxMzoxNVrOF94WOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyNDE4MQ==", "bodyText": "let's fix it in this itself?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r400424181", "createdAt": "2020-03-30T19:01:47Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieIndex.java", "diffHunk": "@@ -118,9 +121,10 @@ protected HoodieIndex(HoodieWriteConfig config) {\n   /**\n    * Each index type should implement it's own logic to release any resources acquired during the process.\n    */\n-  public void close() {}\n+  public void close() {\n+  }\n \n   public enum IndexType {\n-    HBASE, INMEMORY, BLOOM, GLOBAL_BLOOM\n+    HBASE, INMEMORY, BLOOM, GLOBAL_BLOOM, SIMPLE", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NzYyNA=="}, "originalCommit": null, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyNTEyOA==", "bodyText": "Will do.. We should really create a new abstract class AbstactFileLevelIndex which can be extended by the BloomIndex and SimpleIndex classes..  HoodieIndex should be left alone, since stuff the HBaseIndex does not really work this way..", "url": "https://github.com/apache/hudi/pull/1402#discussion_r400425128", "createdAt": "2020-03-30T19:03:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODIyMjY4OA=="}, "originalCommit": null, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyNTU5OQ==", "bodyText": "I have paused the other impl for now.. But let's get the abstractions right once and reuse.. with you there", "url": "https://github.com/apache/hudi/pull/1402#discussion_r400425599", "createdAt": "2020-03-30T19:04:12Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NjI5MA=="}, "originalCommit": null, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyNjUzNg==", "bodyText": "yes the above can be reused by using HoodieKey itself.. Things like\nPair<String, String> for recordKey, FileID\nPar<String, String>  for fileID, partitionPath\ncan we replaced by pojos.. will help readability a lot", "url": "https://github.com/apache/hudi/pull/1402#discussion_r400426536", "createdAt": "2020-03-30T19:05:58Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.Optional;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields from parquet and joins with incoming records to find the tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        hoodieKeys.mapToPair(key -> new Tuple2<>(key.getPartitionPath(), key.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> recordKeyLocationRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    JavaPairRDD<HoodieKey, String> keyHoodieKeyPairRDD = hoodieKeys.mapToPair(key -> new Tuple2<>(key, null));\n+\n+    return keyHoodieKeyPairRDD.leftOuterJoin(recordKeyLocationRDD).mapToPair(keyLoc -> {\n+      Option<Pair<String, String>> partitionPathFileidPair;\n+      if (keyLoc._2._2.isPresent()) {\n+        partitionPathFileidPair = Option.of(Pair.of(keyLoc._1().getPartitionPath(), keyLoc._2._2.get().getFileId()));\n+      } else {\n+        partitionPathFileidPair = Option.empty();\n+      }\n+      return new Tuple2<>(keyLoc._1, partitionPathFileidPair);\n+    });\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.persist(config.getBloomIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD =\n+        lookupIndex(partitionRecordKeyPairRDD, jsc, hoodieTable);\n+\n+    // Cache the result, for subsequent stages.\n+    if (config.getBloomIndexUseCaching()) {\n+      keyFilenamePairRDD.persist(StorageLevel.MEMORY_AND_DISK_SER());\n+    }\n+    if (LOG.isDebugEnabled()) {\n+      long totalTaggedRecords = keyFilenamePairRDD.count();\n+      LOG.debug(\"Number of update records (ones tagged with a fileID): \" + totalTaggedRecords);\n+    }\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getBloomIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+      keyFilenamePairRDD.unpersist();\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    // Nope, don't need to do anything.\n+    return true;\n+  }\n+\n+  /**\n+   * This is not global, since we depend on the partitionPath to do the lookup.\n+   */\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  /**\n+   * No indexes into log files yet.\n+   */\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  /**\n+   * Bloom filters are stored, into the same data files.\n+   */\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1Njk5Ng=="}, "originalCommit": null, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyNzEzOQ==", "bodyText": "how much does it increase by... doing the full suite with both indexes may be an overkill IMO", "url": "https://github.com/apache/hudi/pull/1402#discussion_r400427139", "createdAt": "2020-03-30T19:07:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -79,9 +82,22 @@\n import static org.mockito.Mockito.when;\n \n @SuppressWarnings(\"unchecked\")\n+@RunWith(Parameterized.class)\n public class TestHoodieClientOnCopyOnWriteStorage extends TestHoodieClientBase {\n \n   private static final Logger LOG = LogManager.getLogger(TestHoodieClientOnCopyOnWriteStorage.class);\n+  private final IndexType indexType;\n+\n+  @Parameterized.Parameters(name = \"{index}: Test with IndexType={0}\")\n+  public static Collection<Object[]> data() {\n+    Object[][] data =\n+        new Object[][] {{IndexType.BLOOM},{IndexType.GLOBAL_BLOOM},{IndexType.SIMPLE}};", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk3Mjk0MA=="}, "originalCommit": null, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQyOTc0MQ==", "bodyText": "rename the parameters?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r400429741", "createdAt": "2020-03-30T19:11:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.func.LazyIterableIterator;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.PairFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from parquet and joins with incoming records to find the\n+ * tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.persist(config.getSimpleIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(\n+        loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable)).sortBy(Tuple2::_1, true, config.getSimpleIndexParallelism());\n+\n+    // Step 3: Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD = findMatchingFilesForRecordKeys(fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  @Override\n+  protected JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = partitionRecordKeyPairRDD.countByKey();\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    JavaRDD<Tuple2<String, String>> fileInfoList =\n+        jsc.parallelize(loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable));\n+\n+    return findMatchingFilesForRecordKeys(fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  @VisibleForTesting\n+  List<Tuple2<String, String>> loadAllFilesForPartitions(List<String> partitions, final JavaSparkContext jsc,\n+                                                         final HoodieTable hoodieTable) {\n+\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList = loadLatestDataFilesForAllPartitions(partitions,\n+        jsc, hoodieTable);\n+    return partitionPathFileIDList.stream()\n+        .map(pf -> new Tuple2<>(pf.getKey(), pf.getValue())).collect(toList());\n+  }\n+\n+  /**\n+   * Find <HoodieKey, HoodieRecordLocation> for all incoming HoodieKeys\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(JavaRDD<Tuple2<String, String>> partitionToFileIndexInfo,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 140}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDQzMDY0OA==", "bodyText": "I find these hard to read :( ...", "url": "https://github.com/apache/hudi/pull/1402#discussion_r400430648", "createdAt": "2020-03-30T19:13:15Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/bloom/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.bloom;\n+\n+import org.apache.hudi.WriteStatus;\n+import org.apache.hudi.common.model.HoodieDataFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.func.LazyIterableIterator;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import com.clearspring.analytics.util.Lists;\n+import com.google.common.annotations.VisibleForTesting;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.PairFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from parquet and joins with incoming records to find the\n+ * tagged location\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieBloomIndex<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieSimpleIndex.class);\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+\n+    // Step 0: cache the input record RDD\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.persist(config.getSimpleIndexInputStorageLevel());\n+    }\n+\n+    // Step 1: Extract out thinner JavaPairRDD of (partitionPath, recordKey)\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD =\n+        recordRDD.mapToPair(record -> new Tuple2<>(record.getPartitionPath(), record.getRecordKey()));\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(\n+        loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable)).sortBy(Tuple2::_1, true, config.getSimpleIndexParallelism());\n+\n+    // Step 3: Lookup indexes for all the partition/recordkey pair\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> keyFilenamePairRDD = findMatchingFilesForRecordKeys(fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+\n+    // Step 4: Tag the incoming records, as inserts or updates, by joining with existing record keys\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = tagLocationBacktoRecords(keyFilenamePairRDD, recordRDD);\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  /**\n+   * Lookup the location for each record key and return the pair<record_key,location> for all record keys already\n+   * present and drop the record keys if not present.\n+   */\n+  @Override\n+  protected JavaPairRDD<HoodieKey, HoodieRecordLocation> lookupIndex(\n+      JavaPairRDD<String, String> partitionRecordKeyPairRDD, final JavaSparkContext jsc,\n+      final HoodieTable hoodieTable) {\n+    // Obtain records per partition, in the incoming records\n+    Map<String, Long> recordsPerPartition = partitionRecordKeyPairRDD.countByKey();\n+    List<String> affectedPartitionPathList = new ArrayList<>(recordsPerPartition.keySet());\n+\n+    // Step 2: Load all involved files as <Partition, filename> pairs\n+    JavaRDD<Tuple2<String, String>> fileInfoList =\n+        jsc.parallelize(loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable));\n+\n+    return findMatchingFilesForRecordKeys(fileInfoList, partitionRecordKeyPairRDD, hoodieTable);\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  @VisibleForTesting\n+  List<Tuple2<String, String>> loadAllFilesForPartitions(List<String> partitions, final JavaSparkContext jsc,\n+                                                         final HoodieTable hoodieTable) {\n+\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList = loadLatestDataFilesForAllPartitions(partitions,\n+        jsc, hoodieTable);\n+    return partitionPathFileIDList.stream()\n+        .map(pf -> new Tuple2<>(pf.getKey(), pf.getValue())).collect(toList());\n+  }\n+\n+  /**\n+   * Find <HoodieKey, HoodieRecordLocation> for all incoming HoodieKeys\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> findMatchingFilesForRecordKeys(JavaRDD<Tuple2<String, String>> partitionToFileIndexInfo,\n+                                                                                      JavaPairRDD<String, String> partitionRecordKeyPairRDD, HoodieTable hoodieTable) {\n+    // Step 1: for each <partition, fileId> pair, co locate records to be searched for\n+    JavaPairRDD<String, String> partitionFileIndexInfoPairRDD = partitionToFileIndexInfo.mapToPair(entry -> new Tuple2<>(entry._1, entry._2));\n+    JavaPairRDD<String, List<String>> partitionToRecords = partitionRecordKeyPairRDD.groupByKey().mapToPair(entry -> new Tuple2(entry._1, Lists.newArrayList(entry._2)));\n+    JavaPairRDD<String, Tuple2<String, List<String>>> partitionToFileIdAndRecordsPairRDD = partitionFileIndexInfoPairRDD.join(partitionToRecords);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 145}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NzAzNTIx", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-387703521", "createdAt": "2020-04-04T14:21:06Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxODgzMzQx", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-391883341", "createdAt": "2020-04-12T18:23:20Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMlQxODoyMzoyMFrOGEXpPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMlQxODoyNTo1MVrOGEXqqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNDg3Nw==", "bodyText": "If you have more than 100 executors , then your performance will be limited by parallelism.. I think we should have simpleIndexParallelism default to 0. if its 0, we have parallelism=number of files, if its set to a non-zero value, we use that ?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r407234877", "createdAt": "2020-04-12T18:23:20Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.persist(SparkConfigUtils.getBloomIndexInputStorageLevel(config.getProps()));\n+    }\n+\n+    JavaPairRDD<HoodieKey, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getKey(), entry));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(incomingRecords.keys(), jsc, hoodieTable);\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Tagging incoming records with record location\");\n+    JavaRDD<Tuple2<HoodieKey, Tuple2<HoodieRecord, Option<HoodieRecordLocation>>>> untaggedRecordsRDD = incomingRecords.leftOuterJoin(existingRecords)\n+        .map(entry -> new Tuple2(entry._1, new Tuple2(entry._2._1, Option.ofNullable(entry._2._2.orNull()))));\n+\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = untaggedRecordsRDD.map(entry -> getTaggedRecord(entry._2._1, entry._2._2));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not.\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<HoodieKey, Option<HoodieRecordLocation>> incomingRecords =\n+        hoodieKeys.mapToPair(entry -> new Tuple2<>(entry, Option.empty()));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(hoodieKeys, jsc, hoodieTable);\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Joining existing records with incoming keys to find record locations\");\n+    return incomingRecords.leftOuterJoin(existingRecords)\n+        .mapToPair(entry -> new Tuple2(entry._1, Option.ofNullable(entry._2._2.orNull())));\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link HoodieKey}s.\n+   *\n+   * @param hoodieKeys  {@link JavaRDD} of {@link HoodieKey}s for which locations are fetched\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @return {@link JavaPairRDD} of {@link HoodieKey} and {@link HoodieRecordLocation}\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> fetchRecordLocations(JavaRDD<HoodieKey> hoodieKeys, JavaSparkContext jsc, HoodieTable hoodieTable) {\n+\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD = hoodieKeys.mapToPair(entry -> new Tuple2(entry.getPartitionPath(), entry.getRecordKey()));\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Fetching fileInfos for partitions of interest\");\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(\n+        loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable)).sortBy(Tuple2::_1, true, config.getSimpleIndexParallelism());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNTA1Mg==", "bodyText": "getLatestBaseFile() (standard terminology)", "url": "https://github.com/apache/hudi/pull/1402#discussion_r407235052", "createdAt": "2020-04-12T18:24:28Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.persist(SparkConfigUtils.getBloomIndexInputStorageLevel(config.getProps()));\n+    }\n+\n+    JavaPairRDD<HoodieKey, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getKey(), entry));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(incomingRecords.keys(), jsc, hoodieTable);\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Tagging incoming records with record location\");\n+    JavaRDD<Tuple2<HoodieKey, Tuple2<HoodieRecord, Option<HoodieRecordLocation>>>> untaggedRecordsRDD = incomingRecords.leftOuterJoin(existingRecords)\n+        .map(entry -> new Tuple2(entry._1, new Tuple2(entry._2._1, Option.ofNullable(entry._2._2.orNull()))));\n+\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = untaggedRecordsRDD.map(entry -> getTaggedRecord(entry._2._1, entry._2._2));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not.\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<HoodieKey, Option<HoodieRecordLocation>> incomingRecords =\n+        hoodieKeys.mapToPair(entry -> new Tuple2<>(entry, Option.empty()));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(hoodieKeys, jsc, hoodieTable);\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Joining existing records with incoming keys to find record locations\");\n+    return incomingRecords.leftOuterJoin(existingRecords)\n+        .mapToPair(entry -> new Tuple2(entry._1, Option.ofNullable(entry._2._2.orNull())));\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link HoodieKey}s.\n+   *\n+   * @param hoodieKeys  {@link JavaRDD} of {@link HoodieKey}s for which locations are fetched\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @return {@link JavaPairRDD} of {@link HoodieKey} and {@link HoodieRecordLocation}\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> fetchRecordLocations(JavaRDD<HoodieKey> hoodieKeys, JavaSparkContext jsc, HoodieTable hoodieTable) {\n+\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD = hoodieKeys.mapToPair(entry -> new Tuple2(entry.getPartitionPath(), entry.getRecordKey()));\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Fetching fileInfos for partitions of interest\");\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(\n+        loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable)).sortBy(Tuple2::_1, true, config.getSimpleIndexParallelism());\n+\n+    JavaPairRDD<String, String> partitionFileIndexInfoPairRDD = fileInfoList.mapToPair(entry -> new Tuple2<>(entry._1, entry._2));\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Fetching records from all files of interest\");\n+    return partitionFileIndexInfoPairRDD.flatMapToPair(\n+        (PairFlatMapFunction<Tuple2<String, String>, HoodieKey, HoodieRecordLocation>) partitionPathFileId ->\n+            new RecordFetcher(partitionPathFileId, hoodieTable).getResultSet()\n+    );\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  private List<Tuple2<String, String>> loadAllFilesForPartitions(List<String> partitions, final JavaSparkContext jsc,\n+                                                                 final HoodieTable hoodieTable) {\n+\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList = loadLatestDataFilesForAllPartitions(partitions,\n+        jsc, hoodieTable);\n+    return partitionPathFileIDList.stream()\n+        .map(pf -> new Tuple2<>(pf.getKey(), pf.getValue())).collect(toList());\n+  }\n+\n+  /**\n+   * Fetch the latest base file for the given partition and fileId.\n+   *\n+   * @param hoodieTable           instance of {@link HoodieTable} in which the partition exists\n+   * @param partitionPathFilePair Partition path fileId pair\n+   * @return the latest data file for the given partition and fileId\n+   */\n+  private HoodieBaseFile getLatestDataFile(HoodieTable hoodieTable, Pair<String, String> partitionPathFilePair) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNTI0MA==", "bodyText": "we can't call ParquetUtils from here.. that breaks abstraction. Lets design a new handle", "url": "https://github.com/apache/hudi/pull/1402#discussion_r407235240", "createdAt": "2020-04-12T18:25:51Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.persist(SparkConfigUtils.getBloomIndexInputStorageLevel(config.getProps()));\n+    }\n+\n+    JavaPairRDD<HoodieKey, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getKey(), entry));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(incomingRecords.keys(), jsc, hoodieTable);\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Tagging incoming records with record location\");\n+    JavaRDD<Tuple2<HoodieKey, Tuple2<HoodieRecord, Option<HoodieRecordLocation>>>> untaggedRecordsRDD = incomingRecords.leftOuterJoin(existingRecords)\n+        .map(entry -> new Tuple2(entry._1, new Tuple2(entry._2._1, Option.ofNullable(entry._2._2.orNull()))));\n+\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = untaggedRecordsRDD.map(entry -> getTaggedRecord(entry._2._1, entry._2._2));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not.\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<HoodieKey, Option<HoodieRecordLocation>> incomingRecords =\n+        hoodieKeys.mapToPair(entry -> new Tuple2<>(entry, Option.empty()));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(hoodieKeys, jsc, hoodieTable);\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Joining existing records with incoming keys to find record locations\");\n+    return incomingRecords.leftOuterJoin(existingRecords)\n+        .mapToPair(entry -> new Tuple2(entry._1, Option.ofNullable(entry._2._2.orNull())));\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link HoodieKey}s.\n+   *\n+   * @param hoodieKeys  {@link JavaRDD} of {@link HoodieKey}s for which locations are fetched\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @return {@link JavaPairRDD} of {@link HoodieKey} and {@link HoodieRecordLocation}\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> fetchRecordLocations(JavaRDD<HoodieKey> hoodieKeys, JavaSparkContext jsc, HoodieTable hoodieTable) {\n+\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD = hoodieKeys.mapToPair(entry -> new Tuple2(entry.getPartitionPath(), entry.getRecordKey()));\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Fetching fileInfos for partitions of interest\");\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(\n+        loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable)).sortBy(Tuple2::_1, true, config.getSimpleIndexParallelism());\n+\n+    JavaPairRDD<String, String> partitionFileIndexInfoPairRDD = fileInfoList.mapToPair(entry -> new Tuple2<>(entry._1, entry._2));\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Fetching records from all files of interest\");\n+    return partitionFileIndexInfoPairRDD.flatMapToPair(\n+        (PairFlatMapFunction<Tuple2<String, String>, HoodieKey, HoodieRecordLocation>) partitionPathFileId ->\n+            new RecordFetcher(partitionPathFileId, hoodieTable).getResultSet()\n+    );\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  private List<Tuple2<String, String>> loadAllFilesForPartitions(List<String> partitions, final JavaSparkContext jsc,\n+                                                                 final HoodieTable hoodieTable) {\n+\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList = loadLatestDataFilesForAllPartitions(partitions,\n+        jsc, hoodieTable);\n+    return partitionPathFileIDList.stream()\n+        .map(pf -> new Tuple2<>(pf.getKey(), pf.getValue())).collect(toList());\n+  }\n+\n+  /**\n+   * Fetch the latest base file for the given partition and fileId.\n+   *\n+   * @param hoodieTable           instance of {@link HoodieTable} in which the partition exists\n+   * @param partitionPathFilePair Partition path fileId pair\n+   * @return the latest data file for the given partition and fileId\n+   */\n+  private HoodieBaseFile getLatestDataFile(HoodieTable hoodieTable, Pair<String, String> partitionPathFilePair) {\n+    return hoodieTable.getBaseFileOnlyView()\n+        .getLatestBaseFile(partitionPathFilePair.getLeft(), partitionPathFilePair.getRight()).get();\n+  }\n+\n+  /**\n+   * Record Fetcher for a given partitionPath, fileId pair.\n+   */\n+  class RecordFetcher {\n+    private HoodieTable<T> table;\n+    private Tuple2<String, String> partitionPathFileIdPair;\n+\n+    RecordFetcher(Tuple2<String, String> partitionPathFileIdPair, HoodieTable<T> table) {\n+      this.partitionPathFileIdPair = partitionPathFileIdPair;\n+      this.table = table;\n+    }\n+\n+    Iterator<Tuple2<HoodieKey, HoodieRecordLocation>> getResultSet() throws Exception {\n+      HoodieBaseFile baseFile = getLatestDataFile(table, Pair.of(partitionPathFileIdPair._1, partitionPathFileIdPair._2));\n+      List<Pair<HoodieKey, HoodieRecordLocation>> records = ParquetUtils.fetchRecordKeyPartitionPathFromParquet(table.getHadoopConf(), new Path(baseFile.getPath()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 195}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwNTk4NDM0", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-400598434", "createdAt": "2020-04-27T02:58:57Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMjo1ODo1N1rOGMOq-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMjo1ODo1N1rOGMOq-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ3NjQ3Mw==", "bodyText": "@vinothchandar : have removed other index types to see travis CI succeed. I got\n The job exceeded the maximum time limit for jobs, and has been terminated.\nin travis CI when enabled.", "url": "https://github.com/apache/hudi/pull/1402#discussion_r415476473", "createdAt": "2020-04-27T02:58:57Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -80,9 +83,22 @@\n import static org.mockito.Mockito.when;\n \n @SuppressWarnings(\"unchecked\")\n+@RunWith(Parameterized.class)\n public class TestHoodieClientOnCopyOnWriteStorage extends TestHoodieClientBase {\n \n   private static final Logger LOG = LogManager.getLogger(TestHoodieClientOnCopyOnWriteStorage.class);\n+  private final IndexType indexType;\n+\n+  @Parameterized.Parameters(name = \"{index}: Test with IndexType={0}\")\n+  public static Collection<Object[]> data() {\n+    Object[][] data =\n+        new Object[][] {{IndexType.BLOOM}};", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 30}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwNTk4NTg1", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-400598585", "createdAt": "2020-04-27T02:59:37Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMjo1OTozOFrOGMOr0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMjo1OTozOFrOGMOr0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ3NjY5MQ==", "bodyText": "Will remove this file. have integrated into TestHoodieIndex.", "url": "https://github.com/apache/hudi/pull/1402#discussion_r415476691", "createdAt": "2020-04-27T02:59:38Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/index/TestSimpleIndex.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.HoodieClientTestHarness;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.TestRawTripPayload;\n+import org.apache.hudi.common.fs.ConsistencyGuardConfig;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;\n+import org.apache.hudi.common.table.view.FileSystemViewStorageType;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieHBaseIndexConfig;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieStorageConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.bloom.HoodieBloomIndex;\n+import org.apache.hudi.index.bloom.HoodieGlobalBloomIndex;\n+import org.apache.hudi.index.hbase.HBaseIndex;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestSimpleIndex extends HoodieClientTestHarness {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwNTk4Njc0", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-400598674", "createdAt": "2020-04-27T02:59:59Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMjo1OTo1OVrOGMOsRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwMjo1OTo1OVrOGMOsRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTQ3NjgwNA==", "bodyText": "Will remove this file as well. Integrated into TestHoodieIdex.", "url": "https://github.com/apache/hudi/pull/1402#discussion_r415476804", "createdAt": "2020-04-27T02:59:59Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/index/TestGlobalSimpleIndex.java", "diffHunk": "@@ -0,0 +1,332 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.client.HoodieWriteClient;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.common.HoodieClientTestHarness;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.TestRawTripPayload;\n+import org.apache.hudi.common.fs.ConsistencyGuardConfig;\n+import org.apache.hudi.common.model.EmptyHoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodiePartitionMetadata;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;\n+import org.apache.hudi.common.table.view.FileSystemViewStorageType;\n+import org.apache.hudi.common.util.FileIOUtils;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieStorageConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestGlobalSimpleIndex extends HoodieClientTestHarness {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 60}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwNTk4ODAy", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-400598802", "createdAt": "2020-04-27T03:00:30Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAxNTA2NDIw", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-401506420", "createdAt": "2020-04-28T05:25:59Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQwNToyNTo1OVrOGNDIgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOFQxNTo0MjoxNVrOGNajgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMzNjAwMQ==", "bodyText": "just hoodie.simple.index.update.partition.path to be consistent with the other config?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r416336001", "createdAt": "2020-04-28T05:25:59Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieIndexConfig.java", "diffHunk": "@@ -92,6 +100,9 @@\n   public static final String BLOOM_INDEX_UPDATE_PARTITION_PATH = \"hoodie.bloom.index.update.partition.path\";\n   public static final String DEFAULT_BLOOM_INDEX_UPDATE_PARTITION_PATH = \"false\";\n \n+  public static final String GLOBAL_SIMPLE_INDEX_UPDATE_PARTITION_PATH = \"hoodie.global.simple.index.update.partition.path\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjM1NDczNA==", "bodyText": "this can very well go into HoodieIndexUtils ?  There is no instance stats accessed in this method IIUC", "url": "https://github.com/apache/hudi/pull/1402#discussion_r416354734", "createdAt": "2020-04-28T06:15:42Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieIndex.java", "diffHunk": "@@ -128,9 +133,26 @@ protected HoodieIndex(HoodieWriteConfig config) {\n   /**\n    * Each index type should implement it's own logic to release any resources acquired during the process.\n    */\n-  public void close() {}\n+  public void close() {\n+  }\n+\n+  protected HoodieRecord<T> getTaggedRecord(HoodieRecord<T> inputRecord, Option<HoodieRecordLocation> location) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjcxOTc0NA==", "bodyText": "@nsivabalan This is problematic.. since we don't cache (rightfully so) the input here for Global index,\nJavaPairRDD<String, String> partitionRecordKeyPairRDD = hoodieKeys.mapToPair(entry -> new Tuple2(entry.getPartitionPath(), entry.getRecordKey()));\n    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n\nwill collect() once reading the input, and then later  return getTaggedRecords(incomingRecordsKeyedOnRecordKeys, existingRecordsKeyedOnRecordKeys); will also read the input again for joining..\nAlso why fetchRecordLocations() we should be loading all the partitions, not just the ones based on incomingRecord, right?\nGlobal should be much simpler.. All it needs is to create existingRecords based on listing from all partitions and then do the getTaggedRecords() join..", "url": "https://github.com/apache/hudi/pull/1402#discussion_r416719744", "createdAt": "2020-04-28T15:42:15Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieGlobalSimpleIndex.java", "diffHunk": "@@ -0,0 +1,157 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.EmptyHoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A global simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieGlobalSimpleIndex<T extends HoodieRecordPayload> extends HoodieSimpleIndex<T> {\n+\n+  public HoodieGlobalSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    return tagLocationInternal(recordRDD, jsc, hoodieTable);\n+  }\n+\n+  /**\n+   * Tags records location for incoming records.\n+   *\n+   * @param recordRDD   {@link JavaRDD} of incoming records\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} to use\n+   * @return {@link JavaRDD} of records with record locations set\n+   */\n+  protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                                         HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<HoodieKey, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getKey(), entry));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(incomingRecords.keys(), jsc, hoodieTable,", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 78}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAyNjk1Mzkz", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-402695393", "createdAt": "2020-04-29T14:00:13Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxNDowMDoxM1rOGOAP7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQxNDowMDoxM1rOGOAP7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzMzNzMyNQ==", "bodyText": "@vinothchandar : since we are re-using RecordFetcher(used in non-global index) which is returning entries keyed on HoodieKeys, we have to do another map call here after calling fetchRecordLocations. Do you think we can introduce another class to return entries keyed on RecordKeys?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r417337325", "createdAt": "2020-04-29T14:00:13Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieGlobalSimpleIndex.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.EmptyHoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A global simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieGlobalSimpleIndex<T extends HoodieRecordPayload> extends HoodieSimpleIndex<T> {\n+\n+  public HoodieGlobalSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    return tagLocationInternal(recordRDD, jsc, hoodieTable);\n+  }\n+\n+  /**\n+   * Tags records location for incoming records.\n+   *\n+   * @param recordRDD   {@link JavaRDD} of incoming records\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} to use\n+   * @return {@link JavaRDD} of records with record locations set\n+   */\n+  protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                                         HoodieTable<T> hoodieTable) {\n+\n+    JavaPairRDD<String, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getRecordKey(), entry));\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(incomingRecords.values().map(HoodieRecord::getKey), jsc, hoodieTable,\n+        config.getGlobalSimpleIndexParallelism());\n+\n+    JavaPairRDD<String, Tuple2<HoodieKey, HoodieRecordLocation>> existingRecordsKeyedOnRecordKeys = existingRecords.mapToPair(entry -> new Tuple2<>(entry._1.getRecordKey(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAyNjk1Njk1", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-402695695", "createdAt": "2020-04-29T14:00:31Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEwMzkyNDcy", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-410392472", "createdAt": "2020-05-12T20:21:39Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQyMDoyMTozOVrOGUXgkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xMlQyMzoxNjozOVrOGUcNJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDAwOTg3NA==", "bodyText": "need to add such builder methods for all the simple index configs?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424009874", "createdAt": "2020-05-12T20:21:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/config/HoodieIndexConfig.java", "diffHunk": "@@ -201,6 +212,11 @@ public Builder withBloomIndexUpdatePartitionPath(boolean updatePartitionPath) {\n       return this;\n     }\n \n+    public Builder withGlobalSimpleIndexUpdatePartitionPath(boolean updatePartitionPath) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDAxMDY2NA==", "bodyText": "why was this needed?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424010664", "createdAt": "2020-05-12T20:23:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -86,6 +86,9 @@\n \n   private static final Logger LOG = LogManager.getLogger(TestHoodieClientOnCopyOnWriteStorage.class);\n \n+  public TestHoodieClientOnCopyOnWriteStorage() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDAyMzU1NQ==", "bodyText": "we need to persist the input recordRDD here.. not the transformed RDD", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424023555", "createdAt": "2020-05-12T20:47:33Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    return tagLocationInternal(recordRDD, jsc, hoodieTable);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option. Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+\n+    return fetchRecordLocationInternal(hoodieKeys, jsc, hoodieTable, config.getSimpleIndexParallelism());\n+  }\n+\n+  /**\n+   * Tags records location for incoming records.\n+   *\n+   * @param recordRDD   {@link JavaRDD} of incoming records\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} to use\n+   * @return {@link JavaRDD} of records with record locations set\n+   */\n+  protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                                         HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<HoodieKey, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getKey(), entry));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      incomingRecords.persist(SparkConfigUtils.getSimpleIndexInputStorageLevel(config.getProps()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA0Mjk5Mw==", "bodyText": "we can return the full BaseFile here itself and avoid an additional lookup later for Simple Index..\nBloomIndex just uses file IDs so it shuffles less.. but we don't have to be concerned for simple index per se", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424042993", "createdAt": "2020-05-12T21:25:37Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieIndexUtils.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+/**\n+ * Hoodie Index Utilities.\n+ */\n+public class HoodieIndexUtils {\n+\n+  /**\n+   * Fetches Pair of partition path and fileId for interested partitions.\n+   *\n+   * @param partitions  list of partitions of interest\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @return the list of Pairs of partition path and fileId\n+   */\n+  public static List<Pair<String, String>> loadLatestDataFilesForAllPartitions(List<String> partitions, final JavaSparkContext jsc,\n+                                                                               final HoodieTable hoodieTable) {\n+    return jsc.parallelize(partitions, Math.max(partitions.size(), 1)).flatMap(partitionPath -> {\n+      Option<HoodieInstant> latestCommitTime =\n+          hoodieTable.getMetaClient().getCommitsTimeline().filterCompletedInstants().lastInstant();\n+      List<Pair<String, String>> filteredFiles = new ArrayList<>();\n+      if (latestCommitTime.isPresent()) {\n+        filteredFiles = hoodieTable.getBaseFileOnlyView()\n+            .getLatestBaseFilesBeforeOrOn(partitionPath, latestCommitTime.get().getTimestamp())\n+            .map(f -> Pair.of(partitionPath, f.getFileId())).collect(toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA0MzM3MA==", "bodyText": "this parallelize's parallelism has to be determined based on files being read..", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424043370", "createdAt": "2020-05-12T21:26:19Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    return tagLocationInternal(recordRDD, jsc, hoodieTable);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option. Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+\n+    return fetchRecordLocationInternal(hoodieKeys, jsc, hoodieTable, config.getSimpleIndexParallelism());\n+  }\n+\n+  /**\n+   * Tags records location for incoming records.\n+   *\n+   * @param recordRDD   {@link JavaRDD} of incoming records\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} to use\n+   * @return {@link JavaRDD} of records with record locations set\n+   */\n+  protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                                         HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<HoodieKey, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getKey(), entry));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      incomingRecords.persist(SparkConfigUtils.getSimpleIndexInputStorageLevel(config.getProps()));\n+    }\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocationsBasedOnHoodieKey(incomingRecords.keys(), jsc, hoodieTable,\n+        config.getSimpleIndexParallelism());\n+\n+    JavaRDD<Tuple2<HoodieKey, Tuple2<HoodieRecord, Option<HoodieRecordLocation>>>> untaggedRecordsRDD = incomingRecords.leftOuterJoin(existingRecords)\n+        .map(entry -> new Tuple2(entry._1, new Tuple2(entry._2._1, Option.ofNullable(entry._2._2.orNull()))));\n+\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = untaggedRecordsRDD.map(entry -> HoodieIndexUtils.getTaggedRecord(entry._2._1, entry._2._2));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link JavaRDD} of HoodieKeys.\n+   *\n+   * @param hoodieKeys  {@link JavaRDD} of {@link HoodieKey}s\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @param parallelism parallelism to use\n+   * @return Hoodiekeys mapped to partitionpath and filenames\n+   */\n+  JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocationInternal(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                   JavaSparkContext jsc, HoodieTable<T> hoodieTable,\n+                                                                                   int parallelism) {\n+    JavaPairRDD<HoodieKey, Option<HoodieRecordLocation>> incomingRecords =\n+        hoodieKeys.mapToPair(entry -> new Tuple2<>(entry, Option.empty()));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocationsBasedOnHoodieKey(hoodieKeys, jsc, hoodieTable, parallelism);\n+\n+    JavaPairRDD<HoodieKey, Option<HoodieRecordLocation>> recordLocations = incomingRecords.leftOuterJoin(existingRecords)\n+        .mapToPair(entry -> new Tuple2(entry._1, Option.ofNullable(entry._2._2.orNull())));\n+    return recordLocations.mapToPair(entry -> {\n+      if (entry._2.isPresent()) {\n+        return new Tuple2<>(entry._1, Option.of(Pair.of(entry._1.getPartitionPath(), entry._2.get().getFileId())));\n+      } else {\n+        return new Tuple2<>(entry._1, Option.empty());\n+      }\n+    });\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link HoodieKey}s.\n+   *\n+   * @param hoodieKeys  {@link JavaRDD} of {@link HoodieKey}s for which locations are fetched\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @param parallelism parallelism to use\n+   * @return {@link JavaPairRDD} of {@link HoodieKey} and {@link HoodieRecordLocation}\n+   */\n+  protected JavaPairRDD<HoodieKey, HoodieRecordLocation> fetchRecordLocationsBasedOnHoodieKey(JavaRDD<HoodieKey> hoodieKeys, JavaSparkContext jsc, HoodieTable hoodieTable,\n+                                                                                              int parallelism) {\n+\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD = hoodieKeys.mapToPair(entry -> new Tuple2(entry.getPartitionPath(), entry.getRecordKey()));\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA0NDc1Nw==", "bodyText": "why sort by partition, when reading happens file by file?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424044757", "createdAt": "2020-05-12T21:29:23Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    return tagLocationInternal(recordRDD, jsc, hoodieTable);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option. Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+\n+    return fetchRecordLocationInternal(hoodieKeys, jsc, hoodieTable, config.getSimpleIndexParallelism());\n+  }\n+\n+  /**\n+   * Tags records location for incoming records.\n+   *\n+   * @param recordRDD   {@link JavaRDD} of incoming records\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} to use\n+   * @return {@link JavaRDD} of records with record locations set\n+   */\n+  protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                                         HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<HoodieKey, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getKey(), entry));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      incomingRecords.persist(SparkConfigUtils.getSimpleIndexInputStorageLevel(config.getProps()));\n+    }\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocationsBasedOnHoodieKey(incomingRecords.keys(), jsc, hoodieTable,\n+        config.getSimpleIndexParallelism());\n+\n+    JavaRDD<Tuple2<HoodieKey, Tuple2<HoodieRecord, Option<HoodieRecordLocation>>>> untaggedRecordsRDD = incomingRecords.leftOuterJoin(existingRecords)\n+        .map(entry -> new Tuple2(entry._1, new Tuple2(entry._2._1, Option.ofNullable(entry._2._2.orNull()))));\n+\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = untaggedRecordsRDD.map(entry -> HoodieIndexUtils.getTaggedRecord(entry._2._1, entry._2._2));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link JavaRDD} of HoodieKeys.\n+   *\n+   * @param hoodieKeys  {@link JavaRDD} of {@link HoodieKey}s\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @param parallelism parallelism to use\n+   * @return Hoodiekeys mapped to partitionpath and filenames\n+   */\n+  JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocationInternal(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                   JavaSparkContext jsc, HoodieTable<T> hoodieTable,\n+                                                                                   int parallelism) {\n+    JavaPairRDD<HoodieKey, Option<HoodieRecordLocation>> incomingRecords =\n+        hoodieKeys.mapToPair(entry -> new Tuple2<>(entry, Option.empty()));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocationsBasedOnHoodieKey(hoodieKeys, jsc, hoodieTable, parallelism);\n+\n+    JavaPairRDD<HoodieKey, Option<HoodieRecordLocation>> recordLocations = incomingRecords.leftOuterJoin(existingRecords)\n+        .mapToPair(entry -> new Tuple2(entry._1, Option.ofNullable(entry._2._2.orNull())));\n+    return recordLocations.mapToPair(entry -> {\n+      if (entry._2.isPresent()) {\n+        return new Tuple2<>(entry._1, Option.of(Pair.of(entry._1.getPartitionPath(), entry._2.get().getFileId())));\n+      } else {\n+        return new Tuple2<>(entry._1, Option.empty());\n+      }\n+    });\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link HoodieKey}s.\n+   *\n+   * @param hoodieKeys  {@link JavaRDD} of {@link HoodieKey}s for which locations are fetched\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @param parallelism parallelism to use\n+   * @return {@link JavaPairRDD} of {@link HoodieKey} and {@link HoodieRecordLocation}\n+   */\n+  protected JavaPairRDD<HoodieKey, HoodieRecordLocation> fetchRecordLocationsBasedOnHoodieKey(JavaRDD<HoodieKey> hoodieKeys, JavaSparkContext jsc, HoodieTable hoodieTable,\n+                                                                                              int parallelism) {\n+\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD = hoodieKeys.mapToPair(entry -> new Tuple2(entry.getPartitionPath(), entry.getRecordKey()));\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(\n+        loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable)).sortBy(Tuple2::_1, true, parallelism);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 181}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA4NTk3Ng==", "bodyText": "can we use an empty list instead or null", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424085976", "createdAt": "2020-05-12T23:13:45Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java", "diffHunk": "@@ -105,6 +122,60 @@\n     return rowKeys;\n   }\n \n+  /**\n+   * Read the rows with record key and partition path from the given parquet file.\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return Set Set of row keys matching candidateRecordKeys\n+   */\n+  public static List<Pair<HoodieKey, HoodieRecordLocation>> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                                                   String baseInstantTime,\n+                                                                                                   String fileId) {\n+    return fetchRecordKeyPartitionPathFromParquet(configuration, filePath, baseInstantTime, fileId, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA4NjQ2MA==", "bodyText": "can we just limit this method to reading the parquet fields and nothing else.. i.e you can add the HoodieRecordLocation object outside on the caller side.", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424086460", "createdAt": "2020-05-12T23:15:27Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java", "diffHunk": "@@ -105,6 +122,60 @@\n     return rowKeys;\n   }\n \n+  /**\n+   * Read the rows with record key and partition path from the given parquet file.\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return Set Set of row keys matching candidateRecordKeys\n+   */\n+  public static List<Pair<HoodieKey, HoodieRecordLocation>> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                                                   String baseInstantTime,\n+                                                                                                   String fileId) {\n+    return fetchRecordKeyPartitionPathFromParquet(configuration, filePath, baseInstantTime, fileId, null);\n+  }\n+\n+  /**\n+   * Read the rows with record key and partition path from the given parquet file.\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return Set Set of row keys matching candidateRecordKeys\n+   */\n+  static List<Pair<HoodieKey, HoodieRecordLocation>> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                                            String baseInstantTime,\n+                                                                                            String fileId, List<String> recordsToFilter) {\n+    List<Pair<HoodieKey, HoodieRecordLocation>> rows = new ArrayList<>();\n+    try {\n+      if (!filePath.getFileSystem(configuration).exists(filePath)) {\n+        return new ArrayList<>();\n+      }\n+\n+      Configuration conf = new Configuration(configuration);\n+      conf.addResource(FSUtils.getFs(filePath.toString(), conf).getConf());\n+      Schema readSchema = HoodieAvroUtils.getRecordKeyPartitionPathSchema();\n+      AvroReadSupport.setAvroReadSchema(conf, readSchema);\n+      AvroReadSupport.setRequestedProjection(conf, readSchema);\n+      ParquetReader reader = AvroParquetReader.builder(filePath).withConf(conf).build();\n+      Object obj = reader.read();\n+      while (obj != null) {\n+        if (obj instanceof GenericRecord) {\n+          String recordKey = ((GenericRecord) obj).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n+          String partitionPath = ((GenericRecord) obj).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString();\n+          if (recordsToFilter == null) {\n+            rows.add(Pair.of(new HoodieKey(recordKey, partitionPath), new HoodieRecordLocation(baseInstantTime, fileId)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDA4NjgyMA==", "bodyText": "what I mean by handle is a subclass of HoodieReadHandle .. it's an anti pattern to assume its a parquet base file from the index level.. Let's fix this", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424086820", "createdAt": "2020-05-12T23:16:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.PairFlatMapFunction;\n+\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.index.HoodieIndexUtils.loadLatestDataFilesForAllPartitions;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.persist(SparkConfigUtils.getBloomIndexInputStorageLevel(config.getProps()));\n+    }\n+\n+    JavaPairRDD<HoodieKey, HoodieRecord> incomingRecords = recordRDD.mapToPair(entry -> new Tuple2<>(entry.getKey(), entry));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(incomingRecords.keys(), jsc, hoodieTable);\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Tagging incoming records with record location\");\n+    JavaRDD<Tuple2<HoodieKey, Tuple2<HoodieRecord, Option<HoodieRecordLocation>>>> untaggedRecordsRDD = incomingRecords.leftOuterJoin(existingRecords)\n+        .map(entry -> new Tuple2(entry._1, new Tuple2(entry._2._1, Option.ofNullable(entry._2._2.orNull()))));\n+\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = untaggedRecordsRDD.map(entry -> getTaggedRecord(entry._2._1, entry._2._2));\n+\n+    if (config.getSimpleIndexUseCaching()) {\n+      recordRDD.unpersist(); // unpersist the input Record RDD\n+    }\n+    return taggedRecordRDD;\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option.Empty if the key is not.\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+    JavaPairRDD<HoodieKey, Option<HoodieRecordLocation>> incomingRecords =\n+        hoodieKeys.mapToPair(entry -> new Tuple2<>(entry, Option.empty()));\n+\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords = fetchRecordLocations(hoodieKeys, jsc, hoodieTable);\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Joining existing records with incoming keys to find record locations\");\n+    return incomingRecords.leftOuterJoin(existingRecords)\n+        .mapToPair(entry -> new Tuple2(entry._1, Option.ofNullable(entry._2._2.orNull())));\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link HoodieKey}s.\n+   *\n+   * @param hoodieKeys  {@link JavaRDD} of {@link HoodieKey}s for which locations are fetched\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @return {@link JavaPairRDD} of {@link HoodieKey} and {@link HoodieRecordLocation}\n+   */\n+  private JavaPairRDD<HoodieKey, HoodieRecordLocation> fetchRecordLocations(JavaRDD<HoodieKey> hoodieKeys, JavaSparkContext jsc, HoodieTable hoodieTable) {\n+\n+    JavaPairRDD<String, String> partitionRecordKeyPairRDD = hoodieKeys.mapToPair(entry -> new Tuple2(entry.getPartitionPath(), entry.getRecordKey()));\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Fetching fileInfos for partitions of interest\");\n+    List<String> affectedPartitionPathList = partitionRecordKeyPairRDD.map(tuple -> tuple._1).distinct().collect();\n+    JavaRDD<Tuple2<String, String>> fileInfoList = jsc.parallelize(\n+        loadAllFilesForPartitions(affectedPartitionPathList, jsc, hoodieTable)).sortBy(Tuple2::_1, true, config.getSimpleIndexParallelism());\n+\n+    JavaPairRDD<String, String> partitionFileIndexInfoPairRDD = fileInfoList.mapToPair(entry -> new Tuple2<>(entry._1, entry._2));\n+\n+    jsc.setJobGroup(this.getClass().getSimpleName(), \"Fetching records from all files of interest\");\n+    return partitionFileIndexInfoPairRDD.flatMapToPair(\n+        (PairFlatMapFunction<Tuple2<String, String>, HoodieKey, HoodieRecordLocation>) partitionPathFileId ->\n+            new RecordFetcher(partitionPathFileId, hoodieTable).getResultSet()\n+    );\n+  }\n+\n+  /**\n+   * Load all involved files as <Partition, filename> pair RDD.\n+   */\n+  private List<Tuple2<String, String>> loadAllFilesForPartitions(List<String> partitions, final JavaSparkContext jsc,\n+                                                                 final HoodieTable hoodieTable) {\n+\n+    // Obtain the latest data files from all the partitions.\n+    List<Pair<String, String>> partitionPathFileIDList = loadLatestDataFilesForAllPartitions(partitions,\n+        jsc, hoodieTable);\n+    return partitionPathFileIDList.stream()\n+        .map(pf -> new Tuple2<>(pf.getKey(), pf.getValue())).collect(toList());\n+  }\n+\n+  /**\n+   * Fetch the latest base file for the given partition and fileId.\n+   *\n+   * @param hoodieTable           instance of {@link HoodieTable} in which the partition exists\n+   * @param partitionPathFilePair Partition path fileId pair\n+   * @return the latest data file for the given partition and fileId\n+   */\n+  private HoodieBaseFile getLatestDataFile(HoodieTable hoodieTable, Pair<String, String> partitionPathFilePair) {\n+    return hoodieTable.getBaseFileOnlyView()\n+        .getLatestBaseFile(partitionPathFilePair.getLeft(), partitionPathFilePair.getRight()).get();\n+  }\n+\n+  /**\n+   * Record Fetcher for a given partitionPath, fileId pair.\n+   */\n+  class RecordFetcher {\n+    private HoodieTable<T> table;\n+    private Tuple2<String, String> partitionPathFileIdPair;\n+\n+    RecordFetcher(Tuple2<String, String> partitionPathFileIdPair, HoodieTable<T> table) {\n+      this.partitionPathFileIdPair = partitionPathFileIdPair;\n+      this.table = table;\n+    }\n+\n+    Iterator<Tuple2<HoodieKey, HoodieRecordLocation>> getResultSet() throws Exception {\n+      HoodieBaseFile baseFile = getLatestDataFile(table, Pair.of(partitionPathFileIdPair._1, partitionPathFileIdPair._2));\n+      List<Pair<HoodieKey, HoodieRecordLocation>> records = ParquetUtils.fetchRecordKeyPartitionPathFromParquet(table.getHadoopConf(), new Path(baseFile.getPath()),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzIzNTI0MA=="}, "originalCommit": null, "originalPosition": 195}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEwNTQ2ODkz", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-410546893", "createdAt": "2020-05-13T02:31:35Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QwMjozMTozNVrOGUfYzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QwMjo1NjowN1rOGUfwXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDEzODk1Ng==", "bodyText": "we are caching inputRecordRDD, but left outer join is using keyedInputRecordRDD. Thats why I had to cache differently. Can you help explain the rational? looks like we are not effectively using the cache, isn't ?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424138956", "createdAt": "2020-05-13T02:31:35Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/simple/HoodieSimpleIndex.java", "diffHunk": "@@ -0,0 +1,204 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.simple;\n+\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.utils.SparkConfigUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndex;\n+import org.apache.hudi.index.HoodieIndexUtils;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import scala.Tuple2;\n+\n+import static org.apache.hudi.index.HoodieIndexUtils.getLatestBaseFilesForAllPartitions;\n+\n+/**\n+ * A simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieSimpleIndex<T extends HoodieRecordPayload> extends HoodieIndex<T> {\n+\n+  public HoodieSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<WriteStatus> updateLocation(JavaRDD<WriteStatus> writeStatusRDD, JavaSparkContext jsc,\n+                                             HoodieTable<T> hoodieTable) {\n+    return writeStatusRDD;\n+  }\n+\n+  @Override\n+  public boolean rollbackCommit(String commitTime) {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isGlobal() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean canIndexLogFiles() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isImplicitWithStorage() {\n+    return true;\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    return tagLocationInternal(recordRDD, jsc, hoodieTable);\n+  }\n+\n+  /**\n+   * Returns an RDD mapping each HoodieKey with a partitionPath/fileID which contains it. Option. Empty if the key is not\n+   * found.\n+   *\n+   * @param hoodieKeys  keys to lookup\n+   * @param jsc         spark context\n+   * @param hoodieTable hoodie table object\n+   */\n+  @Override\n+  public JavaPairRDD<HoodieKey, Option<Pair<String, String>>> fetchRecordLocation(JavaRDD<HoodieKey> hoodieKeys,\n+                                                                                  JavaSparkContext jsc, HoodieTable<T> hoodieTable) {\n+\n+    return fetchRecordLocationInternal(hoodieKeys, jsc, hoodieTable, config.getSimpleIndexParallelism());\n+  }\n+\n+  /**\n+   * Tags records location for incoming records.\n+   *\n+   * @param inputRecordRDD   {@link JavaRDD} of incoming records\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} to use\n+   * @return {@link JavaRDD} of records with record locations set\n+   */\n+  protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> inputRecordRDD, JavaSparkContext jsc,\n+                                                         HoodieTable<T> hoodieTable) {\n+    if (config.getSimpleIndexUseCaching()) {\n+      inputRecordRDD.persist(SparkConfigUtils.getSimpleIndexInputStorageLevel(config.getProps()));\n+    }\n+\n+    JavaPairRDD<HoodieKey, HoodieRecord<T>> keyedInputRecordRDD = inputRecordRDD.mapToPair(record -> new Tuple2<>(record.getKey(), record));\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> existingLocationsOnTable = fetchRecordLocationsForAffectedPartitions(keyedInputRecordRDD.keys(), jsc, hoodieTable,\n+        config.getSimpleIndexParallelism());\n+\n+    JavaRDD<HoodieRecord<T>> taggedRecordRDD = keyedInputRecordRDD.leftOuterJoin(existingLocationsOnTable)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDE0NDk4OQ==", "bodyText": "if we have had the other fetcher, we could avoid this mapToPair right. Can you help me understand why you have removed it ? I actually started with this solution and decided to create a separate RecordFetcher thinking we could avoid an extra map call.", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424144989", "createdAt": "2020-05-13T02:56:07Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/simple/HoodieGlobalSimpleIndex.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.simple;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.EmptyHoodieRecordPayload;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.index.HoodieIndexUtils;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import scala.Tuple2;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import static org.apache.hudi.index.HoodieIndexUtils.getLatestBaseFilesForAllPartitions;\n+\n+/**\n+ * A global simple index which reads interested fields(record key and partition path) from base files and\n+ * joins with incoming records to find the tagged location.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieGlobalSimpleIndex<T extends HoodieRecordPayload> extends HoodieSimpleIndex<T> {\n+\n+  public HoodieGlobalSimpleIndex(HoodieWriteConfig config) {\n+    super(config);\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> tagLocation(JavaRDD<HoodieRecord<T>> recordRDD, JavaSparkContext jsc,\n+                                              HoodieTable<T> hoodieTable) {\n+    return tagLocationInternal(recordRDD, jsc, hoodieTable);\n+  }\n+\n+  /**\n+   * Tags records location for incoming records.\n+   *\n+   * @param inputRecordRDD   {@link JavaRDD} of incoming records\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} to use\n+   * @return {@link JavaRDD} of records with record locations set\n+   */\n+  protected JavaRDD<HoodieRecord<T>> tagLocationInternal(JavaRDD<HoodieRecord<T>> inputRecordRDD, JavaSparkContext jsc,\n+                                                         HoodieTable<T> hoodieTable) {\n+\n+    JavaPairRDD<String, HoodieRecord<T>> keyedInputRecordRDD = inputRecordRDD.mapToPair(entry -> new Tuple2<>(entry.getRecordKey(), entry));\n+    JavaPairRDD<HoodieKey, HoodieRecordLocation> allRecordLocationsInTable = fetchAllRecordLocations(jsc, hoodieTable,\n+        config.getGlobalSimpleIndexParallelism());\n+    return getTaggedRecords(keyedInputRecordRDD, allRecordLocationsInTable);\n+  }\n+\n+  /**\n+   * Fetch record locations for passed in {@link HoodieKey}s.\n+   *\n+   * @param jsc         instance of {@link JavaSparkContext} to use\n+   * @param hoodieTable instance of {@link HoodieTable} of interest\n+   * @param parallelism parallelism to use\n+   * @return {@link JavaPairRDD} of {@link HoodieKey} and {@link HoodieRecordLocation}\n+   */\n+  protected JavaPairRDD<HoodieKey, HoodieRecordLocation> fetchAllRecordLocations(JavaSparkContext jsc,\n+                                                                                 HoodieTable hoodieTable,\n+                                                                                 int parallelism) {\n+    List<Pair<String, HoodieBaseFile>> latestBaseFiles = getAllBaseFilesInTable(jsc, hoodieTable);\n+    return fetchRecordLocations(jsc, hoodieTable, parallelism, latestBaseFiles);\n+  }\n+\n+  /**\n+   * Load all files for all partitions as <Partition, filename> pair RDD.\n+   */\n+  protected List<Pair<String, HoodieBaseFile>> getAllBaseFilesInTable(final JavaSparkContext jsc, final HoodieTable hoodieTable) {\n+    HoodieTableMetaClient metaClient = hoodieTable.getMetaClient();\n+    try {\n+      List<String> allPartitionPaths = FSUtils.getAllPartitionPaths(metaClient.getFs(), metaClient.getBasePath(), config.shouldAssumeDatePartitioning());\n+      // Obtain the latest data files from all the partitions.\n+      return getLatestBaseFilesForAllPartitions(allPartitionPaths, jsc, hoodieTable);\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Failed to load all partitions\", e);\n+    }\n+  }\n+\n+  /**\n+   * Tag records with right {@link HoodieRecordLocation}.\n+   *\n+   * @param incomingRecords incoming {@link HoodieRecord}s\n+   * @param existingRecords existing records with {@link HoodieRecordLocation}s\n+   * @return {@link JavaRDD} of {@link HoodieRecord}s with tagged {@link HoodieRecordLocation}s\n+   */\n+  private JavaRDD<HoodieRecord<T>> getTaggedRecords(JavaPairRDD<String, HoodieRecord<T>> incomingRecords, JavaPairRDD<HoodieKey, HoodieRecordLocation> existingRecords) {\n+    JavaPairRDD<String, Pair<String, HoodieRecordLocation>> existingRecordByRecordKey = existingRecords\n+        .mapToPair(entry -> new Tuple2<>(entry._1.getRecordKey(), Pair.of(entry._1.getPartitionPath(), entry._2)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 120}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEwODE4MjI0", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-410818224", "createdAt": "2020-05-13T11:07:47Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEwODQ4NTEw", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-410848510", "createdAt": "2020-05-13T11:55:18Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxMTo1NToxOFrOGUuAvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxMTo1NToxOFrOGUuAvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDM3ODU1OQ==", "bodyText": "I thought I will parametrize this test to run for diff indexes and later removed it. missed to remove the constructor.", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424378559", "createdAt": "2020-05-13T11:55:18Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -86,6 +86,9 @@\n \n   private static final Logger LOG = LogManager.getLogger(TestHoodieClientOnCopyOnWriteStorage.class);\n \n+  public TestHoodieClientOnCopyOnWriteStorage() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDAxMDY2NA=="}, "originalCommit": null, "originalPosition": 4}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDExMTU2NjU1", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-411156655", "createdAt": "2020-05-13T17:43:32Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNzo0MzozMlrOGU8pcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xM1QxNzo1MDoxNVrOGU85Uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYxODM1Mg==", "bodyText": "rename to HoodieKeyLocationFetchHandle\nand let's move this to the io module with the other read handles", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424618352", "createdAt": "2020-05-13T17:43:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/main/java/org/apache/hudi/index/simple/HoodieKeyLocationFetcherHandle.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.simple;\n+\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.ParquetUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.io.HoodieReadHandle;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.hadoop.fs.Path;\n+\n+import java.util.Iterator;\n+\n+import scala.Tuple2;\n+\n+/**\n+ * {@link HoodieRecordLocation} fetcher for all records for {@link HoodieBaseFile} of interest.\n+ *\n+ * @param <T>\n+ */\n+public class HoodieKeyLocationFetcherHandle<T extends HoodieRecordPayload> extends HoodieReadHandle<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYyMTYwNA==", "bodyText": "I think we should remove this class and just reuse HoodieKey, which is what we actually read out of the parquet file.. the baseInstantTime and fileId are actually passed in by the caller.. there is no need to actually do this at this level.. We can just reuse the values we pass in at the read handle calling code", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424621604", "createdAt": "2020-05-13T17:48:57Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java", "diffHunk": "@@ -105,6 +121,111 @@\n     return rowKeys;\n   }\n \n+  /**\n+   * Fetch records from the given parquet file as {@link List} of {@link ParquetRowInfo}.\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return {@link List} of {@link ParquetRowInfo}\n+   */\n+  public static List<ParquetRowInfo> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                            String baseInstantTime,\n+                                                                            String fileId) {\n+    return fetchRecordKeyPartitionPathFromParquet(configuration, filePath, baseInstantTime, fileId, Collections.EMPTY_LIST);\n+  }\n+\n+  /**\n+   * Fetch records from the given parquet file as {@link List} of {@link ParquetRowInfo}. Filter for candidates if set.\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return {@link List} of {@link ParquetRowInfo} matching candidateRecordKeys\n+   */\n+  static List<ParquetRowInfo> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                     String baseInstantTime,\n+                                                                     String fileId, List<String> candidatesToFilter) {\n+    List<ParquetRowInfo> parquetRowInfos = new ArrayList<>();\n+    try {\n+      if (!filePath.getFileSystem(configuration).exists(filePath)) {\n+        return new ArrayList<>();\n+      }\n+\n+      Configuration conf = new Configuration(configuration);\n+      conf.addResource(FSUtils.getFs(filePath.toString(), conf).getConf());\n+      Schema readSchema = HoodieAvroUtils.getRecordKeyPartitionPathSchema();\n+      AvroReadSupport.setAvroReadSchema(conf, readSchema);\n+      AvroReadSupport.setRequestedProjection(conf, readSchema);\n+      ParquetReader reader = AvroParquetReader.builder(filePath).withConf(conf).build();\n+      Object obj = reader.read();\n+      while (obj != null) {\n+        if (obj instanceof GenericRecord) {\n+          String recordKey = ((GenericRecord) obj).get(HoodieRecord.RECORD_KEY_METADATA_FIELD).toString();\n+          String partitionPath = ((GenericRecord) obj).get(HoodieRecord.PARTITION_PATH_METADATA_FIELD).toString();\n+          if (candidatesToFilter.isEmpty() || candidatesToFilter.contains(recordKey)) {\n+            parquetRowInfos.add(new ParquetRowInfo(recordKey, partitionPath, baseInstantTime, fileId));\n+          }\n+          obj = reader.read();\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new HoodieIOException(\"Failed to read from Parquet file \" + filePath, e);\n+    }\n+    return parquetRowInfos;\n+  }\n+\n+  /**\n+   * Class to hold info for a row in Parquet like record key, partition path, base instance time and fileId.\n+   */\n+  public static class ParquetRowInfo {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYyMjE5MA==", "bodyText": "we don't intend on doing an filtering right.. i.e this is never called? if so, can we just have the method above?", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424622190", "createdAt": "2020-05-13T17:49:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/ParquetUtils.java", "diffHunk": "@@ -105,6 +121,111 @@\n     return rowKeys;\n   }\n \n+  /**\n+   * Fetch records from the given parquet file as {@link List} of {@link ParquetRowInfo}.\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return {@link List} of {@link ParquetRowInfo}\n+   */\n+  public static List<ParquetRowInfo> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                            String baseInstantTime,\n+                                                                            String fileId) {\n+    return fetchRecordKeyPartitionPathFromParquet(configuration, filePath, baseInstantTime, fileId, Collections.EMPTY_LIST);\n+  }\n+\n+  /**\n+   * Fetch records from the given parquet file as {@link List} of {@link ParquetRowInfo}. Filter for candidates if set.\n+   *\n+   * @param filePath      The parquet file path.\n+   * @param configuration configuration to build fs object\n+   * @return {@link List} of {@link ParquetRowInfo} matching candidateRecordKeys\n+   */\n+  static List<ParquetRowInfo> fetchRecordKeyPartitionPathFromParquet(Configuration configuration, Path filePath,\n+                                                                     String baseInstantTime,\n+                                                                     String fileId, List<String> candidatesToFilter) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDYyMjQxOQ==", "bodyText": "this test also needs to move", "url": "https://github.com/apache/hudi/pull/1402#discussion_r424622419", "createdAt": "2020-05-13T17:50:15Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/src/test/java/org/apache/hudi/index/simple/TestHoodieKeyLocationFetcherHandle.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.index.simple;\n+\n+import org.apache.hudi.common.HoodieClientTestHarness;\n+import org.apache.hudi.common.HoodieClientTestUtils;\n+import org.apache.hudi.common.HoodieTestDataGenerator;\n+import org.apache.hudi.common.TestRawTripPayload;\n+import org.apache.hudi.common.fs.ConsistencyGuardConfig;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordLocation;\n+import org.apache.hudi.common.model.HoodieTestUtils;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.view.FileSystemViewStorageConfig;\n+import org.apache.hudi.common.table.view.FileSystemViewStorageType;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieCompactionConfig;\n+import org.apache.hudi.config.HoodieIndexConfig;\n+import org.apache.hudi.config.HoodieStorageConfig;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.index.HoodieIndexUtils;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+\n+import scala.Tuple2;\n+\n+import static java.util.stream.Collectors.toList;\n+import static org.apache.hudi.common.HoodieTestDataGenerator.AVRO_SCHEMA_WITH_METADATA_FIELDS;\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+\n+/**\n+ * Tests {@link HoodieKeyLocationFetcherHandle}.\n+ */\n+public class TestHoodieKeyLocationFetcherHandle extends HoodieClientTestHarness {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEyNzMxMjQ3", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-412731247", "createdAt": "2020-05-15T15:06:10Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestCommit", "commit": {"oid": "62f8b4c6193b06c7df32381c00082e2f0762a74e", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/62f8b4c6193b06c7df32381c00082e2f0762a74e", "committedDate": "2020-05-16T12:37:34Z", "message": "[HUDI-407] Adding Simple Index to Hoodie. This index finds the location by joining incoming records with records from base files."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "62f8b4c6193b06c7df32381c00082e2f0762a74e", "author": {"user": {"login": "nsivabalan", "name": "Sivabalan Narayanan"}}, "url": "https://github.com/apache/hudi/commit/62f8b4c6193b06c7df32381c00082e2f0762a74e", "committedDate": "2020-05-16T12:37:34Z", "message": "[HUDI-407] Adding Simple Index to Hoodie. This index finds the location by joining incoming records with records from base files."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzMjI1ODI5", "url": "https://github.com/apache/hudi/pull/1402#pullrequestreview-413225829", "createdAt": "2020-05-18T01:31:08Z", "commit": {"oid": "62f8b4c6193b06c7df32381c00082e2f0762a74e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3862, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}