{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQxNzgwODEw", "number": 1774, "reviewThreads": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMTozOToyMlrOEOpVRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxMToxNjoyOFrOERZ7ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNzkyNzA4OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMTozOToyMlrOGx6NFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMTozOToyMlrOGx6NFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk4NzAyOA==", "bodyText": "Replace == to equal method", "url": "https://github.com/apache/hudi/pull/1774#discussion_r454987028", "createdAt": "2020-07-15T11:39:22Z", "author": {"login": "yanghua"}, "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ;\n+\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Base class to run cmd and generate data in hive.\n+ */\n+public class HoodieTestHiveBase extends ITTestBase {\n+\n+  protected enum PartitionType {\n+    SINGLE_KEY_PARTITIONED, MULTI_KEYS_PARTITIONED, NON_PARTITIONED,\n+  }\n+\n+  /**\n+   * A basic integration test that runs HoodieJavaApp to create a sample Hoodie data-set and performs upserts on it.\n+   * Hive integration and upsert functionality is checked by running a count query in hive console. TODO: Add\n+   * spark-shell test-case\n+   */\n+  public void generateDataByHoodieJavaApp(String hiveTableName, String tableType, PartitionType partitionType,\n+      String commitType, String hoodieTableName) throws Exception {\n+\n+    String hdfsPath = getHdfsPath(hiveTableName);\n+    String hdfsUrl = \"hdfs://namenode\" + hdfsPath;\n+\n+    // Drop Table if it exists\n+    try {\n+      dropHiveTables(hiveTableName, tableType);\n+    } catch (AssertionError ex) {\n+      // In travis, sometimes, the hivemetastore is not ready even though we wait for the port to be up\n+      // Workaround to sleep for 5 secs and retry\n+      Thread.sleep(5000);\n+      dropHiveTables(hiveTableName, tableType);\n+    }\n+\n+    // Ensure table does not exist\n+    Pair<String, String> stdOutErr = null;\n+    if (commitType == \"overwrite\") {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNzkyNzc0OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMTozOTozOFrOGx6New==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMTozOTozOFrOGx6New==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk4NzEzMQ==", "bodyText": "Let's make it configurable?", "url": "https://github.com/apache/hudi/pull/1774#discussion_r454987131", "createdAt": "2020-07-15T11:39:38Z", "author": {"login": "yanghua"}, "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ;\n+\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Base class to run cmd and generate data in hive.\n+ */\n+public class HoodieTestHiveBase extends ITTestBase {\n+\n+  protected enum PartitionType {\n+    SINGLE_KEY_PARTITIONED, MULTI_KEYS_PARTITIONED, NON_PARTITIONED,\n+  }\n+\n+  /**\n+   * A basic integration test that runs HoodieJavaApp to create a sample Hoodie data-set and performs upserts on it.\n+   * Hive integration and upsert functionality is checked by running a count query in hive console. TODO: Add\n+   * spark-shell test-case\n+   */\n+  public void generateDataByHoodieJavaApp(String hiveTableName, String tableType, PartitionType partitionType,\n+      String commitType, String hoodieTableName) throws Exception {\n+\n+    String hdfsPath = getHdfsPath(hiveTableName);\n+    String hdfsUrl = \"hdfs://namenode\" + hdfsPath;\n+\n+    // Drop Table if it exists\n+    try {\n+      dropHiveTables(hiveTableName, tableType);\n+    } catch (AssertionError ex) {\n+      // In travis, sometimes, the hivemetastore is not ready even though we wait for the port to be up\n+      // Workaround to sleep for 5 secs and retry\n+      Thread.sleep(5000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNzkyOTA2OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMTozOTo1OVrOGx6OMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMTozOTo1OVrOGx6OMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk4NzMxNQ==", "bodyText": "Let's use String.format()?", "url": "https://github.com/apache/hudi/pull/1774#discussion_r454987315", "createdAt": "2020-07-15T11:39:59Z", "author": {"login": "yanghua"}, "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ;\n+\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Base class to run cmd and generate data in hive.\n+ */\n+public class HoodieTestHiveBase extends ITTestBase {\n+\n+  protected enum PartitionType {\n+    SINGLE_KEY_PARTITIONED, MULTI_KEYS_PARTITIONED, NON_PARTITIONED,\n+  }\n+\n+  /**\n+   * A basic integration test that runs HoodieJavaApp to create a sample Hoodie data-set and performs upserts on it.\n+   * Hive integration and upsert functionality is checked by running a count query in hive console. TODO: Add\n+   * spark-shell test-case\n+   */\n+  public void generateDataByHoodieJavaApp(String hiveTableName, String tableType, PartitionType partitionType,\n+      String commitType, String hoodieTableName) throws Exception {\n+\n+    String hdfsPath = getHdfsPath(hiveTableName);\n+    String hdfsUrl = \"hdfs://namenode\" + hdfsPath;\n+\n+    // Drop Table if it exists\n+    try {\n+      dropHiveTables(hiveTableName, tableType);\n+    } catch (AssertionError ex) {\n+      // In travis, sometimes, the hivemetastore is not ready even though we wait for the port to be up\n+      // Workaround to sleep for 5 secs and retry\n+      Thread.sleep(5000);\n+      dropHiveTables(hiveTableName, tableType);\n+    }\n+\n+    // Ensure table does not exist\n+    Pair<String, String> stdOutErr = null;\n+    if (commitType == \"overwrite\") {\n+      stdOutErr = executeHiveCommand(\"show tables like '\" + hiveTableName + \"'\");\n+      assertTrue(stdOutErr.getLeft().isEmpty(), \"Dropped table \" + hiveTableName + \" exists!\");\n+    }\n+\n+    // Run Hoodie Java App\n+    String cmd = HOODIE_JAVA_APP + \" HoodieJavaGenerateApp --hive-sync --table-path \" + hdfsUrl + \" --hive-url \" + HIVE_SERVER_JDBC_URL", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzNzkzMjQ5OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMTo0MTowMFrOGx6QSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMTo0MTowMFrOGx6QSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk4Nzg1MA==", "bodyText": "No need to initialize it to be null.", "url": "https://github.com/apache/hudi/pull/1774#discussion_r454987850", "createdAt": "2020-07-15T11:41:00Z", "author": {"login": "yanghua"}, "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ;\n+\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Base class to run cmd and generate data in hive.\n+ */\n+public class HoodieTestHiveBase extends ITTestBase {\n+\n+  protected enum PartitionType {\n+    SINGLE_KEY_PARTITIONED, MULTI_KEYS_PARTITIONED, NON_PARTITIONED,\n+  }\n+\n+  /**\n+   * A basic integration test that runs HoodieJavaApp to create a sample Hoodie data-set and performs upserts on it.\n+   * Hive integration and upsert functionality is checked by running a count query in hive console. TODO: Add\n+   * spark-shell test-case\n+   */\n+  public void generateDataByHoodieJavaApp(String hiveTableName, String tableType, PartitionType partitionType,\n+      String commitType, String hoodieTableName) throws Exception {\n+\n+    String hdfsPath = getHdfsPath(hiveTableName);\n+    String hdfsUrl = \"hdfs://namenode\" + hdfsPath;\n+\n+    // Drop Table if it exists\n+    try {\n+      dropHiveTables(hiveTableName, tableType);\n+    } catch (AssertionError ex) {\n+      // In travis, sometimes, the hivemetastore is not ready even though we wait for the port to be up\n+      // Workaround to sleep for 5 secs and retry\n+      Thread.sleep(5000);\n+      dropHiveTables(hiveTableName, tableType);\n+    }\n+\n+    // Ensure table does not exist\n+    Pair<String, String> stdOutErr = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MzI5MDY2OnYy", "diffSide": "RIGHT", "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieSyncCommand.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxMDo1NzoyMFrOG0GFCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxMDo1NzoyMFrOG0GFCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzI3ODcyOQ==", "bodyText": "Good catch!", "url": "https://github.com/apache/hudi/pull/1774#discussion_r457278729", "createdAt": "2020-07-20T10:57:20Z", "author": {"login": "yanghua"}, "path": "hudi-cli/src/main/java/org/apache/hudi/cli/commands/HoodieSyncCommand.java", "diffHunk": "@@ -74,9 +74,9 @@ public String validateSync(\n     }\n \n     String targetLatestCommit =\n-        targetTimeline.getInstants().iterator().hasNext() ? \"0\" : targetTimeline.lastInstant().get().getTimestamp();\n+        targetTimeline.getInstants().iterator().hasNext() ? targetTimeline.lastInstant().get().getTimestamp() : \"0\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92f36ae11a8bb52e5e068cf565da95fbd6f29906"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MzMxMjgwOnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxMTowMjo1OFrOG0GSeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxMTowMjo1OFrOG0GSeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzI4MjE3MA==", "bodyText": "Can we define a default constant for this?", "url": "https://github.com/apache/hudi/pull/1774#discussion_r457282170", "createdAt": "2020-07-20T11:02:58Z", "author": {"login": "yanghua"}, "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ;\n+\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Base class to run cmd and generate data in hive.\n+ */\n+public class HoodieTestHiveBase extends ITTestBase {\n+\n+  protected enum PartitionType {\n+    SINGLE_KEY_PARTITIONED, MULTI_KEYS_PARTITIONED, NON_PARTITIONED,\n+  }\n+\n+  /**\n+   * A basic integration test that runs HoodieJavaApp to create a sample Hoodie data-set and performs upserts on it.\n+   * Hive integration and upsert functionality is checked by running a count query in hive console. TODO: Add\n+   * spark-shell test-case\n+   */\n+  public void generateDataByHoodieJavaApp(String hiveTableName, String tableType, PartitionType partitionType,\n+      String commitType, String hoodieTableName) throws Exception {\n+\n+    String hdfsPath = getHdfsPath(hiveTableName);\n+    String hdfsUrl = \"hdfs://namenode\" + hdfsPath;\n+\n+    Pair<String, String> stdOutErr;\n+    if (\"overwrite\".equals(commitType)) {\n+      // Drop Table if it exists\n+      try {\n+        dropHiveTables(hiveTableName, tableType);\n+      } catch (AssertionError ex) {\n+        // In travis, sometimes, the hivemetastore is not ready even though we wait for the port to be up\n+        // Workaround to sleep for 5 secs and retry\n+        // Set sleep time by hoodie.hiveserver.time.wait\n+        Thread.sleep(getTimeWait());\n+        dropHiveTables(hiveTableName, tableType);\n+      }\n+\n+      // Ensure table does not exist\n+      stdOutErr = executeHiveCommand(\"show tables like '\" + hiveTableName + \"'\");\n+      assertTrue(stdOutErr.getLeft().isEmpty(), \"Dropped table \" + hiveTableName + \" exists!\");\n+    }\n+\n+    // Run Hoodie Java App\n+    String cmd = String.format(\"%s %s --hive-sync --table-path %s  --hive-url %s  --table-type %s  --hive-table %s\" +\n+        \" --commit-type %s  --table-name %s\", HOODIE_JAVA_APP, \"HoodieJavaGenerateApp\", hdfsUrl, HIVE_SERVER_JDBC_URL,\n+        tableType, hiveTableName, commitType, hoodieTableName);\n+    if (partitionType == PartitionType.MULTI_KEYS_PARTITIONED) {\n+      cmd = cmd + \" --use-multi-partition-keys\";\n+    } else if (partitionType == PartitionType.NON_PARTITIONED){\n+      cmd = cmd + \" --non-partitioned\";\n+    }\n+    executeCommandStringInDocker(ADHOC_1_CONTAINER, cmd, true);\n+\n+    String snapshotTableName = getSnapshotTableName(tableType, hiveTableName);\n+\n+    // Ensure table does exist\n+    stdOutErr = executeHiveCommand(\"show tables like '\" + snapshotTableName + \"'\");\n+    assertEquals(snapshotTableName, stdOutErr.getLeft(), \"Table exists\");\n+  }\n+\n+  protected void dropHiveTables(String hiveTableName, String tableType) throws Exception {\n+    if (tableType.equals(HoodieTableType.MERGE_ON_READ.name())) {\n+      executeHiveCommand(\"drop table if exists \" + hiveTableName + \"_rt\");\n+      executeHiveCommand(\"drop table if exists \" + hiveTableName + \"_ro\");\n+    } else {\n+      executeHiveCommand(\"drop table if exists \" + hiveTableName);\n+    }\n+  }\n+\n+  protected String getHdfsPath(String hiveTableName) {\n+    return \"/\" + hiveTableName;\n+  }\n+\n+  protected String getSnapshotTableName(String tableType, String hiveTableName) {\n+    return tableType.equals(HoodieTableType.MERGE_ON_READ.name())\n+        ? hiveTableName + \"_rt\" : hiveTableName;\n+  }\n+\n+  private int getTimeWait() {\n+    int timeWait = 5000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92f36ae11a8bb52e5e068cf565da95fbd6f29906"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MzMxOTEzOnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxMTowNDozNVrOG0GWPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxMTowNDozNVrOG0GWPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzI4MzEzMg==", "bodyText": "I would not suggest using assertation in normal methods. We can throw an exception directly. WDYT?", "url": "https://github.com/apache/hudi/pull/1774#discussion_r457283132", "createdAt": "2020-07-20T11:04:35Z", "author": {"login": "yanghua"}, "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ;\n+\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Base class to run cmd and generate data in hive.\n+ */\n+public class HoodieTestHiveBase extends ITTestBase {\n+\n+  protected enum PartitionType {\n+    SINGLE_KEY_PARTITIONED, MULTI_KEYS_PARTITIONED, NON_PARTITIONED,\n+  }\n+\n+  /**\n+   * A basic integration test that runs HoodieJavaApp to create a sample Hoodie data-set and performs upserts on it.\n+   * Hive integration and upsert functionality is checked by running a count query in hive console. TODO: Add\n+   * spark-shell test-case\n+   */\n+  public void generateDataByHoodieJavaApp(String hiveTableName, String tableType, PartitionType partitionType,\n+      String commitType, String hoodieTableName) throws Exception {\n+\n+    String hdfsPath = getHdfsPath(hiveTableName);\n+    String hdfsUrl = \"hdfs://namenode\" + hdfsPath;\n+\n+    Pair<String, String> stdOutErr;\n+    if (\"overwrite\".equals(commitType)) {\n+      // Drop Table if it exists\n+      try {\n+        dropHiveTables(hiveTableName, tableType);\n+      } catch (AssertionError ex) {\n+        // In travis, sometimes, the hivemetastore is not ready even though we wait for the port to be up\n+        // Workaround to sleep for 5 secs and retry\n+        // Set sleep time by hoodie.hiveserver.time.wait\n+        Thread.sleep(getTimeWait());\n+        dropHiveTables(hiveTableName, tableType);\n+      }\n+\n+      // Ensure table does not exist\n+      stdOutErr = executeHiveCommand(\"show tables like '\" + hiveTableName + \"'\");\n+      assertTrue(stdOutErr.getLeft().isEmpty(), \"Dropped table \" + hiveTableName + \" exists!\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92f36ae11a8bb52e5e068cf565da95fbd6f29906"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MzMyNTk1OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxMTowNjoxMFrOG0GZzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxMTowNjoxMFrOG0GZzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzI4NDA0Nw==", "bodyText": "Can we define a constant for multiple usages? IMO, hard code is not a good practice.", "url": "https://github.com/apache/hudi/pull/1774#discussion_r457284047", "createdAt": "2020-07-20T11:06:10Z", "author": {"login": "yanghua"}, "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ;\n+\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Base class to run cmd and generate data in hive.\n+ */\n+public class HoodieTestHiveBase extends ITTestBase {\n+\n+  protected enum PartitionType {\n+    SINGLE_KEY_PARTITIONED, MULTI_KEYS_PARTITIONED, NON_PARTITIONED,\n+  }\n+\n+  /**\n+   * A basic integration test that runs HoodieJavaApp to create a sample Hoodie data-set and performs upserts on it.\n+   * Hive integration and upsert functionality is checked by running a count query in hive console. TODO: Add\n+   * spark-shell test-case\n+   */\n+  public void generateDataByHoodieJavaApp(String hiveTableName, String tableType, PartitionType partitionType,\n+      String commitType, String hoodieTableName) throws Exception {\n+\n+    String hdfsPath = getHdfsPath(hiveTableName);\n+    String hdfsUrl = \"hdfs://namenode\" + hdfsPath;\n+\n+    Pair<String, String> stdOutErr;\n+    if (\"overwrite\".equals(commitType)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92f36ae11a8bb52e5e068cf565da95fbd6f29906"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MzMzODg0OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxMTowOToxOVrOG0GhBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxMTowOToxOVrOG0GhBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzI4NTg5Mw==", "bodyText": "getHDFSPath looks better?", "url": "https://github.com/apache/hudi/pull/1774#discussion_r457285893", "createdAt": "2020-07-20T11:09:19Z", "author": {"login": "yanghua"}, "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/HoodieTestHiveBase.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ;\n+\n+import org.apache.hudi.common.config.TypedProperties;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.util.collection.Pair;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+import static org.junit.jupiter.api.Assertions.assertEquals;\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Base class to run cmd and generate data in hive.\n+ */\n+public class HoodieTestHiveBase extends ITTestBase {\n+\n+  protected enum PartitionType {\n+    SINGLE_KEY_PARTITIONED, MULTI_KEYS_PARTITIONED, NON_PARTITIONED,\n+  }\n+\n+  /**\n+   * A basic integration test that runs HoodieJavaApp to create a sample Hoodie data-set and performs upserts on it.\n+   * Hive integration and upsert functionality is checked by running a count query in hive console. TODO: Add\n+   * spark-shell test-case\n+   */\n+  public void generateDataByHoodieJavaApp(String hiveTableName, String tableType, PartitionType partitionType,\n+      String commitType, String hoodieTableName) throws Exception {\n+\n+    String hdfsPath = getHdfsPath(hiveTableName);\n+    String hdfsUrl = \"hdfs://namenode\" + hdfsPath;\n+\n+    Pair<String, String> stdOutErr;\n+    if (\"overwrite\".equals(commitType)) {\n+      // Drop Table if it exists\n+      try {\n+        dropHiveTables(hiveTableName, tableType);\n+      } catch (AssertionError ex) {\n+        // In travis, sometimes, the hivemetastore is not ready even though we wait for the port to be up\n+        // Workaround to sleep for 5 secs and retry\n+        // Set sleep time by hoodie.hiveserver.time.wait\n+        Thread.sleep(getTimeWait());\n+        dropHiveTables(hiveTableName, tableType);\n+      }\n+\n+      // Ensure table does not exist\n+      stdOutErr = executeHiveCommand(\"show tables like '\" + hiveTableName + \"'\");\n+      assertTrue(stdOutErr.getLeft().isEmpty(), \"Dropped table \" + hiveTableName + \" exists!\");\n+    }\n+\n+    // Run Hoodie Java App\n+    String cmd = String.format(\"%s %s --hive-sync --table-path %s  --hive-url %s  --table-type %s  --hive-table %s\" +\n+        \" --commit-type %s  --table-name %s\", HOODIE_JAVA_APP, \"HoodieJavaGenerateApp\", hdfsUrl, HIVE_SERVER_JDBC_URL,\n+        tableType, hiveTableName, commitType, hoodieTableName);\n+    if (partitionType == PartitionType.MULTI_KEYS_PARTITIONED) {\n+      cmd = cmd + \" --use-multi-partition-keys\";\n+    } else if (partitionType == PartitionType.NON_PARTITIONED){\n+      cmd = cmd + \" --non-partitioned\";\n+    }\n+    executeCommandStringInDocker(ADHOC_1_CONTAINER, cmd, true);\n+\n+    String snapshotTableName = getSnapshotTableName(tableType, hiveTableName);\n+\n+    // Ensure table does exist\n+    stdOutErr = executeHiveCommand(\"show tables like '\" + snapshotTableName + \"'\");\n+    assertEquals(snapshotTableName, stdOutErr.getLeft(), \"Table exists\");\n+  }\n+\n+  protected void dropHiveTables(String hiveTableName, String tableType) throws Exception {\n+    if (tableType.equals(HoodieTableType.MERGE_ON_READ.name())) {\n+      executeHiveCommand(\"drop table if exists \" + hiveTableName + \"_rt\");\n+      executeHiveCommand(\"drop table if exists \" + hiveTableName + \"_ro\");\n+    } else {\n+      executeHiveCommand(\"drop table if exists \" + hiveTableName);\n+    }\n+  }\n+\n+  protected String getHdfsPath(String hiveTableName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92f36ae11a8bb52e5e068cf565da95fbd6f29906"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MzM1MTk3OnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/command/ITTestHoodieSyncCommand.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxMToxMjoxN1rOG0GoNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxMToxMjoxN1rOG0GoNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzI4NzczMw==", "bodyText": "Using StringBuilder  to build cmd is a better choice?", "url": "https://github.com/apache/hudi/pull/1774#discussion_r457287733", "createdAt": "2020-07-20T11:12:17Z", "author": {"login": "yanghua"}, "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/command/ITTestHoodieSyncCommand.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.integ.command;\n+\n+import org.apache.hudi.common.model.HoodieFileFormat;\n+import org.apache.hudi.common.model.HoodieTableType;\n+\n+import org.apache.hudi.integ.HoodieTestHiveBase;\n+\n+import org.junit.jupiter.api.Test;\n+\n+import static org.junit.jupiter.api.Assertions.assertTrue;\n+\n+/**\n+ * Integration test class for HoodieSyncCommand in hudi-cli module.\n+ */\n+public class ITTestHoodieSyncCommand extends HoodieTestHiveBase {\n+\n+  private static final String HUDI_CLI_TOOL = HOODIE_WS_ROOT + \"/hudi-cli/hudi-cli.sh\";\n+  private static final String SYNC_VALIDATE_COMMANDS = HOODIE_WS_ROOT + \"/docker/demo/sync-validate.commands\";\n+\n+  @Test\n+  public void testValidateSync() throws Exception {\n+    String hiveTableName = \"docker_hoodie_sync_valid_test\";\n+    String hiveTableName2 = \"docker_hoodie_sync_valid_test_2\";\n+\n+    generateDataByHoodieJavaApp(\n+        hiveTableName, HoodieTableType.COPY_ON_WRITE.name(), PartitionType.SINGLE_KEY_PARTITIONED, \"overwrite\", hiveTableName);\n+\n+    syncHoodieTable(hiveTableName2, \"INSERT\");\n+\n+    generateDataByHoodieJavaApp(\n+        hiveTableName, HoodieTableType.COPY_ON_WRITE.name(), PartitionType.SINGLE_KEY_PARTITIONED, \"append\", hiveTableName);\n+\n+    TestExecStartResultCallback result =\n+        executeCommandStringInDocker(ADHOC_1_CONTAINER, HUDI_CLI_TOOL + \" --cmdfile \" + SYNC_VALIDATE_COMMANDS, true);\n+\n+    String expected = String.format(\"Count difference now is (count(%s) - count(%s) == %d. Catch up count is %d\",\n+        hiveTableName, hiveTableName2, 100, 200);\n+    assertTrue(result.getStderr().toString().contains(expected));\n+\n+    dropHiveTables(hiveTableName, HoodieTableType.COPY_ON_WRITE.name());\n+    dropHiveTables(hiveTableName2, HoodieTableType.COPY_ON_WRITE.name());\n+  }\n+\n+  private void syncHoodieTable(String hiveTableName, String op) throws Exception {\n+    String cmd = \"spark-submit --packages org.apache.spark:spark-avro_2.11:2.4.4 \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92f36ae11a8bb52e5e068cf565da95fbd6f29906"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1MzM2MDQ3OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/test/java/HoodieJavaGenerateApp.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxMToxNDozMVrOG0GtQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQxMToxNDozMVrOG0GtQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzI4OTAyNw==", "bodyText": "HoodieJavaGenerateApp?", "url": "https://github.com/apache/hudi/pull/1774#discussion_r457289027", "createdAt": "2020-07-20T11:14:31Z", "author": {"login": "yanghua"}, "path": "hudi-spark/src/test/java/HoodieJavaGenerateApp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.HoodieDataSourceHelpers;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.hive.MultiPartKeysValueExtractor;\n+import org.apache.hudi.hive.NonPartitionedExtractor;\n+import org.apache.hudi.keygen.NonpartitionedKeyGenerator;\n+import org.apache.hudi.keygen.SimpleKeyGenerator;\n+import org.apache.hudi.testutils.DataSourceTestUtils;\n+import org.apache.hudi.testutils.HoodieTestDataGenerator;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieJavaGenerateApp {\n+  @Parameter(names = {\"--table-path\", \"-p\"}, description = \"path for Hoodie sample table\")\n+  private String tablePath = \"file:///tmp/hoodie/sample-table\";\n+\n+  @Parameter(names = {\"--table-name\", \"-n\"}, description = \"table name for Hoodie sample table\")\n+  private String tableName = \"hoodie_test\";\n+\n+  @Parameter(names = {\"--table-type\", \"-t\"}, description = \"One of COPY_ON_WRITE or MERGE_ON_READ\")\n+  private String tableType = HoodieTableType.COPY_ON_WRITE.name();\n+\n+  @Parameter(names = {\"--hive-sync\", \"-hv\"}, description = \"Enable syncing to hive\")\n+  private Boolean enableHiveSync = false;\n+\n+  @Parameter(names = {\"--hive-db\", \"-hd\"}, description = \"hive database\")\n+  private String hiveDB = \"default\";\n+\n+  @Parameter(names = {\"--hive-table\", \"-ht\"}, description = \"hive table\")\n+  private String hiveTable = \"hoodie_sample_test\";\n+\n+  @Parameter(names = {\"--hive-user\", \"-hu\"}, description = \"hive username\")\n+  private String hiveUser = \"hive\";\n+\n+  @Parameter(names = {\"--hive-password\", \"-hp\"}, description = \"hive password\")\n+  private String hivePass = \"hive\";\n+\n+  @Parameter(names = {\"--hive-url\", \"-hl\"}, description = \"hive JDBC URL\")\n+  private String hiveJdbcUrl = \"jdbc:hive2://localhost:10000\";\n+\n+  @Parameter(names = {\"--non-partitioned\", \"-np\"}, description = \"Use non-partitioned Table\")\n+  private Boolean nonPartitionedTable = false;\n+\n+  @Parameter(names = {\"--use-multi-partition-keys\", \"-mp\"}, description = \"Use Multiple Partition Keys\")\n+  private Boolean useMultiPartitionKeys = false;\n+\n+  @Parameter(names = {\"--commit-type\", \"-ct\"}, description = \"How may commits will run\")\n+  private String commitType = \"overwrite\";\n+\n+  @Parameter(names = {\"--help\", \"-h\"}, help = true)\n+  public Boolean help = false;\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieJavaApp.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "92f36ae11a8bb52e5e068cf565da95fbd6f29906"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2Njg1NzA2OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/test/java/HoodieJavaGenerateApp.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxMToxNTowMlrOG2GFuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxMToxNTowMlrOG2GFuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTM3NjA1Nw==", "bodyText": "Why it is not -hs? Does it cause conflicts?", "url": "https://github.com/apache/hudi/pull/1774#discussion_r459376057", "createdAt": "2020-07-23T11:15:02Z", "author": {"login": "yanghua"}, "path": "hudi-spark/src/test/java/HoodieJavaGenerateApp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.HoodieDataSourceHelpers;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.hive.MultiPartKeysValueExtractor;\n+import org.apache.hudi.hive.NonPartitionedExtractor;\n+import org.apache.hudi.keygen.NonpartitionedKeyGenerator;\n+import org.apache.hudi.keygen.SimpleKeyGenerator;\n+import org.apache.hudi.testutils.DataSourceTestUtils;\n+import org.apache.hudi.testutils.HoodieTestDataGenerator;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieJavaGenerateApp {\n+  @Parameter(names = {\"--table-path\", \"-p\"}, description = \"path for Hoodie sample table\")\n+  private String tablePath = \"file:///tmp/hoodie/sample-table\";\n+\n+  @Parameter(names = {\"--table-name\", \"-n\"}, description = \"table name for Hoodie sample table\")\n+  private String tableName = \"hoodie_test\";\n+\n+  @Parameter(names = {\"--table-type\", \"-t\"}, description = \"One of COPY_ON_WRITE or MERGE_ON_READ\")\n+  private String tableType = HoodieTableType.COPY_ON_WRITE.name();\n+\n+  @Parameter(names = {\"--hive-sync\", \"-hv\"}, description = \"Enable syncing to hive\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b9c5ba145ab010706677d962472f4e8e5287974"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2Njg2MTE1OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/test/java/HoodieJavaGenerateApp.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxMToxNjoyOFrOG2GIQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxMToxNjoyOFrOG2GIQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTM3NjcwNw==", "bodyText": "Let's apply a unified rule about the first latter of the description?", "url": "https://github.com/apache/hudi/pull/1774#discussion_r459376707", "createdAt": "2020-07-23T11:16:28Z", "author": {"login": "yanghua"}, "path": "hudi-spark/src/test/java/HoodieJavaGenerateApp.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import com.beust.jcommander.JCommander;\n+import com.beust.jcommander.Parameter;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hudi.DataSourceWriteOptions;\n+import org.apache.hudi.HoodieDataSourceHelpers;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieTableType;\n+import org.apache.hudi.common.table.timeline.HoodieActiveTimeline;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.hive.MultiPartKeysValueExtractor;\n+import org.apache.hudi.hive.NonPartitionedExtractor;\n+import org.apache.hudi.keygen.NonpartitionedKeyGenerator;\n+import org.apache.hudi.keygen.SimpleKeyGenerator;\n+import org.apache.hudi.testutils.DataSourceTestUtils;\n+import org.apache.hudi.testutils.HoodieTestDataGenerator;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.DataFrameWriter;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+public class HoodieJavaGenerateApp {\n+  @Parameter(names = {\"--table-path\", \"-p\"}, description = \"path for Hoodie sample table\")\n+  private String tablePath = \"file:///tmp/hoodie/sample-table\";\n+\n+  @Parameter(names = {\"--table-name\", \"-n\"}, description = \"table name for Hoodie sample table\")\n+  private String tableName = \"hoodie_test\";\n+\n+  @Parameter(names = {\"--table-type\", \"-t\"}, description = \"One of COPY_ON_WRITE or MERGE_ON_READ\")\n+  private String tableType = HoodieTableType.COPY_ON_WRITE.name();\n+\n+  @Parameter(names = {\"--hive-sync\", \"-hv\"}, description = \"Enable syncing to hive\")\n+  private Boolean enableHiveSync = false;\n+\n+  @Parameter(names = {\"--hive-db\", \"-hd\"}, description = \"hive database\")\n+  private String hiveDB = \"default\";\n+\n+  @Parameter(names = {\"--hive-table\", \"-ht\"}, description = \"hive table\")\n+  private String hiveTable = \"hoodie_sample_test\";\n+\n+  @Parameter(names = {\"--hive-user\", \"-hu\"}, description = \"hive username\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1b9c5ba145ab010706677d962472f4e8e5287974"}, "originalPosition": 65}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4520, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}