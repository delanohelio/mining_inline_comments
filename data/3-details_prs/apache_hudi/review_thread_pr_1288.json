{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY4MzEyOTE4", "number": 1288, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNTowNjoxOFrODbc_lA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNTowODowMVrODbdASQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMTEzMTcyOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNTowNjoxOFrOFi8sNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNTowNjoxOFrOFi8sNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE5MDI2MA==", "bodyText": "Instead of Spark driver, lets just say when \"spark retries..\"", "url": "https://github.com/apache/hudi/pull/1288#discussion_r372190260", "createdAt": "2020-01-29T05:06:18Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatWriter.java", "diffHunk": "@@ -256,7 +280,22 @@ private void handleAppendExceptionOrRecoverLease(Path path, RemoteException e)\n         throw new HoodieException(e);\n       }\n     } else {\n-      throw new HoodieIOException(\"Failed to open an append stream \", e);\n+      // When fs.append() has failed and an exception is thrown, by closing the output stream\n+      // we shall force hdfs to release the lease on the log file. When Spark driver retries this task (with", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMTEzMjk5OnYy", "diffSide": "RIGHT", "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNTowNzozM1rOFi8s6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNTowNzozM1rOFi8s6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE5MDQ0Mw==", "bodyText": "Can you add a comment here why we are throwing an exception here", "url": "https://github.com/apache/hudi/pull/1288#discussion_r372190443", "createdAt": "2020-01-29T05:07:33Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieLogFormatWriter.java", "diffHunk": "@@ -256,7 +280,22 @@ private void handleAppendExceptionOrRecoverLease(Path path, RemoteException e)\n         throw new HoodieException(e);\n       }\n     } else {\n-      throw new HoodieIOException(\"Failed to open an append stream \", e);\n+      // When fs.append() has failed and an exception is thrown, by closing the output stream\n+      // we shall force hdfs to release the lease on the log file. When Spark driver retries this task (with\n+      // new attemptId, say taskId.1) it will be able to acquire lease on the log file (as output stream was\n+      // closed properly by taskId.0).\n+      //\n+      // If close() call were to fail throwing an exception, our best bet is to rollover to a new log file.\n+      try {\n+        close();\n+        // output stream has been successfully closed and lease on the log file has been released.\n+        throw new HoodieIOException(\"Failed to append to the output stream \", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMwMTEzMzUzOnYy", "diffSide": "RIGHT", "path": "hudi-common/src/test/java/org/apache/hudi/common/table/log/TestHoodieLogFormat.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNTowODowMVrOFi8tSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yOVQwNTowODowMVrOFi8tSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MjE5MDUzNg==", "bodyText": "nit : s/bloc/block", "url": "https://github.com/apache/hudi/pull/1288#discussion_r372190536", "createdAt": "2020-01-29T05:08:01Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/test/java/org/apache/hudi/common/table/log/TestHoodieLogFormat.java", "diffHunk": "@@ -1113,6 +1113,99 @@ public void testAvroLogRecordReaderWithMixedInsertsCorruptsAndRollback()\n     assertEquals(\"We would read 0 records\", 0, scanner.getTotalLogRecords());\n   }\n \n+  /*\n+   * During a spark stage failure, when the stage is retried, tasks that are part of the previous attempt\n+   * of the stage would continue to run.  As a result two different tasks could be performing the same operation.\n+   * When trying to update the log file, only one of the tasks would succeed (one holding lease on the log file).\n+   *\n+   * In order to make progress in this scenario, second task attempting to update the log file would rollover to\n+   * a new version of the log file.  As a result, we might end up with two log files with same set of data records\n+   * present in both of them.\n+   *\n+   * Following uint tests mimic this scenario to ensure that the reader can handle merging multiple log files with\n+   * duplicate data.\n+   *\n+   */\n+  private void testAvroLogRecordReaderMergingMultipleLogFiles(int numRecordsInLog1, int numRecordsInLog2)\n+      throws IOException, URISyntaxException, InterruptedException {\n+    try {\n+      // Write one Data bloc with same InstantTime (written in same batch)", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 20}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 19, "cost": 1, "resetAt": "2021-11-11T21:28:48Z"}}}