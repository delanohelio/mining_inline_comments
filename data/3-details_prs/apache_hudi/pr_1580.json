{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEyMDI2ODcw", "number": 1580, "title": "[HUDI-852] adding check for table name for Append Save mode ", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nAdding check to validate if the table name in the request is same as the table name in the metadata at the path when Save mode is Append if it is not same then it throws a HoodieException stating that hoodie table table_name already exist at the path\nBrief changelog\n\nModified the HoodieSparkSqlWriter : added check along with some other check for other Save mode in the write path\n\nVerify this pull request\nThis change added tests and can be verified as follows:\n\nAdded a test in HoodieSparkSqlWriterSuite to verify the change.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-05-01T07:49:12Z", "url": "https://github.com/apache/hudi/pull/1580", "merged": true, "mergeCommit": {"oid": "5e0f5e5521c52a08c284a92a3a6a00e34805cce5"}, "closed": true, "closedAt": "2020-05-04T06:09:19Z", "author": {"login": "AakashPradeep"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcc8pI4AH2gAyNDEyMDI2ODcwOmM3MWZiMTExNTgyYTBhZDFiZmZiMDAzNjBmMGE5MTI3NTVlNDE2NmM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABceEKMhAFqTQwNTI1ODIzNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "c71fb111582a0ad1bffb00360f0a912755e4166c", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/c71fb111582a0ad1bffb00360f0a912755e4166c", "committedDate": "2020-05-01T07:35:12Z", "message": "adding check for table name for Append Save mode"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0NTMyMTQx", "url": "https://github.com/apache/hudi/pull/1580#pullrequestreview-404532141", "createdAt": "2020-05-02T14:21:21Z", "commit": {"oid": "c71fb111582a0ad1bffb00360f0a912755e4166c"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQxNDoyMToyMlrOGPjkJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQxNDoyMToyMlrOGPjkJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODk2NDUxOQ==", "bodyText": "I think this check is applicable to both the if and else clause. Can you verify that and move it to may be line 85 (before the if clause) ?", "url": "https://github.com/apache/hudi/pull/1580#discussion_r418964519", "createdAt": "2020-05-02T14:21:22Z", "author": {"login": "bhasudha"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -118,6 +118,12 @@ private[hudi] object HoodieSparkSqlWriter {\n         fs.delete(basePath, true)\n         exists = false\n       }\n+      if (exists && mode == SaveMode.Append) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c71fb111582a0ad1bffb00360f0a912755e4166c"}, "originalPosition": 17}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "679218955ed5488915c360a3096000b48a920952", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/679218955ed5488915c360a3096000b48a920952", "committedDate": "2020-05-03T23:51:31Z", "message": "adding existing table validation for delete and upsert operation"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0NzI1MDE1", "url": "https://github.com/apache/hudi/pull/1580#pullrequestreview-404725015", "createdAt": "2020-05-04T06:08:06Z", "commit": {"oid": "679218955ed5488915c360a3096000b48a920952"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA1MjU4MjM3", "url": "https://github.com/apache/hudi/pull/1580#pullrequestreview-405258237", "createdAt": "2020-05-04T18:54:34Z", "commit": {"oid": "679218955ed5488915c360a3096000b48a920952"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxODo1NDozNFrOGQNs5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNFQxODo1NDozNFrOGQNs5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTY1NDg4NQ==", "bodyText": "actually we could have pushed this check into the guts of hoodie-client.. Every write will initialize a HoodieTableMetaClient anyway.. And have it error out from inside, it will save this extra tableMetaClient initialization..  @bhasudha let's file a follow up, if you agree", "url": "https://github.com/apache/hudi/pull/1580#discussion_r419654885", "createdAt": "2020-05-04T18:54:34Z", "author": {"login": "vinothchandar"}, "path": "hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -83,6 +83,13 @@ private[hudi] object HoodieSparkSqlWriter {\n     val fs = basePath.getFileSystem(sparkContext.hadoopConfiguration)\n     var exists = fs.exists(new Path(basePath, HoodieTableMetaClient.METAFOLDER_NAME))\n \n+    if (exists && mode == SaveMode.Append) {\n+      val existingTableName = new HoodieTableMetaClient(sparkContext.hadoopConfiguration, path.get).getTableConfig.getTableName\n+      if (!existingTableName.equals(tblName.get)) {\n+        throw new HoodieException(s\"hoodie table with name $existingTableName already exist at $basePath\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "679218955ed5488915c360a3096000b48a920952"}, "originalPosition": 20}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3063, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}