{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI3ODkzOTI1", "number": 2283, "reviewThreads": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzoxNTozM1rOFUyLHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQyMzozNjo1OVrOFyvyog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU3MzM3ODg3OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzoxNTozM1rOIc94FQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wNFQxNTo0MjoyNVrOIf5_OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NDgyMQ==", "bodyText": "I am new to these, hence might be wrong. but can you please clarify if this should be \"org.apache.hudi\" ?", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567244821", "createdAt": "2021-01-30T13:15:33Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -377,11 +388,71 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.supportTimestamp = parameters.get(HIVE_SUPPORT_TIMESTAMP).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrdered = StructType(dataCols ++ partCols)\n+    val parts = reOrdered.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDMyNjg0MQ==", "bodyText": "Hi @nsivabalan , we have already put the org.apache.hudi.DefaultSource to the META-INFO.services/org.apache.spark.sql.sources.DataSourceRegister, so it is all right to use the short name.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r570326841", "createdAt": "2021-02-04T15:42:25Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -377,11 +388,71 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.supportTimestamp = parameters.get(HIVE_SUPPORT_TIMESTAMP).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrdered = StructType(dataCols ++ partCols)\n+    val parts = reOrdered.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NDgyMQ=="}, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU3MzM4MzMxOnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzoyMDowNVrOIc96Lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wNFQxNTo0MzoxMVrOIf6Bdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NTM1OQ==", "bodyText": "can we rename to something like schemaParts or schemaSlices", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567245359", "createdAt": "2021-01-30T13:20:05Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -377,11 +388,71 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.supportTimestamp = parameters.get(HIVE_SUPPORT_TIMESTAMP).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrdered = StructType(dataCols ++ partCols)\n+    val parts = reOrdered.json.grouped(threshold).toSeq", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDMyNzQxNA==", "bodyText": "Good suggestions!", "url": "https://github.com/apache/hudi/pull/2283#discussion_r570327414", "createdAt": "2021-02-04T15:43:11Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -377,11 +388,71 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.supportTimestamp = parameters.get(HIVE_SUPPORT_TIMESTAMP).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrdered = StructType(dataCols ++ partCols)\n+    val parts = reOrdered.json.grouped(threshold).toSeq", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NTM1OQ=="}, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU3MzM4NDEwOnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzoyMDo0OFrOIc96jA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wNFQxNTo1NDoxOFrOIf6mHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NTQ1Mg==", "bodyText": "can you please add a comment on why we are doing this rather than setting it as just one param. Also, do link any references so that devs are aware of why we are doing and what all props we may need to set.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567245452", "createdAt": "2021-01-30T13:20:48Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -377,11 +388,71 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.supportTimestamp = parameters.get(HIVE_SUPPORT_TIMESTAMP).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrdered = StructType(dataCols ++ partCols)\n+    val parts = reOrdered.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",\n+      \"spark.sql.sources.schema.numParts\" -> parts.size.toString\n+    )\n+    parts.zipWithIndex.foreach { case (part, index) =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDMzNjc5Ng==", "bodyText": "Thanks for the advice, I will add the reference and add more comments.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r570336796", "createdAt": "2021-02-04T15:54:18Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -377,11 +388,71 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.supportTimestamp = parameters.get(HIVE_SUPPORT_TIMESTAMP).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrdered = StructType(dataCols ++ partCols)\n+    val parts = reOrdered.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",\n+      \"spark.sql.sources.schema.numParts\" -> parts.size.toString\n+    )\n+    parts.zipWithIndex.foreach { case (part, index) =>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NTQ1Mg=="}, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU3MzM5NDgyOnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzozNDozMVrOIc9_YA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0yNFQwMzoxODo0OVrOIqpWIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NjY4OA==", "bodyText": "are we just overriding every time and finally setting pathProp to final value ?", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567246688", "createdAt": "2021-01-30T13:34:31Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -377,11 +388,71 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.supportTimestamp = parameters.get(HIVE_SUPPORT_TIMESTAMP).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrdered = StructType(dataCols ++ partCols)\n+    val parts = reOrdered.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",\n+      \"spark.sql.sources.schema.numParts\" -> parts.size.toString\n+    )\n+    parts.zipWithIndex.foreach { case (part, index) =>\n+      properties += s\"spark.sql.sources.schema.part.$index\" -> part\n+    }\n+    // add partition columns\n+    if (partitionSet.nonEmpty) {\n+      properties += \"spark.sql.sources.schema.numPartCols\" -> partitionSet.size.toString\n+      partitionSet.zipWithIndex.foreach { case (partCol, index) =>\n+        properties += s\"spark.sql.sources.schema.partCol.$index\" -> partCol\n+      }\n+    }\n+    var sqlPropertyText = ConfigUtils.configToString(properties)\n+    sqlPropertyText = if (parameters.containsKey(HIVE_TABLE_PROPERTIES)) {\n+      sqlPropertyText + \"\\n\" + parameters(HIVE_TABLE_PROPERTIES)\n+    } else {\n+      sqlPropertyText\n+    }\n+    parameters + (HIVE_TABLE_PROPERTIES -> sqlPropertyText)\n+  }\n+\n+  private def createSqlTableSerdeProperties(parameters: Map[String, String],\n+                                            basePath: String, pathDepth: Int): String = {\n+    assert(pathDepth >= 0, \"Path Depth must great or equal to 0\")\n+    var pathProp = s\"path=$basePath\"\n+    if (pathProp.endsWith(\"/\")) {\n+      pathProp = pathProp.substring(0, pathProp.length - 1)\n+    }\n+    for (_ <- 0 until pathDepth + 1) {\n+      pathProp = s\"$pathProp/*\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MDIxNTk1OA==", "bodyText": "Yes, we append the \"star\" to  the pathProp and generate the final pathProp value.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r580215958", "createdAt": "2021-02-22T12:35:32Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -377,11 +388,71 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.supportTimestamp = parameters.get(HIVE_SUPPORT_TIMESTAMP).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrdered = StructType(dataCols ++ partCols)\n+    val parts = reOrdered.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",\n+      \"spark.sql.sources.schema.numParts\" -> parts.size.toString\n+    )\n+    parts.zipWithIndex.foreach { case (part, index) =>\n+      properties += s\"spark.sql.sources.schema.part.$index\" -> part\n+    }\n+    // add partition columns\n+    if (partitionSet.nonEmpty) {\n+      properties += \"spark.sql.sources.schema.numPartCols\" -> partitionSet.size.toString\n+      partitionSet.zipWithIndex.foreach { case (partCol, index) =>\n+        properties += s\"spark.sql.sources.schema.partCol.$index\" -> partCol\n+      }\n+    }\n+    var sqlPropertyText = ConfigUtils.configToString(properties)\n+    sqlPropertyText = if (parameters.containsKey(HIVE_TABLE_PROPERTIES)) {\n+      sqlPropertyText + \"\\n\" + parameters(HIVE_TABLE_PROPERTIES)\n+    } else {\n+      sqlPropertyText\n+    }\n+    parameters + (HIVE_TABLE_PROPERTIES -> sqlPropertyText)\n+  }\n+\n+  private def createSqlTableSerdeProperties(parameters: Map[String, String],\n+                                            basePath: String, pathDepth: Int): String = {\n+    assert(pathDepth >= 0, \"Path Depth must great or equal to 0\")\n+    var pathProp = s\"path=$basePath\"\n+    if (pathProp.endsWith(\"/\")) {\n+      pathProp = pathProp.substring(0, pathProp.length - 1)\n+    }\n+    for (_ <- 0 until pathDepth + 1) {\n+      pathProp = s\"$pathProp/*\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NjY4OA=="}, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4MTU4ODUxNA==", "bodyText": "Hi @nsivabalan @vinothchandar @n3nash\nCurrently I append \"stars\" to the path for spark because hudi need the \"stars\" when query hudi table. The number of \"stars\" is the same with the size of partition path fields.\nHowever, there are some problems when we save the path with \"stars\" to the meta store.\n1\u3001If we have not enable the encode for partition value (by URL_ENCODE_PARTITIONING_OPT_KEY), the size of partition fields may not equal to the actually generated partition directory level(e.g. pt = \"2021/02/03\", size of partition fields is 1, but the generated partition directory level is 3). In this case, we cannot know the number of \"stars\" to append to the path for spark in the compile time.\n2\u3001Problems for spark sql on hoodie.\nIf the we save the path contains \"stars\" to the meta store, we will have problems with the Insert and Refresh Table commands. Because Insert and Refresh Table cannot allow \"stars\" in the path.\nSo I think we need to support No Stars Query for hoodie table. After support no stars query, we does not need to append \"stars\" to the path. And all the problems above will be solved.\nI have file a issue to solve this: HUDI-1591", "url": "https://github.com/apache/hudi/pull/2283#discussion_r581588514", "createdAt": "2021-02-24T03:18:49Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -377,11 +388,71 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.supportTimestamp = parameters.get(HIVE_SUPPORT_TIMESTAMP).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrdered = StructType(dataCols ++ partCols)\n+    val parts = reOrdered.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",\n+      \"spark.sql.sources.schema.numParts\" -> parts.size.toString\n+    )\n+    parts.zipWithIndex.foreach { case (part, index) =>\n+      properties += s\"spark.sql.sources.schema.part.$index\" -> part\n+    }\n+    // add partition columns\n+    if (partitionSet.nonEmpty) {\n+      properties += \"spark.sql.sources.schema.numPartCols\" -> partitionSet.size.toString\n+      partitionSet.zipWithIndex.foreach { case (partCol, index) =>\n+        properties += s\"spark.sql.sources.schema.partCol.$index\" -> partCol\n+      }\n+    }\n+    var sqlPropertyText = ConfigUtils.configToString(properties)\n+    sqlPropertyText = if (parameters.containsKey(HIVE_TABLE_PROPERTIES)) {\n+      sqlPropertyText + \"\\n\" + parameters(HIVE_TABLE_PROPERTIES)\n+    } else {\n+      sqlPropertyText\n+    }\n+    parameters + (HIVE_TABLE_PROPERTIES -> sqlPropertyText)\n+  }\n+\n+  private def createSqlTableSerdeProperties(parameters: Map[String, String],\n+                                            basePath: String, pathDepth: Int): String = {\n+    assert(pathDepth >= 0, \"Path Depth must great or equal to 0\")\n+    var pathProp = s\"path=$basePath\"\n+    if (pathProp.endsWith(\"/\")) {\n+      pathProp = pathProp.substring(0, pathProp.length - 1)\n+    }\n+    for (_ <- 0 until pathDepth + 1) {\n+      pathProp = s\"$pathProp/*\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NjY4OA=="}, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU3MzM5NzQ5OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/HoodieSparkSqlWriterSuite.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzozNzoxM1rOIc-Anw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzozNzoxM1rOIc-Anw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NzAwNw==", "bodyText": "Ideally it would be nice to construct the expected output rather than hardcoding. Can we at least use structType to construct the schema parts in this expected output. would be good to avoid hardcoding it.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567247007", "createdAt": "2021-01-30T13:37:13Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/HoodieSparkSqlWriterSuite.scala", "diffHunk": "@@ -397,6 +401,46 @@ class HoodieSparkSqlWriterSuite extends FunSuite with Matchers {\n       }\n     })\n \n+  test(\"Test build sync config for spark sql\") {\n+    initSparkContext(\"test build sync config\")\n+    val addSqlTablePropertiesMethod =\n+        HoodieSparkSqlWriter.getClass.getDeclaredMethod(\"addSqlTableProperties\",\n+          classOf[SQLConf], classOf[StructType], classOf[Map[_,_]])\n+    addSqlTablePropertiesMethod.setAccessible(true)\n+\n+    val schema = DataSourceTestUtils.getStructTypeExampleSchema\n+    val structType = AvroConversionUtils.convertAvroSchemaToStructType(schema)\n+    val basePath = \"/tmp/hoodie_test\"\n+    val params = Map(\n+      \"path\" -> basePath,\n+      DataSourceWriteOptions.TABLE_NAME_OPT_KEY -> \"test_hoodie\",\n+      DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"partition\"\n+    )\n+    val parameters = HoodieWriterUtils.parametersWithWriteDefaults(params)\n+    val newParams = addSqlTablePropertiesMethod.invoke(HoodieSparkSqlWriter,\n+      spark.sessionState.conf, structType, parameters)\n+      .asInstanceOf[Map[String, String]]\n+\n+    val buildSyncConfigMethod =\n+      HoodieSparkSqlWriter.getClass.getDeclaredMethod(\"buildSyncConfig\", classOf[Path],\n+        classOf[Map[_,_]])\n+    buildSyncConfigMethod.setAccessible(true)\n+\n+    val hiveSyncConfig = buildSyncConfigMethod.invoke(HoodieSparkSqlWriter,\n+      new Path(basePath), newParams).asInstanceOf[HiveSyncConfig]\n+\n+    assertResult(\"spark.sql.sources.provider=hudi\\n\" +\n+      \"spark.sql.sources.schema.partCol.0=partition\\n\" +\n+      \"spark.sql.sources.schema.numParts=1\\n\" +\n+      \"spark.sql.sources.schema.numPartCols=1\\n\" +\n+      \"spark.sql.sources.schema.part.0=\" +\n+      \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"_row_key\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":false,\\\"metadata\\\":{}},\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU3MzM5NzcwOnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/HoodieSparkSqlWriterSuite.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzozNzoyN1rOIc-Atg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzozNzoyN1rOIc-Atg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NzAzMA==", "bodyText": "same comment as above.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567247030", "createdAt": "2021-01-30T13:37:27Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/HoodieSparkSqlWriterSuite.scala", "diffHunk": "@@ -397,6 +401,46 @@ class HoodieSparkSqlWriterSuite extends FunSuite with Matchers {\n       }\n     })\n \n+  test(\"Test build sync config for spark sql\") {\n+    initSparkContext(\"test build sync config\")\n+    val addSqlTablePropertiesMethod =\n+        HoodieSparkSqlWriter.getClass.getDeclaredMethod(\"addSqlTableProperties\",\n+          classOf[SQLConf], classOf[StructType], classOf[Map[_,_]])\n+    addSqlTablePropertiesMethod.setAccessible(true)\n+\n+    val schema = DataSourceTestUtils.getStructTypeExampleSchema\n+    val structType = AvroConversionUtils.convertAvroSchemaToStructType(schema)\n+    val basePath = \"/tmp/hoodie_test\"\n+    val params = Map(\n+      \"path\" -> basePath,\n+      DataSourceWriteOptions.TABLE_NAME_OPT_KEY -> \"test_hoodie\",\n+      DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"partition\"\n+    )\n+    val parameters = HoodieWriterUtils.parametersWithWriteDefaults(params)\n+    val newParams = addSqlTablePropertiesMethod.invoke(HoodieSparkSqlWriter,\n+      spark.sessionState.conf, structType, parameters)\n+      .asInstanceOf[Map[String, String]]\n+\n+    val buildSyncConfigMethod =\n+      HoodieSparkSqlWriter.getClass.getDeclaredMethod(\"buildSyncConfig\", classOf[Path],\n+        classOf[Map[_,_]])\n+    buildSyncConfigMethod.setAccessible(true)\n+\n+    val hiveSyncConfig = buildSyncConfigMethod.invoke(HoodieSparkSqlWriter,\n+      new Path(basePath), newParams).asInstanceOf[HiveSyncConfig]\n+\n+    assertResult(\"spark.sql.sources.provider=hudi\\n\" +\n+      \"spark.sql.sources.schema.partCol.0=partition\\n\" +\n+      \"spark.sql.sources.schema.numParts=1\\n\" +\n+      \"spark.sql.sources.schema.numPartCols=1\\n\" +\n+      \"spark.sql.sources.schema.part.0=\" +\n+      \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"_row_key\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":false,\\\"metadata\\\":{}},\" +\n+      \"{\\\"name\\\":\\\"ts\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},\" +\n+      \"{\\\"name\\\":\\\"partition\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":false,\\\"metadata\\\":{}}]}\")(hiveSyncConfig.tableProperties)\n+\n+    assertResult(\"path=/tmp/hoodie_test/*/*\")(hiveSyncConfig.serdeProperties)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU3MzQwMDE5OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/HoodieSparkSqlWriterSuite.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzozOToyNFrOIc-B2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzozOToyNFrOIc-B2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NzMyMQ==", "bodyText": "do you think we need to assert HIVE_TABLE_PROPERTIES as well ?", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567247321", "createdAt": "2021-01-30T13:39:24Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/test/scala/org/apache/hudi/functional/HoodieSparkSqlWriterSuite.scala", "diffHunk": "@@ -397,6 +401,46 @@ class HoodieSparkSqlWriterSuite extends FunSuite with Matchers {\n       }\n     })\n \n+  test(\"Test build sync config for spark sql\") {\n+    initSparkContext(\"test build sync config\")\n+    val addSqlTablePropertiesMethod =\n+        HoodieSparkSqlWriter.getClass.getDeclaredMethod(\"addSqlTableProperties\",\n+          classOf[SQLConf], classOf[StructType], classOf[Map[_,_]])\n+    addSqlTablePropertiesMethod.setAccessible(true)\n+\n+    val schema = DataSourceTestUtils.getStructTypeExampleSchema\n+    val structType = AvroConversionUtils.convertAvroSchemaToStructType(schema)\n+    val basePath = \"/tmp/hoodie_test\"\n+    val params = Map(\n+      \"path\" -> basePath,\n+      DataSourceWriteOptions.TABLE_NAME_OPT_KEY -> \"test_hoodie\",\n+      DataSourceWriteOptions.HIVE_PARTITION_FIELDS_OPT_KEY -> \"partition\"\n+    )\n+    val parameters = HoodieWriterUtils.parametersWithWriteDefaults(params)\n+    val newParams = addSqlTablePropertiesMethod.invoke(HoodieSparkSqlWriter,\n+      spark.sessionState.conf, structType, parameters)\n+      .asInstanceOf[Map[String, String]]\n+\n+    val buildSyncConfigMethod =\n+      HoodieSparkSqlWriter.getClass.getDeclaredMethod(\"buildSyncConfig\", classOf[Path],\n+        classOf[Map[_,_]])\n+    buildSyncConfigMethod.setAccessible(true)\n+\n+    val hiveSyncConfig = buildSyncConfigMethod.invoke(HoodieSparkSqlWriter,\n+      new Path(basePath), newParams).asInstanceOf[HiveSyncConfig]\n+\n+    assertResult(\"spark.sql.sources.provider=hudi\\n\" +\n+      \"spark.sql.sources.schema.partCol.0=partition\\n\" +\n+      \"spark.sql.sources.schema.numParts=1\\n\" +\n+      \"spark.sql.sources.schema.numPartCols=1\\n\" +\n+      \"spark.sql.sources.schema.part.0=\" +\n+      \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"_row_key\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":false,\\\"metadata\\\":{}},\" +\n+      \"{\\\"name\\\":\\\"ts\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},\" +\n+      \"{\\\"name\\\":\\\"partition\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":false,\\\"metadata\\\":{}}]}\")(hiveSyncConfig.tableProperties)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU3MzQwMjIyOnYy", "diffSide": "RIGHT", "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzo0MjoxMVrOIc-C0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzo0MjoxMVrOIc-C0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NzU3MA==", "bodyText": "since this is abstract class and not every implementation will have some concrete override, can we make this empty here so that HoodieDLAClient does not need to do a no op override.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567247570", "createdAt": "2021-01-30T13:42:11Z", "author": {"login": "nsivabalan"}, "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -75,6 +76,8 @@ public abstract void createTable(String tableName, MessageType storageSchema,\n \n   public abstract void updatePartitionsToTable(String tableName, List<String> changedPartitions);\n \n+  public abstract void updateTableProperties(String tableName, Map<String, String> tableProperties);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU3MzQwMzAzOnYy", "diffSide": "RIGHT", "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzo0MzoyOVrOIc-DOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzo0MzoyOVrOIc-DOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0NzY3NA==", "bodyText": "I understand it's not part of this diff. But wondering if you can add some java docs to this class in general. I realized we don't have any one. (at line 41 ish) .", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567247674", "createdAt": "2021-01-30T13:43:29Z", "author": {"login": "nsivabalan"}, "path": "hudi-sync/hudi-sync-common/src/main/java/org/apache/hudi/sync/common/AbstractSyncHoodieClient.java", "diffHunk": "@@ -63,7 +63,8 @@ public AbstractSyncHoodieClient(String basePath, boolean assumeDatePartitioning,\n   }\n \n   public abstract void createTable(String tableName, MessageType storageSchema,\n-                                   String inputFormatClass, String outputFormatClass, String serdeClass);\n+                                   String inputFormatClass, String outputFormatClass,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU3MzQwODczOnYy", "diffSide": "RIGHT", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzo0OTo0MVrOIc-F4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0wNFQxNjowNzoxNlrOIf7PZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0ODM1NA==", "bodyText": "minor. \"Failed to update...\". remove extra \"get\"", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567248354", "createdAt": "2021-01-30T13:49:41Z", "author": {"login": "nsivabalan"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -138,6 +138,27 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit\n     }\n   }\n \n+  /**\n+   * Update the table properties to the table.\n+   * @param tableProperties\n+   */\n+  @Override\n+  public void updateTableProperties(String tableName, Map<String, String> tableProperties) {\n+    if (tableProperties == null || tableProperties.size() == 0) {\n+      return;\n+    }\n+    try {\n+      Table table = client.getTable(syncConfig.databaseName, tableName);\n+      for (Map.Entry<String, String> entry: tableProperties.entrySet()) {\n+        table.putToParameters(entry.getKey(), entry.getValue());\n+      }\n+      client.alter_table(syncConfig.databaseName, tableName, table);\n+    } catch (Exception e) {\n+      throw new HoodieHiveSyncException(\"Failed to get update table properties for table: \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MDM0NzM2Ng==", "bodyText": "Thanks for your correct.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r570347366", "createdAt": "2021-02-04T16:07:16Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -138,6 +138,27 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit\n     }\n   }\n \n+  /**\n+   * Update the table properties to the table.\n+   * @param tableProperties\n+   */\n+  @Override\n+  public void updateTableProperties(String tableName, Map<String, String> tableProperties) {\n+    if (tableProperties == null || tableProperties.size() == 0) {\n+      return;\n+    }\n+    try {\n+      Table table = client.getTable(syncConfig.databaseName, tableName);\n+      for (Map.Entry<String, String> entry: tableProperties.entrySet()) {\n+        table.putToParameters(entry.getKey(), entry.getValue());\n+      }\n+      client.alter_table(syncConfig.databaseName, tableName, table);\n+    } catch (Exception e) {\n+      throw new HoodieHiveSyncException(\"Failed to get update table properties for table: \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0ODM1NA=="}, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzU3MzQxMDE4OnYy", "diffSide": "RIGHT", "path": "hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxMzo1MTowOVrOIc-GkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0zMFQxNDoyODo0OFrOIc-V3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0ODUyOA==", "bodyText": "Would be nice if you write a test for the actual problem you faced as per the title/desc. And ensure that the test fails w/o this patch and succeeds with this patch.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567248528", "createdAt": "2021-01-30T13:51:09Z", "author": {"login": "nsivabalan"}, "path": "hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "diffHunk": "@@ -249,6 +255,54 @@ public void testBasicSync(boolean useJdbc, boolean useSchemaFromCommitMetadata)\n         \"The last commit that was sycned should be 100\");\n   }\n \n+  @ParameterizedTest\n+  @MethodSource({\"useJdbcAndSchemaFromCommitMetadata\"})\n+  public void testSyncWithProperties(boolean useJdbc, boolean useSchemaFromCommitMetadata) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI1MjQ0Ng==", "bodyText": "Thanks @nsivabalan  for these nice suggestions, I will spend some time to process.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r567252446", "createdAt": "2021-01-30T14:28:48Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/TestHiveSyncTool.java", "diffHunk": "@@ -249,6 +255,54 @@ public void testBasicSync(boolean useJdbc, boolean useSchemaFromCommitMetadata)\n         \"The last commit that was sycned should be 100\");\n   }\n \n+  @ParameterizedTest\n+  @MethodSource({\"useJdbcAndSchemaFromCommitMetadata\"})\n+  public void testSyncWithProperties(boolean useJdbc, boolean useSchemaFromCommitMetadata) throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2NzI0ODUyOA=="}, "originalCommit": {"oid": "63cbf0148a033fe511d49b381691d126f78f8828"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcyMTMzNzI4OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wOFQxNTo0MDo0NlrOIyNdwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wOVQwMjoxODozNFrOIykBrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTUyMDMyMA==", "bodyText": "@pengzhiwei2018 hello, how spark sql judge to use datasource ? I see just set the table properties . But not the \"ROW FORMAT SERDE \"", "url": "https://github.com/apache/hudi/pull/2283#discussion_r589520320", "createdAt": "2021-03-08T15:40:46Z", "author": {"login": "lw309637554"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -378,11 +389,75 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.autoCreateDatabase = parameters.get(HIVE_AUTO_CREATE_DATABASE_OPT_KEY).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    // Convert the schema and partition info used by spark sql to hive table properties.\n+    // The following code refers to the spark code in\n+    // https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala\n+\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partitionCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrderedType = StructType(dataCols ++ partitionCols)\n+    val schemaParts = reOrderedType.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "872519b8a0b0dfc883e09f25cf1c20d27c36caa7"}, "originalPosition": 106}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTg4OTk2NQ==", "bodyText": "Hi @lw309637554 , If the table properties contains the spark.sql.sources.provider and schema/partition properties, spark will trait it as a datasource table.\nThe PR has set the serde properties in createSqlTableSerdeProperties, that also will be used by spark datasource table.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r589889965", "createdAt": "2021-03-09T02:18:34Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -378,11 +389,75 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.autoCreateDatabase = parameters.get(HIVE_AUTO_CREATE_DATABASE_OPT_KEY).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    // Convert the schema and partition info used by spark sql to hive table properties.\n+    // The following code refers to the spark code in\n+    // https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala\n+\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partitionCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrderedType = StructType(dataCols ++ partitionCols)\n+    val schemaParts = reOrderedType.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTUyMDMyMA=="}, "originalCommit": {"oid": "872519b8a0b0dfc883e09f25cf1c20d27c36caa7"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzcyMTM0MDg5OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wOFQxNTo0MTozNlrOIyNgFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wOVQwMjoyMDowMlrOIykECQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTUyMDkxNw==", "bodyText": "if we can persist this properties to metatable , not the hive table properties?", "url": "https://github.com/apache/hudi/pull/2283#discussion_r589520917", "createdAt": "2021-03-08T15:41:36Z", "author": {"login": "lw309637554"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -378,11 +389,75 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.autoCreateDatabase = parameters.get(HIVE_AUTO_CREATE_DATABASE_OPT_KEY).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    // Convert the schema and partition info used by spark sql to hive table properties.\n+    // The following code refers to the spark code in\n+    // https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala\n+\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partitionCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrderedType = StructType(dataCols ++ partitionCols)\n+    val schemaParts = reOrderedType.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",\n+      \"spark.sql.sources.schema.numParts\" -> schemaParts.size.toString", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "872519b8a0b0dfc883e09f25cf1c20d27c36caa7"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTg5MDU2OQ==", "bodyText": "Spark need this properties when load the meta data from the hive meta store.So we should store them there.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r589890569", "createdAt": "2021-03-09T02:20:02Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -378,11 +389,75 @@ private[hudi] object HoodieSparkSqlWriter {\n     hiveSyncConfig.autoCreateDatabase = parameters.get(HIVE_AUTO_CREATE_DATABASE_OPT_KEY).exists(r => r.toBoolean)\n     hiveSyncConfig.decodePartition = parameters.getOrElse(URL_ENCODE_PARTITIONING_OPT_KEY,\n       DEFAULT_URL_ENCODE_PARTITIONING_OPT_VAL).toBoolean\n+    hiveSyncConfig.tableProperties = parameters.getOrElse(HIVE_TABLE_PROPERTIES, null)\n+    hiveSyncConfig.serdeProperties = createSqlTableSerdeProperties(parameters, basePath.toString,\n+      hiveSyncConfig.partitionFields.size())\n     hiveSyncConfig\n   }\n \n-  private def metaSync(parameters: Map[String, String],\n-                       basePath: Path,\n+  /**\n+    * Add Spark Sql related table properties to the HIVE_TABLE_PROPERTIES.\n+    * @param sqlConf\n+    * @param schema\n+    * @param parameters\n+    * @return A new parameters added the HIVE_TABLE_PROPERTIES property.\n+    */\n+  private def addSqlTableProperties(sqlConf: SQLConf, schema: StructType,\n+                                    parameters: Map[String, String]): Map[String, String] = {\n+    // Convert the schema and partition info used by spark sql to hive table properties.\n+    // The following code refers to the spark code in\n+    // https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveExternalCatalog.scala\n+\n+    val partitionSet = parameters(HIVE_PARTITION_FIELDS_OPT_KEY)\n+      .split(\",\").map(_.trim).filter(!_.isEmpty).toSet\n+    val threshold = sqlConf.getConf(SCHEMA_STRING_LENGTH_THRESHOLD)\n+\n+    val (partitionCols, dataCols) = schema.partition(c => partitionSet.contains(c.name))\n+    val reOrderedType = StructType(dataCols ++ partitionCols)\n+    val schemaParts = reOrderedType.json.grouped(threshold).toSeq\n+\n+    var properties = Map(\n+      \"spark.sql.sources.provider\" -> \"hudi\",\n+      \"spark.sql.sources.schema.numParts\" -> schemaParts.size.toString", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4OTUyMDkxNw=="}, "originalCommit": {"oid": "872519b8a0b0dfc883e09f25cf1c20d27c36caa7"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg4NzM0OTk2OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQyMjozMjozOFrOJJ95hA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xOVQwMzozNDowMlrOJLAicQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQzMTEwOA==", "bodyText": "Can be moved to metaSync or syncHive method.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614431108", "createdAt": "2021-04-15T22:32:38Z", "author": {"login": "umehrot2"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -306,7 +311,10 @@ private[hudi] object HoodieSparkSqlWriter {\n     } finally {\n       writeClient.close()\n     }\n-    val metaSyncSuccess = metaSync(parameters, basePath, jsc.hadoopConfiguration)\n+    val newParameters =\n+      addSqlTableProperties(sqlContext.sparkSession.sessionState.conf, df.schema, parameters)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNTUyMjkyOQ==", "bodyText": "yeah, moving the addSqlTableProperties  to  metaSync can simplify the logical.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r615522929", "createdAt": "2021-04-19T03:34:02Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -306,7 +311,10 @@ private[hudi] object HoodieSparkSqlWriter {\n     } finally {\n       writeClient.close()\n     }\n-    val metaSyncSuccess = metaSync(parameters, basePath, jsc.hadoopConfiguration)\n+    val newParameters =\n+      addSqlTableProperties(sqlContext.sparkSession.sessionState.conf, df.schema, parameters)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQzMTEwOA=="}, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg4NzM1NTU0OnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQyMjozNTowNlrOJJ985w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xOVQwODowMjo1OVrOJLG4zQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQzMTk3NQ==", "bodyText": "This modification seems unnecessary, as hadoopConf is not being used.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614431975", "createdAt": "2021-04-15T22:35:06Z", "author": {"login": "umehrot2"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -388,7 +399,8 @@ private[hudi] object HoodieSparkSqlWriter {\n     }\n   }\n \n-  private def syncHive(basePath: Path, fs: FileSystem, parameters: Map[String, String]): Boolean = {\n+  private def syncHive(basePath: Path, fs: FileSystem, parameters: Map[String, String],\n+                       hadoopConf: Configuration): Boolean = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNTYyNjk1Nw==", "bodyText": "Fixed!", "url": "https://github.com/apache/hudi/pull/2283#discussion_r615626957", "createdAt": "2021-04-19T08:02:59Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -388,7 +399,8 @@ private[hudi] object HoodieSparkSqlWriter {\n     }\n   }\n \n-  private def syncHive(basePath: Path, fs: FileSystem, parameters: Map[String, String]): Boolean = {\n+  private def syncHive(basePath: Path, fs: FileSystem, parameters: Map[String, String],\n+                       hadoopConf: Configuration): Boolean = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQzMTk3NQ=="}, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg4NzM3OTYxOnYy", "diffSide": "RIGHT", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQyMjo0Mjo0OVrOJJ-J1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQyMjo0Mjo0OVrOJJ-J1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQzNTI4NA==", "bodyText": "Can you improve the javadoc ? It has missing properties and descriptions.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614435284", "createdAt": "2021-04-15T22:42:49Z", "author": {"login": "umehrot2"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -138,6 +138,27 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit\n     }\n   }\n \n+  /**\n+   * Update the table properties to the table.\n+   * @param tableProperties\n+   */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg4NzM4MzM1OnYy", "diffSide": "RIGHT", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQyMjo0NDoyMFrOJJ-MAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQyMjo0NDoyMFrOJJ-MAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQzNTg0Mw==", "bodyText": "nit: tableProperties.isEmpty() ?", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614435843", "createdAt": "2021-04-15T22:44:20Z", "author": {"login": "umehrot2"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java", "diffHunk": "@@ -138,6 +138,27 @@ public void updatePartitionsToTable(String tableName, List<String> changedPartit\n     }\n   }\n \n+  /**\n+   * Update the table properties to the table.\n+   * @param tableProperties\n+   */\n+  @Override\n+  public void updateTableProperties(String tableName, Map<String, String> tableProperties) {\n+    if (tableProperties == null || tableProperties.size() == 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg4NzUyNDAwOnYy", "diffSide": "RIGHT", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQyMzoyMDo1NVrOJJ_gtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0yMFQwMjoyNToxN1rOJLv1Cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ1NzUyNw==", "bodyText": "Can't we sync this while creating the table itself, like you are doing for serde properties ?", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614457527", "createdAt": "2021-04-15T23:20:55Z", "author": {"login": "umehrot2"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java", "diffHunk": "@@ -164,7 +165,13 @@ private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat) {\n     LOG.info(\"Storage partitions scan complete. Found \" + writtenPartitionsSince.size());\n     // Sync the partitions if needed\n     syncPartitions(tableName, writtenPartitionsSince);\n-\n+    // Sync the table properties if need\n+    if (cfg.tableProperties != null) {\n+      Map<String, String> tableProperties = ConfigUtils.toMap(cfg.tableProperties);\n+      hoodieHiveClient.updateTableProperties(tableName, tableProperties);\n+      LOG.info(\"Sync table properties for \" + tableName + \", table properties is: \"\n+          + cfg.tableProperties);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNTUyNTg1Mg==", "bodyText": "Well, the tableProperties may change if the schema has changed. So we need to update the table properties  by a separate interface.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r615525852", "createdAt": "2021-04-19T03:46:17Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java", "diffHunk": "@@ -164,7 +165,13 @@ private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat) {\n     LOG.info(\"Storage partitions scan complete. Found \" + writtenPartitionsSince.size());\n     // Sync the partitions if needed\n     syncPartitions(tableName, writtenPartitionsSince);\n-\n+    // Sync the table properties if need\n+    if (cfg.tableProperties != null) {\n+      Map<String, String> tableProperties = ConfigUtils.toMap(cfg.tableProperties);\n+      hoodieHiveClient.updateTableProperties(tableName, tableProperties);\n+      LOG.info(\"Sync table properties for \" + tableName + \", table properties is: \"\n+          + cfg.tableProperties);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ1NzUyNw=="}, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNjI4ODQ1Nw==", "bodyText": "@pengzhiwei2018 I understand that. In the syncSchema() there is a check for schema difference. Perhaps we should move this inside that to avoid this being run every time. And for the first time when table is created, we can do it as part of create table. Thoughts ?", "url": "https://github.com/apache/hudi/pull/2283#discussion_r616288457", "createdAt": "2021-04-20T01:55:10Z", "author": {"login": "umehrot2"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java", "diffHunk": "@@ -164,7 +165,13 @@ private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat) {\n     LOG.info(\"Storage partitions scan complete. Found \" + writtenPartitionsSince.size());\n     // Sync the partitions if needed\n     syncPartitions(tableName, writtenPartitionsSince);\n-\n+    // Sync the table properties if need\n+    if (cfg.tableProperties != null) {\n+      Map<String, String> tableProperties = ConfigUtils.toMap(cfg.tableProperties);\n+      hoodieHiveClient.updateTableProperties(tableName, tableProperties);\n+      LOG.info(\"Sync table properties for \" + tableName + \", table properties is: \"\n+          + cfg.tableProperties);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ1NzUyNw=="}, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNjI5NzczOQ==", "bodyText": "Yeah, +1 for this.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r616297739", "createdAt": "2021-04-20T02:25:17Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncTool.java", "diffHunk": "@@ -164,7 +165,13 @@ private void syncHoodieTable(String tableName, boolean useRealtimeInputFormat) {\n     LOG.info(\"Storage partitions scan complete. Found \" + writtenPartitionsSince.size());\n     // Sync the partitions if needed\n     syncPartitions(tableName, writtenPartitionsSince);\n-\n+    // Sync the table properties if need\n+    if (cfg.tableProperties != null) {\n+      Map<String, String> tableProperties = ConfigUtils.toMap(cfg.tableProperties);\n+      hoodieHiveClient.updateTableProperties(tableName, tableProperties);\n+      LOG.info(\"Sync table properties for \" + tableName + \", table properties is: \"\n+          + cfg.tableProperties);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ1NzUyNw=="}, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg4NzU0OTgzOnYy", "diffSide": "RIGHT", "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncConfig.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQyMzozMToyNlrOJJ_vgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xOVQwMzo0NzowNVrOJLAufA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ2MTMxNQ==", "bodyText": "Can you update the toString() in this class ?", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614461315", "createdAt": "2021-04-15T23:31:26Z", "author": {"login": "umehrot2"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncConfig.java", "diffHunk": "@@ -88,6 +88,12 @@\n   @Parameter(names = {\"--verify-metadata-file-listing\"}, description = \"Verify file listing from Hudi's metadata against file system\")\n   public Boolean verifyMetadataFileListing = HoodieMetadataConfig.DEFAULT_METADATA_VALIDATE;\n \n+  @Parameter(names = {\"--table-properties\"}, description = \"Table properties to hive table\")\n+  public String tableProperties;\n+\n+  @Parameter(names = {\"--serde-properties\"}, description = \"Serde properties to hive table\")\n+  public String serdeProperties;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNTUyNjAxMg==", "bodyText": "Yes, thanks for remind me.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r615526012", "createdAt": "2021-04-19T03:47:05Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-sync/hudi-hive-sync/src/main/java/org/apache/hudi/hive/HiveSyncConfig.java", "diffHunk": "@@ -88,6 +88,12 @@\n   @Parameter(names = {\"--verify-metadata-file-listing\"}, description = \"Verify file listing from Hudi's metadata against file system\")\n   public Boolean verifyMetadataFileListing = HoodieMetadataConfig.DEFAULT_METADATA_VALIDATE;\n \n+  @Parameter(names = {\"--table-properties\"}, description = \"Table properties to hive table\")\n+  public String tableProperties;\n+\n+  @Parameter(names = {\"--serde-properties\"}, description = \"Serde properties to hive table\")\n+  public String serdeProperties;\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ2MTMxNQ=="}, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzg4NzU2MTMwOnYy", "diffSide": "RIGHT", "path": "hudi-spark-datasource/hudi-spark-common/src/main/scala/org/apache/hudi/DataSourceOptions.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xNVQyMzozNjo1OVrOJJ_2Qw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wNC0xOVQwMzo1ODo1MlrOJLA5dQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ2MzA0Mw==", "bodyText": "Lets introduce another additional boolean property hoodie.datasource.hive_sync.sync_as_datasource and put the feature behind it. We can use true by default, but atleast there would be a way to turn it off. This is going to change the way spark sql queries currently run with Hudi, and is a huge change.", "url": "https://github.com/apache/hudi/pull/2283#discussion_r614463043", "createdAt": "2021-04-15T23:36:59Z", "author": {"login": "umehrot2"}, "path": "hudi-spark-datasource/hudi-spark-common/src/main/scala/org/apache/hudi/DataSourceOptions.scala", "diffHunk": "@@ -353,6 +353,8 @@ object DataSourceWriteOptions {\n   val HIVE_IGNORE_EXCEPTIONS_OPT_KEY = \"hoodie.datasource.hive_sync.ignore_exceptions\"\n   val HIVE_SKIP_RO_SUFFIX = \"hoodie.datasource.hive_sync.skip_ro_suffix\"\n   val HIVE_SUPPORT_TIMESTAMP = \"hoodie.datasource.hive_sync.support_timestamp\"\n+  val HIVE_TABLE_PROPERTIES = \"hoodie.datasource.hive_sync.table_properties\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNTUyODgyMQ==", "bodyText": "Good suggestion! +1 for this!", "url": "https://github.com/apache/hudi/pull/2283#discussion_r615528821", "createdAt": "2021-04-19T03:58:52Z", "author": {"login": "pengzhiwei2018"}, "path": "hudi-spark-datasource/hudi-spark-common/src/main/scala/org/apache/hudi/DataSourceOptions.scala", "diffHunk": "@@ -353,6 +353,8 @@ object DataSourceWriteOptions {\n   val HIVE_IGNORE_EXCEPTIONS_OPT_KEY = \"hoodie.datasource.hive_sync.ignore_exceptions\"\n   val HIVE_SKIP_RO_SUFFIX = \"hoodie.datasource.hive_sync.skip_ro_suffix\"\n   val HIVE_SUPPORT_TIMESTAMP = \"hoodie.datasource.hive_sync.support_timestamp\"\n+  val HIVE_TABLE_PROPERTIES = \"hoodie.datasource.hive_sync.table_properties\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDQ2MzA0Mw=="}, "originalCommit": {"oid": "6343d09682dbcfb6e9716312889d876178d6349b"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4210, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}