{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM2NTkxOTY2", "number": 2328, "title": "[HUDI-1451] Support bulk insert v2 with Spark 3.0.0", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nRecently, we made Hudi support Spark 3.0.0: #2208. However, there was a large refactor done to Spark datasource V2 API interfaces from version 2.4.4 \u2192 3.0.0. So currently our bulk insert v2(https://issues.apache.org/jira/browse/HUDI-1013) feature is marked as unsupported with Spark3.\nWe need to redesign the bulk insert v2 part with datasource v2 api in Spark 3.0.0.\nBrief change log\nCreate a new internal datasource org.apache.hudi.spark3.internal which implements bulk insert v2 with datasource v2 API in Spark 3.0.0.\nMost code are migrated from datasource org.apache.hudi.internal.\nVerify this pull request\nThis change added tests and can be verified as follows:\n\nAdded two unit tests to verify the change.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-12-11T07:29:57Z", "url": "https://github.com/apache/hudi/pull/2328", "merged": true, "mergeCommit": {"oid": "286055ce34bdbbac68c995a2710fc7be07734b12"}, "closed": true, "closedAt": "2020-12-25T14:43:34Z", "author": {"login": "zhedoubushishi"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdmpjnZAFqTU1MzM5MzE2OQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdppYq1gFqTU1ODgyMTUxNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzMzkzMTY5", "url": "https://github.com/apache/hudi/pull/2328#pullrequestreview-553393169", "createdAt": "2020-12-16T06:37:20Z", "commit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNjozNzoyMFrOIG0_JQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQwNzowNjoxMlrOIG2cwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDAzMDUwMQ==", "bodyText": "all other configs are named as \"bulkinsert\". can we fix this one too instead of \"bulk_insert\".", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544030501", "createdAt": "2020-12-16T06:37:20Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -70,6 +70,7 @@\n   public static final String INSERT_PARALLELISM = \"hoodie.insert.shuffle.parallelism\";\n   public static final String BULKINSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\";\n   public static final String BULKINSERT_USER_DEFINED_PARTITIONER_CLASS = \"hoodie.bulkinsert.user.defined.partitioner.class\";\n+  public static final String BULKINSERT_INPUT_DATA_SCHEMA_DDL = \"hoodie.bulk_insert.schema.ddl\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0MDM4NA==", "bodyText": "do you think we can create \"BaseDefaultSource\" and move SparkSession, Configuration and  getters there. So that spark 3 and spark2 defaultSource can extend from it.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544040384", "createdAt": "2020-12-16T06:49:15Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/DefaultSource.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.DataSourceUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableProvider;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n+\n+import java.util.Map;\n+\n+/**\n+ * DataSource V2 implementation for managing internal write logic. Only called internally.\n+ * This class is only compatible with datasource V2 API in Spark 3.\n+ */\n+public class DefaultSource implements TableProvider {\n+\n+  private SparkSession sparkSession = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0NjYzMw==", "bodyText": "I assume this class is almost exact replica of HoodieDataSourceInternalWriter", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544046633", "createdAt": "2020-12-16T06:57:08Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.DataSourceUtils;\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieInstant.State;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.write.DataWriterFactory;\n+import org.apache.spark.sql.connector.write.WriterCommitMessage;\n+import org.apache.spark.sql.connector.write.BatchWrite;\n+import org.apache.spark.sql.connector.write.PhysicalWriteInfo;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implementation of {@link BatchWrite} for datasource \"hudi.spark3.internal\" to be used in datasource implementation\n+ * of bulk insert.\n+ */\n+public class HoodieDataSourceInternalBatchWriter implements BatchWrite {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0NzU3MQ==", "bodyText": "I assume this is exact replica of HoodieBulkInsertDataInternalWriterFactory in spark2 datasource.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544047571", "createdAt": "2020-12-16T06:58:17Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieBulkInsertDataInternalWriterFactory.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.write.DataWriter;\n+import org.apache.spark.sql.connector.write.DataWriterFactory;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * Factory to assist in instantiating {@link HoodieBulkInsertDataInternalWriter}.\n+ */\n+public class HoodieBulkInsertDataInternalWriterFactory implements DataWriterFactory {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0ODU2NA==", "bodyText": "is there any changes in this class compared to HoodieBulkInsertDataInternalWriter in spark 2 datasource?", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544048564", "createdAt": "2020-12-16T06:59:22Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieBulkInsertDataInternalWriter.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.io.HoodieRowCreateHandle;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.write.DataWriter;\n+import org.apache.spark.sql.connector.write.WriterCommitMessage;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+\n+/**\n+ * Hoodie's Implementation of {@link DataWriter<InternalRow>}. This is used in data source \"hudi.spark3.internal\" implementation for bulk insert.\n+ */\n+public class HoodieBulkInsertDataInternalWriter implements DataWriter<InternalRow> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA1MjQ0NA==", "bodyText": "java docs.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544052444", "createdAt": "2020-12-16T07:03:56Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalTable.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.catalog.SupportsWrite;\n+import org.apache.spark.sql.connector.catalog.TableCapability;\n+import org.apache.spark.sql.connector.write.LogicalWriteInfo;\n+import org.apache.spark.sql.connector.write.WriteBuilder;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.HashSet;\n+import java.util.Set;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA1NDE0OA==", "bodyText": "why named it as \"...Writer\" even though interface is called Write. i.e. BatchWrite.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544054148", "createdAt": "2020-12-16T07:05:43Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriter.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.DataSourceUtils;\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.client.SparkRDDWriteClient;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieWriteStat;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieInstant.State;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.write.DataWriterFactory;\n+import org.apache.spark.sql.connector.write.WriterCommitMessage;\n+import org.apache.spark.sql.connector.write.BatchWrite;\n+import org.apache.spark.sql.connector.write.PhysicalWriteInfo;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implementation of {@link BatchWrite} for datasource \"hudi.spark3.internal\" to be used in datasource implementation\n+ * of bulk insert.\n+ */\n+public class HoodieDataSourceInternalBatchWriter implements BatchWrite {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA0NjYzMw=="}, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDA1NDQ2NA==", "bodyText": "why suffixed it as \"WriterBuilder\" while interface is called \"WriteBuilder\"", "url": "https://github.com/apache/hudi/pull/2328#discussion_r544054464", "createdAt": "2020-12-16T07:06:12Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/main/java/org/apache/hudi/spark3/internal/HoodieDataSourceInternalBatchWriterBuilder.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.write.BatchWrite;\n+import org.apache.spark.sql.connector.write.WriteBuilder;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * Implementation of {@link WriteBuilder} for datasource \"hudi.spark3.internal\" to be used in datasource implementation\n+ * of bulk insert.\n+ */\n+public class HoodieDataSourceInternalBatchWriterBuilder implements WriteBuilder {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0a7688aa4e9ec19574b7e5fb4bc89ee0bf220ff1"}, "originalPosition": 33}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "10891a44835ccc4b61862e371c9524bb7fa59a0c", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/10891a44835ccc4b61862e371c9524bb7fa59a0c", "committedDate": "2020-12-22T18:19:42Z", "message": "add docs"}, "afterCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/b01dd14366b04e57b4860b57b13891a025360fe4", "committedDate": "2020-12-23T06:32:09Z", "message": "[HUDI-1451] Support bulk insert v2 with Spark 3.0.0"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4MTM5MjYz", "url": "https://github.com/apache/hudi/pull/2328#pullrequestreview-558139263", "createdAt": "2020-12-23T17:37:26Z", "commit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QxNzozNzoyNlrOIKr_bQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yM1QxODoyMjoxN1rOIKuKag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA3NzQyMQ==", "bodyText": "does it makes sense to print Arrays.toString(writeStatuses.toArray()) ?", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548077421", "createdAt": "2020-12-23T17:37:26Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseHoodieWriterCommitMessage.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+\n+import java.util.List;\n+\n+/**\n+ * Base class for HoodieWriterCommitMessage used by Spark datasource v2.\n+ */\n+public class BaseHoodieWriterCommitMessage {\n+\n+  private List<HoodieInternalWriteStatus> writeStatuses;\n+\n+  public BaseHoodieWriterCommitMessage(List<HoodieInternalWriteStatus> writeStatuses) {\n+    this.writeStatuses = writeStatuses;\n+  }\n+\n+  public List<HoodieInternalWriteStatus> getWriteStatuses() {\n+    return writeStatuses;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return \"HoodieWriterCommitMessage{\" + \"writeStatuses=\" + writeStatuses + '}';", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA3ODE0Mw==", "bodyText": "my bad. since this is not public class per se, we can remove \"Hoodie\" prefix. btw, can you help me understand why this needs to be public? package private should work right?", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548078143", "createdAt": "2020-12-23T17:38:18Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/HoodieBulkInsertDataInternalWriterHelper.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.io.HoodieRowCreateHandle;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.StructType;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+\n+/**\n+ * Helper class for HoodieBulkInsertDataInternalWriter used by Spark datasource v2.\n+ */\n+public class HoodieBulkInsertDataInternalWriterHelper {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODA4NTI3Mw==", "bodyText": "2 cents. feel free to take a call. Should we explicitly check only if spark version  is 2 or 3 we will proceed, if not, will throw an exception. just in case, in future, when we have spark 4, we don't need to make any code fixes.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548085273", "createdAt": "2020-12-23T17:47:21Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala", "diffHunk": "@@ -130,9 +130,6 @@ private[hudi] object HoodieSparkSqlWriter {\n       // scalastyle:off\n       if (parameters(ENABLE_ROW_WRITER_OPT_KEY).toBoolean &&\n         operation == WriteOperationType.BULK_INSERT) {\n-        if (!SPARK_VERSION.startsWith(\"2.\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODEwNjk3OQ==", "bodyText": "help me understand. do we need two separate derived classes for each datasource. Why can't we just use this class directly in both datasources?", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548106979", "createdAt": "2020-12-23T18:13:59Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark-common/src/main/java/org/apache/hudi/internal/BaseHoodieWriterCommitMessage.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.internal;\n+\n+import org.apache.hudi.client.HoodieInternalWriteStatus;\n+\n+import java.util.List;\n+\n+/**\n+ * Base class for HoodieWriterCommitMessage used by Spark datasource v2.\n+ */\n+public class BaseHoodieWriterCommitMessage {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMDI3OA==", "bodyText": "can we test both classes in one test class only? may be parametrized?", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548110278", "createdAt": "2020-12-23T18:18:13Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark2/src/test/java/org/apache/hudi/internal/TestHoodieBulkInsertDataInternalWriter.java", "diffHunk": "@@ -18,63 +18,32 @@\n \n package org.apache.hudi.internal;\n \n-import org.apache.hudi.client.HoodieInternalWriteStatus;\n-import org.apache.hudi.common.model.HoodieRecord;\n-import org.apache.hudi.common.model.HoodieWriteStat;\n import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n import org.apache.hudi.config.HoodieWriteConfig;\n import org.apache.hudi.table.HoodieSparkTable;\n import org.apache.hudi.table.HoodieTable;\n-import org.apache.hudi.testutils.HoodieClientTestHarness;\n \n-import org.apache.spark.package$;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.InternalRow;\n-import org.junit.jupiter.api.AfterEach;\n-import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n \n import java.util.ArrayList;\n import java.util.List;\n-import java.util.Random;\n \n import static org.apache.hudi.testutils.SparkDatasetTestUtils.ENCODER;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.STRUCT_TYPE;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.getConfigBuilder;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.getInternalRowWithError;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.getRandomRows;\n import static org.apache.hudi.testutils.SparkDatasetTestUtils.toInternalRows;\n-import static org.junit.jupiter.api.Assertions.assertEquals;\n-import static org.junit.jupiter.api.Assertions.assertFalse;\n-import static org.junit.jupiter.api.Assertions.assertNotNull;\n-import static org.junit.jupiter.api.Assertions.assertNull;\n-import static org.junit.jupiter.api.Assertions.assertTrue;\n import static org.junit.jupiter.api.Assertions.fail;\n-import static org.junit.jupiter.api.Assumptions.assumeTrue;\n \n /**\n  * Unit tests {@link HoodieBulkInsertDataInternalWriter}.\n  */\n-public class TestHoodieBulkInsertDataInternalWriter extends HoodieClientTestHarness {\n-\n-  private static final Random RANDOM = new Random();\n-\n-  @BeforeEach\n-  public void setUp() throws Exception {\n-    // this test is only compatible with spark 2\n-    assumeTrue(package$.MODULE$.SPARK_VERSION().startsWith(\"2.\"));\n-    initSparkContexts(\"TestHoodieBulkInsertDataInternalWriter\");\n-    initPath();\n-    initFileSystem();\n-    initTestDataGenerator();\n-    initMetaClient();\n-  }\n-\n-  @AfterEach\n-  public void tearDown() throws Exception {\n-    cleanupResources();\n-  }\n+public class TestHoodieBulkInsertDataInternalWriter extends", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMjM1NQ==", "bodyText": "again I see some opportunity for code reuse here. Do you think we can avoid duplicating the test code across two test classes. Except for few lines, rest could be made common.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548112355", "createdAt": "2020-12-23T18:21:12Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieDataSourceInternalBatchWrite.java", "diffHunk": "@@ -0,0 +1,264 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.internal.HoodieDataSourceInternalWriterTestBase;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.testutils.HoodieClientTestUtils;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.write.DataWriter;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.ENCODER;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.STRUCT_TYPE;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getConfigBuilder;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getRandomRows;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.toInternalRows;\n+\n+/**\n+ * Unit tests {@link HoodieDataSourceInternalBatchWrite}.\n+ */\n+public class TestHoodieDataSourceInternalBatchWrite extends\n+    HoodieDataSourceInternalWriterTestBase {\n+\n+  @Test\n+  public void testDataSourceWriter() throws Exception {\n+    // init config and table\n+    HoodieWriteConfig cfg = getConfigBuilder(basePath).build();\n+    HoodieTable table = HoodieSparkTable.create(cfg, context, metaClient);\n+    String instantTime = \"001\";\n+    // init writer\n+    HoodieDataSourceInternalBatchWrite dataSourceInternalBatchWrite =\n+        new HoodieDataSourceInternalBatchWrite(instantTime, cfg, STRUCT_TYPE, sqlContext.sparkSession(), hadoopConf);\n+    DataWriter<InternalRow> writer = dataSourceInternalBatchWrite.createBatchWriterFactory(null).createWriter(0, RANDOM.nextLong());\n+\n+    String[] partitionPaths = HoodieTestDataGenerator.DEFAULT_PARTITION_PATHS;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODExMzAwMg==", "bodyText": "same comment as other tests.", "url": "https://github.com/apache/hudi/pull/2328#discussion_r548113002", "createdAt": "2020-12-23T18:22:17Z", "author": {"login": "nsivabalan"}, "path": "hudi-spark-datasource/hudi-spark3/src/test/scala/org/apache/hudi/spark3/internal/TestHoodieBulkInsertDataInternalWriter.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.spark3.internal;\n+\n+import org.apache.hudi.common.testutils.HoodieTestDataGenerator;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.internal.HoodieBulkInsertDataInternalWriterTestBase;\n+import org.apache.hudi.table.HoodieSparkTable;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.junit.jupiter.api.Test;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.ENCODER;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.STRUCT_TYPE;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getConfigBuilder;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getInternalRowWithError;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.getRandomRows;\n+import static org.apache.hudi.testutils.SparkDatasetTestUtils.toInternalRows;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+/**\n+ * Unit tests {@link HoodieBulkInsertDataInternalWriter}.\n+ */\n+public class TestHoodieBulkInsertDataInternalWriter extends\n+    HoodieBulkInsertDataInternalWriterTestBase {\n+\n+  @Test\n+  public void testDataInternalWriter() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4"}, "originalPosition": 50}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a7b5d803bba9bd59b0bb8974c672297c78074fa3", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/a7b5d803bba9bd59b0bb8974c672297c78074fa3", "committedDate": "2020-12-25T13:16:03Z", "message": "[HUDI-1451] Support bulk insert v2 with Spark 3.0.0"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b01dd14366b04e57b4860b57b13891a025360fe4", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/b01dd14366b04e57b4860b57b13891a025360fe4", "committedDate": "2020-12-23T06:32:09Z", "message": "[HUDI-1451] Support bulk insert v2 with Spark 3.0.0"}, "afterCommit": {"oid": "a7b5d803bba9bd59b0bb8974c672297c78074fa3", "author": {"user": null}, "url": "https://github.com/apache/hudi/commit/a7b5d803bba9bd59b0bb8974c672297c78074fa3", "committedDate": "2020-12-25T13:16:03Z", "message": "[HUDI-1451] Support bulk insert v2 with Spark 3.0.0"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU4ODIxNTE2", "url": "https://github.com/apache/hudi/pull/2328#pullrequestreview-558821516", "createdAt": "2020-12-25T14:41:11Z", "commit": {"oid": "a7b5d803bba9bd59b0bb8974c672297c78074fa3"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3997, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}