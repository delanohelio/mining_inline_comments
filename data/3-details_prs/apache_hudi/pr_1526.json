{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA0OTA4OTAz", "number": 1526, "title": "[HUDI-783] Add pyspark example in quickstart", "bodyText": "Tips\n\nThank you very much for contributing to Apache Hudi.\nPlease review https://hudi.apache.org/contributing.html before opening a pull request.\n\nWhat is the purpose of the pull request\nTo add the example of hudi pyspark\n(For example: This pull request adds quick-start document.)\nBrief change log\nModify 1_1_quick_start doc.\n(for example:)\n\nModify AnnotationLocation checkstyle rule in checkstyle.xml\n\nVerify this pull request\n(Please pick either of the following options)\nThis pull request is a trivial rework / code cleanup without any test coverage.\nyes\n(or)\nThis pull request is already covered by existing tests, such as (please describe tests).\n(or)\nThis change added tests and can be verified as follows:\n(example:)\n\nAdded integration tests for end-to-end.\nAdded HoodieClientWriteTest to verify the change.\nManually verified the change by running a job locally.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-04-17T04:04:01Z", "url": "https://github.com/apache/hudi/pull/1526", "merged": true, "mergeCommit": {"oid": "531b42a529b7a2f248c062c1e5b351e79239baca"}, "closed": true, "closedAt": "2020-05-04T00:18:18Z", "author": {"login": "EdwinGuo"}, "timelineItems": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcYXnzAgH2gAyNDA0OTA4OTAzOmI5ODUzMTA5MTE4MWExZWY4MzY2ZDIzNzM0ZjM3ODA1YWU3ODBlY2E=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcdrlE1AFqTQwNDYyMzE4MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "b98531091181a1ef8366d23734f37805ae780eca", "author": {"user": {"login": "EdwinGuo", "name": "Edwin Guo"}}, "url": "https://github.com/apache/hudi/commit/b98531091181a1ef8366d23734f37805ae780eca", "committedDate": "2020-04-17T02:11:33Z", "message": "[HUDI-783] Pyspark Docs - Add Hudi pyspark insert example."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cbdbc9d37e87157d6b137197293dc4b8842d5e89", "author": {"user": {"login": "EdwinGuo", "name": "Edwin Guo"}}, "url": "https://github.com/apache/hudi/commit/cbdbc9d37e87157d6b137197293dc4b8842d5e89", "committedDate": "2020-04-17T02:41:57Z", "message": "[HUDI-783] Pyspark Docs - Add Hudi pyspark query and update example."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "56ad2b930330dcda05575bcbc9711e07dc633ba5", "author": {"user": {"login": "EdwinGuo", "name": "Edwin Guo"}}, "url": "https://github.com/apache/hudi/commit/56ad2b930330dcda05575bcbc9711e07dc633ba5", "committedDate": "2020-04-17T03:08:02Z", "message": "[HUDI-783] Pyspark Docs - Add Hudi pyspark incremental example."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9da28e3a24ef8472be1efdf23a047ba71e37364d", "author": {"user": {"login": "EdwinGuo", "name": "Edwin Guo"}}, "url": "https://github.com/apache/hudi/commit/9da28e3a24ef8472be1efdf23a047ba71e37364d", "committedDate": "2020-04-17T04:01:07Z", "message": "Pyspark Docs - Add Hudi pyspark delete example."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8ce9b81e33f100451c18ae89a79e417a6040f023", "author": {"user": {"login": "EdwinGuo", "name": "Edwin Guo"}}, "url": "https://github.com/apache/hudi/commit/8ce9b81e33f100451c18ae89a79e417a6040f023", "committedDate": "2020-04-17T19:04:00Z", "message": "[HUDI-783] Pyspark Docs - Fix syntax issue"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1ODEwMjc4", "url": "https://github.com/apache/hudi/pull/1526#pullrequestreview-395810278", "createdAt": "2020-04-17T22:36:16Z", "commit": {"oid": "8ce9b81e33f100451c18ae89a79e417a6040f023"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMjozNjoxNlrOGHfA8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMjozNjoxNlrOGHfA8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUwMTM2MA==", "bodyText": "Add two spaces before each line.", "url": "https://github.com/apache/hudi/pull/1526#discussion_r410501360", "createdAt": "2020-04-17T22:36:16Z", "author": {"login": "lamberken"}, "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -68,6 +81,27 @@ df.write.format(\"hudi\").\n   save(basePath)\n ``` \n \n+{% highlight python %}\n+inserts = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateInserts(10))\n+df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n+\n+hudi_options = {\n+'hoodie.table.name': tableName,\n+'hoodie.datasource.write.recordkey.field': 'uuid',\n+'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n+'hoodie.datasource.write.table.name': tableName,\n+'hoodie.datasource.write.operation': 'insert',\n+'hoodie.datasource.write.precombine.field': 'ts',\n+'hoodie.upsert.shuffle.parallelism': 2, \n+'hoodie.insert.shuffle.parallelism': 2\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ce9b81e33f100451c18ae89a79e417a6040f023"}, "originalPosition": 44}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk1ODEwNDg0", "url": "https://github.com/apache/hudi/pull/1526#pullrequestreview-395810484", "createdAt": "2020-04-17T22:37:00Z", "commit": {"oid": "8ce9b81e33f100451c18ae89a79e417a6040f023"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMjozNzowMFrOGHfBjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMjozNzowMFrOGHfBjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUwMTUxNg==", "bodyText": "Add two spaces before each line", "url": "https://github.com/apache/hudi/pull/1526#discussion_r410501516", "createdAt": "2020-04-17T22:37:00Z", "author": {"login": "lamberken"}, "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -148,6 +203,31 @@ tripsIncrementalDF.createOrReplaceTempView(\"hudi_trips_incremental\")\n spark.sql(\"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare > 20.0\").show()\n ``` \n \n+{% highlight python %}\n+# reload data\n+spark. \\\n+  read. \\\n+  format(\"hudi\"). \\\n+  load(basePath + \"/*/*/*/*\"). \\\n+  createOrReplaceTempView(\"hudi_trips_snapshot\")\n+\n+commits = list(map(lambda row: row[0], spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_snapshot order by commitTime\").limit(50).collect()))\n+beginTime = commits[len(commits) - 2] # commit time we are interested in\n+\n+# incrementally query data\n+incremental_read_options = {\n+'hoodie.datasource.query.type': 'incremental',\n+'hoodie.datasource.read.begin.instanttime': 'beginTime',", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8ce9b81e33f100451c18ae89a79e417a6040f023"}, "originalPosition": 117}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c3785cce1911637c7e0aab517c35f4124a14d1e2", "author": {"user": {"login": "EdwinGuo", "name": "Edwin Guo"}}, "url": "https://github.com/apache/hudi/commit/c3785cce1911637c7e0aab517c35f4124a14d1e2", "committedDate": "2020-04-19T14:23:25Z", "message": "[HUDI-1526] Address PR comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "540ad98a5e3a24082098dc4a27927cda3509654b", "author": {"user": {"login": "EdwinGuo", "name": "Edwin Guo"}}, "url": "https://github.com/apache/hudi/commit/540ad98a5e3a24082098dc4a27927cda3509654b", "committedDate": "2020-04-19T21:15:39Z", "message": "[HUDI-1526] Pyspark Docs - Fix syntax issue"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6beded1a11b6a9e37d65ab804800d6402d7c1f80", "author": {"user": {"login": "EdwinGuo", "name": "Edwin Guo"}}, "url": "https://github.com/apache/hudi/commit/6beded1a11b6a9e37d65ab804800d6402d7c1f80", "committedDate": "2020-04-19T22:55:20Z", "message": "[HUDI-783] Pyspark Docs - Update Formatting."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk2NjQ0MTY2", "url": "https://github.com/apache/hudi/pull/1526#pullrequestreview-396644166", "createdAt": "2020-04-20T17:02:40Z", "commit": {"oid": "6beded1a11b6a9e37d65ab804800d6402d7c1f80"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a24c849659e9d3695598082286d7583ee741c155", "author": {"user": {"login": "EdwinGuo", "name": "Edwin Guo"}}, "url": "https://github.com/apache/hudi/commit/a24c849659e9d3695598082286d7583ee741c155", "committedDate": "2020-04-26T15:37:22Z", "message": "[HUDI-1526] Reformatted the docs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwNjIwMTc1", "url": "https://github.com/apache/hudi/pull/1526#pullrequestreview-400620175", "createdAt": "2020-04-27T04:34:06Z", "commit": {"oid": "a24c849659e9d3695598082286d7583ee741c155"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwNDozNDowNlrOGMQSvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yN1QwNDozNjoxM1rOGMQVWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTUwMzAzNg==", "bodyText": "@EdwinGuo - The incremental query section is already there between the lines 332-368, please remove this duplicate content.", "url": "https://github.com/apache/hudi/pull/1526#discussion_r415503036", "createdAt": "2020-04-27T04:34:06Z", "author": {"login": "vingov"}, "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -204,6 +213,262 @@ spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").count()\n ```\n Note: Only `Append` mode is supported for delete operation.\n \n+# Pyspark example\n+## Setup\n+\n+Hudi works with Spark-2.x versions. You can follow instructions [here](https://spark.apache.org/downloads.html) for setting up spark. \n+From the extracted directory run spark-shell with Hudi as:\n+\n+```python\n+# pyspark\n+export PYSPARK_PYTHON=$(which python3)\n+spark-2.4.4-bin-hadoop2.7/bin/pyspark \\\n+  --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating,org.apache.spark:spark-avro_2.11:2.4.4 \\\n+  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'\n+```\n+\n+<div class=\"notice--info\">\n+  <h4>Please note the following: </h4>\n+<ul>\n+  <li>spark-avro module needs to be specified in --packages as it is not included with spark-shell by default</li>\n+  <li>spark-avro and spark versions must match (we have used 2.4.4 for both above)</li>\n+  <li>we have used hudi-spark-bundle built for scala 2.11 since the spark-avro module used also depends on 2.11. \n+         If spark-avro_2.12 is used, correspondingly hudi-spark-bundle_2.12 needs to be used. </li>\n+</ul>\n+</div>\n+\n+Setup table name, base path and a data generator to generate records for this guide.\n+\n+```python\n+# pyspark\n+tableName = \"hudi_trips_cow\"\n+basePath = \"file:///tmp/hudi_trips_cow\"\n+dataGen = sc._jvm.org.apache.hudi.QuickstartUtils.DataGenerator()\n+```\n+\n+The [DataGenerator](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L50) \n+can generate sample inserts and updates based on the the sample trip schema [here](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L57)\n+{: .notice--info}\n+\n+\n+## Insert data\n+\n+Generate some new trips, load them into a DataFrame and write the DataFrame into the Hudi table as below.\n+\n+```python\n+# pyspark\n+inserts = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateInserts(10))\n+df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n+\n+hudi_options = {\n+  'hoodie.table.name': tableName,\n+  'hoodie.datasource.write.recordkey.field': 'uuid',\n+  'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n+  'hoodie.datasource.write.table.name': tableName,\n+  'hoodie.datasource.write.operation': 'insert',\n+  'hoodie.datasource.write.precombine.field': 'ts',\n+  'hoodie.upsert.shuffle.parallelism': 2, \n+  'hoodie.insert.shuffle.parallelism': 2\n+}\n+\n+df.write.format(\"hudi\"). \\\n+  options(**hudi_options). \\\n+  mode(\"overwrite\"). \\\n+  save(basePath)\n+```\n+\n+`mode(Overwrite)` overwrites and recreates the table if it already exists.\n+You can check the data generated under `/tmp/hudi_trips_cow/<region>/<country>/<city>/`. We provided a record key \n+(`uuid` in [schema](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L58)), partition field (`region/county/city`) and combine logic (`ts` in \n+[schema](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L58)) to ensure trip records are unique within each partition. For more info, refer to \n+[Modeling data stored in Hudi](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=113709185#FAQ-HowdoImodelthedatastoredinHudi)\n+and for info on ways to ingest data into Hudi, refer to [Writing Hudi Tables](/docs/writing_data.html).\n+Here we are using the default write operation : `upsert`. If you have a workload without updates, you can also issue \n+`insert` or `bulk_insert` operations which could be faster. To know more, refer to [Write operations](/docs/writing_data#write-operations)\n+{: .notice--info}\n+\n+## Query data \n+\n+Load the data files into a DataFrame.\n+\n+```python\n+# pyspark\n+tripsSnapshotDF = spark. \\\n+  read. \\\n+  format(\"hudi\"). \\\n+  load(basePath + \"/*/*/*/*\")\n+\n+tripsSnapshotDF.createOrReplaceTempView(\"hudi_trips_snapshot\")\n+\n+spark.sql(\"select fare, begin_lon, begin_lat, ts from  hudi_trips_snapshot where fare > 20.0\").show()\n+spark.sql(\"select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from  hudi_trips_snapshot\").show()\n+```\n+\n+This query provides snapshot querying of the ingested data. Since our partition path (`region/country/city`) is 3 levels nested \n+from base path we ve used `load(basePath + \"/*/*/*/*\")`. \n+Refer to [Table types and queries](/docs/concepts#table-types--queries) for more info on all table types and query types supported.\n+{: .notice--info}\n+\n+## Update data\n+\n+This is similar to inserting new data. Generate updates to existing trips using the data generator, load into a DataFrame \n+and write DataFrame into the hudi table.\n+\n+```python\n+# pyspark\n+updates = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateUpdates(10))\n+df = spark.read.json(spark.sparkContext.parallelize(updates, 2))\n+df.write.format(\"hudi\"). \\\n+  options(**hudi_options). \\\n+  mode(\"append\"). \\\n+  save(basePath)\n+```\n+\n+Notice that the save mode is now `Append`. In general, always use append mode unless you are trying to create the table for the first time.\n+[Querying](#query-data) the data again will now show updated trips. Each write operation generates a new [commit](http://hudi.incubator.apache.org/docs/concepts.html) \n+denoted by the timestamp. Look for changes in `_hoodie_commit_time`, `rider`, `driver` fields for the same `_hoodie_record_key`s in previous commit. \n+{: .notice--info}\n+\n+## Incremental query\n+\n+Hudi also provides capability to obtain a stream of records that changed since given commit timestamp. \n+This can be achieved using Hudi's incremental querying and providing a begin time from which changes need to be streamed. \n+We do not need to specify endTime, if we want all changes after the given commit (as is the common case). \n+\n+```python\n+# pyspark\n+# reload data\n+spark. \\\n+  read. \\\n+  format(\"hudi\"). \\\n+  load(basePath + \"/*/*/*/*\"). \\\n+  createOrReplaceTempView(\"hudi_trips_snapshot\")\n+\n+commits = list(map(lambda row: row[0], spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_snapshot order by commitTime\").limit(50).collect()))\n+beginTime = commits[len(commits) - 2] # commit time we are interested in\n+\n+# incrementally query data\n+incremental_read_options = {\n+  'hoodie.datasource.query.type': 'incremental',\n+  'hoodie.datasource.read.begin.instanttime': 'beginTime',\n+}\n+\n+tripsIncrementalDF = spark.read.format(\"hudi\"). \\\n+  options(**incremental_read_options). \\\n+  load(basePath)\n+tripsIncrementalDF.createOrReplaceTempView(\"hudi_trips_incremental\")\n+\n+spark.sql(\"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare > 20.0\").show()\n+```\n+\n+This will give all changes that happened after the beginTime commit with the filter of fare > 20.0. The unique thing about this\n+feature is that it now lets you author streaming pipelines on batch data.\n+{: .notice--info}\n+\n+```\n+\n+## Incremental query\n+\n+Hudi also provides capability to obtain a stream of records that changed since given commit timestamp. \n+This can be achieved using Hudi's incremental querying and providing a begin time from which changes need to be streamed. \n+We do not need to specify endTime, if we want all changes after the given commit (as is the common case). \n+\n+```python\n+# pyspark\n+# reload data\n+spark. \\\n+  read. \\\n+  format(\"hudi\"). \\\n+  load(basePath + \"/*/*/*/*\"). \\\n+  createOrReplaceTempView(\"hudi_trips_snapshot\")\n+\n+commits = list(map(lambda row: row[0], spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_snapshot order by commitTime\").limit(50).collect()))\n+beginTime = commits[len(commits) - 2] # commit time we are interested in\n+\n+# incrementally query data\n+incremental_read_options = {\n+  'hoodie.datasource.query.type': 'incremental',\n+  'hoodie.datasource.read.begin.instanttime': 'beginTime',\n+}\n+\n+tripsIncrementalDF = spark.read.format(\"hudi\"). \\\n+  options(**incremental_read_options). \\\n+  load(basePath)\n+tripsIncrementalDF.createOrReplaceTempView(\"hudi_trips_incremental\")\n+\n+spark.sql(\"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare > 20.0\").show()\n+```\n+\n+This will give all changes that happened after the beginTime commit with the filter of fare > 20.0. The unique thing about this\n+feature is that it now lets you author streaming pipelines on batch data.\n+{: .notice--info}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a24c849659e9d3695598082286d7583ee741c155"}, "originalPosition": 280}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNTUwMzcwNQ==", "bodyText": "@EdwinGuo - Please remove the quotes around the beginTime, its a variable and not a string literal value.", "url": "https://github.com/apache/hudi/pull/1526#discussion_r415503705", "createdAt": "2020-04-27T04:36:13Z", "author": {"login": "vingov"}, "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -204,6 +213,262 @@ spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").count()\n ```\n Note: Only `Append` mode is supported for delete operation.\n \n+# Pyspark example\n+## Setup\n+\n+Hudi works with Spark-2.x versions. You can follow instructions [here](https://spark.apache.org/downloads.html) for setting up spark. \n+From the extracted directory run spark-shell with Hudi as:\n+\n+```python\n+# pyspark\n+export PYSPARK_PYTHON=$(which python3)\n+spark-2.4.4-bin-hadoop2.7/bin/pyspark \\\n+  --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating,org.apache.spark:spark-avro_2.11:2.4.4 \\\n+  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'\n+```\n+\n+<div class=\"notice--info\">\n+  <h4>Please note the following: </h4>\n+<ul>\n+  <li>spark-avro module needs to be specified in --packages as it is not included with spark-shell by default</li>\n+  <li>spark-avro and spark versions must match (we have used 2.4.4 for both above)</li>\n+  <li>we have used hudi-spark-bundle built for scala 2.11 since the spark-avro module used also depends on 2.11. \n+         If spark-avro_2.12 is used, correspondingly hudi-spark-bundle_2.12 needs to be used. </li>\n+</ul>\n+</div>\n+\n+Setup table name, base path and a data generator to generate records for this guide.\n+\n+```python\n+# pyspark\n+tableName = \"hudi_trips_cow\"\n+basePath = \"file:///tmp/hudi_trips_cow\"\n+dataGen = sc._jvm.org.apache.hudi.QuickstartUtils.DataGenerator()\n+```\n+\n+The [DataGenerator](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L50) \n+can generate sample inserts and updates based on the the sample trip schema [here](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L57)\n+{: .notice--info}\n+\n+\n+## Insert data\n+\n+Generate some new trips, load them into a DataFrame and write the DataFrame into the Hudi table as below.\n+\n+```python\n+# pyspark\n+inserts = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateInserts(10))\n+df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n+\n+hudi_options = {\n+  'hoodie.table.name': tableName,\n+  'hoodie.datasource.write.recordkey.field': 'uuid',\n+  'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n+  'hoodie.datasource.write.table.name': tableName,\n+  'hoodie.datasource.write.operation': 'insert',\n+  'hoodie.datasource.write.precombine.field': 'ts',\n+  'hoodie.upsert.shuffle.parallelism': 2, \n+  'hoodie.insert.shuffle.parallelism': 2\n+}\n+\n+df.write.format(\"hudi\"). \\\n+  options(**hudi_options). \\\n+  mode(\"overwrite\"). \\\n+  save(basePath)\n+```\n+\n+`mode(Overwrite)` overwrites and recreates the table if it already exists.\n+You can check the data generated under `/tmp/hudi_trips_cow/<region>/<country>/<city>/`. We provided a record key \n+(`uuid` in [schema](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L58)), partition field (`region/county/city`) and combine logic (`ts` in \n+[schema](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L58)) to ensure trip records are unique within each partition. For more info, refer to \n+[Modeling data stored in Hudi](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=113709185#FAQ-HowdoImodelthedatastoredinHudi)\n+and for info on ways to ingest data into Hudi, refer to [Writing Hudi Tables](/docs/writing_data.html).\n+Here we are using the default write operation : `upsert`. If you have a workload without updates, you can also issue \n+`insert` or `bulk_insert` operations which could be faster. To know more, refer to [Write operations](/docs/writing_data#write-operations)\n+{: .notice--info}\n+\n+## Query data \n+\n+Load the data files into a DataFrame.\n+\n+```python\n+# pyspark\n+tripsSnapshotDF = spark. \\\n+  read. \\\n+  format(\"hudi\"). \\\n+  load(basePath + \"/*/*/*/*\")\n+\n+tripsSnapshotDF.createOrReplaceTempView(\"hudi_trips_snapshot\")\n+\n+spark.sql(\"select fare, begin_lon, begin_lat, ts from  hudi_trips_snapshot where fare > 20.0\").show()\n+spark.sql(\"select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from  hudi_trips_snapshot\").show()\n+```\n+\n+This query provides snapshot querying of the ingested data. Since our partition path (`region/country/city`) is 3 levels nested \n+from base path we ve used `load(basePath + \"/*/*/*/*\")`. \n+Refer to [Table types and queries](/docs/concepts#table-types--queries) for more info on all table types and query types supported.\n+{: .notice--info}\n+\n+## Update data\n+\n+This is similar to inserting new data. Generate updates to existing trips using the data generator, load into a DataFrame \n+and write DataFrame into the hudi table.\n+\n+```python\n+# pyspark\n+updates = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateUpdates(10))\n+df = spark.read.json(spark.sparkContext.parallelize(updates, 2))\n+df.write.format(\"hudi\"). \\\n+  options(**hudi_options). \\\n+  mode(\"append\"). \\\n+  save(basePath)\n+```\n+\n+Notice that the save mode is now `Append`. In general, always use append mode unless you are trying to create the table for the first time.\n+[Querying](#query-data) the data again will now show updated trips. Each write operation generates a new [commit](http://hudi.incubator.apache.org/docs/concepts.html) \n+denoted by the timestamp. Look for changes in `_hoodie_commit_time`, `rider`, `driver` fields for the same `_hoodie_record_key`s in previous commit. \n+{: .notice--info}\n+\n+## Incremental query\n+\n+Hudi also provides capability to obtain a stream of records that changed since given commit timestamp. \n+This can be achieved using Hudi's incremental querying and providing a begin time from which changes need to be streamed. \n+We do not need to specify endTime, if we want all changes after the given commit (as is the common case). \n+\n+```python\n+# pyspark\n+# reload data\n+spark. \\\n+  read. \\\n+  format(\"hudi\"). \\\n+  load(basePath + \"/*/*/*/*\"). \\\n+  createOrReplaceTempView(\"hudi_trips_snapshot\")\n+\n+commits = list(map(lambda row: row[0], spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_snapshot order by commitTime\").limit(50).collect()))\n+beginTime = commits[len(commits) - 2] # commit time we are interested in\n+\n+# incrementally query data\n+incremental_read_options = {\n+  'hoodie.datasource.query.type': 'incremental',\n+  'hoodie.datasource.read.begin.instanttime': 'beginTime',", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a24c849659e9d3695598082286d7583ee741c155"}, "originalPosition": 229}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ad1e36f3125db404986823341c8b4ba5066ef405", "author": {"user": {"login": "EdwinGuo", "name": "Edwin Guo"}}, "url": "https://github.com/apache/hudi/commit/ad1e36f3125db404986823341c8b4ba5066ef405", "committedDate": "2020-04-27T12:18:07Z", "message": "[HUDI-1526] Address pr docs comments."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0NTM3MzUw", "url": "https://github.com/apache/hudi/pull/1526#pullrequestreview-404537350", "createdAt": "2020-05-02T15:32:49Z", "commit": {"oid": "ad1e36f3125db404986823341c8b4ba5066ef405"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQxNTozMjo0OVrOGPkBoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMlQxNTozNDowOFrOGPkCQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODk3MjA2NQ==", "bodyText": "hi @EdwinGuo, syntax error here, please use\npoint_in_time_read_options = {\n  'hoodie.datasource.query.type': 'incremental',\n  'hoodie.datasource.read.end.instanttime': endTime,\n  'hoodie.datasource.read.begin.instanttime': beginTime\n}", "url": "https://github.com/apache/hudi/pull/1526#discussion_r418972065", "createdAt": "2020-05-02T15:32:49Z", "author": {"login": "lamberken"}, "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -204,6 +213,224 @@ spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").count()\n ```\n Note: Only `Append` mode is supported for delete operation.\n \n+# Pyspark example\n+## Setup\n+\n+Hudi works with Spark-2.x versions. You can follow instructions [here](https://spark.apache.org/downloads.html) for setting up spark. \n+From the extracted directory run spark-shell with Hudi as:\n+\n+```python\n+# pyspark\n+export PYSPARK_PYTHON=$(which python3)\n+spark-2.4.4-bin-hadoop2.7/bin/pyspark \\\n+  --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating,org.apache.spark:spark-avro_2.11:2.4.4 \\\n+  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'\n+```\n+\n+<div class=\"notice--info\">\n+  <h4>Please note the following: </h4>\n+<ul>\n+  <li>spark-avro module needs to be specified in --packages as it is not included with spark-shell by default</li>\n+  <li>spark-avro and spark versions must match (we have used 2.4.4 for both above)</li>\n+  <li>we have used hudi-spark-bundle built for scala 2.11 since the spark-avro module used also depends on 2.11. \n+         If spark-avro_2.12 is used, correspondingly hudi-spark-bundle_2.12 needs to be used. </li>\n+</ul>\n+</div>\n+\n+Setup table name, base path and a data generator to generate records for this guide.\n+\n+```python\n+# pyspark\n+tableName = \"hudi_trips_cow\"\n+basePath = \"file:///tmp/hudi_trips_cow\"\n+dataGen = sc._jvm.org.apache.hudi.QuickstartUtils.DataGenerator()\n+```\n+\n+The [DataGenerator](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L50) \n+can generate sample inserts and updates based on the the sample trip schema [here](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L57)\n+{: .notice--info}\n+\n+\n+## Insert data\n+\n+Generate some new trips, load them into a DataFrame and write the DataFrame into the Hudi table as below.\n+\n+```python\n+# pyspark\n+inserts = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateInserts(10))\n+df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n+\n+hudi_options = {\n+  'hoodie.table.name': tableName,\n+  'hoodie.datasource.write.recordkey.field': 'uuid',\n+  'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n+  'hoodie.datasource.write.table.name': tableName,\n+  'hoodie.datasource.write.operation': 'insert',\n+  'hoodie.datasource.write.precombine.field': 'ts',\n+  'hoodie.upsert.shuffle.parallelism': 2, \n+  'hoodie.insert.shuffle.parallelism': 2\n+}\n+\n+df.write.format(\"hudi\"). \\\n+  options(**hudi_options). \\\n+  mode(\"overwrite\"). \\\n+  save(basePath)\n+```\n+\n+`mode(Overwrite)` overwrites and recreates the table if it already exists.\n+You can check the data generated under `/tmp/hudi_trips_cow/<region>/<country>/<city>/`. We provided a record key \n+(`uuid` in [schema](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L58)), partition field (`region/county/city`) and combine logic (`ts` in \n+[schema](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L58)) to ensure trip records are unique within each partition. For more info, refer to \n+[Modeling data stored in Hudi](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=113709185#FAQ-HowdoImodelthedatastoredinHudi)\n+and for info on ways to ingest data into Hudi, refer to [Writing Hudi Tables](/docs/writing_data.html).\n+Here we are using the default write operation : `upsert`. If you have a workload without updates, you can also issue \n+`insert` or `bulk_insert` operations which could be faster. To know more, refer to [Write operations](/docs/writing_data#write-operations)\n+{: .notice--info}\n+\n+## Query data \n+\n+Load the data files into a DataFrame.\n+\n+```python\n+# pyspark\n+tripsSnapshotDF = spark. \\\n+  read. \\\n+  format(\"hudi\"). \\\n+  load(basePath + \"/*/*/*/*\")\n+\n+tripsSnapshotDF.createOrReplaceTempView(\"hudi_trips_snapshot\")\n+\n+spark.sql(\"select fare, begin_lon, begin_lat, ts from  hudi_trips_snapshot where fare > 20.0\").show()\n+spark.sql(\"select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from  hudi_trips_snapshot\").show()\n+```\n+\n+This query provides snapshot querying of the ingested data. Since our partition path (`region/country/city`) is 3 levels nested \n+from base path we ve used `load(basePath + \"/*/*/*/*\")`. \n+Refer to [Table types and queries](/docs/concepts#table-types--queries) for more info on all table types and query types supported.\n+{: .notice--info}\n+\n+## Update data\n+\n+This is similar to inserting new data. Generate updates to existing trips using the data generator, load into a DataFrame \n+and write DataFrame into the hudi table.\n+\n+```python\n+# pyspark\n+updates = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateUpdates(10))\n+df = spark.read.json(spark.sparkContext.parallelize(updates, 2))\n+df.write.format(\"hudi\"). \\\n+  options(**hudi_options). \\\n+  mode(\"append\"). \\\n+  save(basePath)\n+```\n+\n+Notice that the save mode is now `Append`. In general, always use append mode unless you are trying to create the table for the first time.\n+[Querying](#query-data) the data again will now show updated trips. Each write operation generates a new [commit](http://hudi.incubator.apache.org/docs/concepts.html) \n+denoted by the timestamp. Look for changes in `_hoodie_commit_time`, `rider`, `driver` fields for the same `_hoodie_record_key`s in previous commit. \n+{: .notice--info}\n+\n+## Incremental query\n+\n+Hudi also provides capability to obtain a stream of records that changed since given commit timestamp. \n+This can be achieved using Hudi's incremental querying and providing a begin time from which changes need to be streamed. \n+We do not need to specify endTime, if we want all changes after the given commit (as is the common case). \n+\n+```python\n+# pyspark\n+# reload data\n+spark. \\\n+  read. \\\n+  format(\"hudi\"). \\\n+  load(basePath + \"/*/*/*/*\"). \\\n+  createOrReplaceTempView(\"hudi_trips_snapshot\")\n+\n+commits = list(map(lambda row: row[0], spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_snapshot order by commitTime\").limit(50).collect()))\n+beginTime = commits[len(commits) - 2] # commit time we are interested in\n+\n+# incrementally query data\n+incremental_read_options = {\n+  'hoodie.datasource.query.type': 'incremental',\n+  'hoodie.datasource.read.begin.instanttime': beginTime,\n+}\n+\n+tripsIncrementalDF = spark.read.format(\"hudi\"). \\\n+  options(**incremental_read_options). \\\n+  load(basePath)\n+tripsIncrementalDF.createOrReplaceTempView(\"hudi_trips_incremental\")\n+\n+spark.sql(\"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare > 20.0\").show()\n+```\n+\n+This will give all changes that happened after the beginTime commit with the filter of fare > 20.0. The unique thing about this\n+feature is that it now lets you author streaming pipelines on batch data.\n+{: .notice--info}\n+\n+## Point in time query\n+\n+Lets look at how to query data as of a specific time. The specific time can be represented by pointing endTime to a \n+specific commit time and beginTime to \"000\" (denoting earliest possible commit time). \n+\n+```python\n+# pyspark\n+beginTime = \"000\" # Represents all commits > this time.\n+endTime = commits[len(commits) - 2]\n+\n+# query point in time data\n+point_in_time_read_options = {**incremental_read_options, \n+                              **{\"hoodie.datasource.read.end.instanttime\": endTime,\n+                                \"hoodie.datasource.read.begin.instanttime\": beginTime}}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad1e36f3125db404986823341c8b4ba5066ef405"}, "originalPosition": 257}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODk3MjIyNw==", "bodyText": "here, need two blank space.", "url": "https://github.com/apache/hudi/pull/1526#discussion_r418972227", "createdAt": "2020-05-02T15:34:08Z", "author": {"login": "lamberken"}, "path": "docs/_docs/1_1_quick_start_guide.md", "diffHunk": "@@ -204,6 +213,224 @@ spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").count()\n ```\n Note: Only `Append` mode is supported for delete operation.\n \n+# Pyspark example\n+## Setup\n+\n+Hudi works with Spark-2.x versions. You can follow instructions [here](https://spark.apache.org/downloads.html) for setting up spark. \n+From the extracted directory run spark-shell with Hudi as:\n+\n+```python\n+# pyspark\n+export PYSPARK_PYTHON=$(which python3)\n+spark-2.4.4-bin-hadoop2.7/bin/pyspark \\\n+  --packages org.apache.hudi:hudi-spark-bundle_2.11:0.5.1-incubating,org.apache.spark:spark-avro_2.11:2.4.4 \\\n+  --conf 'spark.serializer=org.apache.spark.serializer.KryoSerializer'\n+```\n+\n+<div class=\"notice--info\">\n+  <h4>Please note the following: </h4>\n+<ul>\n+  <li>spark-avro module needs to be specified in --packages as it is not included with spark-shell by default</li>\n+  <li>spark-avro and spark versions must match (we have used 2.4.4 for both above)</li>\n+  <li>we have used hudi-spark-bundle built for scala 2.11 since the spark-avro module used also depends on 2.11. \n+         If spark-avro_2.12 is used, correspondingly hudi-spark-bundle_2.12 needs to be used. </li>\n+</ul>\n+</div>\n+\n+Setup table name, base path and a data generator to generate records for this guide.\n+\n+```python\n+# pyspark\n+tableName = \"hudi_trips_cow\"\n+basePath = \"file:///tmp/hudi_trips_cow\"\n+dataGen = sc._jvm.org.apache.hudi.QuickstartUtils.DataGenerator()\n+```\n+\n+The [DataGenerator](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L50) \n+can generate sample inserts and updates based on the the sample trip schema [here](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L57)\n+{: .notice--info}\n+\n+\n+## Insert data\n+\n+Generate some new trips, load them into a DataFrame and write the DataFrame into the Hudi table as below.\n+\n+```python\n+# pyspark\n+inserts = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateInserts(10))\n+df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n+\n+hudi_options = {\n+  'hoodie.table.name': tableName,\n+  'hoodie.datasource.write.recordkey.field': 'uuid',\n+  'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n+  'hoodie.datasource.write.table.name': tableName,\n+  'hoodie.datasource.write.operation': 'insert',\n+  'hoodie.datasource.write.precombine.field': 'ts',\n+  'hoodie.upsert.shuffle.parallelism': 2, \n+  'hoodie.insert.shuffle.parallelism': 2\n+}\n+\n+df.write.format(\"hudi\"). \\\n+  options(**hudi_options). \\\n+  mode(\"overwrite\"). \\\n+  save(basePath)\n+```\n+\n+`mode(Overwrite)` overwrites and recreates the table if it already exists.\n+You can check the data generated under `/tmp/hudi_trips_cow/<region>/<country>/<city>/`. We provided a record key \n+(`uuid` in [schema](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L58)), partition field (`region/county/city`) and combine logic (`ts` in \n+[schema](https://github.com/apache/incubator-hudi/blob/master/hudi-spark/src/main/java/org/apache/hudi/QuickstartUtils.java#L58)) to ensure trip records are unique within each partition. For more info, refer to \n+[Modeling data stored in Hudi](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=113709185#FAQ-HowdoImodelthedatastoredinHudi)\n+and for info on ways to ingest data into Hudi, refer to [Writing Hudi Tables](/docs/writing_data.html).\n+Here we are using the default write operation : `upsert`. If you have a workload without updates, you can also issue \n+`insert` or `bulk_insert` operations which could be faster. To know more, refer to [Write operations](/docs/writing_data#write-operations)\n+{: .notice--info}\n+\n+## Query data \n+\n+Load the data files into a DataFrame.\n+\n+```python\n+# pyspark\n+tripsSnapshotDF = spark. \\\n+  read. \\\n+  format(\"hudi\"). \\\n+  load(basePath + \"/*/*/*/*\")\n+\n+tripsSnapshotDF.createOrReplaceTempView(\"hudi_trips_snapshot\")\n+\n+spark.sql(\"select fare, begin_lon, begin_lat, ts from  hudi_trips_snapshot where fare > 20.0\").show()\n+spark.sql(\"select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from  hudi_trips_snapshot\").show()\n+```\n+\n+This query provides snapshot querying of the ingested data. Since our partition path (`region/country/city`) is 3 levels nested \n+from base path we ve used `load(basePath + \"/*/*/*/*\")`. \n+Refer to [Table types and queries](/docs/concepts#table-types--queries) for more info on all table types and query types supported.\n+{: .notice--info}\n+\n+## Update data\n+\n+This is similar to inserting new data. Generate updates to existing trips using the data generator, load into a DataFrame \n+and write DataFrame into the hudi table.\n+\n+```python\n+# pyspark\n+updates = sc._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateUpdates(10))\n+df = spark.read.json(spark.sparkContext.parallelize(updates, 2))\n+df.write.format(\"hudi\"). \\\n+  options(**hudi_options). \\\n+  mode(\"append\"). \\\n+  save(basePath)\n+```\n+\n+Notice that the save mode is now `Append`. In general, always use append mode unless you are trying to create the table for the first time.\n+[Querying](#query-data) the data again will now show updated trips. Each write operation generates a new [commit](http://hudi.incubator.apache.org/docs/concepts.html) \n+denoted by the timestamp. Look for changes in `_hoodie_commit_time`, `rider`, `driver` fields for the same `_hoodie_record_key`s in previous commit. \n+{: .notice--info}\n+\n+## Incremental query\n+\n+Hudi also provides capability to obtain a stream of records that changed since given commit timestamp. \n+This can be achieved using Hudi's incremental querying and providing a begin time from which changes need to be streamed. \n+We do not need to specify endTime, if we want all changes after the given commit (as is the common case). \n+\n+```python\n+# pyspark\n+# reload data\n+spark. \\\n+  read. \\\n+  format(\"hudi\"). \\\n+  load(basePath + \"/*/*/*/*\"). \\\n+  createOrReplaceTempView(\"hudi_trips_snapshot\")\n+\n+commits = list(map(lambda row: row[0], spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_snapshot order by commitTime\").limit(50).collect()))\n+beginTime = commits[len(commits) - 2] # commit time we are interested in\n+\n+# incrementally query data\n+incremental_read_options = {\n+  'hoodie.datasource.query.type': 'incremental',\n+  'hoodie.datasource.read.begin.instanttime': beginTime,\n+}\n+\n+tripsIncrementalDF = spark.read.format(\"hudi\"). \\\n+  options(**incremental_read_options). \\\n+  load(basePath)\n+tripsIncrementalDF.createOrReplaceTempView(\"hudi_trips_incremental\")\n+\n+spark.sql(\"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from  hudi_trips_incremental where fare > 20.0\").show()\n+```\n+\n+This will give all changes that happened after the beginTime commit with the filter of fare > 20.0. The unique thing about this\n+feature is that it now lets you author streaming pipelines on batch data.\n+{: .notice--info}\n+\n+## Point in time query\n+\n+Lets look at how to query data as of a specific time. The specific time can be represented by pointing endTime to a \n+specific commit time and beginTime to \"000\" (denoting earliest possible commit time). \n+\n+```python\n+# pyspark\n+beginTime = \"000\" # Represents all commits > this time.\n+endTime = commits[len(commits) - 2]\n+\n+# query point in time data\n+point_in_time_read_options = {**incremental_read_options, \n+                              **{\"hoodie.datasource.read.end.instanttime\": endTime,\n+                                \"hoodie.datasource.read.begin.instanttime\": beginTime}}\n+tripsPointInTimeDF = spark.read.format(\"hudi\"). \\\n+  options(**point_in_time_read_options). \\\n+  load(basePath)\n+\n+tripsPointInTimeDF.createOrReplaceTempView(\"hudi_trips_point_in_time\")\n+spark.sql(\"select `_hoodie_commit_time`, fare, begin_lon, begin_lat, ts from hudi_trips_point_in_time where fare > 20.0\").show()\n+```\n+\n+## Delete data {#deletes}\n+Delete records for the HoodieKeys passed in.\n+\n+Note: Only `Append` mode is supported for delete operation.\n+\n+```python\n+# pyspark\n+# fetch total records count\n+spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").count()\n+# fetch two records to be deleted\n+ds = spark.sql(\"select uuid, partitionPath from hudi_trips_snapshot\").limit(2)\n+\n+# issue deletes\n+hudi_delete_options = {\n+'hoodie.table.name': tableName,\n+'hoodie.datasource.write.recordkey.field': 'uuid',\n+'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n+'hoodie.datasource.write.table.name': tableName,\n+'hoodie.datasource.write.operation': 'delete',\n+'hoodie.datasource.write.precombine.field': 'ts',\n+'hoodie.upsert.shuffle.parallelism': 2, \n+'hoodie.insert.shuffle.parallelism': 2", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad1e36f3125db404986823341c8b4ba5066ef405"}, "originalPosition": 287}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b886c74da079f976e6c36bf71fea1cf3ed177550", "author": {"user": {"login": "EdwinGuo", "name": "Edwin Guo"}}, "url": "https://github.com/apache/hudi/commit/b886c74da079f976e6c36bf71fea1cf3ed177550", "committedDate": "2020-05-03T13:27:38Z", "message": "[HUDI-783] Add pyspark example in quickstart #1526"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA0NjIzMTgx", "url": "https://github.com/apache/hudi/pull/1526#pullrequestreview-404623181", "createdAt": "2020-05-03T14:16:18Z", "commit": {"oid": "b886c74da079f976e6c36bf71fea1cf3ed177550"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3426, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}