{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzY0NDUyNTgz", "number": 1248, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xOFQxODo0ODowOVrODZAlSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xOFQxODo0OTowNVrODZAlag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3NTUwNTM2OnYy", "diffSide": "RIGHT", "path": "docs/quickstart.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xOFQxODo0ODowOVrOFfLuYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xOFQxODo0ODowOVrOFfLuYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI0MjI3Mw==", "bodyText": "could we avoid doing the insert again?  can we not reuse from the insert/update done above?", "url": "https://github.com/apache/hudi/pull/1248#discussion_r368242273", "createdAt": "2020-01-18T18:48:09Z", "author": {"login": "vinothchandar"}, "path": "docs/quickstart.md", "diffHunk": "@@ -109,6 +109,57 @@ Notice that the save mode is now `Append`. In general, always use append mode un\n [Querying](#query) the data again will now show updated trips. Each write operation generates a new [commit](http://hudi.incubator.apache.org/concepts.html) \n denoted by the timestamp. Look for changes in `_hoodie_commit_time`, `rider`, `driver` fields for the same `_hoodie_record_key`s in previous commit. \n \n+## Delete data {#deletes}\n+Delete records for the HoodieKeys passed in. Lets first generate a new batch of insert and delete the same. Query to verify\n+that all records are deleted.\n+\n+```\n+val inserts = convertToStringList(dataGen.generateInserts(10))", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjI3NTUwNTcwOnYy", "diffSide": "RIGHT", "path": "docs/quickstart.md", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xOFQxODo0OTowNVrOFfLujQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xOFQyMTowODo0MFrOFfMHSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI0MjMxNw==", "bodyText": "ideally,. we just have this part and remove everything above this line, to keep the quickstart small", "url": "https://github.com/apache/hudi/pull/1248#discussion_r368242317", "createdAt": "2020-01-18T18:49:05Z", "author": {"login": "vinothchandar"}, "path": "docs/quickstart.md", "diffHunk": "@@ -109,6 +109,57 @@ Notice that the save mode is now `Append`. In general, always use append mode un\n [Querying](#query) the data again will now show updated trips. Each write operation generates a new [commit](http://hudi.incubator.apache.org/concepts.html) \n denoted by the timestamp. Look for changes in `_hoodie_commit_time`, `rider`, `driver` fields for the same `_hoodie_record_key`s in previous commit. \n \n+## Delete data {#deletes}\n+Delete records for the HoodieKeys passed in. Lets first generate a new batch of insert and delete the same. Query to verify\n+that all records are deleted.\n+\n+```\n+val inserts = convertToStringList(dataGen.generateInserts(10))\n+val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n+df.write.format(\"org.apache.hudi\").\n+    options(getQuickstartWriteConfigs).\n+    option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n+    option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n+    option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+    option(TABLE_NAME, tableName).\n+    mode(Overwrite).\n+    save(basePath);\n+\n+// Fetch the rider value for the batch of records inserted just now\n+val roDeleteViewDF = spark.\n+    read.\n+    format(\"org.apache.hudi\").\n+    load(basePath + \"/*/*/*/*\")\n+roDeleteViewDF.registerTempTable(\"hudi_ro_table\")\n+spark.sql(\"select distinct rider from  hudi_ro_table where\").show()\n+\n+// replace the rider value in below query to a value from above. \"rider-213\" is first batch and \"rider-284\" is second batch.\n+val ds = spark.sql(\"select uuid, partitionPath from hudi_ro_table where rider = 'rider-284'\")\n+\n+// issue deletes", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI0ODM2NA==", "bodyText": "I am deleting an entire batch of inserts and hence thought will do a new batch of inserts and delete the entire batch.", "url": "https://github.com/apache/hudi/pull/1248#discussion_r368248364", "createdAt": "2020-01-18T21:02:26Z", "author": {"login": "nsivabalan"}, "path": "docs/quickstart.md", "diffHunk": "@@ -109,6 +109,57 @@ Notice that the save mode is now `Append`. In general, always use append mode un\n [Querying](#query) the data again will now show updated trips. Each write operation generates a new [commit](http://hudi.incubator.apache.org/concepts.html) \n denoted by the timestamp. Look for changes in `_hoodie_commit_time`, `rider`, `driver` fields for the same `_hoodie_record_key`s in previous commit. \n \n+## Delete data {#deletes}\n+Delete records for the HoodieKeys passed in. Lets first generate a new batch of insert and delete the same. Query to verify\n+that all records are deleted.\n+\n+```\n+val inserts = convertToStringList(dataGen.generateInserts(10))\n+val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n+df.write.format(\"org.apache.hudi\").\n+    options(getQuickstartWriteConfigs).\n+    option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n+    option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n+    option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+    option(TABLE_NAME, tableName).\n+    mode(Overwrite).\n+    save(basePath);\n+\n+// Fetch the rider value for the batch of records inserted just now\n+val roDeleteViewDF = spark.\n+    read.\n+    format(\"org.apache.hudi\").\n+    load(basePath + \"/*/*/*/*\")\n+roDeleteViewDF.registerTempTable(\"hudi_ro_table\")\n+spark.sql(\"select distinct rider from  hudi_ro_table where\").show()\n+\n+// replace the rider value in below query to a value from above. \"rider-213\" is first batch and \"rider-284\" is second batch.\n+val ds = spark.sql(\"select uuid, partitionPath from hudi_ro_table where rider = 'rider-284'\")\n+\n+// issue deletes", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI0MjMxNw=="}, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI0ODU2MQ==", "bodyText": "So, if I do the same with initial insert batch, then all records will be deleted. But don't want to disrupt the flow for rest of the quick start.", "url": "https://github.com/apache/hudi/pull/1248#discussion_r368248561", "createdAt": "2020-01-18T21:06:44Z", "author": {"login": "nsivabalan"}, "path": "docs/quickstart.md", "diffHunk": "@@ -109,6 +109,57 @@ Notice that the save mode is now `Append`. In general, always use append mode un\n [Querying](#query) the data again will now show updated trips. Each write operation generates a new [commit](http://hudi.incubator.apache.org/concepts.html) \n denoted by the timestamp. Look for changes in `_hoodie_commit_time`, `rider`, `driver` fields for the same `_hoodie_record_key`s in previous commit. \n \n+## Delete data {#deletes}\n+Delete records for the HoodieKeys passed in. Lets first generate a new batch of insert and delete the same. Query to verify\n+that all records are deleted.\n+\n+```\n+val inserts = convertToStringList(dataGen.generateInserts(10))\n+val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n+df.write.format(\"org.apache.hudi\").\n+    options(getQuickstartWriteConfigs).\n+    option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n+    option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n+    option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+    option(TABLE_NAME, tableName).\n+    mode(Overwrite).\n+    save(basePath);\n+\n+// Fetch the rider value for the batch of records inserted just now\n+val roDeleteViewDF = spark.\n+    read.\n+    format(\"org.apache.hudi\").\n+    load(basePath + \"/*/*/*/*\")\n+roDeleteViewDF.registerTempTable(\"hudi_ro_table\")\n+spark.sql(\"select distinct rider from  hudi_ro_table where\").show()\n+\n+// replace the rider value in below query to a value from above. \"rider-213\" is first batch and \"rider-284\" is second batch.\n+val ds = spark.sql(\"select uuid, partitionPath from hudi_ro_table where rider = 'rider-284'\")\n+\n+// issue deletes", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI0MjMxNw=="}, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI0ODU4OA==", "bodyText": "I can move it as the last section. Hope thats fine.", "url": "https://github.com/apache/hudi/pull/1248#discussion_r368248588", "createdAt": "2020-01-18T21:07:36Z", "author": {"login": "nsivabalan"}, "path": "docs/quickstart.md", "diffHunk": "@@ -109,6 +109,57 @@ Notice that the save mode is now `Append`. In general, always use append mode un\n [Querying](#query) the data again will now show updated trips. Each write operation generates a new [commit](http://hudi.incubator.apache.org/concepts.html) \n denoted by the timestamp. Look for changes in `_hoodie_commit_time`, `rider`, `driver` fields for the same `_hoodie_record_key`s in previous commit. \n \n+## Delete data {#deletes}\n+Delete records for the HoodieKeys passed in. Lets first generate a new batch of insert and delete the same. Query to verify\n+that all records are deleted.\n+\n+```\n+val inserts = convertToStringList(dataGen.generateInserts(10))\n+val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n+df.write.format(\"org.apache.hudi\").\n+    options(getQuickstartWriteConfigs).\n+    option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n+    option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n+    option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+    option(TABLE_NAME, tableName).\n+    mode(Overwrite).\n+    save(basePath);\n+\n+// Fetch the rider value for the batch of records inserted just now\n+val roDeleteViewDF = spark.\n+    read.\n+    format(\"org.apache.hudi\").\n+    load(basePath + \"/*/*/*/*\")\n+roDeleteViewDF.registerTempTable(\"hudi_ro_table\")\n+spark.sql(\"select distinct rider from  hudi_ro_table where\").show()\n+\n+// replace the rider value in below query to a value from above. \"rider-213\" is first batch and \"rider-284\" is second batch.\n+val ds = spark.sql(\"select uuid, partitionPath from hudi_ro_table where rider = 'rider-284'\")\n+\n+// issue deletes", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI0MjMxNw=="}, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI0ODU4OQ==", "bodyText": "lets just delete a few existing records? and show that.. you can use .limit(2) to say get just 2 records out of the existing table and delete it", "url": "https://github.com/apache/hudi/pull/1248#discussion_r368248589", "createdAt": "2020-01-18T21:07:39Z", "author": {"login": "vinothchandar"}, "path": "docs/quickstart.md", "diffHunk": "@@ -109,6 +109,57 @@ Notice that the save mode is now `Append`. In general, always use append mode un\n [Querying](#query) the data again will now show updated trips. Each write operation generates a new [commit](http://hudi.incubator.apache.org/concepts.html) \n denoted by the timestamp. Look for changes in `_hoodie_commit_time`, `rider`, `driver` fields for the same `_hoodie_record_key`s in previous commit. \n \n+## Delete data {#deletes}\n+Delete records for the HoodieKeys passed in. Lets first generate a new batch of insert and delete the same. Query to verify\n+that all records are deleted.\n+\n+```\n+val inserts = convertToStringList(dataGen.generateInserts(10))\n+val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n+df.write.format(\"org.apache.hudi\").\n+    options(getQuickstartWriteConfigs).\n+    option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n+    option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n+    option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+    option(TABLE_NAME, tableName).\n+    mode(Overwrite).\n+    save(basePath);\n+\n+// Fetch the rider value for the batch of records inserted just now\n+val roDeleteViewDF = spark.\n+    read.\n+    format(\"org.apache.hudi\").\n+    load(basePath + \"/*/*/*/*\")\n+roDeleteViewDF.registerTempTable(\"hudi_ro_table\")\n+spark.sql(\"select distinct rider from  hudi_ro_table where\").show()\n+\n+// replace the rider value in below query to a value from above. \"rider-213\" is first batch and \"rider-284\" is second batch.\n+val ds = spark.sql(\"select uuid, partitionPath from hudi_ro_table where rider = 'rider-284'\")\n+\n+// issue deletes", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI0MjMxNw=="}, "originalCommit": null, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI0ODY1MQ==", "bodyText": "Lets have it after incremental query.. deletes will conclude the flow of writing and reading nicely", "url": "https://github.com/apache/hudi/pull/1248#discussion_r368248651", "createdAt": "2020-01-18T21:08:40Z", "author": {"login": "vinothchandar"}, "path": "docs/quickstart.md", "diffHunk": "@@ -109,6 +109,57 @@ Notice that the save mode is now `Append`. In general, always use append mode un\n [Querying](#query) the data again will now show updated trips. Each write operation generates a new [commit](http://hudi.incubator.apache.org/concepts.html) \n denoted by the timestamp. Look for changes in `_hoodie_commit_time`, `rider`, `driver` fields for the same `_hoodie_record_key`s in previous commit. \n \n+## Delete data {#deletes}\n+Delete records for the HoodieKeys passed in. Lets first generate a new batch of insert and delete the same. Query to verify\n+that all records are deleted.\n+\n+```\n+val inserts = convertToStringList(dataGen.generateInserts(10))\n+val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n+df.write.format(\"org.apache.hudi\").\n+    options(getQuickstartWriteConfigs).\n+    option(PRECOMBINE_FIELD_OPT_KEY, \"ts\").\n+    option(RECORDKEY_FIELD_OPT_KEY, \"uuid\").\n+    option(PARTITIONPATH_FIELD_OPT_KEY, \"partitionpath\").\n+    option(TABLE_NAME, tableName).\n+    mode(Overwrite).\n+    save(basePath);\n+\n+// Fetch the rider value for the batch of records inserted just now\n+val roDeleteViewDF = spark.\n+    read.\n+    format(\"org.apache.hudi\").\n+    load(basePath + \"/*/*/*/*\")\n+roDeleteViewDF.registerTempTable(\"hudi_ro_table\")\n+spark.sql(\"select distinct rider from  hudi_ro_table where\").show()\n+\n+// replace the rider value in below query to a value from above. \"rider-213\" is first batch and \"rider-284\" is second batch.\n+val ds = spark.sql(\"select uuid, partitionPath from hudi_ro_table where rider = 'rider-284'\")\n+\n+// issue deletes", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODI0MjMxNw=="}, "originalCommit": null, "originalPosition": 31}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4984, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}