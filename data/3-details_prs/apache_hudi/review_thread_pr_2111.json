{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkyNDMwMDE0", "number": 2111, "reviewThreads": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxMTo0MTowNlrOEna8Kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNTowMzoyNVrOFQtKfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5NzcxMzA3OnYy", "diffSide": "RIGHT", "path": "hudi-client/src/main/java/org/apache/hudi/table/WorkloadProfile.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxMTo0MTowNlrOHYAKZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxMTo1ODowOVrOHYApHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDkzMDUzMg==", "bodyText": "WriteOperationType should be Serializable?", "url": "https://github.com/apache/hudi/pull/2111#discussion_r494930532", "createdAt": "2020-09-25T11:41:06Z", "author": {"login": "leesf"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/WorkloadProfile.java", "diffHunk": "@@ -54,13 +55,23 @@\n    */\n   private final WorkloadStat globalStat;\n \n+  /**\n+   * Write operation type.\n+   */\n+  private WriteOperationType operationType;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7313074623a295060fdba16d3b33f4701cce71b8"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDkzODM5Ng==", "bodyText": "@leesf Yes, WriteOperationType should be Serializable, forget to check this. I would like to add implements Serializable.", "url": "https://github.com/apache/hudi/pull/2111#discussion_r494938396", "createdAt": "2020-09-25T11:58:09Z", "author": {"login": "SteNicholas"}, "path": "hudi-client/src/main/java/org/apache/hudi/table/WorkloadProfile.java", "diffHunk": "@@ -54,13 +55,23 @@\n    */\n   private final WorkloadStat globalStat;\n \n+  /**\n+   * Write operation type.\n+   */\n+  private WriteOperationType operationType;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDkzMDUzMg=="}, "originalCommit": {"oid": "7313074623a295060fdba16d3b33f4701cce71b8"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTI5MjIwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMTo1MzoxMFrOHea4Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzo0OToyNlrOHefiKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY1OTc1MQ==", "bodyText": "The method called addUpdateBucket, but the BucketType would be INSERT, it is not very suitable, would we move the logic outside of the method?", "url": "https://github.com/apache/hudi/pull/2111#discussion_r501659751", "createdAt": "2020-10-08T11:53:10Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -112,16 +115,17 @@ private void assignUpdates(WorkloadProfile profile) {\n     for (Map.Entry<String, WorkloadStat> partitionStat : partitionStatEntries) {\n       for (Map.Entry<String, Pair<String, Long>> updateLocEntry :\n           partitionStat.getValue().getUpdateLocationToCount().entrySet()) {\n-        addUpdateBucket(partitionStat.getKey(), updateLocEntry.getKey());\n+        addUpdateBucket(partitionStat.getKey(), updateLocEntry.getKey(), profile.getOperationType());\n       }\n     }\n   }\n \n-  private int addUpdateBucket(String partitionPath, String fileIdHint) {\n+  private int addUpdateBucket(String partitionPath, String fileIdHint, WriteOperationType operationType) {\n     int bucket = totalBuckets;\n     updateLocationToBucket.put(fileIdHint, bucket);\n     BucketInfo bucketInfo = new BucketInfo();\n-    bucketInfo.bucketType = BucketType.UPDATE;\n+    bucketInfo.bucketType = operationType == null || isChangingRecords(operationType)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ce6e021080921c9e01e819b577ab307e7cfee85"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTczNTk3Ng==", "bodyText": "@leesf OK, I would move the logic outside of method addUpdateBucket.", "url": "https://github.com/apache/hudi/pull/2111#discussion_r501735976", "createdAt": "2020-10-08T13:49:26Z", "author": {"login": "SteNicholas"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -112,16 +115,17 @@ private void assignUpdates(WorkloadProfile profile) {\n     for (Map.Entry<String, WorkloadStat> partitionStat : partitionStatEntries) {\n       for (Map.Entry<String, Pair<String, Long>> updateLocEntry :\n           partitionStat.getValue().getUpdateLocationToCount().entrySet()) {\n-        addUpdateBucket(partitionStat.getKey(), updateLocEntry.getKey());\n+        addUpdateBucket(partitionStat.getKey(), updateLocEntry.getKey(), profile.getOperationType());\n       }\n     }\n   }\n \n-  private int addUpdateBucket(String partitionPath, String fileIdHint) {\n+  private int addUpdateBucket(String partitionPath, String fileIdHint, WriteOperationType operationType) {\n     int bucket = totalBuckets;\n     updateLocationToBucket.put(fileIdHint, bucket);\n     BucketInfo bucketInfo = new BucketInfo();\n-    bucketInfo.bucketType = BucketType.UPDATE;\n+    bucketInfo.bucketType = operationType == null || isChangingRecords(operationType)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY1OTc1MQ=="}, "originalCommit": {"oid": "3ce6e021080921c9e01e819b577ab307e7cfee85"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTMwMjIyOnYy", "diffSide": "LEFT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMTo1NTo1MFrOHea-Ww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzo0MDoxNlrOHefGsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY2MTI3NQ==", "bodyText": "must remove these asserts?", "url": "https://github.com/apache/hudi/pull/2111#discussion_r501661275", "createdAt": "2020-10-08T11:55:50Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -743,47 +743,37 @@ public void testSmallInsertHandlingForUpserts() throws Exception {\n    */\n   @Test\n   public void testSmallInsertHandlingForInserts() throws Exception {\n-\n     final String testPartitionPath = \"2016/09/26\";\n     final int insertSplitLimit = 100;\n     // setup the small file handling params\n     HoodieWriteConfig config = getSmallInsertWriteConfig(insertSplitLimit); // hold upto 200 records max\n     dataGen = new HoodieTestDataGenerator(new String[] {testPartitionPath});\n     SparkRDDWriteClient client = getHoodieWriteClient(config, false);\n \n-    // Inserts => will write file1\n     String commitTime1 = \"001\";\n     client.startCommitWithTime(commitTime1);\n     List<HoodieRecord> inserts1 = dataGen.generateInserts(commitTime1, insertSplitLimit); // this writes ~500kb\n     Set<String> keys1 = recordsToRecordKeySet(inserts1);\n     JavaRDD<HoodieRecord> insertRecordsRDD1 = jsc.parallelize(inserts1, 1);\n     List<WriteStatus> statuses = client.insert(insertRecordsRDD1, commitTime1).collect();\n-\n     assertNoWriteErrors(statuses);\n-    assertPartitionMetadata(new String[] {testPartitionPath}, fs);\n-\n+    assertPartitionMetadata(new String[]{testPartitionPath}, fs);\n     assertEquals(1, statuses.size(), \"Just 1 file needs to be added.\");\n-    String file1 = statuses.get(0).getFileId();\n     assertEquals(100,\n         readRowKeysFromParquet(hadoopConf, new Path(basePath, statuses.get(0).getStat().getPath()))\n             .size(), \"file should contain 100 records\");\n \n-    // Second, set of Inserts should just expand file1\n     String commitTime2 = \"002\";\n     client.startCommitWithTime(commitTime2);\n     List<HoodieRecord> inserts2 = dataGen.generateInserts(commitTime2, 40);\n     Set<String> keys2 = recordsToRecordKeySet(inserts2);\n     JavaRDD<HoodieRecord> insertRecordsRDD2 = jsc.parallelize(inserts2, 1);\n     statuses = client.insert(insertRecordsRDD2, commitTime2).collect();\n     assertNoWriteErrors(statuses);\n-\n-    assertEquals(1, statuses.size(), \"Just 1 file needs to be updated.\");\n-    assertEquals(file1, statuses.get(0).getFileId(), \"Existing file should be expanded\");\n-    assertEquals(commitTime1, statuses.get(0).getStat().getPrevCommit(), \"Existing file should be expanded\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ce6e021080921c9e01e819b577ab307e7cfee85"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcyODk0NQ==", "bodyText": "@leesf These asserts makes no sense, because statuses.get(0).getFileId() could be file1+\"-0\" and statuses.get(0).getStat().getPrevCommit() could be null.", "url": "https://github.com/apache/hudi/pull/2111#discussion_r501728945", "createdAt": "2020-10-08T13:40:16Z", "author": {"login": "SteNicholas"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/client/TestHoodieClientOnCopyOnWriteStorage.java", "diffHunk": "@@ -743,47 +743,37 @@ public void testSmallInsertHandlingForUpserts() throws Exception {\n    */\n   @Test\n   public void testSmallInsertHandlingForInserts() throws Exception {\n-\n     final String testPartitionPath = \"2016/09/26\";\n     final int insertSplitLimit = 100;\n     // setup the small file handling params\n     HoodieWriteConfig config = getSmallInsertWriteConfig(insertSplitLimit); // hold upto 200 records max\n     dataGen = new HoodieTestDataGenerator(new String[] {testPartitionPath});\n     SparkRDDWriteClient client = getHoodieWriteClient(config, false);\n \n-    // Inserts => will write file1\n     String commitTime1 = \"001\";\n     client.startCommitWithTime(commitTime1);\n     List<HoodieRecord> inserts1 = dataGen.generateInserts(commitTime1, insertSplitLimit); // this writes ~500kb\n     Set<String> keys1 = recordsToRecordKeySet(inserts1);\n     JavaRDD<HoodieRecord> insertRecordsRDD1 = jsc.parallelize(inserts1, 1);\n     List<WriteStatus> statuses = client.insert(insertRecordsRDD1, commitTime1).collect();\n-\n     assertNoWriteErrors(statuses);\n-    assertPartitionMetadata(new String[] {testPartitionPath}, fs);\n-\n+    assertPartitionMetadata(new String[]{testPartitionPath}, fs);\n     assertEquals(1, statuses.size(), \"Just 1 file needs to be added.\");\n-    String file1 = statuses.get(0).getFileId();\n     assertEquals(100,\n         readRowKeysFromParquet(hadoopConf, new Path(basePath, statuses.get(0).getStat().getPath()))\n             .size(), \"file should contain 100 records\");\n \n-    // Second, set of Inserts should just expand file1\n     String commitTime2 = \"002\";\n     client.startCommitWithTime(commitTime2);\n     List<HoodieRecord> inserts2 = dataGen.generateInserts(commitTime2, 40);\n     Set<String> keys2 = recordsToRecordKeySet(inserts2);\n     JavaRDD<HoodieRecord> insertRecordsRDD2 = jsc.parallelize(inserts2, 1);\n     statuses = client.insert(insertRecordsRDD2, commitTime2).collect();\n     assertNoWriteErrors(statuses);\n-\n-    assertEquals(1, statuses.size(), \"Just 1 file needs to be updated.\");\n-    assertEquals(file1, statuses.get(0).getFileId(), \"Existing file should be expanded\");\n-    assertEquals(commitTime1, statuses.get(0).getStat().getPrevCommit(), \"Existing file should be expanded\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY2MTI3NQ=="}, "originalCommit": {"oid": "3ce6e021080921c9e01e819b577ab307e7cfee85"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTM4OTkxOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjoxODozMFrOHebyFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzo1MzozNlrOHefuyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY3NDUxNg==", "bodyText": "100 should be passed via method parameter.", "url": "https://github.com/apache/hudi/pull/2111#discussion_r501674516", "createdAt": "2020-10-08T12:18:30Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java", "diffHunk": "@@ -90,14 +88,33 @@ private UpsertPartitioner getUpsertPartitioner(int smallFileSize, int numInserts\n     List<HoodieRecord> records = new ArrayList<>();\n     records.addAll(insertRecords);\n     records.addAll(updateRecords);\n-    WorkloadProfile profile = new WorkloadProfile(buildProfile(jsc.parallelize(records)));\n+    WorkloadProfile profile = new WorkloadProfile(buildProfile(jsc.parallelize(records)), WriteOperationType.UPSERT);\n     UpsertPartitioner partitioner = new UpsertPartitioner(profile, context, table, config);\n     assertEquals(0, partitioner.getPartition(\n         new Tuple2<>(updateRecords.get(0).getKey(), Option.ofNullable(updateRecords.get(0).getCurrentLocation()))),\n         \"Update record should have gone to the 1 update partition\");\n     return partitioner;\n   }\n \n+  private UpsertPartitioner getInsertPartitioner(int smallFileSize, int numInserts, int fileSize, String testPartitionPath,\n+      boolean autoSplitInserts) throws Exception {\n+    HoodieWriteConfig config = makeHoodieClientConfigBuilder()\n+            .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(smallFileSize)\n+                    .insertSplitSize(100).autoTuneInsertSplits(autoSplitInserts).build())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ce6e021080921c9e01e819b577ab307e7cfee85"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTczOTIxMA==", "bodyText": "@leesf  I don't think 100 should be passed via method parameter. You could refer to the method getUpsertPartitioner.", "url": "https://github.com/apache/hudi/pull/2111#discussion_r501739210", "createdAt": "2020-10-08T13:53:36Z", "author": {"login": "SteNicholas"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java", "diffHunk": "@@ -90,14 +88,33 @@ private UpsertPartitioner getUpsertPartitioner(int smallFileSize, int numInserts\n     List<HoodieRecord> records = new ArrayList<>();\n     records.addAll(insertRecords);\n     records.addAll(updateRecords);\n-    WorkloadProfile profile = new WorkloadProfile(buildProfile(jsc.parallelize(records)));\n+    WorkloadProfile profile = new WorkloadProfile(buildProfile(jsc.parallelize(records)), WriteOperationType.UPSERT);\n     UpsertPartitioner partitioner = new UpsertPartitioner(profile, context, table, config);\n     assertEquals(0, partitioner.getPartition(\n         new Tuple2<>(updateRecords.get(0).getKey(), Option.ofNullable(updateRecords.get(0).getCurrentLocation()))),\n         \"Update record should have gone to the 1 update partition\");\n     return partitioner;\n   }\n \n+  private UpsertPartitioner getInsertPartitioner(int smallFileSize, int numInserts, int fileSize, String testPartitionPath,\n+      boolean autoSplitInserts) throws Exception {\n+    HoodieWriteConfig config = makeHoodieClientConfigBuilder()\n+            .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(smallFileSize)\n+                    .insertSplitSize(100).autoTuneInsertSplits(autoSplitInserts).build())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY3NDUxNg=="}, "originalCommit": {"oid": "3ce6e021080921c9e01e819b577ab307e7cfee85"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTM5NDA3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjoxOTo0NVrOHeb0nQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjoxOTo0NVrOHeb0nQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY3NTE2NQ==", "bodyText": "it would be better if you would describe how to get 3 expected partitions.", "url": "https://github.com/apache/hudi/pull/2111#discussion_r501675165", "createdAt": "2020-10-08T12:19:45Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java", "diffHunk": "@@ -286,8 +303,48 @@ public void testUpsertPartitionerWithSmallInsertHandling() throws Exception {\n         \"Bucket 3 is INSERT\");\n     assertEquals(4, insertBuckets.size(), \"Total of 4 insert buckets\");\n \n-    weights = new Double[] { 0.08, 0.31, 0.31, 0.31};\n-    cumulativeWeights = new Double[] { 0.08, 0.39, 0.69, 1.0};\n+    weights = new Double[] {0.08, 0.31, 0.31, 0.31};\n+    cumulativeWeights = new Double[] {0.08, 0.39, 0.69, 1.0};\n+    assertInsertBuckets(weights, cumulativeWeights, insertBuckets);\n+  }\n+\n+  @Test\n+  public void testInsertPartitionerWithSmallInsertHandling() throws Exception {\n+    final String testPartitionPath = \"2016/09/26\";\n+    // Inserts  .. Check updates go together & inserts subsplit, after expanding smallest file\n+    UpsertPartitioner partitioner = getInsertPartitioner(1000 * 1024, 400, 800 * 1024, testPartitionPath, false);\n+    List<InsertBucketCumulativeWeightPair> insertBuckets = partitioner.getInsertBuckets(testPartitionPath);\n+\n+    assertEquals(3, partitioner.numPartitions(), \"Should have 3 partitions\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ce6e021080921c9e01e819b577ab307e7cfee85"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTM5NjE4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjoyMDoxNlrOHeb17A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjoyMDoxNlrOHeb17A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY3NTUwMA==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/2111#discussion_r501675500", "createdAt": "2020-10-08T12:20:16Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java", "diffHunk": "@@ -286,8 +303,48 @@ public void testUpsertPartitionerWithSmallInsertHandling() throws Exception {\n         \"Bucket 3 is INSERT\");\n     assertEquals(4, insertBuckets.size(), \"Total of 4 insert buckets\");\n \n-    weights = new Double[] { 0.08, 0.31, 0.31, 0.31};\n-    cumulativeWeights = new Double[] { 0.08, 0.39, 0.69, 1.0};\n+    weights = new Double[] {0.08, 0.31, 0.31, 0.31};\n+    cumulativeWeights = new Double[] {0.08, 0.39, 0.69, 1.0};\n+    assertInsertBuckets(weights, cumulativeWeights, insertBuckets);\n+  }\n+\n+  @Test\n+  public void testInsertPartitionerWithSmallInsertHandling() throws Exception {\n+    final String testPartitionPath = \"2016/09/26\";\n+    // Inserts  .. Check updates go together & inserts subsplit, after expanding smallest file\n+    UpsertPartitioner partitioner = getInsertPartitioner(1000 * 1024, 400, 800 * 1024, testPartitionPath, false);\n+    List<InsertBucketCumulativeWeightPair> insertBuckets = partitioner.getInsertBuckets(testPartitionPath);\n+\n+    assertEquals(3, partitioner.numPartitions(), \"Should have 3 partitions\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(0).bucketType,\n+        \"Bucket 0 is INSERT\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(1).bucketType,\n+        \"Bucket 1 is INSERT\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(2).bucketType,\n+        \"Bucket 2 is INSERT\");\n+    assertEquals(3, insertBuckets.size(), \"Total of 3 insert buckets\");\n+\n+    Double[] weights = {0.5, 0.25, 0.25};\n+    Double[] cumulativeWeights = {0.5, 0.75, 1.0};\n+    assertInsertBuckets(weights, cumulativeWeights, insertBuckets);\n+\n+    // Now with insert split size auto tuned\n+    partitioner = getInsertPartitioner(1000 * 1024, 2400, 800 * 1024, testPartitionPath, true);\n+    insertBuckets = partitioner.getInsertBuckets(testPartitionPath);\n+\n+    assertEquals(4, partitioner.numPartitions(), \"Should have 4 partitions\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ce6e021080921c9e01e819b577ab307e7cfee85"}, "originalPosition": 106}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTQwMjE1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjoyMTo1MlrOHeb5tA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMzozNzoyMFrOHee-GA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY3NjQ2OA==", "bodyText": "would you please use foreach assertion instead of listing all buckets?", "url": "https://github.com/apache/hudi/pull/2111#discussion_r501676468", "createdAt": "2020-10-08T12:21:52Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java", "diffHunk": "@@ -286,8 +303,48 @@ public void testUpsertPartitionerWithSmallInsertHandling() throws Exception {\n         \"Bucket 3 is INSERT\");\n     assertEquals(4, insertBuckets.size(), \"Total of 4 insert buckets\");\n \n-    weights = new Double[] { 0.08, 0.31, 0.31, 0.31};\n-    cumulativeWeights = new Double[] { 0.08, 0.39, 0.69, 1.0};\n+    weights = new Double[] {0.08, 0.31, 0.31, 0.31};\n+    cumulativeWeights = new Double[] {0.08, 0.39, 0.69, 1.0};\n+    assertInsertBuckets(weights, cumulativeWeights, insertBuckets);\n+  }\n+\n+  @Test\n+  public void testInsertPartitionerWithSmallInsertHandling() throws Exception {\n+    final String testPartitionPath = \"2016/09/26\";\n+    // Inserts  .. Check updates go together & inserts subsplit, after expanding smallest file\n+    UpsertPartitioner partitioner = getInsertPartitioner(1000 * 1024, 400, 800 * 1024, testPartitionPath, false);\n+    List<InsertBucketCumulativeWeightPair> insertBuckets = partitioner.getInsertBuckets(testPartitionPath);\n+\n+    assertEquals(3, partitioner.numPartitions(), \"Should have 3 partitions\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(0).bucketType,\n+        \"Bucket 0 is INSERT\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(1).bucketType,\n+        \"Bucket 1 is INSERT\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(2).bucketType,\n+        \"Bucket 2 is INSERT\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ce6e021080921c9e01e819b577ab307e7cfee85"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTcyNjc0NA==", "bodyText": "@leesf Yes, I would like to use foreach assertion instead.", "url": "https://github.com/apache/hudi/pull/2111#discussion_r501726744", "createdAt": "2020-10-08T13:37:20Z", "author": {"login": "SteNicholas"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java", "diffHunk": "@@ -286,8 +303,48 @@ public void testUpsertPartitionerWithSmallInsertHandling() throws Exception {\n         \"Bucket 3 is INSERT\");\n     assertEquals(4, insertBuckets.size(), \"Total of 4 insert buckets\");\n \n-    weights = new Double[] { 0.08, 0.31, 0.31, 0.31};\n-    cumulativeWeights = new Double[] { 0.08, 0.39, 0.69, 1.0};\n+    weights = new Double[] {0.08, 0.31, 0.31, 0.31};\n+    cumulativeWeights = new Double[] {0.08, 0.39, 0.69, 1.0};\n+    assertInsertBuckets(weights, cumulativeWeights, insertBuckets);\n+  }\n+\n+  @Test\n+  public void testInsertPartitionerWithSmallInsertHandling() throws Exception {\n+    final String testPartitionPath = \"2016/09/26\";\n+    // Inserts  .. Check updates go together & inserts subsplit, after expanding smallest file\n+    UpsertPartitioner partitioner = getInsertPartitioner(1000 * 1024, 400, 800 * 1024, testPartitionPath, false);\n+    List<InsertBucketCumulativeWeightPair> insertBuckets = partitioner.getInsertBuckets(testPartitionPath);\n+\n+    assertEquals(3, partitioner.numPartitions(), \"Should have 3 partitions\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(0).bucketType,\n+        \"Bucket 0 is INSERT\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(1).bucketType,\n+        \"Bucket 1 is INSERT\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(2).bucketType,\n+        \"Bucket 2 is INSERT\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY3NjQ2OA=="}, "originalCommit": {"oid": "3ce6e021080921c9e01e819b577ab307e7cfee85"}, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTQwMjg3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjoyMjowNFrOHeb6Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjoyMjowNFrOHeb6Ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY3NjU3OA==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/2111#discussion_r501676578", "createdAt": "2020-10-08T12:22:04Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-spark-client/src/test/java/org/apache/hudi/table/action/commit/TestUpsertPartitioner.java", "diffHunk": "@@ -286,8 +303,48 @@ public void testUpsertPartitionerWithSmallInsertHandling() throws Exception {\n         \"Bucket 3 is INSERT\");\n     assertEquals(4, insertBuckets.size(), \"Total of 4 insert buckets\");\n \n-    weights = new Double[] { 0.08, 0.31, 0.31, 0.31};\n-    cumulativeWeights = new Double[] { 0.08, 0.39, 0.69, 1.0};\n+    weights = new Double[] {0.08, 0.31, 0.31, 0.31};\n+    cumulativeWeights = new Double[] {0.08, 0.39, 0.69, 1.0};\n+    assertInsertBuckets(weights, cumulativeWeights, insertBuckets);\n+  }\n+\n+  @Test\n+  public void testInsertPartitionerWithSmallInsertHandling() throws Exception {\n+    final String testPartitionPath = \"2016/09/26\";\n+    // Inserts  .. Check updates go together & inserts subsplit, after expanding smallest file\n+    UpsertPartitioner partitioner = getInsertPartitioner(1000 * 1024, 400, 800 * 1024, testPartitionPath, false);\n+    List<InsertBucketCumulativeWeightPair> insertBuckets = partitioner.getInsertBuckets(testPartitionPath);\n+\n+    assertEquals(3, partitioner.numPartitions(), \"Should have 3 partitions\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(0).bucketType,\n+        \"Bucket 0 is INSERT\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(1).bucketType,\n+        \"Bucket 1 is INSERT\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(2).bucketType,\n+        \"Bucket 2 is INSERT\");\n+    assertEquals(3, insertBuckets.size(), \"Total of 3 insert buckets\");\n+\n+    Double[] weights = {0.5, 0.25, 0.25};\n+    Double[] cumulativeWeights = {0.5, 0.75, 1.0};\n+    assertInsertBuckets(weights, cumulativeWeights, insertBuckets);\n+\n+    // Now with insert split size auto tuned\n+    partitioner = getInsertPartitioner(1000 * 1024, 2400, 800 * 1024, testPartitionPath, true);\n+    insertBuckets = partitioner.getInsertBuckets(testPartitionPath);\n+\n+    assertEquals(4, partitioner.numPartitions(), \"Should have 4 partitions\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(0).bucketType,\n+        \"Bucket 0 is INSERT\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(1).bucketType,\n+        \"Bucket 1 is INSERT\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(2).bucketType,\n+        \"Bucket 2 is INSERT\");\n+    assertEquals(BucketType.INSERT, partitioner.getBucketInfo(3).bucketType,\n+        \"Bucket 3 is INSERT\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ce6e021080921c9e01e819b577ab307e7cfee85"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTQwNjAxOnYy", "diffSide": "RIGHT", "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/command/ITTestHoodieSyncCommand.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjoyMjo0OVrOHeb74g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjoyMjo0OVrOHeb74g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY3NzAyNg==", "bodyText": "would you please explain why change from 200 to 100 ?", "url": "https://github.com/apache/hudi/pull/2111#discussion_r501677026", "createdAt": "2020-10-08T12:22:49Z", "author": {"login": "leesf"}, "path": "hudi-integ-test/src/test/java/org/apache/hudi/integ/command/ITTestHoodieSyncCommand.java", "diffHunk": "@@ -52,7 +52,7 @@ public void testValidateSync() throws Exception {\n         executeCommandStringInDocker(ADHOC_1_CONTAINER, HUDI_CLI_TOOL + \" --cmdfile \" + SYNC_VALIDATE_COMMANDS, true);\n \n     String expected = String.format(\"Count difference now is (count(%s) - count(%s) == %d. Catch up count is %d\",\n-        hiveTableName, hiveTableName2, 100, 200);\n+        hiveTableName, hiveTableName2, 100, 100);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ce6e021080921c9e01e819b577ab307e7cfee85"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MTQwNjU2OnYy", "diffSide": "RIGHT", "path": "hudi-spark/src/test/scala/org/apache/hudi/functional/TestMORDataSource.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjoyMjo1NlrOHeb8Mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxMjoyMjo1NlrOHeb8Mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTY3NzEwNg==", "bodyText": "ditto", "url": "https://github.com/apache/hudi/pull/2111#discussion_r501677106", "createdAt": "2020-10-08T12:22:56Z", "author": {"login": "leesf"}, "path": "hudi-spark/src/test/scala/org/apache/hudi/functional/TestMORDataSource.scala", "diffHunk": "@@ -211,7 +211,7 @@ class TestMORDataSource extends HoodieClientTestBase {\n     val hudiSnapshotDF5 = spark.read.format(\"org.apache.hudi\")\n       .option(DataSourceReadOptions.QUERY_TYPE_OPT_KEY, DataSourceReadOptions.QUERY_TYPE_SNAPSHOT_OPT_VAL)\n       .load(basePath + \"/*/*/*/*\")\n-    assertEquals(200, hudiSnapshotDF5.count())\n+    assertEquals(300, hudiSnapshotDF5.count())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3ce6e021080921c9e01e819b577ab307e7cfee85"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2NjY0MzM3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxMzoyODoxN1rOHiHzRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxMzoyODoxN1rOHiHzRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTU0MTQ0Ng==", "bodyText": "would also merge\nfor (int b = 0; b < insertBuckets; b++) {\n            bucketNumbers.add(totalBuckets);\n            recordsPerBucket.add(totalUnassignedInserts / insertBuckets);\n            BucketInfo bucketInfo = new BucketInfo();\n            bucketInfo.bucketType = BucketType.INSERT;\n            bucketInfo.partitionPath = partitionPath;\n            bucketInfo.fileIdPrefix = FSUtils.createNewFileIdPfx();\n            bucketInfoMap.put(totalBuckets, bucketInfo);\n            totalBuckets++;\n          }\n\nthe logic to the method?", "url": "https://github.com/apache/hudi/pull/2111#discussion_r505541446", "createdAt": "2020-10-15T13:28:17Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -129,6 +131,18 @@ private int addUpdateBucket(String partitionPath, String fileIdHint) {\n     return bucket;\n   }\n \n+  private int addInsertBucket(String partitionPath, String fileIdHint) {\n+    int bucket = totalBuckets;\n+    updateLocationToBucket.put(fileIdHint, bucket);\n+    BucketInfo bucketInfo = new BucketInfo();\n+    bucketInfo.bucketType = BucketType.INSERT;\n+    bucketInfo.fileIdPrefix = fileIdHint;\n+    bucketInfo.partitionPath = partitionPath;\n+    bucketInfoMap.put(totalBuckets, bucketInfo);\n+    totalBuckets++;\n+    return bucket;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd244bb28bb6826f09a612f4d75ab081d8cb749a"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE2NjY1MTkzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxMzozMDowOVrOHiH4sA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNVQxNjozMzoxNVrOHiQZUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTU0MjgzMg==", "bodyText": "maybe we would create a new FileID instead of using exist small file id @bvaradar WDYT?", "url": "https://github.com/apache/hudi/pull/2111#discussion_r505542832", "createdAt": "2020-10-15T13:30:09Z", "author": {"login": "leesf"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -160,11 +174,15 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)\n           if (recordsToAppend > 0 && totalUnassignedInserts > 0) {\n             // create a new bucket or re-use an existing bucket\n             int bucket;\n-            if (updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n+            // insert new records regardless of small file when using insert operation\n+            if (isChangingRecords(profile.getOperationType())\n+                    && updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n               bucket = updateLocationToBucket.get(smallFile.location.getFileId());\n               LOG.info(\"Assigning \" + recordsToAppend + \" inserts to existing update bucket \" + bucket);\n             } else {\n-              bucket = addUpdateBucket(partitionPath, smallFile.location.getFileId());\n+              bucket = profile.getOperationType() == null || isChangingRecords(profile.getOperationType())\n+                      ? addUpdateBucket(partitionPath, smallFile.location.getFileId())\n+                      : addInsertBucket(partitionPath, smallFile.location.getFileId());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cd244bb28bb6826f09a612f4d75ab081d8cb749a"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTY4MjI1OQ==", "bodyText": "@bvaradar @vinothchandar  WDYT? I thought using existing small file id would be better.", "url": "https://github.com/apache/hudi/pull/2111#discussion_r505682259", "createdAt": "2020-10-15T16:33:15Z", "author": {"login": "SteNicholas"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -160,11 +174,15 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)\n           if (recordsToAppend > 0 && totalUnassignedInserts > 0) {\n             // create a new bucket or re-use an existing bucket\n             int bucket;\n-            if (updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n+            // insert new records regardless of small file when using insert operation\n+            if (isChangingRecords(profile.getOperationType())\n+                    && updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n               bucket = updateLocationToBucket.get(smallFile.location.getFileId());\n               LOG.info(\"Assigning \" + recordsToAppend + \" inserts to existing update bucket \" + bucket);\n             } else {\n-              bucket = addUpdateBucket(partitionPath, smallFile.location.getFileId());\n+              bucket = profile.getOperationType() == null || isChangingRecords(profile.getOperationType())\n+                      ? addUpdateBucket(partitionPath, smallFile.location.getFileId())\n+                      : addInsertBucket(partitionPath, smallFile.location.getFileId());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTU0MjgzMg=="}, "originalCommit": {"oid": "cd244bb28bb6826f09a612f4d75ab081d8cb749a"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4ODUxMDYyOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwNTo0MTozNVrOHlbDFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwNTo0MTozNVrOHlbDFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTAwMjUxOA==", "bodyText": "would really prefer a non -static import here. makes it easier on the eyes.", "url": "https://github.com/apache/hudi/pull/2111#discussion_r509002518", "createdAt": "2020-10-21T05:41:35Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -54,6 +54,8 @@\n \n import scala.Tuple2;\n \n+import static org.apache.hudi.common.model.WriteOperationType.isChangingRecords;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0a56e1337899a4f0ceeec8dc6998f88eb5adcfd"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ4MTE4NDM1OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wN1QwNDozNzo1OVrOIPe1SA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wN1QwNDozNzo1OVrOIPe1SA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzEwNDcxMg==", "bodyText": "I don't want to add config.isRouteInsertsToNewFiles() to all 3 if else conditions and hence, have added a bigger if else for older and new code path.", "url": "https://github.com/apache/hudi/pull/2111#discussion_r553104712", "createdAt": "2021-01-07T04:37:59Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/UpsertPartitioner.java", "diffHunk": "@@ -191,14 +201,29 @@ private void assignInserts(WorkloadProfile profile, HoodieEngineContext context)\n           long recordsToAppend = Math.min((config.getParquetMaxFileSize() - smallFile.sizeBytes) / averageRecordSize,\n               totalUnassignedInserts);\n           if (recordsToAppend > 0 && totalUnassignedInserts > 0) {\n-            // create a new bucket or re-use an existing bucket\n             int bucket;\n-            if (updateLocationToBucket.containsKey(smallFile.location.getFileId())) {\n-              bucket = updateLocationToBucket.get(smallFile.location.getFileId());\n-              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to existing update bucket \" + bucket);\n-            } else {\n-              bucket = addUpdateBucket(partitionPath, smallFile.location.getFileId());\n-              LOG.info(\"Assigning \" + recordsToAppend + \" inserts to new update bucket \" + bucket);\n+            if (config.isRouteInsertsToNewFiles()) {\n+              // if insert operation, route inserts to new files regardless of small file handling.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0081537f85abb1654119bcaef6cb782441e9c600"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ4NTE2MjAwOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjo1NTozN1rOIQD-KA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMjo1NTozN1rOIQD-KA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcxMzE5Mg==", "bodyText": "rename hoodie.merge.allow.duplicate.inserts", "url": "https://github.com/apache/hudi/pull/2111#discussion_r553713192", "createdAt": "2021-01-08T02:55:37Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -133,6 +133,10 @@\n   private static final String MERGE_DATA_VALIDATION_CHECK_ENABLED = \"hoodie.merge.data.validation.enabled\";\n   private static final String DEFAULT_MERGE_DATA_VALIDATION_CHECK_ENABLED = \"false\";\n \n+  // Routes inserts to new files ignoring small file handling\n+  private static final String ROUTE_INSERTS_TO_NEW_FILES = \"hoodie.route.inserts.to.new.files\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dbb835a799e16212c9051571c1a9ee1ef080bcf4"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjA3MTY5OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieConcatHandle.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxODoxNzowNlrOIUnTyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxODoxNzowNlrOIUnTyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODQ4NjQ3Mg==", "bodyText": "@vinothchandar : Have added this new handle and tested that it works as expected. But would like to call out that any new records would just get appended. For instance, if records to be deleted (with \"_hoodie_is_deleted\" set to true) are sent via \"Insert\" operation, this handle will just append and may not recognize the deleted records as we don't do any combineAndUpdate.", "url": "https://github.com/apache/hudi/pull/2111#discussion_r558486472", "createdAt": "2021-01-15T18:17:06Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/io/storage/HoodieConcatHandle.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.io.storage;\n+\n+import org.apache.hudi.client.common.TaskContextSupplier;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieUpsertException;\n+import org.apache.hudi.io.HoodieMergeHandle;\n+import org.apache.hudi.table.HoodieTable;\n+\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+\n+/**\n+ * Handle to concatenate new records to old records w/o any merging. If Operation is set to Inserts, and if {{@link HoodieWriteConfig#isRouteInsertsToNewFiles()}}\n+ * is set, this handle will be used instead of {@link HoodieMergeHandle}\n+ */\n+public class HoodieConcatHandle<T extends HoodieRecordPayload, I, K, O> extends HoodieMergeHandle<T, I, K, O> {\n+\n+  private static final Logger LOG = LogManager.getLogger(HoodieConcatHandle.class);\n+\n+  public HoodieConcatHandle(HoodieWriteConfig config, String instantTime, HoodieTable hoodieTable, Iterator recordItr,\n+      String partitionPath, String fileId, TaskContextSupplier taskContextSupplier) {\n+    super(config, instantTime, hoodieTable, recordItr, partitionPath, fileId, taskContextSupplier);\n+  }\n+\n+  public HoodieConcatHandle(HoodieWriteConfig config, String instantTime, HoodieTable hoodieTable, Map keyToNewRecords, String partitionPath, String fileId,\n+      HoodieBaseFile dataFileToBeMerged, TaskContextSupplier taskContextSupplier) {\n+    super(config, instantTime, hoodieTable, keyToNewRecords, partitionPath, fileId, dataFileToBeMerged, taskContextSupplier);\n+  }\n+\n+  /**\n+   * Write old record as is w/o merging with incoming record.\n+   */\n+  @Override\n+  public void write(GenericRecord oldRecord) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "36d316b4406ce344d92d75b856f2d936160f3393"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNjkwNjA4OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQxMToyMjowOFrOIWKG7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNTowNToxNFrOIWtSIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDEwNTE5Nw==", "bodyText": "IMO, this could be defined as boolean type.", "url": "https://github.com/apache/hudi/pull/2111#discussion_r560105197", "createdAt": "2021-01-19T11:22:08Z", "author": {"login": "SteNicholas"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -133,6 +133,10 @@\n   private static final String MERGE_DATA_VALIDATION_CHECK_ENABLED = \"hoodie.merge.data.validation.enabled\";\n   private static final String DEFAULT_MERGE_DATA_VALIDATION_CHECK_ENABLED = \"false\";\n \n+  // Concats inserts to data files without merging\n+  private static final String MERGE_ALLOW_DUPLICATE_INSERTS = \"hoodie.merge.allow.duplicate.inserts\";\n+  private static final String DEFAULT_MERGE_ALLOW_DUPLICATE_INSERTS = \"false\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b016c69f860f8cdf2bafca24ce5e3f70b84032e"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY4MTUwNg==", "bodyText": "just following the convention we have been using in hoodie so far.", "url": "https://github.com/apache/hudi/pull/2111#discussion_r560681506", "createdAt": "2021-01-20T05:05:14Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -133,6 +133,10 @@\n   private static final String MERGE_DATA_VALIDATION_CHECK_ENABLED = \"hoodie.merge.data.validation.enabled\";\n   private static final String DEFAULT_MERGE_DATA_VALIDATION_CHECK_ENABLED = \"false\";\n \n+  // Concats inserts to data files without merging\n+  private static final String MERGE_ALLOW_DUPLICATE_INSERTS = \"hoodie.merge.allow.duplicate.inserts\";\n+  private static final String DEFAULT_MERGE_ALLOW_DUPLICATE_INSERTS = \"false\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDEwNTE5Nw=="}, "originalCommit": {"oid": "0b016c69f860f8cdf2bafca24ce5e3f70b84032e"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyOTkyMzM3OnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQyMzo1MDoxMFrOIWm_Pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQyMzo1MDoxMFrOIWm_Pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDU3ODM2Nw==", "bodyText": "allowDuplicateInserts()", "url": "https://github.com/apache/hudi/pull/2111#discussion_r560578367", "createdAt": "2021-01-19T23:50:10Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -330,6 +334,10 @@ public boolean isMergeDataValidationCheckEnabled() {\n     return Boolean.parseBoolean(props.getProperty(MERGE_DATA_VALIDATION_CHECK_ENABLED));\n   }\n \n+  public boolean isMergeAllowDuplicateInserts() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b016c69f860f8cdf2bafca24ce5e3f70b84032e"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUzMDYxNTAzOnYy", "diffSide": "RIGHT", "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNTowMzoyNVrOIWtQOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNToyMjozNFrOIWtmJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY4MTAxNg==", "bodyText": "@vinothchandar : does the config name looks ok? somehow I feel it does not exactly convey the actual intent, but talks about the effect. For instance,\nsomething like \"hoodie.concat.inserts.without.merge.on.insert\" or \"hoodie.concat.records.on.insert\" etc.", "url": "https://github.com/apache/hudi/pull/2111#discussion_r560681016", "createdAt": "2021-01-20T05:03:25Z", "author": {"login": "nsivabalan"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -133,6 +133,10 @@\n   private static final String MERGE_DATA_VALIDATION_CHECK_ENABLED = \"hoodie.merge.data.validation.enabled\";\n   private static final String DEFAULT_MERGE_DATA_VALIDATION_CHECK_ENABLED = \"false\";\n \n+  // Concats inserts to data files without merging\n+  private static final String MERGE_ALLOW_DUPLICATE_INSERTS = \"hoodie.merge.allow.duplicate.inserts\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0b016c69f860f8cdf2bafca24ce5e3f70b84032e"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY4NjYyOQ==", "bodyText": "first one is too long IMO. I think hoodie.merge.allow.duplicates.on.insert is a better name (I added the .on) given it explains to the user what the impact is. Whether we concat or prepend in implementation, thats not very useful for the end user", "url": "https://github.com/apache/hudi/pull/2111#discussion_r560686629", "createdAt": "2021-01-20T05:22:34Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieWriteConfig.java", "diffHunk": "@@ -133,6 +133,10 @@\n   private static final String MERGE_DATA_VALIDATION_CHECK_ENABLED = \"hoodie.merge.data.validation.enabled\";\n   private static final String DEFAULT_MERGE_DATA_VALIDATION_CHECK_ENABLED = \"false\";\n \n+  // Concats inserts to data files without merging\n+  private static final String MERGE_ALLOW_DUPLICATE_INSERTS = \"hoodie.merge.allow.duplicate.inserts\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDY4MTAxNg=="}, "originalCommit": {"oid": "0b016c69f860f8cdf2bafca24ce5e3f70b84032e"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4296, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}