{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI0MjQzODY4", "number": 2263, "title": "[HUDI-1075] Implement simple clustering strategies to create and run ClusteringPlan", "bodyText": "What is the purpose of the pull request\nImplement simple clustering strategies to create and run ClusteringPlan\nBrief change log\n\nAdd simple strategy to to schedule clustering and create ClusteringPlan. Initial strategy can limit partitions and bound the amount of data considered for clustering\nAdd simple strategy to run the clustering plan and create 'replacecommit'. Initial strategy is implemented by using BulkInsert partitioner.\nThis can work for both COW and MOR tables. Clustering also does 'compaction' if there are log files. We ensure compaction and clustering do not work on same file groups.\nAdded basic support for sorting by custom column(s). I may refactor this little bit.\nFew other things to pay attention:\n\n\nAfter clustering all new records have a new commit time. I'm trying to see if it's possible to preserve commit_time from original file to support incremental queries.\nSchema is not serializable. So I pass around schema string in some cases. Let me know if there is a better way to restructure code.\nInitial strategy is implemented based on requirements for Uber. But the interfaces are flexible to allow other implementation. But please double check if I made bad assumptions in any of the strategies.\n\nVerify this pull request\n\nManually tested 'inline clustering' end-to-end in local docker setup.\nI will be working on adding lot more unit tests (Publishing this PR to get high level feedback).\nPlan to add integ-tests using test-suite as part of separate PR.\n\nCommitter checklist\n\n\n Has a corresponding JIRA in PR title & commit\n\n\n Commit message is descriptive of the change\n\n\n CI is green\n\n\n Necessary doc changes done or have another open PR\n\n\n For large changes, please consider breaking it into sub-tasks under an umbrella JIRA.", "createdAt": "2020-11-19T21:30:39Z", "url": "https://github.com/apache/hudi/pull/2263", "merged": true, "mergeCommit": {"oid": "959afb8ba4cadf0fe09ddd6d67f9bf38bea13050"}, "closed": true, "closedAt": "2020-12-22T03:18:19Z", "author": {"login": "satishkotha"}, "timelineItems": {"totalCount": 49, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdehXTYgFqTUzNTg2NDE4Ng==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdogVzbgBqjQxMzgyMTU5ODQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1ODY0MTg2", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535864186", "createdAt": "2020-11-21T01:07:16Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMTowNzoxNlrOH3lTrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMTowNzoxNlrOH3lTrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0NDk3NQ==", "bodyText": "Should this be a public method ? Rolling back inflight could probably remain private ? Or is this for the CLI ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528044975", "createdAt": "2020-11-21T01:07:16Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/client/AbstractHoodieWriteClient.java", "diffHunk": "@@ -726,6 +751,54 @@ private void rollbackPendingCommits() {\n     return compactionInstantTimeOpt;\n   }\n \n+  /**\n+   * Schedules a new clustering instant.\n+   *\n+   * @param extraMetadata Extra Metadata to be stored\n+   */\n+  public Option<String> scheduleClustering(Option<Map<String, String>> extraMetadata) throws HoodieIOException {\n+    String instantTime = HoodieActiveTimeline.createNewInstantTime();\n+    return scheduleClusteringAtInstant(instantTime, extraMetadata) ? Option.of(instantTime) : Option.empty();\n+  }\n+\n+  /**\n+   * Schedules a new clustering instant with passed-in instant time.\n+   *\n+   * @param instantTime clustering Instant Time\n+   * @param extraMetadata Extra Metadata to be stored\n+   */\n+  public boolean scheduleClusteringAtInstant(String instantTime, Option<Map<String, String>> extraMetadata) throws HoodieIOException {\n+    LOG.info(\"Scheduling clustering at instant time :\" + instantTime);\n+    Option<HoodieClusteringPlan> plan = createTable(config, hadoopConf)\n+        .scheduleClustering(context, instantTime, extraMetadata);\n+    return plan.isPresent();\n+  }\n+\n+  /**\n+   * Ensures clustering instant is in expected state and performs clustering for the plan stored in metadata.\n+   *\n+   * @param clusteringInstant Clustering Instant Time\n+   * @return Collection of Write Status\n+   */\n+  protected abstract HoodieWriteMetadata<O> cluster(String clusteringInstant, boolean shouldComplete);\n+\n+  /**\n+   * Executes a clustering plan on a table, serially before or after an insert/upsert action.\n+   */\n+  protected Option<String> inlineCluster(Option<Map<String, String>> extraMetadata) {\n+    Option<String> clusteringInstantOpt = scheduleClustering(extraMetadata);\n+    clusteringInstantOpt.ifPresent(clusteringInstant -> {\n+      // inline cluster should auto commit as the user is never given control\n+      cluster(clusteringInstant, true);\n+    });\n+    return clusteringInstantOpt;\n+  }\n+\n+  public void rollbackInflightClustering(HoodieInstant inflightInstant, HoodieTable<T, I, K, O> table) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 110}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1ODY1OTc0", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535865974", "createdAt": "2020-11-21T01:18:21Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToxODoyMVrOH3lbMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToxODoyMVrOH3lbMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0Njg5OA==", "bodyText": "Looks like the execution framework details are in the configs. Would be nice to have a wrapper around reflection utils that can pick the right execution engine based class but keep the strategy class name generic...not sure if it's too late for that..", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528046898", "createdAt": "2020-11-21T01:18:21Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1ODY2MDUy", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535866052", "createdAt": "2020-11-21T01:18:47Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToxODo0N1rOH3lbcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToxODo0N1rOH3lbcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0Njk2MQ==", "bodyText": "extreme nit : s/a/an :)", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528046961", "createdAt": "2020-11-21T01:18:47Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits a inline compaction will be run", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1ODY2MzQ0", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535866344", "createdAt": "2020-11-21T01:20:45Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToyMDo0NVrOH3lctg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToyMDo0NVrOH3lctg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0NzI4Ng==", "bodyText": "Is this in MB ? Can we rename this to withClusteringMaxGroupSizeInMB ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528047286", "createdAt": "2020-11-21T01:20:45Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits a inline compaction will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);\n+\n+  // Each clustering operation can create multiple groups. Total amount of data processed by clustering operation\n+  // is defined by below two properties (CLUSTERING_MAX_GROUP_SIZE * CLUSTERING_MAX_NUM_GROUPS).\n+  // Max amount of data to be included in one group\n+  public static final String CLUSTERING_MAX_GROUP_SIZE = \"hoodie.clustering.max.group.size\";\n+  public static final String DEFAULT_CLUSTERING_MAX_GROUP_SIZE = String.valueOf(2 * 1024 * 1024 * 1024L);\n+\n+  public static final String CLUSTERING_MAX_NUM_GROUPS = \"hoodie.clustering.max.num.groups\";\n+  public static final String DEFAULT_CLUSTERING_MAX_NUM_GROUPS = \"1\";\n+\n+  // Each group can produce 'N' (CLUSTERING_MAX_GROUP_SIZE/CLUSTERING_TARGET_FILE_SIZE) output file groups.\n+  public static final String CLUSTERING_TARGET_FILE_SIZE = \"hoodie.clustering.target.file.size\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_FILE_SIZE = String.valueOf(1 * 1024 * 1024 * 1024L);\n+\n+  // Any strategy specific params can be saved with this prefix\n+  public static final String CLUSTERING_STRATEGY_PARAM_PREFIX = \"hoodie.clustering.strategy.param.\";\n+\n+  // constants related to clustering that may be used by more than 1 strategy.\n+  public static final String SORT_COLUMNS_PROPERTY = HoodieClusteringConfig.CLUSTERING_STRATEGY_PARAM_PREFIX + \"sort.columns\";\n+\n+  public HoodieClusteringConfig(Properties props) {\n+    super(props);\n+  }\n+\n+  public static Builder newBuilder() {\n+    return new Builder();\n+  }\n+\n+  public static class Builder {\n+\n+    private final Properties props = new Properties();\n+\n+    public Builder fromFile(File propertiesFile) throws IOException {\n+      try (FileReader reader = new FileReader(propertiesFile)) {\n+        this.props.load(reader);\n+        return this;\n+      }\n+    }\n+\n+    public Builder withScheduleClusteringStrategyClass(String clusteringStrategyClass) {\n+      props.setProperty(SCHEDULE_CLUSTERING_STRATEGY_CLASS, clusteringStrategyClass);\n+      return this;\n+    }\n+\n+    public Builder withRunClusteringStrategyClass(String runClusteringStrategyClass) {\n+      props.setProperty(RUN_CLUSTERING_STRATEGY_CLASS, runClusteringStrategyClass);\n+      return this;\n+    }\n+\n+    public Builder withClusteringTargetPartitions(String clusteringTargetPartitions) {\n+      props.setProperty(CLUSTERING_TARGET_PARTITIONS, clusteringTargetPartitions);\n+      return this;\n+    }\n+\n+    public Builder withClusteringMaxGroupSize(long clusteringMaxGroupSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 104}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1ODY2Mzg4", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535866388", "createdAt": "2020-11-21T01:21:06Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToyMTowNlrOH3lc8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToyMTowNlrOH3lc8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0NzM0NA==", "bodyText": "Same this, if in MB, add it to the name or java docs ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528047344", "createdAt": "2020-11-21T01:21:06Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits a inline compaction will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);\n+\n+  // Each clustering operation can create multiple groups. Total amount of data processed by clustering operation\n+  // is defined by below two properties (CLUSTERING_MAX_GROUP_SIZE * CLUSTERING_MAX_NUM_GROUPS).\n+  // Max amount of data to be included in one group\n+  public static final String CLUSTERING_MAX_GROUP_SIZE = \"hoodie.clustering.max.group.size\";\n+  public static final String DEFAULT_CLUSTERING_MAX_GROUP_SIZE = String.valueOf(2 * 1024 * 1024 * 1024L);\n+\n+  public static final String CLUSTERING_MAX_NUM_GROUPS = \"hoodie.clustering.max.num.groups\";\n+  public static final String DEFAULT_CLUSTERING_MAX_NUM_GROUPS = \"1\";\n+\n+  // Each group can produce 'N' (CLUSTERING_MAX_GROUP_SIZE/CLUSTERING_TARGET_FILE_SIZE) output file groups.\n+  public static final String CLUSTERING_TARGET_FILE_SIZE = \"hoodie.clustering.target.file.size\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_FILE_SIZE = String.valueOf(1 * 1024 * 1024 * 1024L);\n+\n+  // Any strategy specific params can be saved with this prefix\n+  public static final String CLUSTERING_STRATEGY_PARAM_PREFIX = \"hoodie.clustering.strategy.param.\";\n+\n+  // constants related to clustering that may be used by more than 1 strategy.\n+  public static final String SORT_COLUMNS_PROPERTY = HoodieClusteringConfig.CLUSTERING_STRATEGY_PARAM_PREFIX + \"sort.columns\";\n+\n+  public HoodieClusteringConfig(Properties props) {\n+    super(props);\n+  }\n+\n+  public static Builder newBuilder() {\n+    return new Builder();\n+  }\n+\n+  public static class Builder {\n+\n+    private final Properties props = new Properties();\n+\n+    public Builder fromFile(File propertiesFile) throws IOException {\n+      try (FileReader reader = new FileReader(propertiesFile)) {\n+        this.props.load(reader);\n+        return this;\n+      }\n+    }\n+\n+    public Builder withScheduleClusteringStrategyClass(String clusteringStrategyClass) {\n+      props.setProperty(SCHEDULE_CLUSTERING_STRATEGY_CLASS, clusteringStrategyClass);\n+      return this;\n+    }\n+\n+    public Builder withRunClusteringStrategyClass(String runClusteringStrategyClass) {\n+      props.setProperty(RUN_CLUSTERING_STRATEGY_CLASS, runClusteringStrategyClass);\n+      return this;\n+    }\n+\n+    public Builder withClusteringTargetPartitions(String clusteringTargetPartitions) {\n+      props.setProperty(CLUSTERING_TARGET_PARTITIONS, clusteringTargetPartitions);\n+      return this;\n+    }\n+\n+    public Builder withClusteringMaxGroupSize(long clusteringMaxGroupSize) {\n+      props.setProperty(CLUSTERING_MAX_GROUP_SIZE, String.valueOf(clusteringMaxGroupSize));\n+      return this;\n+    }\n+\n+    public Builder withClusteringMaxNumGroups(int maxNumGroups) {\n+      props.setProperty(CLUSTERING_MAX_NUM_GROUPS, String.valueOf(maxNumGroups));\n+      return this;\n+    }\n+\n+    public Builder withClusteringTargetFileSize(long targetFileSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 114}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1ODY3MzM4", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535867338", "createdAt": "2020-11-21T01:27:24Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToyNzoyNFrOH3lgqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToyNzoyNFrOH3lgqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODI5Nw==", "bodyText": "Should this be named \"cluster\" given the write client also has \"cluster\" ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528048297", "createdAt": "2020-11-21T01:27:24Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/HoodieTable.java", "diffHunk": "@@ -326,6 +327,28 @@ public HoodieActiveTimeline getActiveTimeline() {\n   public abstract HoodieWriteMetadata<O> compact(HoodieEngineContext context,\n                                               String compactionInstantTime);\n \n+\n+  /**\n+   * Schedule clustering for the instant time.\n+   *\n+   * @param context HoodieEngineContext\n+   * @param instantTime Instant Time for scheduling clustering\n+   * @param extraMetadata additional metadata to write into plan\n+   * @return HoodieClusteringPlan, if there is enough data for clustering.\n+   */\n+  public abstract Option<HoodieClusteringPlan> scheduleClustering(HoodieEngineContext context,\n+                                                                  String instantTime,\n+                                                                  Option<Map<String, String>> extraMetadata);\n+\n+  /**\n+   * Run Clustering on the table. Clustering re-arranges the data so that it is optimized for data access.\n+   *\n+   * @param context HoodieEngineContext\n+   * @param clusteringInstantTime Instant Time\n+   */\n+  public abstract HoodieWriteMetadata<O> clustering(HoodieEngineContext context,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 31}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTE3Njgw", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535917680", "createdAt": "2020-11-21T06:13:29Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjoxMzoyOVrOH3navQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjoxMzoyOVrOH3navQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA3OTU0OQ==", "bodyText": "Better name for \"maxDataInGroup\" ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528079549", "createdAt": "2020-11-21T06:13:29Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> extends ScheduleClusteringStrategy<T,I,K,O> {\n+  private static final Logger LOG = LogManager.getLogger(PartitionAwareScheduleClusteringStrategy.class);\n+\n+  public PartitionAwareScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  /**\n+   * Create Clustering group based on files eligible for clustering in the partition.\n+   */\n+  protected abstract Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath,\n+                                                                                     List<FileSlice> fileSlices,\n+                                                                                     long maxDataInGroup);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTI4MDk5", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535928099", "createdAt": "2020-11-21T06:21:23Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjoyMToyM1rOH3ndxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjoyMToyM1rOH3ndxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA4MDMyNA==", "bodyText": "Can you expand on why we need the \"schema\" parameter ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528080324", "createdAt": "2020-11-21T06:21:23Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/RunClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Map;\n+\n+/**\n+ * Pluggable implementation for writing data into new file groups based on ClusteringPlan.\n+ */\n+public abstract class RunClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(RunClusteringStrategy.class);\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public RunClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Run clustering to write inputRecords into new files as defined by rules in strategy parameters. The number of new\n+   * file groups created is bounded by numOutputGroups.\n+   * Note that commit is not done as part of strategy. commit is callers responsibility.\n+   */\n+  public abstract O performClustering(final I inputRecords, final int numOutputGroups, final String instantTime,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTMyNDI5", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535932429", "createdAt": "2020-11-21T06:24:38Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjoyNDozOFrOH3neuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjoyNDozOFrOH3neuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA4MDU2OA==", "bodyText": "Could we use StringUtils.EMPTY or create a constant ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528080568", "createdAt": "2020-11-21T06:24:38Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_IO_WRITE_MB = \"TOTAL_IO_WRITE_MB\";\n+  public static final String TOTAL_IO_MB = \"TOTAL_IO_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size\n+   *\n+   * are not eligible for clustering\n+   */\n+  protected List<FileSlice> getFileSlicesEligibleForClustering(String partition) {\n+    Set<HoodieFileGroupId> fgIdsInPendingCompactionAndClustering = ((SyncableFileSystemView) getHoodieTable().getSliceView()).getPendingCompactionOperations()\n+        .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+        .collect(Collectors.toSet());\n+    fgIdsInPendingCompactionAndClustering.addAll(ClusteringUtils.getAllFileGroupsInPendingClusteringPlans(getHoodieTable().getMetaClient()).keySet());\n+\n+    return hoodieTable.getSliceView().getLatestFileSlices(partition)\n+        // file ids already in clustering are not eligible\n+        .filter(slice -> !fgIdsInPendingCompactionAndClustering.contains(slice.getFileGroupId()))\n+        // files that have basefile size larger than clustering target file size are not eligible (Note that compaction can merge any updates)\n+        .filter(slice -> slice.getBaseFile().map(HoodieBaseFile::getFileSize).orElse(0L) < writeConfig.getClusteringTargetFileSize())\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Get parameters specific to strategy. These parameters are passed from 'schedule clustering' step to\n+   * 'run clustering' step. 'run clustering' step is typically async. So these params help with passing any required\n+   * context from schedule to run step.\n+   */\n+  protected abstract Map<String, String> getStrategyParams();\n+\n+  /**\n+   * Returns any specific parameters to be stored as part of clustering metadata.\n+   */\n+  protected Map<String, String> getExtraMetadata() {\n+    return Collections.emptyMap();\n+  }\n+\n+  /**\n+   * Version to support future changes for plan.\n+   */\n+  protected int getPlanVersion() {\n+    return CLUSTERING_PLAN_VERSION_1;\n+  }\n+\n+  /**\n+   * Transform {@link FileSlice} to {@link HoodieSliceInfo}.\n+   */\n+  protected List<HoodieSliceInfo> getFileSliceInfo(List<FileSlice> slices) {\n+    return slices.stream().map(slice -> new HoodieSliceInfo().newBuilder()\n+        .setPartitionPath(slice.getPartitionPath())\n+        .setFileId(slice.getFileId())\n+        .setDataFilePath(slice.getBaseFile().map(BaseFile::getPath).orElse(\"\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 128}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTQ1MjQ4", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535945248", "createdAt": "2020-11-21T06:43:12Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo0MzoxM1rOH3n72w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo0MzoxM1rOH3n72w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA4ODAyNw==", "bodyText": "Could we split this -> https://github.com/apache/hudi/blob/master/hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/strategy/CompactionStrategy.java into IStrategy that should have common methods expected from all strategies such as buildMetrics, getPlanVersion..and then ScheduleClusteringStrategy can extend it ?\nAfter that, let's move the buildMetrics into a utils class given it's the same across both compaction and clustering right now ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528088027", "createdAt": "2020-11-21T06:43:13Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 50}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTQ1NDc4", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535945478", "createdAt": "2020-11-21T06:47:21Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo0NzoyMVrOH3oIOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo0NzoyMVrOH3oIOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5MTE5Mw==", "bodyText": "nit : updateIndex -> update index, lets follow one pattern", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528091193", "createdAt": "2020-11-21T06:47:21Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/AbstractBulkInsertHelper.java", "diffHunk": "@@ -27,8 +27,20 @@\n \n public abstract class AbstractBulkInsertHelper<T extends HoodieRecordPayload, I, K, O, R> {\n \n+  /**\n+   * Mark instant as inflight, write input records, updateIndex and return result.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTQ1ODI5", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535945829", "createdAt": "2020-11-21T06:53:55Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo1Mzo1NVrOH3oa5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo1Mzo1NVrOH3oa5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5NTk3Mg==", "bodyText": "Should this be a REPLACE_ACTION instead of COMPACTION_ACTION ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528095972", "createdAt": "2020-11-21T06:53:55Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/SparkRDDWriteClient.java", "diffHunk": "@@ -296,6 +298,57 @@ protected void completeCompaction(HoodieCommitMetadata metadata, JavaRDD<WriteSt\n     return statuses;\n   }\n \n+  @Override\n+  protected HoodieWriteMetadata<JavaRDD<WriteStatus>> cluster(String clusteringInstant, boolean shouldComplete) {\n+    HoodieSparkTable<T> table = HoodieSparkTable.create(config, context);\n+    HoodieTimeline pendingClusteringTimeline = table.getActiveTimeline().filterPendingReplaceTimeline();\n+    HoodieInstant inflightInstant = HoodieTimeline.getReplaceCommitInflightInstant(clusteringInstant);\n+    if (pendingClusteringTimeline.containsInstant(inflightInstant)) {\n+      rollbackInflightClustering(inflightInstant, table);\n+      table.getMetaClient().reloadActiveTimeline();\n+    }\n+    clusteringTimer = metrics.getClusteringCtx();\n+    LOG.info(\"Starting clustering at \" + clusteringInstant);\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> clusteringMetadata = table.clustering(context, clusteringInstant);\n+    JavaRDD<WriteStatus> statuses = clusteringMetadata.getWriteStatuses();\n+    if (shouldComplete && clusteringMetadata.getCommitMetadata().isPresent()) {\n+      completeClustering((HoodieReplaceCommitMetadata) clusteringMetadata.getCommitMetadata().get(), statuses, table, clusteringInstant);\n+    }\n+    return clusteringMetadata;\n+  }\n+\n+  protected void completeClustering(HoodieReplaceCommitMetadata metadata, JavaRDD<WriteStatus> writeStatuses,\n+                                    HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n+                                    String clusteringCommitTime) {\n+\n+    List<HoodieWriteStat> writeStats = writeStatuses.map(WriteStatus::getStat).collect();\n+    if (!writeStatuses.filter(WriteStatus::hasErrors).isEmpty()) {\n+      throw new HoodieClusteringException(\"Clustering failed to write to files:\"\n+          + writeStatuses.filter(WriteStatus::hasErrors).map(WriteStatus::getFileId).collect());\n+    }\n+    finalizeWrite(table, clusteringCommitTime, writeStats);\n+    try {\n+      LOG.info(\"Committing Clustering \" + clusteringCommitTime + \". Finished with result \" + metadata);\n+      table.getActiveTimeline().transitionReplaceInflightToComplete(\n+          HoodieTimeline.getReplaceCommitInflightInstant(clusteringCommitTime),\n+          Option.of(metadata.toJsonString().getBytes(StandardCharsets.UTF_8)));\n+    } catch (IOException e) {\n+      throw new HoodieClusteringException(\"unable to transition clustering inflight to complete: \" + clusteringCommitTime,  e);\n+    }\n+\n+    if (clusteringTimer != null) {\n+      long durationInMs = metrics.getDurationInMs(clusteringTimer.stop());\n+      try {\n+        metrics.updateCommitMetrics(HoodieActiveTimeline.COMMIT_FORMATTER.parse(clusteringCommitTime).getTime(),\n+            durationInMs, metadata, HoodieActiveTimeline.COMPACTION_ACTION);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 88}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTQ1ODQ4", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535945848", "createdAt": "2020-11-21T06:54:20Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo1NDoyMFrOH3ocEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo1NDoyMFrOH3ocEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5NjI3NA==", "bodyText": "Why is this not under the strategy package ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528096274", "createdAt": "2020-11-21T06:54:20Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/SparkBulkInsertBasedRunClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.run;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.execution.bulkinsert.RDDCustomColumnsSortPartitioner;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.SparkBulkInsertHelper;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Uses bulk_insert to write data into new files.\n+ */\n+public class SparkBulkInsertBasedRunClusteringStrategy<T extends HoodieRecordPayload<T>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTQ1OTU0", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535945954", "createdAt": "2020-11-21T06:56:30Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo1NjozMFrOH3oiNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo1NjozMFrOH3oiNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5Nzg0Ng==", "bodyText": "same question, should this go under strategy package ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528097846", "createdAt": "2020-11-21T06:56:30Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/schedule/SparkBoundedDayBasedScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.schedule;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.action.cluster.strategy.PartitionAwareScheduleClusteringStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Limits amount of data per clustering operation.\n+ */\n+public class SparkBoundedDayBasedScheduleClusteringStrategy<T extends HoodieRecordPayload<T>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTQ2MDAx", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535946001", "createdAt": "2020-11-21T06:57:30Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo1NzozMFrOH3olBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo1NzozMFrOH3olBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5ODU2NA==", "bodyText": "Could you explain this change please ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528098564", "createdAt": "2020-11-21T06:57:30Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/SparkLazyInsertIterable.java", "diffHunk": "@@ -34,14 +35,18 @@\n \n public class SparkLazyInsertIterable<T extends HoodieRecordPayload> extends HoodieLazyInsertIterable<T> {\n \n+  private boolean addMetadataFields;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTQ2MDEy", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535946012", "createdAt": "2020-11-21T06:57:39Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo1NzozOVrOH3olUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNjo1NzozOVrOH3olUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA5ODY0MQ==", "bodyText": "same", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528098641", "createdAt": "2020-11-21T06:57:39Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/BulkInsertMapFunction.java", "diffHunk": "@@ -41,20 +41,22 @@\n   private HoodieWriteConfig config;\n   private HoodieTable hoodieTable;\n   private List<String> fileIDPrefixes;\n+  private boolean addMetadataFields;\n \n   public BulkInsertMapFunction(String instantTime, boolean areRecordsSorted,\n                                HoodieWriteConfig config, HoodieTable hoodieTable,\n-                               List<String> fileIDPrefixes) {\n+                               List<String> fileIDPrefixes, boolean addMetadataFields) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 9}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTQ2NTk5", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535946599", "createdAt": "2020-11-21T07:09:39Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzowOTo0MFrOH3pGug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzowOTo0MFrOH3pGug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODEwNzE5NA==", "bodyText": "This runs the clustering groups in sequence, is that what is expected ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528107194", "createdAt": "2020-11-21T07:09:40Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // read rdd from input groups in plan\n+    JavaRDD<WriteStatus> writeStatuses = clusteringPlan.getInputGroups().stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 88}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTQ2NzA3", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535946707", "createdAt": "2020-11-21T07:11:48Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzoxMTo0OFrOH3pMqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzoxMTo0OFrOH3pMqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODEwODcxMw==", "bodyText": "Why do we need this reload ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528108713", "createdAt": "2020-11-21T07:11:48Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 84}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTQ2ODAz", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535946803", "createdAt": "2020-11-21T07:13:48Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzoxMzo0OFrOH3pRsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzoxMzo0OFrOH3pRsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODExMDAwMA==", "bodyText": "Java doc please", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528110000", "createdAt": "2020-11-21T07:13:48Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.SpillableMapUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.Iterator;\n+\n+public class HoodieFileSliceReader implements Iterable<HoodieRecord<? extends HoodieRecordPayload>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTQ2OTUx", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535946951", "createdAt": "2020-11-21T07:16:41Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzoxNjo0MVrOH3pZ8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzoxNjo0MVrOH3pZ8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODExMjExMw==", "bodyText": "At a high level, what is the use-case when we would need to cluster every N commits ? For compaction it makes sense..", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528112113", "createdAt": "2020-11-21T07:16:41Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkScheduleClusteringActionExecutor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.cluster.strategy.ScheduleClusteringStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.Map;\n+\n+@SuppressWarnings(\"checkstyle:LineLength\")\n+public class SparkScheduleClusteringActionExecutor<T extends HoodieRecordPayload> extends\n+    BaseScheduleClusteringActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkScheduleClusteringActionExecutor.class);\n+\n+  public SparkScheduleClusteringActionExecutor(HoodieEngineContext context,\n+                                               HoodieWriteConfig config,\n+                                               HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n+                                               String instantTime,\n+                                               Option<Map<String, String>> extraMetadata) {\n+    super(context, config, table, instantTime, extraMetadata);\n+  }\n+\n+  @Override\n+  protected Option<HoodieClusteringPlan> scheduleClustering() {\n+    LOG.info(\"Checking if clustering needs to be run on \" + config.getBasePath());\n+    Option<HoodieInstant> lastClusteringInstant = table.getActiveTimeline().getCompletedReplaceTimeline().lastInstant();\n+\n+    int commitsSinceLastClustering = table.getActiveTimeline().getCommitsTimeline().filterCompletedInstants()\n+        .findInstantsAfter(lastClusteringInstant.map(HoodieInstant::getTimestamp).orElse(\"0\"), Integer.MAX_VALUE)\n+        .countInstants();\n+    if (config.getInlineClusterMaxCommits() > commitsSinceLastClustering) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 61}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTQ3MTg4", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535947188", "createdAt": "2020-11-21T07:21:32Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzoyMTozMlrOH3pnkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzoyMTozMlrOH3pnkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODExNTYwMA==", "bodyText": "So if there are N clustering groups consisting of M file groups that are being converted into O file groups, we will have N different RDD's. There are some performance limitation of unioning multiple different RDDs from what I remember. Can you run a simple test a) take 10000 records, parallelize them with jsc to create 1 RDD and then write them out to a file b) take 10000 records, create 1 RDD for each record and then union them and write them out to a single file ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528115600", "createdAt": "2020-11-21T07:21:32Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // read rdd from input groups in plan\n+    JavaRDD<WriteStatus> writeStatuses = clusteringPlan.getInputGroups().stream()\n+        .map(inputGroup -> runClusteringForGroup(inputGroup, clusteringPlan.getStrategy().getStrategyParams()))\n+        .reduce((rdd1, rdd2) -> engineContext.union(rdd1, rdd2)).orElse(engineContext.emptyRDD());\n+    if (writeStatuses.isEmpty()) {\n+      throw new HoodieClusteringException(\"Clustering plan produced 0 WriteStatus for \" + instantTime + \" #groups: \" + clusteringPlan.getInputGroups().size());\n+    }\n+    // merge all write status\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> writeMetadata = buildWriteMetadata(writeStatuses);\n+    updateIndexAndCommitIfNeeded(writeStatuses, writeMetadata);\n+    if (!writeMetadata.getCommitMetadata().isPresent()) {\n+      HoodieCommitMetadata commitMetadata = CommitUtils.buildMetadata(writeStatuses.map(WriteStatus::getStat).collect(), writeMetadata.getPartitionToReplaceFileIds(),\n+          extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());\n+      writeMetadata.setCommitMetadata(Option.of(commitMetadata));\n+    }\n+    return writeMetadata;\n+  }\n+\n+  private JavaRDD<WriteStatus> runClusteringForGroup(HoodieClusteringGroup clusteringGroup, Map<String, String> strategyParams) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 105}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTQ3MjM3", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535947237", "createdAt": "2020-11-21T07:22:49Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzoyMjo0OVrOH3prJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwNzoyMjo0OVrOH3prJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODExNjUxNg==", "bodyText": "Do we need this level of file readers in this part of the code ? Can we just call compactor.compact(context, compactionPlan, table, config, instantTime) ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528116516", "createdAt": "2020-11-21T07:22:49Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // read rdd from input groups in plan\n+    JavaRDD<WriteStatus> writeStatuses = clusteringPlan.getInputGroups().stream()\n+        .map(inputGroup -> runClusteringForGroup(inputGroup, clusteringPlan.getStrategy().getStrategyParams()))\n+        .reduce((rdd1, rdd2) -> engineContext.union(rdd1, rdd2)).orElse(engineContext.emptyRDD());\n+    if (writeStatuses.isEmpty()) {\n+      throw new HoodieClusteringException(\"Clustering plan produced 0 WriteStatus for \" + instantTime + \" #groups: \" + clusteringPlan.getInputGroups().size());\n+    }\n+    // merge all write status\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> writeMetadata = buildWriteMetadata(writeStatuses);\n+    updateIndexAndCommitIfNeeded(writeStatuses, writeMetadata);\n+    if (!writeMetadata.getCommitMetadata().isPresent()) {\n+      HoodieCommitMetadata commitMetadata = CommitUtils.buildMetadata(writeStatuses.map(WriteStatus::getStat).collect(), writeMetadata.getPartitionToReplaceFileIds(),\n+          extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());\n+      writeMetadata.setCommitMetadata(Option.of(commitMetadata));\n+    }\n+    return writeMetadata;\n+  }\n+\n+  private JavaRDD<WriteStatus> runClusteringForGroup(HoodieClusteringGroup clusteringGroup, Map<String, String> strategyParams) {\n+    JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+    JavaRDD<HoodieRecord<? extends HoodieRecordPayload>> inputRecords = jsc.parallelize(clusteringGroup.getSlices(), clusteringGroup.getSlices().size()).map(sliceInfo -> {\n+      long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config.getProps());\n+      LOG.info(\"MaxMemoryPerCompaction run as part of clustering => \" + maxMemoryPerCompaction);\n+      try {\n+        Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n+        HoodieFileReader<? extends IndexedRecord> baseFileReader = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), new Path(sliceInfo.getDataFilePath()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 112}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTQ3MjQ4", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535947248", "createdAt": "2020-11-21T07:23:11Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTcyOTIy", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535972922", "createdAt": "2020-11-21T15:16:05Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxNToxNjowNVrOH3vMvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxNToxNjowNVrOH3vMvg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIwNzAzOA==", "bodyText": "hello , i am interesting  about the scenario to limit the partition num of clustering. does cluster all partitions have some problem?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528207038", "createdAt": "2020-11-21T15:16:05Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits a inline compaction will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTczNTA0", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535973504", "createdAt": "2020-11-21T15:24:35Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxNToyNDozNlrOH3vP8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxNToyNDozNlrOH3vP8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIwNzg1OA==", "bodyText": "The implement of buildClusteringGroupsForPartition method in SparkBoundedDayBasedScheduleClusteringStrategy. Get params about clustering from maxDataInGroup\u3001getWriteConfig().getClusteringTargetFileSize()\u3001getWriteConfig().getClusteringTargetFileSize(). Maybe other implement will need more params . Can we just abstract buildClusteringGroupsForPartition with the HoodieWriteConfig for param ?  maybe like :\nprotected abstract Stream buildClusteringGroupsForPartition(String partitionPath,\nList fileSlices,\nHoodieWriteConfig writeConfig);", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528207858", "createdAt": "2020-11-21T15:24:36Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> extends ScheduleClusteringStrategy<T,I,K,O> {\n+  private static final Logger LOG = LogManager.getLogger(PartitionAwareScheduleClusteringStrategy.class);\n+\n+  public PartitionAwareScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  /**\n+   * Create Clustering group based on files eligible for clustering in the partition.\n+   */\n+  protected abstract Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath,\n+                                                                                     List<FileSlice> fileSlices,\n+                                                                                     long maxDataInGroup);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM1OTczODcx", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-535973871", "createdAt": "2020-11-21T15:30:48Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxNTozMDo0OFrOH3vShA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQxNTozMDo0OFrOH3vShA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODIwODUxNg==", "bodyText": "if use HoodieClusteringConfig.SORT_COLUMNS_PROPERTY in code  instead of import static better?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528208516", "createdAt": "2020-11-21T15:30:48Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/run/SparkBulkInsertBasedRunClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.run;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.execution.bulkinsert.RDDCustomColumnsSortPartitioner;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.SparkBulkInsertHelper;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.Map;\n+import java.util.Properties;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM2MDI3NTY2", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-536027566", "createdAt": "2020-11-22T04:07:22Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMlQwNDowNzoyMlrOH3zlhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMlQwNDowNzoyMlrOH3zlhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODI3ODkxNg==", "bodyText": "if we should  constraint the columns start of partitionpath and row_key. Or just auto fill the two columns? Because the records of same partition and row_key need in one file slice.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528278916", "createdAt": "2020-11-22T04:07:22Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.execution.bulkinsert;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+/**\n+ * A partitioner that does sorting based on specified column values for each RDD partition.\n+ *\n+ * @param <T> HoodieRecordPayload type\n+ */\n+public class RDDCustomColumnsSortPartitioner<T extends HoodieRecordPayload>\n+    implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {\n+\n+  private final String[] sortColumnNames;\n+  private final String schemaString;\n+\n+  public RDDCustomColumnsSortPartitioner(String[] columnNames, Schema schema) {\n+    this.sortColumnNames = columnNames;\n+    //TODO Schema is not serializable. So convert to String here. Figure out how to improve this\n+    this.schemaString = schema.toString();\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> repartitionRecords(JavaRDD<HoodieRecord<T>> records,\n+                                                     int outputSparkPartitions) {\n+    final String[] sortColumns = this.sortColumnNames;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM2MDU0NzYz", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-536054763", "createdAt": "2020-11-22T11:59:03Z", "commit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMlQxMTo1OTowM1rOH32ZUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMlQxMTo1OTowM1rOH32ZUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODMyNDk0NA==", "bodyText": "can we get pendingClusterFileGroupid  from getHoodieTable().getSliceView() just like getPendingCompactionOperations?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r528324944", "createdAt": "2020-11-22T11:59:03Z", "author": {"login": "lw309637554"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_IO_WRITE_MB = \"TOTAL_IO_WRITE_MB\";\n+  public static final String TOTAL_IO_MB = \"TOTAL_IO_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size\n+   *\n+   * are not eligible for clustering\n+   */\n+  protected List<FileSlice> getFileSlicesEligibleForClustering(String partition) {\n+    Set<HoodieFileGroupId> fgIdsInPendingCompactionAndClustering = ((SyncableFileSystemView) getHoodieTable().getSliceView()).getPendingCompactionOperations()\n+        .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+        .collect(Collectors.toSet());\n+    fgIdsInPendingCompactionAndClustering.addAll(ClusteringUtils.getAllFileGroupsInPendingClusteringPlans(getHoodieTable().getMetaClient()).keySet());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 90}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/dc6d32154dacec181149e41d091a0854b662c362", "committedDate": "2020-11-19T21:12:33Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}, "afterCommit": {"oid": "6e8ea212666c06840129aabcf292dad60b5f3885", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/6e8ea212666c06840129aabcf292dad60b5f3885", "committedDate": "2020-11-30T18:26:52Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyMjEwNDE5", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-542210419", "createdAt": "2020-12-01T19:06:24Z", "commit": {"oid": "6e8ea212666c06840129aabcf292dad60b5f3885"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxOTowNjoyNVrOH87pqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxOTowNjoyNVrOH87pqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1MzkzMA==", "bodyText": "I have a suggestion to change the way we are generating the sort cols. Instead of performing this operation during write where the following code will spend cycles to convert data to generic record again, can we do this during read time when the HoodieRecord is being constructed ? I think you are using HoodieFileSliceReader and HoodieMergedRecordScanner to do this, does it make sense to add a sortKey to HoodieRecord and when you are constructing the hoodie record where you anyways have a handle to the generic record, you can just set that value in the hoodie record ? This will also help avoid passing schemas around for partitioner..\nThe downside of this approach is that the RDD bloats up so the shuffle will be larger so the question is whether that's worse or converting to generic record is worse from a CPU perspective", "url": "https://github.com/apache/hudi/pull/2263#discussion_r533653930", "createdAt": "2020-12-01T19:06:25Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.execution.bulkinsert;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+/**\n+ * A partitioner that does sorting based on specified column values for each RDD partition.\n+ *\n+ * @param <T> HoodieRecordPayload type\n+ */\n+public class RDDCustomColumnsSortPartitioner<T extends HoodieRecordPayload>\n+    implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {\n+\n+  private final String[] sortColumnNames;\n+  private final String schemaString;\n+\n+  public RDDCustomColumnsSortPartitioner(String[] columnNames, Schema schema) {\n+    this.sortColumnNames = columnNames;\n+    //TODO Schema is not serializable. So convert to String here. Figure out how to improve this\n+    this.schemaString = schema.toString();\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> repartitionRecords(JavaRDD<HoodieRecord<T>> records,\n+                                                     int outputSparkPartitions) {\n+    final String[] sortColumns = this.sortColumnNames;\n+    final String schemaStr = this.schemaString;\n+    return records.sortBy(record -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e8ea212666c06840129aabcf292dad60b5f3885"}, "originalPosition": 50}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyMjEyOTg1", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-542212985", "createdAt": "2020-12-01T19:10:00Z", "commit": {"oid": "6e8ea212666c06840129aabcf292dad60b5f3885"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxOToxMDowMVrOH87xdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxOToxMDowMVrOH87xdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzY1NTkyNw==", "bodyText": "The merged record scanner will try to keep an in-memory map, can we optimize for cases when there are no log files ? Or am I missing where this is done for COW / MOR where we are only reading parquet files ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r533655927", "createdAt": "2020-12-01T19:10:01Z", "author": {"login": "n3nash"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // run clustering for each group async and collect WriteStatus\n+    JavaRDD<WriteStatus> writeStatusRDD = clusteringPlan.getInputGroups().stream()\n+        .map(inputGroup -> runClusteringForGroupAsync(inputGroup, clusteringPlan.getStrategy().getStrategyParams()))\n+        .map(CompletableFuture::join)\n+        .reduce((rdd1, rdd2) -> rdd1.union(rdd2)).orElse(engineContext.emptyRDD());\n+    if (writeStatusRDD.isEmpty()) {\n+      throw new HoodieClusteringException(\"Clustering plan produced 0 WriteStatus for \" + instantTime + \" #groups: \" + clusteringPlan.getInputGroups().size());\n+    }\n+\n+    HoodieWriteMetadata<JavaRDD<WriteStatus>> writeMetadata = buildWriteMetadata(writeStatusRDD);\n+    updateIndexAndCommitIfNeeded(writeStatusRDD, writeMetadata);\n+    if (!writeMetadata.getCommitMetadata().isPresent()) {\n+      HoodieCommitMetadata commitMetadata = CommitUtils.buildMetadata(writeStatusRDD.map(WriteStatus::getStat).collect(), writeMetadata.getPartitionToReplaceFileIds(),\n+          extraMetadata, operationType, getSchemaToStoreInCommit(), getCommitActionType());\n+      writeMetadata.setCommitMetadata(Option.of(commitMetadata));\n+    }\n+    return writeMetadata;\n+  }\n+\n+  private CompletableFuture<JavaRDD<WriteStatus>> runClusteringForGroupAsync(HoodieClusteringGroup clusteringGroup, Map<String, String> strategyParams) {\n+    CompletableFuture<JavaRDD<WriteStatus>> writeStatusesFuture = CompletableFuture.supplyAsync(() -> {\n+      JavaSparkContext jsc = HoodieSparkEngineContext.getSparkContext(context);\n+      JavaRDD<HoodieRecord<? extends HoodieRecordPayload>> inputRecords = jsc.parallelize(clusteringGroup.getSlices(), clusteringGroup.getSlices().size()).map(sliceInfo -> {\n+        long maxMemoryPerCompaction = IOUtils.getMaxMemoryPerCompaction(new SparkTaskContextSupplier(), config.getProps());\n+        LOG.info(\"MaxMemoryPerCompaction run as part of clustering => \" + maxMemoryPerCompaction);\n+        try {\n+          Schema readerSchema = HoodieAvroUtils.addMetadataFields(new Schema.Parser().parse(config.getSchema()));\n+          HoodieFileReader<? extends IndexedRecord> baseFileReader = HoodieFileReaderFactory.getFileReader(table.getHadoopConf(), new Path(sliceInfo.getDataFilePath()));\n+          HoodieMergedLogRecordScanner scanner = new HoodieMergedLogRecordScanner(table.getMetaClient().getFs(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e8ea212666c06840129aabcf292dad60b5f3885"}, "originalPosition": 116}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyMjE2ODk5", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-542216899", "createdAt": "2020-12-01T19:15:29Z", "commit": {"oid": "6e8ea212666c06840129aabcf292dad60b5f3885"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6e8ea212666c06840129aabcf292dad60b5f3885", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/6e8ea212666c06840129aabcf292dad60b5f3885", "committedDate": "2020-11-30T18:26:52Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}, "afterCommit": {"oid": "2f632f1f049281d2e3e4ad18a1d2217b3c7bdd1c", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/2f632f1f049281d2e3e4ad18a1d2217b3c7bdd1c", "committedDate": "2020-12-03T08:28:42Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NDcwODg3", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-544470887", "createdAt": "2020-12-03T21:39:38Z", "commit": {"oid": "2f632f1f049281d2e3e4ad18a1d2217b3c7bdd1c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMTozOTozOFrOH-1k1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMTozOTozOFrOH-1k1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY1MTU0MQ==", "bodyText": "Should we make this AbstractLogRecordScanner to allow for UnMergedScanner as well ? @satishkotha", "url": "https://github.com/apache/hudi/pull/2263#discussion_r535651541", "createdAt": "2020-12-03T21:39:38Z", "author": {"login": "n3nash"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.SpillableMapUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.Iterator;\n+\n+/**\n+ * Reads records from base file and merges any updates from log files and provides iterable over all records in the file slice.\n+ */\n+public class HoodieFileSliceReader implements Iterable<HoodieRecord<? extends HoodieRecordPayload>> {\n+  private HoodieMergedLogRecordScanner logRecordScanner;\n+\n+  public static <R extends IndexedRecord, T extends HoodieRecordPayload> HoodieFileSliceReader getFileSliceReader(\n+      HoodieFileReader<R> baseFileReader, HoodieMergedLogRecordScanner scanner, Schema schema, String payloadClass) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f632f1f049281d2e3e4ad18a1d2217b3c7bdd1c"}, "originalPosition": 40}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2f632f1f049281d2e3e4ad18a1d2217b3c7bdd1c", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/2f632f1f049281d2e3e4ad18a1d2217b3c7bdd1c", "committedDate": "2020-12-03T08:28:42Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}, "afterCommit": {"oid": "3f5a847f93e6af8733f5e53ee3ac26863ee2c3ea", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/3f5a847f93e6af8733f5e53ee3ac26863ee2c3ea", "committedDate": "2020-12-03T21:39:35Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NDczODQ1", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-544473845", "createdAt": "2020-12-03T21:42:44Z", "commit": {"oid": "3f5a847f93e6af8733f5e53ee3ac26863ee2c3ea"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3f5a847f93e6af8733f5e53ee3ac26863ee2c3ea", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/3f5a847f93e6af8733f5e53ee3ac26863ee2c3ea", "committedDate": "2020-12-03T21:39:35Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}, "afterCommit": {"oid": "3d52938150d46d25895d734b64dcc4b51c79a1e5", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/3d52938150d46d25895d734b64dcc4b51c79a1e5", "committedDate": "2020-12-03T22:51:09Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3d52938150d46d25895d734b64dcc4b51c79a1e5", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/3d52938150d46d25895d734b64dcc4b51c79a1e5", "committedDate": "2020-12-03T22:51:09Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}, "afterCommit": {"oid": "a4c469f17880f3b7fd7255b362ab0291db69bdf0", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/a4c469f17880f3b7fd7255b362ab0291db69bdf0", "committedDate": "2020-12-03T22:59:12Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a4c469f17880f3b7fd7255b362ab0291db69bdf0", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/a4c469f17880f3b7fd7255b362ab0291db69bdf0", "committedDate": "2020-12-03T22:59:12Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}, "afterCommit": {"oid": "dc13795bbdb7aa0a26bdb9e17f293a337e8efc09", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/dc13795bbdb7aa0a26bdb9e17f293a337e8efc09", "committedDate": "2020-12-03T23:18:40Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "dc13795bbdb7aa0a26bdb9e17f293a337e8efc09", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/dc13795bbdb7aa0a26bdb9e17f293a337e8efc09", "committedDate": "2020-12-03T23:18:40Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}, "afterCommit": {"oid": "5a1e16233ea944237f55e522fae8cf70b4c041a7", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/5a1e16233ea944237f55e522fae8cf70b4c041a7", "committedDate": "2020-12-09T01:28:08Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5a1e16233ea944237f55e522fae8cf70b4c041a7", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/5a1e16233ea944237f55e522fae8cf70b4c041a7", "committedDate": "2020-12-09T01:28:08Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}, "afterCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/f526c2c0eed8134aa513898ba48ef8ddf851272c", "committedDate": "2020-12-09T06:14:09Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4NDQzNjM4", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-548443638", "createdAt": "2020-12-09T17:49:11Z", "commit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0MzI3NDIx", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-554327421", "createdAt": "2020-12-17T07:07:58Z", "commit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "state": "COMMENTED", "comments": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwNzoxMDo1NlrOIHnkxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNDozODozMlrOIH4pag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1OTMzNA==", "bodyText": "lets please add java docs for all these configs?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r544859334", "createdAt": "2020-12-17T07:10:56Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  public static final String ASYNC_CLUSTERING_ENABLED = \"hoodie.clustering.enabled\";\n+  public static final String DEFAULT_ASYNC_CLUSTERING_ENABLED = \"false\";\n+\n+  public static final String SCHEDULE_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.schedule.strategy.class\";\n+  public static final String DEFAULT_SCHEDULE_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.schedule.strategy.SparkBoundedDayBasedScheduleClusteringStrategy\";\n+\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - after few commits an inline clustering will be run\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2MjM2Nw==", "bodyText": "make this more readable, by putting each param on a line like before?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r544862367", "createdAt": "2020-12-17T07:18:02Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java", "diffHunk": "@@ -59,25 +58,39 @@ public static SparkBulkInsertHelper newInstance() {\n   }\n \n   @Override\n-  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(JavaRDD<HoodieRecord<T>> inputRecords,\n-                                                              String instantTime,\n-                                                              HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n-                                                              HoodieWriteConfig config,\n-                                                              BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor,\n-                                                              boolean performDedupe,\n-                                                              Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(final JavaRDD<HoodieRecord<T>> inputRecords, final String instantTime, final HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table, final HoodieWriteConfig config, final BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor, final boolean performDedupe, final Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAzNzE0NQ==", "bodyText": "should this be called replaceTimer as well?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545037145", "createdAt": "2020-12-17T12:02:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/metrics/HoodieMetrics.java", "diffHunk": "@@ -48,6 +49,7 @@\n   private Timer deltaCommitTimer = null;\n   private Timer finalizeTimer = null;\n   private Timer compactionTimer = null;\n+  private Timer clusteringTimer = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NDk5MA==", "bodyText": "I think we should have 50 as the default value for this config and allow any value to passed in, as opposed to having this limit hard-coded.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545044990", "createdAt": "2020-12-17T12:15:51Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> extends ScheduleClusteringStrategy<T,I,K,O> {\n+  private static final Logger LOG = LogManager.getLogger(PartitionAwareScheduleClusteringStrategy.class);\n+  // With more than 50 groups, we see performance degradation with this Strategy implementation.\n+  private static final int MAX_CLUSTERING_GROUPS_STRATEGY = 50;\n+\n+  public PartitionAwareScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  /**\n+   * Create Clustering group based on files eligible for clustering in the partition.\n+   */\n+  protected abstract Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath,\n+                                                                                     List<FileSlice> fileSlices);\n+\n+  /**\n+   * Return list of partition paths to be considered for clustering.\n+   */\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    return partitionPaths;\n+  }\n+\n+  @Override\n+  public Option<HoodieClusteringPlan> generateClusteringPlan() {\n+    try {\n+      HoodieTableMetaClient metaClient = getHoodieTable().getMetaClient();\n+      LOG.info(\"Scheduling clustering for \" + metaClient.getBasePath());\n+      List<String> partitionPaths = FSUtils.getAllPartitionPaths(metaClient.getFs(), metaClient.getBasePath(),\n+          getWriteConfig().shouldAssumeDatePartitioning());\n+\n+      // filter the partition paths if needed to reduce list status\n+      partitionPaths = filterPartitionPaths(partitionPaths);\n+\n+      if (partitionPaths.isEmpty()) {\n+        // In case no partitions could be picked, return no clustering plan\n+        return Option.empty();\n+      }\n+\n+      long maxClusteringGroups = getWriteConfig().getClusteringMaxNumGroups();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA0NTk3Ng==", "bodyText": "why not try to do this in parallel using context.map() etc? it should improve performance as well", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545045976", "createdAt": "2020-12-17T12:17:34Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> extends ScheduleClusteringStrategy<T,I,K,O> {\n+  private static final Logger LOG = LogManager.getLogger(PartitionAwareScheduleClusteringStrategy.class);\n+  // With more than 50 groups, we see performance degradation with this Strategy implementation.\n+  private static final int MAX_CLUSTERING_GROUPS_STRATEGY = 50;\n+\n+  public PartitionAwareScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  /**\n+   * Create Clustering group based on files eligible for clustering in the partition.\n+   */\n+  protected abstract Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath,\n+                                                                                     List<FileSlice> fileSlices);\n+\n+  /**\n+   * Return list of partition paths to be considered for clustering.\n+   */\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    return partitionPaths;\n+  }\n+\n+  @Override\n+  public Option<HoodieClusteringPlan> generateClusteringPlan() {\n+    try {\n+      HoodieTableMetaClient metaClient = getHoodieTable().getMetaClient();\n+      LOG.info(\"Scheduling clustering for \" + metaClient.getBasePath());\n+      List<String> partitionPaths = FSUtils.getAllPartitionPaths(metaClient.getFs(), metaClient.getBasePath(),\n+          getWriteConfig().shouldAssumeDatePartitioning());\n+\n+      // filter the partition paths if needed to reduce list status\n+      partitionPaths = filterPartitionPaths(partitionPaths);\n+\n+      if (partitionPaths.isEmpty()) {\n+        // In case no partitions could be picked, return no clustering plan\n+        return Option.empty();\n+      }\n+\n+      long maxClusteringGroups = getWriteConfig().getClusteringMaxNumGroups();\n+      if (maxClusteringGroups > MAX_CLUSTERING_GROUPS_STRATEGY) {\n+        LOG.warn(\"Reducing max clustering groups to \" + MAX_CLUSTERING_GROUPS_STRATEGY + \" for performance reasons\");\n+        maxClusteringGroups = MAX_CLUSTERING_GROUPS_STRATEGY;\n+      }\n+\n+      List<HoodieClusteringGroup> clusteringGroups = partitionPaths.stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NDc1OA==", "bodyText": "is this like a group number? Seems more like the number of output files produced by that group", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545054758", "createdAt": "2020-12-17T12:32:26Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/schedule/strategy/SparkBoundedDayBasedScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.schedule.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.HoodieSparkMergeOnReadTable;\n+import org.apache.hudi.table.action.cluster.strategy.PartitionAwareScheduleClusteringStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Limits amount of data per clustering operation.\n+ */\n+public class SparkBoundedDayBasedScheduleClusteringStrategy<T extends HoodieRecordPayload<T>>\n+    extends PartitionAwareScheduleClusteringStrategy<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+  private static final Logger LOG = LogManager.getLogger(SparkBoundedDayBasedScheduleClusteringStrategy.class);\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkCopyOnWriteTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkMergeOnReadTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  @Override\n+  protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {\n+    List<Pair<List<FileSlice>, Integer>> fileSliceGroups = new ArrayList<>();\n+    List<FileSlice> currentGroup = new ArrayList<>();\n+    int totalSizeSoFar = 0;\n+    for (FileSlice currentSlice : fileSlices) {\n+      // assume each filegroup size is ~= parquet.max.file.size\n+      totalSizeSoFar += currentSlice.getBaseFile().isPresent() ? currentSlice.getBaseFile().get().getFileSize() : getWriteConfig().getParquetMaxFileSize();\n+      // check if max size is reached and create new group, if needed.\n+      if (totalSizeSoFar >= getWriteConfig().getClusteringMaxBytesInGroup() && !currentGroup.isEmpty()) {\n+        fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+        currentGroup = new ArrayList<>();\n+        totalSizeSoFar = 0;\n+      }\n+      currentGroup.add(currentSlice);\n+    }\n+    if (!currentGroup.isEmpty()) {\n+      fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+    }\n+\n+    return fileSliceGroups.stream().map(fileSliceGroup -> HoodieClusteringGroup.newBuilder()\n+        .setSlices(getFileSliceInfo(fileSliceGroup.getLeft()))\n+        .setNumOutputGroups(fileSliceGroup.getRight())\n+        .setMetrics(buildMetrics(fileSliceGroup.getLeft()))\n+        .build());\n+  }\n+\n+  @Override\n+  protected Map<String, String> getStrategyParams() {\n+    Map<String, String> params = new HashMap<>();\n+    if (getWriteConfig().getProps().containsKey(SORT_COLUMNS_PROPERTY)) {\n+      params.put(SORT_COLUMNS_PROPERTY, getWriteConfig().getProps().getProperty(SORT_COLUMNS_PROPERTY));\n+    }\n+    return params;\n+  }\n+\n+  @Override\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    int targetPartitionsForClustering = getWriteConfig().getTargetPartitionsForClustering();\n+    return partitionPaths.stream().map(partition -> partition.replace(\"/\", \"-\"))\n+        .sorted(Comparator.reverseOrder()).map(partitionPath -> partitionPath.replace(\"-\", \"/\"))\n+        .limit(targetPartitionsForClustering > 0 ? targetPartitionsForClustering : partitionPaths.size())\n+        .collect(Collectors.toList());\n+  }\n+\n+  private int getNumberOfGroups(long groupSize, long targetFileSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NTA4OA==", "bodyText": "why do we need the /,- replace logic. It should correctly even without that?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545055088", "createdAt": "2020-12-17T12:33:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/schedule/strategy/SparkBoundedDayBasedScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.schedule.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.HoodieSparkMergeOnReadTable;\n+import org.apache.hudi.table.action.cluster.strategy.PartitionAwareScheduleClusteringStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Limits amount of data per clustering operation.\n+ */\n+public class SparkBoundedDayBasedScheduleClusteringStrategy<T extends HoodieRecordPayload<T>>\n+    extends PartitionAwareScheduleClusteringStrategy<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+  private static final Logger LOG = LogManager.getLogger(SparkBoundedDayBasedScheduleClusteringStrategy.class);\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkCopyOnWriteTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  public SparkBoundedDayBasedScheduleClusteringStrategy(HoodieSparkMergeOnReadTable<T> table,\n+                                                        HoodieSparkEngineContext engineContext,\n+                                                        HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  @Override\n+  protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {\n+    List<Pair<List<FileSlice>, Integer>> fileSliceGroups = new ArrayList<>();\n+    List<FileSlice> currentGroup = new ArrayList<>();\n+    int totalSizeSoFar = 0;\n+    for (FileSlice currentSlice : fileSlices) {\n+      // assume each filegroup size is ~= parquet.max.file.size\n+      totalSizeSoFar += currentSlice.getBaseFile().isPresent() ? currentSlice.getBaseFile().get().getFileSize() : getWriteConfig().getParquetMaxFileSize();\n+      // check if max size is reached and create new group, if needed.\n+      if (totalSizeSoFar >= getWriteConfig().getClusteringMaxBytesInGroup() && !currentGroup.isEmpty()) {\n+        fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+        currentGroup = new ArrayList<>();\n+        totalSizeSoFar = 0;\n+      }\n+      currentGroup.add(currentSlice);\n+    }\n+    if (!currentGroup.isEmpty()) {\n+      fileSliceGroups.add(Pair.of(currentGroup, getNumberOfGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+    }\n+\n+    return fileSliceGroups.stream().map(fileSliceGroup -> HoodieClusteringGroup.newBuilder()\n+        .setSlices(getFileSliceInfo(fileSliceGroup.getLeft()))\n+        .setNumOutputGroups(fileSliceGroup.getRight())\n+        .setMetrics(buildMetrics(fileSliceGroup.getLeft()))\n+        .build());\n+  }\n+\n+  @Override\n+  protected Map<String, String> getStrategyParams() {\n+    Map<String, String> params = new HashMap<>();\n+    if (getWriteConfig().getProps().containsKey(SORT_COLUMNS_PROPERTY)) {\n+      params.put(SORT_COLUMNS_PROPERTY, getWriteConfig().getProps().getProperty(SORT_COLUMNS_PROPERTY));\n+    }\n+    return params;\n+  }\n+\n+  @Override\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    int targetPartitionsForClustering = getWriteConfig().getTargetPartitionsForClustering();\n+    return partitionPaths.stream().map(partition -> partition.replace(\"/\", \"-\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1NTkxNQ==", "bodyText": "lets rename the parameter to be more descriptive?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545055915", "createdAt": "2020-12-17T12:34:29Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/RunClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.avro.Schema;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Map;\n+\n+/**\n+ * Pluggable implementation for writing data into new file groups based on ClusteringPlan.\n+ */\n+public abstract class RunClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(RunClusteringStrategy.class);\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public RunClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Run clustering to write inputRecords into new files as defined by rules in strategy parameters. The number of new\n+   * file groups created is bounded by numOutputGroups.\n+   * Note that commit is not done as part of strategy. commit is callers responsibility.\n+   */\n+  public abstract O performClustering(final I inputRecords, final int numOutputGroups, final String instantTime,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA4MDMyNA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1OTExMg==", "bodyText": "should the filtering for 2 happen at this level?  that seems like something a specific plan would do.\n1 makes sense to do at this level.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545059112", "createdAt": "2020-12-17T12:39:58Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.FileSliceUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA1OTUzNg==", "bodyText": "use the constant in L123 as well?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545059536", "createdAt": "2020-12-17T12:40:42Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_IO_WRITE_MB = \"TOTAL_IO_WRITE_MB\";\n+  public static final String TOTAL_IO_MB = \"TOTAL_IO_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size\n+   *\n+   * are not eligible for clustering\n+   */\n+  protected List<FileSlice> getFileSlicesEligibleForClustering(String partition) {\n+    Set<HoodieFileGroupId> fgIdsInPendingCompactionAndClustering = ((SyncableFileSystemView) getHoodieTable().getSliceView()).getPendingCompactionOperations()\n+        .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+        .collect(Collectors.toSet());\n+    fgIdsInPendingCompactionAndClustering.addAll(ClusteringUtils.getAllFileGroupsInPendingClusteringPlans(getHoodieTable().getMetaClient()).keySet());\n+\n+    return hoodieTable.getSliceView().getLatestFileSlices(partition)\n+        // file ids already in clustering are not eligible\n+        .filter(slice -> !fgIdsInPendingCompactionAndClustering.contains(slice.getFileGroupId()))\n+        // files that have basefile size larger than clustering target file size are not eligible (Note that compaction can merge any updates)\n+        .filter(slice -> slice.getBaseFile().map(HoodieBaseFile::getFileSize).orElse(0L) < writeConfig.getClusteringTargetFileSize())\n+        .collect(Collectors.toList());\n+  }\n+\n+  /**\n+   * Get parameters specific to strategy. These parameters are passed from 'schedule clustering' step to\n+   * 'run clustering' step. 'run clustering' step is typically async. So these params help with passing any required\n+   * context from schedule to run step.\n+   */\n+  protected abstract Map<String, String> getStrategyParams();\n+\n+  /**\n+   * Returns any specific parameters to be stored as part of clustering metadata.\n+   */\n+  protected Map<String, String> getExtraMetadata() {\n+    return Collections.emptyMap();\n+  }\n+\n+  /**\n+   * Version to support future changes for plan.\n+   */\n+  protected int getPlanVersion() {\n+    return CLUSTERING_PLAN_VERSION_1;\n+  }\n+\n+  /**\n+   * Transform {@link FileSlice} to {@link HoodieSliceInfo}.\n+   */\n+  protected List<HoodieSliceInfo> getFileSliceInfo(List<FileSlice> slices) {\n+    return slices.stream().map(slice -> new HoodieSliceInfo().newBuilder()\n+        .setPartitionPath(slice.getPartitionPath())\n+        .setFileId(slice.getFileId())\n+        .setDataFilePath(slice.getBaseFile().map(BaseFile::getPath).orElse(\"\"))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA4MDU2OA=="}, "originalCommit": {"oid": "dc6d32154dacec181149e41d091a0854b662c362"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MDE0Ng==", "bodyText": "this is actually just a ClusteringPlanStrategy right? i.e it generates clustering plans. It has less to do with scheduling of clustering itself?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545060146", "createdAt": "2020-12-17T12:41:44Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.FileSliceUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MDM1Mg==", "bodyText": "Should we rename this class and the configs?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545060352", "createdAt": "2020-12-17T12:42:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ScheduleClusteringStrategy.java", "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.FileSliceUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ScheduleClusteringStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ScheduleClusteringStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ScheduleClusteringStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MDE0Ng=="}, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MTMzMQ==", "bodyText": "can we rename numOutputGroups to something that can be easily understood in the bulk insert context. If we leak some clustering terminology here, it becomes to harder to read this.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545061331", "createdAt": "2020-12-17T12:43:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/commit/AbstractBulkInsertHelper.java", "diffHunk": "@@ -27,8 +27,21 @@\n \n public abstract class AbstractBulkInsertHelper<T extends HoodieRecordPayload, I, K, O, R> {\n \n+  /**\n+   * Mark instant as inflight, write input records, update index and return result.\n+   */\n   public abstract HoodieWriteMetadata<O> bulkInsert(I inputRecords, String instantTime,\n                                                     HoodieTable<T, I, K, O> table, HoodieWriteConfig config,\n                                                     BaseCommitActionExecutor<T, I, K, O, R> executor, boolean performDedupe,\n                                                     Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner);\n+\n+  /**\n+   * Only write input records. Does not change timeline/index. Return information about new files created.\n+   */\n+  public abstract O bulkInsert(I inputRecords, String instantTime,\n+                               HoodieTable<T, I, K, O> table, HoodieWriteConfig config,\n+                               boolean performDedupe,\n+                               Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner,\n+                               boolean addMetadataFields,\n+                               int numOutputGroups);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTA2MjM0NA==", "bodyText": "are these cleanups strictly needed for this PR? if you have tested them already, its okay. but generally, separating these in a different refactor PR is preferrable.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545062344", "createdAt": "2020-12-17T12:45:20Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/compact/strategy/LogFileSizeBasedCompactionStrategy.java", "diffHunk": "@@ -40,21 +36,6 @@\n public class LogFileSizeBasedCompactionStrategy extends BoundedIOCompactionStrategy\n     implements Comparator<HoodieCompactionOperation> {\n \n-  private static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILE_SIZE\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEzMTY3Ng==", "bodyText": "place each parameter on its own line?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545131676", "createdAt": "2020-12-17T14:28:33Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/commit/SparkBulkInsertHelper.java", "diffHunk": "@@ -59,25 +58,39 @@ public static SparkBulkInsertHelper newInstance() {\n   }\n \n   @Override\n-  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(JavaRDD<HoodieRecord<T>> inputRecords,\n-                                                              String instantTime,\n-                                                              HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table,\n-                                                              HoodieWriteConfig config,\n-                                                              BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor,\n-                                                              boolean performDedupe,\n-                                                              Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> bulkInsert(final JavaRDD<HoodieRecord<T>> inputRecords, final String instantTime, final HoodieTable<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> table, final HoodieWriteConfig config, final BaseCommitActionExecutor<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, R> executor, final boolean performDedupe, final Option<BulkInsertPartitioner<T>> userDefinedBulkInsertPartitioner) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEzNjQwMw==", "bodyText": "should it be called numOutputSlices?  the whole thing  is called a group right?  or may be you mean numOutputFileGroups? in which case, lets rename this to clarify", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545136403", "createdAt": "2020-12-17T14:34:54Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/avro/HoodieClusteringGroup.avsc", "diffHunk": "@@ -40,6 +40,11 @@\n          }],\n          \"default\": null\n       },\n+      {\n+         \"name\":\"numOutputGroups\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTEzOTA1MA==", "bodyText": "this is very metric specific. could we stick it somewhere else closer to actual usage in hudi-client-common? may be rename the class to something like FileSliceMetricUtils ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r545139050", "createdAt": "2020-12-17T14:38:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/util/FileSliceUtils.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.util;\n+\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieLogFile;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * A utility class for numeric.\n+ */\n+public class FileSliceUtils {\n+\n+  public static final String TOTAL_IO_READ_MB = \"TOTAL_IO_READ_MB\";\n+  public static final String TOTAL_IO_WRITE_MB = \"TOTAL_IO_WRITE_MB\";\n+  public static final String TOTAL_IO_MB = \"TOTAL_IO_MB\";\n+  public static final String TOTAL_LOG_FILE_SIZE = \"TOTAL_LOG_FILES_SIZE\";\n+  public static final String TOTAL_LOG_FILES = \"TOTAL_LOG_FILES\";\n+\n+  public static void addFileSliceCommonMetrics(List<FileSlice> fileSlices, Map<String, Double> metrics, long defaultBaseFileSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c"}, "originalPosition": 39}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f526c2c0eed8134aa513898ba48ef8ddf851272c", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/f526c2c0eed8134aa513898ba48ef8ddf851272c", "committedDate": "2020-12-09T06:14:09Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}, "afterCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/039e6a5931e0da738582b5e7c35691cdf76e803a", "committedDate": "2020-12-18T03:38:49Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1ODQzODI3", "url": "https://github.com/apache/hudi/pull/2263#pullrequestreview-555843827", "createdAt": "2020-12-18T22:10:19Z", "commit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "state": "APPROVED", "comments": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjoxMDoxOVrOII0MKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQyMjo0NTozOVrOII04Kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExNDYwMw==", "bodyText": "may be call it execution.  hoodie.clustering.execution.strategy.class ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546114603", "createdAt": "2020-12-18T22:10:19Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  // Config to provide a strategy class to create ClusteringPlan. Class has to be subclass of ClusteringPlanStrategy\n+  public static final String CLUSTERING_PLAN_STRATEGY_CLASS = \"hoodie.clustering.plan.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.plan.strategy.SparkBoundedDayBasedClusteringPlanStrategy\";\n+\n+  // Config to provide a strategy class to execute a ClusteringPlan. Class has to be subclass of RunClusteringStrategy\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExNDk0OA==", "bodyText": "max.commits?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546114948", "createdAt": "2020-12-18T22:11:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  // Config to provide a strategy class to create ClusteringPlan. Class has to be subclass of ClusteringPlanStrategy\n+  public static final String CLUSTERING_PLAN_STRATEGY_CLASS = \"hoodie.clustering.plan.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.plan.strategy.SparkBoundedDayBasedClusteringPlanStrategy\";\n+\n+  // Config to provide a strategy class to execute a ClusteringPlan. Class has to be subclass of RunClusteringStrategy\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - clustering will be run after write operation is complete.\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  // Config to control frequency of clustering\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExNTQzMw==", "bodyText": "replace scan with list?\nshould we also do it based on last N commits? It helps for tables that receive data across partitions? This is probably some follow on work we can do to add a new plan strategy for this. Lets add a JIRA?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546115433", "createdAt": "2020-12-18T22:12:56Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  // Config to provide a strategy class to create ClusteringPlan. Class has to be subclass of ClusteringPlanStrategy\n+  public static final String CLUSTERING_PLAN_STRATEGY_CLASS = \"hoodie.clustering.plan.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.plan.strategy.SparkBoundedDayBasedClusteringPlanStrategy\";\n+\n+  // Config to provide a strategy class to execute a ClusteringPlan. Class has to be subclass of RunClusteringStrategy\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - clustering will be run after write operation is complete.\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  // Config to control frequency of clustering\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  // Number of partitions to scan to create ClusteringPlan.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExNjExOQ==", "bodyText": "hoodie.clustering.plan.strategy.daybased.lookback.partitions or something ties this to a specific strategy and also captures that this look at N partitions from now.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546116119", "createdAt": "2020-12-18T22:14:46Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  // Config to provide a strategy class to create ClusteringPlan. Class has to be subclass of ClusteringPlanStrategy\n+  public static final String CLUSTERING_PLAN_STRATEGY_CLASS = \"hoodie.clustering.plan.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.plan.strategy.SparkBoundedDayBasedClusteringPlanStrategy\";\n+\n+  // Config to provide a strategy class to execute a ClusteringPlan. Class has to be subclass of RunClusteringStrategy\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - clustering will be run after write operation is complete.\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  // Config to control frequency of clustering\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  // Number of partitions to scan to create ClusteringPlan.\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExNjk0MA==", "bodyText": "something to think about. should this be strategy specific too?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546116940", "createdAt": "2020-12-18T22:16:58Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  // Config to provide a strategy class to create ClusteringPlan. Class has to be subclass of ClusteringPlanStrategy\n+  public static final String CLUSTERING_PLAN_STRATEGY_CLASS = \"hoodie.clustering.plan.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.plan.strategy.SparkBoundedDayBasedClusteringPlanStrategy\";\n+\n+  // Config to provide a strategy class to execute a ClusteringPlan. Class has to be subclass of RunClusteringStrategy\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - clustering will be run after write operation is complete.\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  // Config to control frequency of clustering\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  // Number of partitions to scan to create ClusteringPlan.\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);\n+\n+  // Each clustering operation can create multiple groups. Total amount of data processed by clustering operation", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExNzgwOA==", "bodyText": "may be hoodie.clustering.max.bytes.per.group ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546117808", "createdAt": "2020-12-18T22:19:40Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  // Config to provide a strategy class to create ClusteringPlan. Class has to be subclass of ClusteringPlanStrategy\n+  public static final String CLUSTERING_PLAN_STRATEGY_CLASS = \"hoodie.clustering.plan.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.plan.strategy.SparkBoundedDayBasedClusteringPlanStrategy\";\n+\n+  // Config to provide a strategy class to execute a ClusteringPlan. Class has to be subclass of RunClusteringStrategy\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - clustering will be run after write operation is complete.\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  // Config to control frequency of clustering\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  // Number of partitions to scan to create ClusteringPlan.\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);\n+\n+  // Each clustering operation can create multiple groups. Total amount of data processed by clustering operation\n+  // is defined by below two properties (CLUSTERING_MAX_BYTES_IN_GROUP * CLUSTERING_MAX_NUM_GROUPS).\n+  // Max amount of data to be included in one group\n+  public static final String CLUSTERING_MAX_BYTES_IN_GROUP = \"hoodie.clustering.max.bytes.group\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExNzkxNQ==", "bodyText": "strategy specific?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546117915", "createdAt": "2020-12-18T22:20:01Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/config/HoodieClusteringConfig.java", "diffHunk": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.config;\n+\n+import org.apache.hudi.common.config.DefaultHoodieConfig;\n+\n+import java.io.File;\n+import java.io.FileReader;\n+import java.io.IOException;\n+import java.util.Properties;\n+\n+/**\n+ * Clustering specific configs.\n+ */\n+public class HoodieClusteringConfig extends DefaultHoodieConfig {\n+\n+  // Config to provide a strategy class to create ClusteringPlan. Class has to be subclass of ClusteringPlanStrategy\n+  public static final String CLUSTERING_PLAN_STRATEGY_CLASS = \"hoodie.clustering.plan.strategy.class\";\n+  public static final String DEFAULT_CLUSTERING_PLAN_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.plan.strategy.SparkBoundedDayBasedClusteringPlanStrategy\";\n+\n+  // Config to provide a strategy class to execute a ClusteringPlan. Class has to be subclass of RunClusteringStrategy\n+  public static final String RUN_CLUSTERING_STRATEGY_CLASS = \"hoodie.clustering.run.strategy.class\";\n+  public static final String DEFAULT_RUN_CLUSTERING_STRATEGY_CLASS =\n+      \"org.apache.hudi.client.clustering.run.strategy.SparkBulkInsertBasedRunClusteringStrategy\";\n+\n+  // Turn on inline clustering - clustering will be run after write operation is complete.\n+  public static final String INLINE_CLUSTERING_PROP = \"hoodie.clustering.inline\";\n+  private static final String DEFAULT_INLINE_CLUSTERING = \"false\";\n+\n+  // Config to control frequency of clustering\n+  public static final String INLINE_CLUSTERING_NUM_COMMIT_PROP = \"hoodie.clustering.inline.num.commits\";\n+  private static final String DEFAULT_INLINE_CLUSTERING_NUM_COMMITS = \"4\";\n+\n+  // Number of partitions to scan to create ClusteringPlan.\n+  public static final String CLUSTERING_TARGET_PARTITIONS = \"hoodie.clustering.target.partitions\";\n+  public static final String DEFAULT_CLUSTERING_TARGET_PARTITIONS = String.valueOf(2);\n+\n+  // Each clustering operation can create multiple groups. Total amount of data processed by clustering operation\n+  // is defined by below two properties (CLUSTERING_MAX_BYTES_IN_GROUP * CLUSTERING_MAX_NUM_GROUPS).\n+  // Max amount of data to be included in one group\n+  public static final String CLUSTERING_MAX_BYTES_IN_GROUP = \"hoodie.clustering.max.bytes.group\";\n+  public static final String DEFAULT_CLUSTERING_MAX_GROUP_SIZE = String.valueOf(2 * 1024 * 1024 * 1024L);\n+\n+  // Maximum number of groups to create as part of ClusteringPlan. Increasing groups will increase parallelism.\n+  public static final String CLUSTERING_MAX_NUM_GROUPS = \"hoodie.clustering.max.num.groups\";\n+  public static final String DEFAULT_CLUSTERING_MAX_NUM_GROUPS = \"30\";\n+\n+  // Each group can produce 'N' (CLUSTERING_MAX_GROUP_SIZE/CLUSTERING_TARGET_FILE_SIZE) output file groups.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExODExNA==", "bodyText": "drop the Create?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546118114", "createdAt": "2020-12-18T22:20:44Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/BaseCreateClusteringPlanActionExecutor.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieRequestedReplaceMetadata;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.table.timeline.TimelineMetadataUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.BaseActionExecutor;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.Map;\n+\n+public abstract class BaseCreateClusteringPlanActionExecutor<T extends HoodieRecordPayload, I, K, O> extends BaseActionExecutor<T, I, K, O, Option<HoodieClusteringPlan>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExODQ1NQ==", "bodyText": "remove this?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546118455", "createdAt": "2020-12-18T22:21:53Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ClusteringPlanStrategy.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.utils.FileSliceMetricUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ClusteringPlanStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ClusteringPlanStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final transient HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ClusteringPlanStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExODgyMQ==", "bodyText": "EMPTY_STRING?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546118821", "createdAt": "2020-12-18T22:22:48Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ClusteringPlanStrategy.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.utils.FileSliceMetricUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ClusteringPlanStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ClusteringPlanStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final transient HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ClusteringPlanStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size\n+   *\n+   * are not eligible for clustering.\n+   */\n+  protected Stream<FileSlice> getFileSlicesEligibleForClustering(String partition) {\n+    SyncableFileSystemView fileSystemView = (SyncableFileSystemView) getHoodieTable().getSliceView();\n+    Set<HoodieFileGroupId> fgIdsInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+        .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+        .collect(Collectors.toSet());\n+    fgIdsInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getKey).collect(Collectors.toSet()));\n+\n+    return hoodieTable.getSliceView().getLatestFileSlices(partition)\n+        // file ids already in clustering are not eligible\n+        .filter(slice -> !fgIdsInPendingCompactionAndClustering.contains(slice.getFileGroupId()));\n+  }\n+\n+  /**\n+   * Get parameters specific to strategy. These parameters are passed from 'schedule clustering' step to\n+   * 'run clustering' step. 'run clustering' step is typically async. So these params help with passing any required\n+   * context from schedule to run step.\n+   */\n+  protected abstract Map<String, String> getStrategyParams();\n+\n+  /**\n+   * Returns any specific parameters to be stored as part of clustering metadata.\n+   */\n+  protected Map<String, String> getExtraMetadata() {\n+    return Collections.emptyMap();\n+  }\n+\n+  /**\n+   * Version to support future changes for plan.\n+   */\n+  protected int getPlanVersion() {\n+    return CLUSTERING_PLAN_VERSION_1;\n+  }\n+\n+  /**\n+   * Transform {@link FileSlice} to {@link HoodieSliceInfo}.\n+   */\n+  protected List<HoodieSliceInfo> getFileSliceInfo(List<FileSlice> slices) {\n+    return slices.stream().map(slice -> new HoodieSliceInfo().newBuilder()\n+        .setPartitionPath(slice.getPartitionPath())\n+        .setFileId(slice.getFileId())\n+        .setDataFilePath(slice.getBaseFile().map(BaseFile::getPath).orElse(\"\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExOTA1Mg==", "bodyText": "worth thinking about making this a static helper?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546119052", "createdAt": "2020-12-18T22:23:22Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/ClusteringPlanStrategy.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieSliceInfo;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.utils.FileSliceMetricUtils;\n+import org.apache.hudi.common.model.BaseFile;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieFileGroupId;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.view.SyncableFileSystemView;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.StringUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Pluggable implementation for scheduling clustering and creating ClusteringPlan.\n+ */\n+public abstract class ClusteringPlanStrategy<T extends HoodieRecordPayload,I,K,O> implements Serializable {\n+  private static final Logger LOG = LogManager.getLogger(ClusteringPlanStrategy.class);\n+\n+  public static final int CLUSTERING_PLAN_VERSION_1 = 1;\n+\n+  private final HoodieTable<T,I,K,O> hoodieTable;\n+  private final transient HoodieEngineContext engineContext;\n+  private final HoodieWriteConfig writeConfig;\n+\n+  public ClusteringPlanStrategy(HoodieTable table, HoodieEngineContext engineContext, HoodieWriteConfig writeConfig) {\n+    this.writeConfig = writeConfig;\n+    this.hoodieTable = table;\n+    this.engineContext = engineContext;\n+  }\n+\n+  /**\n+   * Generate metadata for grouping eligible files and create a plan. Note that data is not moved around\n+   * as part of this step.\n+   *\n+   * If there is no data available to cluster, return None.\n+   */\n+  public abstract Option<HoodieClusteringPlan> generateClusteringPlan();\n+\n+  /**\n+   * Return file slices eligible for clustering. FileIds in\n+   * 1) pending clustering/compaction\n+   * 2) Larger than clustering target file size\n+   *\n+   * are not eligible for clustering.\n+   */\n+  protected Stream<FileSlice> getFileSlicesEligibleForClustering(String partition) {\n+    SyncableFileSystemView fileSystemView = (SyncableFileSystemView) getHoodieTable().getSliceView();\n+    Set<HoodieFileGroupId> fgIdsInPendingCompactionAndClustering = fileSystemView.getPendingCompactionOperations()\n+        .map(instantTimeOpPair -> instantTimeOpPair.getValue().getFileGroupId())\n+        .collect(Collectors.toSet());\n+    fgIdsInPendingCompactionAndClustering.addAll(fileSystemView.getFileGroupsInPendingClustering().map(Pair::getKey).collect(Collectors.toSet()));\n+\n+    return hoodieTable.getSliceView().getLatestFileSlices(partition)\n+        // file ids already in clustering are not eligible\n+        .filter(slice -> !fgIdsInPendingCompactionAndClustering.contains(slice.getFileGroupId()));\n+  }\n+\n+  /**\n+   * Get parameters specific to strategy. These parameters are passed from 'schedule clustering' step to\n+   * 'run clustering' step. 'run clustering' step is typically async. So these params help with passing any required\n+   * context from schedule to run step.\n+   */\n+  protected abstract Map<String, String> getStrategyParams();\n+\n+  /**\n+   * Returns any specific parameters to be stored as part of clustering metadata.\n+   */\n+  protected Map<String, String> getExtraMetadata() {\n+    return Collections.emptyMap();\n+  }\n+\n+  /**\n+   * Version to support future changes for plan.\n+   */\n+  protected int getPlanVersion() {\n+    return CLUSTERING_PLAN_VERSION_1;\n+  }\n+\n+  /**\n+   * Transform {@link FileSlice} to {@link HoodieSliceInfo}.\n+   */\n+  protected List<HoodieSliceInfo> getFileSliceInfo(List<FileSlice> slices) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExOTY2OQ==", "bodyText": "same something like DayBased? to indicate date partitioning?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546119669", "createdAt": "2020-12-18T22:25:13Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareClusteringPlanStrategy.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareClusteringPlanStrategy<T extends HoodieRecordPayload,I,K,O> extends ClusteringPlanStrategy<T,I,K,O> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyMTM2OQ==", "bodyText": "also rename the subclass to indicate that it prefers the recent partitions?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546121369", "createdAt": "2020-12-18T22:30:25Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-client-common/src/main/java/org/apache/hudi/table/action/cluster/strategy/PartitionAwareClusteringPlanStrategy.java", "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.avro.model.HoodieClusteringStrategy;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.common.fs.FSUtils;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.table.HoodieTableMetaClient;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieIOException;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+/**\n+ * Scheduling strategy with restriction that clustering groups can only contain files from same partition.\n+ */\n+public abstract class PartitionAwareClusteringPlanStrategy<T extends HoodieRecordPayload,I,K,O> extends ClusteringPlanStrategy<T,I,K,O> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjExOTY2OQ=="}, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyMjI2NA==", "bodyText": "Something like hoodie.clustering.small.file.limit", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546122264", "createdAt": "2020-12-18T22:33:32Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/client/clustering/plan/strategy/SparkBoundedDayBasedClusteringPlanStrategy.java", "diffHunk": "@@ -0,0 +1,124 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.client.clustering.plan.strategy;\n+\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.common.model.FileSlice;\n+import org.apache.hudi.common.model.HoodieBaseFile;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.table.HoodieSparkCopyOnWriteTable;\n+import org.apache.hudi.table.HoodieSparkMergeOnReadTable;\n+import org.apache.hudi.table.action.cluster.strategy.PartitionAwareClusteringPlanStrategy;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import static org.apache.hudi.config.HoodieClusteringConfig.SORT_COLUMNS_PROPERTY;\n+\n+/**\n+ * Clustering Strategy based on following.\n+ * 1) Spark execution engine.\n+ * 2) Limits amount of data per clustering operation.\n+ */\n+public class SparkBoundedDayBasedClusteringPlanStrategy<T extends HoodieRecordPayload<T>>\n+    extends PartitionAwareClusteringPlanStrategy<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>> {\n+  private static final Logger LOG = LogManager.getLogger(SparkBoundedDayBasedClusteringPlanStrategy.class);\n+\n+  public SparkBoundedDayBasedClusteringPlanStrategy(HoodieSparkCopyOnWriteTable<T> table,\n+                                                    HoodieSparkEngineContext engineContext,\n+                                                    HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  public SparkBoundedDayBasedClusteringPlanStrategy(HoodieSparkMergeOnReadTable<T> table,\n+                                                    HoodieSparkEngineContext engineContext,\n+                                                    HoodieWriteConfig writeConfig) {\n+    super(table, engineContext, writeConfig);\n+  }\n+\n+  @Override\n+  protected Stream<HoodieClusteringGroup> buildClusteringGroupsForPartition(String partitionPath, List<FileSlice> fileSlices) {\n+    List<Pair<List<FileSlice>, Integer>> fileSliceGroups = new ArrayList<>();\n+    List<FileSlice> currentGroup = new ArrayList<>();\n+    int totalSizeSoFar = 0;\n+    for (FileSlice currentSlice : fileSlices) {\n+      // assume each filegroup size is ~= parquet.max.file.size\n+      totalSizeSoFar += currentSlice.getBaseFile().isPresent() ? currentSlice.getBaseFile().get().getFileSize() : getWriteConfig().getParquetMaxFileSize();\n+      // check if max size is reached and create new group, if needed.\n+      if (totalSizeSoFar >= getWriteConfig().getClusteringMaxBytesInGroup() && !currentGroup.isEmpty()) {\n+        fileSliceGroups.add(Pair.of(currentGroup, getNumberOfOutputFileGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+        currentGroup = new ArrayList<>();\n+        totalSizeSoFar = 0;\n+      }\n+      currentGroup.add(currentSlice);\n+    }\n+    if (!currentGroup.isEmpty()) {\n+      fileSliceGroups.add(Pair.of(currentGroup, getNumberOfOutputFileGroups(totalSizeSoFar, getWriteConfig().getClusteringTargetFileMaxBytes())));\n+    }\n+\n+    return fileSliceGroups.stream().map(fileSliceGroup -> HoodieClusteringGroup.newBuilder()\n+        .setSlices(getFileSliceInfo(fileSliceGroup.getLeft()))\n+        .setNumOutputFileGroups(fileSliceGroup.getRight())\n+        .setMetrics(buildMetrics(fileSliceGroup.getLeft()))\n+        .build());\n+  }\n+\n+  @Override\n+  protected Map<String, String> getStrategyParams() {\n+    Map<String, String> params = new HashMap<>();\n+    if (getWriteConfig().getProps().containsKey(SORT_COLUMNS_PROPERTY)) {\n+      params.put(SORT_COLUMNS_PROPERTY, getWriteConfig().getProps().getProperty(SORT_COLUMNS_PROPERTY));\n+    }\n+    return params;\n+  }\n+\n+  @Override\n+  protected List<String> filterPartitionPaths(List<String> partitionPaths) {\n+    int targetPartitionsForClustering = getWriteConfig().getTargetPartitionsForClustering();\n+    return partitionPaths.stream()\n+        .sorted(Comparator.reverseOrder())\n+        .limit(targetPartitionsForClustering > 0 ? targetPartitionsForClustering : partitionPaths.size())\n+        .collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  protected Stream<FileSlice> getFileSlicesEligibleForClustering(final String partition) {\n+    return super.getFileSlicesEligibleForClustering(partition)\n+        // files that have basefile size larger than clustering target file size are not eligible (Note that compaction can merge any updates)\n+        .filter(slice -> slice.getBaseFile().map(HoodieBaseFile::getFileSize).orElse(0L) < getWriteConfig().getClusteringTargetFileMaxBytes());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyMzA3MQ==", "bodyText": "we can add SerializableSchema (we do this for Hadoop Config object) ?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546123071", "createdAt": "2020-12-18T22:36:11Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.execution.bulkinsert;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+/**\n+ * A partitioner that does sorting based on specified column values for each RDD partition.\n+ *\n+ * @param <T> HoodieRecordPayload type\n+ */\n+public class RDDCustomColumnsSortPartitioner<T extends HoodieRecordPayload>\n+    implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {\n+\n+  private final String[] sortColumnNames;\n+  private final String schemaString;\n+\n+  public RDDCustomColumnsSortPartitioner(String[] columnNames, Schema schema) {\n+    this.sortColumnNames = columnNames;\n+    //TODO Schema is not serializable. So convert to String here. Figure out how to improve this\n+    this.schemaString = schema.toString();\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> repartitionRecords(JavaRDD<HoodieRecord<T>> records,\n+                                                     int outputSparkPartitions) {\n+    final String[] sortColumns = this.sortColumnNames;\n+    final String schemaStr = this.schemaString;\n+    return records.sortBy(record -> {\n+      Schema schema = new Schema.Parser().parse(schemaStr);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyMzYzNw==", "bodyText": "See SerialiableConfiguration for reference", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546123637", "createdAt": "2020-12-18T22:38:03Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/execution/bulkinsert/RDDCustomColumnsSortPartitioner.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.execution.bulkinsert;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.table.BulkInsertPartitioner;\n+import org.apache.spark.api.java.JavaRDD;\n+\n+/**\n+ * A partitioner that does sorting based on specified column values for each RDD partition.\n+ *\n+ * @param <T> HoodieRecordPayload type\n+ */\n+public class RDDCustomColumnsSortPartitioner<T extends HoodieRecordPayload>\n+    implements BulkInsertPartitioner<JavaRDD<HoodieRecord<T>>> {\n+\n+  private final String[] sortColumnNames;\n+  private final String schemaString;\n+\n+  public RDDCustomColumnsSortPartitioner(String[] columnNames, Schema schema) {\n+    this.sortColumnNames = columnNames;\n+    //TODO Schema is not serializable. So convert to String here. Figure out how to improve this\n+    this.schemaString = schema.toString();\n+  }\n+\n+  @Override\n+  public JavaRDD<HoodieRecord<T>> repartitionRecords(JavaRDD<HoodieRecord<T>> records,\n+                                                     int outputSparkPartitions) {\n+    final String[] sortColumns = this.sortColumnNames;\n+    final String schemaStr = this.schemaString;\n+    return records.sortBy(record -> {\n+      Schema schema = new Schema.Parser().parse(schemaStr);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyMzA3MQ=="}, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyNDE4Mg==", "bodyText": "SparkClusteringCommitActionExecutor?", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546124182", "createdAt": "2020-12-18T22:39:48Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.client.utils.ConcatenatingIterator;\n+import org.apache.hudi.common.model.ClusteringOperation;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyNDgzNg==", "bodyText": "this is very cool!", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546124836", "createdAt": "2020-12-18T22:42:07Z", "author": {"login": "vinothchandar"}, "path": "hudi-client/hudi-spark-client/src/main/java/org/apache/hudi/table/action/cluster/SparkRunClusteringCommitActionExecutor.java", "diffHunk": "@@ -0,0 +1,230 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.table.action.cluster;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hudi.avro.HoodieAvroUtils;\n+import org.apache.hudi.avro.model.HoodieClusteringGroup;\n+import org.apache.hudi.avro.model.HoodieClusteringPlan;\n+import org.apache.hudi.client.SparkTaskContextSupplier;\n+import org.apache.hudi.client.WriteStatus;\n+import org.apache.hudi.client.common.HoodieEngineContext;\n+import org.apache.hudi.client.common.HoodieSparkEngineContext;\n+import org.apache.hudi.client.utils.ConcatenatingIterator;\n+import org.apache.hudi.common.model.ClusteringOperation;\n+import org.apache.hudi.common.model.HoodieCommitMetadata;\n+import org.apache.hudi.common.model.HoodieKey;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.model.WriteOperationType;\n+import org.apache.hudi.common.table.log.HoodieFileSliceReader;\n+import org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner;\n+import org.apache.hudi.common.table.timeline.HoodieInstant;\n+import org.apache.hudi.common.table.timeline.HoodieTimeline;\n+import org.apache.hudi.common.util.ClusteringUtils;\n+import org.apache.hudi.common.util.CommitUtils;\n+import org.apache.hudi.common.util.Option;\n+import org.apache.hudi.common.util.ReflectionUtils;\n+import org.apache.hudi.common.util.collection.Pair;\n+import org.apache.hudi.config.HoodieWriteConfig;\n+import org.apache.hudi.exception.HoodieClusteringException;\n+import org.apache.hudi.io.IOUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+import org.apache.hudi.io.storage.HoodieFileReaderFactory;\n+import org.apache.hudi.table.HoodieTable;\n+import org.apache.hudi.table.action.HoodieWriteMetadata;\n+import org.apache.hudi.table.action.cluster.strategy.RunClusteringStrategy;\n+import org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor;\n+import org.apache.log4j.LogManager;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.stream.Collectors;\n+\n+public class SparkRunClusteringCommitActionExecutor<T extends HoodieRecordPayload<T>>\n+    extends BaseSparkCommitActionExecutor<T> {\n+\n+  private static final Logger LOG = LogManager.getLogger(SparkRunClusteringCommitActionExecutor.class);\n+  private final HoodieClusteringPlan clusteringPlan;\n+\n+  public SparkRunClusteringCommitActionExecutor(HoodieEngineContext context,\n+                                                HoodieWriteConfig config, HoodieTable table,\n+                                                String instantTime) {\n+    super(context, config, table, instantTime, WriteOperationType.CLUSTER);\n+    this.clusteringPlan = ClusteringUtils.getClusteringPlan(table.getMetaClient(), HoodieTimeline.getReplaceCommitRequestedInstant(instantTime))\n+      .map(Pair::getRight).orElseThrow(() -> new HoodieClusteringException(\"Unable to read clustering plan for instant: \" + instantTime));\n+  }\n+\n+  @Override\n+  public HoodieWriteMetadata<JavaRDD<WriteStatus>> execute() {\n+    HoodieInstant instant = HoodieTimeline.getReplaceCommitRequestedInstant(instantTime);\n+    // Mark instant as clustering inflight\n+    table.getActiveTimeline().transitionReplaceRequestedToInflight(instant, Option.empty());\n+    table.getMetaClient().reloadActiveTimeline();\n+\n+    JavaSparkContext engineContext = HoodieSparkEngineContext.getSparkContext(context);\n+    // run clustering for each group async and collect WriteStatus\n+    JavaRDD<WriteStatus> writeStatusRDD = clusteringPlan.getInputGroups().stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjEyNTg2Nw==", "bodyText": "Nonetheless we should file a code cleanup JIRA to provide these iterators as core building blocks under a nice abstractions.", "url": "https://github.com/apache/hudi/pull/2263#discussion_r546125867", "createdAt": "2020-12-18T22:45:39Z", "author": {"login": "vinothchandar"}, "path": "hudi-common/src/main/java/org/apache/hudi/common/table/log/HoodieFileSliceReader.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hudi.common.table.log;\n+\n+import org.apache.avro.Schema;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.hudi.common.model.HoodieRecord;\n+import org.apache.hudi.common.model.HoodieRecordPayload;\n+import org.apache.hudi.common.util.SpillableMapUtils;\n+import org.apache.hudi.io.storage.HoodieFileReader;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.Iterator;\n+\n+/**\n+ * Reads records from base file and merges any updates from log files and provides iterable over all records in the file slice.\n+ */\n+public class HoodieFileSliceReader implements Iterable<HoodieRecord<? extends HoodieRecordPayload>> {\n+  private HoodieMergedLogRecordScanner logRecordScanner;\n+\n+  public static <R extends IndexedRecord, T extends HoodieRecordPayload> HoodieFileSliceReader getFileSliceReader(\n+      HoodieFileReader<R> baseFileReader, HoodieMergedLogRecordScanner scanner, Schema schema, String payloadClass) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY1MTU0MQ=="}, "originalCommit": {"oid": "2f632f1f049281d2e3e4ad18a1d2217b3c7bdd1c"}, "originalPosition": 40}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "039e6a5931e0da738582b5e7c35691cdf76e803a", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/039e6a5931e0da738582b5e7c35691cdf76e803a", "committedDate": "2020-12-18T03:38:49Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}, "afterCommit": {"oid": "6191f6c2b487c2c16947aff8b1ff75ea209b0f08", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/6191f6c2b487c2c16947aff8b1ff75ea209b0f08", "committedDate": "2020-12-21T20:11:57Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6191f6c2b487c2c16947aff8b1ff75ea209b0f08", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/6191f6c2b487c2c16947aff8b1ff75ea209b0f08", "committedDate": "2020-12-21T20:11:57Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}, "afterCommit": {"oid": "27935d447e7a32def984fe1c542893fc578cedb9", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/27935d447e7a32def984fe1c542893fc578cedb9", "committedDate": "2020-12-21T23:06:24Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6dc03b65bfd98d0a1fde2d48ebb31f36b1186cf9", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/6dc03b65bfd98d0a1fde2d48ebb31f36b1186cf9", "committedDate": "2020-12-22T01:34:15Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "27935d447e7a32def984fe1c542893fc578cedb9", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/27935d447e7a32def984fe1c542893fc578cedb9", "committedDate": "2020-12-21T23:06:24Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}, "afterCommit": {"oid": "6dc03b65bfd98d0a1fde2d48ebb31f36b1186cf9", "author": {"user": {"login": "satishkotha", "name": null}}, "url": "https://github.com/apache/hudi/commit/6dc03b65bfd98d0a1fde2d48ebb31f36b1186cf9", "committedDate": "2020-12-22T01:34:15Z", "message": "[HUDI-1075] Implement simple clustering strategies to create ClusteringPlan and to run the plan"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4330, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}